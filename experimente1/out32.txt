no display found. Using non-interactive Agg backend
[5, 5, 5]
gpu id:0
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 7416
used     : 3603


Cifar10: True; cifar100: False
False
Files already downloaded and verified

Arch Num:  [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/25]	Time 0.753 (0.753)	Data 0.717 (0.717)	Loss 3.6262 (3.6262)	Acc@1 9.180 (9.180)	Acc@5 48.828 (48.828)
Epoch: [1][1/25]	Time 0.708 (0.731)	Data 0.005 (0.361)	Loss 4.6162 (4.1212)	Acc@1 10.791 (9.985)	Acc@5 52.637 (50.732)
Epoch: [1][2/25]	Time 0.645 (0.702)	Data 0.004 (0.242)	Loss 5.0413 (4.4279)	Acc@1 14.746 (11.572)	Acc@5 51.758 (51.074)
Epoch: [1][3/25]	Time 0.644 (0.688)	Data 0.006 (0.183)	Loss 5.8782 (4.7905)	Acc@1 10.303 (11.255)	Acc@5 50.000 (50.806)
Epoch: [1][4/25]	Time 0.704 (0.691)	Data 0.009 (0.148)	Loss 5.0920 (4.8508)	Acc@1 10.693 (11.143)	Acc@5 51.123 (50.869)
Epoch: [1][5/25]	Time 0.675 (0.688)	Data 0.004 (0.124)	Loss 5.5642 (4.9697)	Acc@1 12.891 (11.434)	Acc@5 49.951 (50.716)
Epoch: [1][6/25]	Time 0.660 (0.684)	Data 0.005 (0.107)	Loss 4.5425 (4.9087)	Acc@1 10.547 (11.307)	Acc@5 48.828 (50.446)
Epoch: [1][7/25]	Time 0.650 (0.680)	Data 0.005 (0.094)	Loss 3.8270 (4.7735)	Acc@1 10.791 (11.243)	Acc@5 49.805 (50.366)
Epoch: [1][8/25]	Time 0.678 (0.680)	Data 0.005 (0.084)	Loss 3.7010 (4.6543)	Acc@1 10.254 (11.133)	Acc@5 50.391 (50.369)
Epoch: [1][9/25]	Time 0.706 (0.682)	Data 0.008 (0.077)	Loss 3.6064 (4.5495)	Acc@1 11.865 (11.206)	Acc@5 50.830 (50.415)
Epoch: [1][10/25]	Time 0.608 (0.675)	Data 0.008 (0.070)	Loss 3.2509 (4.4314)	Acc@1 10.254 (11.119)	Acc@5 51.172 (50.484)
Epoch: [1][11/25]	Time 0.586 (0.668)	Data 0.009 (0.065)	Loss 3.3012 (4.3373)	Acc@1 9.473 (10.982)	Acc@5 48.096 (50.285)
Epoch: [1][12/25]	Time 0.602 (0.663)	Data 0.007 (0.061)	Loss 3.3251 (4.2594)	Acc@1 11.133 (10.994)	Acc@5 48.145 (50.120)
Epoch: [1][13/25]	Time 0.681 (0.664)	Data 0.008 (0.057)	Loss 3.4433 (4.2011)	Acc@1 9.912 (10.917)	Acc@5 49.902 (50.105)
Epoch: [1][14/25]	Time 0.678 (0.665)	Data 0.007 (0.054)	Loss 3.3103 (4.1417)	Acc@1 11.133 (10.931)	Acc@5 51.660 (50.208)
Epoch: [1][15/25]	Time 0.695 (0.667)	Data 0.007 (0.051)	Loss 3.2793 (4.0878)	Acc@1 9.961 (10.870)	Acc@5 51.758 (50.305)
Epoch: [1][16/25]	Time 0.668 (0.667)	Data 0.005 (0.048)	Loss 3.3588 (4.0449)	Acc@1 10.693 (10.860)	Acc@5 51.465 (50.373)
Epoch: [1][17/25]	Time 0.710 (0.669)	Data 0.007 (0.046)	Loss 3.2862 (4.0028)	Acc@1 10.205 (10.824)	Acc@5 52.002 (50.464)
Epoch: [1][18/25]	Time 0.629 (0.667)	Data 0.005 (0.044)	Loss 3.2923 (3.9654)	Acc@1 10.742 (10.819)	Acc@5 53.809 (50.640)
Epoch: [1][19/25]	Time 0.642 (0.666)	Data 0.005 (0.042)	Loss 3.2756 (3.9309)	Acc@1 10.547 (10.806)	Acc@5 51.514 (50.684)
Epoch: [1][20/25]	Time 0.685 (0.667)	Data 0.008 (0.040)	Loss 3.3092 (3.9013)	Acc@1 11.865 (10.856)	Acc@5 52.588 (50.774)
Epoch: [1][21/25]	Time 0.702 (0.669)	Data 0.005 (0.038)	Loss 3.3085 (3.8744)	Acc@1 10.254 (10.829)	Acc@5 50.488 (50.761)
Epoch: [1][22/25]	Time 0.643 (0.667)	Data 0.005 (0.037)	Loss 3.2823 (3.8486)	Acc@1 10.645 (10.821)	Acc@5 52.490 (50.836)
Epoch: [1][23/25]	Time 0.636 (0.666)	Data 0.006 (0.036)	Loss 3.2919 (3.8254)	Acc@1 12.549 (10.893)	Acc@5 53.076 (50.930)
Epoch: [1][24/25]	Time 0.377 (0.655)	Data 0.007 (0.035)	Loss 3.3039 (3.8166)	Acc@1 7.547 (10.836)	Acc@5 48.349 (50.886)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/25]	Time 0.683 (0.683)	Data 0.662 (0.662)	Loss 3.2982 (3.2982)	Acc@1 9.570 (9.570)	Acc@5 50.391 (50.391)
Epoch: [2][1/25]	Time 0.646 (0.664)	Data 0.005 (0.333)	Loss 3.2966 (3.2974)	Acc@1 11.719 (10.645)	Acc@5 52.344 (51.367)
Epoch: [2][2/25]	Time 0.649 (0.659)	Data 0.007 (0.225)	Loss 3.3319 (3.3089)	Acc@1 12.549 (11.279)	Acc@5 50.928 (51.221)
Epoch: [2][3/25]	Time 0.653 (0.658)	Data 0.008 (0.170)	Loss 3.3044 (3.3078)	Acc@1 10.400 (11.060)	Acc@5 50.781 (51.111)
Epoch: [2][4/25]	Time 0.720 (0.670)	Data 0.007 (0.138)	Loss 3.3152 (3.3093)	Acc@1 9.863 (10.820)	Acc@5 50.732 (51.035)
Epoch: [2][5/25]	Time 0.623 (0.662)	Data 0.008 (0.116)	Loss 3.3000 (3.3077)	Acc@1 11.133 (10.872)	Acc@5 53.955 (51.522)
Epoch: [2][6/25]	Time 0.586 (0.651)	Data 0.004 (0.100)	Loss 3.3032 (3.3071)	Acc@1 10.107 (10.763)	Acc@5 52.979 (51.730)
Epoch: [2][7/25]	Time 0.601 (0.645)	Data 0.007 (0.088)	Loss 3.3077 (3.3072)	Acc@1 11.670 (10.876)	Acc@5 51.367 (51.685)
Epoch: [2][8/25]	Time 0.593 (0.639)	Data 0.004 (0.079)	Loss 3.3008 (3.3065)	Acc@1 11.670 (10.965)	Acc@5 53.369 (51.872)
Epoch: [2][9/25]	Time 0.598 (0.635)	Data 0.007 (0.072)	Loss 3.3018 (3.3060)	Acc@1 10.400 (10.908)	Acc@5 53.564 (52.041)
Epoch: [2][10/25]	Time 0.591 (0.631)	Data 0.007 (0.066)	Loss 3.3001 (3.3055)	Acc@1 11.816 (10.991)	Acc@5 52.148 (52.051)
Epoch: [2][11/25]	Time 0.626 (0.631)	Data 0.007 (0.061)	Loss 3.2987 (3.3049)	Acc@1 11.719 (11.051)	Acc@5 53.223 (52.148)
Epoch: [2][12/25]	Time 0.625 (0.630)	Data 0.007 (0.057)	Loss 3.3076 (3.3051)	Acc@1 12.646 (11.174)	Acc@5 55.713 (52.423)
Epoch: [2][13/25]	Time 0.611 (0.629)	Data 0.007 (0.053)	Loss 3.3038 (3.3050)	Acc@1 11.035 (11.164)	Acc@5 53.271 (52.483)
Epoch: [2][14/25]	Time 0.560 (0.624)	Data 0.004 (0.050)	Loss 3.2939 (3.3043)	Acc@1 11.182 (11.165)	Acc@5 51.416 (52.412)
Epoch: [2][15/25]	Time 0.575 (0.621)	Data 0.007 (0.047)	Loss 3.2963 (3.3038)	Acc@1 11.816 (11.206)	Acc@5 53.369 (52.472)
Epoch: [2][16/25]	Time 0.584 (0.619)	Data 0.006 (0.045)	Loss 3.2875 (3.3028)	Acc@1 10.693 (11.176)	Acc@5 54.102 (52.568)
Epoch: [2][17/25]	Time 0.609 (0.618)	Data 0.005 (0.043)	Loss 3.2863 (3.3019)	Acc@1 12.109 (11.228)	Acc@5 54.102 (52.653)
Epoch: [2][18/25]	Time 0.606 (0.618)	Data 0.004 (0.041)	Loss 3.2902 (3.3013)	Acc@1 13.086 (11.326)	Acc@5 53.955 (52.722)
Epoch: [2][19/25]	Time 0.630 (0.618)	Data 0.006 (0.039)	Loss 3.2868 (3.3006)	Acc@1 11.230 (11.321)	Acc@5 53.223 (52.747)
Epoch: [2][20/25]	Time 0.708 (0.623)	Data 0.006 (0.037)	Loss 3.2888 (3.3000)	Acc@1 10.791 (11.296)	Acc@5 53.809 (52.797)
Epoch: [2][21/25]	Time 0.712 (0.627)	Data 0.007 (0.036)	Loss 3.2814 (3.2992)	Acc@1 11.914 (11.324)	Acc@5 51.465 (52.737)
Epoch: [2][22/25]	Time 0.639 (0.627)	Data 0.007 (0.035)	Loss 3.2742 (3.2981)	Acc@1 12.012 (11.354)	Acc@5 55.225 (52.845)
Epoch: [2][23/25]	Time 0.602 (0.626)	Data 0.006 (0.033)	Loss 3.2697 (3.2969)	Acc@1 12.891 (11.418)	Acc@5 56.787 (53.009)
Epoch: [2][24/25]	Time 0.341 (0.615)	Data 0.004 (0.032)	Loss 3.2744 (3.2965)	Acc@1 11.675 (11.422)	Acc@5 54.009 (53.026)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/25]	Time 0.686 (0.686)	Data 0.723 (0.723)	Loss 3.2650 (3.2650)	Acc@1 12.012 (12.012)	Acc@5 54.785 (54.785)
Epoch: [3][1/25]	Time 0.635 (0.661)	Data 0.003 (0.363)	Loss 3.2606 (3.2628)	Acc@1 11.865 (11.938)	Acc@5 56.055 (55.420)
Epoch: [3][2/25]	Time 0.605 (0.642)	Data 0.007 (0.244)	Loss 3.2619 (3.2625)	Acc@1 12.500 (12.126)	Acc@5 57.520 (56.120)
Epoch: [3][3/25]	Time 0.603 (0.632)	Data 0.006 (0.185)	Loss 3.2537 (3.2603)	Acc@1 11.572 (11.987)	Acc@5 57.617 (56.494)
Epoch: [3][4/25]	Time 0.669 (0.640)	Data 0.006 (0.149)	Loss 3.2621 (3.2607)	Acc@1 12.598 (12.109)	Acc@5 57.959 (56.787)
Epoch: [3][5/25]	Time 0.714 (0.652)	Data 0.006 (0.125)	Loss 3.2543 (3.2596)	Acc@1 10.791 (11.890)	Acc@5 54.883 (56.470)
Epoch: [3][6/25]	Time 0.653 (0.652)	Data 0.004 (0.108)	Loss 3.2449 (3.2575)	Acc@1 11.426 (11.823)	Acc@5 56.494 (56.473)
Epoch: [3][7/25]	Time 0.655 (0.653)	Data 0.008 (0.095)	Loss 3.2416 (3.2555)	Acc@1 10.645 (11.676)	Acc@5 57.275 (56.573)
Epoch: [3][8/25]	Time 0.711 (0.659)	Data 0.004 (0.085)	Loss 3.2397 (3.2538)	Acc@1 12.500 (11.768)	Acc@5 58.301 (56.765)
Epoch: [3][9/25]	Time 0.679 (0.661)	Data 0.005 (0.077)	Loss 3.2421 (3.2526)	Acc@1 10.938 (11.685)	Acc@5 56.689 (56.758)
Epoch: [3][10/25]	Time 0.682 (0.663)	Data 0.006 (0.071)	Loss 3.2381 (3.2513)	Acc@1 13.574 (11.856)	Acc@5 57.812 (56.854)
Epoch: [3][11/25]	Time 0.687 (0.665)	Data 0.006 (0.065)	Loss 3.2324 (3.2497)	Acc@1 12.354 (11.898)	Acc@5 58.887 (57.023)
Epoch: [3][12/25]	Time 0.699 (0.668)	Data 0.005 (0.061)	Loss 3.2305 (3.2482)	Acc@1 13.672 (12.034)	Acc@5 58.057 (57.103)
Epoch: [3][13/25]	Time 0.639 (0.666)	Data 0.004 (0.057)	Loss 3.2288 (3.2468)	Acc@1 12.109 (12.040)	Acc@5 55.176 (56.965)
Epoch: [3][14/25]	Time 0.682 (0.667)	Data 0.007 (0.053)	Loss 3.2200 (3.2451)	Acc@1 12.061 (12.041)	Acc@5 57.178 (56.979)
Epoch: [3][15/25]	Time 0.716 (0.670)	Data 0.005 (0.050)	Loss 3.2164 (3.2433)	Acc@1 12.012 (12.039)	Acc@5 56.689 (56.961)
Epoch: [3][16/25]	Time 0.660 (0.669)	Data 0.005 (0.048)	Loss 3.2165 (3.2417)	Acc@1 11.768 (12.023)	Acc@5 57.178 (56.974)
Epoch: [3][17/25]	Time 0.677 (0.670)	Data 0.004 (0.045)	Loss 3.2146 (3.2402)	Acc@1 11.719 (12.006)	Acc@5 55.078 (56.868)
Epoch: [3][18/25]	Time 0.698 (0.671)	Data 0.005 (0.043)	Loss 3.2002 (3.2381)	Acc@1 13.574 (12.089)	Acc@5 59.229 (56.993)
Epoch: [3][19/25]	Time 0.661 (0.671)	Data 0.006 (0.041)	Loss 3.2110 (3.2367)	Acc@1 11.816 (12.075)	Acc@5 56.299 (56.958)
Epoch: [3][20/25]	Time 0.648 (0.670)	Data 0.005 (0.040)	Loss 3.2055 (3.2352)	Acc@1 13.037 (12.121)	Acc@5 55.762 (56.901)
Epoch: [3][21/25]	Time 0.618 (0.667)	Data 0.007 (0.038)	Loss 3.1996 (3.2336)	Acc@1 13.818 (12.198)	Acc@5 57.373 (56.922)
Epoch: [3][22/25]	Time 0.638 (0.666)	Data 0.006 (0.037)	Loss 3.1943 (3.2319)	Acc@1 13.477 (12.254)	Acc@5 57.178 (56.934)
Epoch: [3][23/25]	Time 0.712 (0.668)	Data 0.006 (0.035)	Loss 3.1963 (3.2304)	Acc@1 13.623 (12.311)	Acc@5 56.396 (56.911)
Epoch: [3][24/25]	Time 0.435 (0.659)	Data 0.005 (0.034)	Loss 3.1852 (3.2297)	Acc@1 13.090 (12.324)	Acc@5 63.090 (57.016)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/25]	Time 0.699 (0.699)	Data 0.617 (0.617)	Loss 3.1824 (3.1824)	Acc@1 12.109 (12.109)	Acc@5 57.422 (57.422)
Epoch: [4][1/25]	Time 0.692 (0.696)	Data 0.004 (0.311)	Loss 3.1784 (3.1804)	Acc@1 13.037 (12.573)	Acc@5 58.545 (57.983)
Epoch: [4][2/25]	Time 0.727 (0.706)	Data 0.006 (0.209)	Loss 3.1830 (3.1812)	Acc@1 13.574 (12.907)	Acc@5 57.812 (57.926)
Epoch: [4][3/25]	Time 0.629 (0.687)	Data 0.005 (0.158)	Loss 3.1648 (3.1771)	Acc@1 11.377 (12.524)	Acc@5 58.350 (58.032)
Epoch: [4][4/25]	Time 0.613 (0.672)	Data 0.008 (0.128)	Loss 3.1557 (3.1728)	Acc@1 14.502 (12.920)	Acc@5 60.205 (58.467)
Epoch: [4][5/25]	Time 0.659 (0.670)	Data 0.005 (0.108)	Loss 3.1688 (3.1722)	Acc@1 12.988 (12.931)	Acc@5 58.154 (58.415)
Epoch: [4][6/25]	Time 0.717 (0.677)	Data 0.006 (0.093)	Loss 3.1642 (3.1710)	Acc@1 12.744 (12.905)	Acc@5 58.008 (58.357)
Epoch: [4][7/25]	Time 0.690 (0.678)	Data 0.005 (0.082)	Loss 3.1509 (3.1685)	Acc@1 13.916 (13.031)	Acc@5 61.328 (58.728)
Epoch: [4][8/25]	Time 0.694 (0.680)	Data 0.004 (0.074)	Loss 3.1531 (3.1668)	Acc@1 14.941 (13.243)	Acc@5 58.545 (58.708)
Epoch: [4][9/25]	Time 0.737 (0.686)	Data 0.008 (0.067)	Loss 3.1462 (3.1647)	Acc@1 14.355 (13.354)	Acc@5 60.352 (58.872)
Epoch: [4][10/25]	Time 0.659 (0.683)	Data 0.005 (0.061)	Loss 3.1394 (3.1624)	Acc@1 14.795 (13.485)	Acc@5 59.961 (58.971)
Epoch: [4][11/25]	Time 0.626 (0.679)	Data 0.005 (0.057)	Loss 3.1431 (3.1608)	Acc@1 14.258 (13.550)	Acc@5 60.693 (59.115)
Epoch: [4][12/25]	Time 0.608 (0.673)	Data 0.007 (0.053)	Loss 3.1377 (3.1590)	Acc@1 14.941 (13.657)	Acc@5 61.719 (59.315)
Epoch: [4][13/25]	Time 0.602 (0.668)	Data 0.005 (0.049)	Loss 3.1185 (3.1561)	Acc@1 14.697 (13.731)	Acc@5 64.990 (59.720)
Epoch: [4][14/25]	Time 0.629 (0.665)	Data 0.010 (0.047)	Loss 3.1584 (3.1563)	Acc@1 12.598 (13.656)	Acc@5 59.082 (59.678)
Epoch: [4][15/25]	Time 0.671 (0.666)	Data 0.005 (0.044)	Loss 3.1251 (3.1544)	Acc@1 13.721 (13.660)	Acc@5 61.328 (59.781)
Epoch: [4][16/25]	Time 0.723 (0.669)	Data 0.005 (0.042)	Loss 3.1151 (3.1520)	Acc@1 14.795 (13.726)	Acc@5 62.109 (59.918)
Epoch: [4][17/25]	Time 0.651 (0.668)	Data 0.005 (0.040)	Loss 3.1234 (3.1505)	Acc@1 11.621 (13.609)	Acc@5 61.670 (60.015)
Epoch: [4][18/25]	Time 0.610 (0.665)	Data 0.004 (0.038)	Loss 3.1067 (3.1482)	Acc@1 14.746 (13.669)	Acc@5 64.746 (60.264)
Epoch: [4][19/25]	Time 0.649 (0.664)	Data 0.008 (0.036)	Loss 3.0905 (3.1453)	Acc@1 15.332 (13.752)	Acc@5 64.404 (60.471)
Epoch: [4][20/25]	Time 0.712 (0.667)	Data 0.008 (0.035)	Loss 3.1049 (3.1433)	Acc@1 15.332 (13.828)	Acc@5 62.500 (60.568)
Epoch: [4][21/25]	Time 0.716 (0.669)	Data 0.006 (0.034)	Loss 3.0921 (3.1410)	Acc@1 15.625 (13.909)	Acc@5 63.916 (60.720)
Epoch: [4][22/25]	Time 0.642 (0.668)	Data 0.005 (0.033)	Loss 3.0925 (3.1389)	Acc@1 13.428 (13.888)	Acc@5 63.867 (60.857)
Epoch: [4][23/25]	Time 0.671 (0.668)	Data 0.006 (0.031)	Loss 3.0835 (3.1366)	Acc@1 14.893 (13.930)	Acc@5 62.988 (60.946)
Epoch: [4][24/25]	Time 0.427 (0.658)	Data 0.007 (0.030)	Loss 3.0858 (3.1357)	Acc@1 14.387 (13.938)	Acc@5 64.505 (61.006)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/25]	Time 0.710 (0.710)	Data 0.689 (0.689)	Loss 3.0690 (3.0690)	Acc@1 14.697 (14.697)	Acc@5 65.771 (65.771)
Epoch: [5][1/25]	Time 0.652 (0.681)	Data 0.003 (0.346)	Loss 3.0755 (3.0722)	Acc@1 13.574 (14.136)	Acc@5 64.355 (65.063)
Epoch: [5][2/25]	Time 0.687 (0.683)	Data 0.006 (0.233)	Loss 3.0759 (3.0735)	Acc@1 14.209 (14.160)	Acc@5 62.891 (64.339)
Epoch: [5][3/25]	Time 0.728 (0.694)	Data 0.007 (0.177)	Loss 3.0538 (3.0686)	Acc@1 15.771 (14.563)	Acc@5 65.479 (64.624)
Epoch: [5][4/25]	Time 0.624 (0.680)	Data 0.008 (0.143)	Loss 3.0405 (3.0630)	Acc@1 14.893 (14.629)	Acc@5 67.090 (65.117)
Epoch: [5][5/25]	Time 0.598 (0.667)	Data 0.008 (0.120)	Loss 3.0363 (3.0585)	Acc@1 14.893 (14.673)	Acc@5 67.383 (65.495)
Epoch: [5][6/25]	Time 0.611 (0.659)	Data 0.006 (0.104)	Loss 3.0295 (3.0544)	Acc@1 17.139 (15.025)	Acc@5 67.578 (65.792)
Epoch: [5][7/25]	Time 0.614 (0.653)	Data 0.004 (0.092)	Loss 3.0203 (3.0501)	Acc@1 16.602 (15.222)	Acc@5 68.555 (66.138)
Epoch: [5][8/25]	Time 0.622 (0.650)	Data 0.007 (0.082)	Loss 3.0236 (3.0472)	Acc@1 16.748 (15.392)	Acc@5 67.822 (66.325)
Epoch: [5][9/25]	Time 0.668 (0.652)	Data 0.006 (0.075)	Loss 3.0084 (3.0433)	Acc@1 16.260 (15.479)	Acc@5 68.896 (66.582)
Epoch: [5][10/25]	Time 0.719 (0.658)	Data 0.006 (0.068)	Loss 2.9840 (3.0379)	Acc@1 17.920 (15.700)	Acc@5 72.021 (67.077)
Epoch: [5][11/25]	Time 0.735 (0.664)	Data 0.005 (0.063)	Loss 2.9919 (3.0341)	Acc@1 15.723 (15.702)	Acc@5 71.484 (67.444)
Epoch: [5][12/25]	Time 0.635 (0.662)	Data 0.006 (0.059)	Loss 3.0068 (3.0320)	Acc@1 16.260 (15.745)	Acc@5 69.434 (67.597)
Epoch: [5][13/25]	Time 0.634 (0.660)	Data 0.008 (0.055)	Loss 2.9678 (3.0274)	Acc@1 16.992 (15.834)	Acc@5 70.996 (67.840)
Epoch: [5][14/25]	Time 0.647 (0.659)	Data 0.007 (0.052)	Loss 2.9794 (3.0242)	Acc@1 19.189 (16.058)	Acc@5 70.996 (68.050)
Epoch: [5][15/25]	Time 0.593 (0.655)	Data 0.005 (0.049)	Loss 2.9431 (3.0191)	Acc@1 17.871 (16.171)	Acc@5 73.145 (68.369)
Epoch: [5][16/25]	Time 0.584 (0.651)	Data 0.007 (0.047)	Loss 2.9272 (3.0137)	Acc@1 18.262 (16.294)	Acc@5 73.242 (68.655)
Epoch: [5][17/25]	Time 0.605 (0.648)	Data 0.005 (0.044)	Loss 2.9118 (3.0081)	Acc@1 18.555 (16.420)	Acc@5 75.195 (69.019)
Epoch: [5][18/25]	Time 0.601 (0.646)	Data 0.007 (0.042)	Loss 2.9279 (3.0038)	Acc@1 17.334 (16.468)	Acc@5 73.193 (69.238)
Epoch: [5][19/25]	Time 0.618 (0.644)	Data 0.007 (0.040)	Loss 2.9143 (2.9994)	Acc@1 17.139 (16.501)	Acc@5 74.316 (69.492)
Epoch: [5][20/25]	Time 0.637 (0.644)	Data 0.009 (0.039)	Loss 2.9041 (2.9948)	Acc@1 18.994 (16.620)	Acc@5 75.439 (69.775)
Epoch: [5][21/25]	Time 0.672 (0.645)	Data 0.005 (0.037)	Loss 2.9115 (2.9910)	Acc@1 16.797 (16.628)	Acc@5 72.656 (69.906)
Epoch: [5][22/25]	Time 0.738 (0.649)	Data 0.005 (0.036)	Loss 2.8713 (2.9858)	Acc@1 17.480 (16.665)	Acc@5 76.123 (70.177)
Epoch: [5][23/25]	Time 0.648 (0.649)	Data 0.004 (0.035)	Loss 2.8887 (2.9818)	Acc@1 16.260 (16.648)	Acc@5 74.854 (70.372)
Epoch: [5][24/25]	Time 0.346 (0.637)	Data 0.009 (0.034)	Loss 2.8592 (2.9797)	Acc@1 20.401 (16.712)	Acc@5 76.533 (70.476)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/25]	Time 0.742 (0.742)	Data 0.698 (0.698)	Loss 2.8652 (2.8652)	Acc@1 18.506 (18.506)	Acc@5 75.293 (75.293)
Epoch: [6][1/25]	Time 0.639 (0.691)	Data 0.005 (0.351)	Loss 2.8335 (2.8494)	Acc@1 18.652 (18.579)	Acc@5 77.783 (76.538)
Epoch: [6][2/25]	Time 0.649 (0.677)	Data 0.007 (0.237)	Loss 2.8600 (2.8529)	Acc@1 18.506 (18.555)	Acc@5 76.270 (76.449)
Epoch: [6][3/25]	Time 0.657 (0.672)	Data 0.007 (0.179)	Loss 2.8614 (2.8550)	Acc@1 19.482 (18.787)	Acc@5 76.074 (76.355)
Epoch: [6][4/25]	Time 0.692 (0.676)	Data 0.008 (0.145)	Loss 2.8559 (2.8552)	Acc@1 19.043 (18.838)	Acc@5 75.781 (76.240)
Epoch: [6][5/25]	Time 0.735 (0.686)	Data 0.005 (0.122)	Loss 2.8401 (2.8527)	Acc@1 19.434 (18.937)	Acc@5 76.855 (76.343)
Epoch: [6][6/25]	Time 0.685 (0.686)	Data 0.007 (0.105)	Loss 2.8384 (2.8507)	Acc@1 18.799 (18.917)	Acc@5 76.807 (76.409)
Epoch: [6][7/25]	Time 0.631 (0.679)	Data 0.004 (0.093)	Loss 2.8311 (2.8482)	Acc@1 19.727 (19.019)	Acc@5 77.100 (76.495)
Epoch: [6][8/25]	Time 0.679 (0.679)	Data 0.008 (0.083)	Loss 2.8317 (2.8464)	Acc@1 20.605 (19.195)	Acc@5 76.611 (76.508)
Epoch: [6][9/25]	Time 0.748 (0.686)	Data 0.005 (0.075)	Loss 2.8276 (2.8445)	Acc@1 21.240 (19.399)	Acc@5 76.709 (76.528)
Epoch: [6][10/25]	Time 0.695 (0.687)	Data 0.007 (0.069)	Loss 2.8304 (2.8432)	Acc@1 22.021 (19.638)	Acc@5 77.832 (76.647)
Epoch: [6][11/25]	Time 0.697 (0.688)	Data 0.006 (0.064)	Loss 2.8335 (2.8424)	Acc@1 20.361 (19.698)	Acc@5 78.076 (76.766)
Epoch: [6][12/25]	Time 0.709 (0.689)	Data 0.006 (0.059)	Loss 2.7986 (2.8390)	Acc@1 21.973 (19.873)	Acc@5 78.613 (76.908)
Epoch: [6][13/25]	Time 0.723 (0.692)	Data 0.006 (0.056)	Loss 2.7908 (2.8356)	Acc@1 21.631 (19.999)	Acc@5 79.004 (77.058)
Epoch: [6][14/25]	Time 0.747 (0.695)	Data 0.006 (0.052)	Loss 2.7995 (2.8332)	Acc@1 20.752 (20.049)	Acc@5 78.613 (77.161)
Epoch: [6][15/25]	Time 0.681 (0.694)	Data 0.009 (0.050)	Loss 2.8302 (2.8330)	Acc@1 20.312 (20.065)	Acc@5 76.074 (77.094)
Epoch: [6][16/25]	Time 0.694 (0.694)	Data 0.007 (0.047)	Loss 2.7949 (2.8308)	Acc@1 22.949 (20.235)	Acc@5 77.979 (77.146)
Epoch: [6][17/25]	Time 0.704 (0.695)	Data 0.007 (0.045)	Loss 2.7709 (2.8274)	Acc@1 21.533 (20.307)	Acc@5 79.688 (77.287)
Epoch: [6][18/25]	Time 0.720 (0.696)	Data 0.004 (0.043)	Loss 2.7927 (2.8256)	Acc@1 22.607 (20.428)	Acc@5 77.393 (77.292)
Epoch: [6][19/25]	Time 0.739 (0.698)	Data 0.007 (0.041)	Loss 2.8091 (2.8248)	Acc@1 22.217 (20.518)	Acc@5 77.539 (77.305)
Epoch: [6][20/25]	Time 0.705 (0.699)	Data 0.005 (0.039)	Loss 2.7699 (2.8222)	Acc@1 22.510 (20.612)	Acc@5 78.906 (77.381)
Epoch: [6][21/25]	Time 0.719 (0.700)	Data 0.004 (0.038)	Loss 2.7532 (2.8190)	Acc@1 22.998 (20.721)	Acc@5 79.443 (77.475)
Epoch: [6][22/25]	Time 0.745 (0.702)	Data 0.007 (0.036)	Loss 2.7851 (2.8176)	Acc@1 22.168 (20.784)	Acc@5 77.490 (77.475)
Epoch: [6][23/25]	Time 0.680 (0.701)	Data 0.006 (0.035)	Loss 2.7817 (2.8161)	Acc@1 20.654 (20.778)	Acc@5 79.053 (77.541)
Epoch: [6][24/25]	Time 0.398 (0.689)	Data 0.004 (0.034)	Loss 2.7440 (2.8148)	Acc@1 21.226 (20.786)	Acc@5 81.014 (77.600)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/25]	Time 0.771 (0.771)	Data 0.608 (0.608)	Loss 2.7482 (2.7482)	Acc@1 24.219 (24.219)	Acc@5 79.199 (79.199)
Epoch: [7][1/25]	Time 0.672 (0.722)	Data 0.005 (0.306)	Loss 2.7141 (2.7312)	Acc@1 23.145 (23.682)	Acc@5 81.299 (80.249)
Epoch: [7][2/25]	Time 0.645 (0.696)	Data 0.006 (0.206)	Loss 2.7291 (2.7305)	Acc@1 23.047 (23.470)	Acc@5 79.736 (80.078)
Epoch: [7][3/25]	Time 0.665 (0.688)	Data 0.004 (0.155)	Loss 2.6955 (2.7217)	Acc@1 24.414 (23.706)	Acc@5 82.373 (80.652)
Epoch: [7][4/25]	Time 0.703 (0.691)	Data 0.009 (0.126)	Loss 2.7058 (2.7185)	Acc@1 23.730 (23.711)	Acc@5 82.080 (80.938)
Epoch: [7][5/25]	Time 0.725 (0.697)	Data 0.005 (0.106)	Loss 2.6863 (2.7132)	Acc@1 24.854 (23.901)	Acc@5 80.859 (80.924)
Epoch: [7][6/25]	Time 0.728 (0.701)	Data 0.004 (0.091)	Loss 2.6851 (2.7091)	Acc@1 24.170 (23.940)	Acc@5 80.859 (80.915)
Epoch: [7][7/25]	Time 0.705 (0.702)	Data 0.009 (0.081)	Loss 2.7047 (2.7086)	Acc@1 23.047 (23.828)	Acc@5 81.592 (81.000)
Epoch: [7][8/25]	Time 0.738 (0.706)	Data 0.007 (0.073)	Loss 2.6821 (2.7056)	Acc@1 23.535 (23.796)	Acc@5 81.543 (81.060)
Epoch: [7][9/25]	Time 0.771 (0.712)	Data 0.007 (0.066)	Loss 2.6493 (2.7000)	Acc@1 25.537 (23.970)	Acc@5 81.689 (81.123)
Epoch: [7][10/25]	Time 0.781 (0.719)	Data 0.005 (0.061)	Loss 2.6595 (2.6963)	Acc@1 26.025 (24.157)	Acc@5 82.471 (81.246)
Epoch: [7][11/25]	Time 0.776 (0.723)	Data 0.004 (0.056)	Loss 2.6776 (2.6948)	Acc@1 23.975 (24.141)	Acc@5 81.006 (81.226)
Epoch: [7][12/25]	Time 0.693 (0.721)	Data 0.005 (0.052)	Loss 2.6753 (2.6933)	Acc@1 23.682 (24.106)	Acc@5 79.785 (81.115)
Epoch: [7][13/25]	Time 0.631 (0.715)	Data 0.007 (0.049)	Loss 2.6658 (2.6913)	Acc@1 25.049 (24.173)	Acc@5 80.273 (81.055)
Epoch: [7][14/25]	Time 0.657 (0.711)	Data 0.005 (0.046)	Loss 2.6557 (2.6889)	Acc@1 25.000 (24.229)	Acc@5 81.738 (81.100)
Epoch: [7][15/25]	Time 0.746 (0.713)	Data 0.007 (0.044)	Loss 2.6279 (2.6851)	Acc@1 24.707 (24.258)	Acc@5 82.959 (81.216)
Epoch: [7][16/25]	Time 0.753 (0.715)	Data 0.004 (0.041)	Loss 2.6518 (2.6832)	Acc@1 24.902 (24.296)	Acc@5 81.494 (81.233)
Epoch: [7][17/25]	Time 0.704 (0.715)	Data 0.006 (0.039)	Loss 2.6055 (2.6788)	Acc@1 25.928 (24.387)	Acc@5 82.812 (81.321)
Epoch: [7][18/25]	Time 0.676 (0.713)	Data 0.005 (0.037)	Loss 2.6040 (2.6749)	Acc@1 27.637 (24.558)	Acc@5 81.543 (81.332)
Epoch: [7][19/25]	Time 0.725 (0.713)	Data 0.007 (0.036)	Loss 2.5933 (2.6708)	Acc@1 28.271 (24.744)	Acc@5 81.787 (81.355)
Epoch: [7][20/25]	Time 0.766 (0.716)	Data 0.007 (0.035)	Loss 2.5886 (2.6669)	Acc@1 29.199 (24.956)	Acc@5 82.812 (81.424)
Epoch: [7][21/25]	Time 0.727 (0.716)	Data 0.005 (0.033)	Loss 2.5997 (2.6639)	Acc@1 27.344 (25.064)	Acc@5 82.666 (81.481)
Epoch: [7][22/25]	Time 0.754 (0.718)	Data 0.006 (0.032)	Loss 2.5498 (2.6589)	Acc@1 30.029 (25.280)	Acc@5 83.447 (81.566)
Epoch: [7][23/25]	Time 0.776 (0.720)	Data 0.006 (0.031)	Loss 2.5934 (2.6562)	Acc@1 28.564 (25.417)	Acc@5 82.520 (81.606)
Epoch: [7][24/25]	Time 0.419 (0.708)	Data 0.008 (0.030)	Loss 2.5456 (2.6543)	Acc@1 30.542 (25.504)	Acc@5 85.142 (81.666)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/25]	Time 0.757 (0.757)	Data 0.670 (0.670)	Loss 2.5393 (2.5393)	Acc@1 28.662 (28.662)	Acc@5 84.766 (84.766)
Epoch: [8][1/25]	Time 0.727 (0.742)	Data 0.005 (0.337)	Loss 2.5717 (2.5555)	Acc@1 30.078 (29.370)	Acc@5 82.422 (83.594)
Epoch: [8][2/25]	Time 0.783 (0.756)	Data 0.004 (0.226)	Loss 2.5382 (2.5497)	Acc@1 30.908 (29.883)	Acc@5 83.203 (83.464)
Epoch: [8][3/25]	Time 0.745 (0.753)	Data 0.004 (0.171)	Loss 2.5028 (2.5380)	Acc@1 30.957 (30.151)	Acc@5 84.229 (83.655)
Epoch: [8][4/25]	Time 0.731 (0.749)	Data 0.006 (0.138)	Loss 2.5063 (2.5317)	Acc@1 31.201 (30.361)	Acc@5 84.326 (83.789)
Epoch: [8][5/25]	Time 0.701 (0.741)	Data 0.008 (0.116)	Loss 2.5474 (2.5343)	Acc@1 28.906 (30.119)	Acc@5 82.910 (83.643)
Epoch: [8][6/25]	Time 0.792 (0.748)	Data 0.004 (0.100)	Loss 2.5310 (2.5338)	Acc@1 31.689 (30.343)	Acc@5 83.691 (83.650)
Epoch: [8][7/25]	Time 0.714 (0.744)	Data 0.007 (0.088)	Loss 2.4549 (2.5239)	Acc@1 33.496 (30.737)	Acc@5 86.621 (84.021)
Epoch: [8][8/25]	Time 0.725 (0.742)	Data 0.007 (0.079)	Loss 2.4759 (2.5186)	Acc@1 32.080 (30.887)	Acc@5 84.131 (84.033)
Epoch: [8][9/25]	Time 0.744 (0.742)	Data 0.006 (0.072)	Loss 2.4780 (2.5145)	Acc@1 31.641 (30.962)	Acc@5 85.498 (84.180)
Epoch: [8][10/25]	Time 0.750 (0.743)	Data 0.005 (0.066)	Loss 2.4724 (2.5107)	Acc@1 31.543 (31.015)	Acc@5 84.033 (84.166)
Epoch: [8][11/25]	Time 0.782 (0.746)	Data 0.004 (0.061)	Loss 2.5206 (2.5115)	Acc@1 29.688 (30.904)	Acc@5 83.691 (84.127)
Epoch: [8][12/25]	Time 0.696 (0.742)	Data 0.009 (0.057)	Loss 2.5266 (2.5127)	Acc@1 29.541 (30.799)	Acc@5 83.789 (84.101)
Epoch: [8][13/25]	Time 0.655 (0.736)	Data 0.008 (0.053)	Loss 2.4049 (2.5050)	Acc@1 34.863 (31.090)	Acc@5 86.768 (84.291)
Epoch: [8][14/25]	Time 0.728 (0.735)	Data 0.006 (0.050)	Loss 2.4611 (2.5021)	Acc@1 31.543 (31.120)	Acc@5 85.693 (84.385)
Epoch: [8][15/25]	Time 0.745 (0.736)	Data 0.006 (0.047)	Loss 2.4707 (2.5001)	Acc@1 31.787 (31.161)	Acc@5 84.766 (84.409)
Epoch: [8][16/25]	Time 0.925 (0.747)	Data 0.009 (0.045)	Loss 2.4789 (2.4989)	Acc@1 30.762 (31.138)	Acc@5 84.229 (84.398)
Epoch: [8][17/25]	Time 0.885 (0.755)	Data 0.006 (0.043)	Loss 2.3928 (2.4930)	Acc@1 35.889 (31.402)	Acc@5 87.354 (84.562)
Epoch: [8][18/25]	Time 0.741 (0.754)	Data 0.006 (0.041)	Loss 2.4559 (2.4910)	Acc@1 30.615 (31.361)	Acc@5 84.814 (84.575)
Epoch: [8][19/25]	Time 0.788 (0.756)	Data 0.004 (0.039)	Loss 2.4310 (2.4880)	Acc@1 34.717 (31.528)	Acc@5 85.547 (84.624)
Epoch: [8][20/25]	Time 0.762 (0.756)	Data 0.005 (0.037)	Loss 2.4203 (2.4848)	Acc@1 34.619 (31.676)	Acc@5 85.449 (84.663)
Epoch: [8][21/25]	Time 0.698 (0.753)	Data 0.006 (0.036)	Loss 2.3744 (2.4798)	Acc@1 35.107 (31.831)	Acc@5 86.475 (84.746)
Epoch: [8][22/25]	Time 0.606 (0.747)	Data 0.009 (0.035)	Loss 2.3830 (2.4756)	Acc@1 34.131 (31.931)	Acc@5 87.061 (84.846)
Epoch: [8][23/25]	Time 0.697 (0.745)	Data 0.004 (0.034)	Loss 2.3724 (2.4713)	Acc@1 36.035 (32.102)	Acc@5 87.939 (84.975)
Epoch: [8][24/25]	Time 0.370 (0.730)	Data 0.007 (0.033)	Loss 2.3719 (2.4696)	Acc@1 34.316 (32.140)	Acc@5 87.028 (85.010)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/25]	Time 0.790 (0.790)	Data 0.672 (0.672)	Loss 2.4000 (2.4000)	Acc@1 32.666 (32.666)	Acc@5 86.182 (86.182)
Epoch: [9][1/25]	Time 0.687 (0.738)	Data 0.007 (0.340)	Loss 2.3372 (2.3686)	Acc@1 36.719 (34.692)	Acc@5 88.525 (87.354)
Epoch: [9][2/25]	Time 0.713 (0.730)	Data 0.007 (0.229)	Loss 2.3718 (2.3697)	Acc@1 35.010 (34.798)	Acc@5 87.012 (87.240)
Epoch: [9][3/25]	Time 0.740 (0.732)	Data 0.005 (0.173)	Loss 2.3432 (2.3631)	Acc@1 35.449 (34.961)	Acc@5 87.207 (87.231)
Epoch: [9][4/25]	Time 0.795 (0.745)	Data 0.009 (0.140)	Loss 2.3233 (2.3551)	Acc@1 36.133 (35.195)	Acc@5 87.354 (87.256)
Epoch: [9][5/25]	Time 0.708 (0.739)	Data 0.004 (0.118)	Loss 2.3305 (2.3510)	Acc@1 35.498 (35.246)	Acc@5 87.988 (87.378)
Epoch: [9][6/25]	Time 0.784 (0.745)	Data 0.007 (0.102)	Loss 2.3431 (2.3499)	Acc@1 36.963 (35.491)	Acc@5 86.816 (87.298)
Epoch: [9][7/25]	Time 0.898 (0.764)	Data 0.004 (0.090)	Loss 2.2854 (2.3418)	Acc@1 38.232 (35.834)	Acc@5 88.379 (87.433)
Epoch: [9][8/25]	Time 0.921 (0.782)	Data 0.004 (0.080)	Loss 2.3060 (2.3378)	Acc@1 38.477 (36.127)	Acc@5 88.232 (87.522)
Epoch: [9][9/25]	Time 0.647 (0.768)	Data 0.008 (0.073)	Loss 2.2779 (2.3318)	Acc@1 37.500 (36.265)	Acc@5 88.672 (87.637)
Epoch: [9][10/25]	Time 0.702 (0.762)	Data 0.007 (0.067)	Loss 2.2762 (2.3268)	Acc@1 37.988 (36.421)	Acc@5 89.014 (87.762)
Epoch: [9][11/25]	Time 0.760 (0.762)	Data 0.006 (0.062)	Loss 2.3107 (2.3254)	Acc@1 35.547 (36.348)	Acc@5 88.818 (87.850)
Epoch: [9][12/25]	Time 0.751 (0.761)	Data 0.006 (0.057)	Loss 2.2538 (2.3199)	Acc@1 39.990 (36.629)	Acc@5 88.623 (87.909)
Epoch: [9][13/25]	Time 0.703 (0.757)	Data 0.007 (0.054)	Loss 2.2364 (2.3140)	Acc@1 40.186 (36.883)	Acc@5 88.281 (87.936)
Epoch: [9][14/25]	Time 0.670 (0.751)	Data 0.010 (0.051)	Loss 2.2553 (2.3101)	Acc@1 38.428 (36.986)	Acc@5 89.209 (88.021)
Epoch: [9][15/25]	Time 0.648 (0.745)	Data 0.005 (0.048)	Loss 2.2740 (2.3078)	Acc@1 37.988 (37.048)	Acc@5 88.525 (88.052)
Epoch: [9][16/25]	Time 0.705 (0.742)	Data 0.006 (0.046)	Loss 2.2378 (2.3037)	Acc@1 41.211 (37.293)	Acc@5 88.721 (88.092)
Epoch: [9][17/25]	Time 0.741 (0.742)	Data 0.004 (0.043)	Loss 2.2269 (2.2994)	Acc@1 40.430 (37.467)	Acc@5 88.428 (88.110)
Epoch: [9][18/25]	Time 0.797 (0.745)	Data 0.007 (0.041)	Loss 2.2127 (2.2949)	Acc@1 40.527 (37.628)	Acc@5 89.648 (88.191)
Epoch: [9][19/25]	Time 0.681 (0.742)	Data 0.008 (0.040)	Loss 2.2335 (2.2918)	Acc@1 39.844 (37.739)	Acc@5 88.672 (88.215)
Epoch: [9][20/25]	Time 0.850 (0.747)	Data 0.004 (0.038)	Loss 2.2264 (2.2887)	Acc@1 38.965 (37.798)	Acc@5 89.600 (88.281)
Epoch: [9][21/25]	Time 0.812 (0.750)	Data 0.004 (0.036)	Loss 2.2419 (2.2865)	Acc@1 39.551 (37.877)	Acc@5 89.307 (88.328)
Epoch: [9][22/25]	Time 0.786 (0.752)	Data 0.005 (0.035)	Loss 2.2077 (2.2831)	Acc@1 40.430 (37.988)	Acc@5 90.088 (88.404)
Epoch: [9][23/25]	Time 0.719 (0.750)	Data 0.006 (0.034)	Loss 2.2051 (2.2799)	Acc@1 40.479 (38.092)	Acc@5 90.039 (88.472)
Epoch: [9][24/25]	Time 0.389 (0.736)	Data 0.005 (0.033)	Loss 2.2982 (2.2802)	Acc@1 37.854 (38.088)	Acc@5 88.443 (88.472)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/25]	Time 0.764 (0.764)	Data 0.608 (0.608)	Loss 2.2429 (2.2429)	Acc@1 39.648 (39.648)	Acc@5 88.770 (88.770)
Epoch: [10][1/25]	Time 0.786 (0.775)	Data 0.006 (0.307)	Loss 2.1953 (2.2191)	Acc@1 39.844 (39.746)	Acc@5 89.551 (89.160)
Epoch: [10][2/25]	Time 0.723 (0.758)	Data 0.005 (0.207)	Loss 2.2029 (2.2137)	Acc@1 40.820 (40.104)	Acc@5 89.209 (89.176)
Epoch: [10][3/25]	Time 0.783 (0.764)	Data 0.004 (0.156)	Loss 2.1971 (2.2096)	Acc@1 40.625 (40.234)	Acc@5 88.672 (89.050)
Epoch: [10][4/25]	Time 0.753 (0.762)	Data 0.008 (0.126)	Loss 2.1741 (2.2025)	Acc@1 42.480 (40.684)	Acc@5 90.088 (89.258)
Epoch: [10][5/25]	Time 0.746 (0.759)	Data 0.003 (0.106)	Loss 2.1659 (2.1964)	Acc@1 42.676 (41.016)	Acc@5 89.990 (89.380)
Epoch: [10][6/25]	Time 0.753 (0.758)	Data 0.005 (0.091)	Loss 2.1391 (2.1882)	Acc@1 41.016 (41.016)	Acc@5 91.699 (89.711)
Epoch: [10][7/25]	Time 0.748 (0.757)	Data 0.004 (0.081)	Loss 2.2056 (2.1904)	Acc@1 38.916 (40.753)	Acc@5 89.355 (89.667)
Epoch: [10][8/25]	Time 0.672 (0.748)	Data 0.004 (0.072)	Loss 2.1521 (2.1861)	Acc@1 39.893 (40.658)	Acc@5 89.990 (89.703)
Epoch: [10][9/25]	Time 0.685 (0.741)	Data 0.008 (0.066)	Loss 2.1261 (2.1801)	Acc@1 42.090 (40.801)	Acc@5 91.797 (89.912)
Epoch: [10][10/25]	Time 0.717 (0.739)	Data 0.008 (0.060)	Loss 2.1313 (2.1757)	Acc@1 42.139 (40.922)	Acc@5 90.283 (89.946)
Epoch: [10][11/25]	Time 0.801 (0.744)	Data 0.005 (0.056)	Loss 2.1378 (2.1725)	Acc@1 41.016 (40.930)	Acc@5 91.113 (90.043)
Epoch: [10][12/25]	Time 0.724 (0.743)	Data 0.004 (0.052)	Loss 2.1337 (2.1695)	Acc@1 41.455 (40.971)	Acc@5 89.697 (90.017)
Epoch: [10][13/25]	Time 0.752 (0.743)	Data 0.005 (0.048)	Loss 2.1642 (2.1691)	Acc@1 40.088 (40.908)	Acc@5 89.502 (89.980)
Epoch: [10][14/25]	Time 0.738 (0.743)	Data 0.005 (0.046)	Loss 2.1452 (2.1675)	Acc@1 42.334 (41.003)	Acc@5 89.746 (89.964)
Epoch: [10][15/25]	Time 0.715 (0.741)	Data 0.007 (0.043)	Loss 2.1515 (2.1665)	Acc@1 42.480 (41.095)	Acc@5 89.795 (89.954)
Epoch: [10][16/25]	Time 0.774 (0.743)	Data 0.006 (0.041)	Loss 2.0945 (2.1623)	Acc@1 44.580 (41.300)	Acc@5 91.797 (90.062)
Epoch: [10][17/25]	Time 0.771 (0.745)	Data 0.004 (0.039)	Loss 2.0608 (2.1567)	Acc@1 44.189 (41.461)	Acc@5 91.504 (90.142)
Epoch: [10][18/25]	Time 0.736 (0.744)	Data 0.004 (0.037)	Loss 2.1309 (2.1553)	Acc@1 42.334 (41.506)	Acc@5 90.527 (90.162)
Epoch: [10][19/25]	Time 0.719 (0.743)	Data 0.007 (0.036)	Loss 2.1133 (2.1532)	Acc@1 42.383 (41.550)	Acc@5 90.967 (90.203)
Epoch: [10][20/25]	Time 0.763 (0.744)	Data 0.006 (0.034)	Loss 2.1066 (2.1510)	Acc@1 41.846 (41.564)	Acc@5 90.283 (90.206)
Epoch: [10][21/25]	Time 0.745 (0.744)	Data 0.003 (0.033)	Loss 2.0617 (2.1469)	Acc@1 45.898 (41.761)	Acc@5 91.943 (90.285)
Epoch: [10][22/25]	Time 0.690 (0.742)	Data 0.005 (0.032)	Loss 2.0798 (2.1440)	Acc@1 44.971 (41.901)	Acc@5 90.771 (90.307)
Epoch: [10][23/25]	Time 0.799 (0.744)	Data 0.005 (0.030)	Loss 2.0622 (2.1406)	Acc@1 45.312 (42.043)	Acc@5 91.553 (90.358)
Epoch: [10][24/25]	Time 0.425 (0.731)	Data 0.006 (0.029)	Loss 2.0823 (2.1396)	Acc@1 42.925 (42.058)	Acc@5 91.509 (90.378)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/25]	Time 0.752 (0.752)	Data 0.700 (0.700)	Loss 2.0665 (2.0665)	Acc@1 43.213 (43.213)	Acc@5 91.357 (91.357)
Epoch: [11][1/25]	Time 0.702 (0.727)	Data 0.004 (0.352)	Loss 2.0283 (2.0474)	Acc@1 46.094 (44.653)	Acc@5 91.846 (91.602)
Epoch: [11][2/25]	Time 0.763 (0.739)	Data 0.004 (0.236)	Loss 2.0426 (2.0458)	Acc@1 44.727 (44.678)	Acc@5 92.139 (91.781)
Epoch: [11][3/25]	Time 0.750 (0.742)	Data 0.004 (0.178)	Loss 2.0763 (2.0535)	Acc@1 42.334 (44.092)	Acc@5 91.748 (91.772)
Epoch: [11][4/25]	Time 0.711 (0.736)	Data 0.008 (0.144)	Loss 2.0724 (2.0572)	Acc@1 43.359 (43.945)	Acc@5 90.820 (91.582)
Epoch: [11][5/25]	Time 0.746 (0.737)	Data 0.005 (0.121)	Loss 2.0373 (2.0539)	Acc@1 43.945 (43.945)	Acc@5 91.699 (91.602)
Epoch: [11][6/25]	Time 0.759 (0.740)	Data 0.005 (0.104)	Loss 2.0391 (2.0518)	Acc@1 43.701 (43.910)	Acc@5 91.650 (91.609)
Epoch: [11][7/25]	Time 0.714 (0.737)	Data 0.005 (0.092)	Loss 2.0378 (2.0501)	Acc@1 43.945 (43.915)	Acc@5 91.992 (91.656)
Epoch: [11][8/25]	Time 0.714 (0.734)	Data 0.005 (0.082)	Loss 2.0909 (2.0546)	Acc@1 41.943 (43.696)	Acc@5 89.648 (91.433)
Epoch: [11][9/25]	Time 0.794 (0.740)	Data 0.006 (0.075)	Loss 2.0305 (2.0522)	Acc@1 45.410 (43.867)	Acc@5 91.895 (91.479)
Epoch: [11][10/25]	Time 0.647 (0.732)	Data 0.007 (0.068)	Loss 2.0229 (2.0495)	Acc@1 45.166 (43.985)	Acc@5 91.064 (91.442)
Epoch: [11][11/25]	Time 0.658 (0.726)	Data 0.005 (0.063)	Loss 2.0436 (2.0490)	Acc@1 45.898 (44.145)	Acc@5 91.064 (91.410)
Epoch: [11][12/25]	Time 0.660 (0.721)	Data 0.004 (0.059)	Loss 2.0629 (2.0501)	Acc@1 42.725 (44.035)	Acc@5 92.432 (91.489)
Epoch: [11][13/25]	Time 0.698 (0.719)	Data 0.005 (0.055)	Loss 2.0036 (2.0468)	Acc@1 45.801 (44.162)	Acc@5 92.383 (91.553)
Epoch: [11][14/25]	Time 0.701 (0.718)	Data 0.005 (0.051)	Loss 2.0794 (2.0490)	Acc@1 43.750 (44.134)	Acc@5 91.162 (91.527)
Epoch: [11][15/25]	Time 0.687 (0.716)	Data 0.005 (0.048)	Loss 2.0886 (2.0514)	Acc@1 43.262 (44.080)	Acc@5 90.430 (91.458)
Epoch: [11][16/25]	Time 0.747 (0.718)	Data 0.010 (0.046)	Loss 2.0451 (2.0511)	Acc@1 44.287 (44.092)	Acc@5 90.381 (91.395)
Epoch: [11][17/25]	Time 0.772 (0.721)	Data 0.004 (0.044)	Loss 2.0195 (2.0493)	Acc@1 45.361 (44.162)	Acc@5 92.578 (91.461)
Epoch: [11][18/25]	Time 0.707 (0.720)	Data 0.007 (0.042)	Loss 1.9967 (2.0465)	Acc@1 46.289 (44.274)	Acc@5 92.188 (91.499)
Epoch: [11][19/25]	Time 0.694 (0.719)	Data 0.006 (0.040)	Loss 2.0243 (2.0454)	Acc@1 45.557 (44.338)	Acc@5 90.625 (91.455)
Epoch: [11][20/25]	Time 0.754 (0.720)	Data 0.004 (0.038)	Loss 1.9871 (2.0426)	Acc@1 46.924 (44.461)	Acc@5 92.188 (91.490)
Epoch: [11][21/25]	Time 0.729 (0.721)	Data 0.007 (0.037)	Loss 2.0111 (2.0412)	Acc@1 46.338 (44.547)	Acc@5 91.797 (91.504)
Epoch: [11][22/25]	Time 0.792 (0.724)	Data 0.004 (0.036)	Loss 1.9548 (2.0375)	Acc@1 47.021 (44.654)	Acc@5 93.018 (91.570)
Epoch: [11][23/25]	Time 0.674 (0.722)	Data 0.006 (0.034)	Loss 1.9679 (2.0346)	Acc@1 46.289 (44.722)	Acc@5 93.018 (91.630)
Epoch: [11][24/25]	Time 0.372 (0.708)	Data 0.007 (0.033)	Loss 1.9114 (2.0325)	Acc@1 46.816 (44.758)	Acc@5 92.925 (91.652)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/25]	Time 0.752 (0.752)	Data 0.674 (0.674)	Loss 1.9595 (1.9595)	Acc@1 46.191 (46.191)	Acc@5 92.285 (92.285)
Epoch: [12][1/25]	Time 0.732 (0.742)	Data 0.005 (0.340)	Loss 1.9224 (1.9410)	Acc@1 46.533 (46.362)	Acc@5 93.604 (92.944)
Epoch: [12][2/25]	Time 0.699 (0.728)	Data 0.005 (0.228)	Loss 1.9581 (1.9467)	Acc@1 47.119 (46.615)	Acc@5 90.967 (92.285)
Epoch: [12][3/25]	Time 0.687 (0.717)	Data 0.007 (0.173)	Loss 1.9669 (1.9517)	Acc@1 45.264 (46.277)	Acc@5 93.066 (92.480)
Epoch: [12][4/25]	Time 0.810 (0.736)	Data 0.006 (0.139)	Loss 2.0114 (1.9636)	Acc@1 43.115 (45.645)	Acc@5 92.334 (92.451)
Epoch: [12][5/25]	Time 0.679 (0.726)	Data 0.004 (0.117)	Loss 1.9573 (1.9626)	Acc@1 46.240 (45.744)	Acc@5 93.164 (92.570)
Epoch: [12][6/25]	Time 0.609 (0.710)	Data 0.005 (0.101)	Loss 1.9411 (1.9595)	Acc@1 46.680 (45.878)	Acc@5 92.236 (92.522)
Epoch: [12][7/25]	Time 0.658 (0.703)	Data 0.004 (0.089)	Loss 2.0025 (1.9649)	Acc@1 45.361 (45.813)	Acc@5 91.992 (92.456)
Epoch: [12][8/25]	Time 0.714 (0.704)	Data 0.008 (0.080)	Loss 1.9581 (1.9641)	Acc@1 47.314 (45.980)	Acc@5 91.211 (92.318)
Epoch: [12][9/25]	Time 0.714 (0.705)	Data 0.007 (0.072)	Loss 1.9209 (1.9598)	Acc@1 48.828 (46.265)	Acc@5 92.920 (92.378)
Epoch: [12][10/25]	Time 0.808 (0.715)	Data 0.007 (0.066)	Loss 1.8840 (1.9529)	Acc@1 49.365 (46.547)	Acc@5 92.871 (92.423)
Epoch: [12][11/25]	Time 0.703 (0.714)	Data 0.007 (0.061)	Loss 1.9576 (1.9533)	Acc@1 45.020 (46.419)	Acc@5 93.164 (92.485)
Epoch: [12][12/25]	Time 0.731 (0.715)	Data 0.005 (0.057)	Loss 1.9081 (1.9498)	Acc@1 47.852 (46.529)	Acc@5 93.262 (92.544)
Epoch: [12][13/25]	Time 0.659 (0.711)	Data 0.005 (0.053)	Loss 1.9174 (1.9475)	Acc@1 48.730 (46.687)	Acc@5 92.822 (92.564)
Epoch: [12][14/25]	Time 0.746 (0.713)	Data 0.006 (0.050)	Loss 1.8924 (1.9438)	Acc@1 49.072 (46.846)	Acc@5 92.871 (92.585)
Epoch: [12][15/25]	Time 0.791 (0.718)	Data 0.005 (0.047)	Loss 1.8910 (1.9405)	Acc@1 49.805 (47.031)	Acc@5 92.334 (92.569)
Epoch: [12][16/25]	Time 0.684 (0.716)	Data 0.009 (0.045)	Loss 1.8599 (1.9358)	Acc@1 50.537 (47.237)	Acc@5 92.920 (92.590)
Epoch: [12][17/25]	Time 0.672 (0.714)	Data 0.005 (0.043)	Loss 1.8796 (1.9327)	Acc@1 49.414 (47.358)	Acc@5 93.164 (92.622)
Epoch: [12][18/25]	Time 0.629 (0.709)	Data 0.006 (0.041)	Loss 1.8457 (1.9281)	Acc@1 51.172 (47.559)	Acc@5 93.164 (92.650)
Epoch: [12][19/25]	Time 0.702 (0.709)	Data 0.005 (0.039)	Loss 1.8235 (1.9229)	Acc@1 50.781 (47.720)	Acc@5 93.848 (92.710)
Epoch: [12][20/25]	Time 0.701 (0.709)	Data 0.005 (0.038)	Loss 1.8511 (1.9194)	Acc@1 51.318 (47.891)	Acc@5 92.920 (92.720)
Epoch: [12][21/25]	Time 0.785 (0.712)	Data 0.004 (0.036)	Loss 1.8591 (1.9167)	Acc@1 50.244 (47.998)	Acc@5 92.188 (92.696)
Epoch: [12][22/25]	Time 0.737 (0.713)	Data 0.004 (0.035)	Loss 1.7843 (1.9109)	Acc@1 53.320 (48.229)	Acc@5 93.994 (92.752)
Epoch: [12][23/25]	Time 0.703 (0.713)	Data 0.006 (0.033)	Loss 1.7763 (1.9053)	Acc@1 51.514 (48.366)	Acc@5 93.750 (92.794)
Epoch: [12][24/25]	Time 0.434 (0.702)	Data 0.007 (0.032)	Loss 1.7533 (1.9028)	Acc@1 53.538 (48.454)	Acc@5 94.811 (92.828)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/25]	Time 0.751 (0.751)	Data 0.702 (0.702)	Loss 1.8339 (1.8339)	Acc@1 50.098 (50.098)	Acc@5 94.141 (94.141)
Epoch: [13][1/25]	Time 0.758 (0.755)	Data 0.003 (0.353)	Loss 1.7849 (1.8094)	Acc@1 52.246 (51.172)	Acc@5 94.873 (94.507)
Epoch: [13][2/25]	Time 0.740 (0.750)	Data 0.007 (0.237)	Loss 1.7731 (1.7973)	Acc@1 51.660 (51.335)	Acc@5 94.873 (94.629)
Epoch: [13][3/25]	Time 0.761 (0.753)	Data 0.006 (0.180)	Loss 1.8157 (1.8019)	Acc@1 51.367 (51.343)	Acc@5 94.336 (94.556)
Epoch: [13][4/25]	Time 0.741 (0.750)	Data 0.009 (0.146)	Loss 1.7762 (1.7968)	Acc@1 52.100 (51.494)	Acc@5 93.213 (94.287)
Epoch: [13][5/25]	Time 0.784 (0.756)	Data 0.006 (0.122)	Loss 1.7333 (1.7862)	Acc@1 54.785 (52.043)	Acc@5 94.482 (94.320)
Epoch: [13][6/25]	Time 0.720 (0.751)	Data 0.004 (0.106)	Loss 1.7258 (1.7776)	Acc@1 54.395 (52.379)	Acc@5 94.824 (94.392)
Epoch: [13][7/25]	Time 0.769 (0.753)	Data 0.004 (0.093)	Loss 1.7980 (1.7801)	Acc@1 51.367 (52.252)	Acc@5 93.848 (94.324)
Epoch: [13][8/25]	Time 0.686 (0.746)	Data 0.004 (0.083)	Loss 1.6831 (1.7693)	Acc@1 55.518 (52.615)	Acc@5 95.410 (94.444)
Epoch: [13][9/25]	Time 0.769 (0.748)	Data 0.008 (0.076)	Loss 1.7445 (1.7668)	Acc@1 52.539 (52.607)	Acc@5 94.727 (94.473)
Epoch: [13][10/25]	Time 0.778 (0.751)	Data 0.005 (0.069)	Loss 1.7838 (1.7684)	Acc@1 51.221 (52.481)	Acc@5 94.141 (94.442)
Epoch: [13][11/25]	Time 0.738 (0.750)	Data 0.005 (0.064)	Loss 1.7241 (1.7647)	Acc@1 53.467 (52.563)	Acc@5 95.312 (94.515)
Epoch: [13][12/25]	Time 0.727 (0.748)	Data 0.009 (0.060)	Loss 1.7356 (1.7625)	Acc@1 52.979 (52.595)	Acc@5 94.482 (94.512)
Epoch: [13][13/25]	Time 0.737 (0.747)	Data 0.006 (0.056)	Loss 1.7525 (1.7618)	Acc@1 52.881 (52.616)	Acc@5 94.189 (94.489)
Epoch: [13][14/25]	Time 0.691 (0.743)	Data 0.006 (0.052)	Loss 1.7239 (1.7592)	Acc@1 54.590 (52.747)	Acc@5 94.775 (94.508)
Epoch: [13][15/25]	Time 0.770 (0.745)	Data 0.004 (0.049)	Loss 1.6934 (1.7551)	Acc@1 55.127 (52.896)	Acc@5 95.117 (94.547)
Epoch: [13][16/25]	Time 0.727 (0.744)	Data 0.007 (0.047)	Loss 1.6761 (1.7505)	Acc@1 56.689 (53.119)	Acc@5 93.994 (94.514)
Epoch: [13][17/25]	Time 0.776 (0.746)	Data 0.006 (0.045)	Loss 1.7389 (1.7498)	Acc@1 54.199 (53.179)	Acc@5 93.799 (94.474)
Epoch: [13][18/25]	Time 0.762 (0.747)	Data 0.004 (0.042)	Loss 1.6827 (1.7463)	Acc@1 55.713 (53.313)	Acc@5 95.703 (94.539)
Epoch: [13][19/25]	Time 0.747 (0.747)	Data 0.004 (0.041)	Loss 1.6708 (1.7425)	Acc@1 56.982 (53.496)	Acc@5 95.117 (94.568)
Epoch: [13][20/25]	Time 0.700 (0.744)	Data 0.007 (0.039)	Loss 1.6690 (1.7390)	Acc@1 57.080 (53.667)	Acc@5 94.287 (94.555)
Epoch: [13][21/25]	Time 0.805 (0.747)	Data 0.004 (0.037)	Loss 1.6552 (1.7352)	Acc@1 57.227 (53.829)	Acc@5 95.117 (94.580)
Epoch: [13][22/25]	Time 0.686 (0.744)	Data 0.007 (0.036)	Loss 1.6727 (1.7325)	Acc@1 55.518 (53.902)	Acc@5 95.020 (94.599)
Epoch: [13][23/25]	Time 0.640 (0.740)	Data 0.004 (0.035)	Loss 1.6591 (1.7294)	Acc@1 56.445 (54.008)	Acc@5 94.775 (94.607)
Epoch: [13][24/25]	Time 0.370 (0.725)	Data 0.007 (0.034)	Loss 1.6201 (1.7276)	Acc@1 57.783 (54.072)	Acc@5 94.340 (94.602)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/25]	Time 0.745 (0.745)	Data 0.757 (0.757)	Loss 1.6318 (1.6318)	Acc@1 58.301 (58.301)	Acc@5 94.531 (94.531)
Epoch: [14][1/25]	Time 0.688 (0.716)	Data 0.005 (0.381)	Loss 1.6845 (1.6581)	Acc@1 55.615 (56.958)	Acc@5 94.775 (94.653)
Epoch: [14][2/25]	Time 0.647 (0.693)	Data 0.006 (0.256)	Loss 1.6444 (1.6536)	Acc@1 56.006 (56.641)	Acc@5 95.312 (94.873)
Epoch: [14][3/25]	Time 0.641 (0.680)	Data 0.007 (0.194)	Loss 1.6635 (1.6561)	Acc@1 55.957 (56.470)	Acc@5 94.482 (94.775)
Epoch: [14][4/25]	Time 0.678 (0.680)	Data 0.009 (0.157)	Loss 1.7000 (1.6648)	Acc@1 56.104 (56.396)	Acc@5 94.092 (94.639)
Epoch: [14][5/25]	Time 0.636 (0.672)	Data 0.005 (0.132)	Loss 1.6379 (1.6604)	Acc@1 57.227 (56.535)	Acc@5 94.727 (94.653)
Epoch: [14][6/25]	Time 0.633 (0.667)	Data 0.003 (0.113)	Loss 1.6507 (1.6590)	Acc@1 56.006 (56.459)	Acc@5 95.215 (94.734)
Epoch: [14][7/25]	Time 0.688 (0.669)	Data 0.003 (0.099)	Loss 1.6030 (1.6520)	Acc@1 58.740 (56.744)	Acc@5 95.459 (94.824)
Epoch: [14][8/25]	Time 0.629 (0.665)	Data 0.005 (0.089)	Loss 1.6056 (1.6468)	Acc@1 58.447 (56.934)	Acc@5 95.215 (94.868)
Epoch: [14][9/25]	Time 0.644 (0.663)	Data 0.004 (0.080)	Loss 1.6123 (1.6434)	Acc@1 58.496 (57.090)	Acc@5 95.068 (94.888)
Epoch: [14][10/25]	Time 0.647 (0.661)	Data 0.005 (0.074)	Loss 1.6143 (1.6407)	Acc@1 57.861 (57.160)	Acc@5 95.361 (94.931)
Epoch: [14][11/25]	Time 0.664 (0.662)	Data 0.005 (0.068)	Loss 1.5766 (1.6354)	Acc@1 59.277 (57.336)	Acc@5 95.068 (94.942)
Epoch: [14][12/25]	Time 0.665 (0.662)	Data 0.005 (0.063)	Loss 1.5507 (1.6289)	Acc@1 60.254 (57.561)	Acc@5 95.654 (94.997)
Epoch: [14][13/25]	Time 0.653 (0.661)	Data 0.008 (0.059)	Loss 1.5288 (1.6217)	Acc@1 60.205 (57.750)	Acc@5 96.338 (95.093)
Epoch: [14][14/25]	Time 0.661 (0.661)	Data 0.008 (0.056)	Loss 1.6080 (1.6208)	Acc@1 57.373 (57.725)	Acc@5 95.801 (95.140)
Epoch: [14][15/25]	Time 0.656 (0.661)	Data 0.008 (0.053)	Loss 1.5696 (1.6176)	Acc@1 58.740 (57.788)	Acc@5 95.850 (95.184)
Epoch: [14][16/25]	Time 0.647 (0.660)	Data 0.008 (0.050)	Loss 1.5698 (1.6148)	Acc@1 58.984 (57.858)	Acc@5 95.117 (95.180)
Epoch: [14][17/25]	Time 0.662 (0.660)	Data 0.007 (0.048)	Loss 1.5858 (1.6132)	Acc@1 58.057 (57.869)	Acc@5 95.850 (95.218)
Epoch: [14][18/25]	Time 0.683 (0.661)	Data 0.006 (0.045)	Loss 1.5774 (1.6113)	Acc@1 58.057 (57.879)	Acc@5 95.850 (95.251)
Epoch: [14][19/25]	Time 0.663 (0.661)	Data 0.006 (0.043)	Loss 1.5487 (1.6082)	Acc@1 59.424 (57.957)	Acc@5 95.850 (95.281)
Epoch: [14][20/25]	Time 0.636 (0.660)	Data 0.003 (0.042)	Loss 1.5447 (1.6051)	Acc@1 59.717 (58.040)	Acc@5 95.801 (95.306)
Epoch: [14][21/25]	Time 0.680 (0.661)	Data 0.006 (0.040)	Loss 1.5302 (1.6017)	Acc@1 59.766 (58.119)	Acc@5 96.387 (95.355)
Epoch: [14][22/25]	Time 0.654 (0.661)	Data 0.007 (0.039)	Loss 1.5740 (1.6005)	Acc@1 59.131 (58.163)	Acc@5 95.752 (95.372)
Epoch: [14][23/25]	Time 0.668 (0.661)	Data 0.007 (0.037)	Loss 1.5118 (1.5968)	Acc@1 61.133 (58.287)	Acc@5 96.338 (95.412)
Epoch: [14][24/25]	Time 0.401 (0.651)	Data 0.008 (0.036)	Loss 1.5124 (1.5954)	Acc@1 59.434 (58.306)	Acc@5 96.698 (95.434)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/25]	Time 0.741 (0.741)	Data 0.806 (0.806)	Loss 1.5395 (1.5395)	Acc@1 60.059 (60.059)	Acc@5 95.801 (95.801)
Epoch: [15][1/25]	Time 0.685 (0.713)	Data 0.003 (0.404)	Loss 1.5353 (1.5374)	Acc@1 60.107 (60.083)	Acc@5 96.143 (95.972)
Epoch: [15][2/25]	Time 0.698 (0.708)	Data 0.003 (0.270)	Loss 1.5056 (1.5268)	Acc@1 61.621 (60.596)	Acc@5 96.533 (96.159)
Epoch: [15][3/25]	Time 0.783 (0.727)	Data 0.007 (0.205)	Loss 1.5046 (1.5212)	Acc@1 60.938 (60.681)	Acc@5 95.996 (96.118)
Epoch: [15][4/25]	Time 0.765 (0.734)	Data 0.009 (0.166)	Loss 1.4870 (1.5144)	Acc@1 62.646 (61.074)	Acc@5 95.752 (96.045)
Epoch: [15][5/25]	Time 0.742 (0.736)	Data 0.006 (0.139)	Loss 1.4838 (1.5093)	Acc@1 62.402 (61.296)	Acc@5 96.924 (96.191)
Epoch: [15][6/25]	Time 0.749 (0.737)	Data 0.007 (0.120)	Loss 1.5185 (1.5106)	Acc@1 59.033 (60.972)	Acc@5 95.801 (96.136)
Epoch: [15][7/25]	Time 0.716 (0.735)	Data 0.004 (0.106)	Loss 1.4644 (1.5048)	Acc@1 62.744 (61.194)	Acc@5 96.289 (96.155)
Epoch: [15][8/25]	Time 0.701 (0.731)	Data 0.006 (0.094)	Loss 1.4815 (1.5022)	Acc@1 62.500 (61.339)	Acc@5 96.289 (96.170)
Epoch: [15][9/25]	Time 0.794 (0.737)	Data 0.004 (0.085)	Loss 1.5146 (1.5035)	Acc@1 61.035 (61.309)	Acc@5 96.191 (96.172)
Epoch: [15][10/25]	Time 0.718 (0.736)	Data 0.004 (0.078)	Loss 1.4778 (1.5011)	Acc@1 62.207 (61.390)	Acc@5 96.729 (96.222)
Epoch: [15][11/25]	Time 0.688 (0.732)	Data 0.008 (0.072)	Loss 1.4861 (1.4999)	Acc@1 61.816 (61.426)	Acc@5 95.947 (96.200)
Epoch: [15][12/25]	Time 0.762 (0.734)	Data 0.006 (0.067)	Loss 1.4961 (1.4996)	Acc@1 59.912 (61.309)	Acc@5 96.289 (96.206)
Epoch: [15][13/25]	Time 0.709 (0.732)	Data 0.004 (0.063)	Loss 1.4873 (1.4987)	Acc@1 61.182 (61.300)	Acc@5 96.143 (96.202)
Epoch: [15][14/25]	Time 0.773 (0.735)	Data 0.004 (0.059)	Loss 1.4690 (1.4967)	Acc@1 63.379 (61.439)	Acc@5 96.387 (96.214)
Epoch: [15][15/25]	Time 0.763 (0.737)	Data 0.006 (0.055)	Loss 1.4623 (1.4946)	Acc@1 63.721 (61.581)	Acc@5 95.654 (96.179)
Epoch: [15][16/25]	Time 0.689 (0.734)	Data 0.008 (0.053)	Loss 1.4395 (1.4913)	Acc@1 62.451 (61.633)	Acc@5 96.387 (96.191)
Epoch: [15][17/25]	Time 0.761 (0.735)	Data 0.004 (0.050)	Loss 1.4330 (1.4881)	Acc@1 65.039 (61.822)	Acc@5 96.240 (96.194)
Epoch: [15][18/25]	Time 0.743 (0.736)	Data 0.006 (0.048)	Loss 1.4256 (1.4848)	Acc@1 63.330 (61.901)	Acc@5 97.021 (96.238)
Epoch: [15][19/25]	Time 0.670 (0.733)	Data 0.005 (0.045)	Loss 1.4244 (1.4818)	Acc@1 64.502 (62.031)	Acc@5 96.924 (96.272)
Epoch: [15][20/25]	Time 0.737 (0.733)	Data 0.004 (0.044)	Loss 1.3995 (1.4779)	Acc@1 64.209 (62.135)	Acc@5 96.436 (96.280)
Epoch: [15][21/25]	Time 0.758 (0.734)	Data 0.006 (0.042)	Loss 1.4532 (1.4767)	Acc@1 63.379 (62.191)	Acc@5 96.191 (96.276)
Epoch: [15][22/25]	Time 0.750 (0.735)	Data 0.005 (0.040)	Loss 1.4276 (1.4746)	Acc@1 64.502 (62.292)	Acc@5 96.338 (96.278)
Epoch: [15][23/25]	Time 0.731 (0.734)	Data 0.004 (0.039)	Loss 1.4194 (1.4723)	Acc@1 65.234 (62.415)	Acc@5 96.582 (96.291)
Epoch: [15][24/25]	Time 0.405 (0.721)	Data 0.005 (0.037)	Loss 1.4457 (1.4719)	Acc@1 63.797 (62.438)	Acc@5 95.755 (96.282)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/25]	Time 0.716 (0.716)	Data 0.640 (0.640)	Loss 1.4033 (1.4033)	Acc@1 64.648 (64.648)	Acc@5 97.119 (97.119)
Epoch: [16][1/25]	Time 0.730 (0.723)	Data 0.006 (0.323)	Loss 1.4331 (1.4182)	Acc@1 63.184 (63.916)	Acc@5 96.777 (96.948)
Epoch: [16][2/25]	Time 0.717 (0.721)	Data 0.005 (0.217)	Loss 1.3581 (1.3982)	Acc@1 66.211 (64.681)	Acc@5 96.924 (96.940)
Epoch: [16][3/25]	Time 0.791 (0.738)	Data 0.005 (0.164)	Loss 1.3468 (1.3853)	Acc@1 67.432 (65.369)	Acc@5 96.729 (96.887)
Epoch: [16][4/25]	Time 0.744 (0.740)	Data 0.006 (0.132)	Loss 1.3741 (1.3831)	Acc@1 66.504 (65.596)	Acc@5 96.973 (96.904)
Epoch: [16][5/25]	Time 0.743 (0.740)	Data 0.004 (0.111)	Loss 1.3404 (1.3760)	Acc@1 66.406 (65.731)	Acc@5 96.826 (96.891)
Epoch: [16][6/25]	Time 0.736 (0.740)	Data 0.004 (0.096)	Loss 1.3868 (1.3775)	Acc@1 66.016 (65.771)	Acc@5 96.777 (96.875)
Epoch: [16][7/25]	Time 0.743 (0.740)	Data 0.005 (0.084)	Loss 1.3394 (1.3727)	Acc@1 66.748 (65.894)	Acc@5 97.217 (96.918)
Epoch: [16][8/25]	Time 0.758 (0.742)	Data 0.005 (0.076)	Loss 1.3553 (1.3708)	Acc@1 65.625 (65.864)	Acc@5 96.924 (96.918)
Epoch: [16][9/25]	Time 0.766 (0.744)	Data 0.004 (0.068)	Loss 1.3433 (1.3681)	Acc@1 65.430 (65.820)	Acc@5 97.168 (96.943)
Epoch: [16][10/25]	Time 0.719 (0.742)	Data 0.007 (0.063)	Loss 1.3559 (1.3670)	Acc@1 65.479 (65.789)	Acc@5 97.510 (96.995)
Epoch: [16][11/25]	Time 0.755 (0.743)	Data 0.004 (0.058)	Loss 1.3604 (1.3664)	Acc@1 66.797 (65.873)	Acc@5 95.947 (96.908)
Epoch: [16][12/25]	Time 0.716 (0.741)	Data 0.007 (0.054)	Loss 1.3243 (1.3632)	Acc@1 66.650 (65.933)	Acc@5 97.119 (96.924)
Epoch: [16][13/25]	Time 0.638 (0.734)	Data 0.005 (0.051)	Loss 1.3356 (1.3612)	Acc@1 66.846 (65.998)	Acc@5 97.021 (96.931)
Epoch: [16][14/25]	Time 0.698 (0.731)	Data 0.005 (0.048)	Loss 1.3236 (1.3587)	Acc@1 67.822 (66.120)	Acc@5 97.070 (96.940)
Epoch: [16][15/25]	Time 0.737 (0.732)	Data 0.007 (0.045)	Loss 1.3191 (1.3562)	Acc@1 67.871 (66.229)	Acc@5 97.314 (96.964)
Epoch: [16][16/25]	Time 0.780 (0.735)	Data 0.007 (0.043)	Loss 1.3758 (1.3574)	Acc@1 65.723 (66.199)	Acc@5 96.533 (96.938)
Epoch: [16][17/25]	Time 0.709 (0.733)	Data 0.005 (0.041)	Loss 1.3027 (1.3543)	Acc@1 67.725 (66.284)	Acc@5 97.021 (96.943)
Epoch: [16][18/25]	Time 0.658 (0.729)	Data 0.005 (0.039)	Loss 1.3072 (1.3519)	Acc@1 66.797 (66.311)	Acc@5 97.607 (96.978)
Epoch: [16][19/25]	Time 0.626 (0.724)	Data 0.005 (0.037)	Loss 1.3553 (1.3520)	Acc@1 65.625 (66.277)	Acc@5 97.510 (97.004)
Epoch: [16][20/25]	Time 0.674 (0.722)	Data 0.007 (0.036)	Loss 1.3007 (1.3496)	Acc@1 67.627 (66.341)	Acc@5 97.656 (97.035)
Epoch: [16][21/25]	Time 0.660 (0.719)	Data 0.005 (0.034)	Loss 1.2911 (1.3469)	Acc@1 66.895 (66.366)	Acc@5 97.461 (97.055)
Epoch: [16][22/25]	Time 0.670 (0.717)	Data 0.006 (0.033)	Loss 1.3132 (1.3455)	Acc@1 67.090 (66.398)	Acc@5 97.754 (97.085)
Epoch: [16][23/25]	Time 0.741 (0.718)	Data 0.005 (0.032)	Loss 1.2645 (1.3421)	Acc@1 69.141 (66.512)	Acc@5 97.705 (97.111)
Epoch: [16][24/25]	Time 0.436 (0.706)	Data 0.007 (0.031)	Loss 1.3091 (1.3415)	Acc@1 68.750 (66.550)	Acc@5 96.108 (97.094)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/25]	Time 0.765 (0.765)	Data 0.676 (0.676)	Loss 1.2704 (1.2704)	Acc@1 69.678 (69.678)	Acc@5 97.217 (97.217)
Epoch: [17][1/25]	Time 0.735 (0.750)	Data 0.006 (0.341)	Loss 1.2743 (1.2723)	Acc@1 69.141 (69.409)	Acc@5 97.510 (97.363)
Epoch: [17][2/25]	Time 0.710 (0.737)	Data 0.008 (0.230)	Loss 1.2348 (1.2598)	Acc@1 71.240 (70.020)	Acc@5 97.607 (97.445)
Epoch: [17][3/25]	Time 0.771 (0.745)	Data 0.007 (0.174)	Loss 1.2788 (1.2646)	Acc@1 69.043 (69.775)	Acc@5 97.949 (97.571)
Epoch: [17][4/25]	Time 0.693 (0.735)	Data 0.006 (0.140)	Loss 1.3111 (1.2739)	Acc@1 67.090 (69.238)	Acc@5 97.705 (97.598)
Epoch: [17][5/25]	Time 0.677 (0.725)	Data 0.006 (0.118)	Loss 1.2623 (1.2719)	Acc@1 68.457 (69.108)	Acc@5 97.559 (97.591)
Epoch: [17][6/25]	Time 0.659 (0.716)	Data 0.006 (0.102)	Loss 1.2536 (1.2693)	Acc@1 68.750 (69.057)	Acc@5 97.705 (97.607)
Epoch: [17][7/25]	Time 0.666 (0.710)	Data 0.005 (0.090)	Loss 1.2783 (1.2704)	Acc@1 68.750 (69.019)	Acc@5 97.412 (97.583)
Epoch: [17][8/25]	Time 0.671 (0.705)	Data 0.007 (0.081)	Loss 1.2606 (1.2693)	Acc@1 69.043 (69.021)	Acc@5 97.266 (97.548)
Epoch: [17][9/25]	Time 0.662 (0.701)	Data 0.005 (0.073)	Loss 1.2540 (1.2678)	Acc@1 69.775 (69.097)	Acc@5 96.777 (97.471)
Epoch: [17][10/25]	Time 0.635 (0.695)	Data 0.008 (0.067)	Loss 1.2341 (1.2647)	Acc@1 70.654 (69.238)	Acc@5 97.705 (97.492)
Epoch: [17][11/25]	Time 0.660 (0.692)	Data 0.009 (0.062)	Loss 1.2310 (1.2619)	Acc@1 69.727 (69.279)	Acc@5 98.145 (97.546)
Epoch: [17][12/25]	Time 0.650 (0.689)	Data 0.004 (0.058)	Loss 1.2309 (1.2595)	Acc@1 70.410 (69.366)	Acc@5 97.998 (97.581)
Epoch: [17][13/25]	Time 0.702 (0.690)	Data 0.006 (0.054)	Loss 1.2684 (1.2602)	Acc@1 68.701 (69.318)	Acc@5 97.559 (97.580)
Epoch: [17][14/25]	Time 0.760 (0.694)	Data 0.006 (0.051)	Loss 1.2106 (1.2569)	Acc@1 70.361 (69.388)	Acc@5 97.803 (97.594)
Epoch: [17][15/25]	Time 0.728 (0.697)	Data 0.008 (0.048)	Loss 1.2168 (1.2544)	Acc@1 71.191 (69.501)	Acc@5 97.656 (97.598)
Epoch: [17][16/25]	Time 0.739 (0.699)	Data 0.007 (0.046)	Loss 1.2942 (1.2567)	Acc@1 67.627 (69.391)	Acc@5 96.973 (97.561)
Epoch: [17][17/25]	Time 0.730 (0.701)	Data 0.007 (0.044)	Loss 1.2505 (1.2564)	Acc@1 67.822 (69.303)	Acc@5 96.973 (97.529)
Epoch: [17][18/25]	Time 0.781 (0.705)	Data 0.006 (0.042)	Loss 1.2410 (1.2556)	Acc@1 69.775 (69.328)	Acc@5 97.559 (97.530)
Epoch: [17][19/25]	Time 0.716 (0.706)	Data 0.007 (0.040)	Loss 1.2359 (1.2546)	Acc@1 69.043 (69.314)	Acc@5 97.656 (97.537)
Epoch: [17][20/25]	Time 0.639 (0.702)	Data 0.005 (0.038)	Loss 1.2388 (1.2538)	Acc@1 68.994 (69.299)	Acc@5 97.656 (97.542)
Epoch: [17][21/25]	Time 0.675 (0.701)	Data 0.009 (0.037)	Loss 1.2532 (1.2538)	Acc@1 69.043 (69.287)	Acc@5 97.412 (97.536)
Epoch: [17][22/25]	Time 0.667 (0.700)	Data 0.005 (0.036)	Loss 1.1955 (1.2513)	Acc@1 71.631 (69.389)	Acc@5 97.803 (97.548)
Epoch: [17][23/25]	Time 0.638 (0.697)	Data 0.005 (0.034)	Loss 1.2470 (1.2511)	Acc@1 70.068 (69.417)	Acc@5 97.070 (97.528)
Epoch: [17][24/25]	Time 0.338 (0.683)	Data 0.006 (0.033)	Loss 1.1417 (1.2492)	Acc@1 71.580 (69.454)	Acc@5 98.113 (97.538)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/25]	Time 0.689 (0.689)	Data 0.666 (0.666)	Loss 1.2099 (1.2099)	Acc@1 70.117 (70.117)	Acc@5 97.607 (97.607)
Epoch: [18][1/25]	Time 0.887 (0.788)	Data 0.004 (0.335)	Loss 1.1922 (1.2011)	Acc@1 71.240 (70.679)	Acc@5 97.900 (97.754)
Epoch: [18][2/25]	Time 0.870 (0.815)	Data 0.003 (0.224)	Loss 1.1818 (1.1946)	Acc@1 72.021 (71.126)	Acc@5 98.193 (97.900)
Epoch: [18][3/25]	Time 0.775 (0.805)	Data 0.004 (0.169)	Loss 1.2059 (1.1974)	Acc@1 70.703 (71.021)	Acc@5 97.754 (97.864)
Epoch: [18][4/25]	Time 0.768 (0.798)	Data 0.006 (0.137)	Loss 1.1691 (1.1918)	Acc@1 70.996 (71.016)	Acc@5 98.535 (97.998)
Epoch: [18][5/25]	Time 0.722 (0.785)	Data 0.005 (0.115)	Loss 1.2098 (1.1948)	Acc@1 70.215 (70.882)	Acc@5 97.607 (97.933)
Epoch: [18][6/25]	Time 0.739 (0.778)	Data 0.004 (0.099)	Loss 1.1525 (1.1887)	Acc@1 72.266 (71.080)	Acc@5 98.340 (97.991)
Epoch: [18][7/25]	Time 0.743 (0.774)	Data 0.005 (0.087)	Loss 1.2371 (1.1948)	Acc@1 69.531 (70.886)	Acc@5 98.145 (98.010)
Epoch: [18][8/25]	Time 0.781 (0.775)	Data 0.005 (0.078)	Loss 1.1791 (1.1930)	Acc@1 72.607 (71.077)	Acc@5 98.047 (98.014)
Epoch: [18][9/25]	Time 0.735 (0.771)	Data 0.005 (0.071)	Loss 1.1829 (1.1920)	Acc@1 71.045 (71.074)	Acc@5 98.096 (98.022)
Epoch: [18][10/25]	Time 0.724 (0.767)	Data 0.007 (0.065)	Loss 1.1743 (1.1904)	Acc@1 72.363 (71.191)	Acc@5 97.852 (98.007)
Epoch: [18][11/25]	Time 0.742 (0.764)	Data 0.007 (0.060)	Loss 1.2267 (1.1934)	Acc@1 70.361 (71.122)	Acc@5 96.729 (97.900)
Epoch: [18][12/25]	Time 0.782 (0.766)	Data 0.008 (0.056)	Loss 1.1867 (1.1929)	Acc@1 72.070 (71.195)	Acc@5 97.412 (97.863)
Epoch: [18][13/25]	Time 0.739 (0.764)	Data 0.007 (0.053)	Loss 1.1788 (1.1919)	Acc@1 72.900 (71.317)	Acc@5 97.656 (97.848)
Epoch: [18][14/25]	Time 0.703 (0.760)	Data 0.006 (0.049)	Loss 1.1530 (1.1893)	Acc@1 71.729 (71.344)	Acc@5 98.242 (97.874)
Epoch: [18][15/25]	Time 0.760 (0.760)	Data 0.007 (0.047)	Loss 1.1686 (1.1880)	Acc@1 72.705 (71.429)	Acc@5 97.559 (97.855)
Epoch: [18][16/25]	Time 0.704 (0.757)	Data 0.005 (0.044)	Loss 1.1889 (1.1881)	Acc@1 71.582 (71.438)	Acc@5 98.047 (97.866)
Epoch: [18][17/25]	Time 0.701 (0.753)	Data 0.007 (0.042)	Loss 1.1373 (1.1852)	Acc@1 73.975 (71.579)	Acc@5 97.559 (97.849)
Epoch: [18][18/25]	Time 0.698 (0.751)	Data 0.006 (0.040)	Loss 1.1907 (1.1855)	Acc@1 70.557 (71.525)	Acc@5 98.096 (97.862)
Epoch: [18][19/25]	Time 0.830 (0.754)	Data 0.005 (0.039)	Loss 1.1112 (1.1818)	Acc@1 73.486 (71.624)	Acc@5 98.291 (97.883)
Epoch: [18][20/25]	Time 0.840 (0.759)	Data 0.005 (0.037)	Loss 1.1712 (1.1813)	Acc@1 71.680 (71.626)	Acc@5 97.754 (97.877)
Epoch: [18][21/25]	Time 0.748 (0.758)	Data 0.005 (0.036)	Loss 1.1641 (1.1805)	Acc@1 71.631 (71.626)	Acc@5 98.096 (97.887)
Epoch: [18][22/25]	Time 0.680 (0.755)	Data 0.008 (0.034)	Loss 1.1211 (1.1779)	Acc@1 73.828 (71.722)	Acc@5 97.900 (97.888)
Epoch: [18][23/25]	Time 0.845 (0.758)	Data 0.007 (0.033)	Loss 1.1452 (1.1766)	Acc@1 72.412 (71.751)	Acc@5 98.145 (97.898)
Epoch: [18][24/25]	Time 0.448 (0.746)	Data 0.003 (0.032)	Loss 1.1436 (1.1760)	Acc@1 73.585 (71.782)	Acc@5 97.995 (97.900)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/25]	Time 0.810 (0.810)	Data 0.606 (0.606)	Loss 1.0880 (1.0880)	Acc@1 75.000 (75.000)	Acc@5 98.291 (98.291)
Epoch: [19][1/25]	Time 0.703 (0.757)	Data 0.006 (0.306)	Loss 1.1153 (1.1016)	Acc@1 73.633 (74.316)	Acc@5 97.998 (98.145)
Epoch: [19][2/25]	Time 0.650 (0.721)	Data 0.007 (0.206)	Loss 1.1210 (1.1081)	Acc@1 73.730 (74.121)	Acc@5 98.340 (98.210)
Epoch: [19][3/25]	Time 0.654 (0.704)	Data 0.006 (0.156)	Loss 1.1120 (1.1091)	Acc@1 73.877 (74.060)	Acc@5 98.779 (98.352)
Epoch: [19][4/25]	Time 0.617 (0.687)	Data 0.007 (0.126)	Loss 1.1049 (1.1082)	Acc@1 73.828 (74.014)	Acc@5 97.900 (98.262)
Epoch: [19][5/25]	Time 0.661 (0.683)	Data 0.009 (0.107)	Loss 1.1403 (1.1136)	Acc@1 72.607 (73.779)	Acc@5 97.998 (98.218)
Epoch: [19][6/25]	Time 0.671 (0.681)	Data 0.005 (0.092)	Loss 1.1357 (1.1167)	Acc@1 72.852 (73.647)	Acc@5 98.096 (98.200)
Epoch: [19][7/25]	Time 0.715 (0.685)	Data 0.003 (0.081)	Loss 1.1114 (1.1161)	Acc@1 73.193 (73.590)	Acc@5 98.535 (98.242)
Epoch: [19][8/25]	Time 0.712 (0.688)	Data 0.007 (0.073)	Loss 1.1042 (1.1148)	Acc@1 73.926 (73.627)	Acc@5 98.486 (98.269)
Epoch: [19][9/25]	Time 0.774 (0.697)	Data 0.010 (0.067)	Loss 1.1152 (1.1148)	Acc@1 74.170 (73.682)	Acc@5 98.193 (98.262)
Epoch: [19][10/25]	Time 0.673 (0.695)	Data 0.005 (0.061)	Loss 1.1081 (1.1142)	Acc@1 73.633 (73.677)	Acc@5 98.584 (98.291)
Epoch: [19][11/25]	Time 0.672 (0.693)	Data 0.006 (0.056)	Loss 1.1066 (1.1136)	Acc@1 74.268 (73.726)	Acc@5 97.803 (98.250)
Epoch: [19][12/25]	Time 0.652 (0.690)	Data 0.007 (0.053)	Loss 1.0834 (1.1112)	Acc@1 74.414 (73.779)	Acc@5 98.584 (98.276)
Epoch: [19][13/25]	Time 0.722 (0.692)	Data 0.008 (0.049)	Loss 1.0978 (1.1103)	Acc@1 73.535 (73.762)	Acc@5 98.438 (98.288)
Epoch: [19][14/25]	Time 0.740 (0.695)	Data 0.007 (0.047)	Loss 1.0535 (1.1065)	Acc@1 76.855 (73.968)	Acc@5 98.047 (98.271)
Epoch: [19][15/25]	Time 0.754 (0.699)	Data 0.004 (0.044)	Loss 1.1130 (1.1069)	Acc@1 73.438 (73.935)	Acc@5 97.852 (98.245)
Epoch: [19][16/25]	Time 0.733 (0.701)	Data 0.007 (0.042)	Loss 1.0730 (1.1049)	Acc@1 74.951 (73.995)	Acc@5 98.730 (98.274)
Epoch: [19][17/25]	Time 0.791 (0.706)	Data 0.004 (0.040)	Loss 1.0716 (1.1031)	Acc@1 74.609 (74.029)	Acc@5 97.949 (98.256)
Epoch: [19][18/25]	Time 0.700 (0.706)	Data 0.004 (0.038)	Loss 1.0878 (1.1023)	Acc@1 74.316 (74.044)	Acc@5 98.682 (98.278)
Epoch: [19][19/25]	Time 0.621 (0.701)	Data 0.008 (0.036)	Loss 1.0950 (1.1019)	Acc@1 73.584 (74.021)	Acc@5 98.877 (98.308)
Epoch: [19][20/25]	Time 0.651 (0.699)	Data 0.007 (0.035)	Loss 1.0846 (1.1011)	Acc@1 74.414 (74.040)	Acc@5 98.584 (98.321)
Epoch: [19][21/25]	Time 0.671 (0.698)	Data 0.006 (0.034)	Loss 1.0908 (1.1006)	Acc@1 74.072 (74.041)	Acc@5 98.047 (98.309)
Epoch: [19][22/25]	Time 0.741 (0.700)	Data 0.008 (0.032)	Loss 1.0323 (1.0976)	Acc@1 75.781 (74.117)	Acc@5 98.486 (98.316)
Epoch: [19][23/25]	Time 0.773 (0.703)	Data 0.007 (0.031)	Loss 1.0256 (1.0946)	Acc@1 77.197 (74.245)	Acc@5 98.926 (98.342)
Epoch: [19][24/25]	Time 0.420 (0.691)	Data 0.004 (0.030)	Loss 1.1736 (1.0960)	Acc@1 72.170 (74.210)	Acc@5 97.524 (98.328)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/25]	Time 0.710 (0.710)	Data 0.602 (0.602)	Loss 1.0483 (1.0483)	Acc@1 75.928 (75.928)	Acc@5 98.730 (98.730)
Epoch: [20][1/25]	Time 0.663 (0.687)	Data 0.007 (0.304)	Loss 1.0527 (1.0505)	Acc@1 75.342 (75.635)	Acc@5 98.828 (98.779)
Epoch: [20][2/25]	Time 0.736 (0.703)	Data 0.009 (0.206)	Loss 1.0749 (1.0586)	Acc@1 74.609 (75.293)	Acc@5 97.998 (98.519)
Epoch: [20][3/25]	Time 0.744 (0.713)	Data 0.006 (0.156)	Loss 1.0339 (1.0524)	Acc@1 76.270 (75.537)	Acc@5 98.340 (98.474)
Epoch: [20][4/25]	Time 0.679 (0.706)	Data 0.005 (0.126)	Loss 1.0524 (1.0524)	Acc@1 74.756 (75.381)	Acc@5 98.145 (98.408)
Epoch: [20][5/25]	Time 0.671 (0.701)	Data 0.007 (0.106)	Loss 1.0487 (1.0518)	Acc@1 75.732 (75.439)	Acc@5 98.877 (98.486)
Epoch: [20][6/25]	Time 0.652 (0.694)	Data 0.007 (0.092)	Loss 1.0545 (1.0522)	Acc@1 74.951 (75.370)	Acc@5 98.828 (98.535)
Epoch: [20][7/25]	Time 0.688 (0.693)	Data 0.009 (0.082)	Loss 1.0741 (1.0549)	Acc@1 74.219 (75.226)	Acc@5 98.096 (98.480)
Epoch: [20][8/25]	Time 0.765 (0.701)	Data 0.006 (0.073)	Loss 1.0491 (1.0543)	Acc@1 75.635 (75.271)	Acc@5 98.682 (98.503)
Epoch: [20][9/25]	Time 0.737 (0.705)	Data 0.005 (0.066)	Loss 1.0354 (1.0524)	Acc@1 76.562 (75.400)	Acc@5 98.486 (98.501)
Epoch: [20][10/25]	Time 0.726 (0.707)	Data 0.005 (0.061)	Loss 1.0414 (1.0514)	Acc@1 75.439 (75.404)	Acc@5 98.291 (98.482)
Epoch: [20][11/25]	Time 0.754 (0.710)	Data 0.005 (0.056)	Loss 1.0482 (1.0511)	Acc@1 75.342 (75.399)	Acc@5 98.730 (98.503)
Epoch: [20][12/25]	Time 0.712 (0.711)	Data 0.004 (0.052)	Loss 1.0428 (1.0505)	Acc@1 76.123 (75.454)	Acc@5 98.242 (98.483)
Epoch: [20][13/25]	Time 0.759 (0.714)	Data 0.005 (0.049)	Loss 1.0206 (1.0483)	Acc@1 76.953 (75.562)	Acc@5 98.340 (98.472)
Epoch: [20][14/25]	Time 0.761 (0.717)	Data 0.004 (0.046)	Loss 1.0877 (1.0510)	Acc@1 74.023 (75.459)	Acc@5 98.633 (98.483)
Epoch: [20][15/25]	Time 0.679 (0.715)	Data 0.006 (0.043)	Loss 0.9986 (1.0477)	Acc@1 76.318 (75.513)	Acc@5 98.730 (98.499)
Epoch: [20][16/25]	Time 0.657 (0.711)	Data 0.007 (0.041)	Loss 1.0153 (1.0458)	Acc@1 76.660 (75.580)	Acc@5 98.389 (98.492)
Epoch: [20][17/25]	Time 0.666 (0.709)	Data 0.006 (0.039)	Loss 1.0325 (1.0451)	Acc@1 75.586 (75.581)	Acc@5 98.926 (98.516)
Epoch: [20][18/25]	Time 0.693 (0.708)	Data 0.004 (0.037)	Loss 1.0519 (1.0454)	Acc@1 75.635 (75.583)	Acc@5 98.682 (98.525)
Epoch: [20][19/25]	Time 0.656 (0.705)	Data 0.007 (0.036)	Loss 1.0562 (1.0460)	Acc@1 75.928 (75.601)	Acc@5 98.291 (98.513)
Epoch: [20][20/25]	Time 0.657 (0.703)	Data 0.004 (0.034)	Loss 1.0223 (1.0448)	Acc@1 77.002 (75.667)	Acc@5 98.193 (98.498)
Epoch: [20][21/25]	Time 0.656 (0.701)	Data 0.005 (0.033)	Loss 1.0148 (1.0435)	Acc@1 77.197 (75.737)	Acc@5 98.193 (98.484)
Epoch: [20][22/25]	Time 0.639 (0.698)	Data 0.005 (0.032)	Loss 0.9690 (1.0402)	Acc@1 78.955 (75.877)	Acc@5 98.779 (98.497)
Epoch: [20][23/25]	Time 0.704 (0.698)	Data 0.006 (0.031)	Loss 1.0260 (1.0396)	Acc@1 76.221 (75.891)	Acc@5 98.584 (98.501)
Epoch: [20][24/25]	Time 0.359 (0.685)	Data 0.004 (0.030)	Loss 1.0282 (1.0394)	Acc@1 74.882 (75.874)	Acc@5 98.703 (98.504)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/25]	Time 0.774 (0.774)	Data 0.730 (0.730)	Loss 1.0194 (1.0194)	Acc@1 76.953 (76.953)	Acc@5 97.949 (97.949)
Epoch: [21][1/25]	Time 0.715 (0.745)	Data 0.003 (0.366)	Loss 0.9437 (0.9815)	Acc@1 79.297 (78.125)	Acc@5 99.121 (98.535)
Epoch: [21][2/25]	Time 0.721 (0.737)	Data 0.006 (0.246)	Loss 0.9083 (0.9571)	Acc@1 81.445 (79.232)	Acc@5 99.170 (98.747)
Epoch: [21][3/25]	Time 0.755 (0.741)	Data 0.003 (0.185)	Loss 0.9427 (0.9535)	Acc@1 78.711 (79.102)	Acc@5 98.975 (98.804)
Epoch: [21][4/25]	Time 0.675 (0.728)	Data 0.008 (0.150)	Loss 0.9501 (0.9529)	Acc@1 77.539 (78.789)	Acc@5 98.926 (98.828)
Epoch: [21][5/25]	Time 0.727 (0.728)	Data 0.007 (0.126)	Loss 0.9078 (0.9454)	Acc@1 80.811 (79.126)	Acc@5 99.170 (98.885)
Epoch: [21][6/25]	Time 0.759 (0.732)	Data 0.005 (0.109)	Loss 0.9203 (0.9418)	Acc@1 80.176 (79.276)	Acc@5 99.121 (98.919)
Epoch: [21][7/25]	Time 0.728 (0.732)	Data 0.004 (0.096)	Loss 0.9645 (0.9446)	Acc@1 77.832 (79.095)	Acc@5 98.926 (98.920)
Epoch: [21][8/25]	Time 0.740 (0.733)	Data 0.006 (0.086)	Loss 0.9626 (0.9466)	Acc@1 78.223 (78.998)	Acc@5 98.486 (98.872)
Epoch: [21][9/25]	Time 0.750 (0.734)	Data 0.004 (0.077)	Loss 0.9685 (0.9488)	Acc@1 78.125 (78.911)	Acc@5 98.828 (98.867)
Epoch: [21][10/25]	Time 0.708 (0.732)	Data 0.006 (0.071)	Loss 0.9847 (0.9521)	Acc@1 76.855 (78.724)	Acc@5 98.926 (98.873)
Epoch: [21][11/25]	Time 0.659 (0.726)	Data 0.007 (0.066)	Loss 0.9990 (0.9560)	Acc@1 76.367 (78.528)	Acc@5 98.730 (98.861)
Epoch: [21][12/25]	Time 0.658 (0.721)	Data 0.008 (0.061)	Loss 0.9991 (0.9593)	Acc@1 77.002 (78.410)	Acc@5 98.438 (98.828)
Epoch: [21][13/25]	Time 0.695 (0.719)	Data 0.006 (0.057)	Loss 0.9931 (0.9617)	Acc@1 77.148 (78.320)	Acc@5 98.682 (98.818)
Epoch: [21][14/25]	Time 0.659 (0.715)	Data 0.004 (0.054)	Loss 0.9789 (0.9629)	Acc@1 78.271 (78.317)	Acc@5 98.486 (98.796)
Epoch: [21][15/25]	Time 0.751 (0.717)	Data 0.007 (0.051)	Loss 1.0105 (0.9658)	Acc@1 77.051 (78.238)	Acc@5 98.193 (98.758)
Epoch: [21][16/25]	Time 0.785 (0.721)	Data 0.006 (0.048)	Loss 0.9558 (0.9652)	Acc@1 78.467 (78.251)	Acc@5 98.779 (98.759)
Epoch: [21][17/25]	Time 0.746 (0.722)	Data 0.006 (0.046)	Loss 0.9772 (0.9659)	Acc@1 77.344 (78.201)	Acc@5 99.023 (98.774)
Epoch: [21][18/25]	Time 0.779 (0.725)	Data 0.006 (0.044)	Loss 0.9511 (0.9651)	Acc@1 78.516 (78.218)	Acc@5 98.828 (98.777)
Epoch: [21][19/25]	Time 0.684 (0.723)	Data 0.004 (0.042)	Loss 0.9984 (0.9668)	Acc@1 77.197 (78.167)	Acc@5 98.193 (98.748)
Epoch: [21][20/25]	Time 0.742 (0.724)	Data 0.006 (0.040)	Loss 0.9398 (0.9655)	Acc@1 79.541 (78.232)	Acc@5 98.828 (98.751)
Epoch: [21][21/25]	Time 0.752 (0.725)	Data 0.005 (0.039)	Loss 0.9490 (0.9647)	Acc@1 77.979 (78.220)	Acc@5 98.730 (98.750)
Epoch: [21][22/25]	Time 0.672 (0.723)	Data 0.006 (0.037)	Loss 0.9621 (0.9646)	Acc@1 78.516 (78.233)	Acc@5 98.486 (98.739)
Epoch: [21][23/25]	Time 0.667 (0.721)	Data 0.004 (0.036)	Loss 0.9956 (0.9659)	Acc@1 77.393 (78.198)	Acc@5 98.340 (98.722)
Epoch: [21][24/25]	Time 0.361 (0.706)	Data 0.004 (0.034)	Loss 0.9503 (0.9657)	Acc@1 78.774 (78.208)	Acc@5 98.939 (98.726)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/25]	Time 0.740 (0.740)	Data 0.681 (0.681)	Loss 0.9739 (0.9739)	Acc@1 77.686 (77.686)	Acc@5 98.633 (98.633)
Epoch: [22][1/25]	Time 0.707 (0.724)	Data 0.006 (0.344)	Loss 0.9382 (0.9561)	Acc@1 78.271 (77.979)	Acc@5 98.828 (98.730)
Epoch: [22][2/25]	Time 0.727 (0.725)	Data 0.005 (0.231)	Loss 0.9842 (0.9655)	Acc@1 77.100 (77.686)	Acc@5 98.779 (98.747)
Epoch: [22][3/25]	Time 0.749 (0.731)	Data 0.004 (0.174)	Loss 0.9762 (0.9682)	Acc@1 78.125 (77.795)	Acc@5 99.072 (98.828)
Epoch: [22][4/25]	Time 0.724 (0.729)	Data 0.007 (0.141)	Loss 0.9873 (0.9720)	Acc@1 77.979 (77.832)	Acc@5 99.072 (98.877)
Epoch: [22][5/25]	Time 0.763 (0.735)	Data 0.005 (0.118)	Loss 0.9528 (0.9688)	Acc@1 78.125 (77.881)	Acc@5 98.730 (98.853)
Epoch: [22][6/25]	Time 0.745 (0.736)	Data 0.005 (0.102)	Loss 0.9555 (0.9669)	Acc@1 79.053 (78.048)	Acc@5 98.438 (98.793)
Epoch: [22][7/25]	Time 0.763 (0.740)	Data 0.004 (0.090)	Loss 0.9702 (0.9673)	Acc@1 78.418 (78.094)	Acc@5 98.340 (98.737)
Epoch: [22][8/25]	Time 0.740 (0.740)	Data 0.006 (0.080)	Loss 0.9714 (0.9678)	Acc@1 77.783 (78.060)	Acc@5 98.535 (98.714)
Epoch: [22][9/25]	Time 0.757 (0.741)	Data 0.005 (0.073)	Loss 0.9669 (0.9677)	Acc@1 77.490 (78.003)	Acc@5 98.975 (98.740)
Epoch: [22][10/25]	Time 0.682 (0.736)	Data 0.005 (0.067)	Loss 0.9091 (0.9623)	Acc@1 79.395 (78.129)	Acc@5 99.072 (98.770)
Epoch: [22][11/25]	Time 0.750 (0.737)	Data 0.007 (0.062)	Loss 0.9442 (0.9608)	Acc@1 79.346 (78.231)	Acc@5 98.975 (98.787)
Epoch: [22][12/25]	Time 0.757 (0.739)	Data 0.006 (0.057)	Loss 0.9999 (0.9638)	Acc@1 76.025 (78.061)	Acc@5 98.730 (98.783)
Epoch: [22][13/25]	Time 0.772 (0.741)	Data 0.008 (0.054)	Loss 0.9951 (0.9661)	Acc@1 76.855 (77.975)	Acc@5 98.242 (98.744)
Epoch: [22][14/25]	Time 0.743 (0.741)	Data 0.005 (0.051)	Loss 0.9861 (0.9674)	Acc@1 77.637 (77.952)	Acc@5 99.023 (98.763)
Epoch: [22][15/25]	Time 0.786 (0.744)	Data 0.004 (0.048)	Loss 0.9752 (0.9679)	Acc@1 77.637 (77.933)	Acc@5 98.633 (98.755)
Epoch: [22][16/25]	Time 0.718 (0.742)	Data 0.008 (0.045)	Loss 0.9406 (0.9663)	Acc@1 78.027 (77.938)	Acc@5 98.975 (98.768)
Epoch: [22][17/25]	Time 0.644 (0.737)	Data 0.007 (0.043)	Loss 0.9100 (0.9632)	Acc@1 79.297 (78.014)	Acc@5 99.170 (98.790)
Epoch: [22][18/25]	Time 0.690 (0.735)	Data 0.004 (0.041)	Loss 0.9518 (0.9626)	Acc@1 78.369 (78.032)	Acc@5 98.682 (98.784)
Epoch: [22][19/25]	Time 0.645 (0.730)	Data 0.004 (0.039)	Loss 0.9674 (0.9628)	Acc@1 77.686 (78.015)	Acc@5 98.438 (98.767)
Epoch: [22][20/25]	Time 0.684 (0.728)	Data 0.006 (0.038)	Loss 0.9412 (0.9618)	Acc@1 78.369 (78.032)	Acc@5 99.268 (98.791)
Epoch: [22][21/25]	Time 0.709 (0.727)	Data 0.004 (0.036)	Loss 0.9443 (0.9610)	Acc@1 78.174 (78.038)	Acc@5 98.877 (98.795)
Epoch: [22][22/25]	Time 0.796 (0.730)	Data 0.005 (0.035)	Loss 0.9880 (0.9622)	Acc@1 77.930 (78.034)	Acc@5 98.389 (98.777)
Epoch: [22][23/25]	Time 0.728 (0.730)	Data 0.007 (0.034)	Loss 0.9868 (0.9632)	Acc@1 77.051 (77.993)	Acc@5 98.633 (98.771)
Epoch: [22][24/25]	Time 0.424 (0.718)	Data 0.006 (0.033)	Loss 0.9218 (0.9625)	Acc@1 78.774 (78.006)	Acc@5 99.175 (98.778)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/25]	Time 0.716 (0.716)	Data 0.730 (0.730)	Loss 0.9920 (0.9920)	Acc@1 76.514 (76.514)	Acc@5 98.486 (98.486)
Epoch: [23][1/25]	Time 0.724 (0.720)	Data 0.006 (0.368)	Loss 0.9934 (0.9927)	Acc@1 76.318 (76.416)	Acc@5 98.242 (98.364)
Epoch: [23][2/25]	Time 0.749 (0.730)	Data 0.003 (0.246)	Loss 0.9499 (0.9784)	Acc@1 77.441 (76.758)	Acc@5 98.828 (98.519)
Epoch: [23][3/25]	Time 0.757 (0.737)	Data 0.005 (0.186)	Loss 0.9963 (0.9829)	Acc@1 76.416 (76.672)	Acc@5 98.438 (98.499)
Epoch: [23][4/25]	Time 0.741 (0.737)	Data 0.005 (0.150)	Loss 0.9674 (0.9798)	Acc@1 77.881 (76.914)	Acc@5 98.730 (98.545)
Epoch: [23][5/25]	Time 0.716 (0.734)	Data 0.005 (0.126)	Loss 0.9451 (0.9740)	Acc@1 78.125 (77.116)	Acc@5 98.730 (98.576)
Epoch: [23][6/25]	Time 0.753 (0.737)	Data 0.007 (0.109)	Loss 0.9739 (0.9740)	Acc@1 77.344 (77.148)	Acc@5 98.730 (98.598)
Epoch: [23][7/25]	Time 0.787 (0.743)	Data 0.004 (0.096)	Loss 0.9732 (0.9739)	Acc@1 77.344 (77.173)	Acc@5 98.438 (98.578)
Epoch: [23][8/25]	Time 0.729 (0.741)	Data 0.005 (0.086)	Loss 0.9920 (0.9759)	Acc@1 76.758 (77.127)	Acc@5 98.438 (98.562)
Epoch: [23][9/25]	Time 0.779 (0.745)	Data 0.007 (0.078)	Loss 0.9371 (0.9720)	Acc@1 78.125 (77.227)	Acc@5 98.975 (98.604)
Epoch: [23][10/25]	Time 0.791 (0.749)	Data 0.008 (0.071)	Loss 0.9379 (0.9689)	Acc@1 78.760 (77.366)	Acc@5 98.438 (98.588)
Epoch: [23][11/25]	Time 0.743 (0.749)	Data 0.006 (0.066)	Loss 0.9522 (0.9675)	Acc@1 78.369 (77.450)	Acc@5 98.389 (98.572)
Epoch: [23][12/25]	Time 0.751 (0.749)	Data 0.006 (0.061)	Loss 0.9730 (0.9679)	Acc@1 77.295 (77.438)	Acc@5 98.877 (98.595)
Epoch: [23][13/25]	Time 0.715 (0.747)	Data 0.004 (0.057)	Loss 0.9215 (0.9646)	Acc@1 78.809 (77.536)	Acc@5 98.438 (98.584)
Epoch: [23][14/25]	Time 0.652 (0.740)	Data 0.008 (0.054)	Loss 0.9597 (0.9643)	Acc@1 78.125 (77.575)	Acc@5 98.926 (98.607)
Epoch: [23][15/25]	Time 0.662 (0.735)	Data 0.005 (0.051)	Loss 0.9413 (0.9629)	Acc@1 78.564 (77.637)	Acc@5 98.389 (98.593)
Epoch: [23][16/25]	Time 0.635 (0.729)	Data 0.007 (0.048)	Loss 0.9348 (0.9612)	Acc@1 78.613 (77.694)	Acc@5 98.926 (98.613)
Epoch: [23][17/25]	Time 0.625 (0.724)	Data 0.007 (0.046)	Loss 0.9325 (0.9596)	Acc@1 78.857 (77.759)	Acc@5 98.975 (98.633)
Epoch: [23][18/25]	Time 0.623 (0.718)	Data 0.007 (0.044)	Loss 0.9248 (0.9578)	Acc@1 78.809 (77.814)	Acc@5 99.170 (98.661)
Epoch: [23][19/25]	Time 0.661 (0.716)	Data 0.008 (0.042)	Loss 0.8855 (0.9542)	Acc@1 79.199 (77.883)	Acc@5 99.219 (98.689)
Epoch: [23][20/25]	Time 0.628 (0.711)	Data 0.004 (0.040)	Loss 0.8958 (0.9514)	Acc@1 80.762 (78.020)	Acc@5 98.584 (98.684)
Epoch: [23][21/25]	Time 0.681 (0.710)	Data 0.005 (0.039)	Loss 0.9164 (0.9498)	Acc@1 79.053 (78.067)	Acc@5 98.877 (98.693)
Epoch: [23][22/25]	Time 0.681 (0.709)	Data 0.005 (0.037)	Loss 0.9040 (0.9478)	Acc@1 79.932 (78.148)	Acc@5 98.975 (98.705)
Epoch: [23][23/25]	Time 0.695 (0.708)	Data 0.007 (0.036)	Loss 0.9153 (0.9465)	Acc@1 80.225 (78.235)	Acc@5 98.291 (98.688)
Epoch: [23][24/25]	Time 0.447 (0.698)	Data 0.006 (0.035)	Loss 0.9084 (0.9458)	Acc@1 79.127 (78.250)	Acc@5 98.467 (98.684)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/25]	Time 0.792 (0.792)	Data 0.605 (0.605)	Loss 0.9344 (0.9344)	Acc@1 79.199 (79.199)	Acc@5 98.340 (98.340)
Epoch: [24][1/25]	Time 0.734 (0.763)	Data 0.009 (0.307)	Loss 0.8844 (0.9094)	Acc@1 80.566 (79.883)	Acc@5 99.219 (98.779)
Epoch: [24][2/25]	Time 0.717 (0.748)	Data 0.006 (0.207)	Loss 0.9388 (0.9192)	Acc@1 78.613 (79.460)	Acc@5 98.438 (98.665)
Epoch: [24][3/25]	Time 0.759 (0.751)	Data 0.006 (0.156)	Loss 0.9009 (0.9146)	Acc@1 80.713 (79.773)	Acc@5 98.926 (98.730)
Epoch: [24][4/25]	Time 0.778 (0.756)	Data 0.005 (0.126)	Loss 0.8916 (0.9100)	Acc@1 80.078 (79.834)	Acc@5 99.072 (98.799)
Epoch: [24][5/25]	Time 0.702 (0.747)	Data 0.008 (0.106)	Loss 0.8798 (0.9050)	Acc@1 80.420 (79.932)	Acc@5 99.219 (98.869)
Epoch: [24][6/25]	Time 0.703 (0.741)	Data 0.003 (0.092)	Loss 0.8840 (0.9020)	Acc@1 80.615 (80.029)	Acc@5 98.926 (98.877)
Epoch: [24][7/25]	Time 0.746 (0.741)	Data 0.005 (0.081)	Loss 0.8796 (0.8992)	Acc@1 80.518 (80.090)	Acc@5 98.828 (98.871)
Epoch: [24][8/25]	Time 0.770 (0.745)	Data 0.004 (0.072)	Loss 0.8739 (0.8964)	Acc@1 81.689 (80.268)	Acc@5 98.730 (98.855)
Epoch: [24][9/25]	Time 0.706 (0.741)	Data 0.007 (0.066)	Loss 0.8704 (0.8938)	Acc@1 80.078 (80.249)	Acc@5 98.975 (98.867)
Epoch: [24][10/25]	Time 0.644 (0.732)	Data 0.007 (0.060)	Loss 0.8798 (0.8925)	Acc@1 80.078 (80.233)	Acc@5 98.779 (98.859)
Epoch: [24][11/25]	Time 0.672 (0.727)	Data 0.008 (0.056)	Loss 0.8709 (0.8907)	Acc@1 81.250 (80.318)	Acc@5 99.072 (98.877)
Epoch: [24][12/25]	Time 0.627 (0.719)	Data 0.005 (0.052)	Loss 0.8597 (0.8883)	Acc@1 80.811 (80.356)	Acc@5 98.828 (98.873)
Epoch: [24][13/25]	Time 0.634 (0.713)	Data 0.007 (0.049)	Loss 0.8627 (0.8865)	Acc@1 80.420 (80.361)	Acc@5 99.170 (98.894)
Epoch: [24][14/25]	Time 0.691 (0.712)	Data 0.008 (0.046)	Loss 0.8550 (0.8844)	Acc@1 81.592 (80.443)	Acc@5 98.926 (98.896)
Epoch: [24][15/25]	Time 0.731 (0.713)	Data 0.006 (0.044)	Loss 0.8770 (0.8839)	Acc@1 80.176 (80.426)	Acc@5 99.121 (98.911)
Epoch: [24][16/25]	Time 0.764 (0.716)	Data 0.007 (0.041)	Loss 0.8525 (0.8821)	Acc@1 81.396 (80.483)	Acc@5 99.121 (98.923)
Epoch: [24][17/25]	Time 0.731 (0.717)	Data 0.005 (0.039)	Loss 0.8499 (0.8803)	Acc@1 81.592 (80.545)	Acc@5 99.121 (98.934)
Epoch: [24][18/25]	Time 0.740 (0.718)	Data 0.005 (0.038)	Loss 0.8979 (0.8812)	Acc@1 79.150 (80.471)	Acc@5 99.170 (98.946)
Epoch: [24][19/25]	Time 0.766 (0.720)	Data 0.007 (0.036)	Loss 0.8340 (0.8789)	Acc@1 82.422 (80.569)	Acc@5 99.023 (98.950)
Epoch: [24][20/25]	Time 0.697 (0.719)	Data 0.006 (0.035)	Loss 0.8608 (0.8780)	Acc@1 81.152 (80.597)	Acc@5 98.682 (98.937)
Epoch: [24][21/25]	Time 0.674 (0.717)	Data 0.007 (0.033)	Loss 0.8888 (0.8785)	Acc@1 80.273 (80.582)	Acc@5 98.584 (98.921)
Epoch: [24][22/25]	Time 0.724 (0.718)	Data 0.005 (0.032)	Loss 0.8611 (0.8777)	Acc@1 81.348 (80.615)	Acc@5 98.779 (98.915)
Epoch: [24][23/25]	Time 0.759 (0.719)	Data 0.006 (0.031)	Loss 0.8690 (0.8774)	Acc@1 80.566 (80.613)	Acc@5 99.170 (98.926)
Epoch: [24][24/25]	Time 0.409 (0.707)	Data 0.004 (0.030)	Loss 0.8503 (0.8769)	Acc@1 81.250 (80.624)	Acc@5 99.646 (98.938)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/25]	Time 0.735 (0.735)	Data 0.623 (0.623)	Loss 0.8589 (0.8589)	Acc@1 80.078 (80.078)	Acc@5 99.023 (99.023)
Epoch: [25][1/25]	Time 0.726 (0.730)	Data 0.008 (0.316)	Loss 0.9113 (0.8851)	Acc@1 80.273 (80.176)	Acc@5 98.877 (98.950)
Epoch: [25][2/25]	Time 0.754 (0.738)	Data 0.004 (0.212)	Loss 0.8499 (0.8734)	Acc@1 80.615 (80.322)	Acc@5 98.828 (98.910)
Epoch: [25][3/25]	Time 0.745 (0.740)	Data 0.007 (0.160)	Loss 0.8670 (0.8718)	Acc@1 80.664 (80.408)	Acc@5 99.268 (98.999)
Epoch: [25][4/25]	Time 0.728 (0.738)	Data 0.006 (0.129)	Loss 0.8781 (0.8731)	Acc@1 80.762 (80.479)	Acc@5 99.121 (99.023)
Epoch: [25][5/25]	Time 0.755 (0.740)	Data 0.008 (0.109)	Loss 0.8440 (0.8682)	Acc@1 81.592 (80.664)	Acc@5 99.219 (99.056)
Epoch: [25][6/25]	Time 0.707 (0.736)	Data 0.003 (0.094)	Loss 0.8162 (0.8608)	Acc@1 82.031 (80.859)	Acc@5 98.975 (99.044)
Epoch: [25][7/25]	Time 0.692 (0.730)	Data 0.006 (0.083)	Loss 0.8982 (0.8655)	Acc@1 78.906 (80.615)	Acc@5 98.926 (99.030)
Epoch: [25][8/25]	Time 0.691 (0.726)	Data 0.005 (0.074)	Loss 0.8909 (0.8683)	Acc@1 80.176 (80.566)	Acc@5 98.877 (99.013)
Epoch: [25][9/25]	Time 0.631 (0.716)	Data 0.004 (0.067)	Loss 0.8503 (0.8665)	Acc@1 80.518 (80.562)	Acc@5 98.828 (98.994)
Epoch: [25][10/25]	Time 0.736 (0.718)	Data 0.007 (0.062)	Loss 0.8632 (0.8662)	Acc@1 81.201 (80.620)	Acc@5 98.877 (98.983)
Epoch: [25][11/25]	Time 0.813 (0.726)	Data 0.005 (0.057)	Loss 0.8937 (0.8685)	Acc@1 80.029 (80.570)	Acc@5 98.584 (98.950)
Epoch: [25][12/25]	Time 0.646 (0.720)	Data 0.004 (0.053)	Loss 0.8503 (0.8671)	Acc@1 81.738 (80.660)	Acc@5 98.828 (98.941)
Epoch: [25][13/25]	Time 0.652 (0.715)	Data 0.004 (0.050)	Loss 0.8899 (0.8687)	Acc@1 79.150 (80.552)	Acc@5 98.779 (98.929)
Epoch: [25][14/25]	Time 0.669 (0.712)	Data 0.008 (0.047)	Loss 0.8775 (0.8693)	Acc@1 79.541 (80.485)	Acc@5 99.219 (98.949)
Epoch: [25][15/25]	Time 0.688 (0.711)	Data 0.007 (0.044)	Loss 0.8390 (0.8674)	Acc@1 81.543 (80.551)	Acc@5 98.926 (98.947)
Epoch: [25][16/25]	Time 0.682 (0.709)	Data 0.006 (0.042)	Loss 0.8307 (0.8653)	Acc@1 81.982 (80.635)	Acc@5 99.121 (98.957)
Epoch: [25][17/25]	Time 0.651 (0.706)	Data 0.005 (0.040)	Loss 0.8370 (0.8637)	Acc@1 82.422 (80.735)	Acc@5 98.730 (98.945)
Epoch: [25][18/25]	Time 0.781 (0.710)	Data 0.007 (0.038)	Loss 0.8179 (0.8613)	Acc@1 82.764 (80.841)	Acc@5 99.121 (98.954)
Epoch: [25][19/25]	Time 0.783 (0.713)	Data 0.004 (0.037)	Loss 0.8522 (0.8608)	Acc@1 81.348 (80.867)	Acc@5 99.121 (98.962)
Epoch: [25][20/25]	Time 0.722 (0.714)	Data 0.004 (0.035)	Loss 0.8490 (0.8603)	Acc@1 81.787 (80.911)	Acc@5 98.828 (98.956)
Epoch: [25][21/25]	Time 0.634 (0.710)	Data 0.006 (0.034)	Loss 0.8444 (0.8595)	Acc@1 83.154 (81.013)	Acc@5 98.779 (98.948)
Epoch: [25][22/25]	Time 0.666 (0.708)	Data 0.007 (0.033)	Loss 0.8350 (0.8585)	Acc@1 81.787 (81.046)	Acc@5 98.828 (98.943)
Epoch: [25][23/25]	Time 0.666 (0.706)	Data 0.005 (0.031)	Loss 0.8705 (0.8590)	Acc@1 79.443 (80.979)	Acc@5 98.730 (98.934)
Epoch: [25][24/25]	Time 0.351 (0.692)	Data 0.006 (0.030)	Loss 0.8823 (0.8594)	Acc@1 81.486 (80.988)	Acc@5 99.292 (98.940)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [26 | 180] LR: 0.100000
Epoch: [26][0/25]	Time 0.717 (0.717)	Data 0.684 (0.684)	Loss 0.8624 (0.8624)	Acc@1 80.322 (80.322)	Acc@5 98.633 (98.633)
Epoch: [26][1/25]	Time 0.730 (0.724)	Data 0.005 (0.344)	Loss 0.7807 (0.8215)	Acc@1 83.301 (81.812)	Acc@5 99.316 (98.975)
Epoch: [26][2/25]	Time 0.782 (0.743)	Data 0.005 (0.231)	Loss 0.8328 (0.8253)	Acc@1 82.275 (81.966)	Acc@5 99.414 (99.121)
Epoch: [26][3/25]	Time 0.696 (0.731)	Data 0.005 (0.175)	Loss 0.8744 (0.8376)	Acc@1 80.713 (81.653)	Acc@5 99.170 (99.133)
Epoch: [26][4/25]	Time 0.743 (0.733)	Data 0.007 (0.141)	Loss 0.8088 (0.8318)	Acc@1 82.812 (81.885)	Acc@5 99.365 (99.180)
Epoch: [26][5/25]	Time 0.754 (0.737)	Data 0.004 (0.118)	Loss 0.7921 (0.8252)	Acc@1 83.008 (82.072)	Acc@5 99.365 (99.211)
Epoch: [26][6/25]	Time 0.718 (0.734)	Data 0.005 (0.102)	Loss 0.8330 (0.8263)	Acc@1 83.008 (82.206)	Acc@5 98.633 (99.128)
Epoch: [26][7/25]	Time 0.686 (0.728)	Data 0.005 (0.090)	Loss 0.8405 (0.8281)	Acc@1 81.006 (82.056)	Acc@5 99.316 (99.152)
Epoch: [26][8/25]	Time 0.760 (0.732)	Data 0.006 (0.081)	Loss 0.8051 (0.8255)	Acc@1 81.885 (82.037)	Acc@5 99.316 (99.170)
Epoch: [26][9/25]	Time 0.729 (0.731)	Data 0.004 (0.073)	Loss 0.8190 (0.8249)	Acc@1 82.471 (82.080)	Acc@5 99.170 (99.170)
Epoch: [26][10/25]	Time 0.771 (0.735)	Data 0.005 (0.067)	Loss 0.8195 (0.8244)	Acc@1 81.348 (82.013)	Acc@5 99.121 (99.165)
Epoch: [26][11/25]	Time 0.687 (0.731)	Data 0.007 (0.062)	Loss 0.8353 (0.8253)	Acc@1 80.322 (81.873)	Acc@5 99.365 (99.182)
Epoch: [26][12/25]	Time 0.672 (0.726)	Data 0.004 (0.057)	Loss 0.8280 (0.8255)	Acc@1 81.982 (81.881)	Acc@5 98.975 (99.166)
Epoch: [26][13/25]	Time 0.712 (0.725)	Data 0.006 (0.054)	Loss 0.7971 (0.8235)	Acc@1 83.252 (81.979)	Acc@5 99.219 (99.170)
Epoch: [26][14/25]	Time 0.678 (0.722)	Data 0.010 (0.051)	Loss 0.8409 (0.8246)	Acc@1 82.080 (81.986)	Acc@5 99.121 (99.167)
Epoch: [26][15/25]	Time 0.622 (0.716)	Data 0.005 (0.048)	Loss 0.8045 (0.8234)	Acc@1 83.008 (82.050)	Acc@5 99.023 (99.158)
Epoch: [26][16/25]	Time 0.684 (0.714)	Data 0.008 (0.046)	Loss 0.8441 (0.8246)	Acc@1 80.762 (81.974)	Acc@5 99.170 (99.158)
Epoch: [26][17/25]	Time 0.630 (0.709)	Data 0.005 (0.043)	Loss 0.8386 (0.8254)	Acc@1 81.152 (81.928)	Acc@5 99.023 (99.151)
Epoch: [26][18/25]	Time 0.646 (0.706)	Data 0.005 (0.041)	Loss 0.8248 (0.8253)	Acc@1 81.982 (81.931)	Acc@5 99.023 (99.144)
Epoch: [26][19/25]	Time 0.659 (0.704)	Data 0.004 (0.039)	Loss 0.7878 (0.8235)	Acc@1 83.301 (82.000)	Acc@5 99.219 (99.148)
Epoch: [26][20/25]	Time 0.654 (0.701)	Data 0.004 (0.038)	Loss 0.8348 (0.8240)	Acc@1 81.299 (81.966)	Acc@5 99.023 (99.142)
Epoch: [26][21/25]	Time 0.649 (0.699)	Data 0.006 (0.036)	Loss 0.8372 (0.8246)	Acc@1 81.396 (81.940)	Acc@5 98.682 (99.121)
Epoch: [26][22/25]	Time 0.654 (0.697)	Data 0.005 (0.035)	Loss 0.8195 (0.8244)	Acc@1 82.227 (81.953)	Acc@5 99.316 (99.130)
Epoch: [26][23/25]	Time 0.657 (0.695)	Data 0.004 (0.034)	Loss 0.7921 (0.8230)	Acc@1 83.008 (81.997)	Acc@5 98.975 (99.123)
Epoch: [26][24/25]	Time 0.354 (0.682)	Data 0.004 (0.032)	Loss 0.8451 (0.8234)	Acc@1 81.250 (81.984)	Acc@5 99.292 (99.126)

Epoch: [27 | 180] LR: 0.100000
Epoch: [27][0/25]	Time 0.735 (0.735)	Data 0.756 (0.756)	Loss 0.8190 (0.8190)	Acc@1 81.689 (81.689)	Acc@5 99.219 (99.219)
Epoch: [27][1/25]	Time 0.739 (0.737)	Data 0.005 (0.380)	Loss 0.8254 (0.8222)	Acc@1 81.885 (81.787)	Acc@5 98.926 (99.072)
Epoch: [27][2/25]	Time 0.746 (0.740)	Data 0.008 (0.256)	Loss 0.8089 (0.8178)	Acc@1 81.543 (81.706)	Acc@5 99.268 (99.137)
Epoch: [27][3/25]	Time 0.724 (0.736)	Data 0.004 (0.193)	Loss 0.8127 (0.8165)	Acc@1 81.934 (81.763)	Acc@5 99.170 (99.146)
Epoch: [27][4/25]	Time 0.729 (0.735)	Data 0.007 (0.156)	Loss 0.8332 (0.8199)	Acc@1 82.031 (81.816)	Acc@5 98.926 (99.102)
Epoch: [27][5/25]	Time 0.729 (0.734)	Data 0.004 (0.131)	Loss 0.8142 (0.8189)	Acc@1 82.568 (81.942)	Acc@5 99.268 (99.129)
Epoch: [27][6/25]	Time 0.761 (0.738)	Data 0.007 (0.113)	Loss 0.8194 (0.8190)	Acc@1 82.568 (82.031)	Acc@5 99.072 (99.121)
Epoch: [27][7/25]	Time 0.738 (0.738)	Data 0.003 (0.099)	Loss 0.7813 (0.8143)	Acc@1 83.154 (82.172)	Acc@5 99.316 (99.146)
Epoch: [27][8/25]	Time 0.696 (0.733)	Data 0.005 (0.089)	Loss 0.8316 (0.8162)	Acc@1 81.201 (82.064)	Acc@5 99.023 (99.132)
Epoch: [27][9/25]	Time 0.787 (0.738)	Data 0.010 (0.081)	Loss 0.8071 (0.8153)	Acc@1 82.129 (82.070)	Acc@5 98.779 (99.097)
Epoch: [27][10/25]	Time 0.795 (0.744)	Data 0.008 (0.074)	Loss 0.8234 (0.8160)	Acc@1 81.250 (81.996)	Acc@5 98.926 (99.081)
Epoch: [27][11/25]	Time 0.717 (0.741)	Data 0.008 (0.069)	Loss 0.8329 (0.8174)	Acc@1 81.592 (81.962)	Acc@5 99.316 (99.101)
Epoch: [27][12/25]	Time 0.752 (0.742)	Data 0.007 (0.064)	Loss 0.8382 (0.8190)	Acc@1 81.836 (81.952)	Acc@5 99.268 (99.114)
Epoch: [27][13/25]	Time 0.715 (0.740)	Data 0.005 (0.060)	Loss 0.7904 (0.8170)	Acc@1 83.545 (82.066)	Acc@5 99.512 (99.142)
Epoch: [27][14/25]	Time 0.711 (0.738)	Data 0.005 (0.056)	Loss 0.8032 (0.8161)	Acc@1 82.471 (82.093)	Acc@5 99.414 (99.160)
Epoch: [27][15/25]	Time 0.768 (0.740)	Data 0.005 (0.053)	Loss 0.8343 (0.8172)	Acc@1 81.006 (82.025)	Acc@5 99.316 (99.170)
Epoch: [27][16/25]	Time 0.822 (0.745)	Data 0.007 (0.050)	Loss 0.8628 (0.8199)	Acc@1 80.273 (81.922)	Acc@5 99.121 (99.167)
Epoch: [27][17/25]	Time 0.881 (0.753)	Data 0.005 (0.048)	Loss 0.8404 (0.8210)	Acc@1 80.762 (81.858)	Acc@5 99.023 (99.159)
Epoch: [27][18/25]	Time 0.792 (0.755)	Data 0.006 (0.046)	Loss 0.7969 (0.8198)	Acc@1 82.715 (81.903)	Acc@5 99.365 (99.170)
Epoch: [27][19/25]	Time 0.737 (0.754)	Data 0.007 (0.044)	Loss 0.8539 (0.8215)	Acc@1 80.664 (81.841)	Acc@5 98.779 (99.150)
Epoch: [27][20/25]	Time 0.711 (0.752)	Data 0.007 (0.042)	Loss 0.8692 (0.8237)	Acc@1 80.029 (81.755)	Acc@5 98.877 (99.137)
Epoch: [27][21/25]	Time 0.726 (0.750)	Data 0.005 (0.040)	Loss 0.8033 (0.8228)	Acc@1 82.910 (81.807)	Acc@5 99.072 (99.134)
Epoch: [27][22/25]	Time 0.760 (0.751)	Data 0.007 (0.039)	Loss 0.8438 (0.8237)	Acc@1 81.299 (81.785)	Acc@5 99.121 (99.134)
Epoch: [27][23/25]	Time 0.704 (0.749)	Data 0.004 (0.037)	Loss 0.8288 (0.8239)	Acc@1 81.885 (81.789)	Acc@5 98.926 (99.125)
Epoch: [27][24/25]	Time 0.409 (0.735)	Data 0.007 (0.036)	Loss 0.8224 (0.8239)	Acc@1 81.840 (81.790)	Acc@5 99.410 (99.130)

Epoch: [28 | 180] LR: 0.100000
Epoch: [28][0/25]	Time 0.842 (0.842)	Data 0.711 (0.711)	Loss 0.8397 (0.8397)	Acc@1 80.859 (80.859)	Acc@5 99.121 (99.121)
Epoch: [28][1/25]	Time 0.787 (0.815)	Data 0.003 (0.357)	Loss 0.8074 (0.8235)	Acc@1 82.031 (81.445)	Acc@5 99.512 (99.316)
Epoch: [28][2/25]	Time 0.789 (0.806)	Data 0.006 (0.240)	Loss 0.8767 (0.8412)	Acc@1 79.688 (80.859)	Acc@5 98.779 (99.137)
Epoch: [28][3/25]	Time 0.755 (0.794)	Data 0.005 (0.182)	Loss 0.8130 (0.8342)	Acc@1 82.275 (81.213)	Acc@5 98.877 (99.072)
Epoch: [28][4/25]	Time 0.766 (0.788)	Data 0.006 (0.147)	Loss 0.7924 (0.8258)	Acc@1 81.982 (81.367)	Acc@5 99.609 (99.180)
Epoch: [28][5/25]	Time 0.730 (0.778)	Data 0.007 (0.123)	Loss 0.8466 (0.8293)	Acc@1 81.201 (81.340)	Acc@5 98.682 (99.097)
Epoch: [28][6/25]	Time 0.755 (0.775)	Data 0.006 (0.107)	Loss 0.7710 (0.8210)	Acc@1 82.764 (81.543)	Acc@5 99.414 (99.142)
Epoch: [28][7/25]	Time 0.764 (0.774)	Data 0.006 (0.094)	Loss 0.8302 (0.8221)	Acc@1 82.227 (81.628)	Acc@5 98.828 (99.103)
Epoch: [28][8/25]	Time 0.677 (0.763)	Data 0.005 (0.084)	Loss 0.8264 (0.8226)	Acc@1 82.275 (81.700)	Acc@5 98.877 (99.078)
Epoch: [28][9/25]	Time 0.641 (0.751)	Data 0.007 (0.076)	Loss 0.8238 (0.8227)	Acc@1 81.885 (81.719)	Acc@5 98.926 (99.062)
Epoch: [28][10/25]	Time 0.689 (0.745)	Data 0.008 (0.070)	Loss 0.7891 (0.8197)	Acc@1 82.617 (81.800)	Acc@5 99.219 (99.077)
Epoch: [28][11/25]	Time 0.684 (0.740)	Data 0.005 (0.065)	Loss 0.8046 (0.8184)	Acc@1 83.301 (81.925)	Acc@5 99.072 (99.076)
Epoch: [28][12/25]	Time 0.783 (0.743)	Data 0.007 (0.060)	Loss 0.7689 (0.8146)	Acc@1 83.789 (82.069)	Acc@5 99.414 (99.102)
Epoch: [28][13/25]	Time 0.760 (0.744)	Data 0.006 (0.056)	Loss 0.7835 (0.8124)	Acc@1 82.373 (82.091)	Acc@5 99.219 (99.111)
Epoch: [28][14/25]	Time 0.710 (0.742)	Data 0.004 (0.053)	Loss 0.8113 (0.8123)	Acc@1 81.836 (82.074)	Acc@5 99.023 (99.105)
Epoch: [28][15/25]	Time 0.702 (0.740)	Data 0.005 (0.050)	Loss 0.8370 (0.8138)	Acc@1 81.396 (82.031)	Acc@5 98.682 (99.078)
Epoch: [28][16/25]	Time 0.673 (0.736)	Data 0.005 (0.047)	Loss 0.8522 (0.8161)	Acc@1 81.396 (81.994)	Acc@5 99.219 (99.087)
Epoch: [28][17/25]	Time 0.765 (0.737)	Data 0.004 (0.045)	Loss 0.8087 (0.8157)	Acc@1 81.787 (81.982)	Acc@5 98.828 (99.072)
Epoch: [28][18/25]	Time 0.747 (0.738)	Data 0.006 (0.043)	Loss 0.7729 (0.8134)	Acc@1 83.545 (82.065)	Acc@5 99.072 (99.072)
Epoch: [28][19/25]	Time 0.711 (0.737)	Data 0.007 (0.041)	Loss 0.7949 (0.8125)	Acc@1 82.422 (82.083)	Acc@5 98.926 (99.065)
Epoch: [28][20/25]	Time 0.665 (0.733)	Data 0.006 (0.039)	Loss 0.7907 (0.8115)	Acc@1 82.959 (82.124)	Acc@5 99.268 (99.075)
Epoch: [28][21/25]	Time 0.677 (0.731)	Data 0.007 (0.038)	Loss 0.7624 (0.8092)	Acc@1 83.350 (82.180)	Acc@5 99.512 (99.094)
Epoch: [28][22/25]	Time 0.784 (0.733)	Data 0.007 (0.037)	Loss 0.7980 (0.8087)	Acc@1 82.666 (82.201)	Acc@5 99.414 (99.108)
Epoch: [28][23/25]	Time 0.697 (0.731)	Data 0.006 (0.035)	Loss 0.8118 (0.8089)	Acc@1 81.787 (82.184)	Acc@5 99.268 (99.115)
Epoch: [28][24/25]	Time 0.372 (0.717)	Data 0.007 (0.034)	Loss 0.8307 (0.8092)	Acc@1 81.014 (82.164)	Acc@5 99.410 (99.120)

Epoch: [29 | 180] LR: 0.100000
Epoch: [29][0/25]	Time 0.756 (0.756)	Data 0.638 (0.638)	Loss 0.7955 (0.7955)	Acc@1 83.105 (83.105)	Acc@5 98.779 (98.779)
Epoch: [29][1/25]	Time 0.716 (0.736)	Data 0.005 (0.321)	Loss 0.7826 (0.7890)	Acc@1 83.789 (83.447)	Acc@5 99.268 (99.023)
Epoch: [29][2/25]	Time 0.726 (0.733)	Data 0.010 (0.218)	Loss 0.8070 (0.7950)	Acc@1 82.422 (83.105)	Acc@5 99.023 (99.023)
Epoch: [29][3/25]	Time 0.692 (0.722)	Data 0.007 (0.165)	Loss 0.8194 (0.8011)	Acc@1 81.543 (82.715)	Acc@5 99.268 (99.084)
Epoch: [29][4/25]	Time 0.733 (0.724)	Data 0.004 (0.133)	Loss 0.7905 (0.7990)	Acc@1 83.301 (82.832)	Acc@5 99.170 (99.102)
Epoch: [29][5/25]	Time 0.742 (0.727)	Data 0.008 (0.112)	Loss 0.7636 (0.7931)	Acc@1 84.619 (83.130)	Acc@5 99.170 (99.113)
Epoch: [29][6/25]	Time 0.777 (0.734)	Data 0.005 (0.097)	Loss 0.8008 (0.7942)	Acc@1 82.568 (83.050)	Acc@5 98.730 (99.058)
Epoch: [29][7/25]	Time 0.712 (0.732)	Data 0.008 (0.086)	Loss 0.7833 (0.7928)	Acc@1 83.398 (83.093)	Acc@5 99.023 (99.054)
Epoch: [29][8/25]	Time 0.756 (0.734)	Data 0.008 (0.077)	Loss 0.8064 (0.7943)	Acc@1 82.520 (83.030)	Acc@5 99.121 (99.061)
Epoch: [29][9/25]	Time 0.775 (0.738)	Data 0.005 (0.070)	Loss 0.7627 (0.7912)	Acc@1 83.740 (83.101)	Acc@5 98.926 (99.048)
Epoch: [29][10/25]	Time 0.702 (0.735)	Data 0.005 (0.064)	Loss 0.8090 (0.7928)	Acc@1 82.275 (83.026)	Acc@5 99.268 (99.068)
Epoch: [29][11/25]	Time 0.741 (0.736)	Data 0.005 (0.059)	Loss 0.8096 (0.7942)	Acc@1 82.568 (82.987)	Acc@5 99.170 (99.076)
Epoch: [29][12/25]	Time 0.756 (0.737)	Data 0.006 (0.055)	Loss 0.7839 (0.7934)	Acc@1 84.180 (83.079)	Acc@5 98.779 (99.053)
Epoch: [29][13/25]	Time 0.733 (0.737)	Data 0.005 (0.051)	Loss 0.8302 (0.7960)	Acc@1 81.934 (82.997)	Acc@5 99.023 (99.051)
Epoch: [29][14/25]	Time 0.631 (0.730)	Data 0.007 (0.048)	Loss 0.7895 (0.7956)	Acc@1 83.057 (83.001)	Acc@5 99.463 (99.079)
Epoch: [29][15/25]	Time 0.644 (0.724)	Data 0.006 (0.046)	Loss 0.8069 (0.7963)	Acc@1 82.471 (82.968)	Acc@5 99.365 (99.097)
Epoch: [29][16/25]	Time 0.660 (0.721)	Data 0.005 (0.043)	Loss 0.8049 (0.7968)	Acc@1 83.105 (82.976)	Acc@5 99.023 (99.092)
Epoch: [29][17/25]	Time 0.646 (0.717)	Data 0.006 (0.041)	Loss 0.8301 (0.7987)	Acc@1 82.178 (82.932)	Acc@5 98.975 (99.086)
Epoch: [29][18/25]	Time 0.643 (0.713)	Data 0.007 (0.039)	Loss 0.7814 (0.7978)	Acc@1 83.984 (82.987)	Acc@5 99.414 (99.103)
Epoch: [29][19/25]	Time 0.653 (0.710)	Data 0.004 (0.038)	Loss 0.8007 (0.7979)	Acc@1 82.227 (82.949)	Acc@5 99.316 (99.114)
Epoch: [29][20/25]	Time 0.637 (0.706)	Data 0.008 (0.036)	Loss 0.7892 (0.7975)	Acc@1 82.617 (82.933)	Acc@5 99.219 (99.119)
Epoch: [29][21/25]	Time 0.647 (0.704)	Data 0.005 (0.035)	Loss 0.7877 (0.7970)	Acc@1 82.373 (82.908)	Acc@5 99.463 (99.134)
Epoch: [29][22/25]	Time 0.643 (0.701)	Data 0.004 (0.034)	Loss 0.7971 (0.7970)	Acc@1 82.471 (82.889)	Acc@5 99.170 (99.136)
Epoch: [29][23/25]	Time 0.670 (0.700)	Data 0.008 (0.032)	Loss 0.8108 (0.7976)	Acc@1 81.738 (82.841)	Acc@5 98.877 (99.125)
Epoch: [29][24/25]	Time 0.410 (0.688)	Data 0.007 (0.031)	Loss 0.7686 (0.7971)	Acc@1 83.726 (82.856)	Acc@5 98.939 (99.122)

Epoch: [30 | 180] LR: 0.100000
Epoch: [30][0/25]	Time 0.795 (0.795)	Data 0.740 (0.740)	Loss 0.7679 (0.7679)	Acc@1 83.984 (83.984)	Acc@5 99.072 (99.072)
Epoch: [30][1/25]	Time 0.714 (0.754)	Data 0.004 (0.372)	Loss 0.7893 (0.7786)	Acc@1 83.057 (83.521)	Acc@5 99.170 (99.121)
Epoch: [30][2/25]	Time 0.727 (0.745)	Data 0.007 (0.250)	Loss 0.7720 (0.7764)	Acc@1 84.082 (83.708)	Acc@5 99.023 (99.089)
Epoch: [30][3/25]	Time 0.770 (0.751)	Data 0.004 (0.189)	Loss 0.7600 (0.7723)	Acc@1 84.375 (83.875)	Acc@5 99.023 (99.072)
Epoch: [30][4/25]	Time 0.714 (0.744)	Data 0.007 (0.152)	Loss 0.7971 (0.7773)	Acc@1 82.812 (83.662)	Acc@5 99.219 (99.102)
Epoch: [30][5/25]	Time 0.745 (0.744)	Data 0.003 (0.127)	Loss 0.7798 (0.7777)	Acc@1 83.887 (83.700)	Acc@5 99.023 (99.089)
Epoch: [30][6/25]	Time 0.766 (0.747)	Data 0.008 (0.110)	Loss 0.7539 (0.7743)	Acc@1 84.375 (83.796)	Acc@5 99.268 (99.114)
Epoch: [30][7/25]	Time 0.697 (0.741)	Data 0.005 (0.097)	Loss 0.7720 (0.7740)	Acc@1 83.105 (83.710)	Acc@5 99.268 (99.133)
Epoch: [30][8/25]	Time 0.664 (0.732)	Data 0.008 (0.087)	Loss 0.7854 (0.7753)	Acc@1 83.789 (83.719)	Acc@5 99.609 (99.186)
Epoch: [30][9/25]	Time 0.664 (0.725)	Data 0.005 (0.079)	Loss 0.7859 (0.7763)	Acc@1 82.178 (83.564)	Acc@5 98.926 (99.160)
Epoch: [30][10/25]	Time 0.636 (0.717)	Data 0.008 (0.072)	Loss 0.7707 (0.7758)	Acc@1 82.617 (83.478)	Acc@5 99.316 (99.174)
Epoch: [30][11/25]	Time 0.665 (0.713)	Data 0.005 (0.067)	Loss 0.7562 (0.7742)	Acc@1 84.277 (83.545)	Acc@5 99.512 (99.202)
Epoch: [30][12/25]	Time 0.690 (0.711)	Data 0.007 (0.062)	Loss 0.8182 (0.7776)	Acc@1 80.957 (83.346)	Acc@5 99.219 (99.204)
Epoch: [30][13/25]	Time 0.745 (0.714)	Data 0.007 (0.058)	Loss 0.7726 (0.7772)	Acc@1 82.861 (83.311)	Acc@5 99.512 (99.226)
Epoch: [30][14/25]	Time 0.711 (0.713)	Data 0.007 (0.055)	Loss 0.7916 (0.7782)	Acc@1 82.764 (83.275)	Acc@5 99.023 (99.212)
Epoch: [30][15/25]	Time 0.710 (0.713)	Data 0.008 (0.052)	Loss 0.7661 (0.7774)	Acc@1 82.861 (83.249)	Acc@5 99.316 (99.219)
Epoch: [30][16/25]	Time 0.720 (0.714)	Data 0.008 (0.049)	Loss 0.7772 (0.7774)	Acc@1 83.301 (83.252)	Acc@5 98.730 (99.190)
Epoch: [30][17/25]	Time 0.769 (0.717)	Data 0.006 (0.047)	Loss 0.7587 (0.7764)	Acc@1 83.643 (83.274)	Acc@5 99.414 (99.202)
Epoch: [30][18/25]	Time 0.721 (0.717)	Data 0.005 (0.045)	Loss 0.7960 (0.7774)	Acc@1 82.275 (83.221)	Acc@5 99.268 (99.206)
Epoch: [30][19/25]	Time 0.721 (0.717)	Data 0.005 (0.043)	Loss 0.7954 (0.7783)	Acc@1 82.568 (83.188)	Acc@5 99.512 (99.221)
Epoch: [30][20/25]	Time 0.749 (0.719)	Data 0.007 (0.041)	Loss 0.7627 (0.7775)	Acc@1 84.424 (83.247)	Acc@5 99.219 (99.221)
Epoch: [30][21/25]	Time 0.736 (0.719)	Data 0.006 (0.039)	Loss 0.8079 (0.7789)	Acc@1 82.080 (83.194)	Acc@5 98.877 (99.205)
Epoch: [30][22/25]	Time 0.750 (0.721)	Data 0.004 (0.038)	Loss 0.7947 (0.7796)	Acc@1 82.080 (83.146)	Acc@5 98.730 (99.185)
Epoch: [30][23/25]	Time 0.796 (0.724)	Data 0.007 (0.037)	Loss 0.7771 (0.7795)	Acc@1 83.838 (83.175)	Acc@5 98.828 (99.170)
Epoch: [30][24/25]	Time 0.415 (0.712)	Data 0.007 (0.035)	Loss 0.7722 (0.7794)	Acc@1 84.788 (83.202)	Acc@5 99.528 (99.176)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [31 | 180] LR: 0.100000
Epoch: [31][0/25]	Time 0.760 (0.760)	Data 0.596 (0.596)	Loss 0.7581 (0.7581)	Acc@1 84.033 (84.033)	Acc@5 99.219 (99.219)
Epoch: [31][1/25]	Time 0.690 (0.725)	Data 0.008 (0.302)	Loss 0.6674 (0.7128)	Acc@1 86.523 (85.278)	Acc@5 99.658 (99.438)
Epoch: [31][2/25]	Time 0.743 (0.731)	Data 0.006 (0.203)	Loss 0.7339 (0.7198)	Acc@1 85.156 (85.238)	Acc@5 99.121 (99.333)
Epoch: [31][3/25]	Time 0.784 (0.744)	Data 0.006 (0.154)	Loss 0.7138 (0.7183)	Acc@1 85.449 (85.291)	Acc@5 99.609 (99.402)
Epoch: [31][4/25]	Time 0.673 (0.730)	Data 0.005 (0.124)	Loss 0.6976 (0.7142)	Acc@1 86.914 (85.615)	Acc@5 98.877 (99.297)
Epoch: [31][5/25]	Time 0.667 (0.719)	Data 0.005 (0.104)	Loss 0.6920 (0.7105)	Acc@1 86.035 (85.685)	Acc@5 99.609 (99.349)
Epoch: [31][6/25]	Time 0.662 (0.711)	Data 0.007 (0.090)	Loss 0.7124 (0.7107)	Acc@1 85.010 (85.589)	Acc@5 99.414 (99.358)
Epoch: [31][7/25]	Time 0.673 (0.707)	Data 0.003 (0.079)	Loss 0.7155 (0.7113)	Acc@1 85.400 (85.565)	Acc@5 99.365 (99.359)
Epoch: [31][8/25]	Time 0.715 (0.707)	Data 0.005 (0.071)	Loss 0.7440 (0.7150)	Acc@1 84.473 (85.444)	Acc@5 99.414 (99.365)
Epoch: [31][9/25]	Time 0.757 (0.712)	Data 0.004 (0.064)	Loss 0.7366 (0.7171)	Acc@1 85.107 (85.410)	Acc@5 99.365 (99.365)
Epoch: [31][10/25]	Time 0.695 (0.711)	Data 0.006 (0.059)	Loss 0.6917 (0.7148)	Acc@1 86.084 (85.471)	Acc@5 99.512 (99.379)
Epoch: [31][11/25]	Time 0.653 (0.706)	Data 0.007 (0.055)	Loss 0.6974 (0.7134)	Acc@1 85.547 (85.478)	Acc@5 99.414 (99.382)
Epoch: [31][12/25]	Time 0.639 (0.701)	Data 0.007 (0.051)	Loss 0.7274 (0.7144)	Acc@1 85.400 (85.472)	Acc@5 99.463 (99.388)
Epoch: [31][13/25]	Time 0.609 (0.694)	Data 0.005 (0.048)	Loss 0.7491 (0.7169)	Acc@1 84.766 (85.421)	Acc@5 99.072 (99.365)
Epoch: [31][14/25]	Time 0.641 (0.691)	Data 0.004 (0.045)	Loss 0.7205 (0.7172)	Acc@1 85.254 (85.410)	Acc@5 99.365 (99.365)
Epoch: [31][15/25]	Time 0.641 (0.688)	Data 0.006 (0.042)	Loss 0.7198 (0.7173)	Acc@1 85.840 (85.437)	Acc@5 99.219 (99.356)
Epoch: [31][16/25]	Time 0.669 (0.687)	Data 0.007 (0.040)	Loss 0.7273 (0.7179)	Acc@1 85.059 (85.415)	Acc@5 99.219 (99.348)
Epoch: [31][17/25]	Time 0.715 (0.688)	Data 0.004 (0.038)	Loss 0.7146 (0.7177)	Acc@1 85.254 (85.406)	Acc@5 99.414 (99.352)
Epoch: [31][18/25]	Time 0.783 (0.693)	Data 0.005 (0.037)	Loss 0.7227 (0.7180)	Acc@1 85.352 (85.403)	Acc@5 99.170 (99.342)
Epoch: [31][19/25]	Time 0.724 (0.695)	Data 0.006 (0.035)	Loss 0.7421 (0.7192)	Acc@1 83.936 (85.330)	Acc@5 99.072 (99.329)
Epoch: [31][20/25]	Time 0.715 (0.696)	Data 0.004 (0.034)	Loss 0.7414 (0.7202)	Acc@1 84.424 (85.286)	Acc@5 99.268 (99.326)
Epoch: [31][21/25]	Time 0.764 (0.699)	Data 0.004 (0.032)	Loss 0.6871 (0.7187)	Acc@1 86.328 (85.334)	Acc@5 99.561 (99.336)
Epoch: [31][22/25]	Time 0.698 (0.699)	Data 0.007 (0.031)	Loss 0.7464 (0.7199)	Acc@1 83.496 (85.254)	Acc@5 99.219 (99.331)
Epoch: [31][23/25]	Time 0.721 (0.700)	Data 0.007 (0.030)	Loss 0.7477 (0.7211)	Acc@1 83.447 (85.179)	Acc@5 99.268 (99.329)
Epoch: [31][24/25]	Time 0.439 (0.689)	Data 0.004 (0.029)	Loss 0.8004 (0.7224)	Acc@1 81.368 (85.114)	Acc@5 99.292 (99.328)

Epoch: [32 | 180] LR: 0.100000
Epoch: [32][0/25]	Time 0.774 (0.774)	Data 0.667 (0.667)	Loss 0.7330 (0.7330)	Acc@1 84.668 (84.668)	Acc@5 99.414 (99.414)
Epoch: [32][1/25]	Time 0.702 (0.738)	Data 0.006 (0.337)	Loss 0.7486 (0.7408)	Acc@1 84.814 (84.741)	Acc@5 99.170 (99.292)
Epoch: [32][2/25]	Time 0.752 (0.743)	Data 0.004 (0.226)	Loss 0.7496 (0.7438)	Acc@1 83.936 (84.473)	Acc@5 99.316 (99.300)
Epoch: [32][3/25]	Time 0.766 (0.749)	Data 0.007 (0.171)	Loss 0.7660 (0.7493)	Acc@1 83.447 (84.216)	Acc@5 99.170 (99.268)
Epoch: [32][4/25]	Time 0.696 (0.738)	Data 0.007 (0.138)	Loss 0.7844 (0.7563)	Acc@1 83.740 (84.121)	Acc@5 98.975 (99.209)
Epoch: [32][5/25]	Time 0.727 (0.736)	Data 0.009 (0.117)	Loss 0.7816 (0.7605)	Acc@1 81.494 (83.683)	Acc@5 99.121 (99.194)
Epoch: [32][6/25]	Time 0.756 (0.739)	Data 0.006 (0.101)	Loss 0.7664 (0.7614)	Acc@1 83.496 (83.657)	Acc@5 99.316 (99.212)
Epoch: [32][7/25]	Time 0.693 (0.733)	Data 0.007 (0.089)	Loss 0.7693 (0.7624)	Acc@1 83.594 (83.649)	Acc@5 99.170 (99.207)
Epoch: [32][8/25]	Time 0.674 (0.727)	Data 0.008 (0.080)	Loss 0.7571 (0.7618)	Acc@1 83.691 (83.653)	Acc@5 99.121 (99.197)
Epoch: [32][9/25]	Time 0.650 (0.719)	Data 0.007 (0.073)	Loss 0.7788 (0.7635)	Acc@1 82.373 (83.525)	Acc@5 99.316 (99.209)
Epoch: [32][10/25]	Time 0.679 (0.715)	Data 0.005 (0.067)	Loss 0.7502 (0.7623)	Acc@1 84.375 (83.603)	Acc@5 99.219 (99.210)
Epoch: [32][11/25]	Time 0.698 (0.714)	Data 0.009 (0.062)	Loss 0.7492 (0.7612)	Acc@1 84.082 (83.643)	Acc@5 99.219 (99.211)
Epoch: [32][12/25]	Time 0.741 (0.716)	Data 0.006 (0.057)	Loss 0.7616 (0.7612)	Acc@1 83.691 (83.646)	Acc@5 99.121 (99.204)
Epoch: [32][13/25]	Time 0.727 (0.717)	Data 0.004 (0.054)	Loss 0.7903 (0.7633)	Acc@1 82.764 (83.583)	Acc@5 98.926 (99.184)
Epoch: [32][14/25]	Time 0.721 (0.717)	Data 0.010 (0.051)	Loss 0.7900 (0.7651)	Acc@1 82.764 (83.529)	Acc@5 98.877 (99.163)
Epoch: [32][15/25]	Time 0.779 (0.721)	Data 0.008 (0.048)	Loss 0.7930 (0.7668)	Acc@1 82.373 (83.456)	Acc@5 99.072 (99.158)
Epoch: [32][16/25]	Time 0.707 (0.720)	Data 0.005 (0.046)	Loss 0.7412 (0.7653)	Acc@1 84.912 (83.542)	Acc@5 99.268 (99.164)
Epoch: [32][17/25]	Time 0.615 (0.714)	Data 0.007 (0.043)	Loss 0.7474 (0.7643)	Acc@1 85.010 (83.624)	Acc@5 99.268 (99.170)
Epoch: [32][18/25]	Time 0.672 (0.712)	Data 0.005 (0.041)	Loss 0.7529 (0.7637)	Acc@1 84.814 (83.686)	Acc@5 99.512 (99.188)
Epoch: [32][19/25]	Time 0.662 (0.709)	Data 0.005 (0.040)	Loss 0.7975 (0.7654)	Acc@1 83.105 (83.657)	Acc@5 98.877 (99.172)
Epoch: [32][20/25]	Time 0.698 (0.709)	Data 0.005 (0.038)	Loss 0.7305 (0.7637)	Acc@1 84.619 (83.703)	Acc@5 99.316 (99.179)
Epoch: [32][21/25]	Time 0.784 (0.712)	Data 0.004 (0.036)	Loss 0.7553 (0.7634)	Acc@1 83.447 (83.691)	Acc@5 99.268 (99.183)
Epoch: [32][22/25]	Time 0.715 (0.712)	Data 0.005 (0.035)	Loss 0.7527 (0.7629)	Acc@1 83.447 (83.681)	Acc@5 99.414 (99.193)
Epoch: [32][23/25]	Time 0.633 (0.709)	Data 0.007 (0.034)	Loss 0.7360 (0.7618)	Acc@1 84.326 (83.708)	Acc@5 99.072 (99.188)
Epoch: [32][24/25]	Time 0.362 (0.695)	Data 0.004 (0.033)	Loss 0.7793 (0.7621)	Acc@1 81.958 (83.678)	Acc@5 99.175 (99.188)

Epoch: [33 | 180] LR: 0.100000
Epoch: [33][0/25]	Time 0.719 (0.719)	Data 0.665 (0.665)	Loss 0.7711 (0.7711)	Acc@1 83.350 (83.350)	Acc@5 98.730 (98.730)
Epoch: [33][1/25]	Time 0.689 (0.704)	Data 0.006 (0.336)	Loss 0.7485 (0.7598)	Acc@1 84.521 (83.936)	Acc@5 99.414 (99.072)
Epoch: [33][2/25]	Time 0.786 (0.731)	Data 0.004 (0.225)	Loss 0.7475 (0.7557)	Acc@1 83.887 (83.919)	Acc@5 99.268 (99.137)
Epoch: [33][3/25]	Time 0.701 (0.724)	Data 0.005 (0.170)	Loss 0.7835 (0.7626)	Acc@1 83.154 (83.728)	Acc@5 99.365 (99.194)
Epoch: [33][4/25]	Time 0.631 (0.705)	Data 0.008 (0.138)	Loss 0.7263 (0.7554)	Acc@1 84.180 (83.818)	Acc@5 99.561 (99.268)
Epoch: [33][5/25]	Time 0.670 (0.699)	Data 0.006 (0.116)	Loss 0.7760 (0.7588)	Acc@1 83.447 (83.757)	Acc@5 98.779 (99.186)
Epoch: [33][6/25]	Time 0.680 (0.697)	Data 0.007 (0.100)	Loss 0.7532 (0.7580)	Acc@1 84.082 (83.803)	Acc@5 99.121 (99.177)
Epoch: [33][7/25]	Time 0.731 (0.701)	Data 0.007 (0.089)	Loss 0.7420 (0.7560)	Acc@1 84.619 (83.905)	Acc@5 99.414 (99.207)
Epoch: [33][8/25]	Time 0.754 (0.707)	Data 0.006 (0.079)	Loss 0.7776 (0.7584)	Acc@1 82.178 (83.713)	Acc@5 99.072 (99.192)
Epoch: [33][9/25]	Time 0.759 (0.712)	Data 0.008 (0.072)	Loss 0.7638 (0.7589)	Acc@1 82.812 (83.623)	Acc@5 99.170 (99.189)
Epoch: [33][10/25]	Time 0.693 (0.710)	Data 0.004 (0.066)	Loss 0.7295 (0.7563)	Acc@1 85.107 (83.758)	Acc@5 99.023 (99.174)
Epoch: [33][11/25]	Time 0.768 (0.715)	Data 0.004 (0.061)	Loss 0.7685 (0.7573)	Acc@1 83.545 (83.740)	Acc@5 99.219 (99.178)
Epoch: [33][12/25]	Time 0.683 (0.713)	Data 0.004 (0.057)	Loss 0.7508 (0.7568)	Acc@1 84.668 (83.812)	Acc@5 99.219 (99.181)
Epoch: [33][13/25]	Time 0.684 (0.710)	Data 0.005 (0.053)	Loss 0.7099 (0.7534)	Acc@1 85.205 (83.911)	Acc@5 99.463 (99.201)
Epoch: [33][14/25]	Time 0.683 (0.709)	Data 0.008 (0.050)	Loss 0.7306 (0.7519)	Acc@1 84.424 (83.945)	Acc@5 99.512 (99.222)
Epoch: [33][15/25]	Time 0.664 (0.706)	Data 0.008 (0.047)	Loss 0.7104 (0.7493)	Acc@1 85.498 (84.042)	Acc@5 99.414 (99.234)
Epoch: [33][16/25]	Time 0.675 (0.704)	Data 0.006 (0.045)	Loss 0.7125 (0.7472)	Acc@1 85.498 (84.128)	Acc@5 99.268 (99.236)
Epoch: [33][17/25]	Time 0.687 (0.703)	Data 0.004 (0.043)	Loss 0.7709 (0.7485)	Acc@1 82.324 (84.028)	Acc@5 99.463 (99.249)
Epoch: [33][18/25]	Time 0.739 (0.705)	Data 0.005 (0.041)	Loss 0.7653 (0.7494)	Acc@1 83.789 (84.015)	Acc@5 99.316 (99.252)
Epoch: [33][19/25]	Time 0.772 (0.708)	Data 0.006 (0.039)	Loss 0.7322 (0.7485)	Acc@1 85.400 (84.084)	Acc@5 99.365 (99.258)
Epoch: [33][20/25]	Time 0.731 (0.709)	Data 0.006 (0.037)	Loss 0.7304 (0.7476)	Acc@1 85.303 (84.142)	Acc@5 99.023 (99.247)
Epoch: [33][21/25]	Time 0.733 (0.711)	Data 0.007 (0.036)	Loss 0.7465 (0.7476)	Acc@1 84.033 (84.138)	Acc@5 99.023 (99.237)
Epoch: [33][22/25]	Time 0.797 (0.714)	Data 0.004 (0.035)	Loss 0.7848 (0.7492)	Acc@1 83.008 (84.088)	Acc@5 99.219 (99.236)
Epoch: [33][23/25]	Time 0.673 (0.713)	Data 0.005 (0.033)	Loss 0.7408 (0.7489)	Acc@1 83.984 (84.084)	Acc@5 99.414 (99.243)
Epoch: [33][24/25]	Time 0.368 (0.699)	Data 0.008 (0.032)	Loss 0.7013 (0.7481)	Acc@1 87.028 (84.134)	Acc@5 98.939 (99.238)

Epoch: [34 | 180] LR: 0.100000
Epoch: [34][0/25]	Time 0.777 (0.777)	Data 0.630 (0.630)	Loss 0.7615 (0.7615)	Acc@1 84.082 (84.082)	Acc@5 99.219 (99.219)
Epoch: [34][1/25]	Time 0.754 (0.765)	Data 0.009 (0.319)	Loss 0.7505 (0.7560)	Acc@1 83.594 (83.838)	Acc@5 99.316 (99.268)
Epoch: [34][2/25]	Time 0.740 (0.757)	Data 0.004 (0.214)	Loss 0.7289 (0.7470)	Acc@1 84.766 (84.147)	Acc@5 99.268 (99.268)
Epoch: [34][3/25]	Time 0.746 (0.754)	Data 0.004 (0.162)	Loss 0.7182 (0.7398)	Acc@1 85.352 (84.448)	Acc@5 99.365 (99.292)
Epoch: [34][4/25]	Time 0.769 (0.757)	Data 0.007 (0.131)	Loss 0.7411 (0.7400)	Acc@1 84.082 (84.375)	Acc@5 99.072 (99.248)
Epoch: [34][5/25]	Time 0.756 (0.757)	Data 0.005 (0.110)	Loss 0.6937 (0.7323)	Acc@1 85.596 (84.578)	Acc@5 99.268 (99.251)
Epoch: [34][6/25]	Time 0.729 (0.753)	Data 0.006 (0.095)	Loss 0.7039 (0.7283)	Acc@1 85.596 (84.724)	Acc@5 99.170 (99.240)
Epoch: [34][7/25]	Time 0.650 (0.740)	Data 0.003 (0.083)	Loss 0.7253 (0.7279)	Acc@1 85.352 (84.802)	Acc@5 99.609 (99.286)
Epoch: [34][8/25]	Time 0.659 (0.731)	Data 0.006 (0.075)	Loss 0.7026 (0.7251)	Acc@1 86.377 (84.977)	Acc@5 99.268 (99.284)
Epoch: [34][9/25]	Time 0.662 (0.724)	Data 0.004 (0.068)	Loss 0.7209 (0.7247)	Acc@1 85.547 (85.034)	Acc@5 99.414 (99.297)
Epoch: [34][10/25]	Time 0.630 (0.716)	Data 0.007 (0.062)	Loss 0.7369 (0.7258)	Acc@1 84.717 (85.005)	Acc@5 99.365 (99.303)
Epoch: [34][11/25]	Time 0.715 (0.716)	Data 0.006 (0.058)	Loss 0.7010 (0.7237)	Acc@1 85.791 (85.071)	Acc@5 99.365 (99.308)
Epoch: [34][12/25]	Time 0.793 (0.722)	Data 0.005 (0.054)	Loss 0.7225 (0.7236)	Acc@1 84.814 (85.051)	Acc@5 99.561 (99.328)
Epoch: [34][13/25]	Time 0.714 (0.721)	Data 0.005 (0.050)	Loss 0.7270 (0.7239)	Acc@1 85.449 (85.080)	Acc@5 99.219 (99.320)
Epoch: [34][14/25]	Time 0.669 (0.718)	Data 0.007 (0.047)	Loss 0.7338 (0.7245)	Acc@1 84.424 (85.036)	Acc@5 99.072 (99.303)
Epoch: [34][15/25]	Time 0.673 (0.715)	Data 0.006 (0.045)	Loss 0.7151 (0.7239)	Acc@1 84.814 (85.022)	Acc@5 99.707 (99.329)
Epoch: [34][16/25]	Time 0.722 (0.715)	Data 0.008 (0.043)	Loss 0.7043 (0.7228)	Acc@1 84.619 (84.998)	Acc@5 99.609 (99.345)
Epoch: [34][17/25]	Time 0.745 (0.717)	Data 0.005 (0.040)	Loss 0.7156 (0.7224)	Acc@1 86.035 (85.056)	Acc@5 99.316 (99.344)
Epoch: [34][18/25]	Time 0.694 (0.716)	Data 0.005 (0.039)	Loss 0.7224 (0.7224)	Acc@1 85.303 (85.069)	Acc@5 99.316 (99.342)
Epoch: [34][19/25]	Time 0.661 (0.713)	Data 0.004 (0.037)	Loss 0.7155 (0.7220)	Acc@1 85.547 (85.093)	Acc@5 99.219 (99.336)
Epoch: [34][20/25]	Time 0.645 (0.710)	Data 0.007 (0.035)	Loss 0.7373 (0.7228)	Acc@1 84.961 (85.086)	Acc@5 99.463 (99.342)
Epoch: [34][21/25]	Time 0.706 (0.710)	Data 0.004 (0.034)	Loss 0.7213 (0.7227)	Acc@1 84.912 (85.079)	Acc@5 99.414 (99.345)
Epoch: [34][22/25]	Time 0.752 (0.711)	Data 0.005 (0.033)	Loss 0.7764 (0.7250)	Acc@1 83.643 (85.016)	Acc@5 98.877 (99.325)
Epoch: [34][23/25]	Time 0.766 (0.714)	Data 0.005 (0.032)	Loss 0.7168 (0.7247)	Acc@1 85.547 (85.038)	Acc@5 99.414 (99.329)
Epoch: [34][24/25]	Time 0.423 (0.702)	Data 0.004 (0.031)	Loss 0.7702 (0.7255)	Acc@1 83.255 (85.008)	Acc@5 99.175 (99.326)

Epoch: [35 | 180] LR: 0.100000
Epoch: [35][0/25]	Time 0.757 (0.757)	Data 0.700 (0.700)	Loss 0.7377 (0.7377)	Acc@1 83.691 (83.691)	Acc@5 99.268 (99.268)
Epoch: [35][1/25]	Time 0.685 (0.721)	Data 0.007 (0.354)	Loss 0.7491 (0.7434)	Acc@1 83.594 (83.643)	Acc@5 99.512 (99.390)
Epoch: [35][2/25]	Time 0.682 (0.708)	Data 0.004 (0.237)	Loss 0.7164 (0.7344)	Acc@1 85.596 (84.294)	Acc@5 99.268 (99.349)
Epoch: [35][3/25]	Time 0.764 (0.722)	Data 0.005 (0.179)	Loss 0.7345 (0.7344)	Acc@1 85.107 (84.497)	Acc@5 99.365 (99.353)
Epoch: [35][4/25]	Time 0.757 (0.729)	Data 0.006 (0.145)	Loss 0.7320 (0.7340)	Acc@1 84.180 (84.434)	Acc@5 99.561 (99.395)
Epoch: [35][5/25]	Time 0.731 (0.729)	Data 0.005 (0.121)	Loss 0.7287 (0.7331)	Acc@1 84.424 (84.432)	Acc@5 99.414 (99.398)
Epoch: [35][6/25]	Time 0.751 (0.732)	Data 0.005 (0.105)	Loss 0.7251 (0.7319)	Acc@1 84.229 (84.403)	Acc@5 99.658 (99.435)
Epoch: [35][7/25]	Time 0.712 (0.730)	Data 0.006 (0.092)	Loss 0.7261 (0.7312)	Acc@1 84.473 (84.412)	Acc@5 99.414 (99.432)
Epoch: [35][8/25]	Time 0.728 (0.730)	Data 0.007 (0.083)	Loss 0.6818 (0.7257)	Acc@1 86.182 (84.608)	Acc@5 99.365 (99.425)
Epoch: [35][9/25]	Time 0.776 (0.734)	Data 0.006 (0.075)	Loss 0.6885 (0.7220)	Acc@1 85.645 (84.712)	Acc@5 99.316 (99.414)
Epoch: [35][10/25]	Time 0.681 (0.729)	Data 0.007 (0.069)	Loss 0.7330 (0.7230)	Acc@1 84.521 (84.695)	Acc@5 99.365 (99.410)
Epoch: [35][11/25]	Time 0.610 (0.719)	Data 0.005 (0.064)	Loss 0.7524 (0.7254)	Acc@1 84.473 (84.676)	Acc@5 98.975 (99.373)
Epoch: [35][12/25]	Time 0.669 (0.716)	Data 0.004 (0.059)	Loss 0.7230 (0.7253)	Acc@1 84.863 (84.691)	Acc@5 99.170 (99.358)
Epoch: [35][13/25]	Time 0.641 (0.710)	Data 0.008 (0.055)	Loss 0.7196 (0.7248)	Acc@1 84.961 (84.710)	Acc@5 99.121 (99.341)
Epoch: [35][14/25]	Time 0.680 (0.708)	Data 0.007 (0.052)	Loss 0.7266 (0.7250)	Acc@1 84.961 (84.727)	Acc@5 99.072 (99.323)
Epoch: [35][15/25]	Time 0.656 (0.705)	Data 0.007 (0.049)	Loss 0.7167 (0.7244)	Acc@1 85.986 (84.805)	Acc@5 99.512 (99.335)
Epoch: [35][16/25]	Time 0.667 (0.703)	Data 0.007 (0.047)	Loss 0.7069 (0.7234)	Acc@1 85.498 (84.846)	Acc@5 99.268 (99.331)
Epoch: [35][17/25]	Time 0.653 (0.700)	Data 0.008 (0.045)	Loss 0.7072 (0.7225)	Acc@1 85.986 (84.909)	Acc@5 99.316 (99.330)
Epoch: [35][18/25]	Time 0.675 (0.699)	Data 0.004 (0.043)	Loss 0.7170 (0.7222)	Acc@1 85.449 (84.938)	Acc@5 99.170 (99.322)
Epoch: [35][19/25]	Time 0.610 (0.694)	Data 0.005 (0.041)	Loss 0.7186 (0.7220)	Acc@1 85.596 (84.971)	Acc@5 99.512 (99.331)
Epoch: [35][20/25]	Time 0.664 (0.693)	Data 0.006 (0.039)	Loss 0.6918 (0.7206)	Acc@1 85.303 (84.987)	Acc@5 99.463 (99.337)
Epoch: [35][21/25]	Time 0.651 (0.691)	Data 0.007 (0.038)	Loss 0.7060 (0.7199)	Acc@1 86.182 (85.041)	Acc@5 99.512 (99.345)
Epoch: [35][22/25]	Time 0.636 (0.688)	Data 0.008 (0.036)	Loss 0.7232 (0.7201)	Acc@1 84.229 (85.006)	Acc@5 99.463 (99.350)
Epoch: [35][23/25]	Time 0.646 (0.687)	Data 0.005 (0.035)	Loss 0.7305 (0.7205)	Acc@1 84.717 (84.993)	Acc@5 99.268 (99.347)
Epoch: [35][24/25]	Time 0.368 (0.674)	Data 0.007 (0.034)	Loss 0.7065 (0.7203)	Acc@1 85.731 (85.006)	Acc@5 99.528 (99.350)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 484496 ; 487386 ; 0.9940704082595725

Epoch: [36 | 180] LR: 0.100000
Epoch: [36][0/25]	Time 0.743 (0.743)	Data 0.638 (0.638)	Loss 0.6942 (0.6942)	Acc@1 85.596 (85.596)	Acc@5 99.414 (99.414)
Epoch: [36][1/25]	Time 0.765 (0.754)	Data 0.005 (0.322)	Loss 0.6437 (0.6689)	Acc@1 87.451 (86.523)	Acc@5 99.512 (99.463)
Epoch: [36][2/25]	Time 0.696 (0.735)	Data 0.004 (0.216)	Loss 0.6823 (0.6734)	Acc@1 86.328 (86.458)	Acc@5 99.463 (99.463)
Epoch: [36][3/25]	Time 0.776 (0.745)	Data 0.006 (0.163)	Loss 0.6569 (0.6693)	Acc@1 87.207 (86.646)	Acc@5 99.658 (99.512)
Epoch: [36][4/25]	Time 0.761 (0.748)	Data 0.008 (0.132)	Loss 0.6458 (0.6646)	Acc@1 87.695 (86.855)	Acc@5 99.609 (99.531)
Epoch: [36][5/25]	Time 0.692 (0.739)	Data 0.005 (0.111)	Loss 0.6699 (0.6655)	Acc@1 86.768 (86.841)	Acc@5 99.707 (99.561)
Epoch: [36][6/25]	Time 0.619 (0.722)	Data 0.003 (0.096)	Loss 0.6434 (0.6623)	Acc@1 87.305 (86.907)	Acc@5 99.512 (99.554)
Epoch: [36][7/25]	Time 0.640 (0.712)	Data 0.006 (0.085)	Loss 0.6354 (0.6590)	Acc@1 88.184 (87.067)	Acc@5 99.756 (99.579)
Epoch: [36][8/25]	Time 0.628 (0.702)	Data 0.004 (0.076)	Loss 0.6694 (0.6601)	Acc@1 86.572 (87.012)	Acc@5 99.463 (99.566)
Epoch: [36][9/25]	Time 0.669 (0.699)	Data 0.007 (0.069)	Loss 0.6611 (0.6602)	Acc@1 86.768 (86.987)	Acc@5 99.512 (99.561)
Epoch: [36][10/25]	Time 0.666 (0.696)	Data 0.007 (0.063)	Loss 0.6849 (0.6625)	Acc@1 85.840 (86.883)	Acc@5 99.316 (99.538)
Epoch: [36][11/25]	Time 0.717 (0.698)	Data 0.005 (0.058)	Loss 0.6700 (0.6631)	Acc@1 87.012 (86.894)	Acc@5 99.512 (99.536)
Epoch: [36][12/25]	Time 0.734 (0.701)	Data 0.008 (0.054)	Loss 0.6546 (0.6624)	Acc@1 87.158 (86.914)	Acc@5 99.561 (99.538)
Epoch: [36][13/25]	Time 0.774 (0.706)	Data 0.004 (0.051)	Loss 0.6742 (0.6633)	Acc@1 85.938 (86.844)	Acc@5 99.512 (99.536)
Epoch: [36][14/25]	Time 0.694 (0.705)	Data 0.006 (0.048)	Loss 0.6880 (0.6649)	Acc@1 85.693 (86.768)	Acc@5 99.365 (99.525)
Epoch: [36][15/25]	Time 0.655 (0.702)	Data 0.005 (0.045)	Loss 0.6469 (0.6638)	Acc@1 87.695 (86.826)	Acc@5 99.414 (99.518)
Epoch: [36][16/25]	Time 0.620 (0.697)	Data 0.005 (0.043)	Loss 0.6386 (0.6623)	Acc@1 87.646 (86.874)	Acc@5 99.365 (99.509)
Epoch: [36][17/25]	Time 0.650 (0.695)	Data 0.007 (0.041)	Loss 0.6998 (0.6644)	Acc@1 86.084 (86.830)	Acc@5 99.365 (99.501)
Epoch: [36][18/25]	Time 0.677 (0.694)	Data 0.006 (0.039)	Loss 0.7124 (0.6669)	Acc@1 85.645 (86.768)	Acc@5 99.170 (99.483)
Epoch: [36][19/25]	Time 0.757 (0.697)	Data 0.006 (0.037)	Loss 0.6968 (0.6684)	Acc@1 85.986 (86.729)	Acc@5 99.268 (99.473)
Epoch: [36][20/25]	Time 0.786 (0.701)	Data 0.005 (0.036)	Loss 0.7086 (0.6703)	Acc@1 85.791 (86.684)	Acc@5 98.779 (99.440)
Epoch: [36][21/25]	Time 0.711 (0.702)	Data 0.005 (0.034)	Loss 0.6691 (0.6703)	Acc@1 85.840 (86.646)	Acc@5 99.609 (99.447)
Epoch: [36][22/25]	Time 0.686 (0.701)	Data 0.007 (0.033)	Loss 0.6843 (0.6709)	Acc@1 86.084 (86.621)	Acc@5 99.365 (99.444)
Epoch: [36][23/25]	Time 0.659 (0.699)	Data 0.008 (0.032)	Loss 0.7087 (0.6725)	Acc@1 85.303 (86.566)	Acc@5 99.365 (99.441)
Epoch: [36][24/25]	Time 0.420 (0.688)	Data 0.007 (0.031)	Loss 0.6554 (0.6722)	Acc@1 87.382 (86.580)	Acc@5 99.646 (99.444)

Epoch: [37 | 180] LR: 0.100000
Epoch: [37][0/25]	Time 0.762 (0.762)	Data 0.605 (0.605)	Loss 0.6948 (0.6948)	Acc@1 86.426 (86.426)	Acc@5 99.121 (99.121)
Epoch: [37][1/25]	Time 0.712 (0.737)	Data 0.008 (0.307)	Loss 0.7298 (0.7123)	Acc@1 83.887 (85.156)	Acc@5 99.316 (99.219)
Epoch: [37][2/25]	Time 0.765 (0.746)	Data 0.005 (0.206)	Loss 0.6847 (0.7031)	Acc@1 86.279 (85.531)	Acc@5 99.463 (99.300)
Epoch: [37][3/25]	Time 0.712 (0.738)	Data 0.007 (0.156)	Loss 0.7051 (0.7036)	Acc@1 85.645 (85.559)	Acc@5 99.365 (99.316)
Epoch: [37][4/25]	Time 0.758 (0.742)	Data 0.005 (0.126)	Loss 0.6971 (0.7023)	Acc@1 85.547 (85.557)	Acc@5 99.170 (99.287)
Epoch: [37][5/25]	Time 0.778 (0.748)	Data 0.005 (0.106)	Loss 0.7488 (0.7101)	Acc@1 84.033 (85.303)	Acc@5 98.975 (99.235)
Epoch: [37][6/25]	Time 0.871 (0.765)	Data 0.008 (0.092)	Loss 0.6832 (0.7062)	Acc@1 86.133 (85.421)	Acc@5 99.414 (99.261)
Epoch: [37][7/25]	Time 0.840 (0.775)	Data 0.006 (0.081)	Loss 0.7007 (0.7055)	Acc@1 85.010 (85.370)	Acc@5 99.561 (99.298)
Epoch: [37][8/25]	Time 0.742 (0.771)	Data 0.004 (0.073)	Loss 0.6445 (0.6988)	Acc@1 87.061 (85.558)	Acc@5 99.561 (99.327)
Epoch: [37][9/25]	Time 0.859 (0.780)	Data 0.006 (0.066)	Loss 0.7073 (0.6996)	Acc@1 85.010 (85.503)	Acc@5 99.609 (99.355)
Epoch: [37][10/25]	Time 0.928 (0.793)	Data 0.005 (0.060)	Loss 0.6832 (0.6981)	Acc@1 86.328 (85.578)	Acc@5 99.512 (99.370)
Epoch: [37][11/25]	Time 0.654 (0.782)	Data 0.007 (0.056)	Loss 0.7266 (0.7005)	Acc@1 84.717 (85.506)	Acc@5 99.512 (99.382)
Epoch: [37][12/25]	Time 0.660 (0.773)	Data 0.005 (0.052)	Loss 0.7390 (0.7035)	Acc@1 84.180 (85.404)	Acc@5 99.316 (99.377)
Epoch: [37][13/25]	Time 0.646 (0.763)	Data 0.009 (0.049)	Loss 0.6884 (0.7024)	Acc@1 85.596 (85.418)	Acc@5 99.316 (99.372)
Epoch: [37][14/25]	Time 0.816 (0.767)	Data 0.005 (0.046)	Loss 0.7003 (0.7022)	Acc@1 85.547 (85.426)	Acc@5 99.561 (99.385)
Epoch: [37][15/25]	Time 0.788 (0.768)	Data 0.005 (0.043)	Loss 0.7491 (0.7052)	Acc@1 84.473 (85.367)	Acc@5 99.121 (99.368)
Epoch: [37][16/25]	Time 0.688 (0.764)	Data 0.006 (0.041)	Loss 0.7347 (0.7069)	Acc@1 84.033 (85.288)	Acc@5 99.121 (99.354)
Epoch: [37][17/25]	Time 0.751 (0.763)	Data 0.005 (0.039)	Loss 0.7001 (0.7065)	Acc@1 85.352 (85.292)	Acc@5 99.414 (99.357)
Epoch: [37][18/25]	Time 0.780 (0.764)	Data 0.005 (0.037)	Loss 0.7111 (0.7068)	Acc@1 84.814 (85.267)	Acc@5 99.219 (99.350)
Epoch: [37][19/25]	Time 0.735 (0.762)	Data 0.007 (0.036)	Loss 0.7147 (0.7072)	Acc@1 85.156 (85.261)	Acc@5 99.512 (99.358)
Epoch: [37][20/25]	Time 0.763 (0.762)	Data 0.006 (0.034)	Loss 0.6724 (0.7055)	Acc@1 87.256 (85.356)	Acc@5 99.316 (99.356)
Epoch: [37][21/25]	Time 0.745 (0.762)	Data 0.004 (0.033)	Loss 0.6857 (0.7046)	Acc@1 85.889 (85.380)	Acc@5 99.463 (99.361)
Epoch: [37][22/25]	Time 0.707 (0.759)	Data 0.004 (0.032)	Loss 0.6675 (0.7030)	Acc@1 86.670 (85.436)	Acc@5 99.512 (99.367)
Epoch: [37][23/25]	Time 0.737 (0.758)	Data 0.004 (0.031)	Loss 0.6806 (0.7021)	Acc@1 86.914 (85.498)	Acc@5 99.268 (99.363)
Epoch: [37][24/25]	Time 0.420 (0.745)	Data 0.005 (0.030)	Loss 0.7152 (0.7023)	Acc@1 84.670 (85.484)	Acc@5 99.292 (99.362)

Epoch: [38 | 180] LR: 0.100000
Epoch: [38][0/25]	Time 0.760 (0.760)	Data 0.809 (0.809)	Loss 0.6843 (0.6843)	Acc@1 85.791 (85.791)	Acc@5 99.268 (99.268)
Epoch: [38][1/25]	Time 0.794 (0.777)	Data 0.004 (0.407)	Loss 0.6627 (0.6735)	Acc@1 87.158 (86.475)	Acc@5 99.756 (99.512)
Epoch: [38][2/25]	Time 0.706 (0.753)	Data 0.003 (0.272)	Loss 0.6909 (0.6793)	Acc@1 86.475 (86.475)	Acc@5 99.219 (99.414)
Epoch: [38][3/25]	Time 0.633 (0.723)	Data 0.006 (0.206)	Loss 0.6972 (0.6838)	Acc@1 85.791 (86.304)	Acc@5 99.609 (99.463)
Epoch: [38][4/25]	Time 0.646 (0.708)	Data 0.006 (0.166)	Loss 0.6691 (0.6808)	Acc@1 86.963 (86.436)	Acc@5 99.512 (99.473)
Epoch: [38][5/25]	Time 0.641 (0.697)	Data 0.004 (0.139)	Loss 0.6567 (0.6768)	Acc@1 87.305 (86.580)	Acc@5 99.414 (99.463)
Epoch: [38][6/25]	Time 0.620 (0.686)	Data 0.005 (0.120)	Loss 0.6940 (0.6793)	Acc@1 86.768 (86.607)	Acc@5 99.512 (99.470)
Epoch: [38][7/25]	Time 0.645 (0.681)	Data 0.005 (0.105)	Loss 0.7060 (0.6826)	Acc@1 84.814 (86.383)	Acc@5 99.512 (99.475)
Epoch: [38][8/25]	Time 0.698 (0.683)	Data 0.005 (0.094)	Loss 0.6970 (0.6842)	Acc@1 85.791 (86.317)	Acc@5 99.463 (99.474)
Epoch: [38][9/25]	Time 0.747 (0.689)	Data 0.006 (0.085)	Loss 0.6818 (0.6840)	Acc@1 86.377 (86.323)	Acc@5 99.707 (99.497)
Epoch: [38][10/25]	Time 0.781 (0.697)	Data 0.005 (0.078)	Loss 0.6862 (0.6842)	Acc@1 86.523 (86.341)	Acc@5 99.414 (99.490)
Epoch: [38][11/25]	Time 0.702 (0.698)	Data 0.006 (0.072)	Loss 0.6626 (0.6824)	Acc@1 86.670 (86.369)	Acc@5 99.561 (99.495)
Epoch: [38][12/25]	Time 0.675 (0.696)	Data 0.005 (0.067)	Loss 0.7090 (0.6844)	Acc@1 85.596 (86.309)	Acc@5 99.316 (99.482)
Epoch: [38][13/25]	Time 0.723 (0.698)	Data 0.005 (0.062)	Loss 0.7218 (0.6871)	Acc@1 85.107 (86.223)	Acc@5 99.121 (99.456)
Epoch: [38][14/25]	Time 0.734 (0.700)	Data 0.005 (0.059)	Loss 0.7527 (0.6915)	Acc@1 83.301 (86.029)	Acc@5 99.414 (99.453)
Epoch: [38][15/25]	Time 0.716 (0.701)	Data 0.005 (0.055)	Loss 0.6624 (0.6896)	Acc@1 87.256 (86.105)	Acc@5 99.365 (99.448)
Epoch: [38][16/25]	Time 0.727 (0.703)	Data 0.006 (0.052)	Loss 0.7187 (0.6914)	Acc@1 85.352 (86.061)	Acc@5 99.414 (99.446)
Epoch: [38][17/25]	Time 0.796 (0.708)	Data 0.007 (0.050)	Loss 0.6970 (0.6917)	Acc@1 85.547 (86.032)	Acc@5 99.316 (99.438)
Epoch: [38][18/25]	Time 0.722 (0.709)	Data 0.007 (0.048)	Loss 0.7180 (0.6931)	Acc@1 84.619 (85.958)	Acc@5 99.512 (99.442)
Epoch: [38][19/25]	Time 0.714 (0.709)	Data 0.004 (0.045)	Loss 0.7097 (0.6939)	Acc@1 85.986 (85.959)	Acc@5 98.975 (99.419)
Epoch: [38][20/25]	Time 0.761 (0.712)	Data 0.005 (0.044)	Loss 0.7077 (0.6945)	Acc@1 85.205 (85.924)	Acc@5 99.512 (99.423)
Epoch: [38][21/25]	Time 0.732 (0.712)	Data 0.007 (0.042)	Loss 0.7129 (0.6954)	Acc@1 83.936 (85.833)	Acc@5 99.365 (99.421)
Epoch: [38][22/25]	Time 0.702 (0.712)	Data 0.007 (0.040)	Loss 0.7033 (0.6957)	Acc@1 85.645 (85.825)	Acc@5 99.414 (99.420)
Epoch: [38][23/25]	Time 0.752 (0.714)	Data 0.007 (0.039)	Loss 0.7237 (0.6969)	Acc@1 84.473 (85.769)	Acc@5 99.316 (99.416)
Epoch: [38][24/25]	Time 0.436 (0.703)	Data 0.005 (0.038)	Loss 0.6719 (0.6965)	Acc@1 85.967 (85.772)	Acc@5 99.175 (99.412)

Epoch: [39 | 180] LR: 0.100000
Epoch: [39][0/25]	Time 0.758 (0.758)	Data 0.676 (0.676)	Loss 0.6971 (0.6971)	Acc@1 85.693 (85.693)	Acc@5 99.365 (99.365)
Epoch: [39][1/25]	Time 0.710 (0.734)	Data 0.004 (0.340)	Loss 0.7256 (0.7114)	Acc@1 84.668 (85.181)	Acc@5 99.561 (99.463)
Epoch: [39][2/25]	Time 0.701 (0.723)	Data 0.005 (0.228)	Loss 0.7091 (0.7106)	Acc@1 85.156 (85.173)	Acc@5 99.414 (99.447)
Epoch: [39][3/25]	Time 0.733 (0.726)	Data 0.007 (0.173)	Loss 0.7247 (0.7141)	Acc@1 84.668 (85.046)	Acc@5 99.512 (99.463)
Epoch: [39][4/25]	Time 0.774 (0.735)	Data 0.009 (0.140)	Loss 0.6829 (0.7079)	Acc@1 85.791 (85.195)	Acc@5 99.219 (99.414)
Epoch: [39][5/25]	Time 0.707 (0.731)	Data 0.004 (0.118)	Loss 0.7056 (0.7075)	Acc@1 85.059 (85.173)	Acc@5 99.512 (99.430)
Epoch: [39][6/25]	Time 0.729 (0.730)	Data 0.009 (0.102)	Loss 0.6982 (0.7062)	Acc@1 85.645 (85.240)	Acc@5 99.512 (99.442)
Epoch: [39][7/25]	Time 0.767 (0.735)	Data 0.008 (0.090)	Loss 0.6881 (0.7039)	Acc@1 85.547 (85.278)	Acc@5 99.561 (99.457)
Epoch: [39][8/25]	Time 0.697 (0.731)	Data 0.005 (0.081)	Loss 0.6782 (0.7011)	Acc@1 86.865 (85.455)	Acc@5 99.854 (99.501)
Epoch: [39][9/25]	Time 0.763 (0.734)	Data 0.007 (0.073)	Loss 0.7022 (0.7012)	Acc@1 85.059 (85.415)	Acc@5 99.463 (99.497)
Epoch: [39][10/25]	Time 0.759 (0.736)	Data 0.004 (0.067)	Loss 0.6669 (0.6981)	Acc@1 85.938 (85.463)	Acc@5 99.512 (99.498)
Epoch: [39][11/25]	Time 0.699 (0.733)	Data 0.005 (0.062)	Loss 0.7121 (0.6992)	Acc@1 85.400 (85.457)	Acc@5 99.121 (99.467)
Epoch: [39][12/25]	Time 0.716 (0.732)	Data 0.004 (0.057)	Loss 0.6925 (0.6987)	Acc@1 85.596 (85.468)	Acc@5 99.219 (99.448)
Epoch: [39][13/25]	Time 0.658 (0.726)	Data 0.005 (0.054)	Loss 0.6910 (0.6982)	Acc@1 86.475 (85.540)	Acc@5 99.268 (99.435)
Epoch: [39][14/25]	Time 0.666 (0.722)	Data 0.007 (0.051)	Loss 0.7103 (0.6990)	Acc@1 85.645 (85.547)	Acc@5 99.121 (99.414)
Epoch: [39][15/25]	Time 0.671 (0.719)	Data 0.007 (0.048)	Loss 0.6568 (0.6963)	Acc@1 86.377 (85.599)	Acc@5 99.463 (99.417)
Epoch: [39][16/25]	Time 0.718 (0.719)	Data 0.008 (0.046)	Loss 0.7010 (0.6966)	Acc@1 85.547 (85.596)	Acc@5 99.365 (99.414)
Epoch: [39][17/25]	Time 0.768 (0.722)	Data 0.005 (0.043)	Loss 0.7188 (0.6979)	Acc@1 85.547 (85.593)	Acc@5 99.170 (99.400)
Epoch: [39][18/25]	Time 0.699 (0.721)	Data 0.007 (0.041)	Loss 0.7181 (0.6989)	Acc@1 84.912 (85.557)	Acc@5 99.365 (99.399)
Epoch: [39][19/25]	Time 0.717 (0.720)	Data 0.005 (0.040)	Loss 0.7034 (0.6991)	Acc@1 85.840 (85.571)	Acc@5 99.365 (99.397)
Epoch: [39][20/25]	Time 0.666 (0.718)	Data 0.008 (0.038)	Loss 0.6815 (0.6983)	Acc@1 85.742 (85.579)	Acc@5 99.512 (99.402)
Epoch: [39][21/25]	Time 0.743 (0.719)	Data 0.009 (0.037)	Loss 0.7030 (0.6985)	Acc@1 85.254 (85.565)	Acc@5 99.219 (99.394)
Epoch: [39][22/25]	Time 0.804 (0.723)	Data 0.005 (0.035)	Loss 0.6857 (0.6980)	Acc@1 85.107 (85.545)	Acc@5 99.609 (99.403)
Epoch: [39][23/25]	Time 0.657 (0.720)	Data 0.007 (0.034)	Loss 0.6908 (0.6977)	Acc@1 85.938 (85.561)	Acc@5 99.414 (99.404)
Epoch: [39][24/25]	Time 0.380 (0.706)	Data 0.008 (0.033)	Loss 0.6831 (0.6974)	Acc@1 86.203 (85.572)	Acc@5 99.764 (99.410)

Epoch: [40 | 180] LR: 0.100000
Epoch: [40][0/25]	Time 0.697 (0.697)	Data 0.658 (0.658)	Loss 0.6813 (0.6813)	Acc@1 85.889 (85.889)	Acc@5 99.902 (99.902)
Epoch: [40][1/25]	Time 0.630 (0.663)	Data 0.005 (0.331)	Loss 0.6581 (0.6697)	Acc@1 86.816 (86.353)	Acc@5 99.561 (99.731)
Epoch: [40][2/25]	Time 0.662 (0.663)	Data 0.006 (0.223)	Loss 0.6988 (0.6794)	Acc@1 85.693 (86.133)	Acc@5 99.316 (99.593)
Epoch: [40][3/25]	Time 0.692 (0.670)	Data 0.008 (0.169)	Loss 0.7252 (0.6908)	Acc@1 84.863 (85.815)	Acc@5 99.170 (99.487)
Epoch: [40][4/25]	Time 0.777 (0.692)	Data 0.008 (0.137)	Loss 0.7282 (0.6983)	Acc@1 84.326 (85.518)	Acc@5 99.219 (99.434)
Epoch: [40][5/25]	Time 0.661 (0.686)	Data 0.007 (0.115)	Loss 0.6946 (0.6977)	Acc@1 85.449 (85.506)	Acc@5 99.463 (99.438)
Epoch: [40][6/25]	Time 0.682 (0.686)	Data 0.005 (0.100)	Loss 0.6791 (0.6950)	Acc@1 85.938 (85.568)	Acc@5 99.365 (99.428)
Epoch: [40][7/25]	Time 0.619 (0.677)	Data 0.006 (0.088)	Loss 0.6626 (0.6910)	Acc@1 86.328 (85.663)	Acc@5 99.756 (99.469)
Epoch: [40][8/25]	Time 0.663 (0.676)	Data 0.006 (0.079)	Loss 0.6988 (0.6918)	Acc@1 85.840 (85.683)	Acc@5 99.072 (99.425)
Epoch: [40][9/25]	Time 0.653 (0.674)	Data 0.005 (0.071)	Loss 0.6749 (0.6901)	Acc@1 86.621 (85.776)	Acc@5 99.658 (99.448)
Epoch: [40][10/25]	Time 0.673 (0.674)	Data 0.008 (0.066)	Loss 0.6784 (0.6891)	Acc@1 86.279 (85.822)	Acc@5 99.512 (99.454)
Epoch: [40][11/25]	Time 0.641 (0.671)	Data 0.004 (0.060)	Loss 0.6481 (0.6857)	Acc@1 87.109 (85.929)	Acc@5 99.707 (99.475)
Epoch: [40][12/25]	Time 0.667 (0.671)	Data 0.007 (0.056)	Loss 0.6686 (0.6843)	Acc@1 85.889 (85.926)	Acc@5 99.512 (99.478)
Epoch: [40][13/25]	Time 0.690 (0.672)	Data 0.006 (0.053)	Loss 0.6754 (0.6837)	Acc@1 86.523 (85.969)	Acc@5 99.463 (99.477)
Epoch: [40][14/25]	Time 0.720 (0.675)	Data 0.007 (0.050)	Loss 0.7004 (0.6848)	Acc@1 85.254 (85.921)	Acc@5 99.268 (99.463)
Epoch: [40][15/25]	Time 0.770 (0.681)	Data 0.005 (0.047)	Loss 0.6814 (0.6846)	Acc@1 86.475 (85.956)	Acc@5 99.414 (99.460)
Epoch: [40][16/25]	Time 0.703 (0.682)	Data 0.008 (0.045)	Loss 0.7126 (0.6863)	Acc@1 85.840 (85.949)	Acc@5 99.414 (99.457)
Epoch: [40][17/25]	Time 0.657 (0.681)	Data 0.005 (0.042)	Loss 0.6906 (0.6865)	Acc@1 86.084 (85.956)	Acc@5 99.707 (99.471)
Epoch: [40][18/25]	Time 0.703 (0.682)	Data 0.006 (0.041)	Loss 0.6993 (0.6872)	Acc@1 85.693 (85.943)	Acc@5 99.268 (99.460)
Epoch: [40][19/25]	Time 0.751 (0.686)	Data 0.009 (0.039)	Loss 0.6333 (0.6845)	Acc@1 87.744 (86.033)	Acc@5 99.463 (99.460)
Epoch: [40][20/25]	Time 0.695 (0.686)	Data 0.005 (0.037)	Loss 0.6761 (0.6841)	Acc@1 87.012 (86.079)	Acc@5 99.219 (99.449)
Epoch: [40][21/25]	Time 0.738 (0.688)	Data 0.007 (0.036)	Loss 0.7286 (0.6861)	Acc@1 85.010 (86.031)	Acc@5 99.414 (99.447)
Epoch: [40][22/25]	Time 0.790 (0.693)	Data 0.005 (0.035)	Loss 0.6929 (0.6864)	Acc@1 86.182 (86.037)	Acc@5 99.463 (99.448)
Epoch: [40][23/25]	Time 0.680 (0.692)	Data 0.008 (0.033)	Loss 0.6660 (0.6855)	Acc@1 86.816 (86.070)	Acc@5 99.658 (99.457)
Epoch: [40][24/25]	Time 0.372 (0.679)	Data 0.005 (0.032)	Loss 0.6816 (0.6855)	Acc@1 86.085 (86.070)	Acc@5 99.882 (99.464)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 31, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 477700 ; 487386 ; 0.9801266347412523

Epoch: [41 | 180] LR: 0.100000
Epoch: [41][0/25]	Time 0.730 (0.730)	Data 0.693 (0.693)	Loss 0.6704 (0.6704)	Acc@1 86.572 (86.572)	Acc@5 99.658 (99.658)
Epoch: [41][1/25]	Time 0.668 (0.699)	Data 0.010 (0.351)	Loss 0.6278 (0.6491)	Acc@1 87.988 (87.280)	Acc@5 99.365 (99.512)
Epoch: [41][2/25]	Time 0.720 (0.706)	Data 0.006 (0.236)	Loss 0.5944 (0.6309)	Acc@1 88.574 (87.712)	Acc@5 99.805 (99.609)
Epoch: [41][3/25]	Time 0.730 (0.712)	Data 0.003 (0.178)	Loss 0.6367 (0.6323)	Acc@1 87.402 (87.634)	Acc@5 99.512 (99.585)
Epoch: [41][4/25]	Time 0.723 (0.714)	Data 0.008 (0.144)	Loss 0.6186 (0.6296)	Acc@1 88.428 (87.793)	Acc@5 99.463 (99.561)
Epoch: [41][5/25]	Time 0.667 (0.706)	Data 0.004 (0.121)	Loss 0.6127 (0.6268)	Acc@1 88.916 (87.980)	Acc@5 99.609 (99.569)
Epoch: [41][6/25]	Time 0.716 (0.708)	Data 0.003 (0.104)	Loss 0.5982 (0.6227)	Acc@1 88.867 (88.107)	Acc@5 99.658 (99.581)
Epoch: [41][7/25]	Time 0.734 (0.711)	Data 0.003 (0.091)	Loss 0.6024 (0.6201)	Acc@1 88.672 (88.177)	Acc@5 99.805 (99.609)
Epoch: [41][8/25]	Time 0.699 (0.710)	Data 0.008 (0.082)	Loss 0.6195 (0.6201)	Acc@1 87.793 (88.135)	Acc@5 99.609 (99.609)
Epoch: [41][9/25]	Time 0.680 (0.707)	Data 0.007 (0.074)	Loss 0.6406 (0.6221)	Acc@1 87.549 (88.076)	Acc@5 99.365 (99.585)
Epoch: [41][10/25]	Time 0.755 (0.711)	Data 0.005 (0.068)	Loss 0.6387 (0.6236)	Acc@1 88.525 (88.117)	Acc@5 99.414 (99.569)
Epoch: [41][11/25]	Time 0.693 (0.710)	Data 0.004 (0.063)	Loss 0.6448 (0.6254)	Acc@1 87.207 (88.041)	Acc@5 99.561 (99.569)
Epoch: [41][12/25]	Time 0.720 (0.711)	Data 0.006 (0.059)	Loss 0.6271 (0.6255)	Acc@1 87.549 (88.003)	Acc@5 99.609 (99.572)
Epoch: [41][13/25]	Time 0.701 (0.710)	Data 0.007 (0.055)	Loss 0.6331 (0.6261)	Acc@1 87.451 (87.964)	Acc@5 99.609 (99.574)
Epoch: [41][14/25]	Time 0.725 (0.711)	Data 0.005 (0.052)	Loss 0.6414 (0.6271)	Acc@1 87.158 (87.910)	Acc@5 99.658 (99.580)
Epoch: [41][15/25]	Time 0.717 (0.711)	Data 0.005 (0.049)	Loss 0.6311 (0.6273)	Acc@1 88.037 (87.918)	Acc@5 99.707 (99.588)
Epoch: [41][16/25]	Time 0.720 (0.712)	Data 0.007 (0.046)	Loss 0.6986 (0.6315)	Acc@1 86.182 (87.816)	Acc@5 99.316 (99.572)
Epoch: [41][17/25]	Time 0.748 (0.714)	Data 0.006 (0.044)	Loss 0.6688 (0.6336)	Acc@1 86.523 (87.744)	Acc@5 99.512 (99.569)
Epoch: [41][18/25]	Time 0.751 (0.716)	Data 0.006 (0.042)	Loss 0.6613 (0.6350)	Acc@1 86.475 (87.677)	Acc@5 99.561 (99.568)
Epoch: [41][19/25]	Time 0.757 (0.718)	Data 0.005 (0.040)	Loss 0.6698 (0.6368)	Acc@1 86.230 (87.605)	Acc@5 99.512 (99.565)
Epoch: [41][20/25]	Time 0.738 (0.719)	Data 0.005 (0.038)	Loss 0.6732 (0.6385)	Acc@1 86.084 (87.533)	Acc@5 99.414 (99.558)
Epoch: [41][21/25]	Time 0.707 (0.718)	Data 0.004 (0.037)	Loss 0.6842 (0.6406)	Acc@1 85.791 (87.453)	Acc@5 98.975 (99.532)
Epoch: [41][22/25]	Time 0.777 (0.721)	Data 0.009 (0.036)	Loss 0.6928 (0.6429)	Acc@1 85.986 (87.390)	Acc@5 99.170 (99.516)
Epoch: [41][23/25]	Time 0.731 (0.721)	Data 0.004 (0.034)	Loss 0.6642 (0.6438)	Acc@1 86.621 (87.358)	Acc@5 99.658 (99.522)
Epoch: [41][24/25]	Time 0.421 (0.709)	Data 0.005 (0.033)	Loss 0.6297 (0.6435)	Acc@1 88.561 (87.378)	Acc@5 99.528 (99.522)

Epoch: [42 | 180] LR: 0.100000
Epoch: [42][0/25]	Time 0.712 (0.712)	Data 0.733 (0.733)	Loss 0.6287 (0.6287)	Acc@1 88.281 (88.281)	Acc@5 99.561 (99.561)
Epoch: [42][1/25]	Time 0.692 (0.702)	Data 0.004 (0.369)	Loss 0.6818 (0.6552)	Acc@1 85.840 (87.061)	Acc@5 99.561 (99.561)
Epoch: [42][2/25]	Time 0.769 (0.724)	Data 0.006 (0.248)	Loss 0.6363 (0.6489)	Acc@1 87.500 (87.207)	Acc@5 99.512 (99.544)
Epoch: [42][3/25]	Time 0.785 (0.739)	Data 0.004 (0.187)	Loss 0.6703 (0.6543)	Acc@1 85.986 (86.902)	Acc@5 99.707 (99.585)
Epoch: [42][4/25]	Time 0.682 (0.728)	Data 0.006 (0.151)	Loss 0.6390 (0.6512)	Acc@1 87.842 (87.090)	Acc@5 99.658 (99.600)
Epoch: [42][5/25]	Time 0.700 (0.723)	Data 0.005 (0.127)	Loss 0.6325 (0.6481)	Acc@1 88.281 (87.288)	Acc@5 99.805 (99.634)
Epoch: [42][6/25]	Time 0.658 (0.714)	Data 0.008 (0.110)	Loss 0.6771 (0.6522)	Acc@1 87.061 (87.256)	Acc@5 99.463 (99.609)
Epoch: [42][7/25]	Time 0.715 (0.714)	Data 0.004 (0.096)	Loss 0.6581 (0.6530)	Acc@1 86.816 (87.201)	Acc@5 99.463 (99.591)
Epoch: [42][8/25]	Time 0.748 (0.718)	Data 0.004 (0.086)	Loss 0.7005 (0.6583)	Acc@1 85.010 (86.957)	Acc@5 99.268 (99.555)
Epoch: [42][9/25]	Time 0.702 (0.716)	Data 0.005 (0.078)	Loss 0.6630 (0.6587)	Acc@1 87.646 (87.026)	Acc@5 99.365 (99.536)
Epoch: [42][10/25]	Time 0.624 (0.708)	Data 0.004 (0.071)	Loss 0.6713 (0.6599)	Acc@1 86.426 (86.972)	Acc@5 99.365 (99.521)
Epoch: [42][11/25]	Time 0.677 (0.705)	Data 0.005 (0.066)	Loss 0.7036 (0.6635)	Acc@1 85.791 (86.873)	Acc@5 99.463 (99.516)
Epoch: [42][12/25]	Time 0.625 (0.699)	Data 0.005 (0.061)	Loss 0.6798 (0.6648)	Acc@1 86.523 (86.846)	Acc@5 99.512 (99.515)
Epoch: [42][13/25]	Time 0.659 (0.696)	Data 0.005 (0.057)	Loss 0.6725 (0.6653)	Acc@1 86.230 (86.802)	Acc@5 99.561 (99.519)
Epoch: [42][14/25]	Time 0.637 (0.692)	Data 0.008 (0.054)	Loss 0.6905 (0.6670)	Acc@1 85.742 (86.732)	Acc@5 99.463 (99.515)
Epoch: [42][15/25]	Time 0.658 (0.690)	Data 0.008 (0.051)	Loss 0.6683 (0.6671)	Acc@1 86.963 (86.746)	Acc@5 99.463 (99.512)
Epoch: [42][16/25]	Time 0.640 (0.687)	Data 0.007 (0.048)	Loss 0.6997 (0.6690)	Acc@1 85.547 (86.676)	Acc@5 99.463 (99.509)
Epoch: [42][17/25]	Time 0.664 (0.686)	Data 0.006 (0.046)	Loss 0.7106 (0.6713)	Acc@1 85.205 (86.594)	Acc@5 99.463 (99.506)
Epoch: [42][18/25]	Time 0.627 (0.683)	Data 0.007 (0.044)	Loss 0.6824 (0.6719)	Acc@1 86.426 (86.585)	Acc@5 99.170 (99.489)
Epoch: [42][19/25]	Time 0.642 (0.681)	Data 0.007 (0.042)	Loss 0.7196 (0.6743)	Acc@1 85.645 (86.538)	Acc@5 99.219 (99.475)
Epoch: [42][20/25]	Time 0.620 (0.678)	Data 0.006 (0.040)	Loss 0.6887 (0.6750)	Acc@1 85.547 (86.491)	Acc@5 99.512 (99.477)
Epoch: [42][21/25]	Time 0.650 (0.677)	Data 0.004 (0.039)	Loss 0.6746 (0.6749)	Acc@1 85.498 (86.446)	Acc@5 99.268 (99.467)
Epoch: [42][22/25]	Time 0.621 (0.674)	Data 0.006 (0.037)	Loss 0.6890 (0.6756)	Acc@1 85.596 (86.409)	Acc@5 99.512 (99.469)
Epoch: [42][23/25]	Time 0.643 (0.673)	Data 0.007 (0.036)	Loss 0.7140 (0.6772)	Acc@1 85.010 (86.351)	Acc@5 99.414 (99.467)
Epoch: [42][24/25]	Time 0.339 (0.659)	Data 0.007 (0.035)	Loss 0.6654 (0.6770)	Acc@1 86.557 (86.354)	Acc@5 99.646 (99.470)

Epoch: [43 | 180] LR: 0.100000
Epoch: [43][0/25]	Time 0.783 (0.783)	Data 0.675 (0.675)	Loss 0.6720 (0.6720)	Acc@1 85.986 (85.986)	Acc@5 99.365 (99.365)
Epoch: [43][1/25]	Time 0.716 (0.749)	Data 0.014 (0.345)	Loss 0.6837 (0.6779)	Acc@1 85.791 (85.889)	Acc@5 99.365 (99.365)
Epoch: [43][2/25]	Time 0.688 (0.729)	Data 0.005 (0.232)	Loss 0.7023 (0.6860)	Acc@1 85.156 (85.645)	Acc@5 99.512 (99.414)
Epoch: [43][3/25]	Time 0.731 (0.730)	Data 0.006 (0.175)	Loss 0.6925 (0.6876)	Acc@1 85.889 (85.706)	Acc@5 99.316 (99.390)
Epoch: [43][4/25]	Time 0.768 (0.737)	Data 0.009 (0.142)	Loss 0.7472 (0.6996)	Acc@1 83.594 (85.283)	Acc@5 99.365 (99.385)
Epoch: [43][5/25]	Time 0.727 (0.735)	Data 0.005 (0.119)	Loss 0.6996 (0.6996)	Acc@1 86.230 (85.441)	Acc@5 99.219 (99.357)
Epoch: [43][6/25]	Time 0.661 (0.725)	Data 0.004 (0.103)	Loss 0.6805 (0.6968)	Acc@1 85.254 (85.414)	Acc@5 99.268 (99.344)
Epoch: [43][7/25]	Time 0.692 (0.721)	Data 0.007 (0.091)	Loss 0.6979 (0.6970)	Acc@1 85.449 (85.419)	Acc@5 99.365 (99.347)
Epoch: [43][8/25]	Time 0.718 (0.720)	Data 0.009 (0.082)	Loss 0.6628 (0.6932)	Acc@1 86.377 (85.525)	Acc@5 99.365 (99.349)
Epoch: [43][9/25]	Time 0.766 (0.725)	Data 0.004 (0.074)	Loss 0.7005 (0.6939)	Acc@1 86.084 (85.581)	Acc@5 99.414 (99.355)
Epoch: [43][10/25]	Time 0.714 (0.724)	Data 0.006 (0.068)	Loss 0.6481 (0.6897)	Acc@1 86.475 (85.662)	Acc@5 99.414 (99.361)
Epoch: [43][11/25]	Time 0.672 (0.720)	Data 0.008 (0.063)	Loss 0.6836 (0.6892)	Acc@1 86.230 (85.710)	Acc@5 99.268 (99.353)
Epoch: [43][12/25]	Time 0.659 (0.715)	Data 0.008 (0.059)	Loss 0.7064 (0.6905)	Acc@1 85.645 (85.705)	Acc@5 99.316 (99.350)
Epoch: [43][13/25]	Time 0.720 (0.715)	Data 0.006 (0.055)	Loss 0.6940 (0.6908)	Acc@1 86.279 (85.746)	Acc@5 99.219 (99.341)
Epoch: [43][14/25]	Time 0.786 (0.720)	Data 0.005 (0.051)	Loss 0.6648 (0.6891)	Acc@1 86.963 (85.827)	Acc@5 99.268 (99.336)
Epoch: [43][15/25]	Time 0.685 (0.718)	Data 0.005 (0.048)	Loss 0.6960 (0.6895)	Acc@1 86.426 (85.864)	Acc@5 99.414 (99.341)
Epoch: [43][16/25]	Time 0.623 (0.712)	Data 0.008 (0.046)	Loss 0.6974 (0.6900)	Acc@1 85.596 (85.848)	Acc@5 99.170 (99.331)
Epoch: [43][17/25]	Time 0.683 (0.711)	Data 0.003 (0.044)	Loss 0.6825 (0.6895)	Acc@1 86.230 (85.870)	Acc@5 99.658 (99.349)
Epoch: [43][18/25]	Time 0.645 (0.707)	Data 0.004 (0.042)	Loss 0.6665 (0.6883)	Acc@1 87.207 (85.940)	Acc@5 99.365 (99.350)
Epoch: [43][19/25]	Time 0.720 (0.708)	Data 0.005 (0.040)	Loss 0.6840 (0.6881)	Acc@1 86.426 (85.964)	Acc@5 99.365 (99.351)
Epoch: [43][20/25]	Time 0.714 (0.708)	Data 0.004 (0.038)	Loss 0.6537 (0.6865)	Acc@1 86.768 (86.003)	Acc@5 99.316 (99.349)
Epoch: [43][21/25]	Time 0.737 (0.709)	Data 0.006 (0.037)	Loss 0.6815 (0.6862)	Acc@1 86.377 (86.020)	Acc@5 99.316 (99.347)
Epoch: [43][22/25]	Time 0.686 (0.708)	Data 0.004 (0.035)	Loss 0.6774 (0.6859)	Acc@1 86.621 (86.046)	Acc@5 99.756 (99.365)
Epoch: [43][23/25]	Time 0.724 (0.709)	Data 0.005 (0.034)	Loss 0.6958 (0.6863)	Acc@1 85.742 (86.033)	Acc@5 99.463 (99.369)
Epoch: [43][24/25]	Time 0.464 (0.699)	Data 0.006 (0.033)	Loss 0.6412 (0.6855)	Acc@1 87.854 (86.064)	Acc@5 99.764 (99.376)

Epoch: [44 | 180] LR: 0.100000
Epoch: [44][0/25]	Time 0.770 (0.770)	Data 0.601 (0.601)	Loss 0.6663 (0.6663)	Acc@1 86.426 (86.426)	Acc@5 99.561 (99.561)
Epoch: [44][1/25]	Time 0.721 (0.745)	Data 0.007 (0.304)	Loss 0.6937 (0.6800)	Acc@1 86.035 (86.230)	Acc@5 99.414 (99.487)
Epoch: [44][2/25]	Time 0.699 (0.730)	Data 0.005 (0.204)	Loss 0.6243 (0.6614)	Acc@1 88.086 (86.849)	Acc@5 99.658 (99.544)
Epoch: [44][3/25]	Time 0.738 (0.732)	Data 0.004 (0.154)	Loss 0.6623 (0.6616)	Acc@1 86.865 (86.853)	Acc@5 99.561 (99.548)
Epoch: [44][4/25]	Time 0.793 (0.744)	Data 0.006 (0.124)	Loss 0.7221 (0.6737)	Acc@1 84.961 (86.475)	Acc@5 99.170 (99.473)
Epoch: [44][5/25]	Time 0.671 (0.732)	Data 0.008 (0.105)	Loss 0.6860 (0.6758)	Acc@1 85.400 (86.296)	Acc@5 99.414 (99.463)
Epoch: [44][6/25]	Time 0.674 (0.724)	Data 0.005 (0.091)	Loss 0.6578 (0.6732)	Acc@1 87.158 (86.419)	Acc@5 99.512 (99.470)
Epoch: [44][7/25]	Time 0.651 (0.715)	Data 0.004 (0.080)	Loss 0.6469 (0.6699)	Acc@1 87.305 (86.530)	Acc@5 99.658 (99.493)
Epoch: [44][8/25]	Time 0.704 (0.713)	Data 0.006 (0.072)	Loss 0.6639 (0.6692)	Acc@1 86.865 (86.567)	Acc@5 99.316 (99.474)
Epoch: [44][9/25]	Time 0.750 (0.717)	Data 0.005 (0.065)	Loss 0.6689 (0.6692)	Acc@1 86.621 (86.572)	Acc@5 99.512 (99.478)
Epoch: [44][10/25]	Time 0.729 (0.718)	Data 0.004 (0.059)	Loss 0.6870 (0.6708)	Acc@1 85.449 (86.470)	Acc@5 99.561 (99.485)
Epoch: [44][11/25]	Time 0.758 (0.721)	Data 0.006 (0.055)	Loss 0.6664 (0.6705)	Acc@1 87.158 (86.528)	Acc@5 99.512 (99.487)
Epoch: [44][12/25]	Time 0.763 (0.725)	Data 0.005 (0.051)	Loss 0.6819 (0.6713)	Acc@1 85.986 (86.486)	Acc@5 99.463 (99.485)
Epoch: [44][13/25]	Time 0.699 (0.723)	Data 0.005 (0.048)	Loss 0.6775 (0.6718)	Acc@1 86.328 (86.475)	Acc@5 99.414 (99.480)
Epoch: [44][14/25]	Time 0.640 (0.717)	Data 0.005 (0.045)	Loss 0.6832 (0.6725)	Acc@1 86.719 (86.491)	Acc@5 99.609 (99.489)
Epoch: [44][15/25]	Time 0.660 (0.714)	Data 0.005 (0.042)	Loss 0.6918 (0.6737)	Acc@1 86.182 (86.472)	Acc@5 99.414 (99.484)
Epoch: [44][16/25]	Time 0.618 (0.708)	Data 0.004 (0.040)	Loss 0.6725 (0.6737)	Acc@1 85.498 (86.414)	Acc@5 99.463 (99.483)
Epoch: [44][17/25]	Time 0.662 (0.705)	Data 0.006 (0.038)	Loss 0.6559 (0.6727)	Acc@1 86.865 (86.439)	Acc@5 99.561 (99.487)
Epoch: [44][18/25]	Time 0.610 (0.700)	Data 0.009 (0.037)	Loss 0.6872 (0.6734)	Acc@1 85.938 (86.413)	Acc@5 99.414 (99.483)
Epoch: [44][19/25]	Time 0.639 (0.697)	Data 0.006 (0.035)	Loss 0.6636 (0.6730)	Acc@1 87.354 (86.460)	Acc@5 99.658 (99.492)
Epoch: [44][20/25]	Time 0.608 (0.693)	Data 0.005 (0.034)	Loss 0.7131 (0.6749)	Acc@1 84.863 (86.384)	Acc@5 99.414 (99.488)
Epoch: [44][21/25]	Time 0.673 (0.692)	Data 0.007 (0.033)	Loss 0.7359 (0.6776)	Acc@1 83.984 (86.275)	Acc@5 99.365 (99.483)
Epoch: [44][22/25]	Time 0.707 (0.693)	Data 0.007 (0.032)	Loss 0.6946 (0.6784)	Acc@1 85.498 (86.241)	Acc@5 99.365 (99.478)
Epoch: [44][23/25]	Time 0.744 (0.695)	Data 0.007 (0.030)	Loss 0.6835 (0.6786)	Acc@1 86.084 (86.235)	Acc@5 99.609 (99.483)
Epoch: [44][24/25]	Time 0.473 (0.686)	Data 0.004 (0.029)	Loss 0.6899 (0.6788)	Acc@1 85.024 (86.214)	Acc@5 99.646 (99.486)

Epoch: [45 | 180] LR: 0.100000
Epoch: [45][0/25]	Time 0.729 (0.729)	Data 0.653 (0.653)	Loss 0.6823 (0.6823)	Acc@1 86.279 (86.279)	Acc@5 99.463 (99.463)
Epoch: [45][1/25]	Time 0.700 (0.715)	Data 0.007 (0.330)	Loss 0.7239 (0.7031)	Acc@1 84.766 (85.522)	Acc@5 99.512 (99.487)
Epoch: [45][2/25]	Time 0.754 (0.728)	Data 0.006 (0.222)	Loss 0.6704 (0.6922)	Acc@1 86.182 (85.742)	Acc@5 99.561 (99.512)
Epoch: [45][3/25]	Time 0.760 (0.736)	Data 0.004 (0.168)	Loss 0.6603 (0.6842)	Acc@1 87.207 (86.108)	Acc@5 99.609 (99.536)
Epoch: [45][4/25]	Time 0.719 (0.733)	Data 0.006 (0.135)	Loss 0.6621 (0.6798)	Acc@1 87.891 (86.465)	Acc@5 99.463 (99.521)
Epoch: [45][5/25]	Time 0.688 (0.725)	Data 0.007 (0.114)	Loss 0.6766 (0.6793)	Acc@1 87.305 (86.605)	Acc@5 99.561 (99.528)
Epoch: [45][6/25]	Time 0.740 (0.727)	Data 0.004 (0.098)	Loss 0.6730 (0.6784)	Acc@1 86.865 (86.642)	Acc@5 99.609 (99.540)
Epoch: [45][7/25]	Time 0.734 (0.728)	Data 0.005 (0.087)	Loss 0.6510 (0.6749)	Acc@1 87.305 (86.725)	Acc@5 99.854 (99.579)
Epoch: [45][8/25]	Time 0.667 (0.721)	Data 0.005 (0.078)	Loss 0.6504 (0.6722)	Acc@1 87.744 (86.838)	Acc@5 99.316 (99.550)
Epoch: [45][9/25]	Time 0.637 (0.713)	Data 0.007 (0.071)	Loss 0.6956 (0.6746)	Acc@1 86.572 (86.812)	Acc@5 99.170 (99.512)
Epoch: [45][10/25]	Time 0.655 (0.708)	Data 0.005 (0.065)	Loss 0.6760 (0.6747)	Acc@1 86.572 (86.790)	Acc@5 99.512 (99.512)
Epoch: [45][11/25]	Time 0.660 (0.704)	Data 0.006 (0.060)	Loss 0.6808 (0.6752)	Acc@1 85.791 (86.707)	Acc@5 99.463 (99.508)
Epoch: [45][12/25]	Time 0.647 (0.699)	Data 0.005 (0.055)	Loss 0.6415 (0.6726)	Acc@1 88.232 (86.824)	Acc@5 99.268 (99.489)
Epoch: [45][13/25]	Time 0.648 (0.696)	Data 0.008 (0.052)	Loss 0.6708 (0.6725)	Acc@1 86.377 (86.792)	Acc@5 99.609 (99.498)
Epoch: [45][14/25]	Time 0.672 (0.694)	Data 0.005 (0.049)	Loss 0.6799 (0.6730)	Acc@1 86.719 (86.787)	Acc@5 99.268 (99.482)
Epoch: [45][15/25]	Time 0.750 (0.698)	Data 0.008 (0.046)	Loss 0.6792 (0.6734)	Acc@1 85.742 (86.722)	Acc@5 99.512 (99.484)
Epoch: [45][16/25]	Time 0.750 (0.701)	Data 0.008 (0.044)	Loss 0.6931 (0.6745)	Acc@1 85.498 (86.650)	Acc@5 99.268 (99.472)
Epoch: [45][17/25]	Time 0.742 (0.703)	Data 0.006 (0.042)	Loss 0.6525 (0.6733)	Acc@1 87.109 (86.675)	Acc@5 99.609 (99.479)
Epoch: [45][18/25]	Time 0.720 (0.704)	Data 0.004 (0.040)	Loss 0.6684 (0.6730)	Acc@1 86.279 (86.655)	Acc@5 99.365 (99.473)
Epoch: [45][19/25]	Time 0.702 (0.704)	Data 0.004 (0.038)	Loss 0.6522 (0.6720)	Acc@1 86.963 (86.670)	Acc@5 99.561 (99.478)
Epoch: [45][20/25]	Time 0.744 (0.706)	Data 0.005 (0.037)	Loss 0.6522 (0.6710)	Acc@1 86.963 (86.684)	Acc@5 99.512 (99.479)
Epoch: [45][21/25]	Time 0.695 (0.705)	Data 0.005 (0.035)	Loss 0.6926 (0.6720)	Acc@1 86.084 (86.657)	Acc@5 99.561 (99.483)
Epoch: [45][22/25]	Time 0.722 (0.706)	Data 0.007 (0.034)	Loss 0.6714 (0.6720)	Acc@1 86.719 (86.659)	Acc@5 99.561 (99.486)
Epoch: [45][23/25]	Time 0.764 (0.708)	Data 0.005 (0.033)	Loss 0.6844 (0.6725)	Acc@1 85.742 (86.621)	Acc@5 99.609 (99.491)
Epoch: [45][24/25]	Time 0.419 (0.697)	Data 0.004 (0.032)	Loss 0.6624 (0.6723)	Acc@1 88.208 (86.648)	Acc@5 99.410 (99.490)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 30, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 471770 ; 487386 ; 0.9679596869832124

Epoch: [46 | 180] LR: 0.100000
Epoch: [46][0/25]	Time 0.755 (0.755)	Data 0.801 (0.801)	Loss 0.6306 (0.6306)	Acc@1 88.281 (88.281)	Acc@5 99.561 (99.561)
Epoch: [46][1/25]	Time 0.743 (0.749)	Data 0.005 (0.403)	Loss 0.5971 (0.6139)	Acc@1 89.160 (88.721)	Acc@5 99.805 (99.683)
Epoch: [46][2/25]	Time 0.694 (0.731)	Data 0.004 (0.270)	Loss 0.6161 (0.6146)	Acc@1 88.477 (88.639)	Acc@5 99.707 (99.691)
Epoch: [46][3/25]	Time 0.682 (0.719)	Data 0.007 (0.204)	Loss 0.6149 (0.6147)	Acc@1 88.623 (88.635)	Acc@5 99.854 (99.731)
Epoch: [46][4/25]	Time 0.691 (0.713)	Data 0.006 (0.165)	Loss 0.5776 (0.6073)	Acc@1 89.941 (88.896)	Acc@5 99.854 (99.756)
Epoch: [46][5/25]	Time 0.711 (0.713)	Data 0.005 (0.138)	Loss 0.5910 (0.6046)	Acc@1 89.404 (88.981)	Acc@5 99.756 (99.756)
Epoch: [46][6/25]	Time 0.762 (0.720)	Data 0.004 (0.119)	Loss 0.6061 (0.6048)	Acc@1 88.135 (88.860)	Acc@5 99.512 (99.721)
Epoch: [46][7/25]	Time 0.752 (0.724)	Data 0.005 (0.105)	Loss 0.5901 (0.6029)	Acc@1 90.186 (89.026)	Acc@5 99.707 (99.719)
Epoch: [46][8/25]	Time 0.730 (0.725)	Data 0.005 (0.094)	Loss 0.6026 (0.6029)	Acc@1 88.721 (88.992)	Acc@5 99.756 (99.723)
Epoch: [46][9/25]	Time 0.697 (0.722)	Data 0.006 (0.085)	Loss 0.5701 (0.5996)	Acc@1 90.576 (89.150)	Acc@5 99.707 (99.722)
Epoch: [46][10/25]	Time 0.694 (0.719)	Data 0.005 (0.078)	Loss 0.5929 (0.5990)	Acc@1 89.307 (89.165)	Acc@5 99.414 (99.694)
Epoch: [46][11/25]	Time 0.731 (0.720)	Data 0.009 (0.072)	Loss 0.6042 (0.5994)	Acc@1 89.160 (89.164)	Acc@5 99.658 (99.691)
Epoch: [46][12/25]	Time 0.667 (0.716)	Data 0.006 (0.067)	Loss 0.6367 (0.6023)	Acc@1 88.086 (89.081)	Acc@5 99.561 (99.681)
Epoch: [46][13/25]	Time 0.714 (0.716)	Data 0.008 (0.063)	Loss 0.6628 (0.6066)	Acc@1 86.523 (88.899)	Acc@5 99.561 (99.672)
Epoch: [46][14/25]	Time 0.654 (0.712)	Data 0.004 (0.059)	Loss 0.6328 (0.6084)	Acc@1 87.354 (88.796)	Acc@5 99.609 (99.668)
Epoch: [46][15/25]	Time 0.700 (0.711)	Data 0.007 (0.055)	Loss 0.6398 (0.6103)	Acc@1 87.646 (88.724)	Acc@5 99.512 (99.658)
Epoch: [46][16/25]	Time 0.840 (0.719)	Data 0.006 (0.053)	Loss 0.6405 (0.6121)	Acc@1 87.354 (88.643)	Acc@5 99.756 (99.664)
Epoch: [46][17/25]	Time 0.828 (0.725)	Data 0.004 (0.050)	Loss 0.6183 (0.6125)	Acc@1 87.988 (88.607)	Acc@5 99.463 (99.653)
Epoch: [46][18/25]	Time 0.647 (0.721)	Data 0.005 (0.047)	Loss 0.6065 (0.6121)	Acc@1 88.379 (88.595)	Acc@5 99.512 (99.645)
Epoch: [46][19/25]	Time 0.706 (0.720)	Data 0.007 (0.045)	Loss 0.6838 (0.6157)	Acc@1 86.670 (88.499)	Acc@5 99.414 (99.634)
Epoch: [46][20/25]	Time 0.799 (0.724)	Data 0.005 (0.043)	Loss 0.6118 (0.6155)	Acc@1 87.402 (88.446)	Acc@5 99.561 (99.630)
Epoch: [46][21/25]	Time 0.847 (0.729)	Data 0.005 (0.042)	Loss 0.6434 (0.6168)	Acc@1 87.207 (88.390)	Acc@5 99.365 (99.618)
Epoch: [46][22/25]	Time 0.776 (0.731)	Data 0.004 (0.040)	Loss 0.6691 (0.6191)	Acc@1 86.670 (88.315)	Acc@5 99.561 (99.616)
Epoch: [46][23/25]	Time 0.732 (0.731)	Data 0.004 (0.039)	Loss 0.6384 (0.6199)	Acc@1 87.793 (88.293)	Acc@5 99.512 (99.611)
Epoch: [46][24/25]	Time 0.447 (0.720)	Data 0.007 (0.037)	Loss 0.6801 (0.6209)	Acc@1 85.967 (88.254)	Acc@5 99.764 (99.614)

Epoch: [47 | 180] LR: 0.100000
Epoch: [47][0/25]	Time 0.823 (0.823)	Data 0.677 (0.677)	Loss 0.6381 (0.6381)	Acc@1 87.793 (87.793)	Acc@5 99.658 (99.658)
Epoch: [47][1/25]	Time 0.708 (0.766)	Data 0.006 (0.342)	Loss 0.6454 (0.6417)	Acc@1 87.402 (87.598)	Acc@5 99.658 (99.658)
Epoch: [47][2/25]	Time 0.673 (0.735)	Data 0.003 (0.229)	Loss 0.6307 (0.6381)	Acc@1 88.428 (87.874)	Acc@5 99.609 (99.642)
Epoch: [47][3/25]	Time 0.661 (0.716)	Data 0.005 (0.173)	Loss 0.6511 (0.6413)	Acc@1 86.621 (87.561)	Acc@5 99.561 (99.622)
Epoch: [47][4/25]	Time 0.672 (0.708)	Data 0.007 (0.140)	Loss 0.6379 (0.6406)	Acc@1 87.549 (87.559)	Acc@5 99.658 (99.629)
Epoch: [47][5/25]	Time 0.733 (0.712)	Data 0.009 (0.118)	Loss 0.6500 (0.6422)	Acc@1 86.865 (87.443)	Acc@5 99.512 (99.609)
Epoch: [47][6/25]	Time 0.722 (0.713)	Data 0.007 (0.102)	Loss 0.6819 (0.6479)	Acc@1 86.865 (87.360)	Acc@5 99.463 (99.588)
Epoch: [47][7/25]	Time 0.734 (0.716)	Data 0.007 (0.090)	Loss 0.6797 (0.6519)	Acc@1 86.719 (87.280)	Acc@5 99.414 (99.567)
Epoch: [47][8/25]	Time 0.764 (0.721)	Data 0.008 (0.081)	Loss 0.6458 (0.6512)	Acc@1 87.646 (87.321)	Acc@5 99.609 (99.571)
Epoch: [47][9/25]	Time 0.669 (0.716)	Data 0.005 (0.073)	Loss 0.6283 (0.6489)	Acc@1 88.672 (87.456)	Acc@5 99.561 (99.570)
Epoch: [47][10/25]	Time 0.611 (0.706)	Data 0.005 (0.067)	Loss 0.6692 (0.6507)	Acc@1 86.475 (87.367)	Acc@5 99.609 (99.574)
Epoch: [47][11/25]	Time 0.648 (0.702)	Data 0.006 (0.062)	Loss 0.6444 (0.6502)	Acc@1 87.061 (87.341)	Acc@5 99.609 (99.577)
Epoch: [47][12/25]	Time 0.640 (0.697)	Data 0.004 (0.058)	Loss 0.6918 (0.6534)	Acc@1 85.254 (87.181)	Acc@5 99.316 (99.557)
Epoch: [47][13/25]	Time 0.705 (0.697)	Data 0.007 (0.054)	Loss 0.6570 (0.6537)	Acc@1 87.158 (87.179)	Acc@5 99.316 (99.540)
Epoch: [47][14/25]	Time 0.743 (0.700)	Data 0.005 (0.051)	Loss 0.6759 (0.6552)	Acc@1 86.768 (87.152)	Acc@5 99.463 (99.535)
Epoch: [47][15/25]	Time 0.656 (0.698)	Data 0.005 (0.048)	Loss 0.6806 (0.6567)	Acc@1 86.377 (87.103)	Acc@5 99.268 (99.518)
Epoch: [47][16/25]	Time 0.652 (0.695)	Data 0.005 (0.045)	Loss 0.6365 (0.6556)	Acc@1 88.281 (87.173)	Acc@5 99.561 (99.520)
Epoch: [47][17/25]	Time 0.649 (0.692)	Data 0.008 (0.043)	Loss 0.6740 (0.6566)	Acc@1 86.084 (87.112)	Acc@5 99.609 (99.525)
Epoch: [47][18/25]	Time 0.690 (0.692)	Data 0.007 (0.041)	Loss 0.6633 (0.6569)	Acc@1 85.791 (87.043)	Acc@5 99.658 (99.532)
Epoch: [47][19/25]	Time 0.674 (0.691)	Data 0.005 (0.040)	Loss 0.6467 (0.6564)	Acc@1 87.451 (87.063)	Acc@5 99.365 (99.524)
Epoch: [47][20/25]	Time 0.692 (0.691)	Data 0.006 (0.038)	Loss 0.6698 (0.6571)	Acc@1 86.084 (87.016)	Acc@5 99.365 (99.516)
Epoch: [47][21/25]	Time 0.745 (0.694)	Data 0.008 (0.037)	Loss 0.6616 (0.6573)	Acc@1 86.328 (86.985)	Acc@5 99.805 (99.529)
Epoch: [47][22/25]	Time 0.693 (0.694)	Data 0.005 (0.035)	Loss 0.6152 (0.6554)	Acc@1 87.598 (87.012)	Acc@5 99.463 (99.527)
Epoch: [47][23/25]	Time 0.679 (0.693)	Data 0.008 (0.034)	Loss 0.6490 (0.6552)	Acc@1 86.328 (86.983)	Acc@5 99.561 (99.528)
Epoch: [47][24/25]	Time 0.357 (0.680)	Data 0.007 (0.033)	Loss 0.6562 (0.6552)	Acc@1 86.085 (86.968)	Acc@5 99.764 (99.532)

Epoch: [48 | 180] LR: 0.100000
Epoch: [48][0/25]	Time 0.705 (0.705)	Data 0.630 (0.630)	Loss 0.6444 (0.6444)	Acc@1 87.695 (87.695)	Acc@5 99.365 (99.365)
Epoch: [48][1/25]	Time 0.726 (0.716)	Data 0.007 (0.318)	Loss 0.6854 (0.6649)	Acc@1 85.986 (86.841)	Acc@5 99.072 (99.219)
Epoch: [48][2/25]	Time 0.684 (0.705)	Data 0.011 (0.216)	Loss 0.6591 (0.6629)	Acc@1 87.744 (87.142)	Acc@5 99.072 (99.170)
Epoch: [48][3/25]	Time 0.737 (0.713)	Data 0.004 (0.163)	Loss 0.6427 (0.6579)	Acc@1 87.793 (87.305)	Acc@5 99.658 (99.292)
Epoch: [48][4/25]	Time 0.688 (0.708)	Data 0.007 (0.132)	Loss 0.6496 (0.6562)	Acc@1 87.061 (87.256)	Acc@5 99.414 (99.316)
Epoch: [48][5/25]	Time 0.666 (0.701)	Data 0.004 (0.110)	Loss 0.6317 (0.6522)	Acc@1 87.891 (87.362)	Acc@5 99.658 (99.373)
Epoch: [48][6/25]	Time 0.750 (0.708)	Data 0.008 (0.096)	Loss 0.6499 (0.6518)	Acc@1 87.256 (87.347)	Acc@5 99.414 (99.379)
Epoch: [48][7/25]	Time 0.740 (0.712)	Data 0.006 (0.085)	Loss 0.6588 (0.6527)	Acc@1 86.523 (87.244)	Acc@5 99.609 (99.408)
Epoch: [48][8/25]	Time 0.678 (0.708)	Data 0.007 (0.076)	Loss 0.6323 (0.6504)	Acc@1 87.695 (87.294)	Acc@5 99.707 (99.441)
Epoch: [48][9/25]	Time 0.717 (0.709)	Data 0.006 (0.069)	Loss 0.6424 (0.6496)	Acc@1 86.914 (87.256)	Acc@5 99.463 (99.443)
Epoch: [48][10/25]	Time 0.765 (0.714)	Data 0.005 (0.063)	Loss 0.6538 (0.6500)	Acc@1 86.963 (87.229)	Acc@5 99.463 (99.445)
Epoch: [48][11/25]	Time 0.687 (0.712)	Data 0.005 (0.058)	Loss 0.6852 (0.6529)	Acc@1 86.523 (87.170)	Acc@5 99.365 (99.438)
Epoch: [48][12/25]	Time 0.640 (0.707)	Data 0.009 (0.054)	Loss 0.6751 (0.6546)	Acc@1 86.816 (87.143)	Acc@5 99.561 (99.448)
Epoch: [48][13/25]	Time 0.612 (0.700)	Data 0.010 (0.051)	Loss 0.6947 (0.6575)	Acc@1 86.719 (87.113)	Acc@5 99.268 (99.435)
Epoch: [48][14/25]	Time 0.640 (0.696)	Data 0.007 (0.048)	Loss 0.6926 (0.6598)	Acc@1 85.498 (87.005)	Acc@5 99.365 (99.430)
Epoch: [48][15/25]	Time 0.606 (0.690)	Data 0.008 (0.046)	Loss 0.6716 (0.6606)	Acc@1 86.328 (86.963)	Acc@5 99.365 (99.426)
Epoch: [48][16/25]	Time 0.602 (0.685)	Data 0.008 (0.044)	Loss 0.6694 (0.6611)	Acc@1 86.816 (86.954)	Acc@5 99.365 (99.423)
Epoch: [48][17/25]	Time 0.642 (0.683)	Data 0.005 (0.041)	Loss 0.6529 (0.6606)	Acc@1 87.354 (86.976)	Acc@5 99.365 (99.419)
Epoch: [48][18/25]	Time 0.628 (0.680)	Data 0.007 (0.040)	Loss 0.6486 (0.6600)	Acc@1 86.670 (86.960)	Acc@5 99.512 (99.424)
Epoch: [48][19/25]	Time 0.614 (0.676)	Data 0.007 (0.038)	Loss 0.6603 (0.6600)	Acc@1 87.109 (86.968)	Acc@5 99.512 (99.429)
Epoch: [48][20/25]	Time 0.634 (0.674)	Data 0.007 (0.037)	Loss 0.6535 (0.6597)	Acc@1 87.305 (86.984)	Acc@5 99.512 (99.433)
Epoch: [48][21/25]	Time 0.647 (0.673)	Data 0.005 (0.035)	Loss 0.6741 (0.6604)	Acc@1 86.328 (86.954)	Acc@5 99.414 (99.432)
Epoch: [48][22/25]	Time 0.628 (0.671)	Data 0.006 (0.034)	Loss 0.6572 (0.6602)	Acc@1 87.305 (86.969)	Acc@5 99.463 (99.433)
Epoch: [48][23/25]	Time 0.675 (0.671)	Data 0.006 (0.033)	Loss 0.6221 (0.6586)	Acc@1 87.988 (87.012)	Acc@5 99.707 (99.445)
Epoch: [48][24/25]	Time 0.392 (0.660)	Data 0.004 (0.032)	Loss 0.6996 (0.6593)	Acc@1 83.373 (86.950)	Acc@5 99.175 (99.440)

Epoch: [49 | 180] LR: 0.100000
Epoch: [49][0/25]	Time 0.751 (0.751)	Data 0.644 (0.644)	Loss 0.6194 (0.6194)	Acc@1 89.111 (89.111)	Acc@5 99.707 (99.707)
Epoch: [49][1/25]	Time 0.769 (0.760)	Data 0.005 (0.325)	Loss 0.6706 (0.6450)	Acc@1 86.719 (87.915)	Acc@5 99.756 (99.731)
Epoch: [49][2/25]	Time 0.696 (0.739)	Data 0.007 (0.219)	Loss 0.6912 (0.6604)	Acc@1 86.084 (87.305)	Acc@5 99.512 (99.658)
Epoch: [49][3/25]	Time 0.736 (0.738)	Data 0.006 (0.166)	Loss 0.6439 (0.6563)	Acc@1 87.207 (87.280)	Acc@5 99.561 (99.634)
Epoch: [49][4/25]	Time 0.727 (0.736)	Data 0.007 (0.134)	Loss 0.6996 (0.6649)	Acc@1 85.010 (86.826)	Acc@5 99.316 (99.570)
Epoch: [49][5/25]	Time 0.743 (0.737)	Data 0.005 (0.112)	Loss 0.6379 (0.6604)	Acc@1 88.281 (87.069)	Acc@5 99.268 (99.520)
Epoch: [49][6/25]	Time 0.767 (0.741)	Data 0.006 (0.097)	Loss 0.6457 (0.6583)	Acc@1 87.354 (87.109)	Acc@5 99.414 (99.505)
Epoch: [49][7/25]	Time 0.696 (0.736)	Data 0.007 (0.086)	Loss 0.6449 (0.6567)	Acc@1 87.256 (87.128)	Acc@5 99.658 (99.524)
Epoch: [49][8/25]	Time 0.693 (0.731)	Data 0.005 (0.077)	Loss 0.6626 (0.6573)	Acc@1 86.426 (87.050)	Acc@5 99.707 (99.544)
Epoch: [49][9/25]	Time 0.750 (0.733)	Data 0.004 (0.070)	Loss 0.6318 (0.6548)	Acc@1 88.232 (87.168)	Acc@5 99.707 (99.561)
Epoch: [49][10/25]	Time 0.695 (0.729)	Data 0.007 (0.064)	Loss 0.6417 (0.6536)	Acc@1 87.549 (87.203)	Acc@5 99.316 (99.538)
Epoch: [49][11/25]	Time 0.749 (0.731)	Data 0.009 (0.059)	Loss 0.6790 (0.6557)	Acc@1 86.328 (87.130)	Acc@5 99.414 (99.528)
Epoch: [49][12/25]	Time 0.746 (0.732)	Data 0.007 (0.055)	Loss 0.6439 (0.6548)	Acc@1 86.377 (87.072)	Acc@5 99.561 (99.530)
Epoch: [49][13/25]	Time 0.704 (0.730)	Data 0.006 (0.052)	Loss 0.6280 (0.6529)	Acc@1 87.695 (87.116)	Acc@5 99.609 (99.536)
Epoch: [49][14/25]	Time 0.681 (0.727)	Data 0.007 (0.049)	Loss 0.6516 (0.6528)	Acc@1 87.646 (87.152)	Acc@5 99.365 (99.525)
Epoch: [49][15/25]	Time 0.657 (0.723)	Data 0.007 (0.046)	Loss 0.6788 (0.6544)	Acc@1 85.938 (87.076)	Acc@5 99.365 (99.515)
Epoch: [49][16/25]	Time 0.712 (0.722)	Data 0.005 (0.044)	Loss 0.6873 (0.6563)	Acc@1 85.547 (86.986)	Acc@5 99.316 (99.503)
Epoch: [49][17/25]	Time 0.723 (0.722)	Data 0.006 (0.042)	Loss 0.7072 (0.6592)	Acc@1 85.303 (86.892)	Acc@5 99.414 (99.498)
Epoch: [49][18/25]	Time 0.682 (0.720)	Data 0.007 (0.040)	Loss 0.6700 (0.6597)	Acc@1 86.816 (86.888)	Acc@5 99.658 (99.507)
Epoch: [49][19/25]	Time 0.657 (0.717)	Data 0.004 (0.038)	Loss 0.6995 (0.6617)	Acc@1 85.498 (86.819)	Acc@5 99.219 (99.492)
Epoch: [49][20/25]	Time 0.674 (0.715)	Data 0.006 (0.037)	Loss 0.6530 (0.6613)	Acc@1 87.402 (86.847)	Acc@5 99.414 (99.488)
Epoch: [49][21/25]	Time 0.654 (0.712)	Data 0.006 (0.035)	Loss 0.6656 (0.6615)	Acc@1 86.963 (86.852)	Acc@5 99.316 (99.481)
Epoch: [49][22/25]	Time 0.741 (0.713)	Data 0.006 (0.034)	Loss 0.7148 (0.6638)	Acc@1 84.863 (86.765)	Acc@5 99.170 (99.467)
Epoch: [49][23/25]	Time 0.705 (0.713)	Data 0.006 (0.033)	Loss 0.6771 (0.6644)	Acc@1 86.768 (86.766)	Acc@5 99.268 (99.459)
Epoch: [49][24/25]	Time 0.365 (0.699)	Data 0.004 (0.032)	Loss 0.6238 (0.6637)	Acc@1 87.736 (86.782)	Acc@5 99.528 (99.460)

Epoch: [50 | 180] LR: 0.100000
Epoch: [50][0/25]	Time 0.697 (0.697)	Data 0.644 (0.644)	Loss 0.6414 (0.6414)	Acc@1 87.012 (87.012)	Acc@5 99.561 (99.561)
Epoch: [50][1/25]	Time 0.681 (0.689)	Data 0.008 (0.326)	Loss 0.6327 (0.6370)	Acc@1 87.646 (87.329)	Acc@5 99.609 (99.585)
Epoch: [50][2/25]	Time 0.706 (0.695)	Data 0.006 (0.219)	Loss 0.6339 (0.6360)	Acc@1 87.842 (87.500)	Acc@5 99.561 (99.577)
Epoch: [50][3/25]	Time 0.755 (0.710)	Data 0.005 (0.166)	Loss 0.6681 (0.6440)	Acc@1 86.768 (87.317)	Acc@5 99.463 (99.548)
Epoch: [50][4/25]	Time 0.715 (0.711)	Data 0.011 (0.135)	Loss 0.6522 (0.6457)	Acc@1 87.842 (87.422)	Acc@5 99.756 (99.590)
Epoch: [50][5/25]	Time 0.751 (0.717)	Data 0.005 (0.113)	Loss 0.6573 (0.6476)	Acc@1 87.158 (87.378)	Acc@5 99.658 (99.601)
Epoch: [50][6/25]	Time 0.759 (0.723)	Data 0.003 (0.097)	Loss 0.6148 (0.6429)	Acc@1 87.744 (87.430)	Acc@5 99.854 (99.637)
Epoch: [50][7/25]	Time 0.763 (0.728)	Data 0.005 (0.086)	Loss 0.6655 (0.6457)	Acc@1 86.816 (87.354)	Acc@5 99.463 (99.615)
Epoch: [50][8/25]	Time 0.712 (0.726)	Data 0.007 (0.077)	Loss 0.6406 (0.6452)	Acc@1 87.939 (87.419)	Acc@5 99.561 (99.609)
Epoch: [50][9/25]	Time 0.712 (0.725)	Data 0.003 (0.070)	Loss 0.6484 (0.6455)	Acc@1 87.061 (87.383)	Acc@5 99.561 (99.604)
Epoch: [50][10/25]	Time 0.730 (0.725)	Data 0.005 (0.064)	Loss 0.6880 (0.6494)	Acc@1 85.986 (87.256)	Acc@5 99.512 (99.596)
Epoch: [50][11/25]	Time 0.712 (0.724)	Data 0.007 (0.059)	Loss 0.6319 (0.6479)	Acc@1 87.598 (87.284)	Acc@5 99.756 (99.609)
Epoch: [50][12/25]	Time 0.733 (0.725)	Data 0.005 (0.055)	Loss 0.6412 (0.6474)	Acc@1 87.256 (87.282)	Acc@5 99.854 (99.628)
Epoch: [50][13/25]	Time 0.748 (0.727)	Data 0.009 (0.052)	Loss 0.6656 (0.6487)	Acc@1 87.207 (87.277)	Acc@5 99.609 (99.627)
Epoch: [50][14/25]	Time 0.675 (0.723)	Data 0.005 (0.048)	Loss 0.6708 (0.6502)	Acc@1 86.768 (87.243)	Acc@5 99.219 (99.600)
Epoch: [50][15/25]	Time 0.739 (0.724)	Data 0.006 (0.046)	Loss 0.6826 (0.6522)	Acc@1 86.035 (87.167)	Acc@5 99.414 (99.588)
Epoch: [50][16/25]	Time 0.746 (0.725)	Data 0.005 (0.043)	Loss 0.6548 (0.6523)	Acc@1 87.305 (87.175)	Acc@5 99.414 (99.578)
Epoch: [50][17/25]	Time 0.709 (0.725)	Data 0.006 (0.041)	Loss 0.6605 (0.6528)	Acc@1 86.914 (87.161)	Acc@5 99.561 (99.577)
Epoch: [50][18/25]	Time 0.669 (0.722)	Data 0.006 (0.039)	Loss 0.6732 (0.6539)	Acc@1 86.475 (87.125)	Acc@5 99.658 (99.581)
Epoch: [50][19/25]	Time 0.727 (0.722)	Data 0.006 (0.038)	Loss 0.6668 (0.6545)	Acc@1 85.889 (87.063)	Acc@5 99.658 (99.585)
Epoch: [50][20/25]	Time 0.744 (0.723)	Data 0.005 (0.036)	Loss 0.6741 (0.6554)	Acc@1 86.670 (87.044)	Acc@5 99.414 (99.577)
Epoch: [50][21/25]	Time 0.650 (0.720)	Data 0.004 (0.035)	Loss 0.6992 (0.6574)	Acc@1 86.035 (86.998)	Acc@5 99.170 (99.558)
Epoch: [50][22/25]	Time 0.665 (0.717)	Data 0.007 (0.034)	Loss 0.6848 (0.6586)	Acc@1 86.377 (86.971)	Acc@5 99.463 (99.554)
Epoch: [50][23/25]	Time 0.624 (0.713)	Data 0.006 (0.032)	Loss 0.7113 (0.6608)	Acc@1 85.059 (86.892)	Acc@5 99.414 (99.548)
Epoch: [50][24/25]	Time 0.380 (0.700)	Data 0.008 (0.031)	Loss 0.7040 (0.6616)	Acc@1 84.670 (86.854)	Acc@5 99.764 (99.552)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 30, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=63, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 459334 ; 487386 ; 0.9424439766427432

Epoch: [51 | 180] LR: 0.100000
Except

J:  8  ;  Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)

X Shape:  torch.Size([2048, 14, 32, 32])
Sollte nicht Conv sondern AvgPool sein 8; [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]

 
 Oops!!!: 
Linear
Traceback (most recent call last):
  File "main.py", line 721, in <module>
    main()
  File "main.py", line 314, in main
    use_gpu_num)
  File "main.py", line 448, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 916, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/functional.py", line 2021, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/functional.py", line 1840, in nll_loss
    ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
RuntimeError: 1only batches of spatial targets supported (non-empty 3D tensors) but got targets of size: : [2048]
no display found. Using non-interactive Agg backend
[5, 5, 5]
gpu id:0
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 6750
used     : 4269


Cifar10: True; cifar100: False
False
Files already downloaded and verified

Arch Num:  [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/25]	Time 0.756 (0.756)	Data 0.748 (0.748)	Loss 3.1074 (3.1074)	Acc@1 8.643 (8.643)	Acc@5 48.877 (48.877)
Epoch: [1][1/25]	Time 0.683 (0.719)	Data 0.006 (0.377)	Loss 4.2839 (3.6956)	Acc@1 9.570 (9.106)	Acc@5 50.342 (49.609)
Epoch: [1][2/25]	Time 0.786 (0.741)	Data 0.005 (0.253)	Loss 4.9183 (4.1032)	Acc@1 10.645 (9.619)	Acc@5 51.758 (50.326)
Epoch: [1][3/25]	Time 0.683 (0.727)	Data 0.005 (0.191)	Loss 4.8684 (4.2945)	Acc@1 11.572 (10.107)	Acc@5 53.369 (51.086)
Epoch: [1][4/25]	Time 0.722 (0.726)	Data 0.008 (0.154)	Loss 4.3538 (4.3063)	Acc@1 11.523 (10.391)	Acc@5 50.391 (50.947)
Epoch: [1][5/25]	Time 0.757 (0.731)	Data 0.004 (0.129)	Loss 3.7401 (4.2120)	Acc@1 10.840 (10.465)	Acc@5 49.756 (50.749)
Epoch: [1][6/25]	Time 0.749 (0.733)	Data 0.003 (0.111)	Loss 3.2728 (4.0778)	Acc@1 9.668 (10.352)	Acc@5 50.586 (50.725)
Epoch: [1][7/25]	Time 0.681 (0.727)	Data 0.005 (0.098)	Loss 3.1610 (3.9632)	Acc@1 11.279 (10.468)	Acc@5 49.414 (50.562)
Epoch: [1][8/25]	Time 0.721 (0.726)	Data 0.004 (0.088)	Loss 3.0006 (3.8562)	Acc@1 10.254 (10.444)	Acc@5 48.779 (50.363)
Epoch: [1][9/25]	Time 0.752 (0.729)	Data 0.004 (0.079)	Loss 3.0093 (3.7716)	Acc@1 11.816 (10.581)	Acc@5 49.609 (50.288)
Epoch: [1][10/25]	Time 0.737 (0.730)	Data 0.005 (0.073)	Loss 2.9580 (3.6976)	Acc@1 11.621 (10.676)	Acc@5 50.928 (50.346)
Epoch: [1][11/25]	Time 0.701 (0.727)	Data 0.004 (0.067)	Loss 2.9616 (3.6363)	Acc@1 11.182 (10.718)	Acc@5 49.463 (50.273)
Epoch: [1][12/25]	Time 0.666 (0.723)	Data 0.008 (0.062)	Loss 2.9559 (3.5839)	Acc@1 11.816 (10.802)	Acc@5 49.805 (50.237)
Epoch: [1][13/25]	Time 0.681 (0.720)	Data 0.007 (0.058)	Loss 2.9669 (3.5399)	Acc@1 11.523 (10.854)	Acc@5 50.537 (50.258)
Epoch: [1][14/25]	Time 0.763 (0.722)	Data 0.005 (0.055)	Loss 2.9721 (3.5020)	Acc@1 9.961 (10.794)	Acc@5 50.098 (50.247)
Epoch: [1][15/25]	Time 0.729 (0.723)	Data 0.006 (0.052)	Loss 2.9673 (3.4686)	Acc@1 12.744 (10.916)	Acc@5 49.512 (50.201)
Epoch: [1][16/25]	Time 0.746 (0.724)	Data 0.009 (0.049)	Loss 2.9536 (3.4383)	Acc@1 11.768 (10.966)	Acc@5 50.098 (50.195)
Epoch: [1][17/25]	Time 0.763 (0.726)	Data 0.006 (0.047)	Loss 2.9563 (3.4115)	Acc@1 10.840 (10.959)	Acc@5 51.074 (50.244)
Epoch: [1][18/25]	Time 0.762 (0.728)	Data 0.007 (0.045)	Loss 2.9646 (3.3880)	Acc@1 10.693 (10.945)	Acc@5 49.268 (50.193)
Epoch: [1][19/25]	Time 0.785 (0.731)	Data 0.006 (0.043)	Loss 2.9656 (3.3669)	Acc@1 10.596 (10.928)	Acc@5 51.318 (50.249)
Epoch: [1][20/25]	Time 0.696 (0.729)	Data 0.006 (0.041)	Loss 2.9650 (3.3477)	Acc@1 11.816 (10.970)	Acc@5 49.658 (50.221)
Epoch: [1][21/25]	Time 0.670 (0.727)	Data 0.008 (0.040)	Loss 2.9565 (3.3300)	Acc@1 11.963 (11.015)	Acc@5 52.002 (50.302)
Epoch: [1][22/25]	Time 0.684 (0.725)	Data 0.004 (0.038)	Loss 2.9573 (3.3137)	Acc@1 11.328 (11.029)	Acc@5 52.246 (50.386)
Epoch: [1][23/25]	Time 0.748 (0.726)	Data 0.005 (0.037)	Loss 2.9572 (3.2989)	Acc@1 12.061 (11.072)	Acc@5 52.734 (50.484)
Epoch: [1][24/25]	Time 0.445 (0.715)	Data 0.005 (0.035)	Loss 2.9540 (3.2930)	Acc@1 11.675 (11.082)	Acc@5 52.476 (50.518)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/25]	Time 0.751 (0.751)	Data 0.625 (0.625)	Loss 2.9513 (2.9513)	Acc@1 11.914 (11.914)	Acc@5 52.686 (52.686)
Epoch: [2][1/25]	Time 0.744 (0.748)	Data 0.004 (0.315)	Loss 2.9590 (2.9551)	Acc@1 10.986 (11.450)	Acc@5 50.977 (51.831)
Epoch: [2][2/25]	Time 0.743 (0.746)	Data 0.005 (0.212)	Loss 2.9501 (2.9535)	Acc@1 11.816 (11.572)	Acc@5 52.344 (52.002)
Epoch: [2][3/25]	Time 0.783 (0.755)	Data 0.006 (0.160)	Loss 2.9457 (2.9515)	Acc@1 11.426 (11.536)	Acc@5 53.760 (52.441)
Epoch: [2][4/25]	Time 0.690 (0.742)	Data 0.006 (0.129)	Loss 2.9448 (2.9502)	Acc@1 12.256 (11.680)	Acc@5 53.076 (52.568)
Epoch: [2][5/25]	Time 0.639 (0.725)	Data 0.006 (0.109)	Loss 2.9472 (2.9497)	Acc@1 11.768 (11.694)	Acc@5 52.246 (52.515)
Epoch: [2][6/25]	Time 0.675 (0.718)	Data 0.006 (0.094)	Loss 2.9440 (2.9489)	Acc@1 12.158 (11.761)	Acc@5 53.809 (52.699)
Epoch: [2][7/25]	Time 0.722 (0.718)	Data 0.007 (0.083)	Loss 2.9435 (2.9482)	Acc@1 11.816 (11.768)	Acc@5 52.344 (52.655)
Epoch: [2][8/25]	Time 0.770 (0.724)	Data 0.004 (0.074)	Loss 2.9444 (2.9478)	Acc@1 10.889 (11.670)	Acc@5 50.732 (52.441)
Epoch: [2][9/25]	Time 0.660 (0.718)	Data 0.006 (0.068)	Loss 2.9285 (2.9459)	Acc@1 13.916 (11.895)	Acc@5 55.615 (52.759)
Epoch: [2][10/25]	Time 0.658 (0.712)	Data 0.009 (0.062)	Loss 2.9432 (2.9456)	Acc@1 10.303 (11.750)	Acc@5 53.223 (52.801)
Epoch: [2][11/25]	Time 0.698 (0.711)	Data 0.007 (0.058)	Loss 2.9284 (2.9442)	Acc@1 12.305 (11.796)	Acc@5 53.809 (52.885)
Epoch: [2][12/25]	Time 0.766 (0.715)	Data 0.007 (0.054)	Loss 2.9312 (2.9432)	Acc@1 10.645 (11.707)	Acc@5 53.174 (52.907)
Epoch: [2][13/25]	Time 0.730 (0.716)	Data 0.005 (0.050)	Loss 2.9334 (2.9425)	Acc@1 11.865 (11.719)	Acc@5 54.541 (53.024)
Epoch: [2][14/25]	Time 0.672 (0.713)	Data 0.005 (0.047)	Loss 2.9247 (2.9413)	Acc@1 11.816 (11.725)	Acc@5 53.662 (53.066)
Epoch: [2][15/25]	Time 0.750 (0.716)	Data 0.008 (0.045)	Loss 2.9274 (2.9404)	Acc@1 10.400 (11.642)	Acc@5 54.248 (53.140)
Epoch: [2][16/25]	Time 0.737 (0.717)	Data 0.007 (0.043)	Loss 2.9205 (2.9393)	Acc@1 11.133 (11.612)	Acc@5 54.492 (53.220)
Epoch: [2][17/25]	Time 0.719 (0.717)	Data 0.005 (0.040)	Loss 2.9177 (2.9381)	Acc@1 11.182 (11.589)	Acc@5 52.295 (53.168)
Epoch: [2][18/25]	Time 0.727 (0.718)	Data 0.005 (0.039)	Loss 2.9228 (2.9373)	Acc@1 11.182 (11.567)	Acc@5 54.736 (53.251)
Epoch: [2][19/25]	Time 0.707 (0.717)	Data 0.007 (0.037)	Loss 2.9113 (2.9360)	Acc@1 11.279 (11.553)	Acc@5 52.881 (53.232)
Epoch: [2][20/25]	Time 0.782 (0.720)	Data 0.006 (0.036)	Loss 2.9074 (2.9346)	Acc@1 12.549 (11.600)	Acc@5 52.881 (53.216)
Epoch: [2][21/25]	Time 0.742 (0.721)	Data 0.006 (0.034)	Loss 2.9056 (2.9333)	Acc@1 11.035 (11.574)	Acc@5 53.320 (53.220)
Epoch: [2][22/25]	Time 0.757 (0.723)	Data 0.007 (0.033)	Loss 2.9035 (2.9320)	Acc@1 11.523 (11.572)	Acc@5 53.418 (53.229)
Epoch: [2][23/25]	Time 0.703 (0.722)	Data 0.005 (0.032)	Loss 2.8983 (2.9306)	Acc@1 12.109 (11.595)	Acc@5 54.736 (53.292)
Epoch: [2][24/25]	Time 0.376 (0.708)	Data 0.006 (0.031)	Loss 2.8966 (2.9300)	Acc@1 13.797 (11.632)	Acc@5 54.835 (53.318)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/25]	Time 0.772 (0.772)	Data 0.757 (0.757)	Loss 2.8916 (2.8916)	Acc@1 12.256 (12.256)	Acc@5 55.322 (55.322)
Epoch: [3][1/25]	Time 0.663 (0.718)	Data 0.005 (0.381)	Loss 2.8887 (2.8901)	Acc@1 13.135 (12.695)	Acc@5 54.346 (54.834)
Epoch: [3][2/25]	Time 0.634 (0.690)	Data 0.006 (0.256)	Loss 2.8891 (2.8898)	Acc@1 12.061 (12.484)	Acc@5 53.076 (54.248)
Epoch: [3][3/25]	Time 0.654 (0.681)	Data 0.005 (0.193)	Loss 2.8734 (2.8857)	Acc@1 13.477 (12.732)	Acc@5 55.566 (54.578)
Epoch: [3][4/25]	Time 0.693 (0.683)	Data 0.007 (0.156)	Loss 2.8723 (2.8830)	Acc@1 12.646 (12.715)	Acc@5 54.688 (54.600)
Epoch: [3][5/25]	Time 0.788 (0.701)	Data 0.006 (0.131)	Loss 2.8888 (2.8840)	Acc@1 12.158 (12.622)	Acc@5 53.467 (54.411)
Epoch: [3][6/25]	Time 0.727 (0.704)	Data 0.008 (0.113)	Loss 2.8871 (2.8844)	Acc@1 11.963 (12.528)	Acc@5 53.369 (54.262)
Epoch: [3][7/25]	Time 0.724 (0.707)	Data 0.004 (0.100)	Loss 2.8716 (2.8828)	Acc@1 13.330 (12.628)	Acc@5 56.250 (54.510)
Epoch: [3][8/25]	Time 0.734 (0.710)	Data 0.004 (0.089)	Loss 2.8808 (2.8826)	Acc@1 13.379 (12.712)	Acc@5 52.344 (54.270)
Epoch: [3][9/25]	Time 0.774 (0.716)	Data 0.005 (0.081)	Loss 2.8859 (2.8829)	Acc@1 11.475 (12.588)	Acc@5 53.857 (54.229)
Epoch: [3][10/25]	Time 0.746 (0.719)	Data 0.007 (0.074)	Loss 2.8732 (2.8821)	Acc@1 13.623 (12.682)	Acc@5 54.541 (54.257)
Epoch: [3][11/25]	Time 0.779 (0.724)	Data 0.005 (0.068)	Loss 2.8675 (2.8808)	Acc@1 12.061 (12.630)	Acc@5 55.420 (54.354)
Epoch: [3][12/25]	Time 0.699 (0.722)	Data 0.008 (0.064)	Loss 2.8730 (2.8802)	Acc@1 12.695 (12.635)	Acc@5 53.613 (54.297)
Epoch: [3][13/25]	Time 0.646 (0.717)	Data 0.006 (0.060)	Loss 2.8533 (2.8783)	Acc@1 13.916 (12.727)	Acc@5 56.641 (54.464)
Epoch: [3][14/25]	Time 0.681 (0.714)	Data 0.008 (0.056)	Loss 2.8675 (2.8776)	Acc@1 11.328 (12.633)	Acc@5 55.908 (54.561)
Epoch: [3][15/25]	Time 0.626 (0.709)	Data 0.007 (0.053)	Loss 2.8574 (2.8763)	Acc@1 13.086 (12.662)	Acc@5 56.152 (54.660)
Epoch: [3][16/25]	Time 0.652 (0.705)	Data 0.008 (0.050)	Loss 2.8690 (2.8759)	Acc@1 13.281 (12.698)	Acc@5 54.053 (54.624)
Epoch: [3][17/25]	Time 0.674 (0.704)	Data 0.005 (0.048)	Loss 2.8605 (2.8750)	Acc@1 12.744 (12.701)	Acc@5 55.859 (54.693)
Epoch: [3][18/25]	Time 0.614 (0.699)	Data 0.005 (0.046)	Loss 2.8534 (2.8739)	Acc@1 14.014 (12.770)	Acc@5 57.910 (54.862)
Epoch: [3][19/25]	Time 0.627 (0.695)	Data 0.006 (0.044)	Loss 2.8581 (2.8731)	Acc@1 12.402 (12.751)	Acc@5 53.516 (54.795)
Epoch: [3][20/25]	Time 0.626 (0.692)	Data 0.004 (0.042)	Loss 2.8447 (2.8718)	Acc@1 12.988 (12.763)	Acc@5 55.908 (54.848)
Epoch: [3][21/25]	Time 0.648 (0.690)	Data 0.007 (0.040)	Loss 2.8472 (2.8706)	Acc@1 13.770 (12.809)	Acc@5 55.273 (54.867)
Epoch: [3][22/25]	Time 0.631 (0.687)	Data 0.005 (0.039)	Loss 2.8486 (2.8697)	Acc@1 13.232 (12.827)	Acc@5 55.664 (54.902)
Epoch: [3][23/25]	Time 0.652 (0.686)	Data 0.005 (0.037)	Loss 2.8473 (2.8688)	Acc@1 13.623 (12.860)	Acc@5 55.420 (54.924)
Epoch: [3][24/25]	Time 0.348 (0.672)	Data 0.007 (0.036)	Loss 2.8320 (2.8681)	Acc@1 11.675 (12.840)	Acc@5 52.712 (54.886)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/25]	Time 0.703 (0.703)	Data 0.769 (0.769)	Loss 2.8367 (2.8367)	Acc@1 13.867 (13.867)	Acc@5 55.859 (55.859)
Epoch: [4][1/25]	Time 0.705 (0.704)	Data 0.005 (0.387)	Loss 2.8343 (2.8355)	Acc@1 13.428 (13.647)	Acc@5 56.348 (56.104)
Epoch: [4][2/25]	Time 0.752 (0.720)	Data 0.004 (0.259)	Loss 2.8338 (2.8349)	Acc@1 14.209 (13.835)	Acc@5 57.959 (56.722)
Epoch: [4][3/25]	Time 0.739 (0.725)	Data 0.004 (0.195)	Loss 2.8163 (2.8303)	Acc@1 14.551 (14.014)	Acc@5 56.934 (56.775)
Epoch: [4][4/25]	Time 0.749 (0.730)	Data 0.009 (0.158)	Loss 2.8293 (2.8301)	Acc@1 14.062 (14.023)	Acc@5 55.420 (56.504)
Epoch: [4][5/25]	Time 0.753 (0.734)	Data 0.004 (0.132)	Loss 2.8302 (2.8301)	Acc@1 12.402 (13.753)	Acc@5 53.320 (55.973)
Epoch: [4][6/25]	Time 0.754 (0.736)	Data 0.005 (0.114)	Loss 2.8291 (2.8299)	Acc@1 11.816 (13.477)	Acc@5 55.273 (55.873)
Epoch: [4][7/25]	Time 0.734 (0.736)	Data 0.005 (0.101)	Loss 2.8228 (2.8291)	Acc@1 13.281 (13.452)	Acc@5 57.520 (56.079)
Epoch: [4][8/25]	Time 0.760 (0.739)	Data 0.005 (0.090)	Loss 2.8066 (2.8266)	Acc@1 13.184 (13.422)	Acc@5 58.496 (56.348)
Epoch: [4][9/25]	Time 0.702 (0.735)	Data 0.005 (0.082)	Loss 2.8183 (2.8257)	Acc@1 12.793 (13.359)	Acc@5 59.229 (56.636)
Epoch: [4][10/25]	Time 0.727 (0.734)	Data 0.005 (0.075)	Loss 2.8095 (2.8243)	Acc@1 13.916 (13.410)	Acc@5 58.496 (56.805)
Epoch: [4][11/25]	Time 0.768 (0.737)	Data 0.005 (0.069)	Loss 2.8030 (2.8225)	Acc@1 14.697 (13.517)	Acc@5 58.887 (56.978)
Epoch: [4][12/25]	Time 0.721 (0.736)	Data 0.007 (0.064)	Loss 2.8206 (2.8223)	Acc@1 12.207 (13.416)	Acc@5 57.275 (57.001)
Epoch: [4][13/25]	Time 0.706 (0.734)	Data 0.004 (0.060)	Loss 2.7977 (2.8206)	Acc@1 13.721 (13.438)	Acc@5 58.936 (57.139)
Epoch: [4][14/25]	Time 0.639 (0.727)	Data 0.004 (0.056)	Loss 2.8006 (2.8193)	Acc@1 13.184 (13.421)	Acc@5 58.301 (57.217)
Epoch: [4][15/25]	Time 0.643 (0.722)	Data 0.009 (0.053)	Loss 2.8041 (2.8183)	Acc@1 13.770 (13.443)	Acc@5 58.057 (57.269)
Epoch: [4][16/25]	Time 0.651 (0.718)	Data 0.004 (0.050)	Loss 2.8102 (2.8178)	Acc@1 13.965 (13.474)	Acc@5 58.350 (57.333)
Epoch: [4][17/25]	Time 0.631 (0.713)	Data 0.006 (0.048)	Loss 2.7940 (2.8165)	Acc@1 13.867 (13.496)	Acc@5 58.496 (57.397)
Epoch: [4][18/25]	Time 0.640 (0.709)	Data 0.005 (0.046)	Loss 2.7990 (2.8156)	Acc@1 13.916 (13.518)	Acc@5 58.594 (57.460)
Epoch: [4][19/25]	Time 0.626 (0.705)	Data 0.008 (0.044)	Loss 2.8031 (2.8150)	Acc@1 13.428 (13.513)	Acc@5 56.787 (57.427)
Epoch: [4][20/25]	Time 0.653 (0.703)	Data 0.006 (0.042)	Loss 2.8072 (2.8146)	Acc@1 13.672 (13.521)	Acc@5 55.762 (57.347)
Epoch: [4][21/25]	Time 0.630 (0.699)	Data 0.007 (0.040)	Loss 2.7909 (2.8135)	Acc@1 13.770 (13.532)	Acc@5 60.449 (57.488)
Epoch: [4][22/25]	Time 0.626 (0.696)	Data 0.005 (0.039)	Loss 2.7809 (2.8121)	Acc@1 14.746 (13.585)	Acc@5 56.836 (57.460)
Epoch: [4][23/25]	Time 0.709 (0.697)	Data 0.006 (0.037)	Loss 2.7949 (2.8114)	Acc@1 13.330 (13.574)	Acc@5 57.080 (57.444)
Epoch: [4][24/25]	Time 0.462 (0.687)	Data 0.004 (0.036)	Loss 2.7646 (2.8106)	Acc@1 15.566 (13.608)	Acc@5 57.783 (57.450)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/25]	Time 0.770 (0.770)	Data 0.616 (0.616)	Loss 2.7737 (2.7737)	Acc@1 13.965 (13.965)	Acc@5 61.035 (61.035)
Epoch: [5][1/25]	Time 0.696 (0.733)	Data 0.006 (0.311)	Loss 2.7788 (2.7762)	Acc@1 15.039 (14.502)	Acc@5 59.277 (60.156)
Epoch: [5][2/25]	Time 0.680 (0.716)	Data 0.004 (0.209)	Loss 2.7821 (2.7782)	Acc@1 14.111 (14.372)	Acc@5 58.496 (59.603)
Epoch: [5][3/25]	Time 0.659 (0.702)	Data 0.007 (0.158)	Loss 2.7726 (2.7768)	Acc@1 14.160 (14.319)	Acc@5 57.959 (59.192)
Epoch: [5][4/25]	Time 0.680 (0.697)	Data 0.005 (0.128)	Loss 2.7569 (2.7728)	Acc@1 16.113 (14.678)	Acc@5 60.352 (59.424)
Epoch: [5][5/25]	Time 0.786 (0.712)	Data 0.006 (0.108)	Loss 2.7673 (2.7719)	Acc@1 13.428 (14.469)	Acc@5 59.473 (59.432)
Epoch: [5][6/25]	Time 0.696 (0.710)	Data 0.004 (0.093)	Loss 2.7535 (2.7693)	Acc@1 15.234 (14.579)	Acc@5 60.889 (59.640)
Epoch: [5][7/25]	Time 0.695 (0.708)	Data 0.004 (0.082)	Loss 2.7610 (2.7682)	Acc@1 13.965 (14.502)	Acc@5 59.717 (59.650)
Epoch: [5][8/25]	Time 0.773 (0.715)	Data 0.004 (0.073)	Loss 2.7492 (2.7661)	Acc@1 15.283 (14.589)	Acc@5 63.086 (60.031)
Epoch: [5][9/25]	Time 0.766 (0.720)	Data 0.006 (0.066)	Loss 2.7245 (2.7619)	Acc@1 16.699 (14.800)	Acc@5 66.162 (60.645)
Epoch: [5][10/25]	Time 0.677 (0.716)	Data 0.007 (0.061)	Loss 2.7123 (2.7574)	Acc@1 16.650 (14.968)	Acc@5 67.188 (61.239)
Epoch: [5][11/25]	Time 0.623 (0.708)	Data 0.007 (0.056)	Loss 2.6832 (2.7512)	Acc@1 16.211 (15.072)	Acc@5 71.484 (62.093)
Epoch: [5][12/25]	Time 0.632 (0.703)	Data 0.004 (0.052)	Loss 2.6562 (2.7439)	Acc@1 18.213 (15.313)	Acc@5 72.949 (62.928)
Epoch: [5][13/25]	Time 0.644 (0.698)	Data 0.006 (0.049)	Loss 2.6500 (2.7372)	Acc@1 18.848 (15.566)	Acc@5 71.436 (63.536)
Epoch: [5][14/25]	Time 0.638 (0.694)	Data 0.007 (0.046)	Loss 2.6613 (2.7322)	Acc@1 18.115 (15.736)	Acc@5 74.365 (64.258)
Epoch: [5][15/25]	Time 0.656 (0.692)	Data 0.006 (0.044)	Loss 2.6929 (2.7297)	Acc@1 16.992 (15.814)	Acc@5 72.754 (64.789)
Epoch: [5][16/25]	Time 0.677 (0.691)	Data 0.008 (0.042)	Loss 2.6604 (2.7256)	Acc@1 14.941 (15.763)	Acc@5 73.828 (65.321)
Epoch: [5][17/25]	Time 0.759 (0.695)	Data 0.005 (0.040)	Loss 2.6817 (2.7232)	Acc@1 15.576 (15.752)	Acc@5 69.775 (65.568)
Epoch: [5][18/25]	Time 0.735 (0.697)	Data 0.005 (0.038)	Loss 2.6481 (2.7192)	Acc@1 15.820 (15.756)	Acc@5 74.463 (66.036)
Epoch: [5][19/25]	Time 0.749 (0.700)	Data 0.006 (0.036)	Loss 2.6231 (2.7144)	Acc@1 18.896 (15.913)	Acc@5 74.707 (66.470)
Epoch: [5][20/25]	Time 0.754 (0.702)	Data 0.006 (0.035)	Loss 2.6163 (2.7098)	Acc@1 18.262 (16.025)	Acc@5 75.635 (66.906)
Epoch: [5][21/25]	Time 0.765 (0.705)	Data 0.004 (0.033)	Loss 2.5911 (2.7044)	Acc@1 17.627 (16.098)	Acc@5 77.832 (67.403)
Epoch: [5][22/25]	Time 0.724 (0.706)	Data 0.005 (0.032)	Loss 2.5863 (2.6992)	Acc@1 17.188 (16.145)	Acc@5 75.928 (67.773)
Epoch: [5][23/25]	Time 0.720 (0.706)	Data 0.004 (0.031)	Loss 2.6005 (2.6951)	Acc@1 17.480 (16.201)	Acc@5 76.758 (68.148)
Epoch: [5][24/25]	Time 0.553 (0.700)	Data 0.005 (0.030)	Loss 2.5554 (2.6927)	Acc@1 18.632 (16.242)	Acc@5 77.948 (68.314)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/25]	Time 0.688 (0.688)	Data 0.794 (0.794)	Loss 2.5373 (2.5373)	Acc@1 20.996 (20.996)	Acc@5 78.613 (78.613)
Epoch: [6][1/25]	Time 0.876 (0.782)	Data 0.004 (0.399)	Loss 2.5586 (2.5480)	Acc@1 19.922 (20.459)	Acc@5 78.467 (78.540)
Epoch: [6][2/25]	Time 0.940 (0.835)	Data 0.006 (0.268)	Loss 2.5308 (2.5423)	Acc@1 20.654 (20.524)	Acc@5 79.980 (79.020)
Epoch: [6][3/25]	Time 0.800 (0.826)	Data 0.004 (0.202)	Loss 2.5258 (2.5381)	Acc@1 18.408 (19.995)	Acc@5 80.273 (79.333)
Epoch: [6][4/25]	Time 0.753 (0.811)	Data 0.008 (0.163)	Loss 2.5032 (2.5311)	Acc@1 20.508 (20.098)	Acc@5 81.592 (79.785)
Epoch: [6][5/25]	Time 0.751 (0.801)	Data 0.005 (0.137)	Loss 2.5195 (2.5292)	Acc@1 19.873 (20.060)	Acc@5 78.369 (79.549)
Epoch: [6][6/25]	Time 0.751 (0.794)	Data 0.007 (0.118)	Loss 2.5225 (2.5282)	Acc@1 19.580 (19.992)	Acc@5 78.564 (79.408)
Epoch: [6][7/25]	Time 0.759 (0.790)	Data 0.005 (0.104)	Loss 2.5314 (2.5286)	Acc@1 19.727 (19.958)	Acc@5 78.271 (79.266)
Epoch: [6][8/25]	Time 0.724 (0.782)	Data 0.005 (0.093)	Loss 2.5405 (2.5300)	Acc@1 18.848 (19.835)	Acc@5 76.953 (79.009)
Epoch: [6][9/25]	Time 0.742 (0.778)	Data 0.004 (0.084)	Loss 2.4987 (2.5268)	Acc@1 23.193 (20.171)	Acc@5 80.078 (79.116)
Epoch: [6][10/25]	Time 0.736 (0.775)	Data 0.006 (0.077)	Loss 2.4721 (2.5219)	Acc@1 23.340 (20.459)	Acc@5 82.666 (79.439)
Epoch: [6][11/25]	Time 0.690 (0.768)	Data 0.005 (0.071)	Loss 2.4781 (2.5182)	Acc@1 21.777 (20.569)	Acc@5 81.006 (79.569)
Epoch: [6][12/25]	Time 0.718 (0.764)	Data 0.004 (0.066)	Loss 2.4960 (2.5165)	Acc@1 21.436 (20.636)	Acc@5 79.834 (79.590)
Epoch: [6][13/25]	Time 0.759 (0.763)	Data 0.006 (0.062)	Loss 2.4716 (2.5133)	Acc@1 22.217 (20.748)	Acc@5 80.762 (79.674)
Epoch: [6][14/25]	Time 0.734 (0.761)	Data 0.006 (0.058)	Loss 2.4735 (2.5106)	Acc@1 22.510 (20.866)	Acc@5 80.225 (79.710)
Epoch: [6][15/25]	Time 0.724 (0.759)	Data 0.006 (0.055)	Loss 2.4618 (2.5076)	Acc@1 21.875 (20.929)	Acc@5 79.980 (79.727)
Epoch: [6][16/25]	Time 0.772 (0.760)	Data 0.008 (0.052)	Loss 2.4690 (2.5053)	Acc@1 23.047 (21.054)	Acc@5 81.250 (79.817)
Epoch: [6][17/25]	Time 0.718 (0.758)	Data 0.005 (0.049)	Loss 2.4545 (2.5025)	Acc@1 21.875 (21.099)	Acc@5 83.496 (80.021)
Epoch: [6][18/25]	Time 0.721 (0.756)	Data 0.007 (0.047)	Loss 2.4603 (2.5003)	Acc@1 21.240 (21.107)	Acc@5 80.908 (80.068)
Epoch: [6][19/25]	Time 0.753 (0.756)	Data 0.005 (0.045)	Loss 2.4597 (2.4982)	Acc@1 20.947 (21.099)	Acc@5 80.420 (80.085)
Epoch: [6][20/25]	Time 0.732 (0.754)	Data 0.005 (0.043)	Loss 2.4535 (2.4961)	Acc@1 22.168 (21.150)	Acc@5 80.469 (80.104)
Epoch: [6][21/25]	Time 0.731 (0.753)	Data 0.006 (0.041)	Loss 2.4234 (2.4928)	Acc@1 21.582 (21.169)	Acc@5 82.227 (80.200)
Epoch: [6][22/25]	Time 0.778 (0.754)	Data 0.004 (0.040)	Loss 2.4628 (2.4915)	Acc@1 20.508 (21.140)	Acc@5 80.127 (80.197)
Epoch: [6][23/25]	Time 0.693 (0.752)	Data 0.004 (0.038)	Loss 2.4185 (2.4885)	Acc@1 23.291 (21.230)	Acc@5 81.299 (80.243)
Epoch: [6][24/25]	Time 0.398 (0.738)	Data 0.005 (0.037)	Loss 2.4326 (2.4875)	Acc@1 22.642 (21.254)	Acc@5 81.722 (80.268)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/25]	Time 0.710 (0.710)	Data 0.647 (0.647)	Loss 2.4499 (2.4499)	Acc@1 20.557 (20.557)	Acc@5 81.152 (81.152)
Epoch: [7][1/25]	Time 0.759 (0.734)	Data 0.004 (0.326)	Loss 2.4199 (2.4349)	Acc@1 20.996 (20.776)	Acc@5 82.324 (81.738)
Epoch: [7][2/25]	Time 0.790 (0.753)	Data 0.005 (0.219)	Loss 2.3953 (2.4217)	Acc@1 21.680 (21.077)	Acc@5 83.008 (82.161)
Epoch: [7][3/25]	Time 0.715 (0.744)	Data 0.006 (0.166)	Loss 2.4024 (2.4169)	Acc@1 22.168 (21.350)	Acc@5 81.982 (82.117)
Epoch: [7][4/25]	Time 0.722 (0.739)	Data 0.006 (0.134)	Loss 2.4021 (2.4139)	Acc@1 23.682 (21.816)	Acc@5 82.715 (82.236)
Epoch: [7][5/25]	Time 0.759 (0.743)	Data 0.004 (0.112)	Loss 2.3998 (2.4116)	Acc@1 21.826 (21.818)	Acc@5 82.324 (82.251)
Epoch: [7][6/25]	Time 0.697 (0.736)	Data 0.006 (0.097)	Loss 2.3828 (2.4075)	Acc@1 26.172 (22.440)	Acc@5 82.080 (82.227)
Epoch: [7][7/25]	Time 0.728 (0.735)	Data 0.006 (0.086)	Loss 2.3803 (2.4041)	Acc@1 25.098 (22.772)	Acc@5 83.203 (82.349)
Epoch: [7][8/25]	Time 0.716 (0.733)	Data 0.006 (0.077)	Loss 2.3722 (2.4005)	Acc@1 25.049 (23.025)	Acc@5 82.520 (82.368)
Epoch: [7][9/25]	Time 0.777 (0.737)	Data 0.006 (0.070)	Loss 2.3636 (2.3968)	Acc@1 24.023 (23.125)	Acc@5 83.740 (82.505)
Epoch: [7][10/25]	Time 0.693 (0.733)	Data 0.005 (0.064)	Loss 2.3735 (2.3947)	Acc@1 23.730 (23.180)	Acc@5 82.715 (82.524)
Epoch: [7][11/25]	Time 0.647 (0.726)	Data 0.005 (0.059)	Loss 2.3467 (2.3907)	Acc@1 25.439 (23.368)	Acc@5 84.424 (82.682)
Epoch: [7][12/25]	Time 0.667 (0.722)	Data 0.004 (0.055)	Loss 2.3345 (2.3864)	Acc@1 25.537 (23.535)	Acc@5 83.887 (82.775)
Epoch: [7][13/25]	Time 0.651 (0.717)	Data 0.007 (0.051)	Loss 2.3284 (2.3823)	Acc@1 25.635 (23.685)	Acc@5 84.766 (82.917)
Epoch: [7][14/25]	Time 0.633 (0.711)	Data 0.006 (0.048)	Loss 2.3409 (2.3795)	Acc@1 24.951 (23.770)	Acc@5 84.131 (82.998)
Epoch: [7][15/25]	Time 0.642 (0.707)	Data 0.005 (0.046)	Loss 2.3399 (2.3770)	Acc@1 24.854 (23.837)	Acc@5 83.984 (83.060)
Epoch: [7][16/25]	Time 0.655 (0.704)	Data 0.004 (0.043)	Loss 2.3434 (2.3750)	Acc@1 24.854 (23.897)	Acc@5 83.301 (83.074)
Epoch: [7][17/25]	Time 0.677 (0.702)	Data 0.006 (0.041)	Loss 2.3135 (2.3716)	Acc@1 26.660 (24.051)	Acc@5 84.863 (83.173)
Epoch: [7][18/25]	Time 0.681 (0.701)	Data 0.007 (0.039)	Loss 2.3192 (2.3689)	Acc@1 27.148 (24.214)	Acc@5 84.229 (83.229)
Epoch: [7][19/25]	Time 0.752 (0.704)	Data 0.005 (0.038)	Loss 2.3332 (2.3671)	Acc@1 26.562 (24.331)	Acc@5 83.350 (83.235)
Epoch: [7][20/25]	Time 0.767 (0.707)	Data 0.005 (0.036)	Loss 2.3042 (2.3641)	Acc@1 27.832 (24.498)	Acc@5 85.010 (83.319)
Epoch: [7][21/25]	Time 0.714 (0.707)	Data 0.004 (0.035)	Loss 2.2803 (2.3603)	Acc@1 28.125 (24.663)	Acc@5 84.570 (83.376)
Epoch: [7][22/25]	Time 0.695 (0.706)	Data 0.004 (0.033)	Loss 2.2894 (2.3572)	Acc@1 27.100 (24.769)	Acc@5 85.449 (83.466)
Epoch: [7][23/25]	Time 0.781 (0.710)	Data 0.007 (0.032)	Loss 2.2959 (2.3546)	Acc@1 28.369 (24.919)	Acc@5 84.131 (83.494)
Epoch: [7][24/25]	Time 0.438 (0.699)	Data 0.006 (0.031)	Loss 2.2703 (2.3532)	Acc@1 25.943 (24.936)	Acc@5 85.024 (83.520)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/25]	Time 0.718 (0.718)	Data 0.816 (0.816)	Loss 2.2953 (2.2953)	Acc@1 25.195 (25.195)	Acc@5 84.326 (84.326)
Epoch: [8][1/25]	Time 0.649 (0.683)	Data 0.005 (0.411)	Loss 2.2772 (2.2862)	Acc@1 26.807 (26.001)	Acc@5 84.375 (84.351)
Epoch: [8][2/25]	Time 0.625 (0.664)	Data 0.006 (0.276)	Loss 2.2613 (2.2779)	Acc@1 29.150 (27.051)	Acc@5 84.863 (84.521)
Epoch: [8][3/25]	Time 0.684 (0.669)	Data 0.008 (0.209)	Loss 2.2684 (2.2755)	Acc@1 27.832 (27.246)	Acc@5 84.277 (84.460)
Epoch: [8][4/25]	Time 0.616 (0.658)	Data 0.006 (0.168)	Loss 2.2558 (2.2716)	Acc@1 28.516 (27.500)	Acc@5 85.498 (84.668)
Epoch: [8][5/25]	Time 0.642 (0.656)	Data 0.004 (0.141)	Loss 2.2099 (2.2613)	Acc@1 28.516 (27.669)	Acc@5 86.084 (84.904)
Epoch: [8][6/25]	Time 0.657 (0.656)	Data 0.005 (0.121)	Loss 2.2295 (2.2568)	Acc@1 30.859 (28.125)	Acc@5 85.254 (84.954)
Epoch: [8][7/25]	Time 0.654 (0.656)	Data 0.008 (0.107)	Loss 2.2226 (2.2525)	Acc@1 31.006 (28.485)	Acc@5 85.303 (84.998)
Epoch: [8][8/25]	Time 0.641 (0.654)	Data 0.006 (0.096)	Loss 2.2152 (2.2483)	Acc@1 29.834 (28.635)	Acc@5 86.133 (85.124)
Epoch: [8][9/25]	Time 0.615 (0.650)	Data 0.010 (0.087)	Loss 2.1953 (2.2430)	Acc@1 31.689 (28.940)	Acc@5 86.914 (85.303)
Epoch: [8][10/25]	Time 0.647 (0.650)	Data 0.004 (0.080)	Loss 2.1495 (2.2345)	Acc@1 32.129 (29.230)	Acc@5 87.939 (85.542)
Epoch: [8][11/25]	Time 0.637 (0.649)	Data 0.005 (0.074)	Loss 2.1748 (2.2296)	Acc@1 32.422 (29.496)	Acc@5 86.768 (85.645)
Epoch: [8][12/25]	Time 0.672 (0.651)	Data 0.004 (0.068)	Loss 2.2023 (2.2275)	Acc@1 31.689 (29.665)	Acc@5 85.889 (85.663)
Epoch: [8][13/25]	Time 0.675 (0.652)	Data 0.005 (0.064)	Loss 2.1566 (2.2224)	Acc@1 34.082 (29.980)	Acc@5 86.182 (85.700)
Epoch: [8][14/25]	Time 0.704 (0.656)	Data 0.004 (0.060)	Loss 2.1456 (2.2173)	Acc@1 32.324 (30.137)	Acc@5 87.451 (85.817)
Epoch: [8][15/25]	Time 0.753 (0.662)	Data 0.004 (0.056)	Loss 2.1683 (2.2142)	Acc@1 33.643 (30.356)	Acc@5 85.547 (85.800)
Epoch: [8][16/25]	Time 0.686 (0.663)	Data 0.005 (0.053)	Loss 2.1813 (2.2123)	Acc@1 32.031 (30.454)	Acc@5 85.645 (85.791)
Epoch: [8][17/25]	Time 0.734 (0.667)	Data 0.008 (0.051)	Loss 2.1811 (2.2106)	Acc@1 32.812 (30.585)	Acc@5 86.035 (85.805)
Epoch: [8][18/25]	Time 0.757 (0.672)	Data 0.006 (0.048)	Loss 2.1101 (2.2053)	Acc@1 35.254 (30.831)	Acc@5 87.695 (85.904)
Epoch: [8][19/25]	Time 0.779 (0.677)	Data 0.004 (0.046)	Loss 2.1068 (2.2003)	Acc@1 33.936 (30.986)	Acc@5 86.914 (85.955)
Epoch: [8][20/25]	Time 0.759 (0.681)	Data 0.005 (0.044)	Loss 2.1314 (2.1971)	Acc@1 33.740 (31.117)	Acc@5 87.061 (86.007)
Epoch: [8][21/25]	Time 0.721 (0.683)	Data 0.006 (0.042)	Loss 2.1136 (2.1933)	Acc@1 32.666 (31.188)	Acc@5 87.695 (86.084)
Epoch: [8][22/25]	Time 0.727 (0.685)	Data 0.005 (0.041)	Loss 2.1063 (2.1895)	Acc@1 32.617 (31.250)	Acc@5 87.402 (86.141)
Epoch: [8][23/25]	Time 0.762 (0.688)	Data 0.008 (0.039)	Loss 2.0889 (2.1853)	Acc@1 34.473 (31.384)	Acc@5 88.721 (86.249)
Epoch: [8][24/25]	Time 0.451 (0.679)	Data 0.004 (0.038)	Loss 2.1188 (2.1842)	Acc@1 32.901 (31.410)	Acc@5 85.613 (86.238)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/25]	Time 0.719 (0.719)	Data 0.676 (0.676)	Loss 2.1532 (2.1532)	Acc@1 33.496 (33.496)	Acc@5 85.791 (85.791)
Epoch: [9][1/25]	Time 0.699 (0.709)	Data 0.005 (0.341)	Loss 2.0954 (2.1243)	Acc@1 34.473 (33.984)	Acc@5 89.062 (87.427)
Epoch: [9][2/25]	Time 0.776 (0.731)	Data 0.008 (0.230)	Loss 2.0909 (2.1132)	Acc@1 34.814 (34.261)	Acc@5 87.549 (87.467)
Epoch: [9][3/25]	Time 0.754 (0.737)	Data 0.003 (0.173)	Loss 2.1278 (2.1168)	Acc@1 33.740 (34.131)	Acc@5 87.207 (87.402)
Epoch: [9][4/25]	Time 0.715 (0.733)	Data 0.008 (0.140)	Loss 2.0913 (2.1117)	Acc@1 34.766 (34.258)	Acc@5 87.793 (87.480)
Epoch: [9][5/25]	Time 0.765 (0.738)	Data 0.006 (0.118)	Loss 2.1028 (2.1102)	Acc@1 35.107 (34.399)	Acc@5 87.256 (87.443)
Epoch: [9][6/25]	Time 0.700 (0.733)	Data 0.005 (0.102)	Loss 2.0779 (2.1056)	Acc@1 35.107 (34.501)	Acc@5 87.012 (87.381)
Epoch: [9][7/25]	Time 0.712 (0.730)	Data 0.003 (0.089)	Loss 2.0630 (2.1003)	Acc@1 36.279 (34.723)	Acc@5 87.451 (87.390)
Epoch: [9][8/25]	Time 0.748 (0.732)	Data 0.005 (0.080)	Loss 2.0410 (2.0937)	Acc@1 36.816 (34.956)	Acc@5 89.160 (87.587)
Epoch: [9][9/25]	Time 0.750 (0.734)	Data 0.005 (0.073)	Loss 2.0866 (2.0930)	Acc@1 33.936 (34.854)	Acc@5 88.086 (87.637)
Epoch: [9][10/25]	Time 0.733 (0.734)	Data 0.008 (0.067)	Loss 2.0565 (2.0897)	Acc@1 36.084 (34.965)	Acc@5 87.793 (87.651)
Epoch: [9][11/25]	Time 0.727 (0.733)	Data 0.006 (0.062)	Loss 2.0108 (2.0831)	Acc@1 36.963 (35.132)	Acc@5 90.039 (87.850)
Epoch: [9][12/25]	Time 0.719 (0.732)	Data 0.005 (0.057)	Loss 2.0162 (2.0779)	Acc@1 35.693 (35.175)	Acc@5 89.453 (87.973)
Epoch: [9][13/25]	Time 0.693 (0.729)	Data 0.005 (0.054)	Loss 2.0836 (2.0783)	Acc@1 35.547 (35.202)	Acc@5 87.354 (87.929)
Epoch: [9][14/25]	Time 0.696 (0.727)	Data 0.006 (0.050)	Loss 2.0364 (2.0756)	Acc@1 35.693 (35.234)	Acc@5 89.062 (88.005)
Epoch: [9][15/25]	Time 0.714 (0.726)	Data 0.004 (0.048)	Loss 2.0171 (2.0719)	Acc@1 35.938 (35.278)	Acc@5 89.941 (88.126)
Epoch: [9][16/25]	Time 0.771 (0.729)	Data 0.007 (0.045)	Loss 2.0173 (2.0687)	Acc@1 36.377 (35.343)	Acc@5 89.502 (88.207)
Epoch: [9][17/25]	Time 0.768 (0.731)	Data 0.007 (0.043)	Loss 2.0049 (2.0651)	Acc@1 37.451 (35.460)	Acc@5 89.648 (88.287)
Epoch: [9][18/25]	Time 0.741 (0.732)	Data 0.004 (0.041)	Loss 2.0357 (2.0636)	Acc@1 37.354 (35.560)	Acc@5 88.135 (88.279)
Epoch: [9][19/25]	Time 0.748 (0.733)	Data 0.004 (0.039)	Loss 2.0213 (2.0615)	Acc@1 38.086 (35.686)	Acc@5 89.453 (88.337)
Epoch: [9][20/25]	Time 0.780 (0.735)	Data 0.005 (0.037)	Loss 2.0226 (2.0596)	Acc@1 36.816 (35.740)	Acc@5 88.721 (88.356)
Epoch: [9][21/25]	Time 0.685 (0.733)	Data 0.004 (0.036)	Loss 1.9962 (2.0567)	Acc@1 36.865 (35.791)	Acc@5 88.867 (88.379)
Epoch: [9][22/25]	Time 0.656 (0.729)	Data 0.006 (0.035)	Loss 1.9878 (2.0537)	Acc@1 38.818 (35.923)	Acc@5 89.404 (88.423)
Epoch: [9][23/25]	Time 0.668 (0.727)	Data 0.007 (0.033)	Loss 1.9752 (2.0505)	Acc@1 38.037 (36.011)	Acc@5 88.965 (88.446)
Epoch: [9][24/25]	Time 0.352 (0.712)	Data 0.006 (0.032)	Loss 2.0165 (2.0499)	Acc@1 35.142 (35.996)	Acc@5 88.325 (88.444)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/25]	Time 0.768 (0.768)	Data 0.613 (0.613)	Loss 2.0124 (2.0124)	Acc@1 36.426 (36.426)	Acc@5 88.281 (88.281)
Epoch: [10][1/25]	Time 0.734 (0.751)	Data 0.012 (0.313)	Loss 1.9823 (1.9973)	Acc@1 37.744 (37.085)	Acc@5 90.430 (89.355)
Epoch: [10][2/25]	Time 0.645 (0.715)	Data 0.004 (0.210)	Loss 1.9914 (1.9954)	Acc@1 36.279 (36.816)	Acc@5 88.965 (89.225)
Epoch: [10][3/25]	Time 0.749 (0.724)	Data 0.006 (0.159)	Loss 1.9506 (1.9842)	Acc@1 39.307 (37.439)	Acc@5 90.088 (89.441)
Epoch: [10][4/25]	Time 0.753 (0.730)	Data 0.005 (0.128)	Loss 1.9671 (1.9807)	Acc@1 39.355 (37.822)	Acc@5 89.307 (89.414)
Epoch: [10][5/25]	Time 0.707 (0.726)	Data 0.006 (0.108)	Loss 1.9573 (1.9768)	Acc@1 39.258 (38.062)	Acc@5 89.209 (89.380)
Epoch: [10][6/25]	Time 0.626 (0.712)	Data 0.005 (0.093)	Loss 1.9319 (1.9704)	Acc@1 40.088 (38.351)	Acc@5 89.990 (89.467)
Epoch: [10][7/25]	Time 0.643 (0.703)	Data 0.005 (0.082)	Loss 1.9409 (1.9667)	Acc@1 40.576 (38.629)	Acc@5 89.893 (89.520)
Epoch: [10][8/25]	Time 0.671 (0.700)	Data 0.006 (0.074)	Loss 1.9509 (1.9650)	Acc@1 39.600 (38.737)	Acc@5 90.332 (89.610)
Epoch: [10][9/25]	Time 0.714 (0.701)	Data 0.008 (0.067)	Loss 1.9625 (1.9647)	Acc@1 37.793 (38.643)	Acc@5 88.379 (89.487)
Epoch: [10][10/25]	Time 0.754 (0.706)	Data 0.008 (0.062)	Loss 1.9128 (1.9600)	Acc@1 42.334 (38.978)	Acc@5 90.381 (89.569)
Epoch: [10][11/25]	Time 0.781 (0.712)	Data 0.005 (0.057)	Loss 1.9314 (1.9576)	Acc@1 39.355 (39.010)	Acc@5 90.625 (89.657)
Epoch: [10][12/25]	Time 0.738 (0.714)	Data 0.006 (0.053)	Loss 1.9357 (1.9559)	Acc@1 39.404 (39.040)	Acc@5 90.381 (89.712)
Epoch: [10][13/25]	Time 0.699 (0.713)	Data 0.006 (0.050)	Loss 1.8984 (1.9518)	Acc@1 41.113 (39.188)	Acc@5 91.357 (89.830)
Epoch: [10][14/25]	Time 0.787 (0.718)	Data 0.005 (0.047)	Loss 1.9256 (1.9501)	Acc@1 41.016 (39.310)	Acc@5 90.137 (89.850)
Epoch: [10][15/25]	Time 0.695 (0.716)	Data 0.007 (0.044)	Loss 1.9294 (1.9488)	Acc@1 40.234 (39.368)	Acc@5 89.551 (89.832)
Epoch: [10][16/25]	Time 0.703 (0.716)	Data 0.007 (0.042)	Loss 1.9139 (1.9467)	Acc@1 40.479 (39.433)	Acc@5 90.527 (89.872)
Epoch: [10][17/25]	Time 0.763 (0.718)	Data 0.006 (0.040)	Loss 1.8754 (1.9428)	Acc@1 42.432 (39.600)	Acc@5 91.943 (89.988)
Epoch: [10][18/25]	Time 0.734 (0.719)	Data 0.005 (0.038)	Loss 1.8803 (1.9395)	Acc@1 41.699 (39.710)	Acc@5 90.625 (90.021)
Epoch: [10][19/25]	Time 0.752 (0.721)	Data 0.006 (0.037)	Loss 1.9034 (1.9377)	Acc@1 42.041 (39.827)	Acc@5 91.406 (90.090)
Epoch: [10][20/25]	Time 0.775 (0.723)	Data 0.006 (0.035)	Loss 1.8540 (1.9337)	Acc@1 43.066 (39.981)	Acc@5 90.918 (90.130)
Epoch: [10][21/25]	Time 0.711 (0.723)	Data 0.005 (0.034)	Loss 1.9156 (1.9329)	Acc@1 41.699 (40.059)	Acc@5 90.967 (90.168)
Epoch: [10][22/25]	Time 0.731 (0.723)	Data 0.005 (0.033)	Loss 1.8408 (1.9289)	Acc@1 43.359 (40.203)	Acc@5 91.699 (90.234)
Epoch: [10][23/25]	Time 0.794 (0.726)	Data 0.008 (0.032)	Loss 1.8613 (1.9261)	Acc@1 42.822 (40.312)	Acc@5 90.723 (90.255)
Epoch: [10][24/25]	Time 0.406 (0.713)	Data 0.004 (0.030)	Loss 1.8247 (1.9243)	Acc@1 45.991 (40.408)	Acc@5 90.920 (90.266)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/25]	Time 0.718 (0.718)	Data 0.671 (0.671)	Loss 1.8584 (1.8584)	Acc@1 43.018 (43.018)	Acc@5 90.771 (90.771)
Epoch: [11][1/25]	Time 0.725 (0.721)	Data 0.004 (0.338)	Loss 1.7867 (1.8225)	Acc@1 45.752 (44.385)	Acc@5 92.041 (91.406)
Epoch: [11][2/25]	Time 0.765 (0.736)	Data 0.006 (0.227)	Loss 1.8078 (1.8176)	Acc@1 45.068 (44.613)	Acc@5 91.650 (91.488)
Epoch: [11][3/25]	Time 0.675 (0.721)	Data 0.006 (0.172)	Loss 1.8422 (1.8238)	Acc@1 42.725 (44.141)	Acc@5 90.771 (91.309)
Epoch: [11][4/25]	Time 0.715 (0.719)	Data 0.008 (0.139)	Loss 1.8485 (1.8287)	Acc@1 42.236 (43.760)	Acc@5 91.602 (91.367)
Epoch: [11][5/25]	Time 0.766 (0.727)	Data 0.006 (0.117)	Loss 1.8144 (1.8263)	Acc@1 45.654 (44.076)	Acc@5 91.748 (91.431)
Epoch: [11][6/25]	Time 0.686 (0.721)	Data 0.004 (0.101)	Loss 1.7964 (1.8221)	Acc@1 45.117 (44.224)	Acc@5 92.090 (91.525)
Epoch: [11][7/25]	Time 0.663 (0.714)	Data 0.004 (0.089)	Loss 1.8163 (1.8213)	Acc@1 44.287 (44.232)	Acc@5 92.627 (91.663)
Epoch: [11][8/25]	Time 0.688 (0.711)	Data 0.008 (0.080)	Loss 1.7698 (1.8156)	Acc@1 46.436 (44.477)	Acc@5 92.480 (91.753)
Epoch: [11][9/25]	Time 0.781 (0.718)	Data 0.008 (0.072)	Loss 1.7905 (1.8131)	Acc@1 44.580 (44.487)	Acc@5 91.992 (91.777)
Epoch: [11][10/25]	Time 0.742 (0.720)	Data 0.004 (0.066)	Loss 1.7626 (1.8085)	Acc@1 46.875 (44.704)	Acc@5 92.041 (91.801)
Epoch: [11][11/25]	Time 0.750 (0.723)	Data 0.006 (0.061)	Loss 1.7870 (1.8067)	Acc@1 46.582 (44.861)	Acc@5 91.992 (91.817)
Epoch: [11][12/25]	Time 0.739 (0.724)	Data 0.008 (0.057)	Loss 1.7814 (1.8048)	Acc@1 45.605 (44.918)	Acc@5 91.113 (91.763)
Epoch: [11][13/25]	Time 0.776 (0.728)	Data 0.009 (0.054)	Loss 1.7973 (1.8042)	Acc@1 44.189 (44.866)	Acc@5 93.115 (91.860)
Epoch: [11][14/25]	Time 0.679 (0.724)	Data 0.004 (0.050)	Loss 1.7762 (1.8024)	Acc@1 45.508 (44.909)	Acc@5 92.139 (91.878)
Epoch: [11][15/25]	Time 0.694 (0.723)	Data 0.004 (0.047)	Loss 1.7662 (1.8001)	Acc@1 46.240 (44.992)	Acc@5 93.506 (91.980)
Epoch: [11][16/25]	Time 0.770 (0.725)	Data 0.009 (0.045)	Loss 1.7668 (1.7981)	Acc@1 47.656 (45.149)	Acc@5 91.406 (91.946)
Epoch: [11][17/25]	Time 0.702 (0.724)	Data 0.005 (0.043)	Loss 1.8273 (1.7998)	Acc@1 44.824 (45.131)	Acc@5 90.332 (91.857)
Epoch: [11][18/25]	Time 0.673 (0.721)	Data 0.005 (0.041)	Loss 1.8164 (1.8006)	Acc@1 45.215 (45.135)	Acc@5 91.797 (91.853)
Epoch: [11][19/25]	Time 0.673 (0.719)	Data 0.005 (0.039)	Loss 1.8160 (1.8014)	Acc@1 43.457 (45.051)	Acc@5 91.650 (91.843)
Epoch: [11][20/25]	Time 0.753 (0.720)	Data 0.006 (0.038)	Loss 1.7704 (1.7999)	Acc@1 45.557 (45.075)	Acc@5 91.650 (91.834)
Epoch: [11][21/25]	Time 0.741 (0.721)	Data 0.005 (0.036)	Loss 1.7833 (1.7992)	Acc@1 45.996 (45.117)	Acc@5 92.041 (91.843)
Epoch: [11][22/25]	Time 0.745 (0.722)	Data 0.006 (0.035)	Loss 1.7412 (1.7967)	Acc@1 46.973 (45.198)	Acc@5 93.506 (91.916)
Epoch: [11][23/25]	Time 0.738 (0.723)	Data 0.005 (0.033)	Loss 1.6871 (1.7921)	Acc@1 49.902 (45.394)	Acc@5 94.043 (92.004)
Epoch: [11][24/25]	Time 0.506 (0.714)	Data 0.007 (0.032)	Loss 1.7347 (1.7911)	Acc@1 48.231 (45.442)	Acc@5 93.042 (92.022)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/25]	Time 0.795 (0.795)	Data 0.719 (0.719)	Loss 1.7328 (1.7328)	Acc@1 47.217 (47.217)	Acc@5 92.725 (92.725)
Epoch: [12][1/25]	Time 0.711 (0.753)	Data 0.006 (0.362)	Loss 1.7357 (1.7343)	Acc@1 46.729 (46.973)	Acc@5 92.969 (92.847)
Epoch: [12][2/25]	Time 0.698 (0.735)	Data 0.006 (0.243)	Loss 1.7660 (1.7449)	Acc@1 44.727 (46.224)	Acc@5 92.578 (92.757)
Epoch: [12][3/25]	Time 0.771 (0.744)	Data 0.006 (0.184)	Loss 1.7468 (1.7453)	Acc@1 47.021 (46.423)	Acc@5 93.018 (92.822)
Epoch: [12][4/25]	Time 0.727 (0.740)	Data 0.008 (0.149)	Loss 1.6982 (1.7359)	Acc@1 49.414 (47.021)	Acc@5 94.482 (93.154)
Epoch: [12][5/25]	Time 0.732 (0.739)	Data 0.007 (0.125)	Loss 1.7171 (1.7328)	Acc@1 48.340 (47.241)	Acc@5 93.262 (93.172)
Epoch: [12][6/25]	Time 0.735 (0.738)	Data 0.008 (0.109)	Loss 1.7235 (1.7315)	Acc@1 48.535 (47.426)	Acc@5 92.676 (93.101)
Epoch: [12][7/25]	Time 0.753 (0.740)	Data 0.003 (0.095)	Loss 1.7057 (1.7282)	Acc@1 49.268 (47.656)	Acc@5 93.066 (93.097)
Epoch: [12][8/25]	Time 0.719 (0.738)	Data 0.005 (0.085)	Loss 1.7098 (1.7262)	Acc@1 50.293 (47.949)	Acc@5 92.383 (93.018)
Epoch: [12][9/25]	Time 0.751 (0.739)	Data 0.004 (0.077)	Loss 1.6743 (1.7210)	Acc@1 50.244 (48.179)	Acc@5 92.773 (92.993)
Epoch: [12][10/25]	Time 0.758 (0.741)	Data 0.005 (0.071)	Loss 1.6791 (1.7172)	Acc@1 48.633 (48.220)	Acc@5 92.920 (92.987)
Epoch: [12][11/25]	Time 0.781 (0.744)	Data 0.004 (0.065)	Loss 1.6447 (1.7112)	Acc@1 51.025 (48.454)	Acc@5 94.238 (93.091)
Epoch: [12][12/25]	Time 0.691 (0.740)	Data 0.005 (0.060)	Loss 1.6555 (1.7069)	Acc@1 48.926 (48.490)	Acc@5 92.920 (93.078)
Epoch: [12][13/25]	Time 0.739 (0.740)	Data 0.005 (0.056)	Loss 1.6376 (1.7019)	Acc@1 49.658 (48.574)	Acc@5 93.945 (93.140)
Epoch: [12][14/25]	Time 0.744 (0.740)	Data 0.006 (0.053)	Loss 1.6157 (1.6962)	Acc@1 49.072 (48.607)	Acc@5 94.238 (93.213)
Epoch: [12][15/25]	Time 0.757 (0.741)	Data 0.006 (0.050)	Loss 1.6640 (1.6942)	Acc@1 50.195 (48.706)	Acc@5 93.506 (93.231)
Epoch: [12][16/25]	Time 0.719 (0.740)	Data 0.008 (0.048)	Loss 1.6898 (1.6939)	Acc@1 47.656 (48.644)	Acc@5 94.678 (93.316)
Epoch: [12][17/25]	Time 0.730 (0.739)	Data 0.004 (0.045)	Loss 1.6690 (1.6925)	Acc@1 49.756 (48.706)	Acc@5 93.164 (93.308)
Epoch: [12][18/25]	Time 0.715 (0.738)	Data 0.004 (0.043)	Loss 1.7275 (1.6944)	Acc@1 47.363 (48.635)	Acc@5 93.115 (93.298)
Epoch: [12][19/25]	Time 0.751 (0.739)	Data 0.004 (0.041)	Loss 1.6280 (1.6911)	Acc@1 51.367 (48.772)	Acc@5 93.896 (93.328)
Epoch: [12][20/25]	Time 0.742 (0.739)	Data 0.007 (0.040)	Loss 1.6824 (1.6906)	Acc@1 50.098 (48.835)	Acc@5 92.432 (93.285)
Epoch: [12][21/25]	Time 0.781 (0.741)	Data 0.006 (0.038)	Loss 1.6548 (1.6890)	Acc@1 49.512 (48.866)	Acc@5 93.164 (93.279)
Epoch: [12][22/25]	Time 0.763 (0.742)	Data 0.005 (0.037)	Loss 1.6765 (1.6885)	Acc@1 50.293 (48.928)	Acc@5 92.725 (93.255)
Epoch: [12][23/25]	Time 0.702 (0.740)	Data 0.005 (0.035)	Loss 1.6471 (1.6868)	Acc@1 49.951 (48.971)	Acc@5 93.945 (93.284)
Epoch: [12][24/25]	Time 0.350 (0.725)	Data 0.005 (0.034)	Loss 1.5635 (1.6847)	Acc@1 53.066 (49.040)	Acc@5 94.929 (93.312)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/25]	Time 0.777 (0.777)	Data 0.662 (0.662)	Loss 1.6146 (1.6146)	Acc@1 52.734 (52.734)	Acc@5 93.945 (93.945)
Epoch: [13][1/25]	Time 0.727 (0.752)	Data 0.012 (0.337)	Loss 1.6034 (1.6090)	Acc@1 51.562 (52.148)	Acc@5 93.604 (93.774)
Epoch: [13][2/25]	Time 0.729 (0.744)	Data 0.003 (0.226)	Loss 1.5912 (1.6031)	Acc@1 53.076 (52.458)	Acc@5 95.068 (94.206)
Epoch: [13][3/25]	Time 0.803 (0.759)	Data 0.004 (0.170)	Loss 1.5909 (1.6001)	Acc@1 52.734 (52.527)	Acc@5 94.580 (94.299)
Epoch: [13][4/25]	Time 0.672 (0.742)	Data 0.006 (0.138)	Loss 1.5948 (1.5990)	Acc@1 51.221 (52.266)	Acc@5 94.580 (94.355)
Epoch: [13][5/25]	Time 0.669 (0.730)	Data 0.008 (0.116)	Loss 1.6232 (1.6030)	Acc@1 50.928 (52.043)	Acc@5 94.043 (94.303)
Epoch: [13][6/25]	Time 0.683 (0.723)	Data 0.008 (0.100)	Loss 1.5739 (1.5989)	Acc@1 52.100 (52.051)	Acc@5 95.068 (94.413)
Epoch: [13][7/25]	Time 0.744 (0.725)	Data 0.004 (0.088)	Loss 1.5577 (1.5937)	Acc@1 53.467 (52.228)	Acc@5 94.824 (94.464)
Epoch: [13][8/25]	Time 0.757 (0.729)	Data 0.006 (0.079)	Loss 1.5610 (1.5901)	Acc@1 53.125 (52.327)	Acc@5 94.238 (94.439)
Epoch: [13][9/25]	Time 0.664 (0.723)	Data 0.006 (0.072)	Loss 1.5673 (1.5878)	Acc@1 52.637 (52.358)	Acc@5 94.678 (94.463)
Epoch: [13][10/25]	Time 0.654 (0.716)	Data 0.007 (0.066)	Loss 1.5173 (1.5814)	Acc@1 53.418 (52.455)	Acc@5 94.971 (94.509)
Epoch: [13][11/25]	Time 0.629 (0.709)	Data 0.010 (0.061)	Loss 1.5771 (1.5810)	Acc@1 52.783 (52.482)	Acc@5 93.750 (94.446)
Epoch: [13][12/25]	Time 0.658 (0.705)	Data 0.009 (0.057)	Loss 1.6136 (1.5836)	Acc@1 51.465 (52.404)	Acc@5 93.652 (94.385)
Epoch: [13][13/25]	Time 0.619 (0.699)	Data 0.008 (0.054)	Loss 1.5683 (1.5825)	Acc@1 55.322 (52.612)	Acc@5 93.994 (94.357)
Epoch: [13][14/25]	Time 0.619 (0.694)	Data 0.009 (0.051)	Loss 1.5247 (1.5786)	Acc@1 56.396 (52.865)	Acc@5 94.238 (94.349)
Epoch: [13][15/25]	Time 0.646 (0.691)	Data 0.006 (0.048)	Loss 1.5231 (1.5751)	Acc@1 55.566 (53.033)	Acc@5 94.189 (94.339)
Epoch: [13][16/25]	Time 0.630 (0.687)	Data 0.008 (0.046)	Loss 1.5298 (1.5725)	Acc@1 54.248 (53.105)	Acc@5 94.287 (94.336)
Epoch: [13][17/25]	Time 0.669 (0.686)	Data 0.007 (0.043)	Loss 1.4653 (1.5665)	Acc@1 56.494 (53.293)	Acc@5 95.654 (94.409)
Epoch: [13][18/25]	Time 0.639 (0.684)	Data 0.009 (0.042)	Loss 1.4615 (1.5610)	Acc@1 57.617 (53.521)	Acc@5 95.166 (94.449)
Epoch: [13][19/25]	Time 0.641 (0.682)	Data 0.007 (0.040)	Loss 1.5579 (1.5608)	Acc@1 55.322 (53.611)	Acc@5 93.701 (94.412)
Epoch: [13][20/25]	Time 0.623 (0.679)	Data 0.007 (0.038)	Loss 1.5309 (1.5594)	Acc@1 56.299 (53.739)	Acc@5 94.580 (94.420)
Epoch: [13][21/25]	Time 0.625 (0.676)	Data 0.006 (0.037)	Loss 1.4873 (1.5561)	Acc@1 56.543 (53.866)	Acc@5 94.287 (94.414)
Epoch: [13][22/25]	Time 0.646 (0.675)	Data 0.006 (0.035)	Loss 1.5916 (1.5577)	Acc@1 52.588 (53.811)	Acc@5 94.238 (94.406)
Epoch: [13][23/25]	Time 0.631 (0.673)	Data 0.007 (0.034)	Loss 1.5914 (1.5591)	Acc@1 52.539 (53.758)	Acc@5 95.361 (94.446)
Epoch: [13][24/25]	Time 0.351 (0.660)	Data 0.007 (0.033)	Loss 1.5314 (1.5586)	Acc@1 54.245 (53.766)	Acc@5 93.042 (94.422)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/25]	Time 0.690 (0.690)	Data 0.610 (0.610)	Loss 1.4798 (1.4798)	Acc@1 56.299 (56.299)	Acc@5 95.557 (95.557)
Epoch: [14][1/25]	Time 0.720 (0.705)	Data 0.008 (0.309)	Loss 1.5348 (1.5073)	Acc@1 54.492 (55.396)	Acc@5 94.189 (94.873)
Epoch: [14][2/25]	Time 0.734 (0.715)	Data 0.005 (0.207)	Loss 1.5451 (1.5199)	Acc@1 54.932 (55.241)	Acc@5 94.531 (94.759)
Epoch: [14][3/25]	Time 0.784 (0.732)	Data 0.006 (0.157)	Loss 1.4456 (1.5013)	Acc@1 58.887 (56.152)	Acc@5 94.775 (94.763)
Epoch: [14][4/25]	Time 0.695 (0.724)	Data 0.007 (0.127)	Loss 1.4887 (1.4988)	Acc@1 55.273 (55.977)	Acc@5 95.801 (94.971)
Epoch: [14][5/25]	Time 0.759 (0.730)	Data 0.008 (0.107)	Loss 1.4655 (1.4933)	Acc@1 58.008 (56.315)	Acc@5 95.850 (95.117)
Epoch: [14][6/25]	Time 0.713 (0.728)	Data 0.004 (0.093)	Loss 1.4482 (1.4868)	Acc@1 57.959 (56.550)	Acc@5 95.557 (95.180)
Epoch: [14][7/25]	Time 0.732 (0.728)	Data 0.008 (0.082)	Loss 1.4386 (1.4808)	Acc@1 58.057 (56.738)	Acc@5 95.801 (95.258)
Epoch: [14][8/25]	Time 0.708 (0.726)	Data 0.008 (0.074)	Loss 1.4449 (1.4768)	Acc@1 57.080 (56.776)	Acc@5 95.117 (95.242)
Epoch: [14][9/25]	Time 0.690 (0.723)	Data 0.005 (0.067)	Loss 1.4530 (1.4744)	Acc@1 57.129 (56.812)	Acc@5 95.264 (95.244)
Epoch: [14][10/25]	Time 0.775 (0.727)	Data 0.006 (0.061)	Loss 1.4007 (1.4677)	Acc@1 60.156 (57.116)	Acc@5 95.508 (95.268)
Epoch: [14][11/25]	Time 0.727 (0.727)	Data 0.005 (0.057)	Loss 1.4115 (1.4630)	Acc@1 58.545 (57.235)	Acc@5 95.654 (95.300)
Epoch: [14][12/25]	Time 0.729 (0.727)	Data 0.005 (0.053)	Loss 1.4338 (1.4608)	Acc@1 57.812 (57.279)	Acc@5 96.533 (95.395)
Epoch: [14][13/25]	Time 0.744 (0.729)	Data 0.007 (0.049)	Loss 1.4152 (1.4575)	Acc@1 57.861 (57.321)	Acc@5 96.094 (95.445)
Epoch: [14][14/25]	Time 0.761 (0.731)	Data 0.005 (0.046)	Loss 1.4085 (1.4543)	Acc@1 59.229 (57.448)	Acc@5 95.361 (95.439)
Epoch: [14][15/25]	Time 0.693 (0.728)	Data 0.004 (0.044)	Loss 1.3864 (1.4500)	Acc@1 59.424 (57.571)	Acc@5 96.631 (95.514)
Epoch: [14][16/25]	Time 0.650 (0.724)	Data 0.005 (0.042)	Loss 1.3666 (1.4451)	Acc@1 61.572 (57.807)	Acc@5 96.045 (95.545)
Epoch: [14][17/25]	Time 0.684 (0.722)	Data 0.006 (0.040)	Loss 1.3666 (1.4408)	Acc@1 60.938 (57.981)	Acc@5 96.338 (95.589)
Epoch: [14][18/25]	Time 0.763 (0.724)	Data 0.006 (0.038)	Loss 1.4014 (1.4387)	Acc@1 59.717 (58.072)	Acc@5 96.240 (95.623)
Epoch: [14][19/25]	Time 0.754 (0.725)	Data 0.005 (0.036)	Loss 1.3425 (1.4339)	Acc@1 61.523 (58.245)	Acc@5 96.289 (95.657)
Epoch: [14][20/25]	Time 0.689 (0.723)	Data 0.005 (0.035)	Loss 1.3539 (1.4301)	Acc@1 61.035 (58.378)	Acc@5 96.533 (95.698)
Epoch: [14][21/25]	Time 0.934 (0.733)	Data 0.006 (0.033)	Loss 1.3418 (1.4261)	Acc@1 61.963 (58.540)	Acc@5 96.143 (95.719)
Epoch: [14][22/25]	Time 0.881 (0.739)	Data 0.003 (0.032)	Loss 1.3770 (1.4239)	Acc@1 61.865 (58.685)	Acc@5 95.508 (95.709)
Epoch: [14][23/25]	Time 0.679 (0.737)	Data 0.009 (0.031)	Loss 1.3814 (1.4222)	Acc@1 60.547 (58.763)	Acc@5 96.338 (95.736)
Epoch: [14][24/25]	Time 0.357 (0.722)	Data 0.006 (0.030)	Loss 1.3202 (1.4204)	Acc@1 62.382 (58.824)	Acc@5 96.816 (95.754)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/25]	Time 0.715 (0.715)	Data 0.616 (0.616)	Loss 1.3312 (1.3312)	Acc@1 62.549 (62.549)	Acc@5 96.387 (96.387)
Epoch: [15][1/25]	Time 0.752 (0.734)	Data 0.007 (0.311)	Loss 1.3027 (1.3170)	Acc@1 61.279 (61.914)	Acc@5 96.973 (96.680)
Epoch: [15][2/25]	Time 0.726 (0.731)	Data 0.004 (0.209)	Loss 1.3576 (1.3305)	Acc@1 61.084 (61.637)	Acc@5 96.289 (96.549)
Epoch: [15][3/25]	Time 0.707 (0.725)	Data 0.006 (0.158)	Loss 1.3584 (1.3375)	Acc@1 61.670 (61.646)	Acc@5 96.289 (96.484)
Epoch: [15][4/25]	Time 0.681 (0.716)	Data 0.006 (0.128)	Loss 1.3401 (1.3380)	Acc@1 61.572 (61.631)	Acc@5 96.533 (96.494)
Epoch: [15][5/25]	Time 0.724 (0.718)	Data 0.006 (0.108)	Loss 1.3225 (1.3354)	Acc@1 62.891 (61.841)	Acc@5 95.801 (96.379)
Epoch: [15][6/25]	Time 0.746 (0.722)	Data 0.004 (0.093)	Loss 1.3433 (1.3365)	Acc@1 62.207 (61.893)	Acc@5 96.191 (96.352)
Epoch: [15][7/25]	Time 0.729 (0.723)	Data 0.006 (0.082)	Loss 1.3203 (1.3345)	Acc@1 63.867 (62.140)	Acc@5 96.143 (96.326)
Epoch: [15][8/25]	Time 0.697 (0.720)	Data 0.007 (0.074)	Loss 1.3060 (1.3313)	Acc@1 62.744 (62.207)	Acc@5 96.631 (96.360)
Epoch: [15][9/25]	Time 0.760 (0.724)	Data 0.004 (0.067)	Loss 1.2566 (1.3239)	Acc@1 65.332 (62.520)	Acc@5 97.021 (96.426)
Epoch: [15][10/25]	Time 0.713 (0.723)	Data 0.005 (0.061)	Loss 1.2857 (1.3204)	Acc@1 63.672 (62.624)	Acc@5 96.973 (96.475)
Epoch: [15][11/25]	Time 0.769 (0.727)	Data 0.006 (0.057)	Loss 1.2478 (1.3143)	Acc@1 63.916 (62.732)	Acc@5 96.826 (96.505)
Epoch: [15][12/25]	Time 0.785 (0.731)	Data 0.007 (0.053)	Loss 1.3028 (1.3135)	Acc@1 64.160 (62.842)	Acc@5 96.582 (96.511)
Epoch: [15][13/25]	Time 0.660 (0.726)	Data 0.007 (0.049)	Loss 1.2631 (1.3099)	Acc@1 65.137 (63.006)	Acc@5 96.533 (96.512)
Epoch: [15][14/25]	Time 0.648 (0.721)	Data 0.005 (0.046)	Loss 1.2779 (1.3077)	Acc@1 64.258 (63.089)	Acc@5 96.924 (96.540)
Epoch: [15][15/25]	Time 0.652 (0.717)	Data 0.006 (0.044)	Loss 1.2850 (1.3063)	Acc@1 63.086 (63.089)	Acc@5 96.387 (96.530)
Epoch: [15][16/25]	Time 0.657 (0.713)	Data 0.009 (0.042)	Loss 1.2610 (1.3036)	Acc@1 64.648 (63.181)	Acc@5 97.070 (96.562)
Epoch: [15][17/25]	Time 0.662 (0.710)	Data 0.006 (0.040)	Loss 1.3147 (1.3043)	Acc@1 61.670 (63.097)	Acc@5 96.387 (96.552)
Epoch: [15][18/25]	Time 0.662 (0.708)	Data 0.005 (0.038)	Loss 1.2533 (1.3016)	Acc@1 64.795 (63.186)	Acc@5 97.168 (96.585)
Epoch: [15][19/25]	Time 0.881 (0.716)	Data 0.006 (0.037)	Loss 1.2704 (1.3000)	Acc@1 64.453 (63.250)	Acc@5 96.143 (96.562)
Epoch: [15][20/25]	Time 0.836 (0.722)	Data 0.004 (0.035)	Loss 1.2446 (1.2974)	Acc@1 65.723 (63.367)	Acc@5 96.826 (96.575)
Epoch: [15][21/25]	Time 0.724 (0.722)	Data 0.005 (0.034)	Loss 1.2302 (1.2943)	Acc@1 65.234 (63.452)	Acc@5 97.021 (96.595)
Epoch: [15][22/25]	Time 0.716 (0.722)	Data 0.004 (0.032)	Loss 1.2284 (1.2915)	Acc@1 64.893 (63.515)	Acc@5 96.631 (96.597)
Epoch: [15][23/25]	Time 0.890 (0.729)	Data 0.004 (0.031)	Loss 1.1867 (1.2871)	Acc@1 67.383 (63.676)	Acc@5 96.924 (96.611)
Epoch: [15][24/25]	Time 0.516 (0.720)	Data 0.004 (0.030)	Loss 1.2189 (1.2859)	Acc@1 67.217 (63.736)	Acc@5 97.288 (96.622)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/25]	Time 0.761 (0.761)	Data 0.631 (0.631)	Loss 1.2577 (1.2577)	Acc@1 64.551 (64.551)	Acc@5 96.338 (96.338)
Epoch: [16][1/25]	Time 0.717 (0.739)	Data 0.007 (0.319)	Loss 1.2682 (1.2629)	Acc@1 65.088 (64.819)	Acc@5 96.338 (96.338)
Epoch: [16][2/25]	Time 0.741 (0.740)	Data 0.006 (0.215)	Loss 1.2463 (1.2574)	Acc@1 64.502 (64.714)	Acc@5 96.484 (96.387)
Epoch: [16][3/25]	Time 0.700 (0.730)	Data 0.005 (0.162)	Loss 1.2557 (1.2570)	Acc@1 64.209 (64.587)	Acc@5 96.680 (96.460)
Epoch: [16][4/25]	Time 0.706 (0.725)	Data 0.005 (0.131)	Loss 1.2442 (1.2544)	Acc@1 65.088 (64.688)	Acc@5 96.631 (96.494)
Epoch: [16][5/25]	Time 0.676 (0.717)	Data 0.005 (0.110)	Loss 1.2142 (1.2477)	Acc@1 65.381 (64.803)	Acc@5 97.314 (96.631)
Epoch: [16][6/25]	Time 0.759 (0.723)	Data 0.007 (0.095)	Loss 1.2150 (1.2430)	Acc@1 65.088 (64.844)	Acc@5 96.875 (96.666)
Epoch: [16][7/25]	Time 0.679 (0.718)	Data 0.005 (0.084)	Loss 1.2403 (1.2427)	Acc@1 64.453 (64.795)	Acc@5 97.070 (96.716)
Epoch: [16][8/25]	Time 0.640 (0.709)	Data 0.005 (0.075)	Loss 1.2063 (1.2387)	Acc@1 65.088 (64.827)	Acc@5 97.119 (96.761)
Epoch: [16][9/25]	Time 0.665 (0.705)	Data 0.004 (0.068)	Loss 1.2484 (1.2396)	Acc@1 65.479 (64.893)	Acc@5 97.314 (96.816)
Epoch: [16][10/25]	Time 0.667 (0.701)	Data 0.008 (0.063)	Loss 1.2182 (1.2377)	Acc@1 65.381 (64.937)	Acc@5 96.826 (96.817)
Epoch: [16][11/25]	Time 0.647 (0.697)	Data 0.005 (0.058)	Loss 1.1883 (1.2336)	Acc@1 67.529 (65.153)	Acc@5 96.777 (96.814)
Epoch: [16][12/25]	Time 0.737 (0.700)	Data 0.008 (0.054)	Loss 1.1853 (1.2299)	Acc@1 66.943 (65.291)	Acc@5 96.875 (96.819)
Epoch: [16][13/25]	Time 0.791 (0.706)	Data 0.006 (0.050)	Loss 1.1915 (1.2271)	Acc@1 66.602 (65.384)	Acc@5 97.559 (96.872)
Epoch: [16][14/25]	Time 0.658 (0.703)	Data 0.007 (0.048)	Loss 1.1980 (1.2252)	Acc@1 66.504 (65.459)	Acc@5 97.021 (96.882)
Epoch: [16][15/25]	Time 0.664 (0.701)	Data 0.006 (0.045)	Loss 1.2092 (1.2242)	Acc@1 66.260 (65.509)	Acc@5 97.461 (96.918)
Epoch: [16][16/25]	Time 0.659 (0.698)	Data 0.006 (0.043)	Loss 1.1555 (1.2201)	Acc@1 68.066 (65.659)	Acc@5 97.412 (96.947)
Epoch: [16][17/25]	Time 0.696 (0.698)	Data 0.004 (0.041)	Loss 1.1320 (1.2152)	Acc@1 68.799 (65.834)	Acc@5 97.607 (96.984)
Epoch: [16][18/25]	Time 0.753 (0.701)	Data 0.004 (0.039)	Loss 1.1738 (1.2131)	Acc@1 67.285 (65.910)	Acc@5 97.217 (96.996)
Epoch: [16][19/25]	Time 0.711 (0.701)	Data 0.006 (0.037)	Loss 1.1678 (1.2108)	Acc@1 67.969 (66.013)	Acc@5 97.266 (97.009)
Epoch: [16][20/25]	Time 0.685 (0.701)	Data 0.004 (0.035)	Loss 1.1305 (1.2070)	Acc@1 69.971 (66.202)	Acc@5 97.510 (97.033)
Epoch: [16][21/25]	Time 0.732 (0.702)	Data 0.007 (0.034)	Loss 1.1209 (1.2031)	Acc@1 69.287 (66.342)	Acc@5 97.412 (97.050)
Epoch: [16][22/25]	Time 0.793 (0.706)	Data 0.004 (0.033)	Loss 1.1339 (1.2001)	Acc@1 68.896 (66.453)	Acc@5 97.705 (97.079)
Epoch: [16][23/25]	Time 0.691 (0.705)	Data 0.005 (0.032)	Loss 1.1443 (1.1977)	Acc@1 69.580 (66.583)	Acc@5 97.119 (97.080)
Epoch: [16][24/25]	Time 0.368 (0.692)	Data 0.007 (0.031)	Loss 1.1725 (1.1973)	Acc@1 68.632 (66.618)	Acc@5 97.406 (97.086)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/25]	Time 0.787 (0.787)	Data 0.685 (0.685)	Loss 1.1713 (1.1713)	Acc@1 67.773 (67.773)	Acc@5 97.314 (97.314)
Epoch: [17][1/25]	Time 0.748 (0.767)	Data 0.003 (0.344)	Loss 1.1507 (1.1610)	Acc@1 68.311 (68.042)	Acc@5 97.363 (97.339)
Epoch: [17][2/25]	Time 0.725 (0.753)	Data 0.005 (0.231)	Loss 1.1341 (1.1521)	Acc@1 68.115 (68.066)	Acc@5 97.656 (97.445)
Epoch: [17][3/25]	Time 0.756 (0.754)	Data 0.003 (0.174)	Loss 1.1178 (1.1435)	Acc@1 68.652 (68.213)	Acc@5 97.607 (97.485)
Epoch: [17][4/25]	Time 0.754 (0.754)	Data 0.005 (0.140)	Loss 1.1138 (1.1375)	Acc@1 70.508 (68.672)	Acc@5 97.510 (97.490)
Epoch: [17][5/25]	Time 0.699 (0.745)	Data 0.006 (0.118)	Loss 1.0972 (1.1308)	Acc@1 69.922 (68.880)	Acc@5 97.803 (97.542)
Epoch: [17][6/25]	Time 0.714 (0.741)	Data 0.008 (0.102)	Loss 1.1235 (1.1298)	Acc@1 68.848 (68.876)	Acc@5 97.998 (97.607)
Epoch: [17][7/25]	Time 0.768 (0.744)	Data 0.005 (0.090)	Loss 1.0421 (1.1188)	Acc@1 71.582 (69.214)	Acc@5 97.852 (97.638)
Epoch: [17][8/25]	Time 0.661 (0.735)	Data 0.005 (0.081)	Loss 1.0974 (1.1164)	Acc@1 70.020 (69.303)	Acc@5 97.656 (97.640)
Epoch: [17][9/25]	Time 0.658 (0.727)	Data 0.006 (0.073)	Loss 1.0832 (1.1131)	Acc@1 70.508 (69.424)	Acc@5 97.754 (97.651)
Epoch: [17][10/25]	Time 0.636 (0.719)	Data 0.005 (0.067)	Loss 1.0949 (1.1115)	Acc@1 70.947 (69.562)	Acc@5 97.803 (97.665)
Epoch: [17][11/25]	Time 0.647 (0.713)	Data 0.010 (0.062)	Loss 1.1033 (1.1108)	Acc@1 70.264 (69.621)	Acc@5 97.266 (97.632)
Epoch: [17][12/25]	Time 0.651 (0.708)	Data 0.005 (0.058)	Loss 1.0795 (1.1084)	Acc@1 71.582 (69.772)	Acc@5 97.363 (97.611)
Epoch: [17][13/25]	Time 0.696 (0.707)	Data 0.004 (0.054)	Loss 1.0762 (1.1061)	Acc@1 71.533 (69.897)	Acc@5 97.900 (97.632)
Epoch: [17][14/25]	Time 0.771 (0.711)	Data 0.006 (0.051)	Loss 1.0686 (1.1036)	Acc@1 70.312 (69.925)	Acc@5 97.754 (97.640)
Epoch: [17][15/25]	Time 0.749 (0.714)	Data 0.006 (0.048)	Loss 1.0427 (1.0998)	Acc@1 72.949 (70.114)	Acc@5 97.949 (97.659)
Epoch: [17][16/25]	Time 0.734 (0.715)	Data 0.005 (0.045)	Loss 0.9973 (1.0937)	Acc@1 73.438 (70.310)	Acc@5 98.242 (97.694)
Epoch: [17][17/25]	Time 0.727 (0.716)	Data 0.007 (0.043)	Loss 1.0173 (1.0895)	Acc@1 72.900 (70.454)	Acc@5 98.145 (97.719)
Epoch: [17][18/25]	Time 0.749 (0.717)	Data 0.007 (0.041)	Loss 1.0346 (1.0866)	Acc@1 73.291 (70.603)	Acc@5 97.949 (97.731)
Epoch: [17][19/25]	Time 0.687 (0.716)	Data 0.004 (0.039)	Loss 1.0290 (1.0837)	Acc@1 72.363 (70.691)	Acc@5 98.047 (97.747)
Epoch: [17][20/25]	Time 0.632 (0.712)	Data 0.009 (0.038)	Loss 1.0380 (1.0815)	Acc@1 71.436 (70.726)	Acc@5 98.193 (97.768)
Epoch: [17][21/25]	Time 0.634 (0.708)	Data 0.005 (0.037)	Loss 1.0290 (1.0792)	Acc@1 73.096 (70.834)	Acc@5 98.535 (97.803)
Epoch: [17][22/25]	Time 0.711 (0.708)	Data 0.006 (0.035)	Loss 1.0031 (1.0759)	Acc@1 73.438 (70.947)	Acc@5 98.047 (97.813)
Epoch: [17][23/25]	Time 0.683 (0.707)	Data 0.005 (0.034)	Loss 0.9840 (1.0720)	Acc@1 75.684 (71.145)	Acc@5 97.998 (97.821)
Epoch: [17][24/25]	Time 0.374 (0.694)	Data 0.008 (0.033)	Loss 1.0159 (1.0711)	Acc@1 73.467 (71.184)	Acc@5 98.585 (97.834)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/25]	Time 0.757 (0.757)	Data 0.621 (0.621)	Loss 1.0001 (1.0001)	Acc@1 73.047 (73.047)	Acc@5 97.998 (97.998)
Epoch: [18][1/25]	Time 0.771 (0.764)	Data 0.004 (0.313)	Loss 0.9535 (0.9768)	Acc@1 75.684 (74.365)	Acc@5 98.486 (98.242)
Epoch: [18][2/25]	Time 0.662 (0.730)	Data 0.006 (0.211)	Loss 0.9973 (0.9836)	Acc@1 73.535 (74.089)	Acc@5 98.096 (98.193)
Epoch: [18][3/25]	Time 0.655 (0.711)	Data 0.006 (0.159)	Loss 1.0424 (0.9983)	Acc@1 71.631 (73.474)	Acc@5 98.047 (98.157)
Epoch: [18][4/25]	Time 0.690 (0.707)	Data 0.007 (0.129)	Loss 1.0457 (1.0078)	Acc@1 72.412 (73.262)	Acc@5 97.998 (98.125)
Epoch: [18][5/25]	Time 0.699 (0.706)	Data 0.005 (0.108)	Loss 1.0020 (1.0068)	Acc@1 72.461 (73.128)	Acc@5 98.389 (98.169)
Epoch: [18][6/25]	Time 0.635 (0.696)	Data 0.006 (0.094)	Loss 1.0006 (1.0059)	Acc@1 74.658 (73.347)	Acc@5 97.852 (98.124)
Epoch: [18][7/25]	Time 0.647 (0.690)	Data 0.003 (0.082)	Loss 1.0032 (1.0056)	Acc@1 74.414 (73.480)	Acc@5 98.633 (98.187)
Epoch: [18][8/25]	Time 0.657 (0.686)	Data 0.006 (0.074)	Loss 1.0388 (1.0093)	Acc@1 73.242 (73.454)	Acc@5 98.096 (98.177)
Epoch: [18][9/25]	Time 0.632 (0.681)	Data 0.007 (0.067)	Loss 0.9689 (1.0052)	Acc@1 74.609 (73.569)	Acc@5 98.730 (98.232)
Epoch: [18][10/25]	Time 0.636 (0.677)	Data 0.007 (0.062)	Loss 1.0374 (1.0082)	Acc@1 73.047 (73.522)	Acc@5 97.754 (98.189)
Epoch: [18][11/25]	Time 0.627 (0.672)	Data 0.005 (0.057)	Loss 1.0344 (1.0103)	Acc@1 72.852 (73.466)	Acc@5 97.852 (98.161)
Epoch: [18][12/25]	Time 0.687 (0.673)	Data 0.005 (0.053)	Loss 1.0294 (1.0118)	Acc@1 73.291 (73.453)	Acc@5 98.047 (98.152)
Epoch: [18][13/25]	Time 0.762 (0.680)	Data 0.005 (0.049)	Loss 1.0147 (1.0120)	Acc@1 73.584 (73.462)	Acc@5 98.096 (98.148)
Epoch: [18][14/25]	Time 0.778 (0.686)	Data 0.007 (0.047)	Loss 0.9776 (1.0097)	Acc@1 74.561 (73.535)	Acc@5 98.340 (98.161)
Epoch: [18][15/25]	Time 0.709 (0.688)	Data 0.005 (0.044)	Loss 0.9992 (1.0091)	Acc@1 73.389 (73.526)	Acc@5 98.389 (98.175)
Epoch: [18][16/25]	Time 0.682 (0.687)	Data 0.009 (0.042)	Loss 0.9730 (1.0069)	Acc@1 74.463 (73.581)	Acc@5 98.047 (98.168)
Epoch: [18][17/25]	Time 0.749 (0.691)	Data 0.007 (0.040)	Loss 0.9588 (1.0043)	Acc@1 74.854 (73.652)	Acc@5 98.242 (98.172)
Epoch: [18][18/25]	Time 0.706 (0.692)	Data 0.006 (0.038)	Loss 0.9863 (1.0033)	Acc@1 75.342 (73.741)	Acc@5 98.047 (98.165)
Epoch: [18][19/25]	Time 0.695 (0.692)	Data 0.005 (0.037)	Loss 0.9074 (0.9985)	Acc@1 76.465 (73.877)	Acc@5 98.340 (98.174)
Epoch: [18][20/25]	Time 0.769 (0.695)	Data 0.008 (0.035)	Loss 0.9665 (0.9970)	Acc@1 74.170 (73.891)	Acc@5 98.535 (98.191)
Epoch: [18][21/25]	Time 0.777 (0.699)	Data 0.006 (0.034)	Loss 0.9404 (0.9944)	Acc@1 75.439 (73.961)	Acc@5 98.730 (98.216)
Epoch: [18][22/25]	Time 0.728 (0.700)	Data 0.007 (0.033)	Loss 0.9453 (0.9923)	Acc@1 76.416 (74.068)	Acc@5 98.145 (98.212)
Epoch: [18][23/25]	Time 0.721 (0.701)	Data 0.008 (0.032)	Loss 0.9238 (0.9894)	Acc@1 76.416 (74.166)	Acc@5 98.877 (98.240)
Epoch: [18][24/25]	Time 0.418 (0.690)	Data 0.005 (0.031)	Loss 0.9239 (0.9883)	Acc@1 77.005 (74.214)	Acc@5 98.349 (98.242)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/25]	Time 0.736 (0.736)	Data 0.778 (0.778)	Loss 0.9029 (0.9029)	Acc@1 76.611 (76.611)	Acc@5 98.486 (98.486)
Epoch: [19][1/25]	Time 0.680 (0.708)	Data 0.007 (0.392)	Loss 0.9188 (0.9108)	Acc@1 77.002 (76.807)	Acc@5 98.486 (98.486)
Epoch: [19][2/25]	Time 0.652 (0.689)	Data 0.003 (0.263)	Loss 0.9275 (0.9164)	Acc@1 76.465 (76.693)	Acc@5 98.145 (98.372)
Epoch: [19][3/25]	Time 0.602 (0.667)	Data 0.007 (0.199)	Loss 0.9362 (0.9213)	Acc@1 75.293 (76.343)	Acc@5 98.535 (98.413)
Epoch: [19][4/25]	Time 0.671 (0.668)	Data 0.007 (0.160)	Loss 0.9511 (0.9273)	Acc@1 75.000 (76.074)	Acc@5 98.242 (98.379)
Epoch: [19][5/25]	Time 0.625 (0.661)	Data 0.007 (0.135)	Loss 0.8756 (0.9187)	Acc@1 77.979 (76.392)	Acc@5 98.682 (98.429)
Epoch: [19][6/25]	Time 0.681 (0.664)	Data 0.007 (0.116)	Loss 0.8983 (0.9158)	Acc@1 77.197 (76.507)	Acc@5 98.682 (98.465)
Epoch: [19][7/25]	Time 0.652 (0.662)	Data 0.006 (0.103)	Loss 0.9339 (0.9180)	Acc@1 76.611 (76.520)	Acc@5 98.633 (98.486)
Epoch: [19][8/25]	Time 0.612 (0.657)	Data 0.007 (0.092)	Loss 0.9306 (0.9194)	Acc@1 77.441 (76.622)	Acc@5 98.633 (98.503)
Epoch: [19][9/25]	Time 0.650 (0.656)	Data 0.007 (0.084)	Loss 0.9223 (0.9197)	Acc@1 76.953 (76.655)	Acc@5 98.730 (98.525)
Epoch: [19][10/25]	Time 0.656 (0.656)	Data 0.004 (0.076)	Loss 0.9047 (0.9183)	Acc@1 77.490 (76.731)	Acc@5 98.633 (98.535)
Epoch: [19][11/25]	Time 0.631 (0.654)	Data 0.005 (0.070)	Loss 0.9260 (0.9190)	Acc@1 78.027 (76.839)	Acc@5 98.389 (98.523)
Epoch: [19][12/25]	Time 0.669 (0.655)	Data 0.007 (0.065)	Loss 0.8829 (0.9162)	Acc@1 78.662 (76.979)	Acc@5 98.535 (98.524)
Epoch: [19][13/25]	Time 0.646 (0.655)	Data 0.008 (0.061)	Loss 0.8765 (0.9134)	Acc@1 78.174 (77.065)	Acc@5 98.291 (98.507)
Epoch: [19][14/25]	Time 0.647 (0.654)	Data 0.007 (0.058)	Loss 0.8880 (0.9117)	Acc@1 77.686 (77.106)	Acc@5 99.023 (98.542)
Epoch: [19][15/25]	Time 0.663 (0.655)	Data 0.008 (0.055)	Loss 0.9029 (0.9111)	Acc@1 77.588 (77.136)	Acc@5 98.633 (98.547)
Epoch: [19][16/25]	Time 0.736 (0.659)	Data 0.007 (0.052)	Loss 0.8561 (0.9079)	Acc@1 78.662 (77.226)	Acc@5 98.828 (98.564)
Epoch: [19][17/25]	Time 0.724 (0.663)	Data 0.005 (0.049)	Loss 0.9014 (0.9075)	Acc@1 77.441 (77.238)	Acc@5 98.975 (98.587)
Epoch: [19][18/25]	Time 0.740 (0.667)	Data 0.006 (0.047)	Loss 0.8253 (0.9032)	Acc@1 80.127 (77.390)	Acc@5 98.730 (98.594)
Epoch: [19][19/25]	Time 0.706 (0.669)	Data 0.004 (0.045)	Loss 0.8842 (0.9023)	Acc@1 77.979 (77.419)	Acc@5 98.438 (98.586)
Epoch: [19][20/25]	Time 0.784 (0.674)	Data 0.004 (0.043)	Loss 0.8827 (0.9013)	Acc@1 77.783 (77.437)	Acc@5 98.682 (98.591)
Epoch: [19][21/25]	Time 0.733 (0.677)	Data 0.004 (0.041)	Loss 0.9112 (0.9018)	Acc@1 76.953 (77.415)	Acc@5 98.535 (98.588)
Epoch: [19][22/25]	Time 0.735 (0.680)	Data 0.004 (0.040)	Loss 0.8445 (0.8993)	Acc@1 79.297 (77.497)	Acc@5 98.828 (98.599)
Epoch: [19][23/25]	Time 0.750 (0.683)	Data 0.005 (0.038)	Loss 0.9250 (0.9003)	Acc@1 75.732 (77.423)	Acc@5 98.584 (98.598)
Epoch: [19][24/25]	Time 0.405 (0.671)	Data 0.006 (0.037)	Loss 0.8868 (0.9001)	Acc@1 75.472 (77.390)	Acc@5 98.585 (98.598)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/25]	Time 0.702 (0.702)	Data 0.793 (0.793)	Loss 0.8587 (0.8587)	Acc@1 78.857 (78.857)	Acc@5 98.877 (98.877)
Epoch: [20][1/25]	Time 0.717 (0.710)	Data 0.006 (0.400)	Loss 0.8582 (0.8584)	Acc@1 79.053 (78.955)	Acc@5 98.633 (98.755)
Epoch: [20][2/25]	Time 0.757 (0.725)	Data 0.003 (0.268)	Loss 0.8582 (0.8584)	Acc@1 78.857 (78.923)	Acc@5 98.877 (98.796)
Epoch: [20][3/25]	Time 0.759 (0.734)	Data 0.005 (0.202)	Loss 0.8050 (0.8450)	Acc@1 79.736 (79.126)	Acc@5 99.023 (98.853)
Epoch: [20][4/25]	Time 0.714 (0.730)	Data 0.007 (0.163)	Loss 0.8463 (0.8453)	Acc@1 78.467 (78.994)	Acc@5 98.926 (98.867)
Epoch: [20][5/25]	Time 0.695 (0.724)	Data 0.003 (0.136)	Loss 0.8654 (0.8486)	Acc@1 79.443 (79.069)	Acc@5 98.486 (98.804)
Epoch: [20][6/25]	Time 0.677 (0.717)	Data 0.004 (0.117)	Loss 0.8652 (0.8510)	Acc@1 78.613 (79.004)	Acc@5 98.438 (98.751)
Epoch: [20][7/25]	Time 0.672 (0.712)	Data 0.004 (0.103)	Loss 0.9019 (0.8574)	Acc@1 77.930 (78.870)	Acc@5 98.633 (98.737)
Epoch: [20][8/25]	Time 0.620 (0.701)	Data 0.006 (0.093)	Loss 0.8497 (0.8565)	Acc@1 78.906 (78.874)	Acc@5 98.633 (98.725)
Epoch: [20][9/25]	Time 0.642 (0.695)	Data 0.009 (0.084)	Loss 0.8930 (0.8602)	Acc@1 77.930 (78.779)	Acc@5 98.633 (98.716)
Epoch: [20][10/25]	Time 0.701 (0.696)	Data 0.004 (0.077)	Loss 0.8573 (0.8599)	Acc@1 78.809 (78.782)	Acc@5 98.633 (98.708)
Epoch: [20][11/25]	Time 0.775 (0.703)	Data 0.007 (0.071)	Loss 0.8418 (0.8584)	Acc@1 79.346 (78.829)	Acc@5 98.779 (98.714)
Epoch: [20][12/25]	Time 0.702 (0.702)	Data 0.007 (0.066)	Loss 0.8775 (0.8599)	Acc@1 77.344 (78.715)	Acc@5 98.779 (98.719)
Epoch: [20][13/25]	Time 0.718 (0.704)	Data 0.004 (0.062)	Loss 0.8422 (0.8586)	Acc@1 79.053 (78.739)	Acc@5 98.730 (98.720)
Epoch: [20][14/25]	Time 0.702 (0.703)	Data 0.006 (0.058)	Loss 0.8629 (0.8589)	Acc@1 79.346 (78.779)	Acc@5 97.949 (98.669)
Epoch: [20][15/25]	Time 0.659 (0.701)	Data 0.007 (0.055)	Loss 0.8160 (0.8562)	Acc@1 80.859 (78.909)	Acc@5 99.023 (98.691)
Epoch: [20][16/25]	Time 0.623 (0.696)	Data 0.009 (0.052)	Loss 0.8310 (0.8547)	Acc@1 79.346 (78.935)	Acc@5 98.877 (98.702)
Epoch: [20][17/25]	Time 0.647 (0.693)	Data 0.004 (0.049)	Loss 0.8757 (0.8559)	Acc@1 78.076 (78.887)	Acc@5 98.633 (98.698)
Epoch: [20][18/25]	Time 0.653 (0.691)	Data 0.006 (0.047)	Loss 0.8582 (0.8560)	Acc@1 78.955 (78.891)	Acc@5 98.779 (98.702)
Epoch: [20][19/25]	Time 0.641 (0.689)	Data 0.006 (0.045)	Loss 0.8634 (0.8564)	Acc@1 77.734 (78.833)	Acc@5 98.389 (98.687)
Epoch: [20][20/25]	Time 0.669 (0.688)	Data 0.006 (0.043)	Loss 0.8489 (0.8560)	Acc@1 78.662 (78.825)	Acc@5 98.486 (98.677)
Epoch: [20][21/25]	Time 0.640 (0.686)	Data 0.005 (0.042)	Loss 0.8246 (0.8546)	Acc@1 80.420 (78.897)	Acc@5 99.023 (98.693)
Epoch: [20][22/25]	Time 0.659 (0.684)	Data 0.004 (0.040)	Loss 0.8588 (0.8548)	Acc@1 78.857 (78.896)	Acc@5 98.730 (98.694)
Epoch: [20][23/25]	Time 0.635 (0.682)	Data 0.009 (0.039)	Loss 0.9234 (0.8576)	Acc@1 76.855 (78.811)	Acc@5 98.486 (98.686)
Epoch: [20][24/25]	Time 0.348 (0.669)	Data 0.007 (0.037)	Loss 0.8471 (0.8575)	Acc@1 78.184 (78.800)	Acc@5 98.939 (98.690)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/25]	Time 0.771 (0.771)	Data 0.625 (0.625)	Loss 0.8254 (0.8254)	Acc@1 79.639 (79.639)	Acc@5 98.877 (98.877)
Epoch: [21][1/25]	Time 0.723 (0.747)	Data 0.005 (0.315)	Loss 0.7499 (0.7876)	Acc@1 82.471 (81.055)	Acc@5 99.121 (98.999)
Epoch: [21][2/25]	Time 0.747 (0.747)	Data 0.006 (0.212)	Loss 0.7805 (0.7853)	Acc@1 82.471 (81.527)	Acc@5 99.170 (99.056)
Epoch: [21][3/25]	Time 0.763 (0.751)	Data 0.004 (0.160)	Loss 0.7950 (0.7877)	Acc@1 81.348 (81.482)	Acc@5 99.023 (99.048)
Epoch: [21][4/25]	Time 0.693 (0.739)	Data 0.004 (0.129)	Loss 0.8178 (0.7937)	Acc@1 79.102 (81.006)	Acc@5 99.170 (99.072)
Epoch: [21][5/25]	Time 0.653 (0.725)	Data 0.006 (0.109)	Loss 0.7269 (0.7826)	Acc@1 83.252 (81.380)	Acc@5 99.512 (99.146)
Epoch: [21][6/25]	Time 0.646 (0.714)	Data 0.007 (0.094)	Loss 0.7729 (0.7812)	Acc@1 81.641 (81.417)	Acc@5 98.828 (99.100)
Epoch: [21][7/25]	Time 0.705 (0.712)	Data 0.008 (0.083)	Loss 0.7464 (0.7768)	Acc@1 82.520 (81.555)	Acc@5 99.170 (99.109)
Epoch: [21][8/25]	Time 0.758 (0.717)	Data 0.004 (0.075)	Loss 0.7881 (0.7781)	Acc@1 81.006 (81.494)	Acc@5 99.121 (99.110)
Epoch: [21][9/25]	Time 0.749 (0.721)	Data 0.007 (0.068)	Loss 0.7679 (0.7771)	Acc@1 82.373 (81.582)	Acc@5 98.828 (99.082)
Epoch: [21][10/25]	Time 0.698 (0.719)	Data 0.006 (0.062)	Loss 0.7213 (0.7720)	Acc@1 84.082 (81.809)	Acc@5 99.219 (99.094)
Epoch: [21][11/25]	Time 0.650 (0.713)	Data 0.004 (0.057)	Loss 0.7738 (0.7722)	Acc@1 81.592 (81.791)	Acc@5 98.584 (99.052)
Epoch: [21][12/25]	Time 0.708 (0.713)	Data 0.007 (0.053)	Loss 0.8146 (0.7754)	Acc@1 79.932 (81.648)	Acc@5 98.828 (99.035)
Epoch: [21][13/25]	Time 0.761 (0.716)	Data 0.004 (0.050)	Loss 0.8004 (0.7772)	Acc@1 80.273 (81.550)	Acc@5 98.486 (98.996)
Epoch: [21][14/25]	Time 0.771 (0.720)	Data 0.006 (0.047)	Loss 0.7625 (0.7762)	Acc@1 81.201 (81.527)	Acc@5 99.268 (99.014)
Epoch: [21][15/25]	Time 0.700 (0.718)	Data 0.004 (0.044)	Loss 0.8059 (0.7781)	Acc@1 80.518 (81.464)	Acc@5 98.779 (98.999)
Epoch: [21][16/25]	Time 0.765 (0.721)	Data 0.006 (0.042)	Loss 0.7710 (0.7777)	Acc@1 81.006 (81.437)	Acc@5 99.268 (99.015)
Epoch: [21][17/25]	Time 0.747 (0.723)	Data 0.005 (0.040)	Loss 0.7964 (0.7787)	Acc@1 80.908 (81.407)	Acc@5 98.633 (98.994)
Epoch: [21][18/25]	Time 0.722 (0.723)	Data 0.004 (0.038)	Loss 0.8487 (0.7824)	Acc@1 78.369 (81.247)	Acc@5 98.877 (98.987)
Epoch: [21][19/25]	Time 0.728 (0.723)	Data 0.006 (0.036)	Loss 0.7984 (0.7832)	Acc@1 80.518 (81.211)	Acc@5 98.730 (98.975)
Epoch: [21][20/25]	Time 0.775 (0.725)	Data 0.007 (0.035)	Loss 0.8111 (0.7845)	Acc@1 80.420 (81.173)	Acc@5 98.828 (98.968)
Epoch: [21][21/25]	Time 0.692 (0.724)	Data 0.007 (0.034)	Loss 0.8708 (0.7884)	Acc@1 79.102 (81.079)	Acc@5 98.730 (98.957)
Epoch: [21][22/25]	Time 0.630 (0.720)	Data 0.008 (0.033)	Loss 0.7900 (0.7885)	Acc@1 80.664 (81.061)	Acc@5 99.072 (98.962)
Epoch: [21][23/25]	Time 0.696 (0.719)	Data 0.005 (0.032)	Loss 0.8073 (0.7893)	Acc@1 80.225 (81.026)	Acc@5 98.828 (98.956)
Epoch: [21][24/25]	Time 0.354 (0.704)	Data 0.005 (0.030)	Loss 0.8561 (0.7904)	Acc@1 79.599 (81.002)	Acc@5 98.349 (98.946)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/25]	Time 0.751 (0.751)	Data 0.668 (0.668)	Loss 0.8095 (0.8095)	Acc@1 79.834 (79.834)	Acc@5 98.877 (98.877)
Epoch: [22][1/25]	Time 0.709 (0.730)	Data 0.005 (0.337)	Loss 0.8443 (0.8269)	Acc@1 80.273 (80.054)	Acc@5 98.828 (98.853)
Epoch: [22][2/25]	Time 0.745 (0.735)	Data 0.003 (0.226)	Loss 0.7846 (0.8128)	Acc@1 81.445 (80.518)	Acc@5 99.219 (98.975)
Epoch: [22][3/25]	Time 0.705 (0.728)	Data 0.005 (0.170)	Loss 0.8352 (0.8184)	Acc@1 78.662 (80.054)	Acc@5 98.730 (98.914)
Epoch: [22][4/25]	Time 0.701 (0.722)	Data 0.006 (0.138)	Loss 0.7977 (0.8143)	Acc@1 81.494 (80.342)	Acc@5 98.730 (98.877)
Epoch: [22][5/25]	Time 0.770 (0.730)	Data 0.003 (0.115)	Loss 0.8453 (0.8194)	Acc@1 80.322 (80.339)	Acc@5 98.584 (98.828)
Epoch: [22][6/25]	Time 0.750 (0.733)	Data 0.003 (0.099)	Loss 0.8254 (0.8203)	Acc@1 80.225 (80.322)	Acc@5 98.535 (98.786)
Epoch: [22][7/25]	Time 0.689 (0.728)	Data 0.004 (0.087)	Loss 0.7764 (0.8148)	Acc@1 81.250 (80.438)	Acc@5 98.975 (98.810)
Epoch: [22][8/25]	Time 0.661 (0.720)	Data 0.008 (0.079)	Loss 0.8210 (0.8155)	Acc@1 79.736 (80.360)	Acc@5 98.877 (98.817)
Epoch: [22][9/25]	Time 0.664 (0.715)	Data 0.007 (0.071)	Loss 0.8090 (0.8148)	Acc@1 80.420 (80.366)	Acc@5 98.975 (98.833)
Epoch: [22][10/25]	Time 0.682 (0.712)	Data 0.004 (0.065)	Loss 0.7634 (0.8102)	Acc@1 81.543 (80.473)	Acc@5 99.023 (98.850)
Epoch: [22][11/25]	Time 0.747 (0.715)	Data 0.005 (0.060)	Loss 0.7850 (0.8081)	Acc@1 81.152 (80.530)	Acc@5 99.072 (98.869)
Epoch: [22][12/25]	Time 0.736 (0.716)	Data 0.007 (0.056)	Loss 0.8133 (0.8085)	Acc@1 80.469 (80.525)	Acc@5 99.023 (98.881)
Epoch: [22][13/25]	Time 0.778 (0.721)	Data 0.005 (0.053)	Loss 0.7998 (0.8078)	Acc@1 80.908 (80.552)	Acc@5 98.779 (98.873)
Epoch: [22][14/25]	Time 0.709 (0.720)	Data 0.007 (0.050)	Loss 0.8083 (0.8079)	Acc@1 80.225 (80.531)	Acc@5 98.975 (98.880)
Epoch: [22][15/25]	Time 0.767 (0.723)	Data 0.005 (0.047)	Loss 0.7601 (0.8049)	Acc@1 81.641 (80.600)	Acc@5 99.072 (98.892)
Epoch: [22][16/25]	Time 0.774 (0.726)	Data 0.006 (0.044)	Loss 0.8222 (0.8059)	Acc@1 79.492 (80.535)	Acc@5 98.682 (98.880)
Epoch: [22][17/25]	Time 0.766 (0.728)	Data 0.007 (0.042)	Loss 0.7822 (0.8046)	Acc@1 81.152 (80.569)	Acc@5 99.121 (98.893)
Epoch: [22][18/25]	Time 0.747 (0.729)	Data 0.007 (0.040)	Loss 0.8219 (0.8055)	Acc@1 80.225 (80.551)	Acc@5 99.121 (98.905)
Epoch: [22][19/25]	Time 0.766 (0.731)	Data 0.006 (0.039)	Loss 0.8066 (0.8056)	Acc@1 81.201 (80.583)	Acc@5 98.486 (98.884)
Epoch: [22][20/25]	Time 0.717 (0.730)	Data 0.005 (0.037)	Loss 0.7588 (0.8033)	Acc@1 82.812 (80.690)	Acc@5 99.072 (98.893)
Epoch: [22][21/25]	Time 0.760 (0.732)	Data 0.006 (0.036)	Loss 0.8132 (0.8038)	Acc@1 80.566 (80.684)	Acc@5 98.877 (98.892)
Epoch: [22][22/25]	Time 0.718 (0.731)	Data 0.005 (0.034)	Loss 0.7654 (0.8021)	Acc@1 82.520 (80.764)	Acc@5 98.926 (98.894)
Epoch: [22][23/25]	Time 0.732 (0.731)	Data 0.004 (0.033)	Loss 0.7819 (0.8013)	Acc@1 81.250 (80.784)	Acc@5 99.072 (98.901)
Epoch: [22][24/25]	Time 0.469 (0.721)	Data 0.006 (0.032)	Loss 0.8313 (0.8018)	Acc@1 80.896 (80.786)	Acc@5 98.939 (98.902)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/25]	Time 0.750 (0.750)	Data 0.731 (0.731)	Loss 0.8309 (0.8309)	Acc@1 79.590 (79.590)	Acc@5 98.877 (98.877)
Epoch: [23][1/25]	Time 0.754 (0.752)	Data 0.005 (0.368)	Loss 0.8011 (0.8160)	Acc@1 80.566 (80.078)	Acc@5 99.072 (98.975)
Epoch: [23][2/25]	Time 0.797 (0.767)	Data 0.005 (0.247)	Loss 0.7868 (0.8063)	Acc@1 81.299 (80.485)	Acc@5 99.316 (99.089)
Epoch: [23][3/25]	Time 0.716 (0.754)	Data 0.004 (0.186)	Loss 0.7951 (0.8035)	Acc@1 81.494 (80.737)	Acc@5 99.121 (99.097)
Epoch: [23][4/25]	Time 0.744 (0.752)	Data 0.006 (0.150)	Loss 0.7381 (0.7904)	Acc@1 83.105 (81.211)	Acc@5 98.926 (99.062)
Epoch: [23][5/25]	Time 0.775 (0.756)	Data 0.004 (0.126)	Loss 0.7394 (0.7819)	Acc@1 83.252 (81.551)	Acc@5 99.316 (99.105)
Epoch: [23][6/25]	Time 0.688 (0.746)	Data 0.004 (0.109)	Loss 0.7376 (0.7756)	Acc@1 82.227 (81.648)	Acc@5 99.512 (99.163)
Epoch: [23][7/25]	Time 0.712 (0.742)	Data 0.007 (0.096)	Loss 0.8102 (0.7799)	Acc@1 80.273 (81.476)	Acc@5 99.072 (99.152)
Epoch: [23][8/25]	Time 0.760 (0.744)	Data 0.006 (0.086)	Loss 0.7626 (0.7780)	Acc@1 82.764 (81.619)	Acc@5 99.170 (99.154)
Epoch: [23][9/25]	Time 0.757 (0.745)	Data 0.007 (0.078)	Loss 0.7317 (0.7734)	Acc@1 82.910 (81.748)	Acc@5 99.072 (99.146)
Epoch: [23][10/25]	Time 0.718 (0.743)	Data 0.005 (0.071)	Loss 0.7603 (0.7722)	Acc@1 81.885 (81.760)	Acc@5 98.877 (99.121)
Epoch: [23][11/25]	Time 0.642 (0.734)	Data 0.004 (0.066)	Loss 0.7631 (0.7714)	Acc@1 82.373 (81.812)	Acc@5 99.072 (99.117)
Epoch: [23][12/25]	Time 0.691 (0.731)	Data 0.005 (0.061)	Loss 0.7697 (0.7713)	Acc@1 81.250 (81.768)	Acc@5 99.268 (99.129)
Epoch: [23][13/25]	Time 0.742 (0.732)	Data 0.006 (0.057)	Loss 0.7509 (0.7698)	Acc@1 82.227 (81.801)	Acc@5 99.072 (99.125)
Epoch: [23][14/25]	Time 0.803 (0.737)	Data 0.005 (0.054)	Loss 0.7451 (0.7682)	Acc@1 82.764 (81.865)	Acc@5 99.121 (99.124)
Epoch: [23][15/25]	Time 0.781 (0.739)	Data 0.005 (0.051)	Loss 0.7254 (0.7655)	Acc@1 83.105 (81.943)	Acc@5 99.023 (99.118)
Epoch: [23][16/25]	Time 0.698 (0.737)	Data 0.008 (0.048)	Loss 0.7551 (0.7649)	Acc@1 82.812 (81.994)	Acc@5 98.828 (99.101)
Epoch: [23][17/25]	Time 0.784 (0.740)	Data 0.005 (0.046)	Loss 0.7721 (0.7653)	Acc@1 81.836 (81.985)	Acc@5 99.219 (99.108)
Epoch: [23][18/25]	Time 0.765 (0.741)	Data 0.005 (0.044)	Loss 0.7432 (0.7641)	Acc@1 83.545 (82.067)	Acc@5 98.535 (99.077)
Epoch: [23][19/25]	Time 0.745 (0.741)	Data 0.004 (0.042)	Loss 0.7575 (0.7638)	Acc@1 82.031 (82.065)	Acc@5 98.975 (99.072)
Epoch: [23][20/25]	Time 0.753 (0.742)	Data 0.006 (0.040)	Loss 0.7508 (0.7632)	Acc@1 82.471 (82.085)	Acc@5 99.365 (99.086)
Epoch: [23][21/25]	Time 0.776 (0.743)	Data 0.007 (0.038)	Loss 0.7151 (0.7610)	Acc@1 83.887 (82.167)	Acc@5 99.121 (99.088)
Epoch: [23][22/25]	Time 0.913 (0.751)	Data 0.006 (0.037)	Loss 0.7392 (0.7601)	Acc@1 83.984 (82.246)	Acc@5 98.975 (99.083)
Epoch: [23][23/25]	Time 0.833 (0.754)	Data 0.006 (0.036)	Loss 0.7532 (0.7598)	Acc@1 81.543 (82.216)	Acc@5 99.121 (99.084)
Epoch: [23][24/25]	Time 0.396 (0.740)	Data 0.004 (0.034)	Loss 0.7405 (0.7594)	Acc@1 83.962 (82.246)	Acc@5 99.057 (99.084)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/25]	Time 0.736 (0.736)	Data 0.663 (0.663)	Loss 0.7486 (0.7486)	Acc@1 82.422 (82.422)	Acc@5 99.365 (99.365)
Epoch: [24][1/25]	Time 0.717 (0.727)	Data 0.004 (0.333)	Loss 0.7361 (0.7423)	Acc@1 83.350 (82.886)	Acc@5 99.463 (99.414)
Epoch: [24][2/25]	Time 0.676 (0.710)	Data 0.005 (0.224)	Loss 0.7710 (0.7519)	Acc@1 81.543 (82.438)	Acc@5 98.584 (99.137)
Epoch: [24][3/25]	Time 0.732 (0.715)	Data 0.003 (0.169)	Loss 0.7813 (0.7592)	Acc@1 81.641 (82.239)	Acc@5 98.730 (99.036)
Epoch: [24][4/25]	Time 0.780 (0.728)	Data 0.007 (0.136)	Loss 0.7388 (0.7551)	Acc@1 83.154 (82.422)	Acc@5 99.463 (99.121)
Epoch: [24][5/25]	Time 0.662 (0.717)	Data 0.005 (0.115)	Loss 0.7464 (0.7537)	Acc@1 82.471 (82.430)	Acc@5 99.072 (99.113)
Epoch: [24][6/25]	Time 0.670 (0.710)	Data 0.007 (0.099)	Loss 0.7709 (0.7561)	Acc@1 81.641 (82.317)	Acc@5 99.072 (99.107)
Epoch: [24][7/25]	Time 0.636 (0.701)	Data 0.003 (0.087)	Loss 0.7839 (0.7596)	Acc@1 81.689 (82.239)	Acc@5 99.268 (99.127)
Epoch: [24][8/25]	Time 0.713 (0.703)	Data 0.006 (0.078)	Loss 0.7481 (0.7583)	Acc@1 83.301 (82.357)	Acc@5 98.975 (99.110)
Epoch: [24][9/25]	Time 0.764 (0.709)	Data 0.006 (0.071)	Loss 0.7625 (0.7587)	Acc@1 82.080 (82.329)	Acc@5 99.316 (99.131)
Epoch: [24][10/25]	Time 0.807 (0.718)	Data 0.006 (0.065)	Loss 0.7103 (0.7543)	Acc@1 83.496 (82.435)	Acc@5 99.365 (99.152)
Epoch: [24][11/25]	Time 0.675 (0.714)	Data 0.007 (0.060)	Loss 0.7183 (0.7513)	Acc@1 83.789 (82.548)	Acc@5 98.926 (99.133)
Epoch: [24][12/25]	Time 0.662 (0.710)	Data 0.006 (0.056)	Loss 0.7515 (0.7514)	Acc@1 81.299 (82.452)	Acc@5 99.268 (99.144)
Epoch: [24][13/25]	Time 0.660 (0.706)	Data 0.005 (0.052)	Loss 0.7266 (0.7496)	Acc@1 82.666 (82.467)	Acc@5 99.023 (99.135)
Epoch: [24][14/25]	Time 0.659 (0.703)	Data 0.007 (0.049)	Loss 0.7512 (0.7497)	Acc@1 82.715 (82.484)	Acc@5 99.268 (99.144)
Epoch: [24][15/25]	Time 0.652 (0.700)	Data 0.006 (0.047)	Loss 0.7372 (0.7489)	Acc@1 82.275 (82.471)	Acc@5 99.170 (99.146)
Epoch: [24][16/25]	Time 0.729 (0.702)	Data 0.006 (0.044)	Loss 0.7144 (0.7469)	Acc@1 82.959 (82.499)	Acc@5 98.975 (99.135)
Epoch: [24][17/25]	Time 0.763 (0.705)	Data 0.006 (0.042)	Loss 0.7355 (0.7462)	Acc@1 83.252 (82.541)	Acc@5 99.316 (99.146)
Epoch: [24][18/25]	Time 0.677 (0.704)	Data 0.007 (0.040)	Loss 0.7144 (0.7446)	Acc@1 83.936 (82.615)	Acc@5 99.121 (99.144)
Epoch: [24][19/25]	Time 0.663 (0.702)	Data 0.007 (0.039)	Loss 0.7298 (0.7438)	Acc@1 83.105 (82.639)	Acc@5 99.316 (99.153)
Epoch: [24][20/25]	Time 0.713 (0.702)	Data 0.004 (0.037)	Loss 0.7449 (0.7439)	Acc@1 83.301 (82.671)	Acc@5 99.023 (99.147)
Epoch: [24][21/25]	Time 0.742 (0.704)	Data 0.004 (0.036)	Loss 0.7125 (0.7425)	Acc@1 84.033 (82.733)	Acc@5 99.268 (99.152)
Epoch: [24][22/25]	Time 0.773 (0.707)	Data 0.004 (0.034)	Loss 0.6977 (0.7405)	Acc@1 85.107 (82.836)	Acc@5 99.170 (99.153)
Epoch: [24][23/25]	Time 0.729 (0.708)	Data 0.004 (0.033)	Loss 0.6967 (0.7387)	Acc@1 84.326 (82.898)	Acc@5 99.316 (99.160)
Epoch: [24][24/25]	Time 0.440 (0.697)	Data 0.007 (0.032)	Loss 0.7329 (0.7386)	Acc@1 83.608 (82.910)	Acc@5 98.821 (99.154)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/25]	Time 0.854 (0.854)	Data 0.622 (0.622)	Loss 0.7208 (0.7208)	Acc@1 83.594 (83.594)	Acc@5 99.365 (99.365)
Epoch: [25][1/25]	Time 0.647 (0.751)	Data 0.009 (0.316)	Loss 0.7236 (0.7222)	Acc@1 83.203 (83.398)	Acc@5 99.414 (99.390)
Epoch: [25][2/25]	Time 0.641 (0.714)	Data 0.005 (0.212)	Loss 0.7158 (0.7201)	Acc@1 83.984 (83.594)	Acc@5 99.072 (99.284)
Epoch: [25][3/25]	Time 0.620 (0.691)	Data 0.003 (0.160)	Loss 0.6959 (0.7141)	Acc@1 82.959 (83.435)	Acc@5 99.512 (99.341)
Epoch: [25][4/25]	Time 0.719 (0.696)	Data 0.006 (0.129)	Loss 0.7222 (0.7157)	Acc@1 82.471 (83.242)	Acc@5 99.219 (99.316)
Epoch: [25][5/25]	Time 0.646 (0.688)	Data 0.010 (0.109)	Loss 0.7001 (0.7131)	Acc@1 83.496 (83.285)	Acc@5 99.609 (99.365)
Epoch: [25][6/25]	Time 0.662 (0.684)	Data 0.004 (0.094)	Loss 0.7152 (0.7134)	Acc@1 83.594 (83.329)	Acc@5 99.268 (99.351)
Epoch: [25][7/25]	Time 0.663 (0.681)	Data 0.007 (0.083)	Loss 0.7229 (0.7146)	Acc@1 83.594 (83.362)	Acc@5 99.170 (99.329)
Epoch: [25][8/25]	Time 0.667 (0.680)	Data 0.007 (0.075)	Loss 0.7217 (0.7154)	Acc@1 83.105 (83.333)	Acc@5 99.316 (99.327)
Epoch: [25][9/25]	Time 0.958 (0.708)	Data 0.008 (0.068)	Loss 0.7612 (0.7200)	Acc@1 82.471 (83.247)	Acc@5 99.072 (99.302)
Epoch: [25][10/25]	Time 0.813 (0.717)	Data 0.005 (0.062)	Loss 0.6837 (0.7167)	Acc@1 84.619 (83.372)	Acc@5 99.365 (99.308)
Epoch: [25][11/25]	Time 0.645 (0.711)	Data 0.004 (0.057)	Loss 0.6961 (0.7149)	Acc@1 84.619 (83.476)	Acc@5 99.121 (99.292)
Epoch: [25][12/25]	Time 0.650 (0.706)	Data 0.007 (0.054)	Loss 0.7118 (0.7147)	Acc@1 83.398 (83.470)	Acc@5 99.219 (99.286)
Epoch: [25][13/25]	Time 0.708 (0.707)	Data 0.007 (0.050)	Loss 0.7115 (0.7145)	Acc@1 83.887 (83.500)	Acc@5 99.219 (99.282)
Epoch: [25][14/25]	Time 0.794 (0.712)	Data 0.008 (0.047)	Loss 0.7195 (0.7148)	Acc@1 83.691 (83.512)	Acc@5 99.268 (99.281)
Epoch: [25][15/25]	Time 0.693 (0.711)	Data 0.007 (0.045)	Loss 0.7226 (0.7153)	Acc@1 82.812 (83.469)	Acc@5 99.414 (99.289)
Epoch: [25][16/25]	Time 0.632 (0.707)	Data 0.011 (0.043)	Loss 0.7175 (0.7154)	Acc@1 82.861 (83.433)	Acc@5 99.170 (99.282)
Epoch: [25][17/25]	Time 0.690 (0.706)	Data 0.005 (0.041)	Loss 0.6938 (0.7142)	Acc@1 84.521 (83.493)	Acc@5 99.365 (99.287)
Epoch: [25][18/25]	Time 0.711 (0.706)	Data 0.004 (0.039)	Loss 0.7230 (0.7147)	Acc@1 83.545 (83.496)	Acc@5 99.023 (99.273)
Epoch: [25][19/25]	Time 0.786 (0.710)	Data 0.006 (0.037)	Loss 0.7335 (0.7156)	Acc@1 82.422 (83.442)	Acc@5 99.414 (99.280)
Epoch: [25][20/25]	Time 0.726 (0.711)	Data 0.007 (0.036)	Loss 0.7213 (0.7159)	Acc@1 84.082 (83.473)	Acc@5 98.779 (99.256)
Epoch: [25][21/25]	Time 0.704 (0.710)	Data 0.004 (0.034)	Loss 0.6947 (0.7149)	Acc@1 85.059 (83.545)	Acc@5 99.121 (99.250)
Epoch: [25][22/25]	Time 0.661 (0.708)	Data 0.005 (0.033)	Loss 0.7090 (0.7147)	Acc@1 83.984 (83.564)	Acc@5 99.072 (99.242)
Epoch: [25][23/25]	Time 0.727 (0.709)	Data 0.007 (0.032)	Loss 0.7342 (0.7155)	Acc@1 83.496 (83.561)	Acc@5 98.877 (99.227)
Epoch: [25][24/25]	Time 0.413 (0.697)	Data 0.006 (0.031)	Loss 0.6926 (0.7151)	Acc@1 84.198 (83.572)	Acc@5 99.528 (99.232)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [26 | 180] LR: 0.100000
Epoch: [26][0/25]	Time 0.744 (0.744)	Data 0.610 (0.610)	Loss 0.7160 (0.7160)	Acc@1 83.643 (83.643)	Acc@5 99.023 (99.023)
Epoch: [26][1/25]	Time 0.701 (0.723)	Data 0.005 (0.308)	Loss 0.6423 (0.6791)	Acc@1 86.328 (84.985)	Acc@5 99.414 (99.219)
Epoch: [26][2/25]	Time 0.676 (0.707)	Data 0.006 (0.207)	Loss 0.6471 (0.6684)	Acc@1 87.109 (85.693)	Acc@5 99.512 (99.316)
Epoch: [26][3/25]	Time 0.736 (0.714)	Data 0.006 (0.157)	Loss 0.6585 (0.6660)	Acc@1 85.596 (85.669)	Acc@5 99.414 (99.341)
Epoch: [26][4/25]	Time 0.751 (0.722)	Data 0.005 (0.127)	Loss 0.6577 (0.6643)	Acc@1 85.400 (85.615)	Acc@5 99.365 (99.346)
Epoch: [26][5/25]	Time 0.711 (0.720)	Data 0.005 (0.106)	Loss 0.6577 (0.6632)	Acc@1 85.059 (85.522)	Acc@5 99.414 (99.357)
Epoch: [26][6/25]	Time 0.686 (0.715)	Data 0.006 (0.092)	Loss 0.6644 (0.6634)	Acc@1 85.645 (85.540)	Acc@5 99.414 (99.365)
Epoch: [26][7/25]	Time 0.625 (0.704)	Data 0.005 (0.081)	Loss 0.6524 (0.6620)	Acc@1 85.547 (85.541)	Acc@5 99.414 (99.371)
Epoch: [26][8/25]	Time 0.659 (0.699)	Data 0.004 (0.073)	Loss 0.6758 (0.6635)	Acc@1 85.107 (85.493)	Acc@5 99.219 (99.354)
Epoch: [26][9/25]	Time 0.638 (0.693)	Data 0.005 (0.066)	Loss 0.6650 (0.6637)	Acc@1 84.717 (85.415)	Acc@5 99.365 (99.355)
Epoch: [26][10/25]	Time 0.666 (0.690)	Data 0.008 (0.061)	Loss 0.6451 (0.6620)	Acc@1 86.279 (85.494)	Acc@5 99.121 (99.334)
Epoch: [26][11/25]	Time 0.654 (0.687)	Data 0.004 (0.056)	Loss 0.6594 (0.6618)	Acc@1 86.279 (85.559)	Acc@5 99.463 (99.345)
Epoch: [26][12/25]	Time 0.635 (0.683)	Data 0.010 (0.052)	Loss 0.6967 (0.6645)	Acc@1 84.619 (85.487)	Acc@5 99.072 (99.324)
Epoch: [26][13/25]	Time 0.711 (0.685)	Data 0.007 (0.049)	Loss 0.6996 (0.6670)	Acc@1 83.691 (85.359)	Acc@5 99.365 (99.327)
Epoch: [26][14/25]	Time 0.778 (0.691)	Data 0.008 (0.046)	Loss 0.6874 (0.6683)	Acc@1 83.984 (85.267)	Acc@5 99.072 (99.310)
Epoch: [26][15/25]	Time 0.693 (0.692)	Data 0.006 (0.044)	Loss 0.6618 (0.6679)	Acc@1 85.010 (85.251)	Acc@5 99.316 (99.310)
Epoch: [26][16/25]	Time 0.689 (0.691)	Data 0.008 (0.042)	Loss 0.6637 (0.6677)	Acc@1 85.889 (85.288)	Acc@5 99.268 (99.308)
Epoch: [26][17/25]	Time 0.724 (0.693)	Data 0.005 (0.040)	Loss 0.6598 (0.6672)	Acc@1 85.596 (85.305)	Acc@5 99.463 (99.316)
Epoch: [26][18/25]	Time 0.772 (0.697)	Data 0.006 (0.038)	Loss 0.6822 (0.6680)	Acc@1 84.180 (85.246)	Acc@5 99.316 (99.316)
Epoch: [26][19/25]	Time 0.729 (0.699)	Data 0.005 (0.036)	Loss 0.6926 (0.6693)	Acc@1 85.010 (85.234)	Acc@5 99.121 (99.307)
Epoch: [26][20/25]	Time 0.712 (0.699)	Data 0.004 (0.035)	Loss 0.6495 (0.6683)	Acc@1 85.840 (85.263)	Acc@5 99.512 (99.316)
Epoch: [26][21/25]	Time 0.747 (0.702)	Data 0.005 (0.033)	Loss 0.6936 (0.6695)	Acc@1 83.984 (85.205)	Acc@5 99.512 (99.325)
Epoch: [26][22/25]	Time 0.759 (0.704)	Data 0.005 (0.032)	Loss 0.6847 (0.6701)	Acc@1 84.717 (85.184)	Acc@5 99.219 (99.321)
Epoch: [26][23/25]	Time 0.714 (0.704)	Data 0.005 (0.031)	Loss 0.7042 (0.6715)	Acc@1 83.447 (85.111)	Acc@5 99.414 (99.325)
Epoch: [26][24/25]	Time 0.372 (0.691)	Data 0.004 (0.030)	Loss 0.7276 (0.6725)	Acc@1 83.373 (85.082)	Acc@5 99.292 (99.324)

Epoch: [27 | 180] LR: 0.100000
Epoch: [27][0/25]	Time 0.730 (0.730)	Data 0.623 (0.623)	Loss 0.7243 (0.7243)	Acc@1 81.738 (81.738)	Acc@5 99.072 (99.072)
Epoch: [27][1/25]	Time 0.694 (0.712)	Data 0.005 (0.314)	Loss 0.6742 (0.6993)	Acc@1 85.400 (83.569)	Acc@5 99.414 (99.243)
Epoch: [27][2/25]	Time 0.746 (0.723)	Data 0.003 (0.211)	Loss 0.7130 (0.7038)	Acc@1 83.496 (83.545)	Acc@5 99.121 (99.202)
Epoch: [27][3/25]	Time 0.774 (0.736)	Data 0.007 (0.160)	Loss 0.6920 (0.7009)	Acc@1 84.033 (83.667)	Acc@5 99.414 (99.255)
Epoch: [27][4/25]	Time 0.734 (0.736)	Data 0.009 (0.129)	Loss 0.6683 (0.6944)	Acc@1 84.814 (83.896)	Acc@5 99.316 (99.268)
Epoch: [27][5/25]	Time 0.684 (0.727)	Data 0.005 (0.109)	Loss 0.6869 (0.6931)	Acc@1 84.473 (83.993)	Acc@5 99.561 (99.316)
Epoch: [27][6/25]	Time 0.679 (0.720)	Data 0.005 (0.094)	Loss 0.6873 (0.6923)	Acc@1 84.619 (84.082)	Acc@5 99.463 (99.337)
Epoch: [27][7/25]	Time 0.757 (0.725)	Data 0.009 (0.083)	Loss 0.7312 (0.6971)	Acc@1 83.154 (83.966)	Acc@5 99.023 (99.298)
Epoch: [27][8/25]	Time 0.751 (0.728)	Data 0.006 (0.075)	Loss 0.6808 (0.6953)	Acc@1 84.766 (84.055)	Acc@5 99.170 (99.284)
Epoch: [27][9/25]	Time 0.703 (0.725)	Data 0.007 (0.068)	Loss 0.7088 (0.6967)	Acc@1 83.008 (83.950)	Acc@5 99.512 (99.307)
Epoch: [27][10/25]	Time 0.756 (0.728)	Data 0.008 (0.063)	Loss 0.6738 (0.6946)	Acc@1 84.961 (84.042)	Acc@5 99.365 (99.312)
Epoch: [27][11/25]	Time 0.731 (0.728)	Data 0.005 (0.058)	Loss 0.6665 (0.6923)	Acc@1 84.717 (84.098)	Acc@5 99.316 (99.312)
Epoch: [27][12/25]	Time 0.743 (0.729)	Data 0.005 (0.054)	Loss 0.6931 (0.6923)	Acc@1 84.521 (84.131)	Acc@5 99.316 (99.313)
Epoch: [27][13/25]	Time 0.741 (0.730)	Data 0.008 (0.050)	Loss 0.6889 (0.6921)	Acc@1 84.668 (84.169)	Acc@5 99.707 (99.341)
Epoch: [27][14/25]	Time 0.774 (0.733)	Data 0.005 (0.047)	Loss 0.6851 (0.6916)	Acc@1 83.789 (84.144)	Acc@5 99.414 (99.346)
Epoch: [27][15/25]	Time 0.719 (0.732)	Data 0.005 (0.045)	Loss 0.6879 (0.6914)	Acc@1 85.596 (84.235)	Acc@5 99.219 (99.338)
Epoch: [27][16/25]	Time 0.763 (0.734)	Data 0.009 (0.043)	Loss 0.6762 (0.6905)	Acc@1 84.961 (84.277)	Acc@5 99.170 (99.328)
Epoch: [27][17/25]	Time 0.759 (0.735)	Data 0.005 (0.041)	Loss 0.6764 (0.6897)	Acc@1 84.863 (84.310)	Acc@5 99.316 (99.327)
Epoch: [27][18/25]	Time 0.697 (0.733)	Data 0.004 (0.039)	Loss 0.6763 (0.6890)	Acc@1 84.473 (84.318)	Acc@5 99.463 (99.334)
Epoch: [27][19/25]	Time 0.744 (0.734)	Data 0.005 (0.037)	Loss 0.6876 (0.6889)	Acc@1 84.717 (84.338)	Acc@5 99.268 (99.331)
Epoch: [27][20/25]	Time 0.774 (0.736)	Data 0.004 (0.035)	Loss 0.6981 (0.6894)	Acc@1 84.229 (84.333)	Acc@5 99.414 (99.335)
Epoch: [27][21/25]	Time 0.703 (0.734)	Data 0.004 (0.034)	Loss 0.6713 (0.6885)	Acc@1 85.254 (84.375)	Acc@5 99.463 (99.341)
Epoch: [27][22/25]	Time 0.656 (0.731)	Data 0.007 (0.033)	Loss 0.6858 (0.6884)	Acc@1 84.717 (84.390)	Acc@5 99.219 (99.336)
Epoch: [27][23/25]	Time 0.660 (0.728)	Data 0.007 (0.032)	Loss 0.7068 (0.6892)	Acc@1 83.838 (84.367)	Acc@5 99.170 (99.329)
Epoch: [27][24/25]	Time 0.367 (0.713)	Data 0.004 (0.031)	Loss 0.6624 (0.6887)	Acc@1 84.316 (84.366)	Acc@5 99.882 (99.338)

Epoch: [28 | 180] LR: 0.100000
Epoch: [28][0/25]	Time 0.807 (0.807)	Data 0.681 (0.681)	Loss 0.6559 (0.6559)	Acc@1 84.912 (84.912)	Acc@5 99.170 (99.170)
Epoch: [28][1/25]	Time 0.799 (0.803)	Data 0.005 (0.343)	Loss 0.6230 (0.6395)	Acc@1 86.279 (85.596)	Acc@5 99.414 (99.292)
Epoch: [28][2/25]	Time 0.707 (0.771)	Data 0.005 (0.230)	Loss 0.6891 (0.6560)	Acc@1 84.277 (85.156)	Acc@5 99.316 (99.300)
Epoch: [28][3/25]	Time 0.673 (0.747)	Data 0.008 (0.175)	Loss 0.6831 (0.6628)	Acc@1 84.473 (84.985)	Acc@5 99.316 (99.304)
Epoch: [28][4/25]	Time 0.744 (0.746)	Data 0.006 (0.141)	Loss 0.6806 (0.6663)	Acc@1 84.717 (84.932)	Acc@5 99.805 (99.404)
Epoch: [28][5/25]	Time 0.759 (0.748)	Data 0.003 (0.118)	Loss 0.6746 (0.6677)	Acc@1 84.717 (84.896)	Acc@5 99.463 (99.414)
Epoch: [28][6/25]	Time 0.727 (0.745)	Data 0.007 (0.102)	Loss 0.7074 (0.6734)	Acc@1 83.740 (84.731)	Acc@5 99.219 (99.386)
Epoch: [28][7/25]	Time 0.626 (0.730)	Data 0.005 (0.090)	Loss 0.7055 (0.6774)	Acc@1 83.545 (84.583)	Acc@5 99.121 (99.353)
Epoch: [28][8/25]	Time 0.689 (0.726)	Data 0.006 (0.081)	Loss 0.7136 (0.6814)	Acc@1 84.131 (84.532)	Acc@5 99.463 (99.365)
Epoch: [28][9/25]	Time 0.652 (0.718)	Data 0.005 (0.073)	Loss 0.7173 (0.6850)	Acc@1 83.545 (84.434)	Acc@5 99.072 (99.336)
Epoch: [28][10/25]	Time 0.694 (0.716)	Data 0.006 (0.067)	Loss 0.6920 (0.6857)	Acc@1 83.838 (84.379)	Acc@5 99.219 (99.325)
Epoch: [28][11/25]	Time 0.756 (0.720)	Data 0.004 (0.062)	Loss 0.6631 (0.6838)	Acc@1 85.938 (84.509)	Acc@5 99.658 (99.353)
Epoch: [28][12/25]	Time 0.726 (0.720)	Data 0.005 (0.057)	Loss 0.7008 (0.6851)	Acc@1 84.277 (84.491)	Acc@5 99.219 (99.343)
Epoch: [28][13/25]	Time 0.658 (0.716)	Data 0.007 (0.054)	Loss 0.6803 (0.6847)	Acc@1 85.254 (84.546)	Acc@5 99.365 (99.344)
Epoch: [28][14/25]	Time 0.684 (0.714)	Data 0.007 (0.051)	Loss 0.7164 (0.6869)	Acc@1 83.105 (84.450)	Acc@5 99.170 (99.333)
Epoch: [28][15/25]	Time 0.692 (0.712)	Data 0.010 (0.048)	Loss 0.6682 (0.6857)	Acc@1 85.010 (84.485)	Acc@5 99.756 (99.359)
Epoch: [28][16/25]	Time 0.788 (0.717)	Data 0.005 (0.046)	Loss 0.6597 (0.6842)	Acc@1 85.742 (84.559)	Acc@5 99.365 (99.359)
Epoch: [28][17/25]	Time 0.717 (0.717)	Data 0.006 (0.043)	Loss 0.6627 (0.6830)	Acc@1 86.328 (84.657)	Acc@5 99.365 (99.360)
Epoch: [28][18/25]	Time 0.715 (0.717)	Data 0.004 (0.041)	Loss 0.7356 (0.6857)	Acc@1 83.252 (84.583)	Acc@5 98.730 (99.327)
Epoch: [28][19/25]	Time 0.727 (0.717)	Data 0.007 (0.040)	Loss 0.7387 (0.6884)	Acc@1 82.715 (84.490)	Acc@5 98.682 (99.294)
Epoch: [28][20/25]	Time 0.775 (0.720)	Data 0.005 (0.038)	Loss 0.6767 (0.6878)	Acc@1 85.010 (84.515)	Acc@5 99.609 (99.309)
Epoch: [28][21/25]	Time 0.693 (0.719)	Data 0.004 (0.036)	Loss 0.6606 (0.6866)	Acc@1 84.863 (84.530)	Acc@5 99.609 (99.323)
Epoch: [28][22/25]	Time 0.634 (0.715)	Data 0.007 (0.035)	Loss 0.6512 (0.6851)	Acc@1 86.084 (84.598)	Acc@5 99.658 (99.338)
Epoch: [28][23/25]	Time 0.663 (0.713)	Data 0.007 (0.034)	Loss 0.6350 (0.6830)	Acc@1 86.572 (84.680)	Acc@5 99.707 (99.353)
Epoch: [28][24/25]	Time 0.355 (0.698)	Data 0.008 (0.033)	Loss 0.7095 (0.6834)	Acc@1 83.844 (84.666)	Acc@5 99.528 (99.356)

Epoch: [29 | 180] LR: 0.100000
Epoch: [29][0/25]	Time 0.766 (0.766)	Data 0.653 (0.653)	Loss 0.7008 (0.7008)	Acc@1 83.936 (83.936)	Acc@5 99.219 (99.219)
Epoch: [29][1/25]	Time 0.694 (0.730)	Data 0.005 (0.329)	Loss 0.7081 (0.7045)	Acc@1 83.887 (83.911)	Acc@5 99.414 (99.316)
Epoch: [29][2/25]	Time 0.710 (0.723)	Data 0.007 (0.222)	Loss 0.6503 (0.6864)	Acc@1 85.791 (84.538)	Acc@5 99.658 (99.430)
Epoch: [29][3/25]	Time 0.749 (0.730)	Data 0.009 (0.168)	Loss 0.6981 (0.6893)	Acc@1 83.789 (84.351)	Acc@5 99.316 (99.402)
Epoch: [29][4/25]	Time 0.733 (0.730)	Data 0.008 (0.136)	Loss 0.6861 (0.6887)	Acc@1 84.521 (84.385)	Acc@5 99.463 (99.414)
Epoch: [29][5/25]	Time 0.753 (0.734)	Data 0.007 (0.115)	Loss 0.6714 (0.6858)	Acc@1 85.254 (84.530)	Acc@5 99.414 (99.414)
Epoch: [29][6/25]	Time 0.733 (0.734)	Data 0.004 (0.099)	Loss 0.6627 (0.6825)	Acc@1 86.133 (84.759)	Acc@5 99.072 (99.365)
Epoch: [29][7/25]	Time 0.751 (0.736)	Data 0.004 (0.087)	Loss 0.7084 (0.6857)	Acc@1 84.033 (84.668)	Acc@5 99.365 (99.365)
Epoch: [29][8/25]	Time 0.788 (0.742)	Data 0.005 (0.078)	Loss 0.6567 (0.6825)	Acc@1 85.107 (84.717)	Acc@5 99.316 (99.360)
Epoch: [29][9/25]	Time 0.718 (0.739)	Data 0.009 (0.071)	Loss 0.7014 (0.6844)	Acc@1 83.447 (84.590)	Acc@5 99.512 (99.375)
Epoch: [29][10/25]	Time 0.721 (0.738)	Data 0.007 (0.065)	Loss 0.6812 (0.6841)	Acc@1 85.352 (84.659)	Acc@5 99.268 (99.365)
Epoch: [29][11/25]	Time 0.726 (0.737)	Data 0.007 (0.060)	Loss 0.6776 (0.6836)	Acc@1 85.547 (84.733)	Acc@5 99.512 (99.377)
Epoch: [29][12/25]	Time 0.797 (0.741)	Data 0.004 (0.056)	Loss 0.6485 (0.6809)	Acc@1 86.670 (84.882)	Acc@5 99.316 (99.373)
Epoch: [29][13/25]	Time 0.713 (0.739)	Data 0.008 (0.053)	Loss 0.6470 (0.6784)	Acc@1 86.182 (84.975)	Acc@5 99.463 (99.379)
Epoch: [29][14/25]	Time 0.736 (0.739)	Data 0.007 (0.050)	Loss 0.6574 (0.6770)	Acc@1 85.547 (85.013)	Acc@5 99.316 (99.375)
Epoch: [29][15/25]	Time 0.780 (0.742)	Data 0.005 (0.047)	Loss 0.6976 (0.6783)	Acc@1 84.717 (84.995)	Acc@5 99.463 (99.380)
Epoch: [29][16/25]	Time 0.695 (0.739)	Data 0.007 (0.044)	Loss 0.6608 (0.6773)	Acc@1 86.035 (85.056)	Acc@5 99.219 (99.371)
Epoch: [29][17/25]	Time 0.750 (0.740)	Data 0.007 (0.042)	Loss 0.6955 (0.6783)	Acc@1 85.254 (85.067)	Acc@5 99.268 (99.365)
Epoch: [29][18/25]	Time 0.770 (0.741)	Data 0.006 (0.040)	Loss 0.6821 (0.6785)	Acc@1 84.766 (85.051)	Acc@5 99.170 (99.355)
Epoch: [29][19/25]	Time 0.736 (0.741)	Data 0.004 (0.039)	Loss 0.6880 (0.6790)	Acc@1 85.352 (85.066)	Acc@5 98.926 (99.333)
Epoch: [29][20/25]	Time 0.786 (0.743)	Data 0.006 (0.037)	Loss 0.6529 (0.6777)	Acc@1 86.865 (85.152)	Acc@5 99.268 (99.330)
Epoch: [29][21/25]	Time 0.753 (0.744)	Data 0.005 (0.036)	Loss 0.6767 (0.6777)	Acc@1 85.547 (85.170)	Acc@5 99.316 (99.330)
Epoch: [29][22/25]	Time 0.710 (0.742)	Data 0.004 (0.034)	Loss 0.6409 (0.6761)	Acc@1 86.621 (85.233)	Acc@5 99.463 (99.336)
Epoch: [29][23/25]	Time 0.747 (0.742)	Data 0.005 (0.033)	Loss 0.6789 (0.6762)	Acc@1 85.498 (85.244)	Acc@5 99.219 (99.331)
Epoch: [29][24/25]	Time 0.407 (0.729)	Data 0.009 (0.032)	Loss 0.7203 (0.6770)	Acc@1 84.080 (85.224)	Acc@5 99.410 (99.332)

Epoch: [30 | 180] LR: 0.100000
Epoch: [30][0/25]	Time 0.746 (0.746)	Data 0.769 (0.769)	Loss 0.6825 (0.6825)	Acc@1 84.082 (84.082)	Acc@5 99.512 (99.512)
Epoch: [30][1/25]	Time 0.690 (0.718)	Data 0.005 (0.387)	Loss 0.7310 (0.7068)	Acc@1 82.471 (83.276)	Acc@5 99.316 (99.414)
Epoch: [30][2/25]	Time 0.795 (0.744)	Data 0.008 (0.261)	Loss 0.6911 (0.7015)	Acc@1 84.570 (83.708)	Acc@5 99.268 (99.365)
Epoch: [30][3/25]	Time 0.748 (0.745)	Data 0.005 (0.197)	Loss 0.7190 (0.7059)	Acc@1 83.594 (83.679)	Acc@5 99.121 (99.304)
Epoch: [30][4/25]	Time 0.710 (0.738)	Data 0.007 (0.159)	Loss 0.6479 (0.6943)	Acc@1 85.986 (84.141)	Acc@5 99.707 (99.385)
Epoch: [30][5/25]	Time 0.754 (0.741)	Data 0.004 (0.133)	Loss 0.6570 (0.6881)	Acc@1 85.986 (84.448)	Acc@5 99.170 (99.349)
Epoch: [30][6/25]	Time 0.720 (0.738)	Data 0.005 (0.115)	Loss 0.6883 (0.6881)	Acc@1 85.303 (84.570)	Acc@5 99.316 (99.344)
Epoch: [30][7/25]	Time 0.759 (0.740)	Data 0.006 (0.101)	Loss 0.6780 (0.6868)	Acc@1 84.912 (84.613)	Acc@5 99.512 (99.365)
Epoch: [30][8/25]	Time 0.715 (0.737)	Data 0.005 (0.091)	Loss 0.6288 (0.6804)	Acc@1 86.914 (84.869)	Acc@5 99.658 (99.398)
Epoch: [30][9/25]	Time 0.763 (0.740)	Data 0.006 (0.082)	Loss 0.6521 (0.6776)	Acc@1 86.182 (85.000)	Acc@5 99.707 (99.429)
Epoch: [30][10/25]	Time 0.776 (0.743)	Data 0.004 (0.075)	Loss 0.6452 (0.6746)	Acc@1 86.035 (85.094)	Acc@5 99.365 (99.423)
Epoch: [30][11/25]	Time 0.718 (0.741)	Data 0.005 (0.069)	Loss 0.6820 (0.6752)	Acc@1 85.010 (85.087)	Acc@5 99.365 (99.418)
Epoch: [30][12/25]	Time 0.651 (0.734)	Data 0.009 (0.065)	Loss 0.6426 (0.6727)	Acc@1 86.865 (85.224)	Acc@5 99.316 (99.410)
Epoch: [30][13/25]	Time 0.627 (0.727)	Data 0.007 (0.060)	Loss 0.6912 (0.6740)	Acc@1 84.912 (85.202)	Acc@5 99.414 (99.411)
Epoch: [30][14/25]	Time 0.713 (0.726)	Data 0.005 (0.057)	Loss 0.6761 (0.6742)	Acc@1 85.400 (85.215)	Acc@5 99.512 (99.417)
Epoch: [30][15/25]	Time 0.706 (0.724)	Data 0.007 (0.054)	Loss 0.6878 (0.6750)	Acc@1 84.570 (85.175)	Acc@5 99.463 (99.420)
Epoch: [30][16/25]	Time 0.745 (0.726)	Data 0.005 (0.051)	Loss 0.6759 (0.6751)	Acc@1 84.912 (85.159)	Acc@5 99.414 (99.420)
Epoch: [30][17/25]	Time 0.742 (0.727)	Data 0.004 (0.048)	Loss 0.6685 (0.6747)	Acc@1 84.961 (85.148)	Acc@5 99.316 (99.414)
Epoch: [30][18/25]	Time 0.719 (0.726)	Data 0.005 (0.046)	Loss 0.6852 (0.6753)	Acc@1 84.961 (85.138)	Acc@5 99.512 (99.419)
Epoch: [30][19/25]	Time 0.777 (0.729)	Data 0.006 (0.044)	Loss 0.6952 (0.6763)	Acc@1 85.352 (85.149)	Acc@5 99.121 (99.404)
Epoch: [30][20/25]	Time 0.750 (0.730)	Data 0.006 (0.042)	Loss 0.7164 (0.6782)	Acc@1 83.203 (85.056)	Acc@5 99.268 (99.398)
Epoch: [30][21/25]	Time 0.670 (0.727)	Data 0.005 (0.040)	Loss 0.7091 (0.6796)	Acc@1 84.131 (85.014)	Acc@5 99.219 (99.390)
Epoch: [30][22/25]	Time 0.654 (0.724)	Data 0.005 (0.039)	Loss 0.6685 (0.6791)	Acc@1 86.377 (85.073)	Acc@5 99.170 (99.380)
Epoch: [30][23/25]	Time 0.639 (0.720)	Data 0.007 (0.038)	Loss 0.6568 (0.6782)	Acc@1 86.035 (85.114)	Acc@5 99.365 (99.379)
Epoch: [30][24/25]	Time 0.392 (0.707)	Data 0.005 (0.036)	Loss 0.6637 (0.6779)	Acc@1 85.377 (85.118)	Acc@5 99.528 (99.382)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [31 | 180] LR: 0.100000
Epoch: [31][0/25]	Time 0.729 (0.729)	Data 0.756 (0.756)	Loss 0.6324 (0.6324)	Acc@1 87.646 (87.646)	Acc@5 99.414 (99.414)
Epoch: [31][1/25]	Time 0.700 (0.714)	Data 0.003 (0.380)	Loss 0.6207 (0.6265)	Acc@1 87.695 (87.671)	Acc@5 99.268 (99.341)
Epoch: [31][2/25]	Time 0.754 (0.728)	Data 0.004 (0.255)	Loss 0.5791 (0.6107)	Acc@1 88.770 (88.037)	Acc@5 99.561 (99.414)
Epoch: [31][3/25]	Time 0.777 (0.740)	Data 0.007 (0.193)	Loss 0.6215 (0.6134)	Acc@1 86.670 (87.695)	Acc@5 99.658 (99.475)
Epoch: [31][4/25]	Time 0.680 (0.728)	Data 0.008 (0.156)	Loss 0.6010 (0.6109)	Acc@1 87.451 (87.646)	Acc@5 99.609 (99.502)
Epoch: [31][5/25]	Time 0.660 (0.717)	Data 0.007 (0.131)	Loss 0.5601 (0.6025)	Acc@1 89.307 (87.923)	Acc@5 99.707 (99.536)
Epoch: [31][6/25]	Time 0.660 (0.709)	Data 0.003 (0.113)	Loss 0.6295 (0.6063)	Acc@1 87.207 (87.821)	Acc@5 99.414 (99.519)
Epoch: [31][7/25]	Time 0.687 (0.706)	Data 0.006 (0.099)	Loss 0.6125 (0.6071)	Acc@1 87.598 (87.793)	Acc@5 99.512 (99.518)
Epoch: [31][8/25]	Time 0.707 (0.706)	Data 0.006 (0.089)	Loss 0.6160 (0.6081)	Acc@1 88.037 (87.820)	Acc@5 99.658 (99.533)
Epoch: [31][9/25]	Time 0.707 (0.706)	Data 0.004 (0.081)	Loss 0.6197 (0.6092)	Acc@1 87.256 (87.764)	Acc@5 99.561 (99.536)
Epoch: [31][10/25]	Time 0.779 (0.713)	Data 0.008 (0.074)	Loss 0.6321 (0.6113)	Acc@1 87.158 (87.709)	Acc@5 99.561 (99.538)
Epoch: [31][11/25]	Time 0.703 (0.712)	Data 0.008 (0.068)	Loss 0.5984 (0.6102)	Acc@1 87.402 (87.683)	Acc@5 99.561 (99.540)
Epoch: [31][12/25]	Time 0.745 (0.714)	Data 0.007 (0.064)	Loss 0.5942 (0.6090)	Acc@1 88.232 (87.725)	Acc@5 99.658 (99.549)
Epoch: [31][13/25]	Time 0.788 (0.720)	Data 0.006 (0.060)	Loss 0.6081 (0.6089)	Acc@1 87.842 (87.734)	Acc@5 99.414 (99.540)
Epoch: [31][14/25]	Time 0.697 (0.718)	Data 0.007 (0.056)	Loss 0.6003 (0.6084)	Acc@1 88.574 (87.790)	Acc@5 99.268 (99.521)
Epoch: [31][15/25]	Time 0.665 (0.715)	Data 0.007 (0.053)	Loss 0.6312 (0.6098)	Acc@1 86.719 (87.723)	Acc@5 99.316 (99.509)
Epoch: [31][16/25]	Time 0.665 (0.712)	Data 0.006 (0.050)	Loss 0.6228 (0.6106)	Acc@1 87.402 (87.704)	Acc@5 99.463 (99.506)
Epoch: [31][17/25]	Time 0.636 (0.708)	Data 0.005 (0.048)	Loss 0.6170 (0.6109)	Acc@1 87.744 (87.706)	Acc@5 99.707 (99.517)
Epoch: [31][18/25]	Time 0.637 (0.704)	Data 0.007 (0.046)	Loss 0.6382 (0.6123)	Acc@1 86.328 (87.634)	Acc@5 99.609 (99.522)
Epoch: [31][19/25]	Time 0.665 (0.702)	Data 0.007 (0.044)	Loss 0.5912 (0.6113)	Acc@1 87.842 (87.644)	Acc@5 99.707 (99.531)
Epoch: [31][20/25]	Time 0.705 (0.702)	Data 0.007 (0.042)	Loss 0.6674 (0.6140)	Acc@1 85.986 (87.565)	Acc@5 99.121 (99.512)
Epoch: [31][21/25]	Time 0.777 (0.706)	Data 0.004 (0.040)	Loss 0.6741 (0.6167)	Acc@1 85.156 (87.456)	Acc@5 99.512 (99.512)
Epoch: [31][22/25]	Time 0.735 (0.707)	Data 0.005 (0.039)	Loss 0.6502 (0.6182)	Acc@1 85.352 (87.364)	Acc@5 99.219 (99.499)
Epoch: [31][23/25]	Time 0.751 (0.709)	Data 0.005 (0.037)	Loss 0.6479 (0.6194)	Acc@1 85.889 (87.303)	Acc@5 99.756 (99.510)
Epoch: [31][24/25]	Time 0.422 (0.697)	Data 0.004 (0.036)	Loss 0.6256 (0.6195)	Acc@1 87.264 (87.302)	Acc@5 99.410 (99.508)

Epoch: [32 | 180] LR: 0.100000
Epoch: [32][0/25]	Time 0.718 (0.718)	Data 0.647 (0.647)	Loss 0.6289 (0.6289)	Acc@1 86.768 (86.768)	Acc@5 99.658 (99.658)
Epoch: [32][1/25]	Time 0.686 (0.702)	Data 0.008 (0.327)	Loss 0.6646 (0.6467)	Acc@1 85.693 (86.230)	Acc@5 99.414 (99.536)
Epoch: [32][2/25]	Time 0.745 (0.716)	Data 0.006 (0.220)	Loss 0.6144 (0.6359)	Acc@1 88.135 (86.865)	Acc@5 99.219 (99.430)
Epoch: [32][3/25]	Time 0.773 (0.731)	Data 0.005 (0.167)	Loss 0.6676 (0.6439)	Acc@1 85.693 (86.572)	Acc@5 99.023 (99.329)
Epoch: [32][4/25]	Time 0.720 (0.729)	Data 0.009 (0.135)	Loss 0.6536 (0.6458)	Acc@1 86.182 (86.494)	Acc@5 99.170 (99.297)
Epoch: [32][5/25]	Time 0.629 (0.712)	Data 0.005 (0.113)	Loss 0.6386 (0.6446)	Acc@1 86.768 (86.540)	Acc@5 99.414 (99.316)
Epoch: [32][6/25]	Time 0.676 (0.707)	Data 0.006 (0.098)	Loss 0.6023 (0.6386)	Acc@1 87.109 (86.621)	Acc@5 99.658 (99.365)
Epoch: [32][7/25]	Time 0.723 (0.709)	Data 0.003 (0.086)	Loss 0.6599 (0.6412)	Acc@1 85.303 (86.456)	Acc@5 99.365 (99.365)
Epoch: [32][8/25]	Time 0.726 (0.711)	Data 0.004 (0.077)	Loss 0.6388 (0.6410)	Acc@1 86.670 (86.480)	Acc@5 99.268 (99.354)
Epoch: [32][9/25]	Time 0.721 (0.712)	Data 0.004 (0.070)	Loss 0.6360 (0.6405)	Acc@1 86.475 (86.479)	Acc@5 99.561 (99.375)
Epoch: [32][10/25]	Time 0.708 (0.712)	Data 0.006 (0.064)	Loss 0.6356 (0.6400)	Acc@1 87.158 (86.541)	Acc@5 99.561 (99.392)
Epoch: [32][11/25]	Time 0.744 (0.714)	Data 0.004 (0.059)	Loss 0.6519 (0.6410)	Acc@1 85.986 (86.495)	Acc@5 99.707 (99.418)
Epoch: [32][12/25]	Time 0.773 (0.719)	Data 0.008 (0.055)	Loss 0.6015 (0.6380)	Acc@1 87.988 (86.610)	Acc@5 99.414 (99.418)
Epoch: [32][13/25]	Time 0.720 (0.719)	Data 0.007 (0.052)	Loss 0.6439 (0.6384)	Acc@1 86.719 (86.618)	Acc@5 99.170 (99.400)
Epoch: [32][14/25]	Time 0.757 (0.721)	Data 0.006 (0.049)	Loss 0.6693 (0.6404)	Acc@1 85.693 (86.556)	Acc@5 99.170 (99.385)
Epoch: [32][15/25]	Time 0.771 (0.724)	Data 0.005 (0.046)	Loss 0.6544 (0.6413)	Acc@1 85.840 (86.511)	Acc@5 99.170 (99.371)
Epoch: [32][16/25]	Time 0.756 (0.726)	Data 0.006 (0.044)	Loss 0.6282 (0.6405)	Acc@1 86.914 (86.535)	Acc@5 99.561 (99.382)
Epoch: [32][17/25]	Time 0.789 (0.730)	Data 0.008 (0.042)	Loss 0.6260 (0.6397)	Acc@1 87.500 (86.589)	Acc@5 99.658 (99.398)
Epoch: [32][18/25]	Time 0.714 (0.729)	Data 0.004 (0.040)	Loss 0.6803 (0.6419)	Acc@1 84.473 (86.477)	Acc@5 99.170 (99.386)
Epoch: [32][19/25]	Time 0.740 (0.730)	Data 0.004 (0.038)	Loss 0.6589 (0.6427)	Acc@1 85.010 (86.404)	Acc@5 99.463 (99.390)
Epoch: [32][20/25]	Time 0.786 (0.732)	Data 0.005 (0.036)	Loss 0.6471 (0.6429)	Acc@1 86.230 (86.396)	Acc@5 99.512 (99.395)
Epoch: [32][21/25]	Time 0.696 (0.731)	Data 0.005 (0.035)	Loss 0.6613 (0.6438)	Acc@1 85.107 (86.337)	Acc@5 99.609 (99.405)
Epoch: [32][22/25]	Time 0.762 (0.732)	Data 0.004 (0.034)	Loss 0.6492 (0.6440)	Acc@1 85.840 (86.315)	Acc@5 99.365 (99.403)
Epoch: [32][23/25]	Time 0.766 (0.733)	Data 0.004 (0.032)	Loss 0.6671 (0.6450)	Acc@1 85.059 (86.263)	Acc@5 99.365 (99.402)
Epoch: [32][24/25]	Time 0.411 (0.720)	Data 0.004 (0.031)	Loss 0.6566 (0.6452)	Acc@1 84.670 (86.236)	Acc@5 99.175 (99.398)

Epoch: [33 | 180] LR: 0.100000
Epoch: [33][0/25]	Time 0.758 (0.758)	Data 0.791 (0.791)	Loss 0.6603 (0.6603)	Acc@1 85.986 (85.986)	Acc@5 99.170 (99.170)
Epoch: [33][1/25]	Time 0.717 (0.738)	Data 0.005 (0.398)	Loss 0.6746 (0.6674)	Acc@1 85.596 (85.791)	Acc@5 99.219 (99.194)
Epoch: [33][2/25]	Time 0.748 (0.741)	Data 0.006 (0.267)	Loss 0.6130 (0.6493)	Acc@1 86.963 (86.182)	Acc@5 99.902 (99.430)
Epoch: [33][3/25]	Time 0.741 (0.741)	Data 0.006 (0.202)	Loss 0.6429 (0.6477)	Acc@1 85.840 (86.096)	Acc@5 99.365 (99.414)
Epoch: [33][4/25]	Time 0.693 (0.732)	Data 0.009 (0.163)	Loss 0.6348 (0.6451)	Acc@1 86.572 (86.191)	Acc@5 99.512 (99.434)
Epoch: [33][5/25]	Time 0.763 (0.737)	Data 0.008 (0.138)	Loss 0.6191 (0.6408)	Acc@1 87.012 (86.328)	Acc@5 99.756 (99.487)
Epoch: [33][6/25]	Time 0.728 (0.736)	Data 0.004 (0.119)	Loss 0.6952 (0.6485)	Acc@1 84.375 (86.049)	Acc@5 99.219 (99.449)
Epoch: [33][7/25]	Time 0.745 (0.737)	Data 0.005 (0.104)	Loss 0.6207 (0.6451)	Acc@1 87.451 (86.224)	Acc@5 99.609 (99.469)
Epoch: [33][8/25]	Time 0.723 (0.735)	Data 0.007 (0.094)	Loss 0.6254 (0.6429)	Acc@1 86.963 (86.306)	Acc@5 99.463 (99.468)
Epoch: [33][9/25]	Time 0.758 (0.738)	Data 0.007 (0.085)	Loss 0.6434 (0.6429)	Acc@1 85.498 (86.226)	Acc@5 99.316 (99.453)
Epoch: [33][10/25]	Time 0.709 (0.735)	Data 0.005 (0.078)	Loss 0.6522 (0.6438)	Acc@1 85.254 (86.137)	Acc@5 99.170 (99.427)
Epoch: [33][11/25]	Time 0.737 (0.735)	Data 0.008 (0.072)	Loss 0.6226 (0.6420)	Acc@1 87.451 (86.247)	Acc@5 99.365 (99.422)
Epoch: [33][12/25]	Time 0.776 (0.738)	Data 0.005 (0.067)	Loss 0.6804 (0.6450)	Acc@1 84.863 (86.140)	Acc@5 99.170 (99.403)
Epoch: [33][13/25]	Time 0.694 (0.735)	Data 0.005 (0.062)	Loss 0.6332 (0.6441)	Acc@1 86.914 (86.196)	Acc@5 99.609 (99.418)
Epoch: [33][14/25]	Time 0.661 (0.730)	Data 0.005 (0.058)	Loss 0.6605 (0.6452)	Acc@1 86.035 (86.185)	Acc@5 99.365 (99.414)
Epoch: [33][15/25]	Time 0.692 (0.728)	Data 0.008 (0.055)	Loss 0.6535 (0.6457)	Acc@1 86.377 (86.197)	Acc@5 99.609 (99.426)
Epoch: [33][16/25]	Time 0.729 (0.728)	Data 0.008 (0.052)	Loss 0.6431 (0.6456)	Acc@1 86.816 (86.233)	Acc@5 99.609 (99.437)
Epoch: [33][17/25]	Time 0.761 (0.730)	Data 0.005 (0.050)	Loss 0.6290 (0.6446)	Acc@1 87.891 (86.325)	Acc@5 99.170 (99.422)
Epoch: [33][18/25]	Time 0.697 (0.728)	Data 0.004 (0.047)	Loss 0.6706 (0.6460)	Acc@1 85.596 (86.287)	Acc@5 99.268 (99.414)
Epoch: [33][19/25]	Time 0.623 (0.723)	Data 0.007 (0.045)	Loss 0.6432 (0.6459)	Acc@1 86.328 (86.289)	Acc@5 99.463 (99.417)
Epoch: [33][20/25]	Time 0.706 (0.722)	Data 0.005 (0.043)	Loss 0.6359 (0.6454)	Acc@1 86.670 (86.307)	Acc@5 99.414 (99.416)
Epoch: [33][21/25]	Time 0.731 (0.722)	Data 0.007 (0.042)	Loss 0.6861 (0.6472)	Acc@1 85.498 (86.270)	Acc@5 99.121 (99.403)
Epoch: [33][22/25]	Time 0.729 (0.723)	Data 0.007 (0.040)	Loss 0.6545 (0.6476)	Acc@1 85.986 (86.258)	Acc@5 99.561 (99.410)
Epoch: [33][23/25]	Time 0.723 (0.723)	Data 0.007 (0.039)	Loss 0.6584 (0.6480)	Acc@1 85.303 (86.218)	Acc@5 99.561 (99.416)
Epoch: [33][24/25]	Time 0.428 (0.711)	Data 0.005 (0.037)	Loss 0.6296 (0.6477)	Acc@1 87.618 (86.242)	Acc@5 99.175 (99.412)

Epoch: [34 | 180] LR: 0.100000
Epoch: [34][0/25]	Time 0.756 (0.756)	Data 0.662 (0.662)	Loss 0.6386 (0.6386)	Acc@1 86.865 (86.865)	Acc@5 99.316 (99.316)
Epoch: [34][1/25]	Time 0.722 (0.739)	Data 0.010 (0.336)	Loss 0.6128 (0.6257)	Acc@1 87.598 (87.231)	Acc@5 99.609 (99.463)
Epoch: [34][2/25]	Time 0.764 (0.748)	Data 0.008 (0.226)	Loss 0.6227 (0.6247)	Acc@1 87.549 (87.337)	Acc@5 99.756 (99.561)
Epoch: [34][3/25]	Time 0.797 (0.760)	Data 0.004 (0.171)	Loss 0.6320 (0.6265)	Acc@1 87.256 (87.317)	Acc@5 99.658 (99.585)
Epoch: [34][4/25]	Time 0.933 (0.794)	Data 0.004 (0.137)	Loss 0.6243 (0.6261)	Acc@1 87.402 (87.334)	Acc@5 99.414 (99.551)
Epoch: [34][5/25]	Time 0.782 (0.792)	Data 0.013 (0.117)	Loss 0.6127 (0.6239)	Acc@1 86.816 (87.248)	Acc@5 99.658 (99.569)
Epoch: [34][6/25]	Time 0.789 (0.792)	Data 0.004 (0.101)	Loss 0.6416 (0.6264)	Acc@1 86.377 (87.123)	Acc@5 99.609 (99.574)
Epoch: [34][7/25]	Time 0.701 (0.781)	Data 0.004 (0.089)	Loss 0.6119 (0.6246)	Acc@1 87.598 (87.183)	Acc@5 99.609 (99.579)
Epoch: [34][8/25]	Time 0.650 (0.766)	Data 0.009 (0.080)	Loss 0.6215 (0.6242)	Acc@1 87.451 (87.212)	Acc@5 99.414 (99.561)
Epoch: [34][9/25]	Time 0.679 (0.757)	Data 0.007 (0.072)	Loss 0.6194 (0.6238)	Acc@1 86.816 (87.173)	Acc@5 99.658 (99.570)
Epoch: [34][10/25]	Time 0.689 (0.751)	Data 0.004 (0.066)	Loss 0.6117 (0.6227)	Acc@1 88.086 (87.256)	Acc@5 99.365 (99.552)
Epoch: [34][11/25]	Time 0.750 (0.751)	Data 0.006 (0.061)	Loss 0.6341 (0.6236)	Acc@1 87.158 (87.248)	Acc@5 99.463 (99.544)
Epoch: [34][12/25]	Time 0.717 (0.748)	Data 0.007 (0.057)	Loss 0.6593 (0.6263)	Acc@1 85.742 (87.132)	Acc@5 99.121 (99.512)
Epoch: [34][13/25]	Time 0.695 (0.745)	Data 0.008 (0.053)	Loss 0.6569 (0.6285)	Acc@1 86.865 (87.113)	Acc@5 99.365 (99.501)
Epoch: [34][14/25]	Time 0.739 (0.744)	Data 0.004 (0.050)	Loss 0.6109 (0.6273)	Acc@1 86.816 (87.093)	Acc@5 99.561 (99.505)
Epoch: [34][15/25]	Time 0.710 (0.742)	Data 0.007 (0.048)	Loss 0.6210 (0.6270)	Acc@1 87.744 (87.134)	Acc@5 99.365 (99.496)
Epoch: [34][16/25]	Time 0.740 (0.742)	Data 0.007 (0.045)	Loss 0.6104 (0.6260)	Acc@1 87.549 (87.158)	Acc@5 99.609 (99.503)
Epoch: [34][17/25]	Time 0.727 (0.741)	Data 0.005 (0.043)	Loss 0.6449 (0.6270)	Acc@1 85.840 (87.085)	Acc@5 99.561 (99.506)
Epoch: [34][18/25]	Time 0.867 (0.748)	Data 0.004 (0.041)	Loss 0.6255 (0.6269)	Acc@1 87.549 (87.109)	Acc@5 99.805 (99.522)
Epoch: [34][19/25]	Time 0.808 (0.751)	Data 0.005 (0.039)	Loss 0.6037 (0.6258)	Acc@1 87.988 (87.153)	Acc@5 99.512 (99.521)
Epoch: [34][20/25]	Time 0.751 (0.751)	Data 0.005 (0.037)	Loss 0.6185 (0.6254)	Acc@1 87.744 (87.181)	Acc@5 99.512 (99.521)
Epoch: [34][21/25]	Time 0.801 (0.753)	Data 0.007 (0.036)	Loss 0.6379 (0.6260)	Acc@1 86.182 (87.136)	Acc@5 99.414 (99.516)
Epoch: [34][22/25]	Time 0.710 (0.751)	Data 0.006 (0.035)	Loss 0.6238 (0.6259)	Acc@1 87.061 (87.133)	Acc@5 99.561 (99.518)
Epoch: [34][23/25]	Time 0.695 (0.749)	Data 0.005 (0.034)	Loss 0.6592 (0.6273)	Acc@1 85.986 (87.085)	Acc@5 99.463 (99.516)
Epoch: [34][24/25]	Time 0.417 (0.736)	Data 0.005 (0.032)	Loss 0.6321 (0.6274)	Acc@1 87.500 (87.092)	Acc@5 99.292 (99.512)

Epoch: [35 | 180] LR: 0.100000
Epoch: [35][0/25]	Time 0.769 (0.769)	Data 0.626 (0.626)	Loss 0.6511 (0.6511)	Acc@1 86.328 (86.328)	Acc@5 99.658 (99.658)
Epoch: [35][1/25]	Time 0.720 (0.744)	Data 0.008 (0.317)	Loss 0.6484 (0.6498)	Acc@1 86.816 (86.572)	Acc@5 99.268 (99.463)
Epoch: [35][2/25]	Time 0.712 (0.733)	Data 0.005 (0.213)	Loss 0.6504 (0.6500)	Acc@1 85.791 (86.312)	Acc@5 99.365 (99.430)
Epoch: [35][3/25]	Time 0.766 (0.742)	Data 0.006 (0.161)	Loss 0.6363 (0.6466)	Acc@1 86.475 (86.353)	Acc@5 99.609 (99.475)
Epoch: [35][4/25]	Time 0.703 (0.734)	Data 0.006 (0.130)	Loss 0.6477 (0.6468)	Acc@1 86.035 (86.289)	Acc@5 99.707 (99.521)
Epoch: [35][5/25]	Time 0.672 (0.724)	Data 0.007 (0.110)	Loss 0.6607 (0.6491)	Acc@1 85.400 (86.141)	Acc@5 99.219 (99.471)
Epoch: [35][6/25]	Time 0.701 (0.720)	Data 0.006 (0.095)	Loss 0.6441 (0.6484)	Acc@1 86.572 (86.203)	Acc@5 99.561 (99.484)
Epoch: [35][7/25]	Time 0.646 (0.711)	Data 0.005 (0.084)	Loss 0.6186 (0.6447)	Acc@1 87.109 (86.316)	Acc@5 99.512 (99.487)
Epoch: [35][8/25]	Time 0.758 (0.716)	Data 0.004 (0.075)	Loss 0.6755 (0.6481)	Acc@1 85.156 (86.187)	Acc@5 99.463 (99.485)
Epoch: [35][9/25]	Time 0.757 (0.720)	Data 0.007 (0.068)	Loss 0.6403 (0.6473)	Acc@1 86.230 (86.191)	Acc@5 99.414 (99.478)
Epoch: [35][10/25]	Time 0.670 (0.716)	Data 0.005 (0.062)	Loss 0.6443 (0.6471)	Acc@1 86.914 (86.257)	Acc@5 99.512 (99.481)
Epoch: [35][11/25]	Time 0.646 (0.710)	Data 0.008 (0.058)	Loss 0.6163 (0.6445)	Acc@1 87.939 (86.397)	Acc@5 99.561 (99.487)
Epoch: [35][12/25]	Time 0.620 (0.703)	Data 0.006 (0.054)	Loss 0.6594 (0.6456)	Acc@1 85.693 (86.343)	Acc@5 99.512 (99.489)
Epoch: [35][13/25]	Time 0.684 (0.702)	Data 0.007 (0.051)	Loss 0.6657 (0.6471)	Acc@1 85.107 (86.255)	Acc@5 99.756 (99.508)
Epoch: [35][14/25]	Time 0.642 (0.698)	Data 0.005 (0.048)	Loss 0.6236 (0.6455)	Acc@1 87.207 (86.318)	Acc@5 99.658 (99.518)
Epoch: [35][15/25]	Time 0.704 (0.698)	Data 0.004 (0.045)	Loss 0.6296 (0.6445)	Acc@1 86.377 (86.322)	Acc@5 99.561 (99.521)
Epoch: [35][16/25]	Time 0.760 (0.702)	Data 0.005 (0.042)	Loss 0.6451 (0.6445)	Acc@1 86.182 (86.314)	Acc@5 99.561 (99.523)
Epoch: [35][17/25]	Time 0.763 (0.705)	Data 0.004 (0.040)	Loss 0.6601 (0.6454)	Acc@1 85.791 (86.285)	Acc@5 99.658 (99.531)
Epoch: [35][18/25]	Time 0.677 (0.704)	Data 0.004 (0.038)	Loss 0.6618 (0.6463)	Acc@1 85.547 (86.246)	Acc@5 99.463 (99.527)
Epoch: [35][19/25]	Time 0.735 (0.705)	Data 0.006 (0.037)	Loss 0.6970 (0.6488)	Acc@1 84.717 (86.169)	Acc@5 99.316 (99.517)
Epoch: [35][20/25]	Time 0.767 (0.708)	Data 0.005 (0.035)	Loss 0.6468 (0.6487)	Acc@1 85.840 (86.154)	Acc@5 99.609 (99.521)
Epoch: [35][21/25]	Time 0.746 (0.710)	Data 0.006 (0.034)	Loss 0.6462 (0.6486)	Acc@1 85.889 (86.142)	Acc@5 99.463 (99.518)
Epoch: [35][22/25]	Time 0.744 (0.711)	Data 0.007 (0.033)	Loss 0.6573 (0.6490)	Acc@1 85.596 (86.118)	Acc@5 99.414 (99.514)
Epoch: [35][23/25]	Time 0.732 (0.712)	Data 0.008 (0.032)	Loss 0.6424 (0.6487)	Acc@1 86.182 (86.121)	Acc@5 99.268 (99.504)
Epoch: [35][24/25]	Time 0.453 (0.702)	Data 0.004 (0.031)	Loss 0.6671 (0.6490)	Acc@1 86.203 (86.122)	Acc@5 99.057 (99.496)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [36 | 180] LR: 0.100000
Epoch: [36][0/25]	Time 0.769 (0.769)	Data 0.615 (0.615)	Loss 0.6157 (0.6157)	Acc@1 87.158 (87.158)	Acc@5 99.561 (99.561)
Epoch: [36][1/25]	Time 0.712 (0.741)	Data 0.004 (0.309)	Loss 0.5877 (0.6017)	Acc@1 88.770 (87.964)	Acc@5 99.512 (99.536)
Epoch: [36][2/25]	Time 0.691 (0.724)	Data 0.009 (0.209)	Loss 0.6141 (0.6058)	Acc@1 86.719 (87.549)	Acc@5 99.561 (99.544)
Epoch: [36][3/25]	Time 0.721 (0.723)	Data 0.005 (0.158)	Loss 0.5692 (0.5967)	Acc@1 88.916 (87.891)	Acc@5 99.609 (99.561)
Epoch: [36][4/25]	Time 0.754 (0.730)	Data 0.005 (0.128)	Loss 0.5891 (0.5952)	Acc@1 88.281 (87.969)	Acc@5 99.658 (99.580)
Epoch: [36][5/25]	Time 0.690 (0.723)	Data 0.004 (0.107)	Loss 0.5854 (0.5935)	Acc@1 88.672 (88.086)	Acc@5 99.463 (99.561)
Epoch: [36][6/25]	Time 0.735 (0.725)	Data 0.006 (0.093)	Loss 0.5978 (0.5941)	Acc@1 88.086 (88.086)	Acc@5 99.756 (99.588)
Epoch: [36][7/25]	Time 0.761 (0.729)	Data 0.003 (0.081)	Loss 0.5592 (0.5898)	Acc@1 89.697 (88.287)	Acc@5 99.707 (99.603)
Epoch: [36][8/25]	Time 0.711 (0.727)	Data 0.005 (0.073)	Loss 0.5808 (0.5888)	Acc@1 88.818 (88.346)	Acc@5 99.561 (99.599)
Epoch: [36][9/25]	Time 0.722 (0.727)	Data 0.007 (0.066)	Loss 0.5945 (0.5894)	Acc@1 88.232 (88.335)	Acc@5 99.512 (99.590)
Epoch: [36][10/25]	Time 0.743 (0.728)	Data 0.005 (0.061)	Loss 0.5613 (0.5868)	Acc@1 88.672 (88.366)	Acc@5 99.707 (99.600)
Epoch: [36][11/25]	Time 0.764 (0.731)	Data 0.005 (0.056)	Loss 0.5892 (0.5870)	Acc@1 87.939 (88.330)	Acc@5 99.609 (99.601)
Epoch: [36][12/25]	Time 0.745 (0.732)	Data 0.005 (0.052)	Loss 0.5769 (0.5862)	Acc@1 89.258 (88.401)	Acc@5 99.365 (99.583)
Epoch: [36][13/25]	Time 0.756 (0.734)	Data 0.005 (0.049)	Loss 0.5968 (0.5870)	Acc@1 88.037 (88.375)	Acc@5 99.658 (99.588)
Epoch: [36][14/25]	Time 0.701 (0.732)	Data 0.007 (0.046)	Loss 0.5940 (0.5874)	Acc@1 88.232 (88.366)	Acc@5 99.561 (99.587)
Epoch: [36][15/25]	Time 0.710 (0.730)	Data 0.005 (0.043)	Loss 0.5620 (0.5859)	Acc@1 89.453 (88.434)	Acc@5 99.658 (99.591)
Epoch: [36][16/25]	Time 0.723 (0.730)	Data 0.007 (0.041)	Loss 0.5761 (0.5853)	Acc@1 88.721 (88.451)	Acc@5 99.561 (99.589)
Epoch: [36][17/25]	Time 0.687 (0.727)	Data 0.004 (0.039)	Loss 0.6003 (0.5861)	Acc@1 88.379 (88.447)	Acc@5 99.609 (99.590)
Epoch: [36][18/25]	Time 0.738 (0.728)	Data 0.004 (0.037)	Loss 0.6138 (0.5876)	Acc@1 88.428 (88.446)	Acc@5 99.561 (99.589)
Epoch: [36][19/25]	Time 0.735 (0.728)	Data 0.004 (0.036)	Loss 0.6040 (0.5884)	Acc@1 87.256 (88.386)	Acc@5 99.512 (99.585)
Epoch: [36][20/25]	Time 0.720 (0.728)	Data 0.004 (0.034)	Loss 0.6118 (0.5895)	Acc@1 88.428 (88.388)	Acc@5 99.707 (99.591)
Epoch: [36][21/25]	Time 0.780 (0.730)	Data 0.005 (0.033)	Loss 0.6227 (0.5910)	Acc@1 87.402 (88.343)	Acc@5 99.316 (99.578)
Epoch: [36][22/25]	Time 0.767 (0.732)	Data 0.004 (0.032)	Loss 0.6125 (0.5920)	Acc@1 87.598 (88.311)	Acc@5 99.512 (99.575)
Epoch: [36][23/25]	Time 0.789 (0.734)	Data 0.006 (0.030)	Loss 0.6065 (0.5926)	Acc@1 87.842 (88.291)	Acc@5 99.609 (99.577)
Epoch: [36][24/25]	Time 0.404 (0.721)	Data 0.005 (0.029)	Loss 0.6178 (0.5930)	Acc@1 86.792 (88.266)	Acc@5 99.646 (99.578)

Epoch: [37 | 180] LR: 0.100000
Epoch: [37][0/25]	Time 0.761 (0.761)	Data 0.654 (0.654)	Loss 0.5666 (0.5666)	Acc@1 88.818 (88.818)	Acc@5 99.707 (99.707)
Epoch: [37][1/25]	Time 0.724 (0.743)	Data 0.007 (0.331)	Loss 0.5759 (0.5713)	Acc@1 89.355 (89.087)	Acc@5 99.658 (99.683)
Epoch: [37][2/25]	Time 0.643 (0.709)	Data 0.005 (0.222)	Loss 0.6223 (0.5883)	Acc@1 86.670 (88.281)	Acc@5 99.316 (99.561)
Epoch: [37][3/25]	Time 0.689 (0.704)	Data 0.005 (0.168)	Loss 0.6276 (0.5981)	Acc@1 87.305 (88.037)	Acc@5 99.365 (99.512)
Epoch: [37][4/25]	Time 0.754 (0.714)	Data 0.005 (0.135)	Loss 0.6098 (0.6004)	Acc@1 87.109 (87.852)	Acc@5 99.609 (99.531)
Epoch: [37][5/25]	Time 0.754 (0.721)	Data 0.004 (0.113)	Loss 0.6186 (0.6035)	Acc@1 86.914 (87.695)	Acc@5 99.561 (99.536)
Epoch: [37][6/25]	Time 0.739 (0.723)	Data 0.006 (0.098)	Loss 0.5961 (0.6024)	Acc@1 87.939 (87.730)	Acc@5 99.512 (99.533)
Epoch: [37][7/25]	Time 0.705 (0.721)	Data 0.007 (0.087)	Loss 0.6276 (0.6056)	Acc@1 86.914 (87.628)	Acc@5 99.805 (99.567)
Epoch: [37][8/25]	Time 0.769 (0.727)	Data 0.005 (0.078)	Loss 0.6294 (0.6082)	Acc@1 86.377 (87.489)	Acc@5 99.268 (99.533)
Epoch: [37][9/25]	Time 0.715 (0.725)	Data 0.005 (0.070)	Loss 0.6219 (0.6096)	Acc@1 86.816 (87.422)	Acc@5 99.707 (99.551)
Epoch: [37][10/25]	Time 0.708 (0.724)	Data 0.004 (0.064)	Loss 0.6440 (0.6127)	Acc@1 85.889 (87.282)	Acc@5 99.170 (99.516)
Epoch: [37][11/25]	Time 0.783 (0.729)	Data 0.005 (0.059)	Loss 0.6451 (0.6154)	Acc@1 86.426 (87.211)	Acc@5 99.463 (99.512)
Epoch: [37][12/25]	Time 0.686 (0.725)	Data 0.007 (0.055)	Loss 0.5953 (0.6139)	Acc@1 88.867 (87.338)	Acc@5 99.512 (99.512)
Epoch: [37][13/25]	Time 0.706 (0.724)	Data 0.007 (0.052)	Loss 0.6500 (0.6164)	Acc@1 86.377 (87.270)	Acc@5 99.609 (99.519)
Epoch: [37][14/25]	Time 0.775 (0.727)	Data 0.005 (0.049)	Loss 0.6045 (0.6156)	Acc@1 88.379 (87.344)	Acc@5 99.512 (99.518)
Epoch: [37][15/25]	Time 0.735 (0.728)	Data 0.006 (0.046)	Loss 0.6074 (0.6151)	Acc@1 88.037 (87.387)	Acc@5 99.463 (99.515)
Epoch: [37][16/25]	Time 0.721 (0.727)	Data 0.007 (0.044)	Loss 0.6438 (0.6168)	Acc@1 86.279 (87.322)	Acc@5 99.658 (99.523)
Epoch: [37][17/25]	Time 0.787 (0.731)	Data 0.005 (0.042)	Loss 0.6366 (0.6179)	Acc@1 86.279 (87.264)	Acc@5 99.609 (99.528)
Epoch: [37][18/25]	Time 0.695 (0.729)	Data 0.004 (0.040)	Loss 0.6319 (0.6187)	Acc@1 87.012 (87.251)	Acc@5 99.512 (99.527)
Epoch: [37][19/25]	Time 0.726 (0.729)	Data 0.008 (0.038)	Loss 0.6426 (0.6199)	Acc@1 85.986 (87.188)	Acc@5 99.463 (99.524)
Epoch: [37][20/25]	Time 0.752 (0.730)	Data 0.005 (0.036)	Loss 0.6239 (0.6200)	Acc@1 87.012 (87.179)	Acc@5 99.561 (99.526)
Epoch: [37][21/25]	Time 0.764 (0.731)	Data 0.007 (0.035)	Loss 0.6289 (0.6204)	Acc@1 86.279 (87.138)	Acc@5 99.463 (99.523)
Epoch: [37][22/25]	Time 0.726 (0.731)	Data 0.006 (0.034)	Loss 0.5983 (0.6195)	Acc@1 88.672 (87.205)	Acc@5 99.658 (99.529)
Epoch: [37][23/25]	Time 0.771 (0.733)	Data 0.007 (0.033)	Loss 0.6259 (0.6198)	Acc@1 87.061 (87.199)	Acc@5 99.414 (99.524)
Epoch: [37][24/25]	Time 0.457 (0.722)	Data 0.007 (0.032)	Loss 0.6275 (0.6199)	Acc@1 88.090 (87.214)	Acc@5 99.646 (99.526)

Epoch: [38 | 180] LR: 0.100000
Epoch: [38][0/25]	Time 0.775 (0.775)	Data 0.651 (0.651)	Loss 0.6138 (0.6138)	Acc@1 87.549 (87.549)	Acc@5 99.365 (99.365)
Epoch: [38][1/25]	Time 0.698 (0.736)	Data 0.011 (0.331)	Loss 0.6131 (0.6134)	Acc@1 88.428 (87.988)	Acc@5 99.512 (99.438)
Epoch: [38][2/25]	Time 0.711 (0.728)	Data 0.003 (0.222)	Loss 0.6268 (0.6179)	Acc@1 87.842 (87.939)	Acc@5 99.512 (99.463)
Epoch: [38][3/25]	Time 0.743 (0.732)	Data 0.007 (0.168)	Loss 0.6340 (0.6219)	Acc@1 87.061 (87.720)	Acc@5 99.463 (99.463)
Epoch: [38][4/25]	Time 0.790 (0.743)	Data 0.006 (0.136)	Loss 0.6296 (0.6234)	Acc@1 87.061 (87.588)	Acc@5 99.658 (99.502)
Epoch: [38][5/25]	Time 0.712 (0.738)	Data 0.006 (0.114)	Loss 0.6156 (0.6221)	Acc@1 87.793 (87.622)	Acc@5 99.609 (99.520)
Epoch: [38][6/25]	Time 0.656 (0.726)	Data 0.005 (0.098)	Loss 0.6267 (0.6228)	Acc@1 86.865 (87.514)	Acc@5 99.512 (99.519)
Epoch: [38][7/25]	Time 0.695 (0.722)	Data 0.006 (0.087)	Loss 0.6241 (0.6230)	Acc@1 87.793 (87.549)	Acc@5 99.316 (99.493)
Epoch: [38][8/25]	Time 0.776 (0.728)	Data 0.006 (0.078)	Loss 0.6240 (0.6231)	Acc@1 87.598 (87.554)	Acc@5 99.512 (99.495)
Epoch: [38][9/25]	Time 0.735 (0.729)	Data 0.005 (0.071)	Loss 0.6658 (0.6274)	Acc@1 85.400 (87.339)	Acc@5 99.365 (99.482)
Epoch: [38][10/25]	Time 0.726 (0.729)	Data 0.008 (0.065)	Loss 0.5926 (0.6242)	Acc@1 88.281 (87.425)	Acc@5 99.707 (99.503)
Epoch: [38][11/25]	Time 0.762 (0.732)	Data 0.006 (0.060)	Loss 0.6474 (0.6261)	Acc@1 87.109 (87.398)	Acc@5 99.365 (99.491)
Epoch: [38][12/25]	Time 0.772 (0.735)	Data 0.006 (0.056)	Loss 0.6574 (0.6285)	Acc@1 86.230 (87.308)	Acc@5 99.121 (99.463)
Epoch: [38][13/25]	Time 0.705 (0.733)	Data 0.007 (0.052)	Loss 0.6200 (0.6279)	Acc@1 86.523 (87.252)	Acc@5 99.609 (99.473)
Epoch: [38][14/25]	Time 0.715 (0.731)	Data 0.006 (0.049)	Loss 0.6294 (0.6280)	Acc@1 86.523 (87.204)	Acc@5 99.609 (99.482)
Epoch: [38][15/25]	Time 0.747 (0.732)	Data 0.010 (0.047)	Loss 0.6206 (0.6276)	Acc@1 87.646 (87.231)	Acc@5 99.609 (99.490)
Epoch: [38][16/25]	Time 0.798 (0.736)	Data 0.007 (0.044)	Loss 0.6045 (0.6262)	Acc@1 88.086 (87.282)	Acc@5 99.854 (99.512)
Epoch: [38][17/25]	Time 0.736 (0.736)	Data 0.005 (0.042)	Loss 0.6348 (0.6267)	Acc@1 86.426 (87.234)	Acc@5 99.463 (99.509)
Epoch: [38][18/25]	Time 0.645 (0.731)	Data 0.007 (0.040)	Loss 0.6355 (0.6271)	Acc@1 87.061 (87.225)	Acc@5 99.609 (99.514)
Epoch: [38][19/25]	Time 0.669 (0.728)	Data 0.007 (0.039)	Loss 0.6516 (0.6284)	Acc@1 86.279 (87.178)	Acc@5 99.512 (99.514)
Epoch: [38][20/25]	Time 0.635 (0.724)	Data 0.005 (0.037)	Loss 0.6232 (0.6281)	Acc@1 87.158 (87.177)	Acc@5 99.561 (99.516)
Epoch: [38][21/25]	Time 0.696 (0.723)	Data 0.006 (0.036)	Loss 0.6514 (0.6292)	Acc@1 86.230 (87.134)	Acc@5 99.414 (99.512)
Epoch: [38][22/25]	Time 0.658 (0.720)	Data 0.006 (0.034)	Loss 0.6695 (0.6309)	Acc@1 85.205 (87.050)	Acc@5 99.512 (99.512)
Epoch: [38][23/25]	Time 0.640 (0.717)	Data 0.004 (0.033)	Loss 0.6575 (0.6320)	Acc@1 86.377 (87.022)	Acc@5 99.268 (99.502)
Epoch: [38][24/25]	Time 0.351 (0.702)	Data 0.007 (0.032)	Loss 0.6392 (0.6322)	Acc@1 88.325 (87.044)	Acc@5 99.057 (99.494)

Epoch: [39 | 180] LR: 0.100000
Epoch: [39][0/25]	Time 0.812 (0.812)	Data 0.625 (0.625)	Loss 0.6252 (0.6252)	Acc@1 86.768 (86.768)	Acc@5 99.365 (99.365)
Epoch: [39][1/25]	Time 0.688 (0.750)	Data 0.004 (0.315)	Loss 0.6060 (0.6156)	Acc@1 88.135 (87.451)	Acc@5 99.756 (99.561)
Epoch: [39][2/25]	Time 0.734 (0.745)	Data 0.005 (0.211)	Loss 0.6259 (0.6190)	Acc@1 87.500 (87.467)	Acc@5 99.512 (99.544)
Epoch: [39][3/25]	Time 0.719 (0.738)	Data 0.005 (0.160)	Loss 0.6117 (0.6172)	Acc@1 87.402 (87.451)	Acc@5 99.463 (99.524)
Epoch: [39][4/25]	Time 0.726 (0.736)	Data 0.007 (0.129)	Loss 0.6183 (0.6174)	Acc@1 87.256 (87.412)	Acc@5 99.756 (99.570)
Epoch: [39][5/25]	Time 0.719 (0.733)	Data 0.005 (0.109)	Loss 0.6327 (0.6200)	Acc@1 87.158 (87.370)	Acc@5 99.512 (99.561)
Epoch: [39][6/25]	Time 0.777 (0.739)	Data 0.007 (0.094)	Loss 0.6220 (0.6202)	Acc@1 87.842 (87.437)	Acc@5 99.561 (99.561)
Epoch: [39][7/25]	Time 0.680 (0.732)	Data 0.004 (0.083)	Loss 0.6327 (0.6218)	Acc@1 86.914 (87.372)	Acc@5 99.414 (99.542)
Epoch: [39][8/25]	Time 0.654 (0.723)	Data 0.004 (0.074)	Loss 0.6490 (0.6248)	Acc@1 86.328 (87.256)	Acc@5 99.121 (99.495)
Epoch: [39][9/25]	Time 0.670 (0.718)	Data 0.005 (0.067)	Loss 0.6177 (0.6241)	Acc@1 87.109 (87.241)	Acc@5 99.805 (99.526)
Epoch: [39][10/25]	Time 0.667 (0.713)	Data 0.007 (0.062)	Loss 0.5956 (0.6215)	Acc@1 87.939 (87.305)	Acc@5 99.609 (99.534)
Epoch: [39][11/25]	Time 0.732 (0.715)	Data 0.005 (0.057)	Loss 0.6409 (0.6231)	Acc@1 86.816 (87.264)	Acc@5 99.121 (99.500)
Epoch: [39][12/25]	Time 0.780 (0.720)	Data 0.005 (0.053)	Loss 0.6052 (0.6218)	Acc@1 87.061 (87.248)	Acc@5 99.609 (99.508)
Epoch: [39][13/25]	Time 0.685 (0.717)	Data 0.007 (0.050)	Loss 0.6258 (0.6220)	Acc@1 87.158 (87.242)	Acc@5 99.512 (99.508)
Epoch: [39][14/25]	Time 0.702 (0.716)	Data 0.007 (0.047)	Loss 0.6231 (0.6221)	Acc@1 87.451 (87.256)	Acc@5 99.707 (99.521)
Epoch: [39][15/25]	Time 0.767 (0.719)	Data 0.007 (0.044)	Loss 0.5944 (0.6204)	Acc@1 88.623 (87.341)	Acc@5 99.561 (99.524)
Epoch: [39][16/25]	Time 0.690 (0.718)	Data 0.006 (0.042)	Loss 0.6276 (0.6208)	Acc@1 86.719 (87.305)	Acc@5 99.268 (99.509)
Epoch: [39][17/25]	Time 0.691 (0.716)	Data 0.005 (0.040)	Loss 0.6385 (0.6218)	Acc@1 87.646 (87.324)	Acc@5 99.512 (99.509)
Epoch: [39][18/25]	Time 0.745 (0.718)	Data 0.006 (0.038)	Loss 0.6098 (0.6212)	Acc@1 88.037 (87.361)	Acc@5 99.609 (99.514)
Epoch: [39][19/25]	Time 0.758 (0.720)	Data 0.005 (0.037)	Loss 0.6681 (0.6235)	Acc@1 85.742 (87.280)	Acc@5 99.463 (99.512)
Epoch: [39][20/25]	Time 0.722 (0.720)	Data 0.005 (0.035)	Loss 0.5913 (0.6220)	Acc@1 88.232 (87.326)	Acc@5 99.561 (99.514)
Epoch: [39][21/25]	Time 0.716 (0.720)	Data 0.006 (0.034)	Loss 0.6538 (0.6234)	Acc@1 86.768 (87.300)	Acc@5 99.463 (99.512)
Epoch: [39][22/25]	Time 0.765 (0.722)	Data 0.004 (0.033)	Loss 0.6680 (0.6254)	Acc@1 85.596 (87.226)	Acc@5 99.365 (99.505)
Epoch: [39][23/25]	Time 0.734 (0.722)	Data 0.007 (0.031)	Loss 0.6283 (0.6255)	Acc@1 87.305 (87.229)	Acc@5 99.316 (99.497)
Epoch: [39][24/25]	Time 0.422 (0.710)	Data 0.007 (0.030)	Loss 0.6961 (0.6267)	Acc@1 83.491 (87.166)	Acc@5 99.764 (99.502)

Epoch: [40 | 180] LR: 0.100000
Epoch: [40][0/25]	Time 0.750 (0.750)	Data 0.618 (0.618)	Loss 0.5986 (0.5986)	Acc@1 87.598 (87.598)	Acc@5 99.463 (99.463)
Epoch: [40][1/25]	Time 0.740 (0.745)	Data 0.012 (0.315)	Loss 0.6342 (0.6164)	Acc@1 86.914 (87.256)	Acc@5 99.365 (99.414)
Epoch: [40][2/25]	Time 0.718 (0.736)	Data 0.006 (0.212)	Loss 0.6070 (0.6133)	Acc@1 88.770 (87.760)	Acc@5 99.219 (99.349)
Epoch: [40][3/25]	Time 0.728 (0.734)	Data 0.004 (0.160)	Loss 0.5849 (0.6062)	Acc@1 88.135 (87.854)	Acc@5 99.658 (99.426)
Epoch: [40][4/25]	Time 0.764 (0.740)	Data 0.010 (0.130)	Loss 0.6205 (0.6091)	Acc@1 87.598 (87.803)	Acc@5 99.414 (99.424)
Epoch: [40][5/25]	Time 0.732 (0.739)	Data 0.009 (0.110)	Loss 0.6256 (0.6118)	Acc@1 87.793 (87.801)	Acc@5 99.463 (99.430)
Epoch: [40][6/25]	Time 0.664 (0.728)	Data 0.006 (0.095)	Loss 0.6479 (0.6170)	Acc@1 86.230 (87.577)	Acc@5 99.414 (99.428)
Epoch: [40][7/25]	Time 0.638 (0.717)	Data 0.004 (0.084)	Loss 0.6171 (0.6170)	Acc@1 87.305 (87.543)	Acc@5 99.414 (99.426)
Epoch: [40][8/25]	Time 0.629 (0.707)	Data 0.005 (0.075)	Loss 0.6524 (0.6209)	Acc@1 86.279 (87.402)	Acc@5 99.463 (99.430)
Epoch: [40][9/25]	Time 0.628 (0.699)	Data 0.005 (0.068)	Loss 0.6400 (0.6228)	Acc@1 86.279 (87.290)	Acc@5 99.561 (99.443)
Epoch: [40][10/25]	Time 0.673 (0.697)	Data 0.006 (0.062)	Loss 0.6214 (0.6227)	Acc@1 87.256 (87.287)	Acc@5 99.658 (99.463)
Epoch: [40][11/25]	Time 0.643 (0.692)	Data 0.005 (0.058)	Loss 0.6302 (0.6233)	Acc@1 87.451 (87.301)	Acc@5 99.805 (99.491)
Epoch: [40][12/25]	Time 0.667 (0.690)	Data 0.007 (0.054)	Loss 0.6449 (0.6250)	Acc@1 86.670 (87.252)	Acc@5 99.561 (99.497)
Epoch: [40][13/25]	Time 0.650 (0.687)	Data 0.005 (0.050)	Loss 0.5896 (0.6225)	Acc@1 89.062 (87.381)	Acc@5 99.658 (99.508)
Epoch: [40][14/25]	Time 0.664 (0.686)	Data 0.006 (0.047)	Loss 0.6070 (0.6214)	Acc@1 87.598 (87.396)	Acc@5 99.609 (99.515)
Epoch: [40][15/25]	Time 0.681 (0.686)	Data 0.005 (0.045)	Loss 0.6441 (0.6228)	Acc@1 86.963 (87.369)	Acc@5 99.463 (99.512)
Epoch: [40][16/25]	Time 0.762 (0.690)	Data 0.006 (0.042)	Loss 0.6308 (0.6233)	Acc@1 86.963 (87.345)	Acc@5 99.463 (99.509)
Epoch: [40][17/25]	Time 0.772 (0.695)	Data 0.004 (0.040)	Loss 0.6500 (0.6248)	Acc@1 86.133 (87.278)	Acc@5 99.219 (99.493)
Epoch: [40][18/25]	Time 0.735 (0.697)	Data 0.005 (0.038)	Loss 0.6450 (0.6259)	Acc@1 86.670 (87.246)	Acc@5 99.658 (99.501)
Epoch: [40][19/25]	Time 0.807 (0.702)	Data 0.006 (0.037)	Loss 0.5786 (0.6235)	Acc@1 89.209 (87.344)	Acc@5 99.805 (99.517)
Epoch: [40][20/25]	Time 0.700 (0.702)	Data 0.004 (0.035)	Loss 0.6419 (0.6244)	Acc@1 86.084 (87.284)	Acc@5 99.414 (99.512)
Epoch: [40][21/25]	Time 0.640 (0.699)	Data 0.005 (0.034)	Loss 0.6472 (0.6254)	Acc@1 87.012 (87.271)	Acc@5 99.414 (99.507)
Epoch: [40][22/25]	Time 0.647 (0.697)	Data 0.006 (0.033)	Loss 0.6155 (0.6250)	Acc@1 87.695 (87.290)	Acc@5 99.512 (99.507)
Epoch: [40][23/25]	Time 0.726 (0.698)	Data 0.006 (0.031)	Loss 0.6302 (0.6252)	Acc@1 86.133 (87.242)	Acc@5 99.414 (99.504)
Epoch: [40][24/25]	Time 0.461 (0.689)	Data 0.006 (0.030)	Loss 0.6217 (0.6251)	Acc@1 87.500 (87.246)	Acc@5 99.528 (99.504)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [41 | 180] LR: 0.100000
Epoch: [41][0/25]	Time 0.738 (0.738)	Data 0.637 (0.637)	Loss 0.6407 (0.6407)	Acc@1 87.256 (87.256)	Acc@5 99.756 (99.756)
Epoch: [41][1/25]	Time 0.690 (0.714)	Data 0.003 (0.320)	Loss 0.5880 (0.6143)	Acc@1 88.525 (87.891)	Acc@5 99.512 (99.634)
Epoch: [41][2/25]	Time 0.655 (0.694)	Data 0.005 (0.215)	Loss 0.5847 (0.6044)	Acc@1 89.014 (88.265)	Acc@5 99.219 (99.495)
Epoch: [41][3/25]	Time 0.696 (0.695)	Data 0.007 (0.163)	Loss 0.5893 (0.6007)	Acc@1 87.988 (88.196)	Acc@5 99.707 (99.548)
Epoch: [41][4/25]	Time 0.760 (0.708)	Data 0.005 (0.131)	Loss 0.5985 (0.6002)	Acc@1 88.281 (88.213)	Acc@5 99.414 (99.521)
Epoch: [41][5/25]	Time 0.671 (0.701)	Data 0.004 (0.110)	Loss 0.6052 (0.6011)	Acc@1 87.939 (88.167)	Acc@5 99.561 (99.528)
Epoch: [41][6/25]	Time 0.628 (0.691)	Data 0.007 (0.095)	Loss 0.5789 (0.5979)	Acc@1 88.574 (88.225)	Acc@5 99.756 (99.561)
Epoch: [41][7/25]	Time 0.648 (0.686)	Data 0.007 (0.084)	Loss 0.5805 (0.5957)	Acc@1 88.672 (88.281)	Acc@5 99.609 (99.567)
Epoch: [41][8/25]	Time 0.640 (0.681)	Data 0.009 (0.076)	Loss 0.6045 (0.5967)	Acc@1 88.428 (88.298)	Acc@5 99.561 (99.566)
Epoch: [41][9/25]	Time 0.620 (0.675)	Data 0.007 (0.069)	Loss 0.5974 (0.5968)	Acc@1 87.891 (88.257)	Acc@5 99.512 (99.561)
Epoch: [41][10/25]	Time 0.627 (0.670)	Data 0.005 (0.063)	Loss 0.5886 (0.5960)	Acc@1 88.379 (88.268)	Acc@5 99.463 (99.552)
Epoch: [41][11/25]	Time 0.683 (0.671)	Data 0.005 (0.058)	Loss 0.6076 (0.5970)	Acc@1 88.379 (88.277)	Acc@5 99.365 (99.536)
Epoch: [41][12/25]	Time 0.731 (0.676)	Data 0.008 (0.054)	Loss 0.6083 (0.5979)	Acc@1 87.354 (88.206)	Acc@5 99.463 (99.530)
Epoch: [41][13/25]	Time 0.740 (0.680)	Data 0.005 (0.051)	Loss 0.5877 (0.5971)	Acc@1 88.477 (88.225)	Acc@5 99.512 (99.529)
Epoch: [41][14/25]	Time 0.749 (0.685)	Data 0.005 (0.048)	Loss 0.6236 (0.5989)	Acc@1 87.158 (88.154)	Acc@5 99.121 (99.502)
Epoch: [41][15/25]	Time 0.727 (0.688)	Data 0.006 (0.045)	Loss 0.5819 (0.5978)	Acc@1 89.453 (88.235)	Acc@5 99.707 (99.515)
Epoch: [41][16/25]	Time 0.735 (0.690)	Data 0.005 (0.043)	Loss 0.6082 (0.5984)	Acc@1 87.549 (88.195)	Acc@5 99.561 (99.517)
Epoch: [41][17/25]	Time 0.745 (0.693)	Data 0.005 (0.041)	Loss 0.6107 (0.5991)	Acc@1 87.939 (88.181)	Acc@5 99.707 (99.528)
Epoch: [41][18/25]	Time 0.714 (0.695)	Data 0.005 (0.039)	Loss 0.6114 (0.5998)	Acc@1 87.012 (88.119)	Acc@5 99.658 (99.535)
Epoch: [41][19/25]	Time 0.758 (0.698)	Data 0.004 (0.037)	Loss 0.5734 (0.5985)	Acc@1 89.551 (88.191)	Acc@5 99.609 (99.539)
Epoch: [41][20/25]	Time 0.719 (0.699)	Data 0.006 (0.036)	Loss 0.6437 (0.6006)	Acc@1 86.426 (88.107)	Acc@5 99.512 (99.537)
Epoch: [41][21/25]	Time 0.734 (0.700)	Data 0.004 (0.034)	Loss 0.5904 (0.6001)	Acc@1 88.623 (88.130)	Acc@5 99.316 (99.527)
Epoch: [41][22/25]	Time 0.714 (0.701)	Data 0.007 (0.033)	Loss 0.6090 (0.6005)	Acc@1 88.721 (88.156)	Acc@5 99.756 (99.537)
Epoch: [41][23/25]	Time 0.743 (0.703)	Data 0.007 (0.032)	Loss 0.6104 (0.6009)	Acc@1 87.695 (88.137)	Acc@5 99.707 (99.544)
Epoch: [41][24/25]	Time 0.487 (0.694)	Data 0.005 (0.031)	Loss 0.6303 (0.6014)	Acc@1 87.028 (88.118)	Acc@5 99.292 (99.540)

Epoch: [42 | 180] LR: 0.100000
Epoch: [42][0/25]	Time 0.844 (0.844)	Data 0.709 (0.709)	Loss 0.6117 (0.6117)	Acc@1 87.842 (87.842)	Acc@5 99.561 (99.561)
Epoch: [42][1/25]	Time 0.676 (0.760)	Data 0.008 (0.358)	Loss 0.6247 (0.6182)	Acc@1 86.865 (87.354)	Acc@5 99.365 (99.463)
Epoch: [42][2/25]	Time 0.662 (0.727)	Data 0.004 (0.240)	Loss 0.6223 (0.6196)	Acc@1 86.523 (87.077)	Acc@5 99.707 (99.544)
Epoch: [42][3/25]	Time 0.708 (0.722)	Data 0.003 (0.181)	Loss 0.6045 (0.6158)	Acc@1 88.477 (87.427)	Acc@5 99.463 (99.524)
Epoch: [42][4/25]	Time 0.718 (0.722)	Data 0.006 (0.146)	Loss 0.6125 (0.6151)	Acc@1 87.451 (87.432)	Acc@5 99.658 (99.551)
Epoch: [42][5/25]	Time 0.799 (0.735)	Data 0.009 (0.123)	Loss 0.6186 (0.6157)	Acc@1 87.256 (87.402)	Acc@5 99.512 (99.544)
Epoch: [42][6/25]	Time 0.756 (0.738)	Data 0.003 (0.106)	Loss 0.6321 (0.6180)	Acc@1 87.402 (87.402)	Acc@5 99.316 (99.512)
Epoch: [42][7/25]	Time 0.771 (0.742)	Data 0.006 (0.093)	Loss 0.5969 (0.6154)	Acc@1 88.086 (87.488)	Acc@5 99.609 (99.524)
Epoch: [42][8/25]	Time 0.702 (0.737)	Data 0.004 (0.084)	Loss 0.6310 (0.6171)	Acc@1 86.865 (87.419)	Acc@5 99.463 (99.517)
Epoch: [42][9/25]	Time 0.811 (0.745)	Data 0.005 (0.076)	Loss 0.6013 (0.6155)	Acc@1 88.965 (87.573)	Acc@5 99.414 (99.507)
Epoch: [42][10/25]	Time 0.717 (0.742)	Data 0.006 (0.069)	Loss 0.6168 (0.6157)	Acc@1 86.572 (87.482)	Acc@5 99.561 (99.512)
Epoch: [42][11/25]	Time 0.723 (0.741)	Data 0.004 (0.064)	Loss 0.6108 (0.6152)	Acc@1 87.891 (87.516)	Acc@5 99.463 (99.508)
Epoch: [42][12/25]	Time 0.765 (0.742)	Data 0.007 (0.060)	Loss 0.5891 (0.6132)	Acc@1 87.793 (87.538)	Acc@5 99.707 (99.523)
Epoch: [42][13/25]	Time 0.703 (0.740)	Data 0.008 (0.056)	Loss 0.5960 (0.6120)	Acc@1 88.135 (87.580)	Acc@5 99.463 (99.519)
Epoch: [42][14/25]	Time 0.724 (0.739)	Data 0.008 (0.053)	Loss 0.6222 (0.6127)	Acc@1 87.549 (87.578)	Acc@5 99.609 (99.525)
Epoch: [42][15/25]	Time 0.745 (0.739)	Data 0.009 (0.050)	Loss 0.5936 (0.6115)	Acc@1 87.988 (87.604)	Acc@5 99.658 (99.533)
Epoch: [42][16/25]	Time 0.672 (0.735)	Data 0.008 (0.047)	Loss 0.6052 (0.6111)	Acc@1 87.891 (87.621)	Acc@5 99.609 (99.538)
Epoch: [42][17/25]	Time 0.689 (0.733)	Data 0.009 (0.045)	Loss 0.5972 (0.6104)	Acc@1 88.232 (87.655)	Acc@5 99.512 (99.536)
Epoch: [42][18/25]	Time 0.750 (0.733)	Data 0.006 (0.043)	Loss 0.5935 (0.6095)	Acc@1 89.014 (87.726)	Acc@5 99.609 (99.540)
Epoch: [42][19/25]	Time 0.761 (0.735)	Data 0.005 (0.041)	Loss 0.5896 (0.6085)	Acc@1 87.988 (87.739)	Acc@5 99.561 (99.541)
Epoch: [42][20/25]	Time 0.744 (0.735)	Data 0.004 (0.040)	Loss 0.6326 (0.6096)	Acc@1 86.963 (87.702)	Acc@5 99.561 (99.542)
Epoch: [42][21/25]	Time 0.753 (0.736)	Data 0.006 (0.038)	Loss 0.6054 (0.6094)	Acc@1 87.695 (87.702)	Acc@5 99.707 (99.549)
Epoch: [42][22/25]	Time 0.726 (0.736)	Data 0.004 (0.037)	Loss 0.6594 (0.6116)	Acc@1 85.791 (87.619)	Acc@5 99.268 (99.537)
Epoch: [42][23/25]	Time 0.673 (0.733)	Data 0.007 (0.035)	Loss 0.5823 (0.6104)	Acc@1 89.014 (87.677)	Acc@5 99.463 (99.534)
Epoch: [42][24/25]	Time 0.351 (0.718)	Data 0.005 (0.034)	Loss 0.5935 (0.6101)	Acc@1 88.325 (87.688)	Acc@5 99.410 (99.532)

Epoch: [43 | 180] LR: 0.100000
Epoch: [43][0/25]	Time 0.743 (0.743)	Data 0.628 (0.628)	Loss 0.6206 (0.6206)	Acc@1 87.451 (87.451)	Acc@5 99.609 (99.609)
Epoch: [43][1/25]	Time 0.777 (0.760)	Data 0.007 (0.317)	Loss 0.5832 (0.6019)	Acc@1 88.818 (88.135)	Acc@5 99.902 (99.756)
Epoch: [43][2/25]	Time 0.778 (0.766)	Data 0.005 (0.213)	Loss 0.6396 (0.6145)	Acc@1 86.377 (87.549)	Acc@5 99.707 (99.740)
Epoch: [43][3/25]	Time 0.720 (0.755)	Data 0.003 (0.161)	Loss 0.6065 (0.6125)	Acc@1 88.574 (87.805)	Acc@5 99.707 (99.731)
Epoch: [43][4/25]	Time 0.739 (0.751)	Data 0.010 (0.130)	Loss 0.5926 (0.6085)	Acc@1 88.184 (87.881)	Acc@5 99.561 (99.697)
Epoch: [43][5/25]	Time 0.765 (0.754)	Data 0.007 (0.110)	Loss 0.5976 (0.6067)	Acc@1 88.623 (88.005)	Acc@5 99.512 (99.666)
Epoch: [43][6/25]	Time 0.757 (0.754)	Data 0.004 (0.095)	Loss 0.5988 (0.6055)	Acc@1 88.721 (88.107)	Acc@5 99.512 (99.644)
Epoch: [43][7/25]	Time 0.792 (0.759)	Data 0.004 (0.083)	Loss 0.5885 (0.6034)	Acc@1 88.379 (88.141)	Acc@5 99.707 (99.652)
Epoch: [43][8/25]	Time 0.821 (0.766)	Data 0.004 (0.075)	Loss 0.6432 (0.6078)	Acc@1 87.354 (88.053)	Acc@5 99.414 (99.626)
Epoch: [43][9/25]	Time 0.731 (0.762)	Data 0.004 (0.068)	Loss 0.5674 (0.6038)	Acc@1 89.404 (88.188)	Acc@5 99.707 (99.634)
Epoch: [43][10/25]	Time 0.692 (0.756)	Data 0.008 (0.062)	Loss 0.6103 (0.6044)	Acc@1 87.793 (88.153)	Acc@5 99.707 (99.640)
Epoch: [43][11/25]	Time 0.674 (0.749)	Data 0.005 (0.057)	Loss 0.5931 (0.6034)	Acc@1 88.623 (88.192)	Acc@5 99.561 (99.634)
Epoch: [43][12/25]	Time 0.737 (0.748)	Data 0.006 (0.054)	Loss 0.6251 (0.6051)	Acc@1 87.695 (88.154)	Acc@5 99.609 (99.632)
Epoch: [43][13/25]	Time 0.781 (0.750)	Data 0.005 (0.050)	Loss 0.6020 (0.6049)	Acc@1 87.500 (88.107)	Acc@5 99.414 (99.616)
Epoch: [43][14/25]	Time 0.769 (0.752)	Data 0.007 (0.047)	Loss 0.6212 (0.6060)	Acc@1 87.646 (88.076)	Acc@5 99.463 (99.606)
Epoch: [43][15/25]	Time 0.751 (0.752)	Data 0.005 (0.045)	Loss 0.6328 (0.6077)	Acc@1 86.914 (88.004)	Acc@5 99.463 (99.597)
Epoch: [43][16/25]	Time 0.723 (0.750)	Data 0.008 (0.042)	Loss 0.6031 (0.6074)	Acc@1 88.037 (88.006)	Acc@5 99.561 (99.595)
Epoch: [43][17/25]	Time 0.715 (0.748)	Data 0.006 (0.040)	Loss 0.6197 (0.6081)	Acc@1 87.549 (87.980)	Acc@5 99.561 (99.593)
Epoch: [43][18/25]	Time 0.668 (0.744)	Data 0.005 (0.039)	Loss 0.6029 (0.6078)	Acc@1 88.184 (87.991)	Acc@5 99.219 (99.573)
Epoch: [43][19/25]	Time 0.644 (0.739)	Data 0.004 (0.037)	Loss 0.5751 (0.6062)	Acc@1 89.111 (88.047)	Acc@5 99.805 (99.585)
Epoch: [43][20/25]	Time 0.634 (0.734)	Data 0.005 (0.035)	Loss 0.5954 (0.6056)	Acc@1 88.037 (88.046)	Acc@5 99.414 (99.577)
Epoch: [43][21/25]	Time 0.667 (0.731)	Data 0.005 (0.034)	Loss 0.5793 (0.6045)	Acc@1 89.307 (88.104)	Acc@5 99.561 (99.576)
Epoch: [43][22/25]	Time 0.622 (0.726)	Data 0.004 (0.033)	Loss 0.6091 (0.6047)	Acc@1 87.891 (88.094)	Acc@5 99.609 (99.578)
Epoch: [43][23/25]	Time 0.650 (0.723)	Data 0.006 (0.032)	Loss 0.6203 (0.6053)	Acc@1 87.598 (88.074)	Acc@5 99.512 (99.575)
Epoch: [43][24/25]	Time 0.343 (0.708)	Data 0.008 (0.031)	Loss 0.5950 (0.6051)	Acc@1 88.797 (88.086)	Acc@5 99.764 (99.578)

Epoch: [44 | 180] LR: 0.100000
Epoch: [44][0/25]	Time 0.697 (0.697)	Data 0.727 (0.727)	Loss 0.5973 (0.5973)	Acc@1 88.232 (88.232)	Acc@5 99.707 (99.707)
Epoch: [44][1/25]	Time 0.745 (0.721)	Data 0.005 (0.366)	Loss 0.5905 (0.5939)	Acc@1 88.867 (88.550)	Acc@5 99.805 (99.756)
Epoch: [44][2/25]	Time 0.732 (0.725)	Data 0.006 (0.246)	Loss 0.5936 (0.5938)	Acc@1 89.014 (88.704)	Acc@5 99.512 (99.674)
Epoch: [44][3/25]	Time 0.694 (0.717)	Data 0.006 (0.186)	Loss 0.5899 (0.5928)	Acc@1 88.232 (88.586)	Acc@5 99.707 (99.683)
Epoch: [44][4/25]	Time 0.775 (0.729)	Data 0.008 (0.150)	Loss 0.6043 (0.5951)	Acc@1 88.330 (88.535)	Acc@5 99.707 (99.688)
Epoch: [44][5/25]	Time 0.705 (0.725)	Data 0.005 (0.126)	Loss 0.6068 (0.5971)	Acc@1 87.598 (88.379)	Acc@5 99.609 (99.674)
Epoch: [44][6/25]	Time 0.727 (0.725)	Data 0.005 (0.109)	Loss 0.6042 (0.5981)	Acc@1 87.402 (88.239)	Acc@5 99.414 (99.637)
Epoch: [44][7/25]	Time 0.725 (0.725)	Data 0.006 (0.096)	Loss 0.6220 (0.6011)	Acc@1 87.939 (88.202)	Acc@5 99.268 (99.591)
Epoch: [44][8/25]	Time 0.690 (0.721)	Data 0.005 (0.086)	Loss 0.6153 (0.6027)	Acc@1 87.891 (88.167)	Acc@5 99.170 (99.544)
Epoch: [44][9/25]	Time 0.659 (0.715)	Data 0.007 (0.078)	Loss 0.5948 (0.6019)	Acc@1 89.014 (88.252)	Acc@5 99.512 (99.541)
Epoch: [44][10/25]	Time 0.786 (0.721)	Data 0.004 (0.071)	Loss 0.6091 (0.6025)	Acc@1 87.695 (88.201)	Acc@5 99.707 (99.556)
Epoch: [44][11/25]	Time 0.768 (0.725)	Data 0.005 (0.066)	Loss 0.6270 (0.6046)	Acc@1 87.988 (88.184)	Acc@5 99.365 (99.540)
Epoch: [44][12/25]	Time 0.677 (0.722)	Data 0.006 (0.061)	Loss 0.6119 (0.6051)	Acc@1 87.842 (88.157)	Acc@5 99.561 (99.542)
Epoch: [44][13/25]	Time 0.682 (0.719)	Data 0.004 (0.057)	Loss 0.6142 (0.6058)	Acc@1 87.646 (88.121)	Acc@5 99.707 (99.554)
Epoch: [44][14/25]	Time 0.647 (0.714)	Data 0.007 (0.054)	Loss 0.6006 (0.6054)	Acc@1 88.135 (88.122)	Acc@5 99.561 (99.554)
Epoch: [44][15/25]	Time 0.716 (0.714)	Data 0.007 (0.051)	Loss 0.5788 (0.6038)	Acc@1 88.721 (88.159)	Acc@5 99.707 (99.564)
Epoch: [44][16/25]	Time 0.763 (0.717)	Data 0.008 (0.048)	Loss 0.6126 (0.6043)	Acc@1 87.305 (88.109)	Acc@5 99.658 (99.569)
Epoch: [44][17/25]	Time 0.723 (0.717)	Data 0.005 (0.046)	Loss 0.6060 (0.6044)	Acc@1 88.232 (88.116)	Acc@5 99.561 (99.569)
Epoch: [44][18/25]	Time 0.649 (0.714)	Data 0.007 (0.044)	Loss 0.6159 (0.6050)	Acc@1 88.135 (88.117)	Acc@5 99.561 (99.568)
Epoch: [44][19/25]	Time 0.713 (0.714)	Data 0.007 (0.042)	Loss 0.6141 (0.6055)	Acc@1 87.646 (88.093)	Acc@5 99.609 (99.570)
Epoch: [44][20/25]	Time 0.750 (0.715)	Data 0.005 (0.040)	Loss 0.6218 (0.6062)	Acc@1 87.549 (88.067)	Acc@5 99.414 (99.563)
Epoch: [44][21/25]	Time 0.727 (0.716)	Data 0.005 (0.039)	Loss 0.5920 (0.6056)	Acc@1 88.184 (88.073)	Acc@5 99.609 (99.565)
Epoch: [44][22/25]	Time 0.768 (0.718)	Data 0.006 (0.037)	Loss 0.6270 (0.6065)	Acc@1 87.061 (88.029)	Acc@5 99.414 (99.558)
Epoch: [44][23/25]	Time 0.707 (0.718)	Data 0.007 (0.036)	Loss 0.6432 (0.6080)	Acc@1 86.719 (87.974)	Acc@5 99.414 (99.552)
Epoch: [44][24/25]	Time 0.425 (0.706)	Data 0.005 (0.035)	Loss 0.6300 (0.6084)	Acc@1 87.500 (87.966)	Acc@5 99.175 (99.546)

Epoch: [45 | 180] LR: 0.100000
Epoch: [45][0/25]	Time 0.721 (0.721)	Data 0.635 (0.635)	Loss 0.5984 (0.5984)	Acc@1 88.135 (88.135)	Acc@5 99.463 (99.463)
Epoch: [45][1/25]	Time 0.687 (0.704)	Data 0.007 (0.321)	Loss 0.6079 (0.6032)	Acc@1 88.184 (88.159)	Acc@5 99.463 (99.463)
Epoch: [45][2/25]	Time 0.682 (0.697)	Data 0.007 (0.216)	Loss 0.6096 (0.6053)	Acc@1 88.135 (88.151)	Acc@5 99.512 (99.479)
Epoch: [45][3/25]	Time 0.734 (0.706)	Data 0.005 (0.164)	Loss 0.6162 (0.6081)	Acc@1 87.598 (88.013)	Acc@5 99.463 (99.475)
Epoch: [45][4/25]	Time 0.780 (0.721)	Data 0.006 (0.132)	Loss 0.5982 (0.6061)	Acc@1 88.721 (88.154)	Acc@5 99.512 (99.482)
Epoch: [45][5/25]	Time 0.700 (0.717)	Data 0.004 (0.111)	Loss 0.5885 (0.6032)	Acc@1 88.965 (88.289)	Acc@5 99.316 (99.455)
Epoch: [45][6/25]	Time 0.653 (0.708)	Data 0.005 (0.096)	Loss 0.6422 (0.6087)	Acc@1 86.230 (87.995)	Acc@5 99.463 (99.456)
Epoch: [45][7/25]	Time 0.637 (0.699)	Data 0.005 (0.084)	Loss 0.6103 (0.6089)	Acc@1 87.793 (87.970)	Acc@5 99.756 (99.493)
Epoch: [45][8/25]	Time 0.660 (0.695)	Data 0.005 (0.076)	Loss 0.6092 (0.6090)	Acc@1 87.549 (87.923)	Acc@5 99.414 (99.485)
Epoch: [45][9/25]	Time 0.713 (0.697)	Data 0.006 (0.069)	Loss 0.6120 (0.6093)	Acc@1 88.135 (87.944)	Acc@5 99.316 (99.468)
Epoch: [45][10/25]	Time 0.745 (0.701)	Data 0.006 (0.063)	Loss 0.6035 (0.6087)	Acc@1 87.891 (87.939)	Acc@5 99.609 (99.481)
Epoch: [45][11/25]	Time 0.752 (0.705)	Data 0.004 (0.058)	Loss 0.5593 (0.6046)	Acc@1 89.795 (88.094)	Acc@5 99.561 (99.487)
Epoch: [45][12/25]	Time 0.735 (0.708)	Data 0.004 (0.054)	Loss 0.6190 (0.6057)	Acc@1 87.305 (88.033)	Acc@5 99.561 (99.493)
Epoch: [45][13/25]	Time 0.696 (0.707)	Data 0.007 (0.050)	Loss 0.6095 (0.6060)	Acc@1 87.402 (87.988)	Acc@5 99.805 (99.515)
Epoch: [45][14/25]	Time 0.777 (0.712)	Data 0.006 (0.048)	Loss 0.6040 (0.6059)	Acc@1 87.939 (87.985)	Acc@5 99.756 (99.531)
Epoch: [45][15/25]	Time 0.712 (0.712)	Data 0.004 (0.045)	Loss 0.5987 (0.6054)	Acc@1 88.867 (88.040)	Acc@5 99.512 (99.530)
Epoch: [45][16/25]	Time 0.682 (0.710)	Data 0.005 (0.042)	Loss 0.5828 (0.6041)	Acc@1 88.916 (88.092)	Acc@5 99.707 (99.540)
Epoch: [45][17/25]	Time 0.749 (0.712)	Data 0.005 (0.040)	Loss 0.6168 (0.6048)	Acc@1 88.184 (88.097)	Acc@5 99.365 (99.531)
Epoch: [45][18/25]	Time 0.688 (0.711)	Data 0.006 (0.039)	Loss 0.6271 (0.6060)	Acc@1 87.109 (88.045)	Acc@5 99.756 (99.543)
Epoch: [45][19/25]	Time 0.657 (0.708)	Data 0.004 (0.037)	Loss 0.5810 (0.6047)	Acc@1 88.184 (88.052)	Acc@5 99.854 (99.558)
Epoch: [45][20/25]	Time 0.689 (0.707)	Data 0.005 (0.035)	Loss 0.6384 (0.6063)	Acc@1 86.816 (87.993)	Acc@5 99.512 (99.556)
Epoch: [45][21/25]	Time 0.751 (0.709)	Data 0.005 (0.034)	Loss 0.6135 (0.6066)	Acc@1 87.598 (87.975)	Acc@5 99.707 (99.563)
Epoch: [45][22/25]	Time 0.756 (0.711)	Data 0.006 (0.033)	Loss 0.5924 (0.6060)	Acc@1 88.867 (88.014)	Acc@5 99.463 (99.558)
Epoch: [45][23/25]	Time 0.683 (0.710)	Data 0.004 (0.031)	Loss 0.6271 (0.6069)	Acc@1 87.451 (87.990)	Acc@5 99.512 (99.556)
Epoch: [45][24/25]	Time 0.403 (0.698)	Data 0.005 (0.030)	Loss 0.6572 (0.6078)	Acc@1 86.557 (87.966)	Acc@5 99.292 (99.552)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 484208 ; 487386 ; 0.9934795008473777

Epoch: [46 | 180] LR: 0.100000
Epoch: [46][0/25]	Time 0.753 (0.753)	Data 0.639 (0.639)	Loss 0.5885 (0.5885)	Acc@1 89.160 (89.160)	Acc@5 99.756 (99.756)
Epoch: [46][1/25]	Time 0.712 (0.732)	Data 0.008 (0.323)	Loss 0.5453 (0.5669)	Acc@1 90.527 (89.844)	Acc@5 99.756 (99.756)
Epoch: [46][2/25]	Time 0.725 (0.730)	Data 0.006 (0.217)	Loss 0.5373 (0.5570)	Acc@1 90.674 (90.120)	Acc@5 99.805 (99.772)
Epoch: [46][3/25]	Time 0.717 (0.727)	Data 0.003 (0.164)	Loss 0.5504 (0.5554)	Acc@1 89.893 (90.063)	Acc@5 99.756 (99.768)
Epoch: [46][4/25]	Time 0.718 (0.725)	Data 0.008 (0.133)	Loss 0.5560 (0.5555)	Acc@1 90.430 (90.137)	Acc@5 99.805 (99.775)
Epoch: [46][5/25]	Time 0.716 (0.723)	Data 0.003 (0.111)	Loss 0.5546 (0.5553)	Acc@1 89.404 (90.015)	Acc@5 99.902 (99.797)
Epoch: [46][6/25]	Time 0.706 (0.721)	Data 0.004 (0.096)	Loss 0.5325 (0.5521)	Acc@1 90.430 (90.074)	Acc@5 99.463 (99.749)
Epoch: [46][7/25]	Time 0.747 (0.724)	Data 0.005 (0.084)	Loss 0.5557 (0.5525)	Acc@1 90.088 (90.076)	Acc@5 99.609 (99.731)
Epoch: [46][8/25]	Time 0.771 (0.730)	Data 0.005 (0.076)	Loss 0.5430 (0.5515)	Acc@1 89.551 (90.017)	Acc@5 99.756 (99.734)
Epoch: [46][9/25]	Time 0.732 (0.730)	Data 0.007 (0.069)	Loss 0.5657 (0.5529)	Acc@1 89.941 (90.010)	Acc@5 99.805 (99.741)
Epoch: [46][10/25]	Time 0.692 (0.726)	Data 0.005 (0.063)	Loss 0.5506 (0.5527)	Acc@1 89.160 (89.933)	Acc@5 99.756 (99.743)
Epoch: [46][11/25]	Time 0.727 (0.726)	Data 0.004 (0.058)	Loss 0.5650 (0.5537)	Acc@1 89.453 (89.893)	Acc@5 99.707 (99.740)
Epoch: [46][12/25]	Time 0.746 (0.728)	Data 0.004 (0.054)	Loss 0.5615 (0.5543)	Acc@1 90.186 (89.915)	Acc@5 99.658 (99.733)
Epoch: [46][13/25]	Time 0.764 (0.731)	Data 0.008 (0.051)	Loss 0.5685 (0.5553)	Acc@1 89.404 (89.879)	Acc@5 99.658 (99.728)
Epoch: [46][14/25]	Time 0.682 (0.727)	Data 0.006 (0.048)	Loss 0.5692 (0.5562)	Acc@1 88.721 (89.801)	Acc@5 99.658 (99.723)
Epoch: [46][15/25]	Time 0.714 (0.726)	Data 0.006 (0.045)	Loss 0.5595 (0.5564)	Acc@1 89.014 (89.752)	Acc@5 99.658 (99.719)
Epoch: [46][16/25]	Time 0.773 (0.729)	Data 0.008 (0.043)	Loss 0.5708 (0.5573)	Acc@1 88.965 (89.706)	Acc@5 99.512 (99.707)
Epoch: [46][17/25]	Time 0.688 (0.727)	Data 0.005 (0.041)	Loss 0.5615 (0.5575)	Acc@1 89.551 (89.697)	Acc@5 99.707 (99.707)
Epoch: [46][18/25]	Time 0.702 (0.726)	Data 0.006 (0.039)	Loss 0.5914 (0.5593)	Acc@1 88.525 (89.636)	Acc@5 99.609 (99.702)
Epoch: [46][19/25]	Time 0.753 (0.727)	Data 0.006 (0.037)	Loss 0.5665 (0.5597)	Acc@1 89.209 (89.614)	Acc@5 99.707 (99.702)
Epoch: [46][20/25]	Time 0.683 (0.725)	Data 0.004 (0.036)	Loss 0.5603 (0.5597)	Acc@1 89.258 (89.597)	Acc@5 99.609 (99.698)
Epoch: [46][21/25]	Time 0.721 (0.725)	Data 0.007 (0.034)	Loss 0.5739 (0.5603)	Acc@1 88.428 (89.544)	Acc@5 99.707 (99.698)
Epoch: [46][22/25]	Time 0.731 (0.725)	Data 0.005 (0.033)	Loss 0.5573 (0.5602)	Acc@1 89.307 (89.534)	Acc@5 99.512 (99.690)
Epoch: [46][23/25]	Time 0.763 (0.727)	Data 0.006 (0.032)	Loss 0.5560 (0.5600)	Acc@1 89.502 (89.532)	Acc@5 99.512 (99.683)
Epoch: [46][24/25]	Time 0.447 (0.715)	Data 0.004 (0.031)	Loss 0.6323 (0.5613)	Acc@1 87.028 (89.490)	Acc@5 98.939 (99.670)

Epoch: [47 | 180] LR: 0.100000
Epoch: [47][0/25]	Time 0.743 (0.743)	Data 0.610 (0.610)	Loss 0.5646 (0.5646)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [47][1/25]	Time 0.763 (0.753)	Data 0.005 (0.307)	Loss 0.5779 (0.5713)	Acc@1 88.428 (89.136)	Acc@5 99.756 (99.683)
Epoch: [47][2/25]	Time 0.726 (0.744)	Data 0.007 (0.207)	Loss 0.5885 (0.5770)	Acc@1 88.916 (89.062)	Acc@5 99.805 (99.723)
Epoch: [47][3/25]	Time 0.766 (0.750)	Data 0.008 (0.157)	Loss 0.6134 (0.5861)	Acc@1 88.086 (88.818)	Acc@5 99.609 (99.695)
Epoch: [47][4/25]	Time 0.690 (0.738)	Data 0.007 (0.127)	Loss 0.6030 (0.5895)	Acc@1 88.574 (88.770)	Acc@5 99.561 (99.668)
Epoch: [47][5/25]	Time 0.626 (0.719)	Data 0.009 (0.108)	Loss 0.5780 (0.5876)	Acc@1 88.379 (88.704)	Acc@5 99.805 (99.691)
Epoch: [47][6/25]	Time 0.650 (0.709)	Data 0.005 (0.093)	Loss 0.5887 (0.5877)	Acc@1 87.939 (88.595)	Acc@5 99.854 (99.714)
Epoch: [47][7/25]	Time 0.606 (0.696)	Data 0.005 (0.082)	Loss 0.5916 (0.5882)	Acc@1 87.988 (88.519)	Acc@5 99.658 (99.707)
Epoch: [47][8/25]	Time 0.649 (0.691)	Data 0.007 (0.074)	Loss 0.5965 (0.5891)	Acc@1 87.842 (88.444)	Acc@5 99.414 (99.674)
Epoch: [47][9/25]	Time 0.647 (0.687)	Data 0.005 (0.067)	Loss 0.5996 (0.5902)	Acc@1 87.549 (88.354)	Acc@5 99.609 (99.668)
Epoch: [47][10/25]	Time 0.669 (0.685)	Data 0.007 (0.061)	Loss 0.6226 (0.5931)	Acc@1 88.428 (88.361)	Acc@5 99.756 (99.676)
Epoch: [47][11/25]	Time 0.627 (0.680)	Data 0.004 (0.056)	Loss 0.6397 (0.5970)	Acc@1 86.865 (88.236)	Acc@5 99.121 (99.630)
Epoch: [47][12/25]	Time 0.653 (0.678)	Data 0.006 (0.053)	Loss 0.6070 (0.5978)	Acc@1 88.086 (88.225)	Acc@5 99.512 (99.621)
Epoch: [47][13/25]	Time 0.718 (0.681)	Data 0.008 (0.049)	Loss 0.6053 (0.5983)	Acc@1 88.770 (88.264)	Acc@5 99.561 (99.616)
Epoch: [47][14/25]	Time 0.738 (0.685)	Data 0.006 (0.046)	Loss 0.5806 (0.5971)	Acc@1 89.307 (88.333)	Acc@5 99.609 (99.616)
Epoch: [47][15/25]	Time 0.717 (0.687)	Data 0.006 (0.044)	Loss 0.5916 (0.5968)	Acc@1 87.744 (88.297)	Acc@5 99.365 (99.600)
Epoch: [47][16/25]	Time 0.684 (0.687)	Data 0.008 (0.042)	Loss 0.6197 (0.5981)	Acc@1 87.158 (88.230)	Acc@5 99.561 (99.598)
Epoch: [47][17/25]	Time 0.727 (0.689)	Data 0.004 (0.040)	Loss 0.5955 (0.5980)	Acc@1 88.184 (88.227)	Acc@5 99.658 (99.601)
Epoch: [47][18/25]	Time 0.786 (0.694)	Data 0.005 (0.038)	Loss 0.5748 (0.5968)	Acc@1 88.721 (88.253)	Acc@5 99.561 (99.599)
Epoch: [47][19/25]	Time 0.691 (0.694)	Data 0.006 (0.036)	Loss 0.5845 (0.5962)	Acc@1 88.281 (88.254)	Acc@5 99.609 (99.600)
Epoch: [47][20/25]	Time 0.654 (0.692)	Data 0.008 (0.035)	Loss 0.6166 (0.5971)	Acc@1 87.793 (88.232)	Acc@5 99.463 (99.593)
Epoch: [47][21/25]	Time 0.664 (0.691)	Data 0.005 (0.034)	Loss 0.5711 (0.5960)	Acc@1 88.574 (88.248)	Acc@5 99.561 (99.592)
Epoch: [47][22/25]	Time 0.673 (0.690)	Data 0.006 (0.032)	Loss 0.6179 (0.5969)	Acc@1 87.402 (88.211)	Acc@5 99.561 (99.590)
Epoch: [47][23/25]	Time 0.757 (0.693)	Data 0.004 (0.031)	Loss 0.6375 (0.5986)	Acc@1 87.012 (88.161)	Acc@5 99.609 (99.591)
Epoch: [47][24/25]	Time 0.460 (0.683)	Data 0.004 (0.030)	Loss 0.5897 (0.5984)	Acc@1 88.679 (88.170)	Acc@5 99.528 (99.590)

Epoch: [48 | 180] LR: 0.100000
Epoch: [48][0/25]	Time 0.740 (0.740)	Data 0.639 (0.639)	Loss 0.5819 (0.5819)	Acc@1 88.770 (88.770)	Acc@5 99.756 (99.756)
Epoch: [48][1/25]	Time 0.743 (0.741)	Data 0.004 (0.321)	Loss 0.5850 (0.5835)	Acc@1 88.086 (88.428)	Acc@5 99.805 (99.780)
Epoch: [48][2/25]	Time 0.761 (0.748)	Data 0.004 (0.216)	Loss 0.5928 (0.5866)	Acc@1 88.037 (88.298)	Acc@5 99.512 (99.691)
Epoch: [48][3/25]	Time 0.687 (0.733)	Data 0.005 (0.163)	Loss 0.5894 (0.5873)	Acc@1 88.867 (88.440)	Acc@5 99.365 (99.609)
Epoch: [48][4/25]	Time 0.759 (0.738)	Data 0.007 (0.132)	Loss 0.6247 (0.5948)	Acc@1 86.963 (88.145)	Acc@5 99.414 (99.570)
Epoch: [48][5/25]	Time 0.758 (0.741)	Data 0.004 (0.111)	Loss 0.5968 (0.5951)	Acc@1 88.770 (88.249)	Acc@5 99.414 (99.544)
Epoch: [48][6/25]	Time 0.715 (0.738)	Data 0.006 (0.096)	Loss 0.5923 (0.5947)	Acc@1 88.623 (88.302)	Acc@5 99.561 (99.547)
Epoch: [48][7/25]	Time 0.694 (0.732)	Data 0.007 (0.085)	Loss 0.6017 (0.5956)	Acc@1 88.184 (88.287)	Acc@5 99.805 (99.579)
Epoch: [48][8/25]	Time 0.723 (0.731)	Data 0.005 (0.076)	Loss 0.5727 (0.5930)	Acc@1 89.551 (88.428)	Acc@5 99.707 (99.593)
Epoch: [48][9/25]	Time 0.740 (0.732)	Data 0.004 (0.069)	Loss 0.5961 (0.5933)	Acc@1 88.525 (88.438)	Acc@5 99.609 (99.595)
Epoch: [48][10/25]	Time 0.722 (0.731)	Data 0.004 (0.063)	Loss 0.6366 (0.5973)	Acc@1 87.061 (88.312)	Acc@5 99.609 (99.596)
Epoch: [48][11/25]	Time 0.744 (0.732)	Data 0.006 (0.058)	Loss 0.6035 (0.5978)	Acc@1 88.232 (88.306)	Acc@5 99.463 (99.585)
Epoch: [48][12/25]	Time 0.766 (0.735)	Data 0.005 (0.054)	Loss 0.6268 (0.6000)	Acc@1 86.963 (88.202)	Acc@5 99.414 (99.572)
Epoch: [48][13/25]	Time 0.684 (0.731)	Data 0.008 (0.051)	Loss 0.5888 (0.5992)	Acc@1 89.062 (88.264)	Acc@5 99.609 (99.574)
Epoch: [48][14/25]	Time 0.659 (0.726)	Data 0.008 (0.048)	Loss 0.5907 (0.5987)	Acc@1 88.428 (88.275)	Acc@5 99.463 (99.567)
Epoch: [48][15/25]	Time 0.623 (0.720)	Data 0.006 (0.045)	Loss 0.6132 (0.5996)	Acc@1 87.695 (88.239)	Acc@5 99.463 (99.561)
Epoch: [48][16/25]	Time 0.623 (0.714)	Data 0.005 (0.043)	Loss 0.6069 (0.6000)	Acc@1 88.330 (88.244)	Acc@5 99.512 (99.558)
Epoch: [48][17/25]	Time 0.659 (0.711)	Data 0.005 (0.041)	Loss 0.6170 (0.6009)	Acc@1 87.793 (88.219)	Acc@5 99.414 (99.550)
Epoch: [48][18/25]	Time 0.671 (0.709)	Data 0.004 (0.039)	Loss 0.6089 (0.6014)	Acc@1 87.549 (88.184)	Acc@5 99.707 (99.558)
Epoch: [48][19/25]	Time 0.642 (0.706)	Data 0.004 (0.037)	Loss 0.5580 (0.5992)	Acc@1 89.404 (88.245)	Acc@5 99.561 (99.558)
Epoch: [48][20/25]	Time 0.653 (0.703)	Data 0.008 (0.036)	Loss 0.5942 (0.5990)	Acc@1 88.672 (88.265)	Acc@5 99.561 (99.558)
Epoch: [48][21/25]	Time 0.627 (0.700)	Data 0.006 (0.034)	Loss 0.5957 (0.5988)	Acc@1 88.379 (88.270)	Acc@5 99.609 (99.561)
Epoch: [48][22/25]	Time 0.626 (0.697)	Data 0.008 (0.033)	Loss 0.6259 (0.6000)	Acc@1 88.037 (88.260)	Acc@5 99.463 (99.556)
Epoch: [48][23/25]	Time 0.663 (0.695)	Data 0.006 (0.032)	Loss 0.5900 (0.5996)	Acc@1 87.939 (88.247)	Acc@5 99.512 (99.554)
Epoch: [48][24/25]	Time 0.357 (0.682)	Data 0.008 (0.031)	Loss 0.5934 (0.5995)	Acc@1 87.382 (88.232)	Acc@5 99.646 (99.556)

Epoch: [49 | 180] LR: 0.100000
Epoch: [49][0/25]	Time 0.779 (0.779)	Data 0.668 (0.668)	Loss 0.6159 (0.6159)	Acc@1 87.549 (87.549)	Acc@5 99.561 (99.561)
Epoch: [49][1/25]	Time 0.698 (0.738)	Data 0.006 (0.337)	Loss 0.5934 (0.6046)	Acc@1 87.695 (87.622)	Acc@5 99.805 (99.683)
Epoch: [49][2/25]	Time 0.644 (0.707)	Data 0.005 (0.226)	Loss 0.6010 (0.6034)	Acc@1 88.965 (88.070)	Acc@5 99.561 (99.642)
Epoch: [49][3/25]	Time 0.649 (0.693)	Data 0.007 (0.171)	Loss 0.5785 (0.5972)	Acc@1 88.818 (88.257)	Acc@5 99.561 (99.622)
Epoch: [49][4/25]	Time 0.657 (0.685)	Data 0.008 (0.139)	Loss 0.6033 (0.5984)	Acc@1 88.477 (88.301)	Acc@5 99.756 (99.648)
Epoch: [49][5/25]	Time 0.621 (0.675)	Data 0.008 (0.117)	Loss 0.5897 (0.5970)	Acc@1 88.672 (88.363)	Acc@5 99.805 (99.674)
Epoch: [49][6/25]	Time 0.653 (0.672)	Data 0.006 (0.101)	Loss 0.5977 (0.5971)	Acc@1 88.330 (88.358)	Acc@5 99.805 (99.693)
Epoch: [49][7/25]	Time 0.627 (0.666)	Data 0.006 (0.089)	Loss 0.5961 (0.5969)	Acc@1 88.721 (88.403)	Acc@5 99.609 (99.683)
Epoch: [49][8/25]	Time 0.656 (0.665)	Data 0.004 (0.080)	Loss 0.5952 (0.5968)	Acc@1 88.672 (88.433)	Acc@5 99.658 (99.680)
Epoch: [49][9/25]	Time 0.626 (0.661)	Data 0.007 (0.072)	Loss 0.5722 (0.5943)	Acc@1 89.453 (88.535)	Acc@5 99.756 (99.688)
Epoch: [49][10/25]	Time 0.650 (0.660)	Data 0.005 (0.066)	Loss 0.6095 (0.5957)	Acc@1 88.037 (88.490)	Acc@5 99.463 (99.667)
Epoch: [49][11/25]	Time 0.655 (0.660)	Data 0.009 (0.061)	Loss 0.6338 (0.5989)	Acc@1 88.281 (88.472)	Acc@5 99.365 (99.642)
Epoch: [49][12/25]	Time 0.645 (0.658)	Data 0.004 (0.057)	Loss 0.5890 (0.5981)	Acc@1 87.988 (88.435)	Acc@5 99.805 (99.654)
Epoch: [49][13/25]	Time 0.689 (0.661)	Data 0.007 (0.053)	Loss 0.5764 (0.5966)	Acc@1 89.502 (88.511)	Acc@5 99.365 (99.634)
Epoch: [49][14/25]	Time 0.765 (0.668)	Data 0.006 (0.050)	Loss 0.5855 (0.5958)	Acc@1 88.965 (88.542)	Acc@5 99.463 (99.622)
Epoch: [49][15/25]	Time 0.678 (0.668)	Data 0.004 (0.047)	Loss 0.6095 (0.5967)	Acc@1 88.623 (88.547)	Acc@5 99.658 (99.625)
Epoch: [49][16/25]	Time 0.650 (0.667)	Data 0.005 (0.045)	Loss 0.6147 (0.5977)	Acc@1 88.135 (88.523)	Acc@5 99.707 (99.629)
Epoch: [49][17/25]	Time 0.710 (0.670)	Data 0.007 (0.043)	Loss 0.6363 (0.5999)	Acc@1 87.793 (88.482)	Acc@5 99.658 (99.631)
Epoch: [49][18/25]	Time 0.712 (0.672)	Data 0.005 (0.041)	Loss 0.6392 (0.6020)	Acc@1 86.914 (88.399)	Acc@5 99.316 (99.615)
Epoch: [49][19/25]	Time 0.744 (0.675)	Data 0.007 (0.039)	Loss 0.5816 (0.6009)	Acc@1 88.818 (88.420)	Acc@5 99.512 (99.609)
Epoch: [49][20/25]	Time 0.710 (0.677)	Data 0.005 (0.037)	Loss 0.5743 (0.5997)	Acc@1 89.014 (88.449)	Acc@5 99.756 (99.616)
Epoch: [49][21/25]	Time 0.718 (0.679)	Data 0.004 (0.036)	Loss 0.6112 (0.6002)	Acc@1 87.695 (88.414)	Acc@5 99.512 (99.612)
Epoch: [49][22/25]	Time 0.738 (0.681)	Data 0.008 (0.035)	Loss 0.5780 (0.5992)	Acc@1 88.477 (88.417)	Acc@5 99.561 (99.609)
Epoch: [49][23/25]	Time 0.701 (0.682)	Data 0.006 (0.034)	Loss 0.5970 (0.5991)	Acc@1 88.965 (88.440)	Acc@5 99.463 (99.603)
Epoch: [49][24/25]	Time 0.378 (0.670)	Data 0.009 (0.033)	Loss 0.6311 (0.5997)	Acc@1 87.972 (88.432)	Acc@5 99.764 (99.606)

Epoch: [50 | 180] LR: 0.100000
Epoch: [50][0/25]	Time 0.727 (0.727)	Data 0.663 (0.663)	Loss 0.5901 (0.5901)	Acc@1 88.086 (88.086)	Acc@5 99.707 (99.707)
Epoch: [50][1/25]	Time 0.737 (0.732)	Data 0.006 (0.335)	Loss 0.6014 (0.5958)	Acc@1 87.939 (88.013)	Acc@5 99.609 (99.658)
Epoch: [50][2/25]	Time 0.775 (0.747)	Data 0.005 (0.225)	Loss 0.6135 (0.6017)	Acc@1 88.477 (88.167)	Acc@5 99.365 (99.561)
Epoch: [50][3/25]	Time 0.772 (0.753)	Data 0.004 (0.170)	Loss 0.6073 (0.6031)	Acc@1 87.207 (87.927)	Acc@5 99.463 (99.536)
Epoch: [50][4/25]	Time 0.703 (0.743)	Data 0.004 (0.137)	Loss 0.5774 (0.5980)	Acc@1 88.818 (88.105)	Acc@5 99.707 (99.570)
Epoch: [50][5/25]	Time 0.650 (0.728)	Data 0.010 (0.115)	Loss 0.6055 (0.5992)	Acc@1 87.842 (88.062)	Acc@5 99.365 (99.536)
Epoch: [50][6/25]	Time 0.630 (0.714)	Data 0.007 (0.100)	Loss 0.6069 (0.6003)	Acc@1 88.135 (88.072)	Acc@5 99.707 (99.561)
Epoch: [50][7/25]	Time 0.660 (0.707)	Data 0.006 (0.088)	Loss 0.5897 (0.5990)	Acc@1 88.135 (88.080)	Acc@5 99.609 (99.567)
Epoch: [50][8/25]	Time 0.626 (0.698)	Data 0.008 (0.079)	Loss 0.6035 (0.5995)	Acc@1 87.793 (88.048)	Acc@5 99.512 (99.561)
Epoch: [50][9/25]	Time 0.633 (0.691)	Data 0.007 (0.072)	Loss 0.5847 (0.5980)	Acc@1 89.258 (88.169)	Acc@5 99.854 (99.590)
Epoch: [50][10/25]	Time 0.632 (0.686)	Data 0.007 (0.066)	Loss 0.6122 (0.5993)	Acc@1 87.451 (88.104)	Acc@5 99.561 (99.587)
Epoch: [50][11/25]	Time 0.656 (0.683)	Data 0.005 (0.061)	Loss 0.6036 (0.5997)	Acc@1 88.623 (88.147)	Acc@5 99.512 (99.581)
Epoch: [50][12/25]	Time 0.656 (0.681)	Data 0.007 (0.057)	Loss 0.6000 (0.5997)	Acc@1 88.330 (88.161)	Acc@5 99.609 (99.583)
Epoch: [50][13/25]	Time 0.693 (0.682)	Data 0.007 (0.053)	Loss 0.6089 (0.6003)	Acc@1 87.793 (88.135)	Acc@5 99.756 (99.595)
Epoch: [50][14/25]	Time 0.772 (0.688)	Data 0.007 (0.050)	Loss 0.5822 (0.5991)	Acc@1 88.623 (88.167)	Acc@5 99.365 (99.580)
Epoch: [50][15/25]	Time 0.710 (0.690)	Data 0.005 (0.047)	Loss 0.6149 (0.6001)	Acc@1 87.598 (88.132)	Acc@5 99.854 (99.597)
Epoch: [50][16/25]	Time 0.680 (0.689)	Data 0.008 (0.045)	Loss 0.6019 (0.6002)	Acc@1 88.965 (88.181)	Acc@5 99.707 (99.604)
Epoch: [50][17/25]	Time 0.661 (0.687)	Data 0.006 (0.043)	Loss 0.6009 (0.6003)	Acc@1 88.428 (88.194)	Acc@5 99.365 (99.590)
Epoch: [50][18/25]	Time 0.754 (0.691)	Data 0.008 (0.041)	Loss 0.6093 (0.6007)	Acc@1 88.086 (88.189)	Acc@5 99.512 (99.586)
Epoch: [50][19/25]	Time 0.757 (0.694)	Data 0.004 (0.039)	Loss 0.6143 (0.6014)	Acc@1 87.695 (88.164)	Acc@5 99.609 (99.587)
Epoch: [50][20/25]	Time 0.724 (0.696)	Data 0.004 (0.037)	Loss 0.5745 (0.6001)	Acc@1 89.404 (88.223)	Acc@5 99.658 (99.591)
Epoch: [50][21/25]	Time 0.739 (0.698)	Data 0.005 (0.036)	Loss 0.6277 (0.6014)	Acc@1 87.451 (88.188)	Acc@5 99.561 (99.589)
Epoch: [50][22/25]	Time 0.744 (0.700)	Data 0.005 (0.035)	Loss 0.5903 (0.6009)	Acc@1 88.770 (88.213)	Acc@5 99.561 (99.588)
Epoch: [50][23/25]	Time 0.745 (0.702)	Data 0.006 (0.033)	Loss 0.6349 (0.6023)	Acc@1 87.158 (88.169)	Acc@5 99.609 (99.589)
Epoch: [50][24/25]	Time 0.415 (0.690)	Data 0.005 (0.032)	Loss 0.6546 (0.6032)	Acc@1 86.675 (88.144)	Acc@5 99.764 (99.592)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 479004 ; 487386 ; 0.9828021321909123

Epoch: [51 | 180] LR: 0.100000
Epoch: [51][0/25]	Time 0.767 (0.767)	Data 0.633 (0.633)	Loss 0.5922 (0.5922)	Acc@1 88.477 (88.477)	Acc@5 99.707 (99.707)
Epoch: [51][1/25]	Time 0.672 (0.719)	Data 0.006 (0.320)	Loss 0.5535 (0.5729)	Acc@1 89.795 (89.136)	Acc@5 99.805 (99.756)
Epoch: [51][2/25]	Time 0.619 (0.686)	Data 0.007 (0.216)	Loss 0.5388 (0.5615)	Acc@1 90.674 (89.648)	Acc@5 99.854 (99.788)
Epoch: [51][3/25]	Time 0.649 (0.677)	Data 0.006 (0.163)	Loss 0.5316 (0.5540)	Acc@1 90.820 (89.941)	Acc@5 99.756 (99.780)
Epoch: [51][4/25]	Time 0.657 (0.673)	Data 0.008 (0.132)	Loss 0.5315 (0.5495)	Acc@1 90.674 (90.088)	Acc@5 99.658 (99.756)
Epoch: [51][5/25]	Time 0.676 (0.673)	Data 0.005 (0.111)	Loss 0.5748 (0.5538)	Acc@1 89.648 (90.015)	Acc@5 99.561 (99.723)
Epoch: [51][6/25]	Time 0.745 (0.684)	Data 0.008 (0.096)	Loss 0.5322 (0.5507)	Acc@1 90.967 (90.151)	Acc@5 99.756 (99.728)
Epoch: [51][7/25]	Time 0.704 (0.686)	Data 0.004 (0.085)	Loss 0.5514 (0.5508)	Acc@1 89.893 (90.118)	Acc@5 99.854 (99.744)
Epoch: [51][8/25]	Time 0.715 (0.689)	Data 0.007 (0.076)	Loss 0.5456 (0.5502)	Acc@1 90.625 (90.175)	Acc@5 99.414 (99.707)
Epoch: [51][9/25]	Time 0.775 (0.698)	Data 0.004 (0.069)	Loss 0.5541 (0.5506)	Acc@1 89.502 (90.107)	Acc@5 99.854 (99.722)
Epoch: [51][10/25]	Time 0.659 (0.694)	Data 0.005 (0.063)	Loss 0.5346 (0.5491)	Acc@1 90.967 (90.186)	Acc@5 99.658 (99.716)
Epoch: [51][11/25]	Time 0.664 (0.692)	Data 0.008 (0.058)	Loss 0.5696 (0.5508)	Acc@1 89.502 (90.129)	Acc@5 99.609 (99.707)
Epoch: [51][12/25]	Time 0.672 (0.690)	Data 0.007 (0.054)	Loss 0.5761 (0.5528)	Acc@1 89.209 (90.058)	Acc@5 99.707 (99.707)
Epoch: [51][13/25]	Time 0.646 (0.687)	Data 0.010 (0.051)	Loss 0.5675 (0.5538)	Acc@1 89.404 (90.011)	Acc@5 99.658 (99.704)
Epoch: [51][14/25]	Time 0.808 (0.695)	Data 0.006 (0.048)	Loss 0.5693 (0.5549)	Acc@1 89.746 (89.993)	Acc@5 99.609 (99.697)
Epoch: [51][15/25]	Time 0.876 (0.707)	Data 0.006 (0.046)	Loss 0.5797 (0.5564)	Acc@1 89.014 (89.932)	Acc@5 99.609 (99.692)
Epoch: [51][16/25]	Time 0.678 (0.705)	Data 0.005 (0.043)	Loss 0.5961 (0.5587)	Acc@1 87.793 (89.806)	Acc@5 99.707 (99.693)
Epoch: [51][17/25]	Time 0.708 (0.705)	Data 0.008 (0.041)	Loss 0.5502 (0.5583)	Acc@1 90.723 (89.857)	Acc@5 99.414 (99.677)
Epoch: [51][18/25]	Time 0.750 (0.707)	Data 0.005 (0.039)	Loss 0.5948 (0.5602)	Acc@1 88.379 (89.780)	Acc@5 99.609 (99.674)
Epoch: [51][19/25]	Time 0.741 (0.709)	Data 0.004 (0.038)	Loss 0.5878 (0.5616)	Acc@1 88.818 (89.731)	Acc@5 99.707 (99.675)
Epoch: [51][20/25]	Time 0.696 (0.708)	Data 0.005 (0.036)	Loss 0.5667 (0.5618)	Acc@1 89.600 (89.725)	Acc@5 99.658 (99.674)
Epoch: [51][21/25]	Time 0.718 (0.709)	Data 0.005 (0.035)	Loss 0.5854 (0.5629)	Acc@1 88.428 (89.666)	Acc@5 99.902 (99.685)
Epoch: [51][22/25]	Time 0.734 (0.710)	Data 0.004 (0.033)	Loss 0.5706 (0.5632)	Acc@1 89.307 (89.651)	Acc@5 99.658 (99.684)
Epoch: [51][23/25]	Time 0.710 (0.710)	Data 0.007 (0.032)	Loss 0.5776 (0.5638)	Acc@1 89.307 (89.636)	Acc@5 99.561 (99.679)
Epoch: [51][24/25]	Time 0.362 (0.696)	Data 0.006 (0.031)	Loss 0.5738 (0.5640)	Acc@1 89.033 (89.626)	Acc@5 99.646 (99.678)

Epoch: [52 | 180] LR: 0.100000
Epoch: [52][0/25]	Time 0.769 (0.769)	Data 0.702 (0.702)	Loss 0.5761 (0.5761)	Acc@1 88.916 (88.916)	Acc@5 99.902 (99.902)
Epoch: [52][1/25]	Time 0.730 (0.749)	Data 0.003 (0.352)	Loss 0.5725 (0.5743)	Acc@1 89.258 (89.087)	Acc@5 99.805 (99.854)
Epoch: [52][2/25]	Time 0.703 (0.734)	Data 0.005 (0.237)	Loss 0.6536 (0.6007)	Acc@1 85.986 (88.053)	Acc@5 99.561 (99.756)
Epoch: [52][3/25]	Time 0.728 (0.732)	Data 0.004 (0.179)	Loss 0.5872 (0.5973)	Acc@1 88.672 (88.208)	Acc@5 99.609 (99.719)
Epoch: [52][4/25]	Time 0.773 (0.740)	Data 0.012 (0.145)	Loss 0.5663 (0.5911)	Acc@1 89.893 (88.545)	Acc@5 99.707 (99.717)
Epoch: [52][5/25]	Time 0.661 (0.727)	Data 0.004 (0.122)	Loss 0.5750 (0.5885)	Acc@1 88.965 (88.615)	Acc@5 99.707 (99.715)
Epoch: [52][6/25]	Time 0.636 (0.714)	Data 0.008 (0.105)	Loss 0.5928 (0.5891)	Acc@1 88.086 (88.539)	Acc@5 99.512 (99.686)
Epoch: [52][7/25]	Time 0.626 (0.703)	Data 0.004 (0.093)	Loss 0.6022 (0.5907)	Acc@1 88.135 (88.489)	Acc@5 99.219 (99.628)
Epoch: [52][8/25]	Time 0.621 (0.694)	Data 0.006 (0.083)	Loss 0.5945 (0.5911)	Acc@1 87.939 (88.428)	Acc@5 99.561 (99.620)
Epoch: [52][9/25]	Time 0.647 (0.689)	Data 0.004 (0.075)	Loss 0.5766 (0.5897)	Acc@1 88.770 (88.462)	Acc@5 99.658 (99.624)
Epoch: [52][10/25]	Time 0.632 (0.684)	Data 0.006 (0.069)	Loss 0.6026 (0.5909)	Acc@1 88.281 (88.445)	Acc@5 99.512 (99.614)
Epoch: [52][11/25]	Time 0.627 (0.679)	Data 0.008 (0.064)	Loss 0.5753 (0.5896)	Acc@1 88.965 (88.489)	Acc@5 99.561 (99.609)
Epoch: [52][12/25]	Time 0.652 (0.677)	Data 0.006 (0.059)	Loss 0.5966 (0.5901)	Acc@1 88.281 (88.473)	Acc@5 99.707 (99.617)
Epoch: [52][13/25]	Time 0.684 (0.678)	Data 0.005 (0.056)	Loss 0.5703 (0.5887)	Acc@1 89.307 (88.532)	Acc@5 99.951 (99.641)
Epoch: [52][14/25]	Time 0.657 (0.676)	Data 0.007 (0.052)	Loss 0.6250 (0.5911)	Acc@1 87.500 (88.464)	Acc@5 99.658 (99.642)
Epoch: [52][15/25]	Time 0.683 (0.677)	Data 0.006 (0.049)	Loss 0.5718 (0.5899)	Acc@1 89.258 (88.513)	Acc@5 99.658 (99.643)
Epoch: [52][16/25]	Time 0.773 (0.682)	Data 0.007 (0.047)	Loss 0.5947 (0.5902)	Acc@1 88.184 (88.494)	Acc@5 99.463 (99.632)
Epoch: [52][17/25]	Time 0.719 (0.684)	Data 0.005 (0.045)	Loss 0.6061 (0.5911)	Acc@1 87.744 (88.452)	Acc@5 99.365 (99.618)
Epoch: [52][18/25]	Time 0.706 (0.686)	Data 0.007 (0.043)	Loss 0.6178 (0.5925)	Acc@1 87.793 (88.417)	Acc@5 99.707 (99.622)
Epoch: [52][19/25]	Time 0.739 (0.688)	Data 0.005 (0.041)	Loss 0.5970 (0.5927)	Acc@1 87.402 (88.367)	Acc@5 99.707 (99.626)
Epoch: [52][20/25]	Time 0.751 (0.691)	Data 0.004 (0.039)	Loss 0.6014 (0.5931)	Acc@1 89.258 (88.409)	Acc@5 99.561 (99.623)
Epoch: [52][21/25]	Time 0.681 (0.691)	Data 0.006 (0.037)	Loss 0.5636 (0.5918)	Acc@1 89.844 (88.474)	Acc@5 99.512 (99.618)
Epoch: [52][22/25]	Time 0.657 (0.689)	Data 0.007 (0.036)	Loss 0.5838 (0.5914)	Acc@1 88.232 (88.464)	Acc@5 99.658 (99.620)
Epoch: [52][23/25]	Time 0.702 (0.690)	Data 0.004 (0.035)	Loss 0.5780 (0.5909)	Acc@1 88.281 (88.456)	Acc@5 99.707 (99.624)
Epoch: [52][24/25]	Time 0.368 (0.677)	Data 0.007 (0.034)	Loss 0.5324 (0.5899)	Acc@1 90.330 (88.488)	Acc@5 99.646 (99.624)

Epoch: [53 | 180] LR: 0.100000
Epoch: [53][0/25]	Time 0.907 (0.907)	Data 0.715 (0.715)	Loss 0.5883 (0.5883)	Acc@1 88.965 (88.965)	Acc@5 99.512 (99.512)
Epoch: [53][1/25]	Time 0.860 (0.884)	Data 0.005 (0.360)	Loss 0.5735 (0.5809)	Acc@1 89.209 (89.087)	Acc@5 99.756 (99.634)
Epoch: [53][2/25]	Time 0.701 (0.823)	Data 0.008 (0.243)	Loss 0.5750 (0.5789)	Acc@1 89.307 (89.160)	Acc@5 99.805 (99.691)
Epoch: [53][3/25]	Time 0.688 (0.789)	Data 0.007 (0.184)	Loss 0.5829 (0.5799)	Acc@1 88.672 (89.038)	Acc@5 99.463 (99.634)
Epoch: [53][4/25]	Time 0.704 (0.772)	Data 0.007 (0.148)	Loss 0.6045 (0.5848)	Acc@1 87.939 (88.818)	Acc@5 99.316 (99.570)
Epoch: [53][5/25]	Time 0.772 (0.772)	Data 0.004 (0.124)	Loss 0.5929 (0.5862)	Acc@1 88.428 (88.753)	Acc@5 99.609 (99.577)
Epoch: [53][6/25]	Time 0.695 (0.761)	Data 0.005 (0.107)	Loss 0.5765 (0.5848)	Acc@1 88.818 (88.763)	Acc@5 99.609 (99.581)
Epoch: [53][7/25]	Time 0.707 (0.754)	Data 0.004 (0.094)	Loss 0.5928 (0.5858)	Acc@1 88.330 (88.708)	Acc@5 99.658 (99.591)
Epoch: [53][8/25]	Time 0.737 (0.752)	Data 0.005 (0.084)	Loss 0.6036 (0.5878)	Acc@1 88.281 (88.661)	Acc@5 99.805 (99.615)
Epoch: [53][9/25]	Time 0.749 (0.752)	Data 0.008 (0.077)	Loss 0.6146 (0.5905)	Acc@1 88.135 (88.608)	Acc@5 99.707 (99.624)
Epoch: [53][10/25]	Time 0.753 (0.752)	Data 0.007 (0.070)	Loss 0.5865 (0.5901)	Acc@1 88.867 (88.632)	Acc@5 99.805 (99.640)
Epoch: [53][11/25]	Time 0.734 (0.751)	Data 0.007 (0.065)	Loss 0.6194 (0.5925)	Acc@1 87.500 (88.538)	Acc@5 99.561 (99.634)
Epoch: [53][12/25]	Time 0.679 (0.745)	Data 0.007 (0.061)	Loss 0.5596 (0.5900)	Acc@1 89.307 (88.597)	Acc@5 99.707 (99.639)
Epoch: [53][13/25]	Time 0.718 (0.743)	Data 0.007 (0.057)	Loss 0.6018 (0.5908)	Acc@1 87.939 (88.550)	Acc@5 99.707 (99.644)
Epoch: [53][14/25]	Time 0.723 (0.742)	Data 0.004 (0.053)	Loss 0.5752 (0.5898)	Acc@1 89.502 (88.613)	Acc@5 99.561 (99.639)
Epoch: [53][15/25]	Time 0.700 (0.739)	Data 0.006 (0.050)	Loss 0.5827 (0.5894)	Acc@1 89.697 (88.681)	Acc@5 99.805 (99.649)
Epoch: [53][16/25]	Time 0.760 (0.741)	Data 0.008 (0.048)	Loss 0.5709 (0.5883)	Acc@1 89.160 (88.709)	Acc@5 99.609 (99.647)
Epoch: [53][17/25]	Time 0.703 (0.739)	Data 0.007 (0.046)	Loss 0.5748 (0.5875)	Acc@1 89.404 (88.748)	Acc@5 99.561 (99.642)
Epoch: [53][18/25]	Time 0.749 (0.739)	Data 0.007 (0.044)	Loss 0.6179 (0.5891)	Acc@1 87.891 (88.703)	Acc@5 99.512 (99.635)
Epoch: [53][19/25]	Time 0.733 (0.739)	Data 0.004 (0.042)	Loss 0.5979 (0.5896)	Acc@1 88.135 (88.674)	Acc@5 99.658 (99.636)
Epoch: [53][20/25]	Time 0.689 (0.736)	Data 0.006 (0.040)	Loss 0.5560 (0.5880)	Acc@1 89.941 (88.735)	Acc@5 99.561 (99.633)
Epoch: [53][21/25]	Time 0.732 (0.736)	Data 0.007 (0.038)	Loss 0.5918 (0.5881)	Acc@1 88.672 (88.732)	Acc@5 99.658 (99.634)
Epoch: [53][22/25]	Time 0.740 (0.736)	Data 0.004 (0.037)	Loss 0.6150 (0.5893)	Acc@1 87.598 (88.682)	Acc@5 99.561 (99.631)
Epoch: [53][23/25]	Time 0.691 (0.734)	Data 0.004 (0.035)	Loss 0.5968 (0.5896)	Acc@1 88.672 (88.682)	Acc@5 99.658 (99.632)
Epoch: [53][24/25]	Time 0.544 (0.727)	Data 0.003 (0.034)	Loss 0.5687 (0.5893)	Acc@1 89.505 (88.696)	Acc@5 99.764 (99.634)

Epoch: [54 | 180] LR: 0.100000
Epoch: [54][0/25]	Time 0.749 (0.749)	Data 0.702 (0.702)	Loss 0.5672 (0.5672)	Acc@1 89.600 (89.600)	Acc@5 99.707 (99.707)
Epoch: [54][1/25]	Time 0.755 (0.752)	Data 0.005 (0.353)	Loss 0.5691 (0.5681)	Acc@1 89.404 (89.502)	Acc@5 99.609 (99.658)
Epoch: [54][2/25]	Time 0.680 (0.728)	Data 0.006 (0.237)	Loss 0.6274 (0.5879)	Acc@1 88.037 (89.014)	Acc@5 99.561 (99.626)
Epoch: [54][3/25]	Time 0.763 (0.737)	Data 0.004 (0.179)	Loss 0.5544 (0.5795)	Acc@1 90.186 (89.307)	Acc@5 99.658 (99.634)
Epoch: [54][4/25]	Time 0.690 (0.727)	Data 0.006 (0.144)	Loss 0.6169 (0.5870)	Acc@1 86.816 (88.809)	Acc@5 99.658 (99.639)
Epoch: [54][5/25]	Time 0.700 (0.723)	Data 0.004 (0.121)	Loss 0.5827 (0.5863)	Acc@1 88.330 (88.729)	Acc@5 99.707 (99.650)
Epoch: [54][6/25]	Time 0.744 (0.726)	Data 0.003 (0.104)	Loss 0.5742 (0.5846)	Acc@1 89.209 (88.797)	Acc@5 99.658 (99.651)
Epoch: [54][7/25]	Time 0.689 (0.721)	Data 0.003 (0.092)	Loss 0.6109 (0.5878)	Acc@1 87.793 (88.672)	Acc@5 99.512 (99.634)
Epoch: [54][8/25]	Time 0.737 (0.723)	Data 0.008 (0.082)	Loss 0.5730 (0.5862)	Acc@1 89.453 (88.759)	Acc@5 99.756 (99.647)
Epoch: [54][9/25]	Time 0.693 (0.720)	Data 0.006 (0.075)	Loss 0.5708 (0.5846)	Acc@1 89.453 (88.828)	Acc@5 99.707 (99.653)
Epoch: [54][10/25]	Time 0.710 (0.719)	Data 0.005 (0.068)	Loss 0.5986 (0.5859)	Acc@1 88.721 (88.818)	Acc@5 99.463 (99.636)
Epoch: [54][11/25]	Time 0.737 (0.720)	Data 0.005 (0.063)	Loss 0.6018 (0.5872)	Acc@1 88.525 (88.794)	Acc@5 99.561 (99.630)
Epoch: [54][12/25]	Time 0.747 (0.723)	Data 0.005 (0.059)	Loss 0.5965 (0.5880)	Acc@1 88.379 (88.762)	Acc@5 99.512 (99.621)
Epoch: [54][13/25]	Time 0.780 (0.727)	Data 0.004 (0.055)	Loss 0.5855 (0.5878)	Acc@1 87.598 (88.679)	Acc@5 99.805 (99.634)
Epoch: [54][14/25]	Time 0.719 (0.726)	Data 0.008 (0.052)	Loss 0.5827 (0.5874)	Acc@1 88.037 (88.636)	Acc@5 99.805 (99.645)
Epoch: [54][15/25]	Time 0.761 (0.728)	Data 0.005 (0.049)	Loss 0.5823 (0.5871)	Acc@1 88.721 (88.641)	Acc@5 99.707 (99.649)
Epoch: [54][16/25]	Time 0.689 (0.726)	Data 0.006 (0.046)	Loss 0.5626 (0.5857)	Acc@1 90.137 (88.729)	Acc@5 99.756 (99.655)
Epoch: [54][17/25]	Time 0.738 (0.727)	Data 0.004 (0.044)	Loss 0.5735 (0.5850)	Acc@1 88.965 (88.742)	Acc@5 99.561 (99.650)
Epoch: [54][18/25]	Time 0.752 (0.728)	Data 0.006 (0.042)	Loss 0.6063 (0.5861)	Acc@1 87.891 (88.698)	Acc@5 99.512 (99.643)
Epoch: [54][19/25]	Time 0.673 (0.725)	Data 0.005 (0.040)	Loss 0.5871 (0.5862)	Acc@1 88.916 (88.708)	Acc@5 99.561 (99.639)
Epoch: [54][20/25]	Time 0.651 (0.722)	Data 0.005 (0.038)	Loss 0.6036 (0.5870)	Acc@1 88.672 (88.707)	Acc@5 99.561 (99.635)
Epoch: [54][21/25]	Time 0.698 (0.721)	Data 0.007 (0.037)	Loss 0.5842 (0.5869)	Acc@1 88.281 (88.687)	Acc@5 99.561 (99.632)
Epoch: [54][22/25]	Time 0.748 (0.722)	Data 0.004 (0.035)	Loss 0.6069 (0.5877)	Acc@1 87.500 (88.636)	Acc@5 99.805 (99.639)
Epoch: [54][23/25]	Time 0.801 (0.725)	Data 0.005 (0.034)	Loss 0.6044 (0.5884)	Acc@1 88.184 (88.617)	Acc@5 99.561 (99.636)
Epoch: [54][24/25]	Time 0.397 (0.712)	Data 0.008 (0.033)	Loss 0.6259 (0.5891)	Acc@1 87.618 (88.600)	Acc@5 99.292 (99.630)

Epoch: [55 | 180] LR: 0.100000
Epoch: [55][0/25]	Time 0.707 (0.707)	Data 0.635 (0.635)	Loss 0.5504 (0.5504)	Acc@1 90.186 (90.186)	Acc@5 99.561 (99.561)
Epoch: [55][1/25]	Time 0.665 (0.686)	Data 0.008 (0.321)	Loss 0.5565 (0.5535)	Acc@1 90.186 (90.186)	Acc@5 99.854 (99.707)
Epoch: [55][2/25]	Time 0.642 (0.671)	Data 0.005 (0.216)	Loss 0.6030 (0.5700)	Acc@1 87.891 (89.421)	Acc@5 99.854 (99.756)
Epoch: [55][3/25]	Time 0.657 (0.668)	Data 0.004 (0.163)	Loss 0.5755 (0.5714)	Acc@1 89.062 (89.331)	Acc@5 99.902 (99.792)
Epoch: [55][4/25]	Time 0.743 (0.683)	Data 0.005 (0.131)	Loss 0.5757 (0.5722)	Acc@1 89.014 (89.268)	Acc@5 99.951 (99.824)
Epoch: [55][5/25]	Time 0.709 (0.687)	Data 0.006 (0.110)	Loss 0.5898 (0.5752)	Acc@1 88.867 (89.201)	Acc@5 99.658 (99.797)
Epoch: [55][6/25]	Time 0.719 (0.692)	Data 0.008 (0.096)	Loss 0.5580 (0.5727)	Acc@1 89.404 (89.230)	Acc@5 99.561 (99.763)
Epoch: [55][7/25]	Time 0.702 (0.693)	Data 0.003 (0.084)	Loss 0.5699 (0.5724)	Acc@1 89.844 (89.307)	Acc@5 99.561 (99.738)
Epoch: [55][8/25]	Time 0.742 (0.698)	Data 0.004 (0.075)	Loss 0.5995 (0.5754)	Acc@1 88.428 (89.209)	Acc@5 99.805 (99.745)
Epoch: [55][9/25]	Time 0.740 (0.703)	Data 0.006 (0.068)	Loss 0.5629 (0.5741)	Acc@1 89.600 (89.248)	Acc@5 99.561 (99.727)
Epoch: [55][10/25]	Time 0.756 (0.707)	Data 0.005 (0.063)	Loss 0.5412 (0.5711)	Acc@1 90.088 (89.324)	Acc@5 99.756 (99.729)
Epoch: [55][11/25]	Time 0.688 (0.706)	Data 0.007 (0.058)	Loss 0.5815 (0.5720)	Acc@1 89.600 (89.347)	Acc@5 99.512 (99.711)
Epoch: [55][12/25]	Time 0.706 (0.706)	Data 0.007 (0.054)	Loss 0.5583 (0.5710)	Acc@1 89.551 (89.363)	Acc@5 99.707 (99.711)
Epoch: [55][13/25]	Time 0.748 (0.709)	Data 0.007 (0.051)	Loss 0.6092 (0.5737)	Acc@1 87.256 (89.212)	Acc@5 99.756 (99.714)
Epoch: [55][14/25]	Time 0.730 (0.710)	Data 0.006 (0.048)	Loss 0.5658 (0.5732)	Acc@1 89.502 (89.232)	Acc@5 99.707 (99.714)
Epoch: [55][15/25]	Time 0.749 (0.713)	Data 0.005 (0.045)	Loss 0.5798 (0.5736)	Acc@1 88.965 (89.215)	Acc@5 99.756 (99.716)
Epoch: [55][16/25]	Time 0.737 (0.714)	Data 0.009 (0.043)	Loss 0.5893 (0.5745)	Acc@1 88.916 (89.197)	Acc@5 99.658 (99.713)
Epoch: [55][17/25]	Time 0.791 (0.718)	Data 0.004 (0.041)	Loss 0.5851 (0.5751)	Acc@1 88.379 (89.152)	Acc@5 99.463 (99.699)
Epoch: [55][18/25]	Time 0.687 (0.717)	Data 0.004 (0.039)	Loss 0.6163 (0.5773)	Acc@1 87.744 (89.078)	Acc@5 99.512 (99.689)
Epoch: [55][19/25]	Time 0.644 (0.713)	Data 0.005 (0.037)	Loss 0.5763 (0.5772)	Acc@1 88.916 (89.070)	Acc@5 99.805 (99.695)
Epoch: [55][20/25]	Time 0.632 (0.709)	Data 0.005 (0.036)	Loss 0.6189 (0.5792)	Acc@1 88.379 (89.037)	Acc@5 99.463 (99.684)
Epoch: [55][21/25]	Time 0.630 (0.706)	Data 0.007 (0.034)	Loss 0.5900 (0.5797)	Acc@1 88.525 (89.014)	Acc@5 99.414 (99.672)
Epoch: [55][22/25]	Time 0.639 (0.703)	Data 0.006 (0.033)	Loss 0.5633 (0.5790)	Acc@1 89.355 (89.029)	Acc@5 99.658 (99.671)
Epoch: [55][23/25]	Time 0.624 (0.700)	Data 0.004 (0.032)	Loss 0.6008 (0.5799)	Acc@1 88.477 (89.006)	Acc@5 99.707 (99.672)
Epoch: [55][24/25]	Time 0.341 (0.685)	Data 0.005 (0.031)	Loss 0.5635 (0.5796)	Acc@1 89.387 (89.012)	Acc@5 99.764 (99.674)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 472644 ; 487386 ; 0.9697529268382761

Epoch: [56 | 180] LR: 0.100000
Epoch: [56][0/25]	Time 0.716 (0.716)	Data 0.762 (0.762)	Loss 0.5293 (0.5293)	Acc@1 91.016 (91.016)	Acc@5 99.707 (99.707)
Epoch: [56][1/25]	Time 0.668 (0.692)	Data 0.007 (0.384)	Loss 0.5433 (0.5363)	Acc@1 90.332 (90.674)	Acc@5 99.658 (99.683)
Epoch: [56][2/25]	Time 0.697 (0.694)	Data 0.003 (0.257)	Loss 0.5204 (0.5310)	Acc@1 91.113 (90.820)	Acc@5 99.805 (99.723)
Epoch: [56][3/25]	Time 0.757 (0.710)	Data 0.007 (0.195)	Loss 0.5029 (0.5240)	Acc@1 91.992 (91.113)	Acc@5 99.854 (99.756)
Epoch: [56][4/25]	Time 0.657 (0.699)	Data 0.006 (0.157)	Loss 0.4906 (0.5173)	Acc@1 92.676 (91.426)	Acc@5 99.805 (99.766)
Epoch: [56][5/25]	Time 0.622 (0.686)	Data 0.005 (0.132)	Loss 0.4864 (0.5122)	Acc@1 92.578 (91.618)	Acc@5 99.805 (99.772)
Epoch: [56][6/25]	Time 0.639 (0.679)	Data 0.005 (0.114)	Loss 0.5190 (0.5131)	Acc@1 91.113 (91.546)	Acc@5 99.707 (99.763)
Epoch: [56][7/25]	Time 0.621 (0.672)	Data 0.007 (0.100)	Loss 0.5089 (0.5126)	Acc@1 91.650 (91.559)	Acc@5 99.805 (99.768)
Epoch: [56][8/25]	Time 0.652 (0.670)	Data 0.005 (0.090)	Loss 0.4868 (0.5097)	Acc@1 92.432 (91.656)	Acc@5 99.805 (99.772)
Epoch: [56][9/25]	Time 0.631 (0.666)	Data 0.008 (0.081)	Loss 0.5419 (0.5129)	Acc@1 90.283 (91.519)	Acc@5 99.463 (99.741)
Epoch: [56][10/25]	Time 0.647 (0.664)	Data 0.007 (0.075)	Loss 0.5165 (0.5133)	Acc@1 91.162 (91.486)	Acc@5 99.658 (99.734)
Epoch: [56][11/25]	Time 0.633 (0.662)	Data 0.005 (0.069)	Loss 0.5561 (0.5168)	Acc@1 89.746 (91.341)	Acc@5 99.707 (99.731)
Epoch: [56][12/25]	Time 0.623 (0.659)	Data 0.004 (0.064)	Loss 0.5303 (0.5179)	Acc@1 89.990 (91.237)	Acc@5 99.756 (99.733)
Epoch: [56][13/25]	Time 0.614 (0.656)	Data 0.009 (0.060)	Loss 0.5311 (0.5188)	Acc@1 90.723 (91.200)	Acc@5 99.902 (99.745)
Epoch: [56][14/25]	Time 0.644 (0.655)	Data 0.007 (0.056)	Loss 0.5333 (0.5198)	Acc@1 89.453 (91.084)	Acc@5 99.805 (99.749)
Epoch: [56][15/25]	Time 0.642 (0.654)	Data 0.005 (0.053)	Loss 0.5185 (0.5197)	Acc@1 90.869 (91.071)	Acc@5 99.756 (99.750)
Epoch: [56][16/25]	Time 0.671 (0.655)	Data 0.005 (0.050)	Loss 0.5563 (0.5219)	Acc@1 89.404 (90.973)	Acc@5 99.854 (99.756)
Epoch: [56][17/25]	Time 0.679 (0.656)	Data 0.004 (0.048)	Loss 0.5359 (0.5226)	Acc@1 91.016 (90.975)	Acc@5 99.609 (99.748)
Epoch: [56][18/25]	Time 0.690 (0.658)	Data 0.005 (0.046)	Loss 0.5342 (0.5233)	Acc@1 90.186 (90.933)	Acc@5 99.805 (99.751)
Epoch: [56][19/25]	Time 0.718 (0.661)	Data 0.008 (0.044)	Loss 0.5467 (0.5244)	Acc@1 89.844 (90.879)	Acc@5 99.854 (99.756)
Epoch: [56][20/25]	Time 0.698 (0.663)	Data 0.004 (0.042)	Loss 0.5429 (0.5253)	Acc@1 90.527 (90.862)	Acc@5 99.609 (99.749)
Epoch: [56][21/25]	Time 0.706 (0.665)	Data 0.006 (0.040)	Loss 0.5509 (0.5265)	Acc@1 89.062 (90.780)	Acc@5 99.756 (99.749)
Epoch: [56][22/25]	Time 0.723 (0.667)	Data 0.005 (0.039)	Loss 0.5702 (0.5284)	Acc@1 88.965 (90.701)	Acc@5 99.658 (99.745)
Epoch: [56][23/25]	Time 0.696 (0.669)	Data 0.004 (0.037)	Loss 0.5295 (0.5284)	Acc@1 90.479 (90.692)	Acc@5 99.902 (99.752)
Epoch: [56][24/25]	Time 0.424 (0.659)	Data 0.004 (0.036)	Loss 0.5749 (0.5292)	Acc@1 89.151 (90.666)	Acc@5 99.528 (99.748)

Epoch: [57 | 180] LR: 0.100000
Epoch: [57][0/25]	Time 0.760 (0.760)	Data 0.649 (0.649)	Loss 0.5042 (0.5042)	Acc@1 91.309 (91.309)	Acc@5 99.854 (99.854)
Epoch: [57][1/25]	Time 0.699 (0.729)	Data 0.008 (0.328)	Loss 0.5320 (0.5181)	Acc@1 89.941 (90.625)	Acc@5 99.805 (99.829)
Epoch: [57][2/25]	Time 0.728 (0.729)	Data 0.006 (0.221)	Loss 0.5620 (0.5327)	Acc@1 89.404 (90.218)	Acc@5 99.561 (99.740)
Epoch: [57][3/25]	Time 0.723 (0.727)	Data 0.004 (0.167)	Loss 0.5643 (0.5406)	Acc@1 89.014 (89.917)	Acc@5 99.609 (99.707)
Epoch: [57][4/25]	Time 0.708 (0.723)	Data 0.005 (0.134)	Loss 0.5518 (0.5429)	Acc@1 89.600 (89.854)	Acc@5 99.707 (99.707)
Epoch: [57][5/25]	Time 0.722 (0.723)	Data 0.007 (0.113)	Loss 0.5611 (0.5459)	Acc@1 89.404 (89.779)	Acc@5 99.756 (99.715)
Epoch: [57][6/25]	Time 0.772 (0.730)	Data 0.004 (0.098)	Loss 0.5092 (0.5407)	Acc@1 91.113 (89.969)	Acc@5 99.854 (99.735)
Epoch: [57][7/25]	Time 0.674 (0.723)	Data 0.005 (0.086)	Loss 0.5361 (0.5401)	Acc@1 90.479 (90.033)	Acc@5 99.658 (99.725)
Epoch: [57][8/25]	Time 0.703 (0.721)	Data 0.006 (0.077)	Loss 0.5489 (0.5411)	Acc@1 90.381 (90.072)	Acc@5 99.805 (99.734)
Epoch: [57][9/25]	Time 0.782 (0.727)	Data 0.005 (0.070)	Loss 0.5716 (0.5441)	Acc@1 88.867 (89.951)	Acc@5 99.463 (99.707)
Epoch: [57][10/25]	Time 0.660 (0.721)	Data 0.006 (0.064)	Loss 0.5337 (0.5432)	Acc@1 90.576 (90.008)	Acc@5 99.609 (99.698)
Epoch: [57][11/25]	Time 0.718 (0.721)	Data 0.008 (0.059)	Loss 0.5463 (0.5434)	Acc@1 89.307 (89.950)	Acc@5 99.805 (99.707)
Epoch: [57][12/25]	Time 0.741 (0.722)	Data 0.005 (0.055)	Loss 0.5713 (0.5456)	Acc@1 89.453 (89.911)	Acc@5 99.707 (99.707)
Epoch: [57][13/25]	Time 0.740 (0.724)	Data 0.004 (0.052)	Loss 0.5344 (0.5448)	Acc@1 89.795 (89.903)	Acc@5 99.707 (99.707)
Epoch: [57][14/25]	Time 0.649 (0.719)	Data 0.007 (0.049)	Loss 0.5824 (0.5473)	Acc@1 88.623 (89.818)	Acc@5 99.658 (99.704)
Epoch: [57][15/25]	Time 0.637 (0.713)	Data 0.007 (0.046)	Loss 0.5633 (0.5483)	Acc@1 89.551 (89.801)	Acc@5 99.805 (99.710)
Epoch: [57][16/25]	Time 0.630 (0.709)	Data 0.007 (0.044)	Loss 0.5902 (0.5508)	Acc@1 88.135 (89.703)	Acc@5 99.707 (99.710)
Epoch: [57][17/25]	Time 0.672 (0.707)	Data 0.008 (0.042)	Loss 0.5790 (0.5523)	Acc@1 88.721 (89.648)	Acc@5 99.854 (99.718)
Epoch: [57][18/25]	Time 0.746 (0.709)	Data 0.004 (0.040)	Loss 0.5567 (0.5526)	Acc@1 89.453 (89.638)	Acc@5 99.707 (99.717)
Epoch: [57][19/25]	Time 0.694 (0.708)	Data 0.004 (0.038)	Loss 0.6038 (0.5551)	Acc@1 88.281 (89.570)	Acc@5 99.902 (99.727)
Epoch: [57][20/25]	Time 0.710 (0.708)	Data 0.005 (0.036)	Loss 0.5660 (0.5556)	Acc@1 89.600 (89.572)	Acc@5 99.707 (99.726)
Epoch: [57][21/25]	Time 0.723 (0.709)	Data 0.007 (0.035)	Loss 0.5924 (0.5573)	Acc@1 88.428 (89.520)	Acc@5 99.414 (99.711)
Epoch: [57][22/25]	Time 0.693 (0.708)	Data 0.005 (0.034)	Loss 0.6121 (0.5597)	Acc@1 87.646 (89.438)	Acc@5 99.512 (99.703)
Epoch: [57][23/25]	Time 0.713 (0.708)	Data 0.008 (0.033)	Loss 0.5733 (0.5603)	Acc@1 89.551 (89.443)	Acc@5 99.561 (99.697)
Epoch: [57][24/25]	Time 0.449 (0.698)	Data 0.005 (0.032)	Loss 0.6425 (0.5617)	Acc@1 87.264 (89.406)	Acc@5 99.057 (99.686)

Epoch: [58 | 180] LR: 0.100000
Epoch: [58][0/25]	Time 0.770 (0.770)	Data 0.637 (0.637)	Loss 0.5976 (0.5976)	Acc@1 89.014 (89.014)	Acc@5 99.219 (99.219)
Epoch: [58][1/25]	Time 0.667 (0.719)	Data 0.007 (0.322)	Loss 0.5470 (0.5723)	Acc@1 90.186 (89.600)	Acc@5 99.609 (99.414)
Epoch: [58][2/25]	Time 0.643 (0.693)	Data 0.005 (0.217)	Loss 0.6211 (0.5886)	Acc@1 87.646 (88.949)	Acc@5 99.805 (99.544)
Epoch: [58][3/25]	Time 0.618 (0.674)	Data 0.005 (0.164)	Loss 0.5848 (0.5876)	Acc@1 89.062 (88.977)	Acc@5 99.609 (99.561)
Epoch: [58][4/25]	Time 0.675 (0.675)	Data 0.006 (0.132)	Loss 0.6207 (0.5942)	Acc@1 86.914 (88.564)	Acc@5 99.658 (99.580)
Epoch: [58][5/25]	Time 0.707 (0.680)	Data 0.007 (0.111)	Loss 0.5806 (0.5920)	Acc@1 89.258 (88.680)	Acc@5 99.707 (99.601)
Epoch: [58][6/25]	Time 0.723 (0.686)	Data 0.005 (0.096)	Loss 0.5988 (0.5930)	Acc@1 88.330 (88.630)	Acc@5 99.561 (99.595)
Epoch: [58][7/25]	Time 0.751 (0.694)	Data 0.004 (0.085)	Loss 0.6307 (0.5977)	Acc@1 86.328 (88.342)	Acc@5 99.707 (99.609)
Epoch: [58][8/25]	Time 0.699 (0.695)	Data 0.006 (0.076)	Loss 0.5758 (0.5952)	Acc@1 88.623 (88.373)	Acc@5 99.707 (99.620)
Epoch: [58][9/25]	Time 0.664 (0.692)	Data 0.005 (0.069)	Loss 0.5841 (0.5941)	Acc@1 88.574 (88.394)	Acc@5 99.854 (99.644)
Epoch: [58][10/25]	Time 0.705 (0.693)	Data 0.004 (0.063)	Loss 0.5568 (0.5907)	Acc@1 90.332 (88.570)	Acc@5 99.854 (99.663)
Epoch: [58][11/25]	Time 0.732 (0.696)	Data 0.007 (0.058)	Loss 0.5750 (0.5894)	Acc@1 89.160 (88.619)	Acc@5 99.609 (99.658)
Epoch: [58][12/25]	Time 0.716 (0.698)	Data 0.006 (0.054)	Loss 0.5696 (0.5879)	Acc@1 89.551 (88.691)	Acc@5 99.463 (99.643)
Epoch: [58][13/25]	Time 0.734 (0.700)	Data 0.005 (0.051)	Loss 0.6180 (0.5900)	Acc@1 87.158 (88.581)	Acc@5 99.561 (99.637)
Epoch: [58][14/25]	Time 0.706 (0.701)	Data 0.005 (0.048)	Loss 0.5633 (0.5883)	Acc@1 89.453 (88.639)	Acc@5 99.805 (99.648)
Epoch: [58][15/25]	Time 0.735 (0.703)	Data 0.008 (0.045)	Loss 0.5803 (0.5878)	Acc@1 88.867 (88.654)	Acc@5 99.707 (99.652)
Epoch: [58][16/25]	Time 0.694 (0.702)	Data 0.009 (0.043)	Loss 0.5948 (0.5882)	Acc@1 88.623 (88.652)	Acc@5 99.658 (99.652)
Epoch: [58][17/25]	Time 0.674 (0.701)	Data 0.004 (0.041)	Loss 0.5924 (0.5884)	Acc@1 88.672 (88.653)	Acc@5 99.707 (99.655)
Epoch: [58][18/25]	Time 0.784 (0.705)	Data 0.006 (0.039)	Loss 0.5666 (0.5873)	Acc@1 90.039 (88.726)	Acc@5 99.756 (99.661)
Epoch: [58][19/25]	Time 0.695 (0.705)	Data 0.007 (0.037)	Loss 0.5873 (0.5873)	Acc@1 88.770 (88.728)	Acc@5 99.463 (99.651)
Epoch: [58][20/25]	Time 0.695 (0.704)	Data 0.005 (0.036)	Loss 0.6053 (0.5881)	Acc@1 87.646 (88.677)	Acc@5 99.561 (99.647)
Epoch: [58][21/25]	Time 0.725 (0.705)	Data 0.006 (0.034)	Loss 0.5834 (0.5879)	Acc@1 89.062 (88.694)	Acc@5 99.463 (99.638)
Epoch: [58][22/25]	Time 0.743 (0.707)	Data 0.005 (0.033)	Loss 0.5890 (0.5880)	Acc@1 88.916 (88.704)	Acc@5 99.414 (99.628)
Epoch: [58][23/25]	Time 0.753 (0.709)	Data 0.004 (0.032)	Loss 0.5765 (0.5875)	Acc@1 88.965 (88.715)	Acc@5 99.609 (99.628)
Epoch: [58][24/25]	Time 0.380 (0.695)	Data 0.007 (0.031)	Loss 0.5717 (0.5872)	Acc@1 88.561 (88.712)	Acc@5 99.882 (99.632)

Epoch: [59 | 180] LR: 0.100000
Epoch: [59][0/25]	Time 0.756 (0.756)	Data 0.666 (0.666)	Loss 0.5732 (0.5732)	Acc@1 87.988 (87.988)	Acc@5 99.805 (99.805)
Epoch: [59][1/25]	Time 0.713 (0.734)	Data 0.007 (0.336)	Loss 0.6190 (0.5961)	Acc@1 87.646 (87.817)	Acc@5 99.756 (99.780)
Epoch: [59][2/25]	Time 0.739 (0.736)	Data 0.007 (0.226)	Loss 0.5771 (0.5898)	Acc@1 89.404 (88.346)	Acc@5 99.512 (99.691)
Epoch: [59][3/25]	Time 0.686 (0.723)	Data 0.005 (0.171)	Loss 0.6127 (0.5955)	Acc@1 88.232 (88.318)	Acc@5 99.512 (99.646)
Epoch: [59][4/25]	Time 0.691 (0.717)	Data 0.007 (0.138)	Loss 0.5871 (0.5938)	Acc@1 89.014 (88.457)	Acc@5 99.609 (99.639)
Epoch: [59][5/25]	Time 0.732 (0.719)	Data 0.006 (0.116)	Loss 0.5766 (0.5910)	Acc@1 89.502 (88.631)	Acc@5 99.707 (99.650)
Epoch: [59][6/25]	Time 0.706 (0.717)	Data 0.007 (0.101)	Loss 0.5917 (0.5911)	Acc@1 89.502 (88.756)	Acc@5 99.463 (99.623)
Epoch: [59][7/25]	Time 0.694 (0.715)	Data 0.006 (0.089)	Loss 0.6006 (0.5923)	Acc@1 88.330 (88.702)	Acc@5 99.512 (99.609)
Epoch: [59][8/25]	Time 0.773 (0.721)	Data 0.006 (0.080)	Loss 0.5877 (0.5918)	Acc@1 88.770 (88.710)	Acc@5 99.561 (99.604)
Epoch: [59][9/25]	Time 0.688 (0.718)	Data 0.006 (0.072)	Loss 0.5787 (0.5905)	Acc@1 89.355 (88.774)	Acc@5 99.414 (99.585)
Epoch: [59][10/25]	Time 0.677 (0.714)	Data 0.005 (0.066)	Loss 0.5600 (0.5877)	Acc@1 89.600 (88.849)	Acc@5 99.561 (99.583)
Epoch: [59][11/25]	Time 0.667 (0.710)	Data 0.005 (0.061)	Loss 0.6142 (0.5899)	Acc@1 87.988 (88.778)	Acc@5 99.658 (99.589)
Epoch: [59][12/25]	Time 0.692 (0.709)	Data 0.005 (0.057)	Loss 0.5836 (0.5894)	Acc@1 89.111 (88.803)	Acc@5 99.805 (99.606)
Epoch: [59][13/25]	Time 0.644 (0.704)	Data 0.008 (0.053)	Loss 0.5813 (0.5888)	Acc@1 89.209 (88.832)	Acc@5 99.658 (99.609)
Epoch: [59][14/25]	Time 0.667 (0.702)	Data 0.006 (0.050)	Loss 0.5822 (0.5884)	Acc@1 88.818 (88.831)	Acc@5 99.658 (99.613)
Epoch: [59][15/25]	Time 0.667 (0.699)	Data 0.005 (0.047)	Loss 0.6040 (0.5894)	Acc@1 88.672 (88.821)	Acc@5 99.414 (99.600)
Epoch: [59][16/25]	Time 0.745 (0.702)	Data 0.005 (0.045)	Loss 0.5658 (0.5880)	Acc@1 88.428 (88.798)	Acc@5 99.805 (99.612)
Epoch: [59][17/25]	Time 0.746 (0.705)	Data 0.006 (0.043)	Loss 0.6000 (0.5887)	Acc@1 88.525 (88.783)	Acc@5 99.512 (99.607)
Epoch: [59][18/25]	Time 0.683 (0.703)	Data 0.004 (0.041)	Loss 0.5956 (0.5890)	Acc@1 88.086 (88.746)	Acc@5 99.512 (99.602)
Epoch: [59][19/25]	Time 0.698 (0.703)	Data 0.004 (0.039)	Loss 0.6041 (0.5898)	Acc@1 87.744 (88.696)	Acc@5 99.707 (99.607)
Epoch: [59][20/25]	Time 0.745 (0.705)	Data 0.006 (0.037)	Loss 0.6227 (0.5913)	Acc@1 87.793 (88.653)	Acc@5 99.512 (99.602)
Epoch: [59][21/25]	Time 0.699 (0.705)	Data 0.004 (0.036)	Loss 0.6019 (0.5918)	Acc@1 87.988 (88.623)	Acc@5 99.756 (99.609)
Epoch: [59][22/25]	Time 0.718 (0.705)	Data 0.004 (0.034)	Loss 0.6335 (0.5936)	Acc@1 87.793 (88.587)	Acc@5 99.609 (99.609)
Epoch: [59][23/25]	Time 0.752 (0.707)	Data 0.006 (0.033)	Loss 0.5714 (0.5927)	Acc@1 89.307 (88.617)	Acc@5 99.609 (99.609)
Epoch: [59][24/25]	Time 0.411 (0.696)	Data 0.006 (0.032)	Loss 0.5841 (0.5926)	Acc@1 89.269 (88.628)	Acc@5 99.646 (99.610)

Epoch: [60 | 180] LR: 0.100000
Epoch: [60][0/25]	Time 0.704 (0.704)	Data 0.624 (0.624)	Loss 0.5826 (0.5826)	Acc@1 90.186 (90.186)	Acc@5 99.658 (99.658)
Epoch: [60][1/25]	Time 0.753 (0.728)	Data 0.013 (0.319)	Loss 0.5865 (0.5845)	Acc@1 88.916 (89.551)	Acc@5 99.512 (99.585)
Epoch: [60][2/25]	Time 0.703 (0.720)	Data 0.004 (0.214)	Loss 0.5851 (0.5847)	Acc@1 88.867 (89.323)	Acc@5 99.561 (99.577)
Epoch: [60][3/25]	Time 0.758 (0.729)	Data 0.006 (0.162)	Loss 0.5860 (0.5851)	Acc@1 89.355 (89.331)	Acc@5 99.707 (99.609)
Epoch: [60][4/25]	Time 0.730 (0.730)	Data 0.008 (0.131)	Loss 0.6028 (0.5886)	Acc@1 88.672 (89.199)	Acc@5 99.512 (99.590)
Epoch: [60][5/25]	Time 0.722 (0.728)	Data 0.005 (0.110)	Loss 0.5774 (0.5867)	Acc@1 88.916 (89.152)	Acc@5 99.805 (99.626)
Epoch: [60][6/25]	Time 0.701 (0.724)	Data 0.006 (0.095)	Loss 0.5840 (0.5863)	Acc@1 88.428 (89.049)	Acc@5 99.805 (99.651)
Epoch: [60][7/25]	Time 0.677 (0.719)	Data 0.004 (0.084)	Loss 0.5461 (0.5813)	Acc@1 90.283 (89.203)	Acc@5 99.561 (99.640)
Epoch: [60][8/25]	Time 0.751 (0.722)	Data 0.006 (0.075)	Loss 0.5709 (0.5801)	Acc@1 89.355 (89.220)	Acc@5 99.854 (99.664)
Epoch: [60][9/25]	Time 0.738 (0.724)	Data 0.008 (0.068)	Loss 0.5676 (0.5789)	Acc@1 88.867 (89.185)	Acc@5 99.707 (99.668)
Epoch: [60][10/25]	Time 0.676 (0.719)	Data 0.004 (0.063)	Loss 0.5657 (0.5777)	Acc@1 89.795 (89.240)	Acc@5 99.609 (99.663)
Epoch: [60][11/25]	Time 0.723 (0.720)	Data 0.009 (0.058)	Loss 0.5882 (0.5786)	Acc@1 88.379 (89.168)	Acc@5 99.512 (99.650)
Epoch: [60][12/25]	Time 0.719 (0.720)	Data 0.005 (0.054)	Loss 0.5686 (0.5778)	Acc@1 89.600 (89.201)	Acc@5 99.561 (99.643)
Epoch: [60][13/25]	Time 0.661 (0.715)	Data 0.005 (0.050)	Loss 0.5659 (0.5770)	Acc@1 89.453 (89.219)	Acc@5 99.463 (99.630)
Epoch: [60][14/25]	Time 0.663 (0.712)	Data 0.007 (0.048)	Loss 0.5728 (0.5767)	Acc@1 88.867 (89.196)	Acc@5 99.512 (99.622)
Epoch: [60][15/25]	Time 0.623 (0.706)	Data 0.007 (0.045)	Loss 0.5904 (0.5775)	Acc@1 88.379 (89.145)	Acc@5 99.658 (99.625)
Epoch: [60][16/25]	Time 0.640 (0.702)	Data 0.009 (0.043)	Loss 0.5725 (0.5772)	Acc@1 89.453 (89.163)	Acc@5 99.609 (99.624)
Epoch: [60][17/25]	Time 0.658 (0.700)	Data 0.005 (0.041)	Loss 0.5666 (0.5767)	Acc@1 89.697 (89.193)	Acc@5 99.756 (99.631)
Epoch: [60][18/25]	Time 0.660 (0.698)	Data 0.008 (0.039)	Loss 0.5785 (0.5768)	Acc@1 89.453 (89.206)	Acc@5 99.805 (99.640)
Epoch: [60][19/25]	Time 0.721 (0.699)	Data 0.007 (0.037)	Loss 0.5579 (0.5758)	Acc@1 89.062 (89.199)	Acc@5 99.609 (99.639)
Epoch: [60][20/25]	Time 0.765 (0.702)	Data 0.007 (0.036)	Loss 0.5686 (0.5755)	Acc@1 90.039 (89.239)	Acc@5 99.756 (99.644)
Epoch: [60][21/25]	Time 0.683 (0.701)	Data 0.004 (0.035)	Loss 0.5672 (0.5751)	Acc@1 88.916 (89.225)	Acc@5 99.561 (99.640)
Epoch: [60][22/25]	Time 0.614 (0.698)	Data 0.004 (0.033)	Loss 0.5623 (0.5745)	Acc@1 89.990 (89.258)	Acc@5 99.805 (99.648)
Epoch: [60][23/25]	Time 0.665 (0.696)	Data 0.006 (0.032)	Loss 0.5985 (0.5755)	Acc@1 88.184 (89.213)	Acc@5 99.561 (99.644)
Epoch: [60][24/25]	Time 0.348 (0.682)	Data 0.006 (0.031)	Loss 0.6194 (0.5763)	Acc@1 87.618 (89.186)	Acc@5 99.292 (99.638)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 468020 ; 487386 ; 0.960265580053592

Epoch: [61 | 180] LR: 0.100000
Epoch: [61][0/25]	Time 0.724 (0.724)	Data 0.627 (0.627)	Loss 0.5956 (0.5956)	Acc@1 88.477 (88.477)	Acc@5 99.512 (99.512)
Epoch: [61][1/25]	Time 0.724 (0.724)	Data 0.006 (0.316)	Loss 0.4980 (0.5468)	Acc@1 91.846 (90.161)	Acc@5 99.609 (99.561)
Epoch: [61][2/25]	Time 0.779 (0.742)	Data 0.008 (0.214)	Loss 0.5076 (0.5337)	Acc@1 91.797 (90.706)	Acc@5 99.951 (99.691)
Epoch: [61][3/25]	Time 0.682 (0.727)	Data 0.003 (0.161)	Loss 0.5451 (0.5366)	Acc@1 90.332 (90.613)	Acc@5 99.561 (99.658)
Epoch: [61][4/25]	Time 0.622 (0.706)	Data 0.009 (0.131)	Loss 0.5085 (0.5309)	Acc@1 91.748 (90.840)	Acc@5 99.902 (99.707)
Epoch: [61][5/25]	Time 0.659 (0.698)	Data 0.007 (0.110)	Loss 0.5396 (0.5324)	Acc@1 90.674 (90.812)	Acc@5 99.756 (99.715)
Epoch: [61][6/25]	Time 0.727 (0.702)	Data 0.007 (0.095)	Loss 0.5273 (0.5317)	Acc@1 91.064 (90.848)	Acc@5 99.902 (99.742)
Epoch: [61][7/25]	Time 0.755 (0.709)	Data 0.007 (0.084)	Loss 0.5190 (0.5301)	Acc@1 90.918 (90.857)	Acc@5 99.707 (99.738)
Epoch: [61][8/25]	Time 0.689 (0.707)	Data 0.008 (0.076)	Loss 0.4955 (0.5262)	Acc@1 92.041 (90.988)	Acc@5 99.756 (99.740)
Epoch: [61][9/25]	Time 0.734 (0.709)	Data 0.010 (0.069)	Loss 0.5121 (0.5248)	Acc@1 91.455 (91.035)	Acc@5 99.707 (99.736)
Epoch: [61][10/25]	Time 0.764 (0.714)	Data 0.007 (0.063)	Loss 0.5399 (0.5262)	Acc@1 90.332 (90.971)	Acc@5 99.707 (99.734)
Epoch: [61][11/25]	Time 0.656 (0.710)	Data 0.008 (0.059)	Loss 0.5174 (0.5255)	Acc@1 91.357 (91.003)	Acc@5 99.707 (99.731)
Epoch: [61][12/25]	Time 0.651 (0.705)	Data 0.005 (0.055)	Loss 0.5271 (0.5256)	Acc@1 90.430 (90.959)	Acc@5 99.658 (99.726)
Epoch: [61][13/25]	Time 0.612 (0.698)	Data 0.007 (0.051)	Loss 0.5484 (0.5272)	Acc@1 89.844 (90.880)	Acc@5 99.463 (99.707)
Epoch: [61][14/25]	Time 0.669 (0.696)	Data 0.007 (0.048)	Loss 0.5415 (0.5282)	Acc@1 90.381 (90.846)	Acc@5 99.707 (99.707)
Epoch: [61][15/25]	Time 0.666 (0.695)	Data 0.004 (0.046)	Loss 0.5739 (0.5310)	Acc@1 89.160 (90.741)	Acc@5 99.609 (99.701)
Epoch: [61][16/25]	Time 0.718 (0.696)	Data 0.008 (0.043)	Loss 0.5307 (0.5310)	Acc@1 90.918 (90.751)	Acc@5 99.756 (99.704)
Epoch: [61][17/25]	Time 0.732 (0.698)	Data 0.004 (0.041)	Loss 0.5627 (0.5328)	Acc@1 89.307 (90.671)	Acc@5 99.707 (99.704)
Epoch: [61][18/25]	Time 0.673 (0.697)	Data 0.004 (0.039)	Loss 0.5264 (0.5324)	Acc@1 90.967 (90.687)	Acc@5 99.609 (99.699)
Epoch: [61][19/25]	Time 0.726 (0.698)	Data 0.007 (0.038)	Loss 0.5389 (0.5328)	Acc@1 90.381 (90.671)	Acc@5 99.756 (99.702)
Epoch: [61][20/25]	Time 0.723 (0.699)	Data 0.004 (0.036)	Loss 0.5375 (0.5330)	Acc@1 90.137 (90.646)	Acc@5 99.609 (99.698)
Epoch: [61][21/25]	Time 0.673 (0.698)	Data 0.005 (0.035)	Loss 0.5432 (0.5335)	Acc@1 90.332 (90.632)	Acc@5 99.561 (99.691)
Epoch: [61][22/25]	Time 0.693 (0.698)	Data 0.004 (0.033)	Loss 0.5511 (0.5342)	Acc@1 90.576 (90.629)	Acc@5 99.707 (99.692)
Epoch: [61][23/25]	Time 0.724 (0.699)	Data 0.004 (0.032)	Loss 0.5566 (0.5352)	Acc@1 89.502 (90.582)	Acc@5 99.707 (99.693)
Epoch: [61][24/25]	Time 0.471 (0.690)	Data 0.004 (0.031)	Loss 0.5732 (0.5358)	Acc@1 89.269 (90.560)	Acc@5 99.646 (99.692)

Epoch: [62 | 180] LR: 0.100000
Epoch: [62][0/25]	Time 0.758 (0.758)	Data 0.588 (0.588)	Loss 0.5742 (0.5742)	Acc@1 89.014 (89.014)	Acc@5 99.707 (99.707)
Epoch: [62][1/25]	Time 0.696 (0.727)	Data 0.007 (0.297)	Loss 0.5528 (0.5635)	Acc@1 89.160 (89.087)	Acc@5 99.707 (99.707)
Epoch: [62][2/25]	Time 0.688 (0.714)	Data 0.004 (0.200)	Loss 0.5513 (0.5595)	Acc@1 90.137 (89.437)	Acc@5 99.756 (99.723)
Epoch: [62][3/25]	Time 0.724 (0.716)	Data 0.005 (0.151)	Loss 0.5577 (0.5590)	Acc@1 89.893 (89.551)	Acc@5 99.609 (99.695)
Epoch: [62][4/25]	Time 0.724 (0.718)	Data 0.004 (0.122)	Loss 0.5482 (0.5569)	Acc@1 90.039 (89.648)	Acc@5 99.854 (99.727)
Epoch: [62][5/25]	Time 0.673 (0.710)	Data 0.004 (0.102)	Loss 0.5858 (0.5617)	Acc@1 88.623 (89.478)	Acc@5 99.561 (99.699)
Epoch: [62][6/25]	Time 0.724 (0.712)	Data 0.005 (0.088)	Loss 0.5542 (0.5606)	Acc@1 89.697 (89.509)	Acc@5 99.854 (99.721)
Epoch: [62][7/25]	Time 0.748 (0.717)	Data 0.006 (0.078)	Loss 0.5950 (0.5649)	Acc@1 88.574 (89.392)	Acc@5 99.707 (99.719)
Epoch: [62][8/25]	Time 0.904 (0.738)	Data 0.004 (0.070)	Loss 0.5736 (0.5659)	Acc@1 89.746 (89.431)	Acc@5 99.463 (99.691)
Epoch: [62][9/25]	Time 0.763 (0.740)	Data 0.006 (0.063)	Loss 0.5620 (0.5655)	Acc@1 89.941 (89.482)	Acc@5 99.756 (99.697)
Epoch: [62][10/25]	Time 0.774 (0.743)	Data 0.004 (0.058)	Loss 0.5718 (0.5661)	Acc@1 89.111 (89.449)	Acc@5 99.707 (99.698)
Epoch: [62][11/25]	Time 0.724 (0.742)	Data 0.005 (0.054)	Loss 0.5854 (0.5677)	Acc@1 88.574 (89.376)	Acc@5 99.658 (99.695)
Epoch: [62][12/25]	Time 0.737 (0.741)	Data 0.004 (0.050)	Loss 0.5533 (0.5666)	Acc@1 89.941 (89.419)	Acc@5 99.707 (99.696)
Epoch: [62][13/25]	Time 0.760 (0.743)	Data 0.007 (0.047)	Loss 0.5744 (0.5671)	Acc@1 89.014 (89.390)	Acc@5 99.854 (99.707)
Epoch: [62][14/25]	Time 0.706 (0.740)	Data 0.005 (0.044)	Loss 0.5703 (0.5673)	Acc@1 88.623 (89.339)	Acc@5 99.658 (99.704)
Epoch: [62][15/25]	Time 0.687 (0.737)	Data 0.008 (0.042)	Loss 0.5564 (0.5667)	Acc@1 89.209 (89.331)	Acc@5 99.805 (99.710)
Epoch: [62][16/25]	Time 0.689 (0.734)	Data 0.005 (0.039)	Loss 0.5755 (0.5672)	Acc@1 88.623 (89.289)	Acc@5 99.512 (99.698)
Epoch: [62][17/25]	Time 0.732 (0.734)	Data 0.006 (0.038)	Loss 0.6247 (0.5704)	Acc@1 87.305 (89.179)	Acc@5 99.658 (99.696)
Epoch: [62][18/25]	Time 0.701 (0.732)	Data 0.007 (0.036)	Loss 0.5760 (0.5707)	Acc@1 88.477 (89.142)	Acc@5 99.854 (99.704)
Epoch: [62][19/25]	Time 0.737 (0.732)	Data 0.009 (0.035)	Loss 0.5912 (0.5717)	Acc@1 89.355 (89.153)	Acc@5 99.365 (99.688)
Epoch: [62][20/25]	Time 0.737 (0.733)	Data 0.004 (0.033)	Loss 0.5912 (0.5726)	Acc@1 88.525 (89.123)	Acc@5 99.561 (99.681)
Epoch: [62][21/25]	Time 0.665 (0.730)	Data 0.007 (0.032)	Loss 0.5962 (0.5737)	Acc@1 89.062 (89.120)	Acc@5 99.658 (99.680)
Epoch: [62][22/25]	Time 0.645 (0.726)	Data 0.006 (0.031)	Loss 0.5816 (0.5740)	Acc@1 88.623 (89.099)	Acc@5 99.561 (99.675)
Epoch: [62][23/25]	Time 0.635 (0.722)	Data 0.004 (0.030)	Loss 0.5803 (0.5743)	Acc@1 89.307 (89.107)	Acc@5 99.805 (99.681)
Epoch: [62][24/25]	Time 0.361 (0.708)	Data 0.005 (0.029)	Loss 0.5398 (0.5737)	Acc@1 90.920 (89.138)	Acc@5 99.528 (99.678)

Epoch: [63 | 180] LR: 0.100000
Epoch: [63][0/25]	Time 0.722 (0.722)	Data 0.634 (0.634)	Loss 0.5651 (0.5651)	Acc@1 89.160 (89.160)	Acc@5 99.658 (99.658)
Epoch: [63][1/25]	Time 0.706 (0.714)	Data 0.005 (0.319)	Loss 0.5556 (0.5603)	Acc@1 90.186 (89.673)	Acc@5 99.561 (99.609)
Epoch: [63][2/25]	Time 0.725 (0.717)	Data 0.007 (0.215)	Loss 0.5771 (0.5659)	Acc@1 88.574 (89.307)	Acc@5 99.609 (99.609)
Epoch: [63][3/25]	Time 0.737 (0.722)	Data 0.005 (0.163)	Loss 0.5808 (0.5696)	Acc@1 89.014 (89.233)	Acc@5 99.707 (99.634)
Epoch: [63][4/25]	Time 0.707 (0.719)	Data 0.007 (0.132)	Loss 0.5387 (0.5635)	Acc@1 89.404 (89.268)	Acc@5 99.805 (99.668)
Epoch: [63][5/25]	Time 0.703 (0.717)	Data 0.008 (0.111)	Loss 0.5894 (0.5678)	Acc@1 88.330 (89.111)	Acc@5 99.658 (99.666)
Epoch: [63][6/25]	Time 0.840 (0.734)	Data 0.005 (0.096)	Loss 0.6088 (0.5736)	Acc@1 87.109 (88.825)	Acc@5 99.561 (99.651)
Epoch: [63][7/25]	Time 0.763 (0.738)	Data 0.005 (0.084)	Loss 0.5725 (0.5735)	Acc@1 88.916 (88.837)	Acc@5 99.561 (99.640)
Epoch: [63][8/25]	Time 0.627 (0.726)	Data 0.004 (0.075)	Loss 0.5567 (0.5716)	Acc@1 89.648 (88.927)	Acc@5 99.707 (99.647)
Epoch: [63][9/25]	Time 0.690 (0.722)	Data 0.007 (0.069)	Loss 0.5649 (0.5710)	Acc@1 89.990 (89.033)	Acc@5 99.707 (99.653)
Epoch: [63][10/25]	Time 0.635 (0.714)	Data 0.006 (0.063)	Loss 0.5914 (0.5728)	Acc@1 88.916 (89.023)	Acc@5 99.316 (99.623)
Epoch: [63][11/25]	Time 0.651 (0.709)	Data 0.005 (0.058)	Loss 0.5855 (0.5739)	Acc@1 88.574 (88.985)	Acc@5 99.609 (99.622)
Epoch: [63][12/25]	Time 0.638 (0.703)	Data 0.010 (0.054)	Loss 0.5898 (0.5751)	Acc@1 88.867 (88.976)	Acc@5 99.316 (99.598)
Epoch: [63][13/25]	Time 0.662 (0.700)	Data 0.008 (0.051)	Loss 0.5878 (0.5760)	Acc@1 88.672 (88.954)	Acc@5 99.707 (99.606)
Epoch: [63][14/25]	Time 0.748 (0.704)	Data 0.005 (0.048)	Loss 0.6091 (0.5782)	Acc@1 87.207 (88.838)	Acc@5 99.414 (99.593)
Epoch: [63][15/25]	Time 0.769 (0.708)	Data 0.008 (0.045)	Loss 0.5592 (0.5770)	Acc@1 89.893 (88.904)	Acc@5 99.756 (99.603)
Epoch: [63][16/25]	Time 0.776 (0.712)	Data 0.008 (0.043)	Loss 0.5659 (0.5764)	Acc@1 89.209 (88.922)	Acc@5 99.854 (99.618)
Epoch: [63][17/25]	Time 0.698 (0.711)	Data 0.006 (0.041)	Loss 0.6109 (0.5783)	Acc@1 87.793 (88.859)	Acc@5 99.561 (99.615)
Epoch: [63][18/25]	Time 0.735 (0.712)	Data 0.005 (0.039)	Loss 0.5867 (0.5787)	Acc@1 88.867 (88.859)	Acc@5 99.414 (99.604)
Epoch: [63][19/25]	Time 0.759 (0.715)	Data 0.006 (0.038)	Loss 0.6260 (0.5811)	Acc@1 87.305 (88.782)	Acc@5 99.512 (99.600)
Epoch: [63][20/25]	Time 0.739 (0.716)	Data 0.006 (0.036)	Loss 0.5807 (0.5811)	Acc@1 88.232 (88.756)	Acc@5 99.609 (99.600)
Epoch: [63][21/25]	Time 0.703 (0.715)	Data 0.005 (0.035)	Loss 0.5796 (0.5810)	Acc@1 88.770 (88.756)	Acc@5 99.609 (99.600)
Epoch: [63][22/25]	Time 0.739 (0.716)	Data 0.008 (0.034)	Loss 0.5671 (0.5804)	Acc@1 89.648 (88.795)	Acc@5 99.805 (99.609)
Epoch: [63][23/25]	Time 0.756 (0.718)	Data 0.006 (0.032)	Loss 0.5979 (0.5811)	Acc@1 88.086 (88.765)	Acc@5 99.658 (99.611)
Epoch: [63][24/25]	Time 0.412 (0.706)	Data 0.004 (0.031)	Loss 0.6297 (0.5820)	Acc@1 86.203 (88.722)	Acc@5 99.882 (99.616)

Epoch: [64 | 180] LR: 0.100000
Epoch: [64][0/25]	Time 0.703 (0.703)	Data 0.777 (0.777)	Loss 0.5623 (0.5623)	Acc@1 89.795 (89.795)	Acc@5 99.805 (99.805)
Epoch: [64][1/25]	Time 0.663 (0.683)	Data 0.005 (0.391)	Loss 0.6012 (0.5817)	Acc@1 87.988 (88.892)	Acc@5 99.561 (99.683)
Epoch: [64][2/25]	Time 0.760 (0.709)	Data 0.006 (0.263)	Loss 0.5777 (0.5804)	Acc@1 89.551 (89.111)	Acc@5 99.707 (99.691)
Epoch: [64][3/25]	Time 0.687 (0.703)	Data 0.007 (0.199)	Loss 0.6022 (0.5858)	Acc@1 88.525 (88.965)	Acc@5 99.365 (99.609)
Epoch: [64][4/25]	Time 0.715 (0.706)	Data 0.007 (0.160)	Loss 0.5711 (0.5829)	Acc@1 89.258 (89.023)	Acc@5 99.902 (99.668)
Epoch: [64][5/25]	Time 0.746 (0.712)	Data 0.007 (0.135)	Loss 0.5848 (0.5832)	Acc@1 88.672 (88.965)	Acc@5 99.658 (99.666)
Epoch: [64][6/25]	Time 0.691 (0.709)	Data 0.005 (0.116)	Loss 0.5991 (0.5855)	Acc@1 87.598 (88.770)	Acc@5 99.707 (99.672)
Epoch: [64][7/25]	Time 0.702 (0.708)	Data 0.005 (0.102)	Loss 0.5823 (0.5851)	Acc@1 88.770 (88.770)	Acc@5 99.609 (99.664)
Epoch: [64][8/25]	Time 0.735 (0.711)	Data 0.006 (0.092)	Loss 0.5705 (0.5835)	Acc@1 89.844 (88.889)	Acc@5 99.756 (99.674)
Epoch: [64][9/25]	Time 0.739 (0.714)	Data 0.007 (0.083)	Loss 0.6071 (0.5858)	Acc@1 88.428 (88.843)	Acc@5 99.609 (99.668)
Epoch: [64][10/25]	Time 0.728 (0.716)	Data 0.005 (0.076)	Loss 0.5685 (0.5842)	Acc@1 89.600 (88.912)	Acc@5 99.561 (99.658)
Epoch: [64][11/25]	Time 0.717 (0.716)	Data 0.006 (0.070)	Loss 0.5705 (0.5831)	Acc@1 89.648 (88.973)	Acc@5 99.854 (99.674)
Epoch: [64][12/25]	Time 0.707 (0.715)	Data 0.006 (0.065)	Loss 0.5894 (0.5836)	Acc@1 88.574 (88.942)	Acc@5 99.609 (99.669)
Epoch: [64][13/25]	Time 0.746 (0.717)	Data 0.004 (0.061)	Loss 0.5783 (0.5832)	Acc@1 89.453 (88.979)	Acc@5 99.658 (99.669)
Epoch: [64][14/25]	Time 0.663 (0.714)	Data 0.004 (0.057)	Loss 0.5975 (0.5842)	Acc@1 89.307 (89.001)	Acc@5 99.414 (99.652)
Epoch: [64][15/25]	Time 0.628 (0.708)	Data 0.005 (0.054)	Loss 0.5900 (0.5845)	Acc@1 88.574 (88.974)	Acc@5 99.756 (99.658)
Epoch: [64][16/25]	Time 0.673 (0.706)	Data 0.008 (0.051)	Loss 0.6025 (0.5856)	Acc@1 88.818 (88.965)	Acc@5 99.658 (99.658)
Epoch: [64][17/25]	Time 0.665 (0.704)	Data 0.005 (0.049)	Loss 0.5818 (0.5854)	Acc@1 89.551 (88.997)	Acc@5 99.707 (99.661)
Epoch: [64][18/25]	Time 0.681 (0.703)	Data 0.005 (0.046)	Loss 0.5565 (0.5838)	Acc@1 89.844 (89.042)	Acc@5 99.658 (99.661)
Epoch: [64][19/25]	Time 0.701 (0.703)	Data 0.006 (0.044)	Loss 0.5715 (0.5832)	Acc@1 89.160 (89.048)	Acc@5 99.805 (99.668)
Epoch: [64][20/25]	Time 0.726 (0.704)	Data 0.006 (0.042)	Loss 0.5793 (0.5830)	Acc@1 88.916 (89.042)	Acc@5 99.512 (99.661)
Epoch: [64][21/25]	Time 0.714 (0.704)	Data 0.004 (0.041)	Loss 0.6028 (0.5839)	Acc@1 88.965 (89.038)	Acc@5 99.219 (99.640)
Epoch: [64][22/25]	Time 0.697 (0.704)	Data 0.004 (0.039)	Loss 0.6045 (0.5848)	Acc@1 88.525 (89.016)	Acc@5 99.463 (99.633)
Epoch: [64][23/25]	Time 0.711 (0.704)	Data 0.004 (0.038)	Loss 0.5790 (0.5846)	Acc@1 89.062 (89.018)	Acc@5 99.805 (99.640)
Epoch: [64][24/25]	Time 0.427 (0.693)	Data 0.004 (0.036)	Loss 0.5910 (0.5847)	Acc@1 89.151 (89.020)	Acc@5 99.528 (99.638)

Epoch: [65 | 180] LR: 0.100000
Epoch: [65][0/25]	Time 0.751 (0.751)	Data 0.672 (0.672)	Loss 0.5684 (0.5684)	Acc@1 89.600 (89.600)	Acc@5 99.609 (99.609)
Epoch: [65][1/25]	Time 0.717 (0.734)	Data 0.003 (0.337)	Loss 0.5907 (0.5795)	Acc@1 89.258 (89.429)	Acc@5 99.365 (99.487)
Epoch: [65][2/25]	Time 0.677 (0.715)	Data 0.005 (0.227)	Loss 0.5613 (0.5734)	Acc@1 90.137 (89.665)	Acc@5 99.609 (99.528)
Epoch: [65][3/25]	Time 0.760 (0.726)	Data 0.005 (0.171)	Loss 0.5801 (0.5751)	Acc@1 89.160 (89.539)	Acc@5 99.951 (99.634)
Epoch: [65][4/25]	Time 0.723 (0.726)	Data 0.008 (0.139)	Loss 0.5752 (0.5751)	Acc@1 89.307 (89.492)	Acc@5 99.561 (99.619)
Epoch: [65][5/25]	Time 0.701 (0.722)	Data 0.003 (0.116)	Loss 0.5699 (0.5743)	Acc@1 89.502 (89.494)	Acc@5 99.805 (99.650)
Epoch: [65][6/25]	Time 0.707 (0.720)	Data 0.005 (0.100)	Loss 0.5869 (0.5761)	Acc@1 89.014 (89.425)	Acc@5 99.756 (99.665)
Epoch: [65][7/25]	Time 0.741 (0.722)	Data 0.005 (0.088)	Loss 0.5789 (0.5764)	Acc@1 89.404 (89.423)	Acc@5 99.609 (99.658)
Epoch: [65][8/25]	Time 0.748 (0.725)	Data 0.005 (0.079)	Loss 0.6012 (0.5792)	Acc@1 88.184 (89.285)	Acc@5 99.756 (99.669)
Epoch: [65][9/25]	Time 0.716 (0.724)	Data 0.004 (0.072)	Loss 0.5270 (0.5740)	Acc@1 91.016 (89.458)	Acc@5 99.805 (99.683)
Epoch: [65][10/25]	Time 0.748 (0.726)	Data 0.004 (0.065)	Loss 0.5614 (0.5728)	Acc@1 89.404 (89.453)	Acc@5 99.805 (99.694)
Epoch: [65][11/25]	Time 0.739 (0.727)	Data 0.005 (0.060)	Loss 0.5723 (0.5728)	Acc@1 89.307 (89.441)	Acc@5 99.902 (99.711)
Epoch: [65][12/25]	Time 0.768 (0.731)	Data 0.006 (0.056)	Loss 0.6010 (0.5749)	Acc@1 88.135 (89.340)	Acc@5 99.756 (99.715)
Epoch: [65][13/25]	Time 0.679 (0.727)	Data 0.005 (0.053)	Loss 0.5560 (0.5736)	Acc@1 89.893 (89.380)	Acc@5 99.707 (99.714)
Epoch: [65][14/25]	Time 0.740 (0.728)	Data 0.007 (0.050)	Loss 0.5767 (0.5738)	Acc@1 88.916 (89.349)	Acc@5 99.609 (99.707)
Epoch: [65][15/25]	Time 0.775 (0.731)	Data 0.008 (0.047)	Loss 0.5564 (0.5727)	Acc@1 89.795 (89.377)	Acc@5 99.609 (99.701)
Epoch: [65][16/25]	Time 0.675 (0.727)	Data 0.006 (0.044)	Loss 0.5934 (0.5739)	Acc@1 88.184 (89.307)	Acc@5 99.805 (99.707)
Epoch: [65][17/25]	Time 0.611 (0.721)	Data 0.005 (0.042)	Loss 0.5711 (0.5738)	Acc@1 89.111 (89.296)	Acc@5 99.707 (99.707)
Epoch: [65][18/25]	Time 0.614 (0.715)	Data 0.008 (0.040)	Loss 0.5768 (0.5739)	Acc@1 88.623 (89.260)	Acc@5 99.609 (99.702)
Epoch: [65][19/25]	Time 0.691 (0.714)	Data 0.009 (0.039)	Loss 0.5954 (0.5750)	Acc@1 88.184 (89.207)	Acc@5 99.609 (99.697)
Epoch: [65][20/25]	Time 0.765 (0.716)	Data 0.006 (0.037)	Loss 0.6018 (0.5763)	Acc@1 87.891 (89.144)	Acc@5 99.658 (99.695)
Epoch: [65][21/25]	Time 0.736 (0.717)	Data 0.005 (0.036)	Loss 0.5908 (0.5769)	Acc@1 88.770 (89.127)	Acc@5 99.512 (99.687)
Epoch: [65][22/25]	Time 0.766 (0.719)	Data 0.005 (0.035)	Loss 0.5780 (0.5770)	Acc@1 88.525 (89.101)	Acc@5 99.658 (99.686)
Epoch: [65][23/25]	Time 0.699 (0.719)	Data 0.008 (0.033)	Loss 0.5729 (0.5768)	Acc@1 88.818 (89.089)	Acc@5 99.561 (99.681)
Epoch: [65][24/25]	Time 0.378 (0.705)	Data 0.005 (0.032)	Loss 0.5971 (0.5772)	Acc@1 87.500 (89.062)	Acc@5 100.000 (99.686)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(21, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 461374 ; 487386 ; 0.9466295708124567

Epoch: [66 | 180] LR: 0.100000
Epoch: [66][0/25]	Time 0.731 (0.731)	Data 0.637 (0.637)	Loss 0.5775 (0.5775)	Acc@1 89.648 (89.648)	Acc@5 99.707 (99.707)
Epoch: [66][1/25]	Time 0.716 (0.723)	Data 0.007 (0.322)	Loss 0.5407 (0.5591)	Acc@1 90.918 (90.283)	Acc@5 99.512 (99.609)
Epoch: [66][2/25]	Time 0.736 (0.728)	Data 0.007 (0.217)	Loss 0.4811 (0.5331)	Acc@1 92.432 (90.999)	Acc@5 99.902 (99.707)
Epoch: [66][3/25]	Time 0.728 (0.728)	Data 0.005 (0.164)	Loss 0.5173 (0.5291)	Acc@1 91.357 (91.089)	Acc@5 99.707 (99.707)
Epoch: [66][4/25]	Time 0.730 (0.728)	Data 0.007 (0.132)	Loss 0.5137 (0.5260)	Acc@1 91.553 (91.182)	Acc@5 99.707 (99.707)
Epoch: [66][5/25]	Time 0.702 (0.724)	Data 0.006 (0.111)	Loss 0.5178 (0.5247)	Acc@1 91.406 (91.219)	Acc@5 99.902 (99.740)
Epoch: [66][6/25]	Time 0.700 (0.720)	Data 0.006 (0.096)	Loss 0.5129 (0.5230)	Acc@1 91.455 (91.253)	Acc@5 99.707 (99.735)
Epoch: [66][7/25]	Time 0.733 (0.722)	Data 0.004 (0.085)	Loss 0.5060 (0.5209)	Acc@1 91.602 (91.296)	Acc@5 99.805 (99.744)
Epoch: [66][8/25]	Time 0.709 (0.720)	Data 0.008 (0.076)	Loss 0.5293 (0.5218)	Acc@1 90.967 (91.260)	Acc@5 99.805 (99.750)
Epoch: [66][9/25]	Time 0.753 (0.724)	Data 0.005 (0.069)	Loss 0.4824 (0.5179)	Acc@1 92.139 (91.348)	Acc@5 99.951 (99.771)
Epoch: [66][10/25]	Time 0.720 (0.723)	Data 0.005 (0.063)	Loss 0.5140 (0.5175)	Acc@1 91.016 (91.317)	Acc@5 99.902 (99.782)
Epoch: [66][11/25]	Time 0.739 (0.725)	Data 0.008 (0.059)	Loss 0.5182 (0.5176)	Acc@1 90.625 (91.260)	Acc@5 99.707 (99.776)
Epoch: [66][12/25]	Time 0.682 (0.721)	Data 0.005 (0.055)	Loss 0.5327 (0.5187)	Acc@1 90.527 (91.203)	Acc@5 99.805 (99.778)
Epoch: [66][13/25]	Time 0.680 (0.718)	Data 0.004 (0.051)	Loss 0.5451 (0.5206)	Acc@1 90.430 (91.148)	Acc@5 99.756 (99.777)
Epoch: [66][14/25]	Time 0.739 (0.720)	Data 0.007 (0.048)	Loss 0.5173 (0.5204)	Acc@1 90.625 (91.113)	Acc@5 99.756 (99.775)
Epoch: [66][15/25]	Time 0.744 (0.721)	Data 0.007 (0.045)	Loss 0.5210 (0.5204)	Acc@1 91.016 (91.107)	Acc@5 99.609 (99.765)
Epoch: [66][16/25]	Time 0.663 (0.718)	Data 0.004 (0.043)	Loss 0.5626 (0.5229)	Acc@1 89.453 (91.010)	Acc@5 99.707 (99.762)
Epoch: [66][17/25]	Time 0.665 (0.715)	Data 0.008 (0.041)	Loss 0.5085 (0.5221)	Acc@1 91.992 (91.064)	Acc@5 99.512 (99.748)
Epoch: [66][18/25]	Time 0.613 (0.710)	Data 0.004 (0.039)	Loss 0.5342 (0.5227)	Acc@1 90.674 (91.044)	Acc@5 99.756 (99.748)
Epoch: [66][19/25]	Time 0.614 (0.705)	Data 0.008 (0.038)	Loss 0.5221 (0.5227)	Acc@1 90.967 (91.040)	Acc@5 99.805 (99.751)
Epoch: [66][20/25]	Time 0.629 (0.701)	Data 0.004 (0.036)	Loss 0.5398 (0.5235)	Acc@1 90.869 (91.032)	Acc@5 99.902 (99.758)
Epoch: [66][21/25]	Time 0.628 (0.698)	Data 0.004 (0.035)	Loss 0.5539 (0.5249)	Acc@1 90.234 (90.996)	Acc@5 99.902 (99.765)
Epoch: [66][22/25]	Time 0.634 (0.695)	Data 0.004 (0.033)	Loss 0.5214 (0.5247)	Acc@1 91.357 (91.011)	Acc@5 99.756 (99.764)
Epoch: [66][23/25]	Time 0.606 (0.691)	Data 0.006 (0.032)	Loss 0.5655 (0.5264)	Acc@1 89.307 (90.940)	Acc@5 99.854 (99.768)
Epoch: [66][24/25]	Time 0.355 (0.678)	Data 0.006 (0.031)	Loss 0.5496 (0.5268)	Acc@1 90.094 (90.926)	Acc@5 99.646 (99.766)

Epoch: [67 | 180] LR: 0.100000
Epoch: [67][0/25]	Time 0.740 (0.740)	Data 0.641 (0.641)	Loss 0.5148 (0.5148)	Acc@1 91.162 (91.162)	Acc@5 99.854 (99.854)
Epoch: [67][1/25]	Time 0.638 (0.689)	Data 0.005 (0.323)	Loss 0.5718 (0.5433)	Acc@1 89.258 (90.210)	Acc@5 99.707 (99.780)
Epoch: [67][2/25]	Time 0.609 (0.663)	Data 0.005 (0.217)	Loss 0.5902 (0.5589)	Acc@1 88.574 (89.665)	Acc@5 99.707 (99.756)
Epoch: [67][3/25]	Time 0.589 (0.644)	Data 0.005 (0.164)	Loss 0.5571 (0.5585)	Acc@1 89.600 (89.648)	Acc@5 99.951 (99.805)
Epoch: [67][4/25]	Time 0.608 (0.637)	Data 0.004 (0.132)	Loss 0.5391 (0.5546)	Acc@1 90.430 (89.805)	Acc@5 99.561 (99.756)
Epoch: [67][5/25]	Time 0.612 (0.633)	Data 0.012 (0.112)	Loss 0.5497 (0.5538)	Acc@1 89.893 (89.819)	Acc@5 99.805 (99.764)
Epoch: [67][6/25]	Time 0.632 (0.633)	Data 0.003 (0.096)	Loss 0.5540 (0.5538)	Acc@1 89.697 (89.802)	Acc@5 99.658 (99.749)
Epoch: [67][7/25]	Time 0.658 (0.636)	Data 0.003 (0.085)	Loss 0.5571 (0.5542)	Acc@1 90.332 (89.868)	Acc@5 99.805 (99.756)
Epoch: [67][8/25]	Time 0.665 (0.639)	Data 0.005 (0.076)	Loss 0.5479 (0.5535)	Acc@1 89.648 (89.844)	Acc@5 99.756 (99.756)
Epoch: [67][9/25]	Time 0.661 (0.641)	Data 0.007 (0.069)	Loss 0.5718 (0.5553)	Acc@1 89.014 (89.761)	Acc@5 99.609 (99.741)
Epoch: [67][10/25]	Time 0.628 (0.640)	Data 0.007 (0.063)	Loss 0.5600 (0.5558)	Acc@1 89.600 (89.746)	Acc@5 99.658 (99.734)
Epoch: [67][11/25]	Time 0.594 (0.636)	Data 0.005 (0.058)	Loss 0.5777 (0.5576)	Acc@1 88.379 (89.632)	Acc@5 99.756 (99.736)
Epoch: [67][12/25]	Time 0.644 (0.637)	Data 0.004 (0.054)	Loss 0.5519 (0.5572)	Acc@1 89.697 (89.637)	Acc@5 99.805 (99.741)
Epoch: [67][13/25]	Time 0.601 (0.634)	Data 0.005 (0.051)	Loss 0.5152 (0.5542)	Acc@1 91.113 (89.743)	Acc@5 99.707 (99.738)
Epoch: [67][14/25]	Time 0.610 (0.633)	Data 0.006 (0.048)	Loss 0.5480 (0.5538)	Acc@1 90.186 (89.772)	Acc@5 99.512 (99.723)
Epoch: [67][15/25]	Time 0.684 (0.636)	Data 0.007 (0.045)	Loss 0.5455 (0.5532)	Acc@1 89.941 (89.783)	Acc@5 99.902 (99.734)
Epoch: [67][16/25]	Time 0.690 (0.639)	Data 0.007 (0.043)	Loss 0.5673 (0.5541)	Acc@1 88.916 (89.732)	Acc@5 99.658 (99.730)
Epoch: [67][17/25]	Time 0.742 (0.645)	Data 0.004 (0.041)	Loss 0.5797 (0.5555)	Acc@1 89.209 (89.703)	Acc@5 99.609 (99.723)
Epoch: [67][18/25]	Time 0.732 (0.649)	Data 0.004 (0.039)	Loss 0.5736 (0.5564)	Acc@1 89.453 (89.690)	Acc@5 99.561 (99.715)
Epoch: [67][19/25]	Time 0.718 (0.653)	Data 0.004 (0.037)	Loss 0.6041 (0.5588)	Acc@1 88.135 (89.612)	Acc@5 99.609 (99.709)
Epoch: [67][20/25]	Time 0.727 (0.656)	Data 0.004 (0.036)	Loss 0.5420 (0.5580)	Acc@1 89.697 (89.616)	Acc@5 99.902 (99.719)
Epoch: [67][21/25]	Time 0.679 (0.657)	Data 0.004 (0.034)	Loss 0.5630 (0.5582)	Acc@1 89.746 (89.622)	Acc@5 99.658 (99.716)
Epoch: [67][22/25]	Time 0.668 (0.658)	Data 0.004 (0.033)	Loss 0.5577 (0.5582)	Acc@1 90.039 (89.640)	Acc@5 99.854 (99.722)
Epoch: [67][23/25]	Time 0.658 (0.658)	Data 0.005 (0.032)	Loss 0.5931 (0.5597)	Acc@1 88.135 (89.577)	Acc@5 99.707 (99.721)
Epoch: [67][24/25]	Time 0.363 (0.646)	Data 0.004 (0.031)	Loss 0.5774 (0.5600)	Acc@1 88.561 (89.560)	Acc@5 99.646 (99.720)

Epoch: [68 | 180] LR: 0.100000
Epoch: [68][0/25]	Time 0.750 (0.750)	Data 0.803 (0.803)	Loss 0.5627 (0.5627)	Acc@1 89.209 (89.209)	Acc@5 99.707 (99.707)
Epoch: [68][1/25]	Time 0.727 (0.739)	Data 0.003 (0.403)	Loss 0.5616 (0.5622)	Acc@1 89.697 (89.453)	Acc@5 99.609 (99.658)
Epoch: [68][2/25]	Time 0.696 (0.725)	Data 0.007 (0.271)	Loss 0.5234 (0.5493)	Acc@1 91.650 (90.186)	Acc@5 99.609 (99.642)
Epoch: [68][3/25]	Time 0.738 (0.728)	Data 0.005 (0.205)	Loss 0.5635 (0.5528)	Acc@1 89.697 (90.063)	Acc@5 99.805 (99.683)
Epoch: [68][4/25]	Time 0.738 (0.730)	Data 0.006 (0.165)	Loss 0.5571 (0.5537)	Acc@1 89.844 (90.020)	Acc@5 99.854 (99.717)
Epoch: [68][5/25]	Time 0.708 (0.726)	Data 0.007 (0.139)	Loss 0.5543 (0.5538)	Acc@1 89.551 (89.941)	Acc@5 99.658 (99.707)
Epoch: [68][6/25]	Time 0.721 (0.726)	Data 0.006 (0.120)	Loss 0.5950 (0.5597)	Acc@1 88.525 (89.739)	Acc@5 99.365 (99.658)
Epoch: [68][7/25]	Time 0.772 (0.731)	Data 0.004 (0.105)	Loss 0.5877 (0.5632)	Acc@1 88.477 (89.581)	Acc@5 99.512 (99.640)
Epoch: [68][8/25]	Time 0.725 (0.731)	Data 0.006 (0.094)	Loss 0.5636 (0.5632)	Acc@1 89.404 (89.562)	Acc@5 99.805 (99.658)
Epoch: [68][9/25]	Time 0.682 (0.726)	Data 0.006 (0.085)	Loss 0.5582 (0.5627)	Acc@1 89.844 (89.590)	Acc@5 99.854 (99.678)
Epoch: [68][10/25]	Time 0.749 (0.728)	Data 0.005 (0.078)	Loss 0.5732 (0.5637)	Acc@1 89.502 (89.582)	Acc@5 99.365 (99.649)
Epoch: [68][11/25]	Time 0.748 (0.730)	Data 0.005 (0.072)	Loss 0.5386 (0.5616)	Acc@1 90.674 (89.673)	Acc@5 99.609 (99.646)
Epoch: [68][12/25]	Time 0.719 (0.729)	Data 0.004 (0.067)	Loss 0.5828 (0.5632)	Acc@1 87.891 (89.536)	Acc@5 99.756 (99.654)
Epoch: [68][13/25]	Time 0.683 (0.726)	Data 0.006 (0.062)	Loss 0.5666 (0.5635)	Acc@1 89.844 (89.558)	Acc@5 99.707 (99.658)
Epoch: [68][14/25]	Time 0.715 (0.725)	Data 0.008 (0.059)	Loss 0.5440 (0.5622)	Acc@1 89.355 (89.544)	Acc@5 99.902 (99.674)
Epoch: [68][15/25]	Time 0.741 (0.726)	Data 0.005 (0.055)	Loss 0.5525 (0.5616)	Acc@1 89.648 (89.551)	Acc@5 99.805 (99.683)
Epoch: [68][16/25]	Time 0.659 (0.722)	Data 0.005 (0.052)	Loss 0.5753 (0.5624)	Acc@1 89.600 (89.554)	Acc@5 99.463 (99.670)
Epoch: [68][17/25]	Time 0.635 (0.717)	Data 0.005 (0.050)	Loss 0.5544 (0.5619)	Acc@1 89.893 (89.572)	Acc@5 99.756 (99.674)
Epoch: [68][18/25]	Time 0.680 (0.715)	Data 0.004 (0.047)	Loss 0.5692 (0.5623)	Acc@1 89.160 (89.551)	Acc@5 99.707 (99.676)
Epoch: [68][19/25]	Time 0.704 (0.715)	Data 0.004 (0.045)	Loss 0.5640 (0.5624)	Acc@1 89.697 (89.558)	Acc@5 99.414 (99.663)
Epoch: [68][20/25]	Time 0.727 (0.715)	Data 0.005 (0.043)	Loss 0.6006 (0.5642)	Acc@1 88.770 (89.521)	Acc@5 99.658 (99.663)
Epoch: [68][21/25]	Time 0.729 (0.716)	Data 0.005 (0.041)	Loss 0.5616 (0.5641)	Acc@1 89.404 (89.515)	Acc@5 99.756 (99.667)
Epoch: [68][22/25]	Time 0.745 (0.717)	Data 0.005 (0.040)	Loss 0.5848 (0.5650)	Acc@1 89.307 (89.506)	Acc@5 99.463 (99.658)
Epoch: [68][23/25]	Time 0.731 (0.718)	Data 0.006 (0.038)	Loss 0.5807 (0.5656)	Acc@1 89.551 (89.508)	Acc@5 99.805 (99.664)
Epoch: [68][24/25]	Time 0.438 (0.706)	Data 0.006 (0.037)	Loss 0.5804 (0.5659)	Acc@1 88.915 (89.498)	Acc@5 99.882 (99.668)

Epoch: [69 | 180] LR: 0.100000
Epoch: [69][0/25]	Time 0.746 (0.746)	Data 0.609 (0.609)	Loss 0.5507 (0.5507)	Acc@1 90.039 (90.039)	Acc@5 99.756 (99.756)
Epoch: [69][1/25]	Time 0.689 (0.718)	Data 0.007 (0.308)	Loss 0.5660 (0.5584)	Acc@1 90.137 (90.088)	Acc@5 99.268 (99.512)
Epoch: [69][2/25]	Time 0.761 (0.732)	Data 0.007 (0.207)	Loss 0.5632 (0.5600)	Acc@1 88.428 (89.535)	Acc@5 99.951 (99.658)
Epoch: [69][3/25]	Time 0.751 (0.737)	Data 0.005 (0.157)	Loss 0.5568 (0.5592)	Acc@1 89.697 (89.575)	Acc@5 99.756 (99.683)
Epoch: [69][4/25]	Time 0.667 (0.723)	Data 0.006 (0.127)	Loss 0.5296 (0.5533)	Acc@1 90.918 (89.844)	Acc@5 99.756 (99.697)
Epoch: [69][5/25]	Time 0.643 (0.710)	Data 0.005 (0.106)	Loss 0.5492 (0.5526)	Acc@1 89.746 (89.827)	Acc@5 99.805 (99.715)
Epoch: [69][6/25]	Time 0.615 (0.696)	Data 0.007 (0.092)	Loss 0.5695 (0.5550)	Acc@1 89.697 (89.809)	Acc@5 99.756 (99.721)
Epoch: [69][7/25]	Time 0.667 (0.693)	Data 0.005 (0.081)	Loss 0.5811 (0.5583)	Acc@1 88.379 (89.630)	Acc@5 99.756 (99.725)
Epoch: [69][8/25]	Time 0.712 (0.695)	Data 0.006 (0.073)	Loss 0.5524 (0.5576)	Acc@1 90.625 (89.741)	Acc@5 99.561 (99.707)
Epoch: [69][9/25]	Time 0.743 (0.700)	Data 0.005 (0.066)	Loss 0.5584 (0.5577)	Acc@1 89.453 (89.712)	Acc@5 99.609 (99.697)
Epoch: [69][10/25]	Time 0.759 (0.705)	Data 0.004 (0.061)	Loss 0.5562 (0.5576)	Acc@1 89.941 (89.733)	Acc@5 99.609 (99.689)
Epoch: [69][11/25]	Time 0.692 (0.704)	Data 0.008 (0.056)	Loss 0.5476 (0.5567)	Acc@1 89.893 (89.746)	Acc@5 99.609 (99.683)
Epoch: [69][12/25]	Time 0.728 (0.706)	Data 0.007 (0.052)	Loss 0.5491 (0.5562)	Acc@1 90.137 (89.776)	Acc@5 99.658 (99.681)
Epoch: [69][13/25]	Time 0.732 (0.708)	Data 0.004 (0.049)	Loss 0.5629 (0.5566)	Acc@1 88.574 (89.690)	Acc@5 99.854 (99.693)
Epoch: [69][14/25]	Time 0.762 (0.711)	Data 0.005 (0.046)	Loss 0.5474 (0.5560)	Acc@1 89.648 (89.688)	Acc@5 99.707 (99.694)
Epoch: [69][15/25]	Time 0.680 (0.709)	Data 0.005 (0.043)	Loss 0.5692 (0.5568)	Acc@1 89.648 (89.685)	Acc@5 99.902 (99.707)
Epoch: [69][16/25]	Time 0.715 (0.710)	Data 0.006 (0.041)	Loss 0.5803 (0.5582)	Acc@1 88.770 (89.631)	Acc@5 99.658 (99.704)
Epoch: [69][17/25]	Time 0.731 (0.711)	Data 0.004 (0.039)	Loss 0.5822 (0.5596)	Acc@1 89.209 (89.608)	Acc@5 99.805 (99.710)
Epoch: [69][18/25]	Time 0.748 (0.713)	Data 0.004 (0.037)	Loss 0.5735 (0.5603)	Acc@1 89.062 (89.579)	Acc@5 99.658 (99.707)
Epoch: [69][19/25]	Time 0.661 (0.710)	Data 0.004 (0.036)	Loss 0.6287 (0.5637)	Acc@1 87.012 (89.451)	Acc@5 99.414 (99.692)
Epoch: [69][20/25]	Time 0.713 (0.710)	Data 0.006 (0.034)	Loss 0.5629 (0.5637)	Acc@1 90.039 (89.479)	Acc@5 99.756 (99.695)
Epoch: [69][21/25]	Time 0.770 (0.713)	Data 0.007 (0.033)	Loss 0.5858 (0.5647)	Acc@1 88.965 (89.455)	Acc@5 99.756 (99.698)
Epoch: [69][22/25]	Time 0.676 (0.711)	Data 0.006 (0.032)	Loss 0.5536 (0.5642)	Acc@1 89.844 (89.472)	Acc@5 99.707 (99.699)
Epoch: [69][23/25]	Time 0.637 (0.708)	Data 0.007 (0.031)	Loss 0.5676 (0.5643)	Acc@1 89.746 (89.484)	Acc@5 99.609 (99.695)
Epoch: [69][24/25]	Time 0.342 (0.694)	Data 0.007 (0.030)	Loss 0.5898 (0.5648)	Acc@1 88.797 (89.472)	Acc@5 99.646 (99.694)

Epoch: [70 | 180] LR: 0.100000
Epoch: [70][0/25]	Time 0.748 (0.748)	Data 0.632 (0.632)	Loss 0.5367 (0.5367)	Acc@1 90.039 (90.039)	Acc@5 99.756 (99.756)
Epoch: [70][1/25]	Time 0.855 (0.802)	Data 0.010 (0.321)	Loss 0.5716 (0.5542)	Acc@1 89.697 (89.868)	Acc@5 99.805 (99.780)
Epoch: [70][2/25]	Time 0.815 (0.806)	Data 0.005 (0.215)	Loss 0.5678 (0.5587)	Acc@1 89.648 (89.795)	Acc@5 99.658 (99.740)
Epoch: [70][3/25]	Time 0.758 (0.794)	Data 0.005 (0.163)	Loss 0.5551 (0.5578)	Acc@1 89.502 (89.722)	Acc@5 99.805 (99.756)
Epoch: [70][4/25]	Time 0.774 (0.790)	Data 0.006 (0.131)	Loss 0.5471 (0.5557)	Acc@1 90.771 (89.932)	Acc@5 99.707 (99.746)
Epoch: [70][5/25]	Time 0.682 (0.772)	Data 0.008 (0.111)	Loss 0.5292 (0.5513)	Acc@1 91.016 (90.112)	Acc@5 99.756 (99.748)
Epoch: [70][6/25]	Time 0.696 (0.761)	Data 0.006 (0.096)	Loss 0.5651 (0.5532)	Acc@1 89.893 (90.081)	Acc@5 99.756 (99.749)
Epoch: [70][7/25]	Time 0.688 (0.752)	Data 0.005 (0.084)	Loss 0.5732 (0.5557)	Acc@1 89.551 (90.015)	Acc@5 99.854 (99.762)
Epoch: [70][8/25]	Time 0.740 (0.751)	Data 0.006 (0.076)	Loss 0.5610 (0.5563)	Acc@1 89.893 (90.001)	Acc@5 99.756 (99.761)
Epoch: [70][9/25]	Time 0.716 (0.747)	Data 0.004 (0.069)	Loss 0.5899 (0.5597)	Acc@1 88.867 (89.888)	Acc@5 99.609 (99.746)
Epoch: [70][10/25]	Time 0.702 (0.743)	Data 0.008 (0.063)	Loss 0.6031 (0.5636)	Acc@1 87.744 (89.693)	Acc@5 99.854 (99.756)
Epoch: [70][11/25]	Time 0.716 (0.741)	Data 0.005 (0.058)	Loss 0.5465 (0.5622)	Acc@1 89.844 (89.705)	Acc@5 99.854 (99.764)
Epoch: [70][12/25]	Time 0.694 (0.737)	Data 0.006 (0.054)	Loss 0.5540 (0.5616)	Acc@1 90.332 (89.754)	Acc@5 99.658 (99.756)
Epoch: [70][13/25]	Time 0.753 (0.738)	Data 0.006 (0.051)	Loss 0.6103 (0.5650)	Acc@1 88.184 (89.641)	Acc@5 99.414 (99.731)
Epoch: [70][14/25]	Time 0.692 (0.735)	Data 0.007 (0.048)	Loss 0.5752 (0.5657)	Acc@1 89.014 (89.600)	Acc@5 99.561 (99.720)
Epoch: [70][15/25]	Time 0.664 (0.731)	Data 0.005 (0.045)	Loss 0.5785 (0.5665)	Acc@1 89.258 (89.578)	Acc@5 99.463 (99.704)
Epoch: [70][16/25]	Time 0.655 (0.726)	Data 0.008 (0.043)	Loss 0.5865 (0.5677)	Acc@1 88.379 (89.508)	Acc@5 99.805 (99.710)
Epoch: [70][17/25]	Time 0.742 (0.727)	Data 0.006 (0.041)	Loss 0.5934 (0.5691)	Acc@1 88.379 (89.445)	Acc@5 99.658 (99.707)
Epoch: [70][18/25]	Time 0.727 (0.727)	Data 0.004 (0.039)	Loss 0.5761 (0.5695)	Acc@1 89.307 (89.438)	Acc@5 99.707 (99.707)
Epoch: [70][19/25]	Time 0.754 (0.729)	Data 0.007 (0.037)	Loss 0.5585 (0.5689)	Acc@1 90.576 (89.495)	Acc@5 99.414 (99.692)
Epoch: [70][20/25]	Time 0.737 (0.729)	Data 0.004 (0.036)	Loss 0.5881 (0.5699)	Acc@1 88.965 (89.469)	Acc@5 99.463 (99.681)
Epoch: [70][21/25]	Time 0.692 (0.727)	Data 0.004 (0.034)	Loss 0.5407 (0.5685)	Acc@1 89.990 (89.493)	Acc@5 99.707 (99.683)
Epoch: [70][22/25]	Time 0.766 (0.729)	Data 0.007 (0.033)	Loss 0.5493 (0.5677)	Acc@1 89.307 (89.485)	Acc@5 99.512 (99.675)
Epoch: [70][23/25]	Time 0.742 (0.730)	Data 0.007 (0.032)	Loss 0.5461 (0.5668)	Acc@1 89.795 (89.498)	Acc@5 99.707 (99.677)
Epoch: [70][24/25]	Time 0.437 (0.718)	Data 0.005 (0.031)	Loss 0.5417 (0.5664)	Acc@1 89.269 (89.494)	Acc@5 99.528 (99.674)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(19, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 458772 ; 487386 ; 0.941290886484224

Epoch: [71 | 180] LR: 0.100000
Epoch: [71][0/25]	Time 0.733 (0.733)	Data 0.650 (0.650)	Loss 0.5912 (0.5912)	Acc@1 87.842 (87.842)	Acc@5 99.512 (99.512)
Epoch: [71][1/25]	Time 0.679 (0.706)	Data 0.006 (0.328)	Loss 0.5197 (0.5555)	Acc@1 90.820 (89.331)	Acc@5 99.658 (99.585)
Epoch: [71][2/25]	Time 0.690 (0.701)	Data 0.003 (0.220)	Loss 0.5056 (0.5388)	Acc@1 91.748 (90.137)	Acc@5 99.854 (99.674)
Epoch: [71][3/25]	Time 0.691 (0.698)	Data 0.003 (0.166)	Loss 0.4913 (0.5270)	Acc@1 91.943 (90.588)	Acc@5 99.951 (99.744)
Epoch: [71][4/25]	Time 0.718 (0.702)	Data 0.006 (0.134)	Loss 0.4812 (0.5178)	Acc@1 92.432 (90.957)	Acc@5 99.854 (99.766)
Epoch: [71][5/25]	Time 0.675 (0.698)	Data 0.006 (0.112)	Loss 0.5110 (0.5167)	Acc@1 91.406 (91.032)	Acc@5 99.707 (99.756)
Epoch: [71][6/25]	Time 0.671 (0.694)	Data 0.005 (0.097)	Loss 0.4941 (0.5134)	Acc@1 92.188 (91.197)	Acc@5 99.854 (99.770)
Epoch: [71][7/25]	Time 0.721 (0.697)	Data 0.006 (0.086)	Loss 0.5178 (0.5140)	Acc@1 90.479 (91.107)	Acc@5 99.756 (99.768)
Epoch: [71][8/25]	Time 0.732 (0.701)	Data 0.005 (0.077)	Loss 0.5182 (0.5145)	Acc@1 90.576 (91.048)	Acc@5 99.756 (99.767)
Epoch: [71][9/25]	Time 0.725 (0.704)	Data 0.005 (0.070)	Loss 0.4797 (0.5110)	Acc@1 93.066 (91.250)	Acc@5 99.951 (99.785)
Epoch: [71][10/25]	Time 0.702 (0.704)	Data 0.006 (0.064)	Loss 0.4965 (0.5097)	Acc@1 91.357 (91.260)	Acc@5 99.951 (99.800)
Epoch: [71][11/25]	Time 0.645 (0.699)	Data 0.004 (0.059)	Loss 0.5153 (0.5101)	Acc@1 91.162 (91.252)	Acc@5 99.512 (99.776)
Epoch: [71][12/25]	Time 0.813 (0.707)	Data 0.007 (0.055)	Loss 0.5200 (0.5109)	Acc@1 91.455 (91.267)	Acc@5 99.707 (99.771)
Epoch: [71][13/25]	Time 0.831 (0.716)	Data 0.007 (0.051)	Loss 0.5191 (0.5115)	Acc@1 90.967 (91.246)	Acc@5 99.707 (99.766)
Epoch: [71][14/25]	Time 0.674 (0.713)	Data 0.007 (0.048)	Loss 0.5053 (0.5111)	Acc@1 91.406 (91.257)	Acc@5 99.854 (99.772)
Epoch: [71][15/25]	Time 0.720 (0.714)	Data 0.005 (0.046)	Loss 0.5424 (0.5130)	Acc@1 90.723 (91.223)	Acc@5 99.756 (99.771)
Epoch: [71][16/25]	Time 0.684 (0.712)	Data 0.008 (0.043)	Loss 0.5183 (0.5133)	Acc@1 90.332 (91.171)	Acc@5 99.756 (99.770)
Epoch: [71][17/25]	Time 0.741 (0.714)	Data 0.006 (0.041)	Loss 0.5271 (0.5141)	Acc@1 90.430 (91.130)	Acc@5 99.805 (99.772)
Epoch: [71][18/25]	Time 0.731 (0.715)	Data 0.006 (0.040)	Loss 0.5169 (0.5143)	Acc@1 91.064 (91.126)	Acc@5 99.756 (99.771)
Epoch: [71][19/25]	Time 0.702 (0.714)	Data 0.005 (0.038)	Loss 0.5278 (0.5149)	Acc@1 91.016 (91.121)	Acc@5 99.658 (99.766)
Epoch: [71][20/25]	Time 0.702 (0.713)	Data 0.006 (0.036)	Loss 0.5501 (0.5166)	Acc@1 90.039 (91.069)	Acc@5 99.658 (99.761)
Epoch: [71][21/25]	Time 0.735 (0.714)	Data 0.005 (0.035)	Loss 0.5309 (0.5173)	Acc@1 90.527 (91.044)	Acc@5 99.854 (99.765)
Epoch: [71][22/25]	Time 0.738 (0.715)	Data 0.006 (0.034)	Loss 0.5328 (0.5179)	Acc@1 90.674 (91.028)	Acc@5 99.561 (99.756)
Epoch: [71][23/25]	Time 0.718 (0.716)	Data 0.004 (0.032)	Loss 0.5356 (0.5187)	Acc@1 90.381 (91.001)	Acc@5 99.805 (99.758)
Epoch: [71][24/25]	Time 0.408 (0.703)	Data 0.006 (0.031)	Loss 0.5465 (0.5191)	Acc@1 90.330 (90.990)	Acc@5 99.882 (99.760)

Epoch: [72 | 180] LR: 0.100000
Epoch: [72][0/25]	Time 0.720 (0.720)	Data 0.624 (0.624)	Loss 0.5486 (0.5486)	Acc@1 90.186 (90.186)	Acc@5 99.658 (99.658)
Epoch: [72][1/25]	Time 0.719 (0.719)	Data 0.004 (0.314)	Loss 0.5376 (0.5431)	Acc@1 90.430 (90.308)	Acc@5 99.658 (99.658)
Epoch: [72][2/25]	Time 0.716 (0.718)	Data 0.003 (0.210)	Loss 0.5613 (0.5492)	Acc@1 89.893 (90.169)	Acc@5 99.805 (99.707)
Epoch: [72][3/25]	Time 0.689 (0.711)	Data 0.004 (0.159)	Loss 0.5227 (0.5425)	Acc@1 90.332 (90.210)	Acc@5 99.756 (99.719)
Epoch: [72][4/25]	Time 0.743 (0.717)	Data 0.006 (0.128)	Loss 0.5309 (0.5402)	Acc@1 90.381 (90.244)	Acc@5 99.854 (99.746)
Epoch: [72][5/25]	Time 0.727 (0.719)	Data 0.003 (0.107)	Loss 0.5512 (0.5421)	Acc@1 89.795 (90.169)	Acc@5 99.805 (99.756)
Epoch: [72][6/25]	Time 0.677 (0.713)	Data 0.005 (0.093)	Loss 0.5654 (0.5454)	Acc@1 89.307 (90.046)	Acc@5 99.756 (99.756)
Epoch: [72][7/25]	Time 0.729 (0.715)	Data 0.004 (0.082)	Loss 0.5450 (0.5453)	Acc@1 90.137 (90.057)	Acc@5 99.756 (99.756)
Epoch: [72][8/25]	Time 0.734 (0.717)	Data 0.007 (0.073)	Loss 0.5538 (0.5463)	Acc@1 89.844 (90.034)	Acc@5 99.658 (99.745)
Epoch: [72][9/25]	Time 0.657 (0.711)	Data 0.006 (0.067)	Loss 0.5213 (0.5438)	Acc@1 91.113 (90.142)	Acc@5 99.756 (99.746)
Epoch: [72][10/25]	Time 0.734 (0.713)	Data 0.007 (0.061)	Loss 0.5555 (0.5449)	Acc@1 89.355 (90.070)	Acc@5 99.561 (99.729)
Epoch: [72][11/25]	Time 0.739 (0.715)	Data 0.005 (0.057)	Loss 0.5625 (0.5463)	Acc@1 89.502 (90.023)	Acc@5 99.658 (99.723)
Epoch: [72][12/25]	Time 0.710 (0.715)	Data 0.007 (0.053)	Loss 0.5810 (0.5490)	Acc@1 89.600 (89.990)	Acc@5 99.756 (99.726)
Epoch: [72][13/25]	Time 0.703 (0.714)	Data 0.004 (0.049)	Loss 0.5805 (0.5512)	Acc@1 88.867 (89.910)	Acc@5 99.658 (99.721)
Epoch: [72][14/25]	Time 0.690 (0.712)	Data 0.006 (0.046)	Loss 0.5534 (0.5514)	Acc@1 89.697 (89.896)	Acc@5 99.707 (99.720)
Epoch: [72][15/25]	Time 0.760 (0.715)	Data 0.010 (0.044)	Loss 0.5839 (0.5534)	Acc@1 88.037 (89.780)	Acc@5 99.658 (99.716)
Epoch: [72][16/25]	Time 0.895 (0.726)	Data 0.007 (0.042)	Loss 0.5960 (0.5559)	Acc@1 88.428 (89.700)	Acc@5 99.365 (99.696)
Epoch: [72][17/25]	Time 0.782 (0.729)	Data 0.005 (0.040)	Loss 0.5681 (0.5566)	Acc@1 89.160 (89.670)	Acc@5 99.658 (99.693)
Epoch: [72][18/25]	Time 0.613 (0.723)	Data 0.007 (0.038)	Loss 0.5681 (0.5572)	Acc@1 89.746 (89.674)	Acc@5 99.707 (99.694)
Epoch: [72][19/25]	Time 0.623 (0.718)	Data 0.007 (0.037)	Loss 0.6082 (0.5598)	Acc@1 87.354 (89.558)	Acc@5 99.609 (99.690)
Epoch: [72][20/25]	Time 0.620 (0.713)	Data 0.006 (0.035)	Loss 0.5558 (0.5596)	Acc@1 90.088 (89.583)	Acc@5 99.561 (99.684)
Epoch: [72][21/25]	Time 0.617 (0.709)	Data 0.007 (0.034)	Loss 0.5848 (0.5607)	Acc@1 88.965 (89.555)	Acc@5 99.561 (99.678)
Epoch: [72][22/25]	Time 0.672 (0.707)	Data 0.007 (0.033)	Loss 0.5736 (0.5613)	Acc@1 89.160 (89.538)	Acc@5 99.756 (99.682)
Epoch: [72][23/25]	Time 0.619 (0.704)	Data 0.008 (0.032)	Loss 0.5718 (0.5617)	Acc@1 89.404 (89.532)	Acc@5 99.561 (99.677)
Epoch: [72][24/25]	Time 0.342 (0.689)	Data 0.004 (0.031)	Loss 0.5688 (0.5618)	Acc@1 89.623 (89.534)	Acc@5 99.764 (99.678)

Epoch: [73 | 180] LR: 0.100000
Epoch: [73][0/25]	Time 0.734 (0.734)	Data 0.664 (0.664)	Loss 0.5399 (0.5399)	Acc@1 90.771 (90.771)	Acc@5 99.756 (99.756)
Epoch: [73][1/25]	Time 0.697 (0.716)	Data 0.011 (0.338)	Loss 0.5595 (0.5497)	Acc@1 89.404 (90.088)	Acc@5 99.756 (99.756)
Epoch: [73][2/25]	Time 0.736 (0.722)	Data 0.005 (0.227)	Loss 0.5925 (0.5640)	Acc@1 87.988 (89.388)	Acc@5 99.512 (99.674)
Epoch: [73][3/25]	Time 0.724 (0.723)	Data 0.004 (0.171)	Loss 0.5632 (0.5638)	Acc@1 90.234 (89.600)	Acc@5 99.609 (99.658)
Epoch: [73][4/25]	Time 0.717 (0.722)	Data 0.007 (0.138)	Loss 0.5517 (0.5614)	Acc@1 90.186 (89.717)	Acc@5 99.707 (99.668)
Epoch: [73][5/25]	Time 0.742 (0.725)	Data 0.009 (0.117)	Loss 0.5885 (0.5659)	Acc@1 88.232 (89.469)	Acc@5 99.756 (99.683)
Epoch: [73][6/25]	Time 0.691 (0.720)	Data 0.005 (0.101)	Loss 0.5670 (0.5660)	Acc@1 89.648 (89.495)	Acc@5 99.854 (99.707)
Epoch: [73][7/25]	Time 0.696 (0.717)	Data 0.004 (0.089)	Loss 0.5669 (0.5662)	Acc@1 89.502 (89.496)	Acc@5 99.658 (99.701)
Epoch: [73][8/25]	Time 0.735 (0.719)	Data 0.007 (0.080)	Loss 0.5727 (0.5669)	Acc@1 89.404 (89.486)	Acc@5 99.805 (99.712)
Epoch: [73][9/25]	Time 0.768 (0.724)	Data 0.005 (0.072)	Loss 0.5469 (0.5649)	Acc@1 89.648 (89.502)	Acc@5 99.902 (99.731)
Epoch: [73][10/25]	Time 0.659 (0.718)	Data 0.006 (0.066)	Loss 0.5790 (0.5662)	Acc@1 89.111 (89.466)	Acc@5 99.561 (99.716)
Epoch: [73][11/25]	Time 0.648 (0.712)	Data 0.005 (0.061)	Loss 0.5555 (0.5653)	Acc@1 89.551 (89.473)	Acc@5 99.902 (99.731)
Epoch: [73][12/25]	Time 0.685 (0.710)	Data 0.007 (0.057)	Loss 0.5782 (0.5663)	Acc@1 88.916 (89.431)	Acc@5 99.561 (99.718)
Epoch: [73][13/25]	Time 0.744 (0.713)	Data 0.005 (0.053)	Loss 0.5695 (0.5665)	Acc@1 89.551 (89.439)	Acc@5 99.805 (99.724)
Epoch: [73][14/25]	Time 0.727 (0.713)	Data 0.005 (0.050)	Loss 0.5857 (0.5678)	Acc@1 88.916 (89.404)	Acc@5 99.707 (99.723)
Epoch: [73][15/25]	Time 0.740 (0.715)	Data 0.005 (0.047)	Loss 0.5844 (0.5688)	Acc@1 88.770 (89.365)	Acc@5 99.609 (99.716)
Epoch: [73][16/25]	Time 0.682 (0.713)	Data 0.009 (0.045)	Loss 0.5404 (0.5672)	Acc@1 90.137 (89.410)	Acc@5 99.756 (99.719)
Epoch: [73][17/25]	Time 0.724 (0.714)	Data 0.007 (0.043)	Loss 0.5894 (0.5684)	Acc@1 89.355 (89.407)	Acc@5 99.414 (99.702)
Epoch: [73][18/25]	Time 0.710 (0.714)	Data 0.007 (0.041)	Loss 0.5734 (0.5686)	Acc@1 90.088 (89.443)	Acc@5 99.756 (99.704)
Epoch: [73][19/25]	Time 0.682 (0.712)	Data 0.006 (0.039)	Loss 0.5453 (0.5675)	Acc@1 90.430 (89.492)	Acc@5 99.805 (99.709)
Epoch: [73][20/25]	Time 0.692 (0.711)	Data 0.004 (0.037)	Loss 0.5774 (0.5680)	Acc@1 88.965 (89.467)	Acc@5 99.365 (99.693)
Epoch: [73][21/25]	Time 0.713 (0.711)	Data 0.007 (0.036)	Loss 0.5619 (0.5677)	Acc@1 89.502 (89.469)	Acc@5 99.805 (99.698)
Epoch: [73][22/25]	Time 0.757 (0.713)	Data 0.005 (0.035)	Loss 0.6002 (0.5691)	Acc@1 88.281 (89.417)	Acc@5 99.463 (99.688)
Epoch: [73][23/25]	Time 0.674 (0.711)	Data 0.006 (0.034)	Loss 0.5859 (0.5698)	Acc@1 89.307 (89.412)	Acc@5 99.658 (99.687)
Epoch: [73][24/25]	Time 0.382 (0.698)	Data 0.006 (0.032)	Loss 0.5861 (0.5701)	Acc@1 89.269 (89.410)	Acc@5 99.646 (99.686)

Epoch: [74 | 180] LR: 0.100000
Epoch: [74][0/25]	Time 0.760 (0.760)	Data 0.604 (0.604)	Loss 0.5783 (0.5783)	Acc@1 89.062 (89.062)	Acc@5 99.658 (99.658)
Epoch: [74][1/25]	Time 0.699 (0.730)	Data 0.006 (0.305)	Loss 0.5713 (0.5748)	Acc@1 89.551 (89.307)	Acc@5 99.609 (99.634)
Epoch: [74][2/25]	Time 0.647 (0.702)	Data 0.006 (0.205)	Loss 0.5695 (0.5730)	Acc@1 88.867 (89.160)	Acc@5 99.561 (99.609)
Epoch: [74][3/25]	Time 0.635 (0.685)	Data 0.006 (0.156)	Loss 0.5730 (0.5730)	Acc@1 88.672 (89.038)	Acc@5 99.756 (99.646)
Epoch: [74][4/25]	Time 0.622 (0.673)	Data 0.006 (0.126)	Loss 0.5596 (0.5703)	Acc@1 89.600 (89.150)	Acc@5 99.756 (99.668)
Epoch: [74][5/25]	Time 0.637 (0.667)	Data 0.008 (0.106)	Loss 0.5543 (0.5677)	Acc@1 90.137 (89.315)	Acc@5 99.512 (99.642)
Epoch: [74][6/25]	Time 0.691 (0.670)	Data 0.005 (0.092)	Loss 0.5593 (0.5665)	Acc@1 89.258 (89.307)	Acc@5 99.561 (99.630)
Epoch: [74][7/25]	Time 0.710 (0.675)	Data 0.006 (0.081)	Loss 0.5513 (0.5646)	Acc@1 90.430 (89.447)	Acc@5 99.512 (99.615)
Epoch: [74][8/25]	Time 0.745 (0.683)	Data 0.004 (0.072)	Loss 0.5702 (0.5652)	Acc@1 88.867 (89.383)	Acc@5 99.658 (99.620)
Epoch: [74][9/25]	Time 0.689 (0.683)	Data 0.005 (0.066)	Loss 0.5323 (0.5619)	Acc@1 90.820 (89.526)	Acc@5 99.756 (99.634)
Epoch: [74][10/25]	Time 0.610 (0.677)	Data 0.008 (0.060)	Loss 0.5718 (0.5628)	Acc@1 88.916 (89.471)	Acc@5 99.561 (99.627)
Epoch: [74][11/25]	Time 0.630 (0.673)	Data 0.005 (0.056)	Loss 0.5694 (0.5634)	Acc@1 89.160 (89.445)	Acc@5 99.609 (99.626)
Epoch: [74][12/25]	Time 0.619 (0.669)	Data 0.008 (0.052)	Loss 0.5725 (0.5641)	Acc@1 89.355 (89.438)	Acc@5 99.609 (99.624)
Epoch: [74][13/25]	Time 0.656 (0.668)	Data 0.005 (0.049)	Loss 0.5763 (0.5649)	Acc@1 89.551 (89.446)	Acc@5 99.414 (99.609)
Epoch: [74][14/25]	Time 0.597 (0.663)	Data 0.006 (0.046)	Loss 0.5664 (0.5650)	Acc@1 88.916 (89.411)	Acc@5 99.902 (99.629)
Epoch: [74][15/25]	Time 0.646 (0.662)	Data 0.006 (0.044)	Loss 0.6017 (0.5673)	Acc@1 87.842 (89.313)	Acc@5 99.561 (99.625)
Epoch: [74][16/25]	Time 0.673 (0.663)	Data 0.005 (0.041)	Loss 0.5595 (0.5669)	Acc@1 89.111 (89.301)	Acc@5 99.805 (99.635)
Epoch: [74][17/25]	Time 0.700 (0.665)	Data 0.004 (0.039)	Loss 0.5543 (0.5662)	Acc@1 89.697 (89.323)	Acc@5 99.756 (99.642)
Epoch: [74][18/25]	Time 0.743 (0.669)	Data 0.007 (0.037)	Loss 0.5404 (0.5648)	Acc@1 90.234 (89.371)	Acc@5 99.756 (99.648)
Epoch: [74][19/25]	Time 0.733 (0.672)	Data 0.004 (0.036)	Loss 0.5327 (0.5632)	Acc@1 90.137 (89.409)	Acc@5 99.854 (99.658)
Epoch: [74][20/25]	Time 0.740 (0.675)	Data 0.004 (0.034)	Loss 0.5591 (0.5630)	Acc@1 89.795 (89.428)	Acc@5 99.854 (99.668)
Epoch: [74][21/25]	Time 0.764 (0.679)	Data 0.004 (0.033)	Loss 0.5435 (0.5621)	Acc@1 89.502 (89.431)	Acc@5 99.805 (99.674)
Epoch: [74][22/25]	Time 0.725 (0.681)	Data 0.005 (0.032)	Loss 0.5755 (0.5627)	Acc@1 89.014 (89.413)	Acc@5 99.561 (99.669)
Epoch: [74][23/25]	Time 0.739 (0.684)	Data 0.005 (0.031)	Loss 0.5683 (0.5629)	Acc@1 89.307 (89.408)	Acc@5 99.902 (99.679)
Epoch: [74][24/25]	Time 0.430 (0.674)	Data 0.004 (0.030)	Loss 0.5510 (0.5627)	Acc@1 90.566 (89.428)	Acc@5 99.646 (99.678)

Epoch: [75 | 180] LR: 0.100000
Epoch: [75][0/25]	Time 0.735 (0.735)	Data 0.604 (0.604)	Loss 0.5806 (0.5806)	Acc@1 89.160 (89.160)	Acc@5 99.658 (99.658)
Epoch: [75][1/25]	Time 0.723 (0.729)	Data 0.007 (0.305)	Loss 0.5784 (0.5795)	Acc@1 88.965 (89.062)	Acc@5 99.463 (99.561)
Epoch: [75][2/25]	Time 0.707 (0.721)	Data 0.005 (0.205)	Loss 0.5535 (0.5708)	Acc@1 90.137 (89.421)	Acc@5 99.658 (99.593)
Epoch: [75][3/25]	Time 0.727 (0.723)	Data 0.003 (0.155)	Loss 0.5507 (0.5658)	Acc@1 89.795 (89.514)	Acc@5 99.561 (99.585)
Epoch: [75][4/25]	Time 0.704 (0.719)	Data 0.006 (0.125)	Loss 0.5551 (0.5636)	Acc@1 90.186 (89.648)	Acc@5 99.414 (99.551)
Epoch: [75][5/25]	Time 0.736 (0.722)	Data 0.008 (0.105)	Loss 0.5728 (0.5652)	Acc@1 89.404 (89.608)	Acc@5 99.707 (99.577)
Epoch: [75][6/25]	Time 0.707 (0.720)	Data 0.006 (0.091)	Loss 0.5295 (0.5601)	Acc@1 90.527 (89.739)	Acc@5 99.951 (99.630)
Epoch: [75][7/25]	Time 0.683 (0.715)	Data 0.007 (0.081)	Loss 0.5570 (0.5597)	Acc@1 89.795 (89.746)	Acc@5 99.756 (99.646)
Epoch: [75][8/25]	Time 0.745 (0.718)	Data 0.009 (0.073)	Loss 0.5334 (0.5568)	Acc@1 90.527 (89.833)	Acc@5 99.609 (99.642)
Epoch: [75][9/25]	Time 0.720 (0.719)	Data 0.007 (0.066)	Loss 0.5410 (0.5552)	Acc@1 90.479 (89.897)	Acc@5 99.854 (99.663)
Epoch: [75][10/25]	Time 0.718 (0.719)	Data 0.004 (0.061)	Loss 0.5738 (0.5569)	Acc@1 89.062 (89.822)	Acc@5 99.854 (99.680)
Epoch: [75][11/25]	Time 0.720 (0.719)	Data 0.004 (0.056)	Loss 0.5492 (0.5562)	Acc@1 89.844 (89.823)	Acc@5 99.951 (99.703)
Epoch: [75][12/25]	Time 0.624 (0.711)	Data 0.007 (0.052)	Loss 0.5719 (0.5574)	Acc@1 88.965 (89.757)	Acc@5 99.658 (99.700)
Epoch: [75][13/25]	Time 0.710 (0.711)	Data 0.005 (0.049)	Loss 0.5327 (0.5557)	Acc@1 90.723 (89.826)	Acc@5 99.854 (99.711)
Epoch: [75][14/25]	Time 0.742 (0.713)	Data 0.006 (0.046)	Loss 0.5422 (0.5548)	Acc@1 89.697 (89.818)	Acc@5 99.756 (99.714)
Epoch: [75][15/25]	Time 0.659 (0.710)	Data 0.006 (0.043)	Loss 0.5708 (0.5558)	Acc@1 89.014 (89.767)	Acc@5 99.658 (99.710)
Epoch: [75][16/25]	Time 0.623 (0.705)	Data 0.005 (0.041)	Loss 0.5844 (0.5575)	Acc@1 88.721 (89.706)	Acc@5 99.658 (99.707)
Epoch: [75][17/25]	Time 0.656 (0.702)	Data 0.006 (0.039)	Loss 0.5718 (0.5583)	Acc@1 89.355 (89.686)	Acc@5 99.756 (99.710)
Epoch: [75][18/25]	Time 0.634 (0.699)	Data 0.004 (0.037)	Loss 0.5518 (0.5579)	Acc@1 89.990 (89.702)	Acc@5 99.609 (99.704)
Epoch: [75][19/25]	Time 0.636 (0.695)	Data 0.007 (0.036)	Loss 0.5421 (0.5571)	Acc@1 90.186 (89.727)	Acc@5 99.707 (99.705)
Epoch: [75][20/25]	Time 0.646 (0.693)	Data 0.006 (0.034)	Loss 0.5695 (0.5577)	Acc@1 89.697 (89.725)	Acc@5 99.316 (99.686)
Epoch: [75][21/25]	Time 0.622 (0.690)	Data 0.004 (0.033)	Loss 0.5578 (0.5577)	Acc@1 89.941 (89.735)	Acc@5 99.512 (99.678)
Epoch: [75][22/25]	Time 0.615 (0.687)	Data 0.005 (0.032)	Loss 0.5640 (0.5580)	Acc@1 89.404 (89.721)	Acc@5 99.658 (99.677)
Epoch: [75][23/25]	Time 0.626 (0.684)	Data 0.004 (0.031)	Loss 0.5624 (0.5582)	Acc@1 89.648 (89.718)	Acc@5 99.805 (99.683)
Epoch: [75][24/25]	Time 0.356 (0.671)	Data 0.004 (0.030)	Loss 0.5438 (0.5579)	Acc@1 91.274 (89.744)	Acc@5 99.410 (99.678)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 452128 ; 487386 ; 0.9276589807667844

Epoch: [76 | 180] LR: 0.100000
Epoch: [76][0/25]	Time 0.706 (0.706)	Data 0.623 (0.623)	Loss 0.5470 (0.5470)	Acc@1 90.332 (90.332)	Acc@5 99.707 (99.707)
Epoch: [76][1/25]	Time 0.703 (0.705)	Data 0.005 (0.314)	Loss 0.4773 (0.5121)	Acc@1 92.920 (91.626)	Acc@5 99.756 (99.731)
Epoch: [76][2/25]	Time 0.678 (0.696)	Data 0.005 (0.211)	Loss 0.4951 (0.5064)	Acc@1 91.650 (91.634)	Acc@5 99.902 (99.788)
Epoch: [76][3/25]	Time 0.759 (0.712)	Data 0.004 (0.159)	Loss 0.5231 (0.5106)	Acc@1 90.576 (91.370)	Acc@5 99.805 (99.792)
Epoch: [76][4/25]	Time 0.658 (0.701)	Data 0.006 (0.129)	Loss 0.4929 (0.5071)	Acc@1 91.699 (91.436)	Acc@5 99.707 (99.775)
Epoch: [76][5/25]	Time 0.601 (0.684)	Data 0.005 (0.108)	Loss 0.5012 (0.5061)	Acc@1 91.699 (91.479)	Acc@5 99.805 (99.780)
Epoch: [76][6/25]	Time 0.616 (0.675)	Data 0.007 (0.094)	Loss 0.5113 (0.5068)	Acc@1 91.113 (91.427)	Acc@5 99.902 (99.798)
Epoch: [76][7/25]	Time 0.640 (0.670)	Data 0.006 (0.083)	Loss 0.4989 (0.5058)	Acc@1 91.748 (91.467)	Acc@5 99.805 (99.799)
Epoch: [76][8/25]	Time 0.659 (0.669)	Data 0.004 (0.074)	Loss 0.5165 (0.5070)	Acc@1 91.162 (91.433)	Acc@5 99.707 (99.788)
Epoch: [76][9/25]	Time 0.700 (0.672)	Data 0.005 (0.067)	Loss 0.4881 (0.5051)	Acc@1 91.602 (91.450)	Acc@5 99.854 (99.795)
Epoch: [76][10/25]	Time 0.723 (0.677)	Data 0.006 (0.062)	Loss 0.4887 (0.5036)	Acc@1 91.846 (91.486)	Acc@5 99.902 (99.805)
Epoch: [76][11/25]	Time 0.659 (0.675)	Data 0.007 (0.057)	Loss 0.5143 (0.5045)	Acc@1 91.406 (91.479)	Acc@5 99.707 (99.797)
Epoch: [76][12/25]	Time 0.624 (0.671)	Data 0.007 (0.053)	Loss 0.5145 (0.5053)	Acc@1 91.162 (91.455)	Acc@5 99.707 (99.790)
Epoch: [76][13/25]	Time 0.608 (0.667)	Data 0.004 (0.050)	Loss 0.5077 (0.5055)	Acc@1 91.846 (91.483)	Acc@5 99.902 (99.798)
Epoch: [76][14/25]	Time 0.638 (0.665)	Data 0.004 (0.047)	Loss 0.4936 (0.5047)	Acc@1 91.846 (91.507)	Acc@5 99.854 (99.801)
Epoch: [76][15/25]	Time 0.678 (0.666)	Data 0.009 (0.044)	Loss 0.4859 (0.5035)	Acc@1 91.797 (91.525)	Acc@5 99.805 (99.802)
Epoch: [76][16/25]	Time 0.722 (0.669)	Data 0.005 (0.042)	Loss 0.5391 (0.5056)	Acc@1 90.869 (91.487)	Acc@5 99.316 (99.773)
Epoch: [76][17/25]	Time 0.730 (0.672)	Data 0.004 (0.040)	Loss 0.5433 (0.5077)	Acc@1 90.723 (91.444)	Acc@5 99.707 (99.769)
Epoch: [76][18/25]	Time 0.685 (0.673)	Data 0.004 (0.038)	Loss 0.5300 (0.5089)	Acc@1 91.504 (91.447)	Acc@5 99.902 (99.776)
Epoch: [76][19/25]	Time 0.704 (0.675)	Data 0.006 (0.036)	Loss 0.5103 (0.5089)	Acc@1 91.064 (91.428)	Acc@5 99.707 (99.773)
Epoch: [76][20/25]	Time 0.771 (0.679)	Data 0.007 (0.035)	Loss 0.5291 (0.5099)	Acc@1 90.869 (91.402)	Acc@5 99.756 (99.772)
Epoch: [76][21/25]	Time 0.662 (0.678)	Data 0.007 (0.034)	Loss 0.5382 (0.5112)	Acc@1 90.381 (91.355)	Acc@5 99.756 (99.771)
Epoch: [76][22/25]	Time 0.709 (0.680)	Data 0.006 (0.033)	Loss 0.5628 (0.5134)	Acc@1 89.648 (91.281)	Acc@5 99.756 (99.771)
Epoch: [76][23/25]	Time 0.728 (0.682)	Data 0.006 (0.031)	Loss 0.5410 (0.5146)	Acc@1 90.479 (91.248)	Acc@5 99.756 (99.770)
Epoch: [76][24/25]	Time 0.419 (0.671)	Data 0.004 (0.030)	Loss 0.5482 (0.5151)	Acc@1 90.330 (91.232)	Acc@5 99.646 (99.768)

Epoch: [77 | 180] LR: 0.100000
Epoch: [77][0/25]	Time 0.686 (0.686)	Data 0.648 (0.648)	Loss 0.5298 (0.5298)	Acc@1 90.479 (90.479)	Acc@5 99.463 (99.463)
Epoch: [77][1/25]	Time 0.744 (0.715)	Data 0.007 (0.328)	Loss 0.5785 (0.5541)	Acc@1 88.818 (89.648)	Acc@5 99.609 (99.536)
Epoch: [77][2/25]	Time 0.731 (0.720)	Data 0.008 (0.221)	Loss 0.5435 (0.5506)	Acc@1 90.039 (89.779)	Acc@5 99.658 (99.577)
Epoch: [77][3/25]	Time 0.694 (0.714)	Data 0.003 (0.167)	Loss 0.5626 (0.5536)	Acc@1 90.088 (89.856)	Acc@5 99.512 (99.561)
Epoch: [77][4/25]	Time 0.747 (0.720)	Data 0.008 (0.135)	Loss 0.5619 (0.5553)	Acc@1 88.770 (89.639)	Acc@5 99.658 (99.580)
Epoch: [77][5/25]	Time 0.720 (0.720)	Data 0.004 (0.113)	Loss 0.5573 (0.5556)	Acc@1 89.453 (89.608)	Acc@5 99.805 (99.618)
Epoch: [77][6/25]	Time 0.680 (0.715)	Data 0.004 (0.098)	Loss 0.5571 (0.5558)	Acc@1 89.990 (89.662)	Acc@5 99.805 (99.644)
Epoch: [77][7/25]	Time 0.716 (0.715)	Data 0.005 (0.086)	Loss 0.5597 (0.5563)	Acc@1 89.355 (89.624)	Acc@5 99.609 (99.640)
Epoch: [77][8/25]	Time 0.718 (0.715)	Data 0.005 (0.077)	Loss 0.5741 (0.5583)	Acc@1 88.818 (89.535)	Acc@5 99.756 (99.653)
Epoch: [77][9/25]	Time 0.734 (0.717)	Data 0.005 (0.070)	Loss 0.5745 (0.5599)	Acc@1 89.014 (89.482)	Acc@5 99.609 (99.648)
Epoch: [77][10/25]	Time 0.667 (0.712)	Data 0.007 (0.064)	Loss 0.5754 (0.5613)	Acc@1 88.525 (89.395)	Acc@5 99.658 (99.649)
Epoch: [77][11/25]	Time 0.638 (0.706)	Data 0.004 (0.059)	Loss 0.5587 (0.5611)	Acc@1 89.258 (89.384)	Acc@5 99.951 (99.674)
Epoch: [77][12/25]	Time 0.662 (0.703)	Data 0.005 (0.055)	Loss 0.5668 (0.5615)	Acc@1 89.502 (89.393)	Acc@5 99.658 (99.673)
Epoch: [77][13/25]	Time 0.700 (0.703)	Data 0.007 (0.052)	Loss 0.5625 (0.5616)	Acc@1 89.404 (89.394)	Acc@5 99.805 (99.683)
Epoch: [77][14/25]	Time 0.730 (0.704)	Data 0.006 (0.048)	Loss 0.5649 (0.5618)	Acc@1 89.307 (89.388)	Acc@5 99.561 (99.674)
Epoch: [77][15/25]	Time 0.713 (0.705)	Data 0.006 (0.046)	Loss 0.5634 (0.5619)	Acc@1 89.258 (89.380)	Acc@5 99.756 (99.680)
Epoch: [77][16/25]	Time 0.660 (0.702)	Data 0.005 (0.043)	Loss 0.5634 (0.5620)	Acc@1 88.428 (89.324)	Acc@5 99.561 (99.673)
Epoch: [77][17/25]	Time 0.719 (0.703)	Data 0.004 (0.041)	Loss 0.5922 (0.5637)	Acc@1 87.842 (89.242)	Acc@5 99.561 (99.666)
Epoch: [77][18/25]	Time 0.721 (0.704)	Data 0.004 (0.039)	Loss 0.5737 (0.5642)	Acc@1 89.014 (89.230)	Acc@5 99.805 (99.674)
Epoch: [77][19/25]	Time 0.695 (0.704)	Data 0.005 (0.038)	Loss 0.5720 (0.5646)	Acc@1 89.014 (89.219)	Acc@5 99.756 (99.678)
Epoch: [77][20/25]	Time 0.679 (0.703)	Data 0.004 (0.036)	Loss 0.5528 (0.5640)	Acc@1 90.283 (89.269)	Acc@5 99.658 (99.677)
Epoch: [77][21/25]	Time 0.742 (0.704)	Data 0.007 (0.035)	Loss 0.5785 (0.5647)	Acc@1 89.111 (89.262)	Acc@5 99.658 (99.676)
Epoch: [77][22/25]	Time 0.727 (0.705)	Data 0.006 (0.033)	Loss 0.5755 (0.5652)	Acc@1 89.453 (89.271)	Acc@5 99.365 (99.662)
Epoch: [77][23/25]	Time 0.667 (0.704)	Data 0.004 (0.032)	Loss 0.5410 (0.5642)	Acc@1 90.723 (89.331)	Acc@5 99.805 (99.668)
Epoch: [77][24/25]	Time 0.404 (0.692)	Data 0.004 (0.031)	Loss 0.5860 (0.5645)	Acc@1 88.915 (89.324)	Acc@5 99.528 (99.666)

Epoch: [78 | 180] LR: 0.100000
Epoch: [78][0/25]	Time 0.735 (0.735)	Data 0.678 (0.678)	Loss 0.5600 (0.5600)	Acc@1 89.746 (89.746)	Acc@5 99.854 (99.854)
Epoch: [78][1/25]	Time 0.747 (0.741)	Data 0.005 (0.342)	Loss 0.5559 (0.5580)	Acc@1 90.234 (89.990)	Acc@5 99.561 (99.707)
Epoch: [78][2/25]	Time 0.688 (0.723)	Data 0.006 (0.230)	Loss 0.5669 (0.5609)	Acc@1 88.379 (89.453)	Acc@5 99.805 (99.740)
Epoch: [78][3/25]	Time 0.700 (0.717)	Data 0.005 (0.174)	Loss 0.5448 (0.5569)	Acc@1 90.039 (89.600)	Acc@5 99.805 (99.756)
Epoch: [78][4/25]	Time 0.697 (0.713)	Data 0.007 (0.140)	Loss 0.5390 (0.5533)	Acc@1 90.430 (89.766)	Acc@5 99.609 (99.727)
Epoch: [78][5/25]	Time 0.683 (0.708)	Data 0.004 (0.118)	Loss 0.5275 (0.5490)	Acc@1 91.162 (89.998)	Acc@5 99.756 (99.731)
Epoch: [78][6/25]	Time 0.695 (0.706)	Data 0.004 (0.102)	Loss 0.5322 (0.5466)	Acc@1 91.162 (90.165)	Acc@5 99.707 (99.728)
Epoch: [78][7/25]	Time 0.717 (0.708)	Data 0.005 (0.089)	Loss 0.5262 (0.5441)	Acc@1 90.869 (90.253)	Acc@5 99.854 (99.744)
Epoch: [78][8/25]	Time 0.739 (0.711)	Data 0.007 (0.080)	Loss 0.5628 (0.5461)	Acc@1 89.355 (90.153)	Acc@5 99.756 (99.745)
Epoch: [78][9/25]	Time 0.698 (0.710)	Data 0.008 (0.073)	Loss 0.5615 (0.5477)	Acc@1 89.941 (90.132)	Acc@5 99.707 (99.741)
Epoch: [78][10/25]	Time 0.716 (0.710)	Data 0.005 (0.067)	Loss 0.5374 (0.5468)	Acc@1 91.113 (90.221)	Acc@5 99.707 (99.738)
Epoch: [78][11/25]	Time 0.733 (0.712)	Data 0.004 (0.062)	Loss 0.5358 (0.5458)	Acc@1 90.576 (90.251)	Acc@5 99.805 (99.744)
Epoch: [78][12/25]	Time 0.681 (0.710)	Data 0.004 (0.057)	Loss 0.5713 (0.5478)	Acc@1 89.111 (90.163)	Acc@5 99.658 (99.737)
Epoch: [78][13/25]	Time 0.615 (0.703)	Data 0.007 (0.054)	Loss 0.5240 (0.5461)	Acc@1 90.967 (90.220)	Acc@5 99.805 (99.742)
Epoch: [78][14/25]	Time 0.675 (0.701)	Data 0.007 (0.051)	Loss 0.5516 (0.5465)	Acc@1 89.795 (90.192)	Acc@5 99.512 (99.727)
Epoch: [78][15/25]	Time 0.642 (0.697)	Data 0.004 (0.048)	Loss 0.5633 (0.5475)	Acc@1 89.697 (90.161)	Acc@5 99.658 (99.722)
Epoch: [78][16/25]	Time 0.699 (0.698)	Data 0.005 (0.045)	Loss 0.5384 (0.5470)	Acc@1 89.941 (90.148)	Acc@5 99.854 (99.730)
Epoch: [78][17/25]	Time 0.754 (0.701)	Data 0.005 (0.043)	Loss 0.5849 (0.5491)	Acc@1 88.721 (90.069)	Acc@5 99.512 (99.718)
Epoch: [78][18/25]	Time 0.651 (0.698)	Data 0.006 (0.041)	Loss 0.5387 (0.5486)	Acc@1 90.039 (90.067)	Acc@5 99.756 (99.720)
Epoch: [78][19/25]	Time 0.620 (0.694)	Data 0.004 (0.039)	Loss 0.5896 (0.5506)	Acc@1 89.258 (90.027)	Acc@5 99.609 (99.714)
Epoch: [78][20/25]	Time 0.622 (0.691)	Data 0.005 (0.038)	Loss 0.5506 (0.5506)	Acc@1 89.600 (90.007)	Acc@5 99.658 (99.712)
Epoch: [78][21/25]	Time 0.625 (0.688)	Data 0.007 (0.036)	Loss 0.5694 (0.5515)	Acc@1 89.453 (89.981)	Acc@5 99.268 (99.691)
Epoch: [78][22/25]	Time 0.610 (0.684)	Data 0.004 (0.035)	Loss 0.5855 (0.5529)	Acc@1 89.062 (89.941)	Acc@5 99.414 (99.679)
Epoch: [78][23/25]	Time 0.591 (0.681)	Data 0.007 (0.034)	Loss 0.5525 (0.5529)	Acc@1 90.186 (89.952)	Acc@5 99.609 (99.677)
Epoch: [78][24/25]	Time 0.361 (0.668)	Data 0.004 (0.032)	Loss 0.5653 (0.5531)	Acc@1 89.741 (89.948)	Acc@5 99.646 (99.676)

Epoch: [79 | 180] LR: 0.100000
Epoch: [79][0/25]	Time 0.702 (0.702)	Data 0.637 (0.637)	Loss 0.5789 (0.5789)	Acc@1 89.209 (89.209)	Acc@5 99.707 (99.707)
Epoch: [79][1/25]	Time 0.678 (0.690)	Data 0.008 (0.322)	Loss 0.5573 (0.5681)	Acc@1 89.111 (89.160)	Acc@5 99.756 (99.731)
Epoch: [79][2/25]	Time 0.693 (0.691)	Data 0.004 (0.216)	Loss 0.5761 (0.5708)	Acc@1 88.916 (89.079)	Acc@5 99.658 (99.707)
Epoch: [79][3/25]	Time 0.707 (0.695)	Data 0.005 (0.163)	Loss 0.5699 (0.5705)	Acc@1 88.965 (89.050)	Acc@5 99.414 (99.634)
Epoch: [79][4/25]	Time 0.762 (0.708)	Data 0.005 (0.132)	Loss 0.5765 (0.5717)	Acc@1 89.453 (89.131)	Acc@5 99.658 (99.639)
Epoch: [79][5/25]	Time 0.677 (0.703)	Data 0.007 (0.111)	Loss 0.5921 (0.5751)	Acc@1 88.477 (89.022)	Acc@5 99.756 (99.658)
Epoch: [79][6/25]	Time 0.671 (0.699)	Data 0.004 (0.096)	Loss 0.5659 (0.5738)	Acc@1 89.600 (89.104)	Acc@5 99.609 (99.651)
Epoch: [79][7/25]	Time 0.858 (0.719)	Data 0.006 (0.084)	Loss 0.5822 (0.5749)	Acc@1 88.428 (89.020)	Acc@5 99.609 (99.646)
Epoch: [79][8/25]	Time 0.814 (0.729)	Data 0.005 (0.076)	Loss 0.5857 (0.5761)	Acc@1 88.965 (89.014)	Acc@5 99.463 (99.626)
Epoch: [79][9/25]	Time 0.643 (0.721)	Data 0.005 (0.069)	Loss 0.5468 (0.5731)	Acc@1 90.332 (89.146)	Acc@5 99.658 (99.629)
Epoch: [79][10/25]	Time 0.676 (0.716)	Data 0.005 (0.063)	Loss 0.5340 (0.5696)	Acc@1 90.576 (89.276)	Acc@5 100.000 (99.663)
Epoch: [79][11/25]	Time 0.752 (0.719)	Data 0.009 (0.058)	Loss 0.5487 (0.5678)	Acc@1 90.430 (89.372)	Acc@5 99.658 (99.662)
Epoch: [79][12/25]	Time 0.722 (0.720)	Data 0.006 (0.054)	Loss 0.5597 (0.5672)	Acc@1 90.088 (89.427)	Acc@5 99.756 (99.669)
Epoch: [79][13/25]	Time 0.750 (0.722)	Data 0.005 (0.051)	Loss 0.5773 (0.5679)	Acc@1 89.209 (89.411)	Acc@5 99.414 (99.651)
Epoch: [79][14/25]	Time 0.771 (0.725)	Data 0.006 (0.048)	Loss 0.5455 (0.5664)	Acc@1 89.746 (89.434)	Acc@5 99.756 (99.658)
Epoch: [79][15/25]	Time 0.666 (0.721)	Data 0.006 (0.045)	Loss 0.5448 (0.5651)	Acc@1 90.576 (89.505)	Acc@5 99.756 (99.664)
Epoch: [79][16/25]	Time 0.723 (0.722)	Data 0.005 (0.043)	Loss 0.5739 (0.5656)	Acc@1 89.600 (89.511)	Acc@5 99.756 (99.670)
Epoch: [79][17/25]	Time 0.725 (0.722)	Data 0.007 (0.041)	Loss 0.5529 (0.5649)	Acc@1 89.893 (89.532)	Acc@5 99.756 (99.674)
Epoch: [79][18/25]	Time 0.692 (0.720)	Data 0.004 (0.039)	Loss 0.5483 (0.5640)	Acc@1 90.771 (89.597)	Acc@5 99.609 (99.671)
Epoch: [79][19/25]	Time 0.706 (0.719)	Data 0.005 (0.037)	Loss 0.5918 (0.5654)	Acc@1 88.428 (89.539)	Acc@5 99.658 (99.670)
Epoch: [79][20/25]	Time 0.756 (0.721)	Data 0.006 (0.036)	Loss 0.5648 (0.5654)	Acc@1 89.746 (89.548)	Acc@5 99.561 (99.665)
Epoch: [79][21/25]	Time 0.667 (0.719)	Data 0.004 (0.034)	Loss 0.5745 (0.5658)	Acc@1 89.453 (89.544)	Acc@5 99.512 (99.658)
Epoch: [79][22/25]	Time 0.635 (0.715)	Data 0.005 (0.033)	Loss 0.5933 (0.5670)	Acc@1 88.525 (89.500)	Acc@5 99.609 (99.656)
Epoch: [79][23/25]	Time 0.609 (0.711)	Data 0.004 (0.032)	Loss 0.5767 (0.5674)	Acc@1 89.551 (89.502)	Acc@5 99.463 (99.648)
Epoch: [79][24/25]	Time 0.368 (0.697)	Data 0.007 (0.031)	Loss 0.5372 (0.5669)	Acc@1 92.335 (89.550)	Acc@5 99.764 (99.650)

Epoch: [80 | 180] LR: 0.100000
Epoch: [80][0/25]	Time 0.729 (0.729)	Data 0.677 (0.677)	Loss 0.5290 (0.5290)	Acc@1 90.918 (90.918)	Acc@5 99.707 (99.707)
Epoch: [80][1/25]	Time 0.706 (0.717)	Data 0.003 (0.340)	Loss 0.5558 (0.5424)	Acc@1 89.453 (90.186)	Acc@5 99.609 (99.658)
Epoch: [80][2/25]	Time 0.727 (0.721)	Data 0.007 (0.229)	Loss 0.5196 (0.5348)	Acc@1 90.967 (90.446)	Acc@5 99.854 (99.723)
Epoch: [80][3/25]	Time 0.685 (0.712)	Data 0.008 (0.174)	Loss 0.5718 (0.5441)	Acc@1 89.307 (90.161)	Acc@5 99.707 (99.719)
Epoch: [80][4/25]	Time 0.687 (0.707)	Data 0.006 (0.140)	Loss 0.5447 (0.5442)	Acc@1 90.771 (90.283)	Acc@5 99.609 (99.697)
Epoch: [80][5/25]	Time 0.745 (0.713)	Data 0.007 (0.118)	Loss 0.5427 (0.5439)	Acc@1 90.088 (90.251)	Acc@5 99.707 (99.699)
Epoch: [80][6/25]	Time 0.711 (0.713)	Data 0.005 (0.102)	Loss 0.5573 (0.5459)	Acc@1 89.648 (90.165)	Acc@5 99.756 (99.707)
Epoch: [80][7/25]	Time 0.681 (0.709)	Data 0.007 (0.090)	Loss 0.5432 (0.5455)	Acc@1 90.137 (90.161)	Acc@5 99.805 (99.719)
Epoch: [80][8/25]	Time 0.676 (0.705)	Data 0.005 (0.081)	Loss 0.5994 (0.5515)	Acc@1 87.939 (89.914)	Acc@5 99.561 (99.702)
Epoch: [80][9/25]	Time 0.615 (0.696)	Data 0.006 (0.073)	Loss 0.5203 (0.5484)	Acc@1 90.674 (89.990)	Acc@5 99.951 (99.727)
Epoch: [80][10/25]	Time 0.591 (0.687)	Data 0.005 (0.067)	Loss 0.5650 (0.5499)	Acc@1 89.990 (89.990)	Acc@5 99.512 (99.707)
Epoch: [80][11/25]	Time 0.613 (0.681)	Data 0.006 (0.062)	Loss 0.5689 (0.5515)	Acc@1 89.697 (89.966)	Acc@5 99.658 (99.703)
Epoch: [80][12/25]	Time 0.611 (0.675)	Data 0.005 (0.058)	Loss 0.5712 (0.5530)	Acc@1 89.893 (89.960)	Acc@5 99.707 (99.703)
Epoch: [80][13/25]	Time 0.623 (0.671)	Data 0.005 (0.054)	Loss 0.5985 (0.5562)	Acc@1 88.281 (89.840)	Acc@5 99.854 (99.714)
Epoch: [80][14/25]	Time 0.658 (0.671)	Data 0.005 (0.051)	Loss 0.5538 (0.5561)	Acc@1 90.283 (89.870)	Acc@5 99.609 (99.707)
Epoch: [80][15/25]	Time 0.632 (0.668)	Data 0.004 (0.048)	Loss 0.5639 (0.5566)	Acc@1 89.648 (89.856)	Acc@5 99.561 (99.698)
Epoch: [80][16/25]	Time 0.726 (0.672)	Data 0.008 (0.045)	Loss 0.5609 (0.5568)	Acc@1 89.404 (89.829)	Acc@5 99.805 (99.704)
Epoch: [80][17/25]	Time 0.754 (0.676)	Data 0.004 (0.043)	Loss 0.5692 (0.5575)	Acc@1 88.965 (89.781)	Acc@5 99.658 (99.702)
Epoch: [80][18/25]	Time 0.695 (0.677)	Data 0.004 (0.041)	Loss 0.5694 (0.5581)	Acc@1 89.795 (89.782)	Acc@5 99.609 (99.697)
Epoch: [80][19/25]	Time 0.675 (0.677)	Data 0.004 (0.039)	Loss 0.5411 (0.5573)	Acc@1 90.869 (89.836)	Acc@5 99.707 (99.697)
Epoch: [80][20/25]	Time 0.732 (0.680)	Data 0.006 (0.038)	Loss 0.5641 (0.5576)	Acc@1 89.160 (89.804)	Acc@5 99.512 (99.688)
Epoch: [80][21/25]	Time 0.679 (0.680)	Data 0.005 (0.036)	Loss 0.5314 (0.5564)	Acc@1 90.918 (89.855)	Acc@5 99.658 (99.687)
Epoch: [80][22/25]	Time 0.664 (0.679)	Data 0.007 (0.035)	Loss 0.5636 (0.5567)	Acc@1 90.039 (89.863)	Acc@5 99.805 (99.692)
Epoch: [80][23/25]	Time 0.621 (0.677)	Data 0.004 (0.034)	Loss 0.5999 (0.5585)	Acc@1 88.330 (89.799)	Acc@5 99.414 (99.681)
Epoch: [80][24/25]	Time 0.357 (0.664)	Data 0.006 (0.032)	Loss 0.5322 (0.5581)	Acc@1 91.863 (89.834)	Acc@5 99.646 (99.680)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(15, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 449240 ; 487386 ; 0.9217334925500528

Epoch: [81 | 180] LR: 0.100000
Epoch: [81][0/25]	Time 0.681 (0.681)	Data 0.704 (0.704)	Loss 0.5745 (0.5745)	Acc@1 88.867 (88.867)	Acc@5 99.561 (99.561)
Epoch: [81][1/25]	Time 0.676 (0.679)	Data 0.007 (0.355)	Loss 0.5029 (0.5387)	Acc@1 91.699 (90.283)	Acc@5 99.756 (99.658)
Epoch: [81][2/25]	Time 0.679 (0.679)	Data 0.007 (0.239)	Loss 0.5078 (0.5284)	Acc@1 91.895 (90.820)	Acc@5 99.609 (99.642)
Epoch: [81][3/25]	Time 0.723 (0.690)	Data 0.004 (0.180)	Loss 0.5106 (0.5240)	Acc@1 91.455 (90.979)	Acc@5 99.854 (99.695)
Epoch: [81][4/25]	Time 0.700 (0.692)	Data 0.006 (0.145)	Loss 0.4847 (0.5161)	Acc@1 92.529 (91.289)	Acc@5 99.854 (99.727)
Epoch: [81][5/25]	Time 0.706 (0.694)	Data 0.007 (0.122)	Loss 0.4861 (0.5111)	Acc@1 92.383 (91.471)	Acc@5 99.658 (99.715)
Epoch: [81][6/25]	Time 0.743 (0.701)	Data 0.007 (0.106)	Loss 0.4831 (0.5071)	Acc@1 92.334 (91.595)	Acc@5 99.854 (99.735)
Epoch: [81][7/25]	Time 0.745 (0.707)	Data 0.003 (0.093)	Loss 0.4666 (0.5020)	Acc@1 92.773 (91.742)	Acc@5 99.854 (99.750)
Epoch: [81][8/25]	Time 0.682 (0.704)	Data 0.005 (0.083)	Loss 0.4835 (0.5000)	Acc@1 91.895 (91.759)	Acc@5 99.658 (99.740)
Epoch: [81][9/25]	Time 0.646 (0.698)	Data 0.007 (0.076)	Loss 0.4985 (0.4998)	Acc@1 91.553 (91.738)	Acc@5 99.854 (99.751)
Epoch: [81][10/25]	Time 0.689 (0.697)	Data 0.008 (0.070)	Loss 0.4920 (0.4991)	Acc@1 91.943 (91.757)	Acc@5 99.854 (99.760)
Epoch: [81][11/25]	Time 0.724 (0.700)	Data 0.005 (0.064)	Loss 0.4971 (0.4990)	Acc@1 91.602 (91.744)	Acc@5 99.951 (99.776)
Epoch: [81][12/25]	Time 0.720 (0.701)	Data 0.004 (0.060)	Loss 0.5007 (0.4991)	Acc@1 91.211 (91.703)	Acc@5 99.854 (99.782)
Epoch: [81][13/25]	Time 0.683 (0.700)	Data 0.006 (0.056)	Loss 0.5029 (0.4994)	Acc@1 91.406 (91.682)	Acc@5 99.854 (99.787)
Epoch: [81][14/25]	Time 0.659 (0.697)	Data 0.007 (0.052)	Loss 0.5324 (0.5016)	Acc@1 89.941 (91.566)	Acc@5 99.658 (99.779)
Epoch: [81][15/25]	Time 0.732 (0.699)	Data 0.004 (0.049)	Loss 0.5365 (0.5037)	Acc@1 90.576 (91.504)	Acc@5 99.756 (99.777)
Epoch: [81][16/25]	Time 0.742 (0.702)	Data 0.007 (0.047)	Loss 0.4949 (0.5032)	Acc@1 91.943 (91.530)	Acc@5 99.756 (99.776)
Epoch: [81][17/25]	Time 0.677 (0.700)	Data 0.004 (0.045)	Loss 0.5395 (0.5052)	Acc@1 89.844 (91.436)	Acc@5 99.609 (99.767)
Epoch: [81][18/25]	Time 0.658 (0.698)	Data 0.005 (0.042)	Loss 0.5181 (0.5059)	Acc@1 91.016 (91.414)	Acc@5 99.902 (99.774)
Epoch: [81][19/25]	Time 0.645 (0.696)	Data 0.007 (0.041)	Loss 0.5237 (0.5068)	Acc@1 90.234 (91.355)	Acc@5 99.951 (99.783)
Epoch: [81][20/25]	Time 0.750 (0.698)	Data 0.007 (0.039)	Loss 0.5281 (0.5078)	Acc@1 90.430 (91.311)	Acc@5 99.756 (99.781)
Epoch: [81][21/25]	Time 0.696 (0.698)	Data 0.006 (0.038)	Loss 0.5388 (0.5092)	Acc@1 90.674 (91.282)	Acc@5 99.756 (99.780)
Epoch: [81][22/25]	Time 0.680 (0.697)	Data 0.007 (0.036)	Loss 0.5284 (0.5101)	Acc@1 90.625 (91.253)	Acc@5 99.561 (99.771)
Epoch: [81][23/25]	Time 0.721 (0.698)	Data 0.004 (0.035)	Loss 0.5558 (0.5120)	Acc@1 89.941 (91.199)	Acc@5 99.805 (99.772)
Epoch: [81][24/25]	Time 0.458 (0.689)	Data 0.005 (0.034)	Loss 0.5691 (0.5129)	Acc@1 89.151 (91.164)	Acc@5 99.528 (99.768)

Epoch: [82 | 180] LR: 0.100000
Epoch: [82][0/25]	Time 0.713 (0.713)	Data 0.641 (0.641)	Loss 0.4985 (0.4985)	Acc@1 92.285 (92.285)	Acc@5 99.902 (99.902)
Epoch: [82][1/25]	Time 0.698 (0.705)	Data 0.005 (0.323)	Loss 0.5607 (0.5296)	Acc@1 89.746 (91.016)	Acc@5 99.609 (99.756)
Epoch: [82][2/25]	Time 0.713 (0.708)	Data 0.005 (0.217)	Loss 0.5486 (0.5359)	Acc@1 90.820 (90.951)	Acc@5 99.463 (99.658)
Epoch: [82][3/25]	Time 0.700 (0.706)	Data 0.006 (0.164)	Loss 0.5474 (0.5388)	Acc@1 89.893 (90.686)	Acc@5 99.658 (99.658)
Epoch: [82][4/25]	Time 0.776 (0.720)	Data 0.008 (0.133)	Loss 0.5548 (0.5420)	Acc@1 89.941 (90.537)	Acc@5 99.658 (99.658)
Epoch: [82][5/25]	Time 0.812 (0.735)	Data 0.005 (0.112)	Loss 0.5426 (0.5421)	Acc@1 91.260 (90.658)	Acc@5 99.805 (99.683)
Epoch: [82][6/25]	Time 0.757 (0.738)	Data 0.005 (0.096)	Loss 0.5248 (0.5396)	Acc@1 91.211 (90.737)	Acc@5 99.658 (99.679)
Epoch: [82][7/25]	Time 0.724 (0.737)	Data 0.005 (0.085)	Loss 0.5468 (0.5405)	Acc@1 90.137 (90.662)	Acc@5 99.756 (99.689)
Epoch: [82][8/25]	Time 0.718 (0.735)	Data 0.006 (0.076)	Loss 0.5568 (0.5423)	Acc@1 89.941 (90.582)	Acc@5 99.463 (99.664)
Epoch: [82][9/25]	Time 0.623 (0.723)	Data 0.006 (0.069)	Loss 0.5536 (0.5434)	Acc@1 89.502 (90.474)	Acc@5 99.707 (99.668)
Epoch: [82][10/25]	Time 0.622 (0.714)	Data 0.006 (0.063)	Loss 0.5155 (0.5409)	Acc@1 91.211 (90.541)	Acc@5 99.658 (99.667)
Epoch: [82][11/25]	Time 0.624 (0.707)	Data 0.007 (0.059)	Loss 0.5378 (0.5406)	Acc@1 90.576 (90.544)	Acc@5 99.756 (99.674)
Epoch: [82][12/25]	Time 0.644 (0.702)	Data 0.010 (0.055)	Loss 0.4982 (0.5374)	Acc@1 90.918 (90.572)	Acc@5 99.707 (99.677)
Epoch: [82][13/25]	Time 0.641 (0.697)	Data 0.005 (0.051)	Loss 0.5392 (0.5375)	Acc@1 90.381 (90.559)	Acc@5 99.512 (99.665)
Epoch: [82][14/25]	Time 0.634 (0.693)	Data 0.004 (0.048)	Loss 0.5655 (0.5394)	Acc@1 89.014 (90.456)	Acc@5 99.561 (99.658)
Epoch: [82][15/25]	Time 0.607 (0.688)	Data 0.005 (0.046)	Loss 0.5801 (0.5419)	Acc@1 89.551 (90.399)	Acc@5 99.805 (99.667)
Epoch: [82][16/25]	Time 0.638 (0.685)	Data 0.008 (0.043)	Loss 0.5145 (0.5403)	Acc@1 91.406 (90.458)	Acc@5 99.609 (99.664)
Epoch: [82][17/25]	Time 0.642 (0.683)	Data 0.007 (0.041)	Loss 0.5476 (0.5407)	Acc@1 89.941 (90.430)	Acc@5 99.658 (99.664)
Epoch: [82][18/25]	Time 0.680 (0.682)	Data 0.005 (0.039)	Loss 0.5606 (0.5418)	Acc@1 89.014 (90.355)	Acc@5 99.854 (99.674)
Epoch: [82][19/25]	Time 0.744 (0.686)	Data 0.008 (0.038)	Loss 0.5437 (0.5419)	Acc@1 90.332 (90.354)	Acc@5 99.756 (99.678)
Epoch: [82][20/25]	Time 0.674 (0.685)	Data 0.006 (0.036)	Loss 0.5272 (0.5412)	Acc@1 90.771 (90.374)	Acc@5 99.707 (99.679)
Epoch: [82][21/25]	Time 0.708 (0.686)	Data 0.004 (0.035)	Loss 0.5647 (0.5422)	Acc@1 89.014 (90.312)	Acc@5 99.756 (99.683)
Epoch: [82][22/25]	Time 0.735 (0.688)	Data 0.007 (0.034)	Loss 0.5484 (0.5425)	Acc@1 89.697 (90.285)	Acc@5 99.805 (99.688)
Epoch: [82][23/25]	Time 0.663 (0.687)	Data 0.005 (0.032)	Loss 0.5507 (0.5428)	Acc@1 89.307 (90.245)	Acc@5 99.854 (99.695)
Epoch: [82][24/25]	Time 0.380 (0.675)	Data 0.006 (0.031)	Loss 0.5892 (0.5436)	Acc@1 88.679 (90.218)	Acc@5 99.410 (99.690)

Epoch: [83 | 180] LR: 0.100000
Epoch: [83][0/25]	Time 0.681 (0.681)	Data 0.693 (0.693)	Loss 0.5251 (0.5251)	Acc@1 90.430 (90.430)	Acc@5 99.805 (99.805)
Epoch: [83][1/25]	Time 0.723 (0.702)	Data 0.005 (0.349)	Loss 0.5496 (0.5373)	Acc@1 90.674 (90.552)	Acc@5 99.756 (99.780)
Epoch: [83][2/25]	Time 0.736 (0.713)	Data 0.005 (0.234)	Loss 0.5184 (0.5310)	Acc@1 90.820 (90.641)	Acc@5 99.658 (99.740)
Epoch: [83][3/25]	Time 0.694 (0.708)	Data 0.005 (0.177)	Loss 0.5376 (0.5327)	Acc@1 90.576 (90.625)	Acc@5 99.756 (99.744)
Epoch: [83][4/25]	Time 0.695 (0.706)	Data 0.007 (0.143)	Loss 0.5315 (0.5324)	Acc@1 90.332 (90.566)	Acc@5 99.512 (99.697)
Epoch: [83][5/25]	Time 0.748 (0.713)	Data 0.004 (0.120)	Loss 0.5409 (0.5338)	Acc@1 90.381 (90.535)	Acc@5 99.854 (99.723)
Epoch: [83][6/25]	Time 0.761 (0.720)	Data 0.007 (0.104)	Loss 0.5630 (0.5380)	Acc@1 89.648 (90.409)	Acc@5 99.707 (99.721)
Epoch: [83][7/25]	Time 0.665 (0.713)	Data 0.006 (0.091)	Loss 0.5612 (0.5409)	Acc@1 89.697 (90.320)	Acc@5 99.512 (99.695)
Epoch: [83][8/25]	Time 0.633 (0.704)	Data 0.005 (0.082)	Loss 0.5455 (0.5414)	Acc@1 90.234 (90.310)	Acc@5 99.805 (99.707)
Epoch: [83][9/25]	Time 0.644 (0.698)	Data 0.005 (0.074)	Loss 0.5514 (0.5424)	Acc@1 90.039 (90.283)	Acc@5 99.561 (99.692)
Epoch: [83][10/25]	Time 0.730 (0.701)	Data 0.005 (0.068)	Loss 0.5568 (0.5437)	Acc@1 90.576 (90.310)	Acc@5 99.658 (99.689)
Epoch: [83][11/25]	Time 0.729 (0.703)	Data 0.006 (0.063)	Loss 0.5758 (0.5464)	Acc@1 89.404 (90.234)	Acc@5 99.707 (99.691)
Epoch: [83][12/25]	Time 0.674 (0.701)	Data 0.004 (0.058)	Loss 0.5428 (0.5461)	Acc@1 90.332 (90.242)	Acc@5 99.805 (99.700)
Epoch: [83][13/25]	Time 0.632 (0.696)	Data 0.005 (0.054)	Loss 0.5436 (0.5459)	Acc@1 89.746 (90.206)	Acc@5 99.707 (99.700)
Epoch: [83][14/25]	Time 0.660 (0.694)	Data 0.004 (0.051)	Loss 0.5649 (0.5472)	Acc@1 89.453 (90.156)	Acc@5 99.756 (99.704)
Epoch: [83][15/25]	Time 0.665 (0.692)	Data 0.006 (0.048)	Loss 0.5439 (0.5470)	Acc@1 90.479 (90.176)	Acc@5 99.512 (99.692)
Epoch: [83][16/25]	Time 0.735 (0.694)	Data 0.006 (0.046)	Loss 0.5549 (0.5475)	Acc@1 90.283 (90.183)	Acc@5 99.707 (99.693)
Epoch: [83][17/25]	Time 0.667 (0.693)	Data 0.006 (0.044)	Loss 0.5950 (0.5501)	Acc@1 88.477 (90.088)	Acc@5 99.609 (99.688)
Epoch: [83][18/25]	Time 0.673 (0.692)	Data 0.005 (0.042)	Loss 0.5628 (0.5508)	Acc@1 89.160 (90.039)	Acc@5 99.707 (99.689)
Epoch: [83][19/25]	Time 0.655 (0.690)	Data 0.004 (0.040)	Loss 0.5577 (0.5511)	Acc@1 89.941 (90.034)	Acc@5 99.561 (99.683)
Epoch: [83][20/25]	Time 0.673 (0.689)	Data 0.007 (0.038)	Loss 0.5555 (0.5513)	Acc@1 90.576 (90.060)	Acc@5 99.609 (99.679)
Epoch: [83][21/25]	Time 0.658 (0.688)	Data 0.006 (0.037)	Loss 0.5374 (0.5507)	Acc@1 90.625 (90.086)	Acc@5 99.658 (99.678)
Epoch: [83][22/25]	Time 0.767 (0.691)	Data 0.004 (0.035)	Loss 0.5518 (0.5507)	Acc@1 89.795 (90.073)	Acc@5 99.658 (99.677)
Epoch: [83][23/25]	Time 0.687 (0.691)	Data 0.004 (0.034)	Loss 0.5422 (0.5504)	Acc@1 89.648 (90.055)	Acc@5 99.561 (99.672)
Epoch: [83][24/25]	Time 0.376 (0.678)	Data 0.006 (0.033)	Loss 0.5982 (0.5512)	Acc@1 88.561 (90.030)	Acc@5 99.764 (99.674)

Epoch: [84 | 180] LR: 0.100000
Epoch: [84][0/25]	Time 0.702 (0.702)	Data 0.702 (0.702)	Loss 0.5641 (0.5641)	Acc@1 88.574 (88.574)	Acc@5 99.805 (99.805)
Epoch: [84][1/25]	Time 0.724 (0.713)	Data 0.007 (0.354)	Loss 0.5430 (0.5536)	Acc@1 89.941 (89.258)	Acc@5 99.707 (99.756)
Epoch: [84][2/25]	Time 0.763 (0.730)	Data 0.007 (0.239)	Loss 0.5607 (0.5559)	Acc@1 89.551 (89.355)	Acc@5 99.707 (99.740)
Epoch: [84][3/25]	Time 0.695 (0.721)	Data 0.007 (0.181)	Loss 0.5752 (0.5607)	Acc@1 88.916 (89.246)	Acc@5 99.707 (99.731)
Epoch: [84][4/25]	Time 0.698 (0.716)	Data 0.006 (0.146)	Loss 0.5755 (0.5637)	Acc@1 88.770 (89.150)	Acc@5 99.658 (99.717)
Epoch: [84][5/25]	Time 0.706 (0.715)	Data 0.007 (0.122)	Loss 0.5789 (0.5662)	Acc@1 88.672 (89.071)	Acc@5 99.756 (99.723)
Epoch: [84][6/25]	Time 0.668 (0.708)	Data 0.004 (0.106)	Loss 0.5656 (0.5661)	Acc@1 90.283 (89.244)	Acc@5 99.414 (99.679)
Epoch: [84][7/25]	Time 0.641 (0.700)	Data 0.003 (0.093)	Loss 0.5903 (0.5692)	Acc@1 88.330 (89.130)	Acc@5 99.707 (99.683)
Epoch: [84][8/25]	Time 0.685 (0.698)	Data 0.010 (0.083)	Loss 0.5574 (0.5679)	Acc@1 89.551 (89.176)	Acc@5 99.658 (99.680)
Epoch: [84][9/25]	Time 0.731 (0.701)	Data 0.005 (0.076)	Loss 0.5682 (0.5679)	Acc@1 89.111 (89.170)	Acc@5 99.658 (99.678)
Epoch: [84][10/25]	Time 0.717 (0.703)	Data 0.006 (0.069)	Loss 0.5784 (0.5688)	Acc@1 89.111 (89.165)	Acc@5 99.707 (99.680)
Epoch: [84][11/25]	Time 0.725 (0.705)	Data 0.010 (0.064)	Loss 0.5423 (0.5666)	Acc@1 90.137 (89.246)	Acc@5 99.561 (99.670)
Epoch: [84][12/25]	Time 0.694 (0.704)	Data 0.004 (0.060)	Loss 0.5492 (0.5653)	Acc@1 90.527 (89.344)	Acc@5 99.756 (99.677)
Epoch: [84][13/25]	Time 0.685 (0.702)	Data 0.005 (0.056)	Loss 0.5426 (0.5637)	Acc@1 91.211 (89.478)	Acc@5 99.658 (99.676)
Epoch: [84][14/25]	Time 0.701 (0.702)	Data 0.005 (0.052)	Loss 0.5900 (0.5654)	Acc@1 88.916 (89.440)	Acc@5 99.609 (99.671)
Epoch: [84][15/25]	Time 0.747 (0.705)	Data 0.005 (0.049)	Loss 0.5582 (0.5650)	Acc@1 89.111 (89.420)	Acc@5 99.658 (99.670)
Epoch: [84][16/25]	Time 0.725 (0.706)	Data 0.007 (0.047)	Loss 0.5405 (0.5635)	Acc@1 90.430 (89.479)	Acc@5 99.805 (99.678)
Epoch: [84][17/25]	Time 0.691 (0.705)	Data 0.005 (0.045)	Loss 0.5720 (0.5640)	Acc@1 89.258 (89.467)	Acc@5 99.658 (99.677)
Epoch: [84][18/25]	Time 0.697 (0.705)	Data 0.007 (0.043)	Loss 0.5582 (0.5637)	Acc@1 89.844 (89.487)	Acc@5 99.805 (99.684)
Epoch: [84][19/25]	Time 0.760 (0.708)	Data 0.004 (0.041)	Loss 0.5572 (0.5634)	Acc@1 89.453 (89.485)	Acc@5 99.658 (99.683)
Epoch: [84][20/25]	Time 0.666 (0.706)	Data 0.006 (0.039)	Loss 0.5677 (0.5636)	Acc@1 88.916 (89.458)	Acc@5 99.707 (99.684)
Epoch: [84][21/25]	Time 0.701 (0.706)	Data 0.007 (0.038)	Loss 0.5594 (0.5634)	Acc@1 89.746 (89.471)	Acc@5 99.854 (99.691)
Epoch: [84][22/25]	Time 0.681 (0.705)	Data 0.005 (0.036)	Loss 0.5407 (0.5624)	Acc@1 90.723 (89.525)	Acc@5 99.561 (99.686)
Epoch: [84][23/25]	Time 0.693 (0.704)	Data 0.005 (0.035)	Loss 0.5618 (0.5624)	Acc@1 89.404 (89.520)	Acc@5 99.658 (99.685)
Epoch: [84][24/25]	Time 0.424 (0.693)	Data 0.007 (0.034)	Loss 0.5333 (0.5619)	Acc@1 91.156 (89.548)	Acc@5 99.528 (99.682)

Epoch: [85 | 180] LR: 0.100000
Epoch: [85][0/25]	Time 0.752 (0.752)	Data 0.636 (0.636)	Loss 0.5553 (0.5553)	Acc@1 89.404 (89.404)	Acc@5 99.707 (99.707)
Epoch: [85][1/25]	Time 0.734 (0.743)	Data 0.005 (0.320)	Loss 0.5415 (0.5484)	Acc@1 90.479 (89.941)	Acc@5 99.756 (99.731)
Epoch: [85][2/25]	Time 0.690 (0.725)	Data 0.007 (0.216)	Loss 0.5509 (0.5492)	Acc@1 89.893 (89.925)	Acc@5 99.756 (99.740)
Epoch: [85][3/25]	Time 0.695 (0.718)	Data 0.003 (0.163)	Loss 0.5391 (0.5467)	Acc@1 90.430 (90.051)	Acc@5 99.805 (99.756)
Epoch: [85][4/25]	Time 0.715 (0.717)	Data 0.007 (0.131)	Loss 0.5447 (0.5463)	Acc@1 89.990 (90.039)	Acc@5 99.854 (99.775)
Epoch: [85][5/25]	Time 0.719 (0.717)	Data 0.007 (0.111)	Loss 0.5503 (0.5470)	Acc@1 90.381 (90.096)	Acc@5 99.561 (99.740)
Epoch: [85][6/25]	Time 0.697 (0.714)	Data 0.008 (0.096)	Loss 0.5700 (0.5503)	Acc@1 89.160 (89.962)	Acc@5 99.609 (99.721)
Epoch: [85][7/25]	Time 0.710 (0.714)	Data 0.004 (0.085)	Loss 0.5427 (0.5493)	Acc@1 90.332 (90.009)	Acc@5 99.854 (99.738)
Epoch: [85][8/25]	Time 0.740 (0.717)	Data 0.006 (0.076)	Loss 0.5453 (0.5489)	Acc@1 89.502 (89.952)	Acc@5 99.902 (99.756)
Epoch: [85][9/25]	Time 0.694 (0.715)	Data 0.007 (0.069)	Loss 0.5509 (0.5491)	Acc@1 90.234 (89.980)	Acc@5 99.609 (99.741)
Epoch: [85][10/25]	Time 0.709 (0.714)	Data 0.007 (0.063)	Loss 0.5293 (0.5473)	Acc@1 90.479 (90.026)	Acc@5 99.805 (99.747)
Epoch: [85][11/25]	Time 0.729 (0.715)	Data 0.006 (0.059)	Loss 0.5585 (0.5482)	Acc@1 89.453 (89.978)	Acc@5 99.707 (99.744)
Epoch: [85][12/25]	Time 0.696 (0.714)	Data 0.006 (0.054)	Loss 0.5749 (0.5503)	Acc@1 88.379 (89.855)	Acc@5 99.658 (99.737)
Epoch: [85][13/25]	Time 0.674 (0.711)	Data 0.007 (0.051)	Loss 0.5611 (0.5510)	Acc@1 89.404 (89.823)	Acc@5 99.805 (99.742)
Epoch: [85][14/25]	Time 0.714 (0.711)	Data 0.005 (0.048)	Loss 0.5606 (0.5517)	Acc@1 90.234 (89.850)	Acc@5 99.756 (99.743)
Epoch: [85][15/25]	Time 0.770 (0.715)	Data 0.008 (0.046)	Loss 0.5385 (0.5508)	Acc@1 90.771 (89.908)	Acc@5 99.609 (99.734)
Epoch: [85][16/25]	Time 0.656 (0.711)	Data 0.008 (0.043)	Loss 0.5599 (0.5514)	Acc@1 90.234 (89.927)	Acc@5 99.854 (99.741)
Epoch: [85][17/25]	Time 0.623 (0.707)	Data 0.006 (0.041)	Loss 0.5679 (0.5523)	Acc@1 89.453 (89.901)	Acc@5 99.512 (99.729)
Epoch: [85][18/25]	Time 0.639 (0.703)	Data 0.005 (0.039)	Loss 0.5534 (0.5524)	Acc@1 90.283 (89.921)	Acc@5 99.609 (99.722)
Epoch: [85][19/25]	Time 0.635 (0.700)	Data 0.004 (0.038)	Loss 0.5599 (0.5527)	Acc@1 90.430 (89.946)	Acc@5 99.756 (99.724)
Epoch: [85][20/25]	Time 0.640 (0.697)	Data 0.005 (0.036)	Loss 0.5498 (0.5526)	Acc@1 90.088 (89.953)	Acc@5 99.805 (99.728)
Epoch: [85][21/25]	Time 0.622 (0.693)	Data 0.007 (0.035)	Loss 0.5644 (0.5531)	Acc@1 89.893 (89.950)	Acc@5 99.609 (99.723)
Epoch: [85][22/25]	Time 0.626 (0.690)	Data 0.005 (0.033)	Loss 0.5476 (0.5529)	Acc@1 89.990 (89.952)	Acc@5 99.805 (99.726)
Epoch: [85][23/25]	Time 0.603 (0.687)	Data 0.005 (0.032)	Loss 0.5475 (0.5527)	Acc@1 90.869 (89.990)	Acc@5 100.000 (99.738)
Epoch: [85][24/25]	Time 0.331 (0.673)	Data 0.007 (0.031)	Loss 0.5719 (0.5530)	Acc@1 90.212 (89.994)	Acc@5 99.764 (99.738)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 444042 ; 487386 ; 0.9110684344646748

Epoch: [86 | 180] LR: 0.100000
Epoch: [86][0/25]	Time 0.712 (0.712)	Data 0.653 (0.653)	Loss 0.5248 (0.5248)	Acc@1 90.918 (90.918)	Acc@5 99.658 (99.658)
Epoch: [86][1/25]	Time 0.698 (0.705)	Data 0.007 (0.330)	Loss 0.5110 (0.5179)	Acc@1 91.406 (91.162)	Acc@5 99.756 (99.707)
Epoch: [86][2/25]	Time 0.734 (0.715)	Data 0.006 (0.222)	Loss 0.4849 (0.5069)	Acc@1 92.285 (91.536)	Acc@5 99.902 (99.772)
Epoch: [86][3/25]	Time 0.749 (0.723)	Data 0.006 (0.168)	Loss 0.4793 (0.5000)	Acc@1 92.383 (91.748)	Acc@5 99.902 (99.805)
Epoch: [86][4/25]	Time 0.702 (0.719)	Data 0.005 (0.135)	Loss 0.4809 (0.4962)	Acc@1 92.334 (91.865)	Acc@5 99.902 (99.824)
Epoch: [86][5/25]	Time 0.714 (0.718)	Data 0.009 (0.114)	Loss 0.4891 (0.4950)	Acc@1 92.529 (91.976)	Acc@5 99.805 (99.821)
Epoch: [86][6/25]	Time 0.699 (0.715)	Data 0.006 (0.099)	Loss 0.5048 (0.4964)	Acc@1 91.455 (91.902)	Acc@5 99.756 (99.812)
Epoch: [86][7/25]	Time 0.693 (0.712)	Data 0.004 (0.087)	Loss 0.4809 (0.4945)	Acc@1 92.188 (91.937)	Acc@5 99.805 (99.811)
Epoch: [86][8/25]	Time 0.682 (0.709)	Data 0.005 (0.078)	Loss 0.4871 (0.4936)	Acc@1 91.943 (91.938)	Acc@5 99.902 (99.821)
Epoch: [86][9/25]	Time 0.724 (0.711)	Data 0.005 (0.070)	Loss 0.5014 (0.4944)	Acc@1 91.260 (91.870)	Acc@5 99.805 (99.819)
Epoch: [86][10/25]	Time 0.738 (0.713)	Data 0.005 (0.064)	Loss 0.4979 (0.4947)	Acc@1 91.992 (91.881)	Acc@5 99.609 (99.800)
Epoch: [86][11/25]	Time 0.678 (0.710)	Data 0.008 (0.060)	Loss 0.5441 (0.4989)	Acc@1 90.771 (91.789)	Acc@5 99.756 (99.797)
Epoch: [86][12/25]	Time 0.628 (0.704)	Data 0.007 (0.056)	Loss 0.5088 (0.4996)	Acc@1 91.797 (91.789)	Acc@5 99.707 (99.790)
Epoch: [86][13/25]	Time 0.625 (0.698)	Data 0.004 (0.052)	Loss 0.5040 (0.4999)	Acc@1 91.211 (91.748)	Acc@5 99.707 (99.784)
Epoch: [86][14/25]	Time 0.662 (0.696)	Data 0.006 (0.049)	Loss 0.5070 (0.5004)	Acc@1 91.748 (91.748)	Acc@5 99.365 (99.756)
Epoch: [86][15/25]	Time 0.758 (0.700)	Data 0.008 (0.046)	Loss 0.5409 (0.5029)	Acc@1 89.941 (91.635)	Acc@5 99.805 (99.759)
Epoch: [86][16/25]	Time 0.678 (0.698)	Data 0.005 (0.044)	Loss 0.5551 (0.5060)	Acc@1 90.186 (91.550)	Acc@5 99.658 (99.753)
Epoch: [86][17/25]	Time 0.720 (0.700)	Data 0.005 (0.042)	Loss 0.5073 (0.5061)	Acc@1 91.504 (91.547)	Acc@5 99.756 (99.753)
Epoch: [86][18/25]	Time 0.709 (0.700)	Data 0.005 (0.040)	Loss 0.5080 (0.5062)	Acc@1 91.553 (91.548)	Acc@5 99.854 (99.758)
Epoch: [86][19/25]	Time 0.710 (0.701)	Data 0.007 (0.038)	Loss 0.5309 (0.5074)	Acc@1 90.771 (91.509)	Acc@5 99.756 (99.758)
Epoch: [86][20/25]	Time 0.695 (0.700)	Data 0.006 (0.037)	Loss 0.5308 (0.5085)	Acc@1 90.527 (91.462)	Acc@5 99.951 (99.767)
Epoch: [86][21/25]	Time 0.779 (0.704)	Data 0.004 (0.035)	Loss 0.5286 (0.5094)	Acc@1 90.723 (91.428)	Acc@5 99.756 (99.767)
Epoch: [86][22/25]	Time 0.666 (0.702)	Data 0.005 (0.034)	Loss 0.5321 (0.5104)	Acc@1 90.967 (91.408)	Acc@5 99.707 (99.764)
Epoch: [86][23/25]	Time 0.682 (0.701)	Data 0.005 (0.033)	Loss 0.5689 (0.5129)	Acc@1 89.160 (91.315)	Acc@5 99.756 (99.764)
Epoch: [86][24/25]	Time 0.333 (0.687)	Data 0.006 (0.032)	Loss 0.5948 (0.5143)	Acc@1 87.264 (91.246)	Acc@5 99.528 (99.760)

Epoch: [87 | 180] LR: 0.100000
Epoch: [87][0/25]	Time 0.732 (0.732)	Data 0.688 (0.688)	Loss 0.5043 (0.5043)	Acc@1 91.797 (91.797)	Acc@5 99.756 (99.756)
Epoch: [87][1/25]	Time 0.710 (0.721)	Data 0.004 (0.346)	Loss 0.5072 (0.5058)	Acc@1 91.309 (91.553)	Acc@5 99.805 (99.780)
Epoch: [87][2/25]	Time 0.658 (0.700)	Data 0.003 (0.232)	Loss 0.5458 (0.5191)	Acc@1 90.088 (91.064)	Acc@5 99.805 (99.788)
Epoch: [87][3/25]	Time 0.695 (0.699)	Data 0.005 (0.175)	Loss 0.5511 (0.5271)	Acc@1 89.697 (90.723)	Acc@5 99.658 (99.756)
Epoch: [87][4/25]	Time 0.715 (0.702)	Data 0.006 (0.141)	Loss 0.5420 (0.5301)	Acc@1 90.137 (90.605)	Acc@5 99.805 (99.766)
Epoch: [87][5/25]	Time 0.751 (0.710)	Data 0.009 (0.119)	Loss 0.5498 (0.5334)	Acc@1 89.941 (90.495)	Acc@5 99.756 (99.764)
Epoch: [87][6/25]	Time 0.673 (0.705)	Data 0.004 (0.103)	Loss 0.5538 (0.5363)	Acc@1 89.795 (90.395)	Acc@5 99.707 (99.756)
Epoch: [87][7/25]	Time 0.665 (0.700)	Data 0.007 (0.091)	Loss 0.5530 (0.5384)	Acc@1 90.625 (90.424)	Acc@5 99.707 (99.750)
Epoch: [87][8/25]	Time 0.726 (0.703)	Data 0.010 (0.082)	Loss 0.5330 (0.5378)	Acc@1 90.381 (90.419)	Acc@5 99.805 (99.756)
Epoch: [87][9/25]	Time 0.716 (0.704)	Data 0.004 (0.074)	Loss 0.5262 (0.5366)	Acc@1 90.869 (90.464)	Acc@5 99.756 (99.756)
Epoch: [87][10/25]	Time 0.663 (0.700)	Data 0.004 (0.068)	Loss 0.5558 (0.5384)	Acc@1 89.648 (90.390)	Acc@5 99.512 (99.734)
Epoch: [87][11/25]	Time 0.736 (0.703)	Data 0.008 (0.063)	Loss 0.5588 (0.5401)	Acc@1 89.600 (90.324)	Acc@5 99.707 (99.731)
Epoch: [87][12/25]	Time 0.728 (0.705)	Data 0.008 (0.059)	Loss 0.5545 (0.5412)	Acc@1 89.600 (90.268)	Acc@5 99.463 (99.711)
Epoch: [87][13/25]	Time 0.680 (0.703)	Data 0.007 (0.055)	Loss 0.5526 (0.5420)	Acc@1 89.746 (90.231)	Acc@5 99.658 (99.707)
Epoch: [87][14/25]	Time 0.711 (0.704)	Data 0.006 (0.052)	Loss 0.5626 (0.5434)	Acc@1 89.697 (90.195)	Acc@5 99.805 (99.714)
Epoch: [87][15/25]	Time 0.748 (0.707)	Data 0.004 (0.049)	Loss 0.5811 (0.5457)	Acc@1 89.355 (90.143)	Acc@5 99.756 (99.716)
Epoch: [87][16/25]	Time 0.697 (0.706)	Data 0.007 (0.046)	Loss 0.5691 (0.5471)	Acc@1 89.746 (90.119)	Acc@5 99.463 (99.701)
Epoch: [87][17/25]	Time 0.716 (0.706)	Data 0.005 (0.044)	Loss 0.5501 (0.5473)	Acc@1 90.186 (90.123)	Acc@5 99.658 (99.699)
Epoch: [87][18/25]	Time 0.668 (0.704)	Data 0.005 (0.042)	Loss 0.5508 (0.5475)	Acc@1 90.088 (90.121)	Acc@5 99.561 (99.692)
Epoch: [87][19/25]	Time 0.729 (0.706)	Data 0.005 (0.040)	Loss 0.5533 (0.5477)	Acc@1 89.551 (90.093)	Acc@5 99.756 (99.695)
Epoch: [87][20/25]	Time 0.746 (0.708)	Data 0.006 (0.038)	Loss 0.5446 (0.5476)	Acc@1 90.234 (90.100)	Acc@5 99.561 (99.688)
Epoch: [87][21/25]	Time 0.687 (0.707)	Data 0.006 (0.037)	Loss 0.5776 (0.5490)	Acc@1 89.844 (90.088)	Acc@5 99.561 (99.683)
Epoch: [87][22/25]	Time 0.706 (0.707)	Data 0.007 (0.035)	Loss 0.5402 (0.5486)	Acc@1 89.746 (90.073)	Acc@5 99.902 (99.692)
Epoch: [87][23/25]	Time 0.748 (0.708)	Data 0.004 (0.034)	Loss 0.5443 (0.5484)	Acc@1 90.430 (90.088)	Acc@5 99.658 (99.691)
Epoch: [87][24/25]	Time 0.449 (0.698)	Data 0.005 (0.033)	Loss 0.5279 (0.5481)	Acc@1 90.212 (90.090)	Acc@5 99.882 (99.694)

Epoch: [88 | 180] LR: 0.100000
Epoch: [88][0/25]	Time 0.706 (0.706)	Data 0.630 (0.630)	Loss 0.5144 (0.5144)	Acc@1 91.162 (91.162)	Acc@5 99.609 (99.609)
Epoch: [88][1/25]	Time 0.685 (0.695)	Data 0.006 (0.318)	Loss 0.5353 (0.5248)	Acc@1 90.723 (90.942)	Acc@5 99.707 (99.658)
Epoch: [88][2/25]	Time 0.672 (0.688)	Data 0.003 (0.213)	Loss 0.5245 (0.5247)	Acc@1 91.113 (90.999)	Acc@5 99.756 (99.691)
Epoch: [88][3/25]	Time 0.732 (0.699)	Data 0.005 (0.161)	Loss 0.5616 (0.5339)	Acc@1 89.600 (90.649)	Acc@5 99.561 (99.658)
Epoch: [88][4/25]	Time 0.677 (0.694)	Data 0.011 (0.131)	Loss 0.5660 (0.5403)	Acc@1 89.893 (90.498)	Acc@5 99.658 (99.658)
Epoch: [88][5/25]	Time 0.644 (0.686)	Data 0.008 (0.110)	Loss 0.5594 (0.5435)	Acc@1 90.137 (90.438)	Acc@5 99.658 (99.658)
Epoch: [88][6/25]	Time 0.644 (0.680)	Data 0.004 (0.095)	Loss 0.5727 (0.5477)	Acc@1 89.111 (90.248)	Acc@5 99.707 (99.665)
Epoch: [88][7/25]	Time 0.720 (0.685)	Data 0.005 (0.084)	Loss 0.5320 (0.5457)	Acc@1 90.869 (90.326)	Acc@5 99.756 (99.677)
Epoch: [88][8/25]	Time 0.763 (0.694)	Data 0.007 (0.075)	Loss 0.5652 (0.5479)	Acc@1 90.039 (90.294)	Acc@5 99.707 (99.680)
Epoch: [88][9/25]	Time 0.683 (0.693)	Data 0.005 (0.068)	Loss 0.5698 (0.5501)	Acc@1 89.209 (90.186)	Acc@5 99.463 (99.658)
Epoch: [88][10/25]	Time 0.711 (0.694)	Data 0.005 (0.063)	Loss 0.5380 (0.5490)	Acc@1 90.576 (90.221)	Acc@5 99.951 (99.685)
Epoch: [88][11/25]	Time 0.872 (0.709)	Data 0.004 (0.058)	Loss 0.5662 (0.5504)	Acc@1 89.941 (90.198)	Acc@5 99.561 (99.674)
Epoch: [88][12/25]	Time 0.807 (0.717)	Data 0.005 (0.054)	Loss 0.5651 (0.5515)	Acc@1 89.600 (90.152)	Acc@5 99.902 (99.692)
Epoch: [88][13/25]	Time 0.678 (0.714)	Data 0.006 (0.050)	Loss 0.5561 (0.5519)	Acc@1 89.355 (90.095)	Acc@5 99.609 (99.686)
Epoch: [88][14/25]	Time 0.665 (0.711)	Data 0.005 (0.047)	Loss 0.5425 (0.5512)	Acc@1 90.479 (90.120)	Acc@5 99.512 (99.674)
Epoch: [88][15/25]	Time 0.718 (0.711)	Data 0.007 (0.045)	Loss 0.5470 (0.5510)	Acc@1 90.674 (90.155)	Acc@5 99.609 (99.670)
Epoch: [88][16/25]	Time 0.716 (0.711)	Data 0.008 (0.043)	Loss 0.5595 (0.5515)	Acc@1 89.551 (90.119)	Acc@5 99.658 (99.670)
Epoch: [88][17/25]	Time 0.728 (0.712)	Data 0.004 (0.040)	Loss 0.5595 (0.5519)	Acc@1 89.111 (90.063)	Acc@5 99.707 (99.672)
Epoch: [88][18/25]	Time 0.724 (0.713)	Data 0.005 (0.039)	Loss 0.5799 (0.5534)	Acc@1 89.746 (90.047)	Acc@5 99.561 (99.666)
Epoch: [88][19/25]	Time 0.728 (0.714)	Data 0.005 (0.037)	Loss 0.5758 (0.5545)	Acc@1 89.453 (90.017)	Acc@5 99.707 (99.668)
Epoch: [88][20/25]	Time 0.612 (0.709)	Data 0.005 (0.035)	Loss 0.5810 (0.5558)	Acc@1 89.062 (89.972)	Acc@5 99.463 (99.658)
Epoch: [88][21/25]	Time 0.666 (0.707)	Data 0.005 (0.034)	Loss 0.5727 (0.5565)	Acc@1 89.307 (89.941)	Acc@5 99.561 (99.654)
Epoch: [88][22/25]	Time 0.663 (0.705)	Data 0.006 (0.033)	Loss 0.5998 (0.5584)	Acc@1 88.037 (89.859)	Acc@5 99.756 (99.658)
Epoch: [88][23/25]	Time 0.663 (0.703)	Data 0.004 (0.032)	Loss 0.5719 (0.5590)	Acc@1 89.551 (89.846)	Acc@5 99.561 (99.654)
Epoch: [88][24/25]	Time 0.411 (0.691)	Data 0.008 (0.031)	Loss 0.5695 (0.5592)	Acc@1 90.212 (89.852)	Acc@5 99.764 (99.656)

Epoch: [89 | 180] LR: 0.100000
Epoch: [89][0/25]	Time 0.611 (0.611)	Data 0.613 (0.613)	Loss 0.5430 (0.5430)	Acc@1 90.771 (90.771)	Acc@5 99.609 (99.609)
Epoch: [89][1/25]	Time 0.633 (0.622)	Data 0.009 (0.311)	Loss 0.5711 (0.5570)	Acc@1 89.404 (90.088)	Acc@5 99.463 (99.536)
Epoch: [89][2/25]	Time 0.645 (0.630)	Data 0.007 (0.210)	Loss 0.5535 (0.5559)	Acc@1 89.648 (89.941)	Acc@5 99.756 (99.609)
Epoch: [89][3/25]	Time 0.631 (0.630)	Data 0.006 (0.159)	Loss 0.5783 (0.5615)	Acc@1 89.551 (89.844)	Acc@5 99.658 (99.622)
Epoch: [89][4/25]	Time 0.552 (0.614)	Data 0.005 (0.128)	Loss 0.5529 (0.5597)	Acc@1 89.697 (89.814)	Acc@5 99.609 (99.619)
Epoch: [89][5/25]	Time 0.611 (0.614)	Data 0.008 (0.108)	Loss 0.5488 (0.5579)	Acc@1 89.697 (89.795)	Acc@5 99.854 (99.658)
Epoch: [89][6/25]	Time 0.659 (0.620)	Data 0.007 (0.093)	Loss 0.5488 (0.5566)	Acc@1 89.990 (89.823)	Acc@5 99.805 (99.679)
Epoch: [89][7/25]	Time 0.657 (0.625)	Data 0.004 (0.082)	Loss 0.5232 (0.5524)	Acc@1 91.113 (89.984)	Acc@5 99.854 (99.701)
Epoch: [89][8/25]	Time 0.581 (0.620)	Data 0.005 (0.074)	Loss 0.5460 (0.5517)	Acc@1 91.064 (90.104)	Acc@5 99.707 (99.702)
Epoch: [89][9/25]	Time 0.602 (0.618)	Data 0.006 (0.067)	Loss 0.5491 (0.5515)	Acc@1 89.746 (90.068)	Acc@5 99.902 (99.722)
Epoch: [89][10/25]	Time 0.636 (0.620)	Data 0.008 (0.062)	Loss 0.5661 (0.5528)	Acc@1 89.209 (89.990)	Acc@5 99.756 (99.725)
Epoch: [89][11/25]	Time 0.650 (0.622)	Data 0.004 (0.057)	Loss 0.5604 (0.5534)	Acc@1 89.404 (89.941)	Acc@5 99.756 (99.727)
Epoch: [89][12/25]	Time 0.631 (0.623)	Data 0.004 (0.053)	Loss 0.5434 (0.5527)	Acc@1 90.088 (89.953)	Acc@5 99.805 (99.733)
Epoch: [89][13/25]	Time 0.632 (0.624)	Data 0.008 (0.050)	Loss 0.5359 (0.5515)	Acc@1 90.088 (89.962)	Acc@5 99.609 (99.724)
Epoch: [89][14/25]	Time 0.653 (0.626)	Data 0.004 (0.047)	Loss 0.5810 (0.5534)	Acc@1 88.965 (89.896)	Acc@5 99.707 (99.723)
Epoch: [89][15/25]	Time 0.660 (0.628)	Data 0.009 (0.044)	Loss 0.5614 (0.5539)	Acc@1 88.574 (89.813)	Acc@5 99.707 (99.722)
Epoch: [89][16/25]	Time 0.597 (0.626)	Data 0.007 (0.042)	Loss 0.5962 (0.5564)	Acc@1 88.867 (89.758)	Acc@5 99.414 (99.704)
Epoch: [89][17/25]	Time 0.596 (0.624)	Data 0.007 (0.040)	Loss 0.5900 (0.5583)	Acc@1 88.965 (89.714)	Acc@5 99.561 (99.696)
Epoch: [89][18/25]	Time 0.662 (0.626)	Data 0.008 (0.038)	Loss 0.5856 (0.5597)	Acc@1 88.623 (89.656)	Acc@5 99.463 (99.684)
Epoch: [89][19/25]	Time 0.684 (0.629)	Data 0.006 (0.037)	Loss 0.5608 (0.5598)	Acc@1 89.258 (89.636)	Acc@5 99.805 (99.690)
Epoch: [89][20/25]	Time 0.600 (0.628)	Data 0.005 (0.035)	Loss 0.5691 (0.5602)	Acc@1 89.990 (89.653)	Acc@5 99.658 (99.688)
Epoch: [89][21/25]	Time 0.606 (0.627)	Data 0.005 (0.034)	Loss 0.5426 (0.5594)	Acc@1 90.479 (89.691)	Acc@5 99.756 (99.691)
Epoch: [89][22/25]	Time 0.614 (0.626)	Data 0.004 (0.033)	Loss 0.5971 (0.5611)	Acc@1 88.525 (89.640)	Acc@5 99.609 (99.688)
Epoch: [89][23/25]	Time 0.652 (0.627)	Data 0.006 (0.031)	Loss 0.5463 (0.5604)	Acc@1 90.283 (89.667)	Acc@5 99.707 (99.689)
Epoch: [89][24/25]	Time 0.438 (0.620)	Data 0.005 (0.030)	Loss 0.5250 (0.5598)	Acc@1 91.392 (89.696)	Acc@5 99.764 (99.690)

Epoch: [90 | 180] LR: 0.100000
Epoch: [90][0/25]	Time 0.641 (0.641)	Data 0.709 (0.709)	Loss 0.5049 (0.5049)	Acc@1 91.602 (91.602)	Acc@5 99.805 (99.805)
Epoch: [90][1/25]	Time 0.647 (0.644)	Data 0.005 (0.357)	Loss 0.5635 (0.5342)	Acc@1 89.453 (90.527)	Acc@5 99.609 (99.707)
Epoch: [90][2/25]	Time 0.700 (0.663)	Data 0.005 (0.240)	Loss 0.5590 (0.5425)	Acc@1 89.209 (90.088)	Acc@5 99.707 (99.707)
Epoch: [90][3/25]	Time 0.592 (0.645)	Data 0.008 (0.182)	Loss 0.5510 (0.5446)	Acc@1 90.234 (90.125)	Acc@5 99.609 (99.683)
Epoch: [90][4/25]	Time 0.624 (0.641)	Data 0.005 (0.147)	Loss 0.5316 (0.5420)	Acc@1 91.260 (90.352)	Acc@5 99.658 (99.678)
Epoch: [90][5/25]	Time 0.648 (0.642)	Data 0.006 (0.123)	Loss 0.5190 (0.5382)	Acc@1 91.211 (90.495)	Acc@5 99.854 (99.707)
Epoch: [90][6/25]	Time 0.597 (0.636)	Data 0.003 (0.106)	Loss 0.5637 (0.5418)	Acc@1 89.795 (90.395)	Acc@5 99.609 (99.693)
Epoch: [90][7/25]	Time 0.562 (0.626)	Data 0.005 (0.093)	Loss 0.5244 (0.5396)	Acc@1 91.162 (90.491)	Acc@5 99.951 (99.725)
Epoch: [90][8/25]	Time 0.604 (0.624)	Data 0.006 (0.084)	Loss 0.5647 (0.5424)	Acc@1 89.209 (90.348)	Acc@5 99.561 (99.707)
Epoch: [90][9/25]	Time 0.615 (0.623)	Data 0.006 (0.076)	Loss 0.5135 (0.5395)	Acc@1 91.504 (90.464)	Acc@5 99.707 (99.707)
Epoch: [90][10/25]	Time 0.637 (0.624)	Data 0.007 (0.070)	Loss 0.5676 (0.5421)	Acc@1 89.502 (90.376)	Acc@5 99.609 (99.698)
Epoch: [90][11/25]	Time 0.621 (0.624)	Data 0.006 (0.064)	Loss 0.5513 (0.5428)	Acc@1 90.283 (90.369)	Acc@5 99.805 (99.707)
Epoch: [90][12/25]	Time 0.823 (0.639)	Data 0.005 (0.060)	Loss 0.5485 (0.5433)	Acc@1 90.576 (90.385)	Acc@5 99.658 (99.703)
Epoch: [90][13/25]	Time 0.692 (0.643)	Data 0.005 (0.056)	Loss 0.5440 (0.5433)	Acc@1 90.332 (90.381)	Acc@5 99.707 (99.704)
Epoch: [90][14/25]	Time 0.619 (0.642)	Data 0.006 (0.053)	Loss 0.5256 (0.5421)	Acc@1 91.406 (90.449)	Acc@5 99.902 (99.717)
Epoch: [90][15/25]	Time 0.665 (0.643)	Data 0.005 (0.050)	Loss 0.5638 (0.5435)	Acc@1 90.039 (90.424)	Acc@5 99.512 (99.704)
Epoch: [90][16/25]	Time 0.615 (0.641)	Data 0.006 (0.047)	Loss 0.5435 (0.5435)	Acc@1 89.746 (90.384)	Acc@5 99.805 (99.710)
Epoch: [90][17/25]	Time 0.669 (0.643)	Data 0.006 (0.045)	Loss 0.5604 (0.5444)	Acc@1 89.941 (90.359)	Acc@5 99.658 (99.707)
Epoch: [90][18/25]	Time 0.617 (0.642)	Data 0.005 (0.043)	Loss 0.5602 (0.5453)	Acc@1 89.551 (90.317)	Acc@5 99.658 (99.704)
Epoch: [90][19/25]	Time 0.669 (0.643)	Data 0.004 (0.041)	Loss 0.5356 (0.5448)	Acc@1 90.674 (90.334)	Acc@5 99.463 (99.692)
Epoch: [90][20/25]	Time 0.636 (0.643)	Data 0.008 (0.039)	Loss 0.5710 (0.5460)	Acc@1 88.916 (90.267)	Acc@5 99.805 (99.698)
Epoch: [90][21/25]	Time 0.588 (0.640)	Data 0.006 (0.038)	Loss 0.5467 (0.5461)	Acc@1 90.234 (90.265)	Acc@5 99.707 (99.698)
Epoch: [90][22/25]	Time 0.678 (0.642)	Data 0.009 (0.036)	Loss 0.5916 (0.5480)	Acc@1 88.135 (90.173)	Acc@5 99.951 (99.709)
Epoch: [90][23/25]	Time 0.636 (0.642)	Data 0.007 (0.035)	Loss 0.5574 (0.5484)	Acc@1 89.990 (90.165)	Acc@5 99.609 (99.705)
Epoch: [90][24/25]	Time 0.347 (0.630)	Data 0.005 (0.034)	Loss 0.5555 (0.5486)	Acc@1 89.741 (90.158)	Acc@5 99.882 (99.708)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 442018 ; 487386 ; 0.9069156684845276

Epoch: [91 | 180] LR: 0.100000
Epoch: [91][0/25]	Time 0.628 (0.628)	Data 0.610 (0.610)	Loss 0.5174 (0.5174)	Acc@1 91.650 (91.650)	Acc@5 99.854 (99.854)
Epoch: [91][1/25]	Time 0.614 (0.621)	Data 0.006 (0.308)	Loss 0.5036 (0.5105)	Acc@1 91.699 (91.675)	Acc@5 99.902 (99.878)
Epoch: [91][2/25]	Time 0.630 (0.624)	Data 0.006 (0.207)	Loss 0.4842 (0.5017)	Acc@1 93.018 (92.122)	Acc@5 99.902 (99.886)
Epoch: [91][3/25]	Time 0.664 (0.634)	Data 0.005 (0.157)	Loss 0.4895 (0.4987)	Acc@1 92.627 (92.249)	Acc@5 99.756 (99.854)
Epoch: [91][4/25]	Time 0.566 (0.621)	Data 0.005 (0.126)	Loss 0.4881 (0.4966)	Acc@1 92.188 (92.236)	Acc@5 99.854 (99.854)
Epoch: [91][5/25]	Time 0.576 (0.613)	Data 0.008 (0.107)	Loss 0.4958 (0.4964)	Acc@1 91.895 (92.179)	Acc@5 99.854 (99.854)
Epoch: [91][6/25]	Time 0.598 (0.611)	Data 0.005 (0.092)	Loss 0.4845 (0.4947)	Acc@1 92.041 (92.160)	Acc@5 99.902 (99.860)
Epoch: [91][7/25]	Time 0.625 (0.613)	Data 0.007 (0.081)	Loss 0.4552 (0.4898)	Acc@1 93.604 (92.340)	Acc@5 99.951 (99.872)
Epoch: [91][8/25]	Time 0.683 (0.621)	Data 0.004 (0.073)	Loss 0.4952 (0.4904)	Acc@1 91.504 (92.247)	Acc@5 99.854 (99.870)
Epoch: [91][9/25]	Time 0.636 (0.622)	Data 0.006 (0.066)	Loss 0.4952 (0.4909)	Acc@1 92.188 (92.241)	Acc@5 99.609 (99.844)
Epoch: [91][10/25]	Time 0.652 (0.625)	Data 0.005 (0.061)	Loss 0.4767 (0.4896)	Acc@1 92.383 (92.254)	Acc@5 99.805 (99.840)
Epoch: [91][11/25]	Time 0.632 (0.625)	Data 0.005 (0.056)	Loss 0.4821 (0.4890)	Acc@1 91.943 (92.228)	Acc@5 99.805 (99.837)
Epoch: [91][12/25]	Time 0.678 (0.629)	Data 0.007 (0.052)	Loss 0.5161 (0.4911)	Acc@1 91.113 (92.142)	Acc@5 99.854 (99.838)
Epoch: [91][13/25]	Time 0.601 (0.627)	Data 0.006 (0.049)	Loss 0.5314 (0.4939)	Acc@1 90.137 (91.999)	Acc@5 99.707 (99.829)
Epoch: [91][14/25]	Time 0.620 (0.627)	Data 0.006 (0.046)	Loss 0.4904 (0.4937)	Acc@1 91.895 (91.992)	Acc@5 99.756 (99.824)
Epoch: [91][15/25]	Time 0.778 (0.636)	Data 0.006 (0.044)	Loss 0.5088 (0.4946)	Acc@1 91.211 (91.943)	Acc@5 99.854 (99.826)
Epoch: [91][16/25]	Time 0.749 (0.643)	Data 0.008 (0.041)	Loss 0.4914 (0.4944)	Acc@1 92.236 (91.961)	Acc@5 99.805 (99.825)
Epoch: [91][17/25]	Time 0.635 (0.643)	Data 0.005 (0.039)	Loss 0.5261 (0.4962)	Acc@1 90.869 (91.900)	Acc@5 99.658 (99.816)
Epoch: [91][18/25]	Time 0.595 (0.640)	Data 0.005 (0.038)	Loss 0.5263 (0.4978)	Acc@1 90.576 (91.830)	Acc@5 99.658 (99.807)
Epoch: [91][19/25]	Time 0.631 (0.640)	Data 0.008 (0.036)	Loss 0.5047 (0.4981)	Acc@1 90.820 (91.780)	Acc@5 99.805 (99.807)
Epoch: [91][20/25]	Time 0.607 (0.638)	Data 0.004 (0.035)	Loss 0.5175 (0.4991)	Acc@1 91.211 (91.753)	Acc@5 99.707 (99.802)
Epoch: [91][21/25]	Time 0.671 (0.640)	Data 0.007 (0.033)	Loss 0.5741 (0.5025)	Acc@1 89.062 (91.630)	Acc@5 99.658 (99.796)
Epoch: [91][22/25]	Time 0.641 (0.640)	Data 0.005 (0.032)	Loss 0.5138 (0.5030)	Acc@1 90.771 (91.593)	Acc@5 99.707 (99.792)
Epoch: [91][23/25]	Time 0.682 (0.641)	Data 0.007 (0.031)	Loss 0.5733 (0.5059)	Acc@1 89.307 (91.498)	Acc@5 99.561 (99.782)
Epoch: [91][24/25]	Time 0.380 (0.631)	Data 0.005 (0.030)	Loss 0.5422 (0.5065)	Acc@1 89.623 (91.466)	Acc@5 99.882 (99.784)

Epoch: [92 | 180] LR: 0.100000
Epoch: [92][0/25]	Time 0.659 (0.659)	Data 0.643 (0.643)	Loss 0.5223 (0.5223)	Acc@1 90.479 (90.479)	Acc@5 99.854 (99.854)
Epoch: [92][1/25]	Time 0.589 (0.624)	Data 0.007 (0.325)	Loss 0.5392 (0.5308)	Acc@1 89.844 (90.161)	Acc@5 99.756 (99.805)
Epoch: [92][2/25]	Time 0.605 (0.618)	Data 0.006 (0.218)	Loss 0.5207 (0.5274)	Acc@1 91.357 (90.560)	Acc@5 99.805 (99.805)
Epoch: [92][3/25]	Time 0.673 (0.632)	Data 0.008 (0.166)	Loss 0.5156 (0.5245)	Acc@1 90.869 (90.637)	Acc@5 99.756 (99.792)
Epoch: [92][4/25]	Time 0.625 (0.630)	Data 0.005 (0.134)	Loss 0.5541 (0.5304)	Acc@1 90.039 (90.518)	Acc@5 99.512 (99.736)
Epoch: [92][5/25]	Time 0.631 (0.630)	Data 0.006 (0.112)	Loss 0.5544 (0.5344)	Acc@1 89.941 (90.422)	Acc@5 99.707 (99.731)
Epoch: [92][6/25]	Time 0.644 (0.632)	Data 0.006 (0.097)	Loss 0.5468 (0.5362)	Acc@1 90.039 (90.367)	Acc@5 99.805 (99.742)
Epoch: [92][7/25]	Time 0.602 (0.629)	Data 0.006 (0.086)	Loss 0.5633 (0.5396)	Acc@1 90.088 (90.332)	Acc@5 99.609 (99.725)
Epoch: [92][8/25]	Time 0.624 (0.628)	Data 0.010 (0.077)	Loss 0.5669 (0.5426)	Acc@1 89.941 (90.289)	Acc@5 99.512 (99.702)
Epoch: [92][9/25]	Time 0.656 (0.631)	Data 0.006 (0.070)	Loss 0.5140 (0.5397)	Acc@1 90.918 (90.352)	Acc@5 99.805 (99.712)
Epoch: [92][10/25]	Time 0.646 (0.632)	Data 0.007 (0.065)	Loss 0.5283 (0.5387)	Acc@1 90.771 (90.390)	Acc@5 99.805 (99.720)
Epoch: [92][11/25]	Time 0.675 (0.636)	Data 0.005 (0.060)	Loss 0.5657 (0.5409)	Acc@1 89.551 (90.320)	Acc@5 99.707 (99.719)
Epoch: [92][12/25]	Time 0.589 (0.632)	Data 0.007 (0.056)	Loss 0.5303 (0.5401)	Acc@1 91.162 (90.385)	Acc@5 99.756 (99.722)
Epoch: [92][13/25]	Time 0.574 (0.628)	Data 0.004 (0.052)	Loss 0.5415 (0.5402)	Acc@1 89.990 (90.356)	Acc@5 99.561 (99.711)
Epoch: [92][14/25]	Time 0.595 (0.626)	Data 0.008 (0.049)	Loss 0.5637 (0.5418)	Acc@1 89.160 (90.277)	Acc@5 99.658 (99.707)
Epoch: [92][15/25]	Time 0.674 (0.629)	Data 0.007 (0.046)	Loss 0.5639 (0.5432)	Acc@1 89.453 (90.225)	Acc@5 99.609 (99.701)
Epoch: [92][16/25]	Time 0.596 (0.627)	Data 0.006 (0.044)	Loss 0.5228 (0.5420)	Acc@1 91.748 (90.315)	Acc@5 99.756 (99.704)
Epoch: [92][17/25]	Time 0.603 (0.626)	Data 0.005 (0.042)	Loss 0.5572 (0.5428)	Acc@1 90.137 (90.305)	Acc@5 99.561 (99.696)
Epoch: [92][18/25]	Time 0.631 (0.626)	Data 0.004 (0.040)	Loss 0.5746 (0.5445)	Acc@1 89.600 (90.268)	Acc@5 99.707 (99.697)
Epoch: [92][19/25]	Time 0.646 (0.627)	Data 0.006 (0.038)	Loss 0.5742 (0.5460)	Acc@1 89.062 (90.208)	Acc@5 99.805 (99.702)
Epoch: [92][20/25]	Time 0.601 (0.626)	Data 0.005 (0.036)	Loss 0.5409 (0.5457)	Acc@1 89.795 (90.188)	Acc@5 99.707 (99.702)
Epoch: [92][21/25]	Time 0.675 (0.628)	Data 0.006 (0.035)	Loss 0.5568 (0.5462)	Acc@1 89.941 (90.177)	Acc@5 99.854 (99.709)
Epoch: [92][22/25]	Time 0.674 (0.630)	Data 0.007 (0.034)	Loss 0.5794 (0.5477)	Acc@1 89.209 (90.135)	Acc@5 99.609 (99.705)
Epoch: [92][23/25]	Time 0.627 (0.630)	Data 0.005 (0.033)	Loss 0.5482 (0.5477)	Acc@1 90.625 (90.155)	Acc@5 99.658 (99.703)
Epoch: [92][24/25]	Time 0.345 (0.618)	Data 0.007 (0.032)	Loss 0.5252 (0.5473)	Acc@1 90.566 (90.162)	Acc@5 99.646 (99.702)

Epoch: [93 | 180] LR: 0.040000
Epoch: [93][0/25]	Time 0.644 (0.644)	Data 0.706 (0.706)	Loss 0.5359 (0.5359)	Acc@1 90.186 (90.186)	Acc@5 99.658 (99.658)
Epoch: [93][1/25]	Time 0.589 (0.616)	Data 0.005 (0.355)	Loss 0.5386 (0.5372)	Acc@1 90.576 (90.381)	Acc@5 99.707 (99.683)
Epoch: [93][2/25]	Time 0.678 (0.637)	Data 0.005 (0.238)	Loss 0.5446 (0.5397)	Acc@1 90.234 (90.332)	Acc@5 99.561 (99.642)
Epoch: [93][3/25]	Time 0.646 (0.639)	Data 0.005 (0.180)	Loss 0.5307 (0.5374)	Acc@1 91.211 (90.552)	Acc@5 99.561 (99.622)
Epoch: [93][4/25]	Time 0.582 (0.628)	Data 0.006 (0.145)	Loss 0.5262 (0.5352)	Acc@1 91.406 (90.723)	Acc@5 99.902 (99.678)
Epoch: [93][5/25]	Time 0.570 (0.618)	Data 0.006 (0.122)	Loss 0.5011 (0.5295)	Acc@1 91.992 (90.934)	Acc@5 99.854 (99.707)
Epoch: [93][6/25]	Time 0.589 (0.614)	Data 0.005 (0.105)	Loss 0.4814 (0.5226)	Acc@1 92.871 (91.211)	Acc@5 99.902 (99.735)
Epoch: [93][7/25]	Time 0.618 (0.614)	Data 0.005 (0.093)	Loss 0.4704 (0.5161)	Acc@1 92.676 (91.394)	Acc@5 99.902 (99.756)
Epoch: [93][8/25]	Time 0.677 (0.621)	Data 0.007 (0.083)	Loss 0.4441 (0.5081)	Acc@1 93.945 (91.678)	Acc@5 99.951 (99.778)
Epoch: [93][9/25]	Time 0.607 (0.620)	Data 0.007 (0.076)	Loss 0.4645 (0.5038)	Acc@1 93.359 (91.846)	Acc@5 99.756 (99.775)
Epoch: [93][10/25]	Time 0.598 (0.618)	Data 0.004 (0.069)	Loss 0.4626 (0.5000)	Acc@1 93.359 (91.983)	Acc@5 99.805 (99.778)
Epoch: [93][11/25]	Time 0.624 (0.618)	Data 0.007 (0.064)	Loss 0.4687 (0.4974)	Acc@1 93.213 (92.086)	Acc@5 99.658 (99.768)
Epoch: [93][12/25]	Time 0.655 (0.621)	Data 0.006 (0.059)	Loss 0.4722 (0.4955)	Acc@1 93.066 (92.161)	Acc@5 99.805 (99.771)
Epoch: [93][13/25]	Time 0.633 (0.622)	Data 0.004 (0.056)	Loss 0.4714 (0.4938)	Acc@1 92.383 (92.177)	Acc@5 99.854 (99.777)
Epoch: [93][14/25]	Time 0.654 (0.624)	Data 0.005 (0.052)	Loss 0.4728 (0.4924)	Acc@1 92.920 (92.227)	Acc@5 100.000 (99.792)
Epoch: [93][15/25]	Time 0.615 (0.624)	Data 0.005 (0.049)	Loss 0.4404 (0.4891)	Acc@1 94.287 (92.355)	Acc@5 99.902 (99.799)
Epoch: [93][16/25]	Time 0.649 (0.625)	Data 0.006 (0.047)	Loss 0.4716 (0.4881)	Acc@1 93.018 (92.394)	Acc@5 99.951 (99.808)
Epoch: [93][17/25]	Time 0.669 (0.627)	Data 0.006 (0.044)	Loss 0.4369 (0.4852)	Acc@1 93.848 (92.475)	Acc@5 99.756 (99.805)
Epoch: [93][18/25]	Time 0.616 (0.627)	Data 0.006 (0.042)	Loss 0.4434 (0.4830)	Acc@1 94.092 (92.560)	Acc@5 99.951 (99.812)
Epoch: [93][19/25]	Time 0.630 (0.627)	Data 0.006 (0.041)	Loss 0.4465 (0.4812)	Acc@1 93.799 (92.622)	Acc@5 99.854 (99.814)
Epoch: [93][20/25]	Time 0.649 (0.628)	Data 0.004 (0.039)	Loss 0.4244 (0.4785)	Acc@1 94.385 (92.706)	Acc@5 99.756 (99.812)
Epoch: [93][21/25]	Time 0.645 (0.629)	Data 0.004 (0.037)	Loss 0.4360 (0.4766)	Acc@1 93.896 (92.760)	Acc@5 100.000 (99.820)
Epoch: [93][22/25]	Time 0.570 (0.626)	Data 0.007 (0.036)	Loss 0.4373 (0.4749)	Acc@1 93.701 (92.801)	Acc@5 99.951 (99.826)
Epoch: [93][23/25]	Time 0.580 (0.624)	Data 0.004 (0.035)	Loss 0.4216 (0.4726)	Acc@1 94.775 (92.883)	Acc@5 99.902 (99.829)
Epoch: [93][24/25]	Time 0.335 (0.613)	Data 0.004 (0.033)	Loss 0.4232 (0.4718)	Acc@1 94.929 (92.918)	Acc@5 99.646 (99.826)

Epoch: [94 | 180] LR: 0.040000
Epoch: [94][0/25]	Time 0.628 (0.628)	Data 0.671 (0.671)	Loss 0.4205 (0.4205)	Acc@1 94.629 (94.629)	Acc@5 100.000 (100.000)
Epoch: [94][1/25]	Time 0.605 (0.616)	Data 0.005 (0.338)	Loss 0.4251 (0.4228)	Acc@1 94.336 (94.482)	Acc@5 99.756 (99.878)
Epoch: [94][2/25]	Time 0.634 (0.622)	Data 0.007 (0.228)	Loss 0.4286 (0.4247)	Acc@1 94.287 (94.417)	Acc@5 100.000 (99.919)
Epoch: [94][3/25]	Time 0.649 (0.629)	Data 0.008 (0.173)	Loss 0.4084 (0.4206)	Acc@1 94.727 (94.495)	Acc@5 99.854 (99.902)
Epoch: [94][4/25]	Time 0.572 (0.618)	Data 0.008 (0.140)	Loss 0.4268 (0.4219)	Acc@1 94.629 (94.521)	Acc@5 99.951 (99.912)
Epoch: [94][5/25]	Time 0.603 (0.615)	Data 0.006 (0.118)	Loss 0.4115 (0.4201)	Acc@1 95.215 (94.637)	Acc@5 99.854 (99.902)
Epoch: [94][6/25]	Time 0.652 (0.621)	Data 0.009 (0.102)	Loss 0.4410 (0.4231)	Acc@1 93.701 (94.503)	Acc@5 99.854 (99.895)
Epoch: [94][7/25]	Time 0.639 (0.623)	Data 0.004 (0.090)	Loss 0.4261 (0.4235)	Acc@1 94.629 (94.519)	Acc@5 99.756 (99.878)
Epoch: [94][8/25]	Time 0.646 (0.625)	Data 0.004 (0.080)	Loss 0.4194 (0.4230)	Acc@1 94.141 (94.477)	Acc@5 100.000 (99.891)
Epoch: [94][9/25]	Time 0.670 (0.630)	Data 0.007 (0.073)	Loss 0.4295 (0.4237)	Acc@1 94.775 (94.507)	Acc@5 99.854 (99.888)
Epoch: [94][10/25]	Time 0.631 (0.630)	Data 0.004 (0.067)	Loss 0.4063 (0.4221)	Acc@1 95.312 (94.580)	Acc@5 99.902 (99.889)
Epoch: [94][11/25]	Time 0.674 (0.634)	Data 0.007 (0.062)	Loss 0.4129 (0.4213)	Acc@1 94.922 (94.609)	Acc@5 99.854 (99.886)
Epoch: [94][12/25]	Time 0.635 (0.634)	Data 0.006 (0.057)	Loss 0.4014 (0.4198)	Acc@1 94.775 (94.621)	Acc@5 99.854 (99.884)
Epoch: [94][13/25]	Time 0.664 (0.636)	Data 0.008 (0.054)	Loss 0.4014 (0.4185)	Acc@1 94.775 (94.632)	Acc@5 99.902 (99.885)
Epoch: [94][14/25]	Time 0.647 (0.637)	Data 0.005 (0.051)	Loss 0.4120 (0.4181)	Acc@1 95.215 (94.671)	Acc@5 99.854 (99.883)
Epoch: [94][15/25]	Time 0.617 (0.635)	Data 0.008 (0.048)	Loss 0.4145 (0.4178)	Acc@1 94.727 (94.675)	Acc@5 99.902 (99.884)
Epoch: [94][16/25]	Time 0.626 (0.635)	Data 0.005 (0.045)	Loss 0.4111 (0.4174)	Acc@1 95.361 (94.715)	Acc@5 100.000 (99.891)
Epoch: [94][17/25]	Time 0.633 (0.635)	Data 0.007 (0.043)	Loss 0.4001 (0.4165)	Acc@1 94.775 (94.718)	Acc@5 99.902 (99.891)
Epoch: [94][18/25]	Time 0.670 (0.637)	Data 0.005 (0.041)	Loss 0.4153 (0.4164)	Acc@1 94.629 (94.714)	Acc@5 99.951 (99.895)
Epoch: [94][19/25]	Time 0.588 (0.634)	Data 0.006 (0.040)	Loss 0.3842 (0.4148)	Acc@1 96.045 (94.780)	Acc@5 99.902 (99.895)
Epoch: [94][20/25]	Time 0.621 (0.634)	Data 0.006 (0.038)	Loss 0.3944 (0.4138)	Acc@1 95.752 (94.827)	Acc@5 99.951 (99.898)
Epoch: [94][21/25]	Time 0.630 (0.633)	Data 0.005 (0.036)	Loss 0.3901 (0.4127)	Acc@1 95.898 (94.875)	Acc@5 99.756 (99.891)
Epoch: [94][22/25]	Time 0.679 (0.635)	Data 0.005 (0.035)	Loss 0.4072 (0.4125)	Acc@1 95.020 (94.882)	Acc@5 99.902 (99.892)
Epoch: [94][23/25]	Time 0.573 (0.633)	Data 0.004 (0.034)	Loss 0.3813 (0.4112)	Acc@1 95.850 (94.922)	Acc@5 99.902 (99.892)
Epoch: [94][24/25]	Time 0.338 (0.621)	Data 0.005 (0.033)	Loss 0.3956 (0.4109)	Acc@1 95.991 (94.940)	Acc@5 100.000 (99.894)

Epoch: [95 | 180] LR: 0.040000
Epoch: [95][0/25]	Time 0.626 (0.626)	Data 0.627 (0.627)	Loss 0.3938 (0.3938)	Acc@1 95.605 (95.605)	Acc@5 99.902 (99.902)
Epoch: [95][1/25]	Time 0.563 (0.594)	Data 0.009 (0.318)	Loss 0.3911 (0.3924)	Acc@1 95.654 (95.630)	Acc@5 99.951 (99.927)
Epoch: [95][2/25]	Time 0.630 (0.606)	Data 0.003 (0.213)	Loss 0.3908 (0.3919)	Acc@1 95.508 (95.589)	Acc@5 100.000 (99.951)
Epoch: [95][3/25]	Time 0.654 (0.618)	Data 0.003 (0.161)	Loss 0.3970 (0.3932)	Acc@1 95.117 (95.471)	Acc@5 100.000 (99.963)
Epoch: [95][4/25]	Time 0.678 (0.630)	Data 0.007 (0.130)	Loss 0.3869 (0.3919)	Acc@1 95.654 (95.508)	Acc@5 99.951 (99.961)
Epoch: [95][5/25]	Time 0.662 (0.635)	Data 0.007 (0.109)	Loss 0.4026 (0.3937)	Acc@1 95.117 (95.443)	Acc@5 99.805 (99.935)
Epoch: [95][6/25]	Time 0.609 (0.632)	Data 0.004 (0.094)	Loss 0.3954 (0.3939)	Acc@1 95.361 (95.431)	Acc@5 99.854 (99.923)
Epoch: [95][7/25]	Time 0.654 (0.634)	Data 0.005 (0.083)	Loss 0.3808 (0.3923)	Acc@1 95.947 (95.496)	Acc@5 99.902 (99.921)
Epoch: [95][8/25]	Time 0.623 (0.633)	Data 0.006 (0.075)	Loss 0.3753 (0.3904)	Acc@1 95.996 (95.551)	Acc@5 99.902 (99.919)
Epoch: [95][9/25]	Time 0.602 (0.630)	Data 0.008 (0.068)	Loss 0.3969 (0.3910)	Acc@1 95.215 (95.518)	Acc@5 99.805 (99.907)
Epoch: [95][10/25]	Time 0.646 (0.631)	Data 0.010 (0.063)	Loss 0.3991 (0.3918)	Acc@1 95.361 (95.503)	Acc@5 99.951 (99.911)
Epoch: [95][11/25]	Time 0.696 (0.637)	Data 0.007 (0.058)	Loss 0.3780 (0.3906)	Acc@1 96.387 (95.577)	Acc@5 99.805 (99.902)
Epoch: [95][12/25]	Time 0.598 (0.634)	Data 0.007 (0.054)	Loss 0.4074 (0.3919)	Acc@1 95.117 (95.542)	Acc@5 99.902 (99.902)
Epoch: [95][13/25]	Time 0.600 (0.631)	Data 0.005 (0.051)	Loss 0.3667 (0.3901)	Acc@1 96.631 (95.619)	Acc@5 100.000 (99.909)
Epoch: [95][14/25]	Time 0.631 (0.631)	Data 0.006 (0.048)	Loss 0.3769 (0.3892)	Acc@1 96.191 (95.658)	Acc@5 99.854 (99.906)
Epoch: [95][15/25]	Time 0.630 (0.631)	Data 0.007 (0.045)	Loss 0.3796 (0.3886)	Acc@1 96.143 (95.688)	Acc@5 99.854 (99.902)
Epoch: [95][16/25]	Time 0.630 (0.631)	Data 0.005 (0.043)	Loss 0.3742 (0.3878)	Acc@1 96.387 (95.729)	Acc@5 100.000 (99.908)
Epoch: [95][17/25]	Time 0.597 (0.629)	Data 0.005 (0.041)	Loss 0.3978 (0.3883)	Acc@1 95.264 (95.703)	Acc@5 99.854 (99.905)
Epoch: [95][18/25]	Time 0.631 (0.629)	Data 0.004 (0.039)	Loss 0.3799 (0.3879)	Acc@1 95.361 (95.685)	Acc@5 99.902 (99.905)
Epoch: [95][19/25]	Time 0.661 (0.631)	Data 0.005 (0.037)	Loss 0.3758 (0.3873)	Acc@1 96.191 (95.710)	Acc@5 99.951 (99.907)
Epoch: [95][20/25]	Time 0.589 (0.629)	Data 0.006 (0.035)	Loss 0.3764 (0.3868)	Acc@1 95.654 (95.708)	Acc@5 100.000 (99.912)
Epoch: [95][21/25]	Time 0.609 (0.628)	Data 0.007 (0.034)	Loss 0.3824 (0.3866)	Acc@1 95.654 (95.705)	Acc@5 99.951 (99.913)
Epoch: [95][22/25]	Time 0.665 (0.630)	Data 0.007 (0.033)	Loss 0.3725 (0.3860)	Acc@1 95.996 (95.718)	Acc@5 99.951 (99.915)
Epoch: [95][23/25]	Time 0.669 (0.631)	Data 0.005 (0.032)	Loss 0.3830 (0.3858)	Acc@1 96.191 (95.738)	Acc@5 100.000 (99.919)
Epoch: [95][24/25]	Time 0.339 (0.620)	Data 0.005 (0.031)	Loss 0.3985 (0.3861)	Acc@1 95.519 (95.734)	Acc@5 99.882 (99.918)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 441440 ; 487386 ; 0.9057297501364422

Epoch: [96 | 180] LR: 0.040000
Epoch: [96][0/25]	Time 0.578 (0.578)	Data 0.803 (0.803)	Loss 0.3730 (0.3730)	Acc@1 95.850 (95.850)	Acc@5 99.951 (99.951)
Epoch: [96][1/25]	Time 0.617 (0.597)	Data 0.005 (0.404)	Loss 0.3684 (0.3707)	Acc@1 96.436 (96.143)	Acc@5 99.951 (99.951)
Epoch: [96][2/25]	Time 0.650 (0.615)	Data 0.005 (0.271)	Loss 0.3751 (0.3722)	Acc@1 95.801 (96.029)	Acc@5 99.951 (99.951)
Epoch: [96][3/25]	Time 0.669 (0.628)	Data 0.005 (0.204)	Loss 0.3662 (0.3707)	Acc@1 95.947 (96.008)	Acc@5 99.902 (99.939)
Epoch: [96][4/25]	Time 0.579 (0.618)	Data 0.008 (0.165)	Loss 0.3758 (0.3717)	Acc@1 95.654 (95.938)	Acc@5 99.951 (99.941)
Epoch: [96][5/25]	Time 0.549 (0.607)	Data 0.006 (0.139)	Loss 0.3684 (0.3711)	Acc@1 96.143 (95.972)	Acc@5 99.951 (99.943)
Epoch: [96][6/25]	Time 0.634 (0.611)	Data 0.006 (0.120)	Loss 0.3716 (0.3712)	Acc@1 96.289 (96.017)	Acc@5 100.000 (99.951)
Epoch: [96][7/25]	Time 0.658 (0.617)	Data 0.004 (0.105)	Loss 0.3828 (0.3727)	Acc@1 95.752 (95.984)	Acc@5 99.854 (99.939)
Epoch: [96][8/25]	Time 0.627 (0.618)	Data 0.006 (0.094)	Loss 0.3838 (0.3739)	Acc@1 95.654 (95.947)	Acc@5 99.805 (99.924)
Epoch: [96][9/25]	Time 0.561 (0.612)	Data 0.005 (0.085)	Loss 0.3621 (0.3727)	Acc@1 96.289 (95.981)	Acc@5 100.000 (99.932)
Epoch: [96][10/25]	Time 0.591 (0.610)	Data 0.005 (0.078)	Loss 0.3811 (0.3735)	Acc@1 95.850 (95.969)	Acc@5 99.902 (99.929)
Epoch: [96][11/25]	Time 0.645 (0.613)	Data 0.006 (0.072)	Loss 0.3821 (0.3742)	Acc@1 95.459 (95.927)	Acc@5 99.854 (99.923)
Epoch: [96][12/25]	Time 0.687 (0.619)	Data 0.005 (0.067)	Loss 0.3723 (0.3741)	Acc@1 96.289 (95.955)	Acc@5 99.854 (99.917)
Epoch: [96][13/25]	Time 0.568 (0.615)	Data 0.005 (0.062)	Loss 0.3837 (0.3747)	Acc@1 95.068 (95.891)	Acc@5 99.951 (99.920)
Epoch: [96][14/25]	Time 0.588 (0.613)	Data 0.007 (0.059)	Loss 0.3771 (0.3749)	Acc@1 95.850 (95.889)	Acc@5 100.000 (99.925)
Epoch: [96][15/25]	Time 0.612 (0.613)	Data 0.008 (0.055)	Loss 0.3754 (0.3749)	Acc@1 96.582 (95.932)	Acc@5 99.902 (99.924)
Epoch: [96][16/25]	Time 0.615 (0.613)	Data 0.005 (0.052)	Loss 0.3684 (0.3746)	Acc@1 95.996 (95.936)	Acc@5 99.902 (99.922)
Epoch: [96][17/25]	Time 0.645 (0.615)	Data 0.006 (0.050)	Loss 0.3863 (0.3752)	Acc@1 95.264 (95.898)	Acc@5 99.902 (99.921)
Epoch: [96][18/25]	Time 0.621 (0.615)	Data 0.006 (0.048)	Loss 0.3770 (0.3753)	Acc@1 95.654 (95.886)	Acc@5 99.951 (99.923)
Epoch: [96][19/25]	Time 0.664 (0.618)	Data 0.005 (0.045)	Loss 0.3751 (0.3753)	Acc@1 95.801 (95.881)	Acc@5 100.000 (99.927)
Epoch: [96][20/25]	Time 0.633 (0.618)	Data 0.007 (0.044)	Loss 0.3832 (0.3757)	Acc@1 95.166 (95.847)	Acc@5 99.902 (99.926)
Epoch: [96][21/25]	Time 0.669 (0.621)	Data 0.004 (0.042)	Loss 0.3731 (0.3755)	Acc@1 95.898 (95.850)	Acc@5 99.854 (99.922)
Epoch: [96][22/25]	Time 0.659 (0.622)	Data 0.004 (0.040)	Loss 0.3769 (0.3756)	Acc@1 95.654 (95.841)	Acc@5 100.000 (99.926)
Epoch: [96][23/25]	Time 0.657 (0.624)	Data 0.004 (0.039)	Loss 0.3663 (0.3752)	Acc@1 96.387 (95.864)	Acc@5 100.000 (99.929)
Epoch: [96][24/25]	Time 0.396 (0.615)	Data 0.005 (0.037)	Loss 0.3563 (0.3749)	Acc@1 96.816 (95.880)	Acc@5 99.764 (99.926)

Epoch: [97 | 180] LR: 0.040000
Epoch: [97][0/25]	Time 0.674 (0.674)	Data 0.593 (0.593)	Loss 0.3626 (0.3626)	Acc@1 96.240 (96.240)	Acc@5 99.902 (99.902)
Epoch: [97][1/25]	Time 0.577 (0.626)	Data 0.005 (0.299)	Loss 0.3543 (0.3585)	Acc@1 96.680 (96.460)	Acc@5 100.000 (99.951)
Epoch: [97][2/25]	Time 0.573 (0.608)	Data 0.004 (0.201)	Loss 0.3757 (0.3642)	Acc@1 95.898 (96.273)	Acc@5 99.854 (99.919)
Epoch: [97][3/25]	Time 0.605 (0.607)	Data 0.005 (0.152)	Loss 0.3667 (0.3648)	Acc@1 96.338 (96.289)	Acc@5 99.854 (99.902)
Epoch: [97][4/25]	Time 0.623 (0.610)	Data 0.005 (0.122)	Loss 0.3764 (0.3672)	Acc@1 95.898 (96.211)	Acc@5 99.951 (99.912)
Epoch: [97][5/25]	Time 0.646 (0.616)	Data 0.006 (0.103)	Loss 0.3786 (0.3691)	Acc@1 95.850 (96.151)	Acc@5 99.951 (99.919)
Epoch: [97][6/25]	Time 0.585 (0.612)	Data 0.003 (0.089)	Loss 0.3536 (0.3669)	Acc@1 96.631 (96.219)	Acc@5 99.951 (99.923)
Epoch: [97][7/25]	Time 0.584 (0.608)	Data 0.004 (0.078)	Loss 0.3601 (0.3660)	Acc@1 96.436 (96.246)	Acc@5 99.902 (99.921)
Epoch: [97][8/25]	Time 0.619 (0.609)	Data 0.005 (0.070)	Loss 0.3551 (0.3648)	Acc@1 96.533 (96.278)	Acc@5 99.902 (99.919)
Epoch: [97][9/25]	Time 0.590 (0.608)	Data 0.006 (0.064)	Loss 0.3626 (0.3646)	Acc@1 96.143 (96.265)	Acc@5 99.951 (99.922)
Epoch: [97][10/25]	Time 0.644 (0.611)	Data 0.005 (0.058)	Loss 0.3528 (0.3635)	Acc@1 96.387 (96.276)	Acc@5 100.000 (99.929)
Epoch: [97][11/25]	Time 0.658 (0.615)	Data 0.005 (0.054)	Loss 0.3688 (0.3640)	Acc@1 96.582 (96.301)	Acc@5 100.000 (99.935)
Epoch: [97][12/25]	Time 0.663 (0.618)	Data 0.006 (0.050)	Loss 0.3629 (0.3639)	Acc@1 96.289 (96.300)	Acc@5 100.000 (99.940)
Epoch: [97][13/25]	Time 0.650 (0.621)	Data 0.007 (0.047)	Loss 0.3572 (0.3634)	Acc@1 96.582 (96.320)	Acc@5 99.902 (99.937)
Epoch: [97][14/25]	Time 0.665 (0.624)	Data 0.007 (0.044)	Loss 0.3663 (0.3636)	Acc@1 96.094 (96.305)	Acc@5 100.000 (99.941)
Epoch: [97][15/25]	Time 0.647 (0.625)	Data 0.004 (0.042)	Loss 0.3612 (0.3634)	Acc@1 96.338 (96.307)	Acc@5 99.902 (99.939)
Epoch: [97][16/25]	Time 0.677 (0.628)	Data 0.008 (0.040)	Loss 0.3658 (0.3636)	Acc@1 96.436 (96.315)	Acc@5 99.854 (99.934)
Epoch: [97][17/25]	Time 0.590 (0.626)	Data 0.004 (0.038)	Loss 0.3666 (0.3637)	Acc@1 96.143 (96.305)	Acc@5 99.951 (99.935)
Epoch: [97][18/25]	Time 0.598 (0.625)	Data 0.006 (0.036)	Loss 0.3560 (0.3633)	Acc@1 96.582 (96.320)	Acc@5 100.000 (99.938)
Epoch: [97][19/25]	Time 0.641 (0.625)	Data 0.006 (0.035)	Loss 0.3530 (0.3628)	Acc@1 96.582 (96.333)	Acc@5 99.951 (99.939)
Epoch: [97][20/25]	Time 0.642 (0.626)	Data 0.004 (0.033)	Loss 0.3593 (0.3627)	Acc@1 96.436 (96.338)	Acc@5 99.854 (99.935)
Epoch: [97][21/25]	Time 0.645 (0.627)	Data 0.005 (0.032)	Loss 0.3564 (0.3624)	Acc@1 96.240 (96.333)	Acc@5 99.902 (99.933)
Epoch: [97][22/25]	Time 0.649 (0.628)	Data 0.005 (0.031)	Loss 0.3523 (0.3619)	Acc@1 97.070 (96.365)	Acc@5 99.902 (99.932)
Epoch: [97][23/25]	Time 0.679 (0.630)	Data 0.006 (0.030)	Loss 0.3661 (0.3621)	Acc@1 95.508 (96.330)	Acc@5 99.951 (99.933)
Epoch: [97][24/25]	Time 0.324 (0.618)	Data 0.007 (0.029)	Loss 0.3855 (0.3625)	Acc@1 95.401 (96.314)	Acc@5 100.000 (99.934)

Epoch: [98 | 180] LR: 0.040000
Epoch: [98][0/25]	Time 0.648 (0.648)	Data 0.627 (0.627)	Loss 0.3599 (0.3599)	Acc@1 96.045 (96.045)	Acc@5 99.951 (99.951)
Epoch: [98][1/25]	Time 0.580 (0.614)	Data 0.008 (0.317)	Loss 0.3547 (0.3573)	Acc@1 96.777 (96.411)	Acc@5 100.000 (99.976)
Epoch: [98][2/25]	Time 0.646 (0.625)	Data 0.005 (0.213)	Loss 0.3530 (0.3559)	Acc@1 96.436 (96.419)	Acc@5 100.000 (99.984)
Epoch: [98][3/25]	Time 0.665 (0.635)	Data 0.005 (0.161)	Loss 0.3470 (0.3536)	Acc@1 96.582 (96.460)	Acc@5 100.000 (99.988)
Epoch: [98][4/25]	Time 0.655 (0.639)	Data 0.006 (0.130)	Loss 0.3597 (0.3549)	Acc@1 96.338 (96.436)	Acc@5 99.951 (99.980)
Epoch: [98][5/25]	Time 0.658 (0.642)	Data 0.005 (0.109)	Loss 0.3600 (0.3557)	Acc@1 96.436 (96.436)	Acc@5 99.902 (99.967)
Epoch: [98][6/25]	Time 0.602 (0.636)	Data 0.003 (0.094)	Loss 0.3506 (0.3550)	Acc@1 96.777 (96.484)	Acc@5 100.000 (99.972)
Epoch: [98][7/25]	Time 0.658 (0.639)	Data 0.006 (0.083)	Loss 0.3700 (0.3568)	Acc@1 96.240 (96.454)	Acc@5 99.902 (99.963)
Epoch: [98][8/25]	Time 0.673 (0.643)	Data 0.007 (0.075)	Loss 0.3490 (0.3560)	Acc@1 96.729 (96.484)	Acc@5 99.951 (99.962)
Epoch: [98][9/25]	Time 0.590 (0.638)	Data 0.006 (0.068)	Loss 0.3648 (0.3569)	Acc@1 96.338 (96.470)	Acc@5 99.854 (99.951)
Epoch: [98][10/25]	Time 0.593 (0.634)	Data 0.007 (0.062)	Loss 0.3527 (0.3565)	Acc@1 96.484 (96.471)	Acc@5 99.854 (99.942)
Epoch: [98][11/25]	Time 0.672 (0.637)	Data 0.006 (0.058)	Loss 0.3599 (0.3568)	Acc@1 96.094 (96.440)	Acc@5 99.854 (99.935)
Epoch: [98][12/25]	Time 0.681 (0.640)	Data 0.005 (0.054)	Loss 0.3457 (0.3559)	Acc@1 96.973 (96.481)	Acc@5 100.000 (99.940)
Epoch: [98][13/25]	Time 0.579 (0.636)	Data 0.006 (0.050)	Loss 0.3536 (0.3557)	Acc@1 96.484 (96.481)	Acc@5 99.902 (99.937)
Epoch: [98][14/25]	Time 0.661 (0.637)	Data 0.008 (0.047)	Loss 0.3514 (0.3555)	Acc@1 96.924 (96.510)	Acc@5 99.902 (99.935)
Epoch: [98][15/25]	Time 0.649 (0.638)	Data 0.005 (0.045)	Loss 0.3477 (0.3550)	Acc@1 96.631 (96.518)	Acc@5 99.951 (99.936)
Epoch: [98][16/25]	Time 0.620 (0.637)	Data 0.007 (0.043)	Loss 0.3521 (0.3548)	Acc@1 96.045 (96.490)	Acc@5 100.000 (99.940)
Epoch: [98][17/25]	Time 0.636 (0.637)	Data 0.004 (0.040)	Loss 0.3587 (0.3550)	Acc@1 95.850 (96.455)	Acc@5 100.000 (99.943)
Epoch: [98][18/25]	Time 0.661 (0.638)	Data 0.006 (0.039)	Loss 0.3440 (0.3544)	Acc@1 96.826 (96.474)	Acc@5 99.902 (99.941)
Epoch: [98][19/25]	Time 0.567 (0.635)	Data 0.004 (0.037)	Loss 0.3461 (0.3540)	Acc@1 96.631 (96.482)	Acc@5 99.951 (99.941)
Epoch: [98][20/25]	Time 0.606 (0.633)	Data 0.007 (0.035)	Loss 0.3511 (0.3539)	Acc@1 96.631 (96.489)	Acc@5 99.951 (99.942)
Epoch: [98][21/25]	Time 0.634 (0.633)	Data 0.005 (0.034)	Loss 0.3489 (0.3536)	Acc@1 96.729 (96.500)	Acc@5 99.902 (99.940)
Epoch: [98][22/25]	Time 0.687 (0.636)	Data 0.007 (0.033)	Loss 0.3617 (0.3540)	Acc@1 95.752 (96.467)	Acc@5 99.902 (99.938)
Epoch: [98][23/25]	Time 0.579 (0.633)	Data 0.004 (0.032)	Loss 0.3696 (0.3546)	Acc@1 95.752 (96.438)	Acc@5 99.902 (99.937)
Epoch: [98][24/25]	Time 0.331 (0.621)	Data 0.005 (0.031)	Loss 0.3525 (0.3546)	Acc@1 96.462 (96.438)	Acc@5 99.882 (99.936)

Epoch: [99 | 180] LR: 0.040000
Epoch: [99][0/25]	Time 0.624 (0.624)	Data 0.591 (0.591)	Loss 0.3426 (0.3426)	Acc@1 97.168 (97.168)	Acc@5 99.854 (99.854)
Epoch: [99][1/25]	Time 0.674 (0.649)	Data 0.007 (0.299)	Loss 0.3609 (0.3518)	Acc@1 96.436 (96.802)	Acc@5 99.902 (99.878)
Epoch: [99][2/25]	Time 0.580 (0.626)	Data 0.009 (0.202)	Loss 0.3272 (0.3436)	Acc@1 97.119 (96.908)	Acc@5 100.000 (99.919)
Epoch: [99][3/25]	Time 0.583 (0.615)	Data 0.006 (0.153)	Loss 0.3388 (0.3424)	Acc@1 97.461 (97.046)	Acc@5 99.951 (99.927)
Epoch: [99][4/25]	Time 0.592 (0.610)	Data 0.005 (0.124)	Loss 0.3406 (0.3420)	Acc@1 96.875 (97.012)	Acc@5 99.902 (99.922)
Epoch: [99][5/25]	Time 0.584 (0.606)	Data 0.006 (0.104)	Loss 0.3362 (0.3410)	Acc@1 97.266 (97.054)	Acc@5 99.902 (99.919)
Epoch: [99][6/25]	Time 0.588 (0.603)	Data 0.004 (0.090)	Loss 0.3493 (0.3422)	Acc@1 95.996 (96.903)	Acc@5 100.000 (99.930)
Epoch: [99][7/25]	Time 0.568 (0.599)	Data 0.005 (0.079)	Loss 0.3414 (0.3421)	Acc@1 96.875 (96.899)	Acc@5 99.951 (99.933)
Epoch: [99][8/25]	Time 0.566 (0.595)	Data 0.005 (0.071)	Loss 0.3404 (0.3419)	Acc@1 97.266 (96.940)	Acc@5 99.951 (99.935)
Epoch: [99][9/25]	Time 0.664 (0.602)	Data 0.006 (0.064)	Loss 0.3292 (0.3406)	Acc@1 97.119 (96.958)	Acc@5 100.000 (99.941)
Epoch: [99][10/25]	Time 0.642 (0.606)	Data 0.008 (0.059)	Loss 0.3344 (0.3401)	Acc@1 97.119 (96.973)	Acc@5 100.000 (99.947)
Epoch: [99][11/25]	Time 0.597 (0.605)	Data 0.004 (0.055)	Loss 0.3523 (0.3411)	Acc@1 96.289 (96.916)	Acc@5 99.951 (99.947)
Epoch: [99][12/25]	Time 0.601 (0.605)	Data 0.007 (0.051)	Loss 0.3317 (0.3404)	Acc@1 96.924 (96.916)	Acc@5 100.000 (99.951)
Epoch: [99][13/25]	Time 0.647 (0.608)	Data 0.004 (0.048)	Loss 0.3411 (0.3404)	Acc@1 96.777 (96.906)	Acc@5 99.951 (99.951)
Epoch: [99][14/25]	Time 0.646 (0.610)	Data 0.007 (0.045)	Loss 0.3291 (0.3397)	Acc@1 97.119 (96.921)	Acc@5 100.000 (99.954)
Epoch: [99][15/25]	Time 0.721 (0.617)	Data 0.006 (0.043)	Loss 0.3546 (0.3406)	Acc@1 96.436 (96.890)	Acc@5 100.000 (99.957)
Epoch: [99][16/25]	Time 0.751 (0.625)	Data 0.006 (0.040)	Loss 0.3554 (0.3415)	Acc@1 96.338 (96.858)	Acc@5 99.854 (99.951)
Epoch: [99][17/25]	Time 0.722 (0.631)	Data 0.004 (0.038)	Loss 0.3361 (0.3412)	Acc@1 97.314 (96.883)	Acc@5 100.000 (99.954)
Epoch: [99][18/25]	Time 0.650 (0.632)	Data 0.004 (0.037)	Loss 0.3494 (0.3416)	Acc@1 96.582 (96.867)	Acc@5 100.000 (99.956)
Epoch: [99][19/25]	Time 0.660 (0.633)	Data 0.004 (0.035)	Loss 0.3337 (0.3412)	Acc@1 97.119 (96.880)	Acc@5 99.902 (99.954)
Epoch: [99][20/25]	Time 0.584 (0.631)	Data 0.004 (0.034)	Loss 0.3501 (0.3416)	Acc@1 96.387 (96.856)	Acc@5 99.951 (99.953)
Epoch: [99][21/25]	Time 0.601 (0.629)	Data 0.005 (0.032)	Loss 0.3465 (0.3419)	Acc@1 96.680 (96.848)	Acc@5 99.902 (99.951)
Epoch: [99][22/25]	Time 0.661 (0.631)	Data 0.005 (0.031)	Loss 0.3404 (0.3418)	Acc@1 96.875 (96.850)	Acc@5 99.902 (99.949)
Epoch: [99][23/25]	Time 0.685 (0.633)	Data 0.005 (0.030)	Loss 0.3547 (0.3423)	Acc@1 96.191 (96.822)	Acc@5 99.951 (99.949)
Epoch: [99][24/25]	Time 0.391 (0.623)	Data 0.005 (0.029)	Loss 0.3602 (0.3426)	Acc@1 95.637 (96.802)	Acc@5 100.000 (99.950)

Epoch: [100 | 180] LR: 0.040000
Epoch: [100][0/25]	Time 0.680 (0.680)	Data 0.748 (0.748)	Loss 0.3415 (0.3415)	Acc@1 96.924 (96.924)	Acc@5 99.951 (99.951)
Epoch: [100][1/25]	Time 0.579 (0.630)	Data 0.003 (0.375)	Loss 0.3237 (0.3326)	Acc@1 97.363 (97.144)	Acc@5 100.000 (99.976)
Epoch: [100][2/25]	Time 0.577 (0.612)	Data 0.004 (0.252)	Loss 0.3384 (0.3346)	Acc@1 96.924 (97.070)	Acc@5 99.902 (99.951)
Epoch: [100][3/25]	Time 0.635 (0.618)	Data 0.006 (0.190)	Loss 0.3315 (0.3338)	Acc@1 97.021 (97.058)	Acc@5 99.902 (99.939)
Epoch: [100][4/25]	Time 0.688 (0.632)	Data 0.006 (0.153)	Loss 0.3157 (0.3302)	Acc@1 97.852 (97.217)	Acc@5 100.000 (99.951)
Epoch: [100][5/25]	Time 0.586 (0.624)	Data 0.008 (0.129)	Loss 0.3398 (0.3318)	Acc@1 96.533 (97.103)	Acc@5 99.854 (99.935)
Epoch: [100][6/25]	Time 0.557 (0.615)	Data 0.005 (0.111)	Loss 0.3237 (0.3306)	Acc@1 97.803 (97.203)	Acc@5 100.000 (99.944)
Epoch: [100][7/25]	Time 0.664 (0.621)	Data 0.004 (0.098)	Loss 0.3404 (0.3319)	Acc@1 96.875 (97.162)	Acc@5 99.951 (99.945)
Epoch: [100][8/25]	Time 0.667 (0.626)	Data 0.005 (0.088)	Loss 0.3335 (0.3320)	Acc@1 97.510 (97.201)	Acc@5 99.902 (99.940)
Epoch: [100][9/25]	Time 0.606 (0.624)	Data 0.005 (0.079)	Loss 0.3436 (0.3332)	Acc@1 97.021 (97.183)	Acc@5 99.951 (99.941)
Epoch: [100][10/25]	Time 0.631 (0.625)	Data 0.005 (0.073)	Loss 0.3451 (0.3343)	Acc@1 96.924 (97.159)	Acc@5 99.902 (99.938)
Epoch: [100][11/25]	Time 0.662 (0.628)	Data 0.006 (0.067)	Loss 0.3428 (0.3350)	Acc@1 96.338 (97.091)	Acc@5 99.951 (99.939)
Epoch: [100][12/25]	Time 0.611 (0.626)	Data 0.005 (0.062)	Loss 0.3229 (0.3341)	Acc@1 97.314 (97.108)	Acc@5 99.951 (99.940)
Epoch: [100][13/25]	Time 0.646 (0.628)	Data 0.007 (0.058)	Loss 0.3246 (0.3334)	Acc@1 97.412 (97.130)	Acc@5 99.951 (99.941)
Epoch: [100][14/25]	Time 0.603 (0.626)	Data 0.004 (0.055)	Loss 0.3479 (0.3343)	Acc@1 96.191 (97.067)	Acc@5 99.854 (99.935)
Epoch: [100][15/25]	Time 0.651 (0.628)	Data 0.005 (0.052)	Loss 0.3310 (0.3341)	Acc@1 97.119 (97.070)	Acc@5 99.951 (99.936)
Epoch: [100][16/25]	Time 0.658 (0.629)	Data 0.006 (0.049)	Loss 0.3265 (0.3337)	Acc@1 96.875 (97.059)	Acc@5 99.951 (99.937)
Epoch: [100][17/25]	Time 0.657 (0.631)	Data 0.007 (0.047)	Loss 0.3271 (0.3333)	Acc@1 96.973 (97.054)	Acc@5 99.951 (99.938)
Epoch: [100][18/25]	Time 0.556 (0.627)	Data 0.005 (0.044)	Loss 0.3282 (0.3330)	Acc@1 96.924 (97.047)	Acc@5 99.951 (99.938)
Epoch: [100][19/25]	Time 0.562 (0.624)	Data 0.007 (0.043)	Loss 0.3496 (0.3339)	Acc@1 96.436 (97.017)	Acc@5 100.000 (99.941)
Epoch: [100][20/25]	Time 0.628 (0.624)	Data 0.005 (0.041)	Loss 0.3351 (0.3339)	Acc@1 96.484 (96.991)	Acc@5 100.000 (99.944)
Epoch: [100][21/25]	Time 0.673 (0.626)	Data 0.006 (0.039)	Loss 0.3336 (0.3339)	Acc@1 96.826 (96.984)	Acc@5 99.951 (99.945)
Epoch: [100][22/25]	Time 0.637 (0.627)	Data 0.005 (0.038)	Loss 0.3298 (0.3337)	Acc@1 97.412 (97.002)	Acc@5 99.902 (99.943)
Epoch: [100][23/25]	Time 0.624 (0.627)	Data 0.005 (0.036)	Loss 0.3434 (0.3341)	Acc@1 96.045 (96.962)	Acc@5 100.000 (99.945)
Epoch: [100][24/25]	Time 0.420 (0.618)	Data 0.006 (0.035)	Loss 0.3531 (0.3345)	Acc@1 95.637 (96.940)	Acc@5 99.882 (99.944)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 440286 ; 487386 ; 0.903362016963967

Epoch: [101 | 180] LR: 0.040000
Epoch: [101][0/25]	Time 0.659 (0.659)	Data 0.610 (0.610)	Loss 0.3274 (0.3274)	Acc@1 97.119 (97.119)	Acc@5 99.951 (99.951)
Epoch: [101][1/25]	Time 0.633 (0.646)	Data 0.005 (0.307)	Loss 0.3280 (0.3277)	Acc@1 97.070 (97.095)	Acc@5 100.000 (99.976)
Epoch: [101][2/25]	Time 0.624 (0.639)	Data 0.004 (0.206)	Loss 0.3262 (0.3272)	Acc@1 97.119 (97.103)	Acc@5 99.902 (99.951)
Epoch: [101][3/25]	Time 0.668 (0.646)	Data 0.007 (0.156)	Loss 0.3094 (0.3228)	Acc@1 98.389 (97.424)	Acc@5 99.951 (99.951)
Epoch: [101][4/25]	Time 0.649 (0.647)	Data 0.005 (0.126)	Loss 0.3296 (0.3241)	Acc@1 97.021 (97.344)	Acc@5 99.951 (99.951)
Epoch: [101][5/25]	Time 0.650 (0.647)	Data 0.006 (0.106)	Loss 0.3198 (0.3234)	Acc@1 97.266 (97.331)	Acc@5 99.951 (99.951)
Epoch: [101][6/25]	Time 0.607 (0.641)	Data 0.007 (0.092)	Loss 0.3313 (0.3245)	Acc@1 96.973 (97.280)	Acc@5 100.000 (99.958)
Epoch: [101][7/25]	Time 0.657 (0.643)	Data 0.007 (0.081)	Loss 0.3233 (0.3244)	Acc@1 97.510 (97.308)	Acc@5 100.000 (99.963)
Epoch: [101][8/25]	Time 0.674 (0.647)	Data 0.006 (0.073)	Loss 0.3402 (0.3261)	Acc@1 96.582 (97.228)	Acc@5 99.902 (99.957)
Epoch: [101][9/25]	Time 0.654 (0.647)	Data 0.007 (0.066)	Loss 0.3282 (0.3263)	Acc@1 96.729 (97.178)	Acc@5 99.951 (99.956)
Epoch: [101][10/25]	Time 0.654 (0.648)	Data 0.004 (0.061)	Loss 0.3243 (0.3262)	Acc@1 97.363 (97.195)	Acc@5 99.951 (99.956)
Epoch: [101][11/25]	Time 0.670 (0.650)	Data 0.007 (0.056)	Loss 0.3334 (0.3268)	Acc@1 96.826 (97.164)	Acc@5 100.000 (99.959)
Epoch: [101][12/25]	Time 0.674 (0.652)	Data 0.005 (0.052)	Loss 0.3295 (0.3270)	Acc@1 97.217 (97.168)	Acc@5 100.000 (99.962)
Epoch: [101][13/25]	Time 0.623 (0.650)	Data 0.007 (0.049)	Loss 0.3148 (0.3261)	Acc@1 97.705 (97.206)	Acc@5 99.902 (99.958)
Epoch: [101][14/25]	Time 0.634 (0.649)	Data 0.005 (0.046)	Loss 0.3197 (0.3257)	Acc@1 97.559 (97.230)	Acc@5 99.951 (99.958)
Epoch: [101][15/25]	Time 0.660 (0.649)	Data 0.004 (0.044)	Loss 0.3259 (0.3257)	Acc@1 97.217 (97.229)	Acc@5 99.902 (99.954)
Epoch: [101][16/25]	Time 0.648 (0.649)	Data 0.005 (0.041)	Loss 0.3442 (0.3268)	Acc@1 96.484 (97.185)	Acc@5 99.951 (99.954)
Epoch: [101][17/25]	Time 0.651 (0.649)	Data 0.005 (0.039)	Loss 0.3299 (0.3270)	Acc@1 96.729 (97.160)	Acc@5 99.902 (99.951)
Epoch: [101][18/25]	Time 0.676 (0.651)	Data 0.006 (0.038)	Loss 0.3111 (0.3261)	Acc@1 98.096 (97.209)	Acc@5 100.000 (99.954)
Epoch: [101][19/25]	Time 0.595 (0.648)	Data 0.006 (0.036)	Loss 0.3309 (0.3264)	Acc@1 96.729 (97.185)	Acc@5 100.000 (99.956)
Epoch: [101][20/25]	Time 0.618 (0.647)	Data 0.006 (0.034)	Loss 0.3479 (0.3274)	Acc@1 96.289 (97.142)	Acc@5 99.951 (99.956)
Epoch: [101][21/25]	Time 0.640 (0.646)	Data 0.008 (0.033)	Loss 0.3343 (0.3277)	Acc@1 96.973 (97.135)	Acc@5 99.902 (99.953)
Epoch: [101][22/25]	Time 0.684 (0.648)	Data 0.005 (0.032)	Loss 0.3201 (0.3274)	Acc@1 97.656 (97.157)	Acc@5 99.951 (99.953)
Epoch: [101][23/25]	Time 0.575 (0.645)	Data 0.005 (0.031)	Loss 0.3403 (0.3279)	Acc@1 96.289 (97.121)	Acc@5 100.000 (99.955)
Epoch: [101][24/25]	Time 0.334 (0.632)	Data 0.005 (0.030)	Loss 0.3329 (0.3280)	Acc@1 96.226 (97.106)	Acc@5 99.882 (99.954)

Epoch: [102 | 180] LR: 0.040000
Epoch: [102][0/25]	Time 0.629 (0.629)	Data 0.632 (0.632)	Loss 0.3197 (0.3197)	Acc@1 97.510 (97.510)	Acc@5 99.951 (99.951)
Epoch: [102][1/25]	Time 0.562 (0.596)	Data 0.006 (0.319)	Loss 0.3329 (0.3263)	Acc@1 97.070 (97.290)	Acc@5 99.951 (99.951)
Epoch: [102][2/25]	Time 0.606 (0.599)	Data 0.004 (0.214)	Loss 0.3171 (0.3232)	Acc@1 97.119 (97.233)	Acc@5 100.000 (99.967)
Epoch: [102][3/25]	Time 0.655 (0.613)	Data 0.005 (0.162)	Loss 0.3212 (0.3227)	Acc@1 97.656 (97.339)	Acc@5 99.951 (99.963)
Epoch: [102][4/25]	Time 0.649 (0.620)	Data 0.006 (0.131)	Loss 0.3274 (0.3236)	Acc@1 97.607 (97.393)	Acc@5 99.951 (99.961)
Epoch: [102][5/25]	Time 0.597 (0.616)	Data 0.006 (0.110)	Loss 0.3185 (0.3228)	Acc@1 97.705 (97.445)	Acc@5 99.902 (99.951)
Epoch: [102][6/25]	Time 0.647 (0.621)	Data 0.006 (0.095)	Loss 0.3139 (0.3215)	Acc@1 97.461 (97.447)	Acc@5 99.951 (99.951)
Epoch: [102][7/25]	Time 0.644 (0.624)	Data 0.008 (0.084)	Loss 0.3343 (0.3231)	Acc@1 97.070 (97.400)	Acc@5 99.951 (99.951)
Epoch: [102][8/25]	Time 0.589 (0.620)	Data 0.007 (0.075)	Loss 0.3176 (0.3225)	Acc@1 97.314 (97.390)	Acc@5 99.951 (99.951)
Epoch: [102][9/25]	Time 0.615 (0.619)	Data 0.008 (0.069)	Loss 0.3139 (0.3216)	Acc@1 97.607 (97.412)	Acc@5 99.951 (99.951)
Epoch: [102][10/25]	Time 0.654 (0.622)	Data 0.009 (0.063)	Loss 0.3161 (0.3211)	Acc@1 97.510 (97.421)	Acc@5 100.000 (99.956)
Epoch: [102][11/25]	Time 0.638 (0.624)	Data 0.005 (0.058)	Loss 0.3229 (0.3213)	Acc@1 97.217 (97.404)	Acc@5 100.000 (99.959)
Epoch: [102][12/25]	Time 0.680 (0.628)	Data 0.005 (0.054)	Loss 0.3211 (0.3213)	Acc@1 97.314 (97.397)	Acc@5 100.000 (99.962)
Epoch: [102][13/25]	Time 0.635 (0.629)	Data 0.005 (0.051)	Loss 0.3286 (0.3218)	Acc@1 96.631 (97.342)	Acc@5 99.902 (99.958)
Epoch: [102][14/25]	Time 0.596 (0.626)	Data 0.008 (0.048)	Loss 0.3243 (0.3220)	Acc@1 97.217 (97.334)	Acc@5 100.000 (99.961)
Epoch: [102][15/25]	Time 0.591 (0.624)	Data 0.005 (0.045)	Loss 0.3215 (0.3219)	Acc@1 97.461 (97.342)	Acc@5 100.000 (99.963)
Epoch: [102][16/25]	Time 0.633 (0.625)	Data 0.008 (0.043)	Loss 0.3182 (0.3217)	Acc@1 97.559 (97.355)	Acc@5 99.951 (99.963)
Epoch: [102][17/25]	Time 0.658 (0.627)	Data 0.007 (0.041)	Loss 0.3197 (0.3216)	Acc@1 97.363 (97.355)	Acc@5 99.951 (99.962)
Epoch: [102][18/25]	Time 0.590 (0.625)	Data 0.008 (0.039)	Loss 0.3379 (0.3225)	Acc@1 96.973 (97.335)	Acc@5 99.902 (99.959)
Epoch: [102][19/25]	Time 0.579 (0.622)	Data 0.005 (0.038)	Loss 0.3222 (0.3224)	Acc@1 97.119 (97.324)	Acc@5 100.000 (99.961)
Epoch: [102][20/25]	Time 0.638 (0.623)	Data 0.008 (0.036)	Loss 0.3259 (0.3226)	Acc@1 96.973 (97.307)	Acc@5 99.951 (99.960)
Epoch: [102][21/25]	Time 0.653 (0.624)	Data 0.006 (0.035)	Loss 0.3180 (0.3224)	Acc@1 97.217 (97.303)	Acc@5 99.951 (99.960)
Epoch: [102][22/25]	Time 0.578 (0.622)	Data 0.004 (0.033)	Loss 0.3227 (0.3224)	Acc@1 97.168 (97.297)	Acc@5 100.000 (99.962)
Epoch: [102][23/25]	Time 0.608 (0.622)	Data 0.006 (0.032)	Loss 0.3293 (0.3227)	Acc@1 96.777 (97.276)	Acc@5 99.902 (99.959)
Epoch: [102][24/25]	Time 0.333 (0.610)	Data 0.008 (0.031)	Loss 0.3144 (0.3226)	Acc@1 97.642 (97.282)	Acc@5 100.000 (99.960)

Epoch: [103 | 180] LR: 0.040000
Epoch: [103][0/25]	Time 0.664 (0.664)	Data 0.593 (0.593)	Loss 0.3203 (0.3203)	Acc@1 97.510 (97.510)	Acc@5 99.951 (99.951)
Epoch: [103][1/25]	Time 0.660 (0.662)	Data 0.006 (0.300)	Loss 0.3058 (0.3131)	Acc@1 97.754 (97.632)	Acc@5 100.000 (99.976)
Epoch: [103][2/25]	Time 0.626 (0.650)	Data 0.006 (0.202)	Loss 0.3195 (0.3152)	Acc@1 97.412 (97.559)	Acc@5 100.000 (99.984)
Epoch: [103][3/25]	Time 0.689 (0.660)	Data 0.005 (0.152)	Loss 0.3186 (0.3161)	Acc@1 97.461 (97.534)	Acc@5 99.951 (99.976)
Epoch: [103][4/25]	Time 0.577 (0.643)	Data 0.005 (0.123)	Loss 0.3274 (0.3183)	Acc@1 96.924 (97.412)	Acc@5 99.902 (99.961)
Epoch: [103][5/25]	Time 0.562 (0.630)	Data 0.005 (0.103)	Loss 0.3083 (0.3166)	Acc@1 97.852 (97.485)	Acc@5 99.951 (99.959)
Epoch: [103][6/25]	Time 0.660 (0.634)	Data 0.007 (0.090)	Loss 0.3139 (0.3163)	Acc@1 97.949 (97.552)	Acc@5 100.000 (99.965)
Epoch: [103][7/25]	Time 0.643 (0.635)	Data 0.004 (0.079)	Loss 0.3117 (0.3157)	Acc@1 97.900 (97.595)	Acc@5 99.951 (99.963)
Epoch: [103][8/25]	Time 0.617 (0.633)	Data 0.005 (0.071)	Loss 0.3101 (0.3151)	Acc@1 97.314 (97.564)	Acc@5 99.951 (99.962)
Epoch: [103][9/25]	Time 0.632 (0.633)	Data 0.007 (0.064)	Loss 0.3124 (0.3148)	Acc@1 97.607 (97.568)	Acc@5 99.951 (99.961)
Epoch: [103][10/25]	Time 0.643 (0.634)	Data 0.006 (0.059)	Loss 0.3271 (0.3159)	Acc@1 97.217 (97.536)	Acc@5 99.902 (99.956)
Epoch: [103][11/25]	Time 0.616 (0.632)	Data 0.005 (0.055)	Loss 0.3070 (0.3152)	Acc@1 97.949 (97.571)	Acc@5 100.000 (99.959)
Epoch: [103][12/25]	Time 0.663 (0.635)	Data 0.006 (0.051)	Loss 0.3132 (0.3150)	Acc@1 97.803 (97.589)	Acc@5 100.000 (99.962)
Epoch: [103][13/25]	Time 0.649 (0.636)	Data 0.006 (0.048)	Loss 0.3159 (0.3151)	Acc@1 97.070 (97.552)	Acc@5 99.951 (99.962)
Epoch: [103][14/25]	Time 0.631 (0.635)	Data 0.008 (0.045)	Loss 0.3155 (0.3151)	Acc@1 97.217 (97.529)	Acc@5 99.951 (99.961)
Epoch: [103][15/25]	Time 0.657 (0.637)	Data 0.004 (0.042)	Loss 0.3077 (0.3147)	Acc@1 97.266 (97.513)	Acc@5 99.951 (99.960)
Epoch: [103][16/25]	Time 0.587 (0.634)	Data 0.007 (0.040)	Loss 0.3153 (0.3147)	Acc@1 97.217 (97.495)	Acc@5 99.951 (99.960)
Epoch: [103][17/25]	Time 0.642 (0.634)	Data 0.004 (0.038)	Loss 0.3279 (0.3154)	Acc@1 97.217 (97.480)	Acc@5 99.707 (99.946)
Epoch: [103][18/25]	Time 0.672 (0.636)	Data 0.004 (0.037)	Loss 0.3214 (0.3157)	Acc@1 97.168 (97.464)	Acc@5 99.902 (99.943)
Epoch: [103][19/25]	Time 0.605 (0.635)	Data 0.004 (0.035)	Loss 0.3136 (0.3156)	Acc@1 97.803 (97.480)	Acc@5 99.902 (99.941)
Epoch: [103][20/25]	Time 0.614 (0.634)	Data 0.004 (0.034)	Loss 0.3054 (0.3152)	Acc@1 98.096 (97.510)	Acc@5 100.000 (99.944)
Epoch: [103][21/25]	Time 0.645 (0.634)	Data 0.006 (0.032)	Loss 0.3282 (0.3157)	Acc@1 96.924 (97.483)	Acc@5 100.000 (99.947)
Epoch: [103][22/25]	Time 0.659 (0.635)	Data 0.004 (0.031)	Loss 0.3070 (0.3154)	Acc@1 97.656 (97.491)	Acc@5 99.951 (99.947)
Epoch: [103][23/25]	Time 0.598 (0.634)	Data 0.005 (0.030)	Loss 0.3206 (0.3156)	Acc@1 96.924 (97.467)	Acc@5 100.000 (99.949)
Epoch: [103][24/25]	Time 0.321 (0.621)	Data 0.007 (0.029)	Loss 0.3189 (0.3156)	Acc@1 97.288 (97.464)	Acc@5 100.000 (99.950)

Epoch: [104 | 180] LR: 0.040000
Epoch: [104][0/25]	Time 0.585 (0.585)	Data 0.640 (0.640)	Loss 0.3194 (0.3194)	Acc@1 97.363 (97.363)	Acc@5 100.000 (100.000)
Epoch: [104][1/25]	Time 0.633 (0.609)	Data 0.007 (0.323)	Loss 0.3068 (0.3131)	Acc@1 97.510 (97.437)	Acc@5 99.951 (99.976)
Epoch: [104][2/25]	Time 0.632 (0.616)	Data 0.006 (0.218)	Loss 0.3064 (0.3108)	Acc@1 97.900 (97.591)	Acc@5 99.951 (99.967)
Epoch: [104][3/25]	Time 0.656 (0.626)	Data 0.005 (0.164)	Loss 0.3226 (0.3138)	Acc@1 96.777 (97.388)	Acc@5 100.000 (99.976)
Epoch: [104][4/25]	Time 0.578 (0.617)	Data 0.006 (0.133)	Loss 0.3025 (0.3115)	Acc@1 97.803 (97.471)	Acc@5 99.951 (99.971)
Epoch: [104][5/25]	Time 0.559 (0.607)	Data 0.006 (0.112)	Loss 0.3118 (0.3116)	Acc@1 97.510 (97.477)	Acc@5 100.000 (99.976)
Epoch: [104][6/25]	Time 0.628 (0.610)	Data 0.004 (0.096)	Loss 0.3012 (0.3101)	Acc@1 97.656 (97.503)	Acc@5 100.000 (99.979)
Epoch: [104][7/25]	Time 0.694 (0.621)	Data 0.005 (0.085)	Loss 0.3107 (0.3102)	Acc@1 97.754 (97.534)	Acc@5 99.902 (99.969)
Epoch: [104][8/25]	Time 0.553 (0.613)	Data 0.006 (0.076)	Loss 0.3050 (0.3096)	Acc@1 97.900 (97.575)	Acc@5 100.000 (99.973)
Epoch: [104][9/25]	Time 0.585 (0.610)	Data 0.007 (0.069)	Loss 0.2997 (0.3086)	Acc@1 98.096 (97.627)	Acc@5 99.902 (99.966)
Epoch: [104][10/25]	Time 0.636 (0.613)	Data 0.006 (0.064)	Loss 0.3073 (0.3085)	Acc@1 97.900 (97.652)	Acc@5 99.951 (99.964)
Epoch: [104][11/25]	Time 0.658 (0.616)	Data 0.005 (0.059)	Loss 0.3032 (0.3081)	Acc@1 97.559 (97.644)	Acc@5 99.951 (99.963)
Epoch: [104][12/25]	Time 0.618 (0.616)	Data 0.008 (0.055)	Loss 0.3018 (0.3076)	Acc@1 98.193 (97.686)	Acc@5 100.000 (99.966)
Epoch: [104][13/25]	Time 0.646 (0.619)	Data 0.005 (0.051)	Loss 0.3005 (0.3071)	Acc@1 97.998 (97.709)	Acc@5 99.951 (99.965)
Epoch: [104][14/25]	Time 0.583 (0.616)	Data 0.004 (0.048)	Loss 0.3231 (0.3081)	Acc@1 96.924 (97.656)	Acc@5 100.000 (99.967)
Epoch: [104][15/25]	Time 0.603 (0.615)	Data 0.006 (0.045)	Loss 0.3089 (0.3082)	Acc@1 97.461 (97.644)	Acc@5 99.951 (99.966)
Epoch: [104][16/25]	Time 0.600 (0.615)	Data 0.006 (0.043)	Loss 0.3040 (0.3079)	Acc@1 97.754 (97.651)	Acc@5 100.000 (99.968)
Epoch: [104][17/25]	Time 0.659 (0.617)	Data 0.004 (0.041)	Loss 0.3142 (0.3083)	Acc@1 97.559 (97.645)	Acc@5 100.000 (99.970)
Epoch: [104][18/25]	Time 0.618 (0.617)	Data 0.006 (0.039)	Loss 0.3180 (0.3088)	Acc@1 97.070 (97.615)	Acc@5 99.951 (99.969)
Epoch: [104][19/25]	Time 0.668 (0.620)	Data 0.006 (0.037)	Loss 0.3115 (0.3089)	Acc@1 97.314 (97.600)	Acc@5 99.902 (99.966)
Epoch: [104][20/25]	Time 0.697 (0.623)	Data 0.005 (0.036)	Loss 0.2977 (0.3084)	Acc@1 98.193 (97.628)	Acc@5 99.902 (99.963)
Epoch: [104][21/25]	Time 0.584 (0.622)	Data 0.008 (0.035)	Loss 0.3092 (0.3084)	Acc@1 97.314 (97.614)	Acc@5 100.000 (99.964)
Epoch: [104][22/25]	Time 0.573 (0.619)	Data 0.005 (0.033)	Loss 0.3171 (0.3088)	Acc@1 97.461 (97.607)	Acc@5 100.000 (99.966)
Epoch: [104][23/25]	Time 0.620 (0.620)	Data 0.004 (0.032)	Loss 0.2922 (0.3081)	Acc@1 97.998 (97.624)	Acc@5 100.000 (99.967)
Epoch: [104][24/25]	Time 0.376 (0.610)	Data 0.006 (0.031)	Loss 0.3036 (0.3080)	Acc@1 97.642 (97.624)	Acc@5 100.000 (99.968)

Epoch: [105 | 180] LR: 0.040000
Epoch: [105][0/25]	Time 0.644 (0.644)	Data 0.623 (0.623)	Loss 0.2988 (0.2988)	Acc@1 97.803 (97.803)	Acc@5 100.000 (100.000)
Epoch: [105][1/25]	Time 0.575 (0.610)	Data 0.007 (0.315)	Loss 0.3000 (0.2994)	Acc@1 97.754 (97.778)	Acc@5 100.000 (100.000)
Epoch: [105][2/25]	Time 0.596 (0.605)	Data 0.005 (0.211)	Loss 0.3043 (0.3010)	Acc@1 97.559 (97.705)	Acc@5 100.000 (100.000)
Epoch: [105][3/25]	Time 0.603 (0.605)	Data 0.006 (0.160)	Loss 0.2916 (0.2987)	Acc@1 98.340 (97.864)	Acc@5 100.000 (100.000)
Epoch: [105][4/25]	Time 0.657 (0.615)	Data 0.007 (0.129)	Loss 0.2953 (0.2980)	Acc@1 97.852 (97.861)	Acc@5 100.000 (100.000)
Epoch: [105][5/25]	Time 0.571 (0.608)	Data 0.007 (0.109)	Loss 0.3086 (0.2998)	Acc@1 97.852 (97.860)	Acc@5 99.951 (99.992)
Epoch: [105][6/25]	Time 0.591 (0.605)	Data 0.005 (0.094)	Loss 0.3096 (0.3012)	Acc@1 97.412 (97.796)	Acc@5 99.951 (99.986)
Epoch: [105][7/25]	Time 0.593 (0.604)	Data 0.008 (0.083)	Loss 0.3101 (0.3023)	Acc@1 97.119 (97.711)	Acc@5 100.000 (99.988)
Epoch: [105][8/25]	Time 0.626 (0.606)	Data 0.007 (0.075)	Loss 0.3017 (0.3022)	Acc@1 97.803 (97.721)	Acc@5 99.951 (99.984)
Epoch: [105][9/25]	Time 0.650 (0.611)	Data 0.007 (0.068)	Loss 0.3164 (0.3036)	Acc@1 97.412 (97.690)	Acc@5 99.902 (99.976)
Epoch: [105][10/25]	Time 0.657 (0.615)	Data 0.005 (0.062)	Loss 0.3030 (0.3036)	Acc@1 97.754 (97.696)	Acc@5 99.951 (99.973)
Epoch: [105][11/25]	Time 0.603 (0.614)	Data 0.006 (0.058)	Loss 0.3111 (0.3042)	Acc@1 97.314 (97.664)	Acc@5 100.000 (99.976)
Epoch: [105][12/25]	Time 0.618 (0.614)	Data 0.005 (0.054)	Loss 0.2998 (0.3039)	Acc@1 97.656 (97.664)	Acc@5 99.951 (99.974)
Epoch: [105][13/25]	Time 0.584 (0.612)	Data 0.006 (0.050)	Loss 0.3053 (0.3040)	Acc@1 97.412 (97.646)	Acc@5 100.000 (99.976)
Epoch: [105][14/25]	Time 0.636 (0.614)	Data 0.006 (0.047)	Loss 0.2949 (0.3034)	Acc@1 98.047 (97.673)	Acc@5 99.951 (99.974)
Epoch: [105][15/25]	Time 0.692 (0.619)	Data 0.009 (0.045)	Loss 0.3037 (0.3034)	Acc@1 97.607 (97.668)	Acc@5 99.951 (99.973)
Epoch: [105][16/25]	Time 0.597 (0.617)	Data 0.005 (0.042)	Loss 0.3058 (0.3035)	Acc@1 97.607 (97.665)	Acc@5 100.000 (99.974)
Epoch: [105][17/25]	Time 0.558 (0.614)	Data 0.004 (0.040)	Loss 0.2959 (0.3031)	Acc@1 98.291 (97.700)	Acc@5 99.951 (99.973)
Epoch: [105][18/25]	Time 0.577 (0.612)	Data 0.004 (0.038)	Loss 0.2995 (0.3029)	Acc@1 97.949 (97.713)	Acc@5 99.902 (99.969)
Epoch: [105][19/25]	Time 0.546 (0.609)	Data 0.009 (0.037)	Loss 0.2912 (0.3023)	Acc@1 98.047 (97.729)	Acc@5 100.000 (99.971)
Epoch: [105][20/25]	Time 0.604 (0.609)	Data 0.004 (0.035)	Loss 0.2972 (0.3021)	Acc@1 97.705 (97.728)	Acc@5 100.000 (99.972)
Epoch: [105][21/25]	Time 0.616 (0.609)	Data 0.004 (0.034)	Loss 0.3026 (0.3021)	Acc@1 97.705 (97.727)	Acc@5 100.000 (99.973)
Epoch: [105][22/25]	Time 0.669 (0.612)	Data 0.007 (0.033)	Loss 0.3005 (0.3020)	Acc@1 97.266 (97.707)	Acc@5 99.951 (99.972)
Epoch: [105][23/25]	Time 0.600 (0.611)	Data 0.005 (0.032)	Loss 0.2941 (0.3017)	Acc@1 97.852 (97.713)	Acc@5 100.000 (99.974)
Epoch: [105][24/25]	Time 0.324 (0.600)	Data 0.006 (0.031)	Loss 0.2967 (0.3016)	Acc@1 97.877 (97.716)	Acc@5 99.882 (99.972)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 439708 ; 487386 ; 0.9021760986158814

Epoch: [106 | 180] LR: 0.040000
Epoch: [106][0/25]	Time 0.632 (0.632)	Data 0.681 (0.681)	Loss 0.2956 (0.2956)	Acc@1 97.949 (97.949)	Acc@5 100.000 (100.000)
Epoch: [106][1/25]	Time 0.587 (0.610)	Data 0.005 (0.343)	Loss 0.2961 (0.2958)	Acc@1 98.145 (98.047)	Acc@5 100.000 (100.000)
Epoch: [106][2/25]	Time 0.663 (0.627)	Data 0.005 (0.230)	Loss 0.2954 (0.2957)	Acc@1 97.803 (97.965)	Acc@5 100.000 (100.000)
Epoch: [106][3/25]	Time 0.652 (0.634)	Data 0.004 (0.174)	Loss 0.2892 (0.2941)	Acc@1 98.096 (97.998)	Acc@5 100.000 (100.000)
Epoch: [106][4/25]	Time 0.678 (0.643)	Data 0.007 (0.140)	Loss 0.2965 (0.2946)	Acc@1 98.047 (98.008)	Acc@5 99.902 (99.980)
Epoch: [106][5/25]	Time 0.664 (0.646)	Data 0.004 (0.118)	Loss 0.2925 (0.2942)	Acc@1 98.047 (98.014)	Acc@5 100.000 (99.984)
Epoch: [106][6/25]	Time 0.671 (0.650)	Data 0.003 (0.101)	Loss 0.2882 (0.2934)	Acc@1 98.242 (98.047)	Acc@5 99.951 (99.979)
Epoch: [106][7/25]	Time 0.651 (0.650)	Data 0.006 (0.089)	Loss 0.2966 (0.2938)	Acc@1 97.705 (98.004)	Acc@5 99.951 (99.976)
Epoch: [106][8/25]	Time 0.598 (0.644)	Data 0.007 (0.080)	Loss 0.2928 (0.2937)	Acc@1 97.754 (97.976)	Acc@5 99.951 (99.973)
Epoch: [106][9/25]	Time 0.611 (0.641)	Data 0.004 (0.072)	Loss 0.2924 (0.2935)	Acc@1 98.047 (97.983)	Acc@5 100.000 (99.976)
Epoch: [106][10/25]	Time 0.672 (0.644)	Data 0.006 (0.066)	Loss 0.2878 (0.2930)	Acc@1 98.145 (97.998)	Acc@5 100.000 (99.978)
Epoch: [106][11/25]	Time 0.581 (0.638)	Data 0.006 (0.061)	Loss 0.2921 (0.2929)	Acc@1 97.949 (97.994)	Acc@5 100.000 (99.980)
Epoch: [106][12/25]	Time 0.589 (0.635)	Data 0.006 (0.057)	Loss 0.3025 (0.2937)	Acc@1 97.559 (97.960)	Acc@5 99.902 (99.974)
Epoch: [106][13/25]	Time 0.612 (0.633)	Data 0.008 (0.054)	Loss 0.2951 (0.2938)	Acc@1 97.803 (97.949)	Acc@5 99.951 (99.972)
Epoch: [106][14/25]	Time 0.644 (0.634)	Data 0.005 (0.050)	Loss 0.2929 (0.2937)	Acc@1 98.193 (97.965)	Acc@5 99.951 (99.971)
Epoch: [106][15/25]	Time 0.619 (0.633)	Data 0.005 (0.048)	Loss 0.3007 (0.2942)	Acc@1 97.754 (97.952)	Acc@5 99.951 (99.969)
Epoch: [106][16/25]	Time 0.632 (0.633)	Data 0.006 (0.045)	Loss 0.2960 (0.2943)	Acc@1 97.949 (97.952)	Acc@5 100.000 (99.971)
Epoch: [106][17/25]	Time 0.654 (0.634)	Data 0.004 (0.043)	Loss 0.2940 (0.2942)	Acc@1 97.900 (97.949)	Acc@5 100.000 (99.973)
Epoch: [106][18/25]	Time 0.663 (0.635)	Data 0.004 (0.041)	Loss 0.2896 (0.2940)	Acc@1 98.242 (97.965)	Acc@5 99.951 (99.972)
Epoch: [106][19/25]	Time 0.684 (0.638)	Data 0.006 (0.039)	Loss 0.2982 (0.2942)	Acc@1 97.803 (97.957)	Acc@5 100.000 (99.973)
Epoch: [106][20/25]	Time 0.625 (0.637)	Data 0.005 (0.037)	Loss 0.2973 (0.2944)	Acc@1 98.047 (97.961)	Acc@5 100.000 (99.974)
Epoch: [106][21/25]	Time 0.644 (0.638)	Data 0.007 (0.036)	Loss 0.2927 (0.2943)	Acc@1 97.705 (97.949)	Acc@5 100.000 (99.976)
Epoch: [106][22/25]	Time 0.685 (0.640)	Data 0.004 (0.035)	Loss 0.2983 (0.2945)	Acc@1 97.754 (97.941)	Acc@5 99.951 (99.975)
Epoch: [106][23/25]	Time 0.665 (0.641)	Data 0.004 (0.033)	Loss 0.2953 (0.2945)	Acc@1 97.461 (97.921)	Acc@5 99.951 (99.974)
Epoch: [106][24/25]	Time 0.344 (0.629)	Data 0.007 (0.032)	Loss 0.2943 (0.2945)	Acc@1 97.759 (97.918)	Acc@5 100.000 (99.974)

Epoch: [107 | 180] LR: 0.040000
Epoch: [107][0/25]	Time 0.621 (0.621)	Data 0.713 (0.713)	Loss 0.2873 (0.2873)	Acc@1 98.096 (98.096)	Acc@5 99.951 (99.951)
Epoch: [107][1/25]	Time 0.563 (0.592)	Data 0.006 (0.360)	Loss 0.2909 (0.2891)	Acc@1 97.949 (98.022)	Acc@5 100.000 (99.976)
Epoch: [107][2/25]	Time 0.594 (0.592)	Data 0.007 (0.242)	Loss 0.2796 (0.2860)	Acc@1 98.584 (98.210)	Acc@5 100.000 (99.984)
Epoch: [107][3/25]	Time 0.608 (0.596)	Data 0.004 (0.183)	Loss 0.2921 (0.2875)	Acc@1 97.949 (98.145)	Acc@5 100.000 (99.988)
Epoch: [107][4/25]	Time 0.598 (0.597)	Data 0.008 (0.148)	Loss 0.2897 (0.2879)	Acc@1 98.047 (98.125)	Acc@5 100.000 (99.990)
Epoch: [107][5/25]	Time 0.670 (0.609)	Data 0.006 (0.124)	Loss 0.2922 (0.2886)	Acc@1 97.754 (98.063)	Acc@5 99.951 (99.984)
Epoch: [107][6/25]	Time 0.592 (0.606)	Data 0.006 (0.107)	Loss 0.2849 (0.2881)	Acc@1 98.486 (98.124)	Acc@5 99.951 (99.979)
Epoch: [107][7/25]	Time 0.656 (0.613)	Data 0.007 (0.095)	Loss 0.2844 (0.2876)	Acc@1 98.096 (98.120)	Acc@5 100.000 (99.982)
Epoch: [107][8/25]	Time 0.658 (0.618)	Data 0.005 (0.085)	Loss 0.2920 (0.2881)	Acc@1 98.047 (98.112)	Acc@5 99.951 (99.978)
Epoch: [107][9/25]	Time 0.575 (0.613)	Data 0.005 (0.077)	Loss 0.2923 (0.2885)	Acc@1 97.900 (98.091)	Acc@5 100.000 (99.980)
Epoch: [107][10/25]	Time 0.592 (0.611)	Data 0.007 (0.070)	Loss 0.2840 (0.2881)	Acc@1 97.998 (98.082)	Acc@5 100.000 (99.982)
Epoch: [107][11/25]	Time 0.594 (0.610)	Data 0.005 (0.065)	Loss 0.2802 (0.2875)	Acc@1 98.535 (98.120)	Acc@5 100.000 (99.984)
Epoch: [107][12/25]	Time 0.662 (0.614)	Data 0.007 (0.060)	Loss 0.2942 (0.2880)	Acc@1 97.607 (98.081)	Acc@5 99.951 (99.981)
Epoch: [107][13/25]	Time 0.591 (0.612)	Data 0.005 (0.056)	Loss 0.2877 (0.2880)	Acc@1 97.900 (98.068)	Acc@5 99.951 (99.979)
Epoch: [107][14/25]	Time 0.581 (0.610)	Data 0.008 (0.053)	Loss 0.2901 (0.2881)	Acc@1 98.047 (98.066)	Acc@5 100.000 (99.980)
Epoch: [107][15/25]	Time 0.587 (0.609)	Data 0.008 (0.050)	Loss 0.2942 (0.2885)	Acc@1 97.705 (98.044)	Acc@5 99.902 (99.976)
Epoch: [107][16/25]	Time 0.654 (0.611)	Data 0.005 (0.048)	Loss 0.2991 (0.2891)	Acc@1 97.559 (98.015)	Acc@5 99.902 (99.971)
Epoch: [107][17/25]	Time 0.673 (0.615)	Data 0.004 (0.045)	Loss 0.2895 (0.2891)	Acc@1 97.852 (98.006)	Acc@5 99.951 (99.970)
Epoch: [107][18/25]	Time 0.571 (0.613)	Data 0.005 (0.043)	Loss 0.2876 (0.2891)	Acc@1 98.047 (98.008)	Acc@5 100.000 (99.972)
Epoch: [107][19/25]	Time 0.613 (0.613)	Data 0.006 (0.041)	Loss 0.2944 (0.2893)	Acc@1 97.705 (97.993)	Acc@5 99.902 (99.968)
Epoch: [107][20/25]	Time 0.605 (0.612)	Data 0.006 (0.040)	Loss 0.2930 (0.2895)	Acc@1 97.803 (97.984)	Acc@5 99.951 (99.967)
Epoch: [107][21/25]	Time 0.686 (0.616)	Data 0.007 (0.038)	Loss 0.2922 (0.2896)	Acc@1 98.047 (97.987)	Acc@5 99.951 (99.967)
Epoch: [107][22/25]	Time 0.583 (0.614)	Data 0.005 (0.037)	Loss 0.2930 (0.2898)	Acc@1 97.656 (97.973)	Acc@5 100.000 (99.968)
Epoch: [107][23/25]	Time 0.580 (0.613)	Data 0.005 (0.035)	Loss 0.2894 (0.2897)	Acc@1 98.242 (97.984)	Acc@5 100.000 (99.969)
Epoch: [107][24/25]	Time 0.351 (0.602)	Data 0.005 (0.034)	Loss 0.2792 (0.2896)	Acc@1 98.231 (97.988)	Acc@5 100.000 (99.970)

Epoch: [108 | 180] LR: 0.040000
Epoch: [108][0/25]	Time 0.632 (0.632)	Data 0.714 (0.714)	Loss 0.2858 (0.2858)	Acc@1 98.145 (98.145)	Acc@5 99.951 (99.951)
Epoch: [108][1/25]	Time 0.559 (0.595)	Data 0.005 (0.360)	Loss 0.2882 (0.2870)	Acc@1 98.145 (98.145)	Acc@5 99.951 (99.951)
Epoch: [108][2/25]	Time 0.567 (0.586)	Data 0.005 (0.241)	Loss 0.2824 (0.2855)	Acc@1 98.193 (98.161)	Acc@5 100.000 (99.967)
Epoch: [108][3/25]	Time 0.617 (0.594)	Data 0.005 (0.182)	Loss 0.2799 (0.2841)	Acc@1 98.291 (98.193)	Acc@5 100.000 (99.976)
Epoch: [108][4/25]	Time 0.670 (0.609)	Data 0.008 (0.147)	Loss 0.2813 (0.2835)	Acc@1 98.340 (98.223)	Acc@5 99.951 (99.971)
Epoch: [108][5/25]	Time 0.588 (0.605)	Data 0.007 (0.124)	Loss 0.2885 (0.2843)	Acc@1 97.803 (98.153)	Acc@5 100.000 (99.976)
Epoch: [108][6/25]	Time 0.599 (0.604)	Data 0.004 (0.107)	Loss 0.2735 (0.2828)	Acc@1 98.486 (98.200)	Acc@5 99.951 (99.972)
Epoch: [108][7/25]	Time 0.646 (0.610)	Data 0.004 (0.094)	Loss 0.2863 (0.2832)	Acc@1 97.852 (98.157)	Acc@5 100.000 (99.976)
Epoch: [108][8/25]	Time 0.696 (0.619)	Data 0.006 (0.084)	Loss 0.2862 (0.2836)	Acc@1 98.145 (98.155)	Acc@5 100.000 (99.978)
Epoch: [108][9/25]	Time 0.566 (0.614)	Data 0.007 (0.076)	Loss 0.2915 (0.2844)	Acc@1 97.900 (98.130)	Acc@5 99.951 (99.976)
Epoch: [108][10/25]	Time 0.657 (0.618)	Data 0.007 (0.070)	Loss 0.2906 (0.2849)	Acc@1 97.900 (98.109)	Acc@5 99.951 (99.973)
Epoch: [108][11/25]	Time 0.686 (0.623)	Data 0.007 (0.065)	Loss 0.2813 (0.2846)	Acc@1 98.486 (98.140)	Acc@5 99.951 (99.972)
Epoch: [108][12/25]	Time 0.595 (0.621)	Data 0.004 (0.060)	Loss 0.2989 (0.2857)	Acc@1 97.363 (98.081)	Acc@5 99.951 (99.970)
Epoch: [108][13/25]	Time 0.613 (0.621)	Data 0.006 (0.056)	Loss 0.2824 (0.2855)	Acc@1 98.438 (98.106)	Acc@5 100.000 (99.972)
Epoch: [108][14/25]	Time 0.763 (0.630)	Data 0.008 (0.053)	Loss 0.2915 (0.2859)	Acc@1 97.754 (98.083)	Acc@5 99.902 (99.967)
Epoch: [108][15/25]	Time 0.757 (0.638)	Data 0.006 (0.050)	Loss 0.2859 (0.2859)	Acc@1 98.193 (98.090)	Acc@5 100.000 (99.969)
Epoch: [108][16/25]	Time 0.566 (0.634)	Data 0.007 (0.048)	Loss 0.2904 (0.2862)	Acc@1 97.754 (98.070)	Acc@5 100.000 (99.971)
Epoch: [108][17/25]	Time 0.620 (0.633)	Data 0.004 (0.045)	Loss 0.2768 (0.2856)	Acc@1 98.145 (98.074)	Acc@5 100.000 (99.973)
Epoch: [108][18/25]	Time 0.671 (0.635)	Data 0.009 (0.043)	Loss 0.2947 (0.2861)	Acc@1 97.803 (98.060)	Acc@5 99.902 (99.969)
Epoch: [108][19/25]	Time 0.597 (0.633)	Data 0.007 (0.041)	Loss 0.2798 (0.2858)	Acc@1 98.340 (98.074)	Acc@5 100.000 (99.971)
Epoch: [108][20/25]	Time 0.654 (0.634)	Data 0.005 (0.040)	Loss 0.2884 (0.2859)	Acc@1 97.656 (98.054)	Acc@5 99.951 (99.970)
Epoch: [108][21/25]	Time 0.653 (0.635)	Data 0.004 (0.038)	Loss 0.2780 (0.2856)	Acc@1 98.242 (98.062)	Acc@5 100.000 (99.971)
Epoch: [108][22/25]	Time 0.655 (0.636)	Data 0.004 (0.037)	Loss 0.2826 (0.2854)	Acc@1 98.291 (98.072)	Acc@5 99.951 (99.970)
Epoch: [108][23/25]	Time 0.683 (0.638)	Data 0.005 (0.035)	Loss 0.2892 (0.2856)	Acc@1 97.754 (98.059)	Acc@5 99.951 (99.969)
Epoch: [108][24/25]	Time 0.337 (0.626)	Data 0.005 (0.034)	Loss 0.2997 (0.2858)	Acc@1 97.288 (98.046)	Acc@5 100.000 (99.970)

Epoch: [109 | 180] LR: 0.040000
Epoch: [109][0/25]	Time 0.604 (0.604)	Data 0.641 (0.641)	Loss 0.2847 (0.2847)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [109][1/25]	Time 0.572 (0.588)	Data 0.003 (0.322)	Loss 0.2878 (0.2862)	Acc@1 97.852 (97.949)	Acc@5 99.902 (99.951)
Epoch: [109][2/25]	Time 0.623 (0.600)	Data 0.006 (0.217)	Loss 0.2876 (0.2867)	Acc@1 97.949 (97.949)	Acc@5 99.902 (99.935)
Epoch: [109][3/25]	Time 0.653 (0.613)	Data 0.005 (0.164)	Loss 0.2829 (0.2858)	Acc@1 97.852 (97.925)	Acc@5 100.000 (99.951)
Epoch: [109][4/25]	Time 0.631 (0.617)	Data 0.010 (0.133)	Loss 0.2845 (0.2855)	Acc@1 97.852 (97.910)	Acc@5 100.000 (99.961)
Epoch: [109][5/25]	Time 0.567 (0.608)	Data 0.005 (0.112)	Loss 0.2759 (0.2839)	Acc@1 98.193 (97.957)	Acc@5 100.000 (99.967)
Epoch: [109][6/25]	Time 0.611 (0.609)	Data 0.005 (0.097)	Loss 0.2793 (0.2832)	Acc@1 98.584 (98.047)	Acc@5 100.000 (99.972)
Epoch: [109][7/25]	Time 0.589 (0.606)	Data 0.003 (0.085)	Loss 0.2795 (0.2828)	Acc@1 98.242 (98.071)	Acc@5 100.000 (99.976)
Epoch: [109][8/25]	Time 0.632 (0.609)	Data 0.004 (0.076)	Loss 0.2675 (0.2811)	Acc@1 98.877 (98.161)	Acc@5 100.000 (99.978)
Epoch: [109][9/25]	Time 0.635 (0.612)	Data 0.005 (0.069)	Loss 0.2762 (0.2806)	Acc@1 98.242 (98.169)	Acc@5 100.000 (99.980)
Epoch: [109][10/25]	Time 0.622 (0.613)	Data 0.004 (0.063)	Loss 0.2906 (0.2815)	Acc@1 97.559 (98.113)	Acc@5 99.951 (99.978)
Epoch: [109][11/25]	Time 0.590 (0.611)	Data 0.007 (0.058)	Loss 0.2784 (0.2812)	Acc@1 98.145 (98.116)	Acc@5 100.000 (99.980)
Epoch: [109][12/25]	Time 0.641 (0.613)	Data 0.007 (0.054)	Loss 0.2736 (0.2807)	Acc@1 98.340 (98.133)	Acc@5 99.951 (99.977)
Epoch: [109][13/25]	Time 0.661 (0.616)	Data 0.006 (0.051)	Loss 0.2772 (0.2804)	Acc@1 98.730 (98.176)	Acc@5 100.000 (99.979)
Epoch: [109][14/25]	Time 0.636 (0.618)	Data 0.004 (0.048)	Loss 0.2892 (0.2810)	Acc@1 98.047 (98.167)	Acc@5 100.000 (99.980)
Epoch: [109][15/25]	Time 0.639 (0.619)	Data 0.005 (0.045)	Loss 0.2823 (0.2811)	Acc@1 98.096 (98.163)	Acc@5 100.000 (99.982)
Epoch: [109][16/25]	Time 0.662 (0.622)	Data 0.008 (0.043)	Loss 0.2863 (0.2814)	Acc@1 97.852 (98.145)	Acc@5 100.000 (99.983)
Epoch: [109][17/25]	Time 0.687 (0.625)	Data 0.004 (0.041)	Loss 0.2878 (0.2817)	Acc@1 97.754 (98.123)	Acc@5 99.951 (99.981)
Epoch: [109][18/25]	Time 0.635 (0.626)	Data 0.004 (0.039)	Loss 0.2803 (0.2817)	Acc@1 98.193 (98.127)	Acc@5 99.951 (99.979)
Epoch: [109][19/25]	Time 0.664 (0.628)	Data 0.007 (0.037)	Loss 0.2826 (0.2817)	Acc@1 98.193 (98.130)	Acc@5 99.951 (99.978)
Epoch: [109][20/25]	Time 0.663 (0.629)	Data 0.006 (0.036)	Loss 0.2863 (0.2819)	Acc@1 97.852 (98.117)	Acc@5 100.000 (99.979)
Epoch: [109][21/25]	Time 0.577 (0.627)	Data 0.006 (0.034)	Loss 0.2857 (0.2821)	Acc@1 98.047 (98.113)	Acc@5 99.951 (99.978)
Epoch: [109][22/25]	Time 0.587 (0.625)	Data 0.006 (0.033)	Loss 0.2813 (0.2821)	Acc@1 97.998 (98.108)	Acc@5 99.951 (99.977)
Epoch: [109][23/25]	Time 0.648 (0.626)	Data 0.005 (0.032)	Loss 0.2837 (0.2821)	Acc@1 98.047 (98.106)	Acc@5 99.951 (99.976)
Epoch: [109][24/25]	Time 0.376 (0.616)	Data 0.007 (0.031)	Loss 0.2918 (0.2823)	Acc@1 97.524 (98.096)	Acc@5 100.000 (99.976)

Epoch: [110 | 180] LR: 0.040000
Epoch: [110][0/25]	Time 0.649 (0.649)	Data 0.663 (0.663)	Loss 0.2874 (0.2874)	Acc@1 97.803 (97.803)	Acc@5 99.902 (99.902)
Epoch: [110][1/25]	Time 0.676 (0.662)	Data 0.003 (0.333)	Loss 0.2796 (0.2835)	Acc@1 98.096 (97.949)	Acc@5 99.951 (99.927)
Epoch: [110][2/25]	Time 0.658 (0.661)	Data 0.005 (0.224)	Loss 0.2706 (0.2792)	Acc@1 98.730 (98.210)	Acc@5 99.902 (99.919)
Epoch: [110][3/25]	Time 0.642 (0.656)	Data 0.005 (0.169)	Loss 0.2932 (0.2827)	Acc@1 97.705 (98.083)	Acc@5 100.000 (99.939)
Epoch: [110][4/25]	Time 0.674 (0.660)	Data 0.007 (0.137)	Loss 0.2726 (0.2807)	Acc@1 98.535 (98.174)	Acc@5 100.000 (99.951)
Epoch: [110][5/25]	Time 0.602 (0.650)	Data 0.004 (0.115)	Loss 0.2768 (0.2800)	Acc@1 98.438 (98.218)	Acc@5 100.000 (99.959)
Epoch: [110][6/25]	Time 0.629 (0.647)	Data 0.004 (0.099)	Loss 0.2805 (0.2801)	Acc@1 98.438 (98.249)	Acc@5 99.951 (99.958)
Epoch: [110][7/25]	Time 0.642 (0.646)	Data 0.003 (0.087)	Loss 0.2815 (0.2803)	Acc@1 97.998 (98.218)	Acc@5 100.000 (99.963)
Epoch: [110][8/25]	Time 0.618 (0.643)	Data 0.006 (0.078)	Loss 0.2798 (0.2802)	Acc@1 98.145 (98.210)	Acc@5 100.000 (99.967)
Epoch: [110][9/25]	Time 0.596 (0.638)	Data 0.007 (0.071)	Loss 0.2752 (0.2797)	Acc@1 98.291 (98.218)	Acc@5 100.000 (99.971)
Epoch: [110][10/25]	Time 0.592 (0.634)	Data 0.004 (0.065)	Loss 0.2744 (0.2792)	Acc@1 98.535 (98.247)	Acc@5 99.951 (99.969)
Epoch: [110][11/25]	Time 0.639 (0.635)	Data 0.005 (0.060)	Loss 0.2783 (0.2792)	Acc@1 98.145 (98.238)	Acc@5 100.000 (99.972)
Epoch: [110][12/25]	Time 0.654 (0.636)	Data 0.005 (0.056)	Loss 0.2690 (0.2784)	Acc@1 98.438 (98.253)	Acc@5 100.000 (99.974)
Epoch: [110][13/25]	Time 0.718 (0.642)	Data 0.006 (0.052)	Loss 0.2894 (0.2792)	Acc@1 97.656 (98.211)	Acc@5 99.902 (99.969)
Epoch: [110][14/25]	Time 0.758 (0.650)	Data 0.005 (0.049)	Loss 0.2809 (0.2793)	Acc@1 98.047 (98.200)	Acc@5 100.000 (99.971)
Epoch: [110][15/25]	Time 0.684 (0.652)	Data 0.009 (0.046)	Loss 0.2739 (0.2790)	Acc@1 98.535 (98.221)	Acc@5 100.000 (99.973)
Epoch: [110][16/25]	Time 0.651 (0.652)	Data 0.008 (0.044)	Loss 0.2748 (0.2787)	Acc@1 98.389 (98.231)	Acc@5 100.000 (99.974)
Epoch: [110][17/25]	Time 0.662 (0.652)	Data 0.004 (0.042)	Loss 0.2855 (0.2791)	Acc@1 98.096 (98.223)	Acc@5 100.000 (99.976)
Epoch: [110][18/25]	Time 0.624 (0.651)	Data 0.004 (0.040)	Loss 0.2750 (0.2789)	Acc@1 98.193 (98.222)	Acc@5 100.000 (99.977)
Epoch: [110][19/25]	Time 0.655 (0.651)	Data 0.004 (0.038)	Loss 0.2733 (0.2786)	Acc@1 98.340 (98.228)	Acc@5 100.000 (99.978)
Epoch: [110][20/25]	Time 0.643 (0.651)	Data 0.007 (0.037)	Loss 0.2675 (0.2781)	Acc@1 98.682 (98.249)	Acc@5 100.000 (99.979)
Epoch: [110][21/25]	Time 0.664 (0.651)	Data 0.006 (0.035)	Loss 0.2781 (0.2781)	Acc@1 97.998 (98.238)	Acc@5 99.902 (99.976)
Epoch: [110][22/25]	Time 0.664 (0.652)	Data 0.004 (0.034)	Loss 0.2801 (0.2782)	Acc@1 97.998 (98.227)	Acc@5 100.000 (99.977)
Epoch: [110][23/25]	Time 0.667 (0.652)	Data 0.004 (0.033)	Loss 0.2711 (0.2779)	Acc@1 98.291 (98.230)	Acc@5 100.000 (99.978)
Epoch: [110][24/25]	Time 0.330 (0.640)	Data 0.005 (0.032)	Loss 0.2654 (0.2776)	Acc@1 98.939 (98.242)	Acc@5 100.000 (99.978)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [111 | 180] LR: 0.040000
Epoch: [111][0/25]	Time 0.628 (0.628)	Data 0.584 (0.584)	Loss 0.2772 (0.2772)	Acc@1 98.584 (98.584)	Acc@5 99.951 (99.951)
Epoch: [111][1/25]	Time 0.637 (0.632)	Data 0.007 (0.295)	Loss 0.2764 (0.2768)	Acc@1 98.096 (98.340)	Acc@5 99.951 (99.951)
Epoch: [111][2/25]	Time 0.640 (0.635)	Data 0.006 (0.199)	Loss 0.2650 (0.2729)	Acc@1 98.730 (98.470)	Acc@5 99.951 (99.951)
Epoch: [111][3/25]	Time 0.621 (0.632)	Data 0.004 (0.150)	Loss 0.2751 (0.2734)	Acc@1 98.340 (98.438)	Acc@5 100.000 (99.963)
Epoch: [111][4/25]	Time 0.620 (0.629)	Data 0.007 (0.121)	Loss 0.2664 (0.2720)	Acc@1 98.291 (98.408)	Acc@5 100.000 (99.971)
Epoch: [111][5/25]	Time 0.654 (0.633)	Data 0.007 (0.102)	Loss 0.2664 (0.2711)	Acc@1 98.242 (98.381)	Acc@5 100.000 (99.976)
Epoch: [111][6/25]	Time 0.640 (0.634)	Data 0.007 (0.089)	Loss 0.2616 (0.2697)	Acc@1 98.633 (98.417)	Acc@5 99.951 (99.972)
Epoch: [111][7/25]	Time 0.676 (0.639)	Data 0.005 (0.078)	Loss 0.2756 (0.2705)	Acc@1 98.535 (98.431)	Acc@5 100.000 (99.976)
Epoch: [111][8/25]	Time 0.689 (0.645)	Data 0.005 (0.070)	Loss 0.2730 (0.2707)	Acc@1 98.486 (98.438)	Acc@5 99.951 (99.973)
Epoch: [111][9/25]	Time 0.578 (0.638)	Data 0.006 (0.064)	Loss 0.2709 (0.2708)	Acc@1 98.486 (98.442)	Acc@5 99.951 (99.971)
Epoch: [111][10/25]	Time 0.570 (0.632)	Data 0.005 (0.058)	Loss 0.2734 (0.2710)	Acc@1 98.438 (98.442)	Acc@5 99.951 (99.969)
Epoch: [111][11/25]	Time 0.576 (0.627)	Data 0.005 (0.054)	Loss 0.2599 (0.2701)	Acc@1 99.072 (98.494)	Acc@5 100.000 (99.972)
Epoch: [111][12/25]	Time 0.576 (0.623)	Data 0.009 (0.050)	Loss 0.2724 (0.2703)	Acc@1 98.584 (98.501)	Acc@5 100.000 (99.974)
Epoch: [111][13/25]	Time 0.670 (0.627)	Data 0.005 (0.047)	Loss 0.2678 (0.2701)	Acc@1 98.730 (98.518)	Acc@5 100.000 (99.976)
Epoch: [111][14/25]	Time 0.612 (0.626)	Data 0.006 (0.044)	Loss 0.2690 (0.2700)	Acc@1 98.438 (98.512)	Acc@5 100.000 (99.977)
Epoch: [111][15/25]	Time 0.625 (0.626)	Data 0.006 (0.042)	Loss 0.2639 (0.2696)	Acc@1 98.486 (98.511)	Acc@5 100.000 (99.979)
Epoch: [111][16/25]	Time 0.614 (0.625)	Data 0.005 (0.040)	Loss 0.2655 (0.2694)	Acc@1 98.730 (98.524)	Acc@5 100.000 (99.980)
Epoch: [111][17/25]	Time 0.655 (0.627)	Data 0.005 (0.038)	Loss 0.2690 (0.2694)	Acc@1 98.633 (98.530)	Acc@5 100.000 (99.981)
Epoch: [111][18/25]	Time 0.648 (0.628)	Data 0.004 (0.036)	Loss 0.2736 (0.2696)	Acc@1 97.949 (98.499)	Acc@5 100.000 (99.982)
Epoch: [111][19/25]	Time 0.607 (0.627)	Data 0.004 (0.035)	Loss 0.2791 (0.2701)	Acc@1 97.998 (98.474)	Acc@5 100.000 (99.983)
Epoch: [111][20/25]	Time 0.657 (0.628)	Data 0.007 (0.033)	Loss 0.2760 (0.2703)	Acc@1 97.803 (98.442)	Acc@5 100.000 (99.984)
Epoch: [111][21/25]	Time 0.624 (0.628)	Data 0.005 (0.032)	Loss 0.2753 (0.2706)	Acc@1 98.242 (98.433)	Acc@5 100.000 (99.984)
Epoch: [111][22/25]	Time 0.639 (0.629)	Data 0.006 (0.031)	Loss 0.2597 (0.2701)	Acc@1 99.023 (98.459)	Acc@5 100.000 (99.985)
Epoch: [111][23/25]	Time 0.634 (0.629)	Data 0.006 (0.030)	Loss 0.2580 (0.2696)	Acc@1 98.828 (98.474)	Acc@5 99.951 (99.984)
Epoch: [111][24/25]	Time 0.317 (0.616)	Data 0.005 (0.029)	Loss 0.2699 (0.2696)	Acc@1 98.349 (98.472)	Acc@5 100.000 (99.984)

Epoch: [112 | 180] LR: 0.040000
Epoch: [112][0/25]	Time 0.655 (0.655)	Data 0.667 (0.667)	Loss 0.2705 (0.2705)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [112][1/25]	Time 0.674 (0.665)	Data 0.003 (0.335)	Loss 0.2660 (0.2683)	Acc@1 98.535 (98.486)	Acc@5 99.951 (99.976)
Epoch: [112][2/25]	Time 0.612 (0.647)	Data 0.006 (0.225)	Loss 0.2661 (0.2675)	Acc@1 98.584 (98.519)	Acc@5 100.000 (99.984)
Epoch: [112][3/25]	Time 0.632 (0.643)	Data 0.004 (0.170)	Loss 0.2726 (0.2688)	Acc@1 98.242 (98.450)	Acc@5 100.000 (99.988)
Epoch: [112][4/25]	Time 0.656 (0.646)	Data 0.008 (0.138)	Loss 0.2654 (0.2681)	Acc@1 98.535 (98.467)	Acc@5 99.951 (99.980)
Epoch: [112][5/25]	Time 0.558 (0.631)	Data 0.004 (0.115)	Loss 0.2781 (0.2698)	Acc@1 98.047 (98.397)	Acc@5 100.000 (99.984)
Epoch: [112][6/25]	Time 0.585 (0.625)	Data 0.006 (0.100)	Loss 0.2652 (0.2691)	Acc@1 98.242 (98.375)	Acc@5 100.000 (99.986)
Epoch: [112][7/25]	Time 0.574 (0.618)	Data 0.004 (0.088)	Loss 0.2675 (0.2689)	Acc@1 98.584 (98.401)	Acc@5 99.951 (99.982)
Epoch: [112][8/25]	Time 0.579 (0.614)	Data 0.007 (0.079)	Loss 0.2749 (0.2696)	Acc@1 97.900 (98.345)	Acc@5 100.000 (99.984)
Epoch: [112][9/25]	Time 0.668 (0.619)	Data 0.007 (0.072)	Loss 0.2687 (0.2695)	Acc@1 98.340 (98.345)	Acc@5 100.000 (99.985)
Epoch: [112][10/25]	Time 0.649 (0.622)	Data 0.005 (0.066)	Loss 0.2657 (0.2692)	Acc@1 98.389 (98.349)	Acc@5 100.000 (99.987)
Epoch: [112][11/25]	Time 0.579 (0.619)	Data 0.006 (0.061)	Loss 0.2719 (0.2694)	Acc@1 98.145 (98.332)	Acc@5 99.951 (99.984)
Epoch: [112][12/25]	Time 0.633 (0.620)	Data 0.007 (0.056)	Loss 0.2692 (0.2694)	Acc@1 98.389 (98.336)	Acc@5 100.000 (99.985)
Epoch: [112][13/25]	Time 0.652 (0.622)	Data 0.006 (0.053)	Loss 0.2734 (0.2697)	Acc@1 98.291 (98.333)	Acc@5 99.951 (99.983)
Epoch: [112][14/25]	Time 0.624 (0.622)	Data 0.004 (0.050)	Loss 0.2696 (0.2696)	Acc@1 98.047 (98.314)	Acc@5 100.000 (99.984)
Epoch: [112][15/25]	Time 0.642 (0.623)	Data 0.005 (0.047)	Loss 0.2661 (0.2694)	Acc@1 98.242 (98.309)	Acc@5 99.951 (99.982)
Epoch: [112][16/25]	Time 0.640 (0.624)	Data 0.006 (0.044)	Loss 0.2732 (0.2696)	Acc@1 98.145 (98.300)	Acc@5 99.951 (99.980)
Epoch: [112][17/25]	Time 0.660 (0.626)	Data 0.007 (0.042)	Loss 0.2704 (0.2697)	Acc@1 98.389 (98.305)	Acc@5 100.000 (99.981)
Epoch: [112][18/25]	Time 0.625 (0.626)	Data 0.006 (0.040)	Loss 0.2635 (0.2694)	Acc@1 98.438 (98.312)	Acc@5 100.000 (99.982)
Epoch: [112][19/25]	Time 0.637 (0.627)	Data 0.007 (0.039)	Loss 0.2688 (0.2693)	Acc@1 98.340 (98.313)	Acc@5 99.951 (99.980)
Epoch: [112][20/25]	Time 0.610 (0.626)	Data 0.004 (0.037)	Loss 0.2701 (0.2694)	Acc@1 98.389 (98.317)	Acc@5 99.951 (99.979)
Epoch: [112][21/25]	Time 0.671 (0.628)	Data 0.004 (0.036)	Loss 0.2770 (0.2697)	Acc@1 98.145 (98.309)	Acc@5 99.951 (99.978)
Epoch: [112][22/25]	Time 0.597 (0.627)	Data 0.004 (0.034)	Loss 0.2722 (0.2698)	Acc@1 97.852 (98.289)	Acc@5 99.951 (99.977)
Epoch: [112][23/25]	Time 0.612 (0.626)	Data 0.004 (0.033)	Loss 0.2637 (0.2696)	Acc@1 98.535 (98.299)	Acc@5 100.000 (99.978)
Epoch: [112][24/25]	Time 0.363 (0.615)	Data 0.006 (0.032)	Loss 0.2605 (0.2694)	Acc@1 98.939 (98.310)	Acc@5 100.000 (99.978)

Epoch: [113 | 180] LR: 0.040000
Epoch: [113][0/25]	Time 0.647 (0.647)	Data 0.605 (0.605)	Loss 0.2618 (0.2618)	Acc@1 98.486 (98.486)	Acc@5 100.000 (100.000)
Epoch: [113][1/25]	Time 0.641 (0.644)	Data 0.005 (0.305)	Loss 0.2724 (0.2671)	Acc@1 98.096 (98.291)	Acc@5 100.000 (100.000)
Epoch: [113][2/25]	Time 0.654 (0.647)	Data 0.005 (0.205)	Loss 0.2753 (0.2698)	Acc@1 97.949 (98.177)	Acc@5 100.000 (100.000)
Epoch: [113][3/25]	Time 0.631 (0.643)	Data 0.006 (0.155)	Loss 0.2613 (0.2677)	Acc@1 98.828 (98.340)	Acc@5 100.000 (100.000)
Epoch: [113][4/25]	Time 0.625 (0.640)	Data 0.006 (0.125)	Loss 0.2651 (0.2672)	Acc@1 98.438 (98.359)	Acc@5 99.902 (99.980)
Epoch: [113][5/25]	Time 0.614 (0.635)	Data 0.004 (0.105)	Loss 0.2627 (0.2664)	Acc@1 98.291 (98.348)	Acc@5 100.000 (99.984)
Epoch: [113][6/25]	Time 0.656 (0.638)	Data 0.008 (0.091)	Loss 0.2503 (0.2641)	Acc@1 98.828 (98.417)	Acc@5 100.000 (99.986)
Epoch: [113][7/25]	Time 0.558 (0.628)	Data 0.004 (0.080)	Loss 0.2610 (0.2637)	Acc@1 98.389 (98.413)	Acc@5 99.902 (99.976)
Epoch: [113][8/25]	Time 0.618 (0.627)	Data 0.007 (0.072)	Loss 0.2679 (0.2642)	Acc@1 98.633 (98.438)	Acc@5 99.951 (99.973)
Epoch: [113][9/25]	Time 0.657 (0.630)	Data 0.006 (0.066)	Loss 0.2691 (0.2647)	Acc@1 98.096 (98.403)	Acc@5 100.000 (99.976)
Epoch: [113][10/25]	Time 0.645 (0.631)	Data 0.005 (0.060)	Loss 0.2675 (0.2649)	Acc@1 98.389 (98.402)	Acc@5 100.000 (99.978)
Epoch: [113][11/25]	Time 0.615 (0.630)	Data 0.006 (0.056)	Loss 0.2677 (0.2652)	Acc@1 98.486 (98.409)	Acc@5 99.951 (99.976)
Epoch: [113][12/25]	Time 0.634 (0.630)	Data 0.004 (0.052)	Loss 0.2635 (0.2650)	Acc@1 98.535 (98.419)	Acc@5 99.951 (99.974)
Epoch: [113][13/25]	Time 0.668 (0.633)	Data 0.005 (0.048)	Loss 0.2602 (0.2647)	Acc@1 98.535 (98.427)	Acc@5 100.000 (99.976)
Epoch: [113][14/25]	Time 0.566 (0.629)	Data 0.004 (0.045)	Loss 0.2694 (0.2650)	Acc@1 98.486 (98.431)	Acc@5 99.951 (99.974)
Epoch: [113][15/25]	Time 0.590 (0.626)	Data 0.008 (0.043)	Loss 0.2608 (0.2647)	Acc@1 98.535 (98.438)	Acc@5 99.951 (99.973)
Epoch: [113][16/25]	Time 0.618 (0.626)	Data 0.006 (0.041)	Loss 0.2662 (0.2648)	Acc@1 98.633 (98.449)	Acc@5 99.951 (99.971)
Epoch: [113][17/25]	Time 0.678 (0.629)	Data 0.007 (0.039)	Loss 0.2731 (0.2653)	Acc@1 98.145 (98.432)	Acc@5 100.000 (99.973)
Epoch: [113][18/25]	Time 0.585 (0.626)	Data 0.007 (0.037)	Loss 0.2678 (0.2654)	Acc@1 98.389 (98.430)	Acc@5 99.951 (99.972)
Epoch: [113][19/25]	Time 0.661 (0.628)	Data 0.007 (0.036)	Loss 0.2710 (0.2657)	Acc@1 98.145 (98.416)	Acc@5 100.000 (99.973)
Epoch: [113][20/25]	Time 0.674 (0.630)	Data 0.008 (0.034)	Loss 0.2658 (0.2657)	Acc@1 98.193 (98.405)	Acc@5 100.000 (99.974)
Epoch: [113][21/25]	Time 0.619 (0.630)	Data 0.004 (0.033)	Loss 0.2671 (0.2658)	Acc@1 98.047 (98.389)	Acc@5 100.000 (99.976)
Epoch: [113][22/25]	Time 0.590 (0.628)	Data 0.006 (0.032)	Loss 0.2663 (0.2658)	Acc@1 98.096 (98.376)	Acc@5 100.000 (99.977)
Epoch: [113][23/25]	Time 0.647 (0.629)	Data 0.006 (0.031)	Loss 0.2676 (0.2659)	Acc@1 98.193 (98.368)	Acc@5 100.000 (99.978)
Epoch: [113][24/25]	Time 0.418 (0.620)	Data 0.005 (0.030)	Loss 0.2500 (0.2656)	Acc@1 98.939 (98.378)	Acc@5 100.000 (99.978)

Epoch: [114 | 180] LR: 0.040000
Epoch: [114][0/25]	Time 0.663 (0.663)	Data 0.595 (0.595)	Loss 0.2608 (0.2608)	Acc@1 98.633 (98.633)	Acc@5 100.000 (100.000)
Epoch: [114][1/25]	Time 0.558 (0.611)	Data 0.005 (0.300)	Loss 0.2585 (0.2597)	Acc@1 98.535 (98.584)	Acc@5 100.000 (100.000)
Epoch: [114][2/25]	Time 0.558 (0.593)	Data 0.008 (0.203)	Loss 0.2580 (0.2591)	Acc@1 98.242 (98.470)	Acc@5 100.000 (100.000)
Epoch: [114][3/25]	Time 0.606 (0.597)	Data 0.006 (0.153)	Loss 0.2550 (0.2581)	Acc@1 98.730 (98.535)	Acc@5 100.000 (100.000)
Epoch: [114][4/25]	Time 0.673 (0.612)	Data 0.006 (0.124)	Loss 0.2578 (0.2580)	Acc@1 98.682 (98.564)	Acc@5 100.000 (100.000)
Epoch: [114][5/25]	Time 0.689 (0.625)	Data 0.007 (0.104)	Loss 0.2617 (0.2586)	Acc@1 98.486 (98.551)	Acc@5 100.000 (100.000)
Epoch: [114][6/25]	Time 0.569 (0.617)	Data 0.005 (0.090)	Loss 0.2655 (0.2596)	Acc@1 98.389 (98.528)	Acc@5 99.951 (99.993)
Epoch: [114][7/25]	Time 0.592 (0.614)	Data 0.004 (0.079)	Loss 0.2641 (0.2602)	Acc@1 98.535 (98.529)	Acc@5 99.951 (99.988)
Epoch: [114][8/25]	Time 0.647 (0.617)	Data 0.005 (0.071)	Loss 0.2699 (0.2613)	Acc@1 98.193 (98.492)	Acc@5 99.951 (99.984)
Epoch: [114][9/25]	Time 0.685 (0.624)	Data 0.007 (0.065)	Loss 0.2663 (0.2618)	Acc@1 98.242 (98.467)	Acc@5 100.000 (99.985)
Epoch: [114][10/25]	Time 0.589 (0.621)	Data 0.005 (0.059)	Loss 0.2599 (0.2616)	Acc@1 98.389 (98.460)	Acc@5 100.000 (99.987)
Epoch: [114][11/25]	Time 0.603 (0.619)	Data 0.005 (0.055)	Loss 0.2548 (0.2610)	Acc@1 98.877 (98.494)	Acc@5 99.951 (99.984)
Epoch: [114][12/25]	Time 0.600 (0.618)	Data 0.007 (0.051)	Loss 0.2589 (0.2609)	Acc@1 98.389 (98.486)	Acc@5 100.000 (99.985)
Epoch: [114][13/25]	Time 0.651 (0.620)	Data 0.010 (0.048)	Loss 0.2638 (0.2611)	Acc@1 98.389 (98.479)	Acc@5 100.000 (99.986)
Epoch: [114][14/25]	Time 0.654 (0.622)	Data 0.004 (0.045)	Loss 0.2619 (0.2611)	Acc@1 98.584 (98.486)	Acc@5 100.000 (99.987)
Epoch: [114][15/25]	Time 0.575 (0.619)	Data 0.005 (0.043)	Loss 0.2628 (0.2612)	Acc@1 98.486 (98.486)	Acc@5 100.000 (99.988)
Epoch: [114][16/25]	Time 0.607 (0.619)	Data 0.007 (0.041)	Loss 0.2749 (0.2620)	Acc@1 97.656 (98.438)	Acc@5 100.000 (99.989)
Epoch: [114][17/25]	Time 0.636 (0.620)	Data 0.006 (0.039)	Loss 0.2605 (0.2620)	Acc@1 98.389 (98.435)	Acc@5 100.000 (99.989)
Epoch: [114][18/25]	Time 0.622 (0.620)	Data 0.005 (0.037)	Loss 0.2562 (0.2616)	Acc@1 98.584 (98.443)	Acc@5 100.000 (99.990)
Epoch: [114][19/25]	Time 0.570 (0.617)	Data 0.006 (0.035)	Loss 0.2641 (0.2618)	Acc@1 97.998 (98.420)	Acc@5 100.000 (99.990)
Epoch: [114][20/25]	Time 0.631 (0.618)	Data 0.004 (0.034)	Loss 0.2679 (0.2621)	Acc@1 97.998 (98.400)	Acc@5 100.000 (99.991)
Epoch: [114][21/25]	Time 0.652 (0.619)	Data 0.006 (0.033)	Loss 0.2650 (0.2622)	Acc@1 98.242 (98.393)	Acc@5 100.000 (99.991)
Epoch: [114][22/25]	Time 0.616 (0.619)	Data 0.005 (0.031)	Loss 0.2561 (0.2619)	Acc@1 98.486 (98.397)	Acc@5 100.000 (99.992)
Epoch: [114][23/25]	Time 0.647 (0.620)	Data 0.006 (0.030)	Loss 0.2615 (0.2619)	Acc@1 98.340 (98.395)	Acc@5 100.000 (99.992)
Epoch: [114][24/25]	Time 0.351 (0.610)	Data 0.006 (0.029)	Loss 0.2771 (0.2622)	Acc@1 97.759 (98.384)	Acc@5 100.000 (99.992)

Epoch: [115 | 180] LR: 0.040000
Epoch: [115][0/25]	Time 0.666 (0.666)	Data 0.776 (0.776)	Loss 0.2588 (0.2588)	Acc@1 98.340 (98.340)	Acc@5 100.000 (100.000)
Epoch: [115][1/25]	Time 0.590 (0.628)	Data 0.006 (0.391)	Loss 0.2497 (0.2542)	Acc@1 98.730 (98.535)	Acc@5 100.000 (100.000)
Epoch: [115][2/25]	Time 0.603 (0.620)	Data 0.003 (0.262)	Loss 0.2587 (0.2557)	Acc@1 98.438 (98.503)	Acc@5 100.000 (100.000)
Epoch: [115][3/25]	Time 0.652 (0.628)	Data 0.007 (0.198)	Loss 0.2593 (0.2566)	Acc@1 98.633 (98.535)	Acc@5 100.000 (100.000)
Epoch: [115][4/25]	Time 0.645 (0.631)	Data 0.009 (0.160)	Loss 0.2582 (0.2569)	Acc@1 98.486 (98.525)	Acc@5 100.000 (100.000)
Epoch: [115][5/25]	Time 0.652 (0.635)	Data 0.005 (0.134)	Loss 0.2660 (0.2584)	Acc@1 98.291 (98.486)	Acc@5 100.000 (100.000)
Epoch: [115][6/25]	Time 0.665 (0.639)	Data 0.003 (0.116)	Loss 0.2533 (0.2577)	Acc@1 98.682 (98.514)	Acc@5 100.000 (100.000)
Epoch: [115][7/25]	Time 0.704 (0.647)	Data 0.004 (0.102)	Loss 0.2488 (0.2566)	Acc@1 98.779 (98.547)	Acc@5 100.000 (100.000)
Epoch: [115][8/25]	Time 0.584 (0.640)	Data 0.005 (0.091)	Loss 0.2611 (0.2571)	Acc@1 98.584 (98.551)	Acc@5 99.951 (99.995)
Epoch: [115][9/25]	Time 0.587 (0.635)	Data 0.005 (0.082)	Loss 0.2600 (0.2574)	Acc@1 98.438 (98.540)	Acc@5 100.000 (99.995)
Epoch: [115][10/25]	Time 0.617 (0.633)	Data 0.006 (0.075)	Loss 0.2589 (0.2575)	Acc@1 98.389 (98.526)	Acc@5 100.000 (99.996)
Epoch: [115][11/25]	Time 0.604 (0.631)	Data 0.004 (0.069)	Loss 0.2574 (0.2575)	Acc@1 98.389 (98.515)	Acc@5 100.000 (99.996)
Epoch: [115][12/25]	Time 0.672 (0.634)	Data 0.005 (0.064)	Loss 0.2587 (0.2576)	Acc@1 98.535 (98.516)	Acc@5 99.951 (99.992)
Epoch: [115][13/25]	Time 0.599 (0.632)	Data 0.009 (0.060)	Loss 0.2565 (0.2575)	Acc@1 98.389 (98.507)	Acc@5 100.000 (99.993)
Epoch: [115][14/25]	Time 0.651 (0.633)	Data 0.005 (0.057)	Loss 0.2589 (0.2576)	Acc@1 98.535 (98.509)	Acc@5 100.000 (99.993)
Epoch: [115][15/25]	Time 0.643 (0.633)	Data 0.005 (0.053)	Loss 0.2672 (0.2582)	Acc@1 98.242 (98.492)	Acc@5 100.000 (99.994)
Epoch: [115][16/25]	Time 0.649 (0.634)	Data 0.006 (0.051)	Loss 0.2517 (0.2578)	Acc@1 98.828 (98.512)	Acc@5 100.000 (99.994)
Epoch: [115][17/25]	Time 0.650 (0.635)	Data 0.004 (0.048)	Loss 0.2501 (0.2574)	Acc@1 98.828 (98.530)	Acc@5 100.000 (99.995)
Epoch: [115][18/25]	Time 0.663 (0.637)	Data 0.005 (0.046)	Loss 0.2631 (0.2577)	Acc@1 98.340 (98.520)	Acc@5 100.000 (99.995)
Epoch: [115][19/25]	Time 0.633 (0.637)	Data 0.004 (0.044)	Loss 0.2513 (0.2574)	Acc@1 98.877 (98.538)	Acc@5 100.000 (99.995)
Epoch: [115][20/25]	Time 0.635 (0.636)	Data 0.005 (0.042)	Loss 0.2586 (0.2574)	Acc@1 98.779 (98.549)	Acc@5 99.951 (99.993)
Epoch: [115][21/25]	Time 0.672 (0.638)	Data 0.006 (0.040)	Loss 0.2630 (0.2577)	Acc@1 98.535 (98.548)	Acc@5 99.951 (99.991)
Epoch: [115][22/25]	Time 0.667 (0.639)	Data 0.004 (0.039)	Loss 0.2520 (0.2574)	Acc@1 98.682 (98.554)	Acc@5 100.000 (99.992)
Epoch: [115][23/25]	Time 0.654 (0.640)	Data 0.005 (0.037)	Loss 0.2594 (0.2575)	Acc@1 98.535 (98.553)	Acc@5 99.951 (99.990)
Epoch: [115][24/25]	Time 0.351 (0.628)	Data 0.005 (0.036)	Loss 0.2774 (0.2579)	Acc@1 98.113 (98.546)	Acc@5 99.882 (99.988)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 439130 ; 487386 ; 0.900990180267796

Epoch: [116 | 180] LR: 0.040000
Epoch: [116][0/25]	Time 0.611 (0.611)	Data 0.634 (0.634)	Loss 0.2450 (0.2450)	Acc@1 98.975 (98.975)	Acc@5 100.000 (100.000)
Epoch: [116][1/25]	Time 0.613 (0.612)	Data 0.006 (0.320)	Loss 0.2459 (0.2455)	Acc@1 98.975 (98.975)	Acc@5 100.000 (100.000)
Epoch: [116][2/25]	Time 0.644 (0.623)	Data 0.006 (0.215)	Loss 0.2489 (0.2466)	Acc@1 98.975 (98.975)	Acc@5 100.000 (100.000)
Epoch: [116][3/25]	Time 0.617 (0.621)	Data 0.004 (0.162)	Loss 0.2407 (0.2451)	Acc@1 99.219 (99.036)	Acc@5 100.000 (100.000)
Epoch: [116][4/25]	Time 0.599 (0.617)	Data 0.010 (0.132)	Loss 0.2470 (0.2455)	Acc@1 98.828 (98.994)	Acc@5 100.000 (100.000)
Epoch: [116][5/25]	Time 0.645 (0.622)	Data 0.005 (0.111)	Loss 0.2595 (0.2478)	Acc@1 98.389 (98.893)	Acc@5 99.951 (99.992)
Epoch: [116][6/25]	Time 0.676 (0.629)	Data 0.006 (0.096)	Loss 0.2488 (0.2480)	Acc@1 98.926 (98.898)	Acc@5 100.000 (99.993)
Epoch: [116][7/25]	Time 0.561 (0.621)	Data 0.005 (0.084)	Loss 0.2517 (0.2484)	Acc@1 98.682 (98.871)	Acc@5 99.951 (99.988)
Epoch: [116][8/25]	Time 0.573 (0.615)	Data 0.008 (0.076)	Loss 0.2551 (0.2492)	Acc@1 98.438 (98.823)	Acc@5 100.000 (99.989)
Epoch: [116][9/25]	Time 0.640 (0.618)	Data 0.005 (0.069)	Loss 0.2409 (0.2483)	Acc@1 99.023 (98.843)	Acc@5 100.000 (99.990)
Epoch: [116][10/25]	Time 0.665 (0.622)	Data 0.007 (0.063)	Loss 0.2487 (0.2484)	Acc@1 98.633 (98.824)	Acc@5 99.951 (99.987)
Epoch: [116][11/25]	Time 0.585 (0.619)	Data 0.006 (0.059)	Loss 0.2493 (0.2485)	Acc@1 98.828 (98.824)	Acc@5 100.000 (99.988)
Epoch: [116][12/25]	Time 0.566 (0.615)	Data 0.007 (0.055)	Loss 0.2555 (0.2490)	Acc@1 98.242 (98.779)	Acc@5 99.951 (99.985)
Epoch: [116][13/25]	Time 0.635 (0.616)	Data 0.007 (0.051)	Loss 0.2508 (0.2491)	Acc@1 98.584 (98.765)	Acc@5 99.951 (99.983)
Epoch: [116][14/25]	Time 0.656 (0.619)	Data 0.007 (0.048)	Loss 0.2537 (0.2494)	Acc@1 98.779 (98.766)	Acc@5 99.951 (99.980)
Epoch: [116][15/25]	Time 0.594 (0.618)	Data 0.006 (0.046)	Loss 0.2564 (0.2499)	Acc@1 98.242 (98.734)	Acc@5 99.951 (99.979)
Epoch: [116][16/25]	Time 0.562 (0.614)	Data 0.006 (0.043)	Loss 0.2565 (0.2503)	Acc@1 98.291 (98.707)	Acc@5 100.000 (99.980)
Epoch: [116][17/25]	Time 0.650 (0.616)	Data 0.005 (0.041)	Loss 0.2418 (0.2498)	Acc@1 99.121 (98.730)	Acc@5 100.000 (99.981)
Epoch: [116][18/25]	Time 0.630 (0.617)	Data 0.007 (0.039)	Loss 0.2524 (0.2499)	Acc@1 98.828 (98.736)	Acc@5 100.000 (99.982)
Epoch: [116][19/25]	Time 0.609 (0.617)	Data 0.005 (0.038)	Loss 0.2509 (0.2500)	Acc@1 98.584 (98.728)	Acc@5 99.951 (99.980)
Epoch: [116][20/25]	Time 0.608 (0.616)	Data 0.007 (0.036)	Loss 0.2425 (0.2496)	Acc@1 99.023 (98.742)	Acc@5 100.000 (99.981)
Epoch: [116][21/25]	Time 0.628 (0.617)	Data 0.006 (0.035)	Loss 0.2616 (0.2502)	Acc@1 98.193 (98.717)	Acc@5 100.000 (99.982)
Epoch: [116][22/25]	Time 0.660 (0.619)	Data 0.007 (0.034)	Loss 0.2539 (0.2503)	Acc@1 98.486 (98.707)	Acc@5 99.951 (99.981)
Epoch: [116][23/25]	Time 0.616 (0.618)	Data 0.003 (0.032)	Loss 0.2507 (0.2503)	Acc@1 98.584 (98.702)	Acc@5 100.000 (99.982)
Epoch: [116][24/25]	Time 0.362 (0.608)	Data 0.005 (0.031)	Loss 0.2616 (0.2505)	Acc@1 97.524 (98.682)	Acc@5 100.000 (99.982)

Epoch: [117 | 180] LR: 0.040000
Epoch: [117][0/25]	Time 0.596 (0.596)	Data 0.591 (0.591)	Loss 0.2457 (0.2457)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [117][1/25]	Time 0.598 (0.597)	Data 0.007 (0.299)	Loss 0.2436 (0.2446)	Acc@1 99.023 (98.926)	Acc@5 100.000 (100.000)
Epoch: [117][2/25]	Time 0.619 (0.604)	Data 0.006 (0.201)	Loss 0.2500 (0.2464)	Acc@1 98.682 (98.844)	Acc@5 100.000 (100.000)
Epoch: [117][3/25]	Time 0.624 (0.609)	Data 0.007 (0.153)	Loss 0.2483 (0.2469)	Acc@1 98.730 (98.816)	Acc@5 100.000 (100.000)
Epoch: [117][4/25]	Time 0.614 (0.610)	Data 0.009 (0.124)	Loss 0.2508 (0.2477)	Acc@1 98.779 (98.809)	Acc@5 100.000 (100.000)
Epoch: [117][5/25]	Time 0.710 (0.627)	Data 0.006 (0.104)	Loss 0.2506 (0.2482)	Acc@1 98.779 (98.804)	Acc@5 100.000 (100.000)
Epoch: [117][6/25]	Time 0.763 (0.646)	Data 0.005 (0.090)	Loss 0.2478 (0.2481)	Acc@1 99.023 (98.835)	Acc@5 100.000 (100.000)
Epoch: [117][7/25]	Time 0.712 (0.654)	Data 0.005 (0.079)	Loss 0.2453 (0.2478)	Acc@1 98.779 (98.828)	Acc@5 100.000 (100.000)
Epoch: [117][8/25]	Time 0.657 (0.655)	Data 0.004 (0.071)	Loss 0.2457 (0.2475)	Acc@1 98.975 (98.844)	Acc@5 100.000 (100.000)
Epoch: [117][9/25]	Time 0.653 (0.655)	Data 0.005 (0.064)	Loss 0.2471 (0.2475)	Acc@1 98.730 (98.833)	Acc@5 100.000 (100.000)
Epoch: [117][10/25]	Time 0.620 (0.651)	Data 0.004 (0.059)	Loss 0.2449 (0.2472)	Acc@1 98.828 (98.833)	Acc@5 100.000 (100.000)
Epoch: [117][11/25]	Time 0.645 (0.651)	Data 0.008 (0.055)	Loss 0.2550 (0.2479)	Acc@1 98.047 (98.767)	Acc@5 100.000 (100.000)
Epoch: [117][12/25]	Time 0.677 (0.653)	Data 0.009 (0.051)	Loss 0.2546 (0.2484)	Acc@1 98.389 (98.738)	Acc@5 100.000 (100.000)
Epoch: [117][13/25]	Time 0.608 (0.650)	Data 0.005 (0.048)	Loss 0.2527 (0.2487)	Acc@1 98.389 (98.713)	Acc@5 99.951 (99.997)
Epoch: [117][14/25]	Time 0.580 (0.645)	Data 0.007 (0.045)	Loss 0.2637 (0.2497)	Acc@1 97.998 (98.665)	Acc@5 100.000 (99.997)
Epoch: [117][15/25]	Time 0.616 (0.643)	Data 0.004 (0.043)	Loss 0.2586 (0.2503)	Acc@1 98.047 (98.627)	Acc@5 100.000 (99.997)
Epoch: [117][16/25]	Time 0.664 (0.644)	Data 0.008 (0.041)	Loss 0.2501 (0.2503)	Acc@1 98.486 (98.618)	Acc@5 100.000 (99.997)
Epoch: [117][17/25]	Time 0.605 (0.642)	Data 0.006 (0.039)	Loss 0.2518 (0.2503)	Acc@1 99.023 (98.641)	Acc@5 100.000 (99.997)
Epoch: [117][18/25]	Time 0.587 (0.639)	Data 0.005 (0.037)	Loss 0.2654 (0.2511)	Acc@1 97.900 (98.602)	Acc@5 99.951 (99.995)
Epoch: [117][19/25]	Time 0.651 (0.640)	Data 0.006 (0.035)	Loss 0.2613 (0.2516)	Acc@1 98.145 (98.579)	Acc@5 100.000 (99.995)
Epoch: [117][20/25]	Time 0.679 (0.642)	Data 0.004 (0.034)	Loss 0.2447 (0.2513)	Acc@1 99.023 (98.600)	Acc@5 99.951 (99.993)
Epoch: [117][21/25]	Time 0.604 (0.640)	Data 0.005 (0.033)	Loss 0.2605 (0.2517)	Acc@1 97.852 (98.566)	Acc@5 99.951 (99.991)
Epoch: [117][22/25]	Time 0.609 (0.639)	Data 0.008 (0.031)	Loss 0.2540 (0.2518)	Acc@1 98.193 (98.550)	Acc@5 100.000 (99.992)
Epoch: [117][23/25]	Time 0.653 (0.639)	Data 0.007 (0.030)	Loss 0.2490 (0.2517)	Acc@1 98.730 (98.558)	Acc@5 99.951 (99.990)
Epoch: [117][24/25]	Time 0.402 (0.630)	Data 0.005 (0.029)	Loss 0.2479 (0.2516)	Acc@1 98.703 (98.560)	Acc@5 100.000 (99.990)

Epoch: [118 | 180] LR: 0.040000
Epoch: [118][0/25]	Time 0.626 (0.626)	Data 0.629 (0.629)	Loss 0.2477 (0.2477)	Acc@1 98.682 (98.682)	Acc@5 100.000 (100.000)
Epoch: [118][1/25]	Time 0.597 (0.612)	Data 0.008 (0.318)	Loss 0.2431 (0.2454)	Acc@1 98.877 (98.779)	Acc@5 100.000 (100.000)
Epoch: [118][2/25]	Time 0.555 (0.593)	Data 0.005 (0.214)	Loss 0.2530 (0.2479)	Acc@1 98.535 (98.698)	Acc@5 100.000 (100.000)
Epoch: [118][3/25]	Time 0.594 (0.593)	Data 0.009 (0.163)	Loss 0.2491 (0.2482)	Acc@1 98.633 (98.682)	Acc@5 100.000 (100.000)
Epoch: [118][4/25]	Time 0.641 (0.603)	Data 0.006 (0.131)	Loss 0.2392 (0.2464)	Acc@1 98.975 (98.740)	Acc@5 100.000 (100.000)
Epoch: [118][5/25]	Time 0.667 (0.613)	Data 0.005 (0.110)	Loss 0.2444 (0.2461)	Acc@1 99.023 (98.787)	Acc@5 100.000 (100.000)
Epoch: [118][6/25]	Time 0.586 (0.609)	Data 0.005 (0.095)	Loss 0.2503 (0.2467)	Acc@1 98.389 (98.730)	Acc@5 100.000 (100.000)
Epoch: [118][7/25]	Time 0.565 (0.604)	Data 0.006 (0.084)	Loss 0.2486 (0.2469)	Acc@1 98.779 (98.737)	Acc@5 99.902 (99.988)
Epoch: [118][8/25]	Time 0.584 (0.602)	Data 0.010 (0.076)	Loss 0.2441 (0.2466)	Acc@1 98.633 (98.725)	Acc@5 100.000 (99.989)
Epoch: [118][9/25]	Time 0.612 (0.603)	Data 0.005 (0.069)	Loss 0.2524 (0.2472)	Acc@1 98.340 (98.687)	Acc@5 100.000 (99.990)
Epoch: [118][10/25]	Time 0.664 (0.608)	Data 0.005 (0.063)	Loss 0.2503 (0.2475)	Acc@1 98.584 (98.677)	Acc@5 100.000 (99.991)
Epoch: [118][11/25]	Time 0.641 (0.611)	Data 0.005 (0.058)	Loss 0.2491 (0.2476)	Acc@1 98.535 (98.665)	Acc@5 100.000 (99.992)
Epoch: [118][12/25]	Time 0.604 (0.611)	Data 0.005 (0.054)	Loss 0.2464 (0.2475)	Acc@1 98.633 (98.663)	Acc@5 100.000 (99.992)
Epoch: [118][13/25]	Time 0.572 (0.608)	Data 0.005 (0.050)	Loss 0.2474 (0.2475)	Acc@1 98.682 (98.664)	Acc@5 99.951 (99.990)
Epoch: [118][14/25]	Time 0.595 (0.607)	Data 0.006 (0.047)	Loss 0.2395 (0.2470)	Acc@1 99.219 (98.701)	Acc@5 100.000 (99.990)
Epoch: [118][15/25]	Time 0.646 (0.609)	Data 0.005 (0.045)	Loss 0.2501 (0.2472)	Acc@1 98.486 (98.688)	Acc@5 100.000 (99.991)
Epoch: [118][16/25]	Time 0.633 (0.611)	Data 0.007 (0.043)	Loss 0.2431 (0.2469)	Acc@1 98.584 (98.682)	Acc@5 100.000 (99.991)
Epoch: [118][17/25]	Time 0.599 (0.610)	Data 0.007 (0.041)	Loss 0.2557 (0.2474)	Acc@1 98.438 (98.668)	Acc@5 99.951 (99.989)
Epoch: [118][18/25]	Time 0.602 (0.610)	Data 0.008 (0.039)	Loss 0.2461 (0.2473)	Acc@1 98.633 (98.666)	Acc@5 100.000 (99.990)
Epoch: [118][19/25]	Time 0.593 (0.609)	Data 0.009 (0.037)	Loss 0.2546 (0.2477)	Acc@1 98.340 (98.650)	Acc@5 100.000 (99.990)
Epoch: [118][20/25]	Time 0.642 (0.610)	Data 0.006 (0.036)	Loss 0.2509 (0.2479)	Acc@1 98.438 (98.640)	Acc@5 100.000 (99.991)
Epoch: [118][21/25]	Time 0.663 (0.613)	Data 0.006 (0.035)	Loss 0.2489 (0.2479)	Acc@1 98.486 (98.633)	Acc@5 99.951 (99.989)
Epoch: [118][22/25]	Time 0.610 (0.613)	Data 0.005 (0.033)	Loss 0.2484 (0.2479)	Acc@1 98.389 (98.622)	Acc@5 100.000 (99.989)
Epoch: [118][23/25]	Time 0.623 (0.613)	Data 0.007 (0.032)	Loss 0.2537 (0.2482)	Acc@1 98.340 (98.610)	Acc@5 100.000 (99.990)
Epoch: [118][24/25]	Time 0.307 (0.601)	Data 0.004 (0.031)	Loss 0.2564 (0.2483)	Acc@1 98.231 (98.604)	Acc@5 100.000 (99.990)

Epoch: [119 | 180] LR: 0.040000
Epoch: [119][0/25]	Time 0.590 (0.590)	Data 0.769 (0.769)	Loss 0.2530 (0.2530)	Acc@1 98.535 (98.535)	Acc@5 100.000 (100.000)
Epoch: [119][1/25]	Time 0.609 (0.599)	Data 0.004 (0.386)	Loss 0.2501 (0.2515)	Acc@1 98.682 (98.608)	Acc@5 100.000 (100.000)
Epoch: [119][2/25]	Time 0.633 (0.611)	Data 0.005 (0.259)	Loss 0.2382 (0.2471)	Acc@1 98.926 (98.714)	Acc@5 100.000 (100.000)
Epoch: [119][3/25]	Time 0.653 (0.621)	Data 0.005 (0.196)	Loss 0.2467 (0.2470)	Acc@1 98.340 (98.621)	Acc@5 99.951 (99.988)
Epoch: [119][4/25]	Time 0.650 (0.627)	Data 0.008 (0.158)	Loss 0.2419 (0.2460)	Acc@1 98.730 (98.643)	Acc@5 100.000 (99.990)
Epoch: [119][5/25]	Time 0.575 (0.618)	Data 0.006 (0.133)	Loss 0.2479 (0.2463)	Acc@1 98.779 (98.665)	Acc@5 100.000 (99.992)
Epoch: [119][6/25]	Time 0.602 (0.616)	Data 0.006 (0.115)	Loss 0.2460 (0.2463)	Acc@1 98.438 (98.633)	Acc@5 99.902 (99.979)
Epoch: [119][7/25]	Time 0.581 (0.611)	Data 0.006 (0.101)	Loss 0.2576 (0.2477)	Acc@1 98.340 (98.596)	Acc@5 100.000 (99.982)
Epoch: [119][8/25]	Time 0.602 (0.610)	Data 0.004 (0.090)	Loss 0.2385 (0.2466)	Acc@1 99.023 (98.644)	Acc@5 100.000 (99.984)
Epoch: [119][9/25]	Time 0.647 (0.614)	Data 0.004 (0.082)	Loss 0.2525 (0.2472)	Acc@1 98.291 (98.608)	Acc@5 99.902 (99.976)
Epoch: [119][10/25]	Time 0.671 (0.619)	Data 0.005 (0.075)	Loss 0.2510 (0.2476)	Acc@1 98.633 (98.611)	Acc@5 99.902 (99.969)
Epoch: [119][11/25]	Time 0.561 (0.614)	Data 0.008 (0.069)	Loss 0.2439 (0.2473)	Acc@1 98.584 (98.608)	Acc@5 100.000 (99.972)
Epoch: [119][12/25]	Time 0.595 (0.613)	Data 0.007 (0.064)	Loss 0.2393 (0.2467)	Acc@1 98.730 (98.618)	Acc@5 100.000 (99.974)
Epoch: [119][13/25]	Time 0.597 (0.612)	Data 0.005 (0.060)	Loss 0.2407 (0.2462)	Acc@1 98.926 (98.640)	Acc@5 99.951 (99.972)
Epoch: [119][14/25]	Time 0.659 (0.615)	Data 0.006 (0.056)	Loss 0.2400 (0.2458)	Acc@1 98.828 (98.652)	Acc@5 99.951 (99.971)
Epoch: [119][15/25]	Time 0.684 (0.619)	Data 0.005 (0.053)	Loss 0.2443 (0.2457)	Acc@1 98.633 (98.651)	Acc@5 100.000 (99.973)
Epoch: [119][16/25]	Time 0.585 (0.617)	Data 0.005 (0.050)	Loss 0.2420 (0.2455)	Acc@1 98.828 (98.662)	Acc@5 100.000 (99.974)
Epoch: [119][17/25]	Time 0.808 (0.628)	Data 0.003 (0.048)	Loss 0.2418 (0.2453)	Acc@1 98.828 (98.671)	Acc@5 100.000 (99.976)
Epoch: [119][18/25]	Time 0.731 (0.633)	Data 0.005 (0.046)	Loss 0.2547 (0.2458)	Acc@1 98.242 (98.648)	Acc@5 100.000 (99.977)
Epoch: [119][19/25]	Time 0.603 (0.632)	Data 0.005 (0.043)	Loss 0.2512 (0.2461)	Acc@1 98.389 (98.635)	Acc@5 99.902 (99.973)
Epoch: [119][20/25]	Time 0.617 (0.631)	Data 0.007 (0.042)	Loss 0.2488 (0.2462)	Acc@1 98.486 (98.628)	Acc@5 99.951 (99.972)
Epoch: [119][21/25]	Time 0.657 (0.632)	Data 0.007 (0.040)	Loss 0.2508 (0.2464)	Acc@1 98.145 (98.606)	Acc@5 99.951 (99.971)
Epoch: [119][22/25]	Time 0.665 (0.634)	Data 0.007 (0.039)	Loss 0.2452 (0.2464)	Acc@1 98.242 (98.590)	Acc@5 100.000 (99.972)
Epoch: [119][23/25]	Time 0.615 (0.633)	Data 0.004 (0.037)	Loss 0.2528 (0.2466)	Acc@1 98.486 (98.586)	Acc@5 100.000 (99.974)
Epoch: [119][24/25]	Time 0.345 (0.621)	Data 0.008 (0.036)	Loss 0.2549 (0.2468)	Acc@1 98.231 (98.580)	Acc@5 100.000 (99.974)

Epoch: [120 | 180] LR: 0.040000
Epoch: [120][0/25]	Time 0.649 (0.649)	Data 0.611 (0.611)	Loss 0.2443 (0.2443)	Acc@1 98.682 (98.682)	Acc@5 100.000 (100.000)
Epoch: [120][1/25]	Time 0.625 (0.637)	Data 0.006 (0.309)	Loss 0.2417 (0.2430)	Acc@1 98.877 (98.779)	Acc@5 100.000 (100.000)
Epoch: [120][2/25]	Time 0.552 (0.609)	Data 0.006 (0.208)	Loss 0.2433 (0.2431)	Acc@1 98.877 (98.812)	Acc@5 100.000 (100.000)
Epoch: [120][3/25]	Time 0.565 (0.598)	Data 0.003 (0.157)	Loss 0.2400 (0.2423)	Acc@1 98.926 (98.840)	Acc@5 100.000 (100.000)
Epoch: [120][4/25]	Time 0.655 (0.609)	Data 0.008 (0.127)	Loss 0.2364 (0.2411)	Acc@1 98.730 (98.818)	Acc@5 100.000 (100.000)
Epoch: [120][5/25]	Time 0.667 (0.619)	Data 0.005 (0.106)	Loss 0.2501 (0.2426)	Acc@1 98.047 (98.690)	Acc@5 100.000 (100.000)
Epoch: [120][6/25]	Time 0.579 (0.613)	Data 0.004 (0.092)	Loss 0.2494 (0.2436)	Acc@1 98.291 (98.633)	Acc@5 99.951 (99.993)
Epoch: [120][7/25]	Time 0.566 (0.607)	Data 0.004 (0.081)	Loss 0.2383 (0.2429)	Acc@1 98.828 (98.657)	Acc@5 100.000 (99.994)
Epoch: [120][8/25]	Time 0.610 (0.608)	Data 0.007 (0.073)	Loss 0.2388 (0.2425)	Acc@1 98.926 (98.687)	Acc@5 100.000 (99.995)
Epoch: [120][9/25]	Time 0.669 (0.614)	Data 0.007 (0.066)	Loss 0.2439 (0.2426)	Acc@1 98.584 (98.677)	Acc@5 100.000 (99.995)
Epoch: [120][10/25]	Time 0.667 (0.619)	Data 0.006 (0.061)	Loss 0.2391 (0.2423)	Acc@1 98.926 (98.699)	Acc@5 100.000 (99.996)
Epoch: [120][11/25]	Time 0.650 (0.621)	Data 0.007 (0.056)	Loss 0.2510 (0.2430)	Acc@1 98.438 (98.678)	Acc@5 99.951 (99.992)
Epoch: [120][12/25]	Time 0.658 (0.624)	Data 0.006 (0.052)	Loss 0.2483 (0.2434)	Acc@1 98.291 (98.648)	Acc@5 100.000 (99.992)
Epoch: [120][13/25]	Time 0.599 (0.622)	Data 0.005 (0.049)	Loss 0.2334 (0.2427)	Acc@1 99.023 (98.675)	Acc@5 100.000 (99.993)
Epoch: [120][14/25]	Time 0.683 (0.626)	Data 0.006 (0.046)	Loss 0.2345 (0.2422)	Acc@1 99.170 (98.708)	Acc@5 100.000 (99.993)
Epoch: [120][15/25]	Time 0.613 (0.626)	Data 0.009 (0.044)	Loss 0.2485 (0.2426)	Acc@1 98.291 (98.682)	Acc@5 100.000 (99.994)
Epoch: [120][16/25]	Time 0.577 (0.623)	Data 0.005 (0.041)	Loss 0.2487 (0.2429)	Acc@1 98.340 (98.662)	Acc@5 99.951 (99.991)
Epoch: [120][17/25]	Time 0.654 (0.624)	Data 0.006 (0.039)	Loss 0.2542 (0.2435)	Acc@1 98.096 (98.630)	Acc@5 100.000 (99.992)
Epoch: [120][18/25]	Time 0.633 (0.625)	Data 0.008 (0.038)	Loss 0.2491 (0.2438)	Acc@1 98.291 (98.612)	Acc@5 100.000 (99.992)
Epoch: [120][19/25]	Time 0.607 (0.624)	Data 0.004 (0.036)	Loss 0.2453 (0.2439)	Acc@1 98.682 (98.616)	Acc@5 100.000 (99.993)
Epoch: [120][20/25]	Time 0.596 (0.623)	Data 0.006 (0.035)	Loss 0.2431 (0.2439)	Acc@1 98.633 (98.617)	Acc@5 100.000 (99.993)
Epoch: [120][21/25]	Time 0.667 (0.625)	Data 0.007 (0.033)	Loss 0.2472 (0.2440)	Acc@1 98.682 (98.619)	Acc@5 100.000 (99.993)
Epoch: [120][22/25]	Time 0.674 (0.627)	Data 0.006 (0.032)	Loss 0.2458 (0.2441)	Acc@1 98.633 (98.620)	Acc@5 100.000 (99.994)
Epoch: [120][23/25]	Time 0.638 (0.627)	Data 0.006 (0.031)	Loss 0.2425 (0.2440)	Acc@1 98.438 (98.612)	Acc@5 100.000 (99.994)
Epoch: [120][24/25]	Time 0.333 (0.616)	Data 0.005 (0.030)	Loss 0.2468 (0.2441)	Acc@1 98.703 (98.614)	Acc@5 100.000 (99.994)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [121 | 180] LR: 0.040000
Epoch: [121][0/25]	Time 0.671 (0.671)	Data 0.759 (0.759)	Loss 0.2412 (0.2412)	Acc@1 98.730 (98.730)	Acc@5 99.951 (99.951)
Epoch: [121][1/25]	Time 0.646 (0.659)	Data 0.005 (0.382)	Loss 0.2328 (0.2370)	Acc@1 99.023 (98.877)	Acc@5 100.000 (99.976)
Epoch: [121][2/25]	Time 0.651 (0.656)	Data 0.006 (0.256)	Loss 0.2318 (0.2353)	Acc@1 99.170 (98.975)	Acc@5 100.000 (99.984)
Epoch: [121][3/25]	Time 0.633 (0.650)	Data 0.003 (0.193)	Loss 0.2371 (0.2357)	Acc@1 98.682 (98.901)	Acc@5 100.000 (99.988)
Epoch: [121][4/25]	Time 0.690 (0.658)	Data 0.007 (0.156)	Loss 0.2310 (0.2348)	Acc@1 99.023 (98.926)	Acc@5 100.000 (99.990)
Epoch: [121][5/25]	Time 0.601 (0.649)	Data 0.007 (0.131)	Loss 0.2353 (0.2349)	Acc@1 98.828 (98.910)	Acc@5 100.000 (99.992)
Epoch: [121][6/25]	Time 0.548 (0.634)	Data 0.004 (0.113)	Loss 0.2323 (0.2345)	Acc@1 99.170 (98.947)	Acc@5 100.000 (99.993)
Epoch: [121][7/25]	Time 0.592 (0.629)	Data 0.004 (0.099)	Loss 0.2328 (0.2343)	Acc@1 99.072 (98.962)	Acc@5 100.000 (99.994)
Epoch: [121][8/25]	Time 0.669 (0.633)	Data 0.008 (0.089)	Loss 0.2402 (0.2349)	Acc@1 98.633 (98.926)	Acc@5 100.000 (99.995)
Epoch: [121][9/25]	Time 0.660 (0.636)	Data 0.006 (0.081)	Loss 0.2385 (0.2353)	Acc@1 98.828 (98.916)	Acc@5 100.000 (99.995)
Epoch: [121][10/25]	Time 0.586 (0.632)	Data 0.007 (0.074)	Loss 0.2383 (0.2356)	Acc@1 98.633 (98.890)	Acc@5 100.000 (99.996)
Epoch: [121][11/25]	Time 0.629 (0.631)	Data 0.007 (0.069)	Loss 0.2271 (0.2349)	Acc@1 99.365 (98.930)	Acc@5 100.000 (99.996)
Epoch: [121][12/25]	Time 0.683 (0.635)	Data 0.007 (0.064)	Loss 0.2377 (0.2351)	Acc@1 98.779 (98.918)	Acc@5 100.000 (99.996)
Epoch: [121][13/25]	Time 0.574 (0.631)	Data 0.004 (0.060)	Loss 0.2399 (0.2354)	Acc@1 98.779 (98.908)	Acc@5 100.000 (99.997)
Epoch: [121][14/25]	Time 0.579 (0.627)	Data 0.004 (0.056)	Loss 0.2512 (0.2365)	Acc@1 98.389 (98.874)	Acc@5 100.000 (99.997)
Epoch: [121][15/25]	Time 0.627 (0.627)	Data 0.006 (0.053)	Loss 0.2376 (0.2366)	Acc@1 99.072 (98.886)	Acc@5 100.000 (99.997)
Epoch: [121][16/25]	Time 0.667 (0.630)	Data 0.007 (0.050)	Loss 0.2377 (0.2366)	Acc@1 98.779 (98.880)	Acc@5 100.000 (99.997)
Epoch: [121][17/25]	Time 0.623 (0.629)	Data 0.005 (0.048)	Loss 0.2356 (0.2366)	Acc@1 99.121 (98.893)	Acc@5 100.000 (99.997)
Epoch: [121][18/25]	Time 0.683 (0.632)	Data 0.007 (0.046)	Loss 0.2337 (0.2364)	Acc@1 98.975 (98.898)	Acc@5 100.000 (99.997)
Epoch: [121][19/25]	Time 0.657 (0.634)	Data 0.005 (0.043)	Loss 0.2434 (0.2368)	Acc@1 98.340 (98.870)	Acc@5 100.000 (99.998)
Epoch: [121][20/25]	Time 0.658 (0.635)	Data 0.005 (0.042)	Loss 0.2333 (0.2366)	Acc@1 99.023 (98.877)	Acc@5 99.951 (99.995)
Epoch: [121][21/25]	Time 0.687 (0.637)	Data 0.010 (0.040)	Loss 0.2412 (0.2368)	Acc@1 98.584 (98.864)	Acc@5 100.000 (99.996)
Epoch: [121][22/25]	Time 0.660 (0.638)	Data 0.005 (0.039)	Loss 0.2413 (0.2370)	Acc@1 98.877 (98.864)	Acc@5 100.000 (99.996)
Epoch: [121][23/25]	Time 0.630 (0.638)	Data 0.005 (0.037)	Loss 0.2379 (0.2370)	Acc@1 98.633 (98.855)	Acc@5 100.000 (99.996)
Epoch: [121][24/25]	Time 0.408 (0.629)	Data 0.006 (0.036)	Loss 0.2357 (0.2370)	Acc@1 99.057 (98.858)	Acc@5 100.000 (99.996)

Epoch: [122 | 180] LR: 0.040000
Epoch: [122][0/25]	Time 0.650 (0.650)	Data 0.587 (0.587)	Loss 0.2380 (0.2380)	Acc@1 98.633 (98.633)	Acc@5 100.000 (100.000)
Epoch: [122][1/25]	Time 0.589 (0.619)	Data 0.006 (0.297)	Loss 0.2347 (0.2363)	Acc@1 98.682 (98.657)	Acc@5 100.000 (100.000)
Epoch: [122][2/25]	Time 0.599 (0.613)	Data 0.007 (0.200)	Loss 0.2390 (0.2372)	Acc@1 98.535 (98.617)	Acc@5 100.000 (100.000)
Epoch: [122][3/25]	Time 0.654 (0.623)	Data 0.005 (0.151)	Loss 0.2352 (0.2367)	Acc@1 98.975 (98.706)	Acc@5 100.000 (100.000)
Epoch: [122][4/25]	Time 0.651 (0.628)	Data 0.005 (0.122)	Loss 0.2333 (0.2360)	Acc@1 98.877 (98.740)	Acc@5 99.951 (99.990)
Epoch: [122][5/25]	Time 0.683 (0.637)	Data 0.005 (0.102)	Loss 0.2371 (0.2362)	Acc@1 98.633 (98.722)	Acc@5 100.000 (99.992)
Epoch: [122][6/25]	Time 0.641 (0.638)	Data 0.006 (0.089)	Loss 0.2306 (0.2354)	Acc@1 99.072 (98.772)	Acc@5 100.000 (99.993)
Epoch: [122][7/25]	Time 0.672 (0.642)	Data 0.003 (0.078)	Loss 0.2412 (0.2361)	Acc@1 98.682 (98.761)	Acc@5 99.951 (99.988)
Epoch: [122][8/25]	Time 0.640 (0.642)	Data 0.004 (0.070)	Loss 0.2320 (0.2357)	Acc@1 98.926 (98.779)	Acc@5 100.000 (99.989)
Epoch: [122][9/25]	Time 0.647 (0.643)	Data 0.005 (0.063)	Loss 0.2360 (0.2357)	Acc@1 98.779 (98.779)	Acc@5 99.951 (99.985)
Epoch: [122][10/25]	Time 0.677 (0.646)	Data 0.005 (0.058)	Loss 0.2395 (0.2361)	Acc@1 98.682 (98.770)	Acc@5 100.000 (99.987)
Epoch: [122][11/25]	Time 0.634 (0.645)	Data 0.005 (0.054)	Loss 0.2342 (0.2359)	Acc@1 98.975 (98.787)	Acc@5 100.000 (99.988)
Epoch: [122][12/25]	Time 0.678 (0.647)	Data 0.006 (0.050)	Loss 0.2386 (0.2361)	Acc@1 98.633 (98.776)	Acc@5 100.000 (99.989)
Epoch: [122][13/25]	Time 0.585 (0.643)	Data 0.006 (0.047)	Loss 0.2371 (0.2362)	Acc@1 98.779 (98.776)	Acc@5 99.951 (99.986)
Epoch: [122][14/25]	Time 0.663 (0.644)	Data 0.004 (0.044)	Loss 0.2348 (0.2361)	Acc@1 98.730 (98.773)	Acc@5 100.000 (99.987)
Epoch: [122][15/25]	Time 0.680 (0.646)	Data 0.007 (0.042)	Loss 0.2349 (0.2360)	Acc@1 98.779 (98.773)	Acc@5 99.902 (99.982)
Epoch: [122][16/25]	Time 0.579 (0.642)	Data 0.005 (0.040)	Loss 0.2433 (0.2364)	Acc@1 98.242 (98.742)	Acc@5 99.951 (99.980)
Epoch: [122][17/25]	Time 0.559 (0.638)	Data 0.004 (0.038)	Loss 0.2391 (0.2366)	Acc@1 98.828 (98.747)	Acc@5 100.000 (99.981)
Epoch: [122][18/25]	Time 0.619 (0.637)	Data 0.005 (0.036)	Loss 0.2432 (0.2369)	Acc@1 98.291 (98.723)	Acc@5 100.000 (99.982)
Epoch: [122][19/25]	Time 0.641 (0.637)	Data 0.005 (0.034)	Loss 0.2453 (0.2374)	Acc@1 98.242 (98.699)	Acc@5 100.000 (99.983)
Epoch: [122][20/25]	Time 0.656 (0.638)	Data 0.006 (0.033)	Loss 0.2458 (0.2378)	Acc@1 98.340 (98.682)	Acc@5 100.000 (99.984)
Epoch: [122][21/25]	Time 0.608 (0.637)	Data 0.004 (0.032)	Loss 0.2374 (0.2377)	Acc@1 98.730 (98.684)	Acc@5 100.000 (99.984)
Epoch: [122][22/25]	Time 0.629 (0.636)	Data 0.008 (0.031)	Loss 0.2338 (0.2376)	Acc@1 98.779 (98.688)	Acc@5 100.000 (99.985)
Epoch: [122][23/25]	Time 0.666 (0.637)	Data 0.008 (0.030)	Loss 0.2372 (0.2376)	Acc@1 98.682 (98.688)	Acc@5 100.000 (99.986)
Epoch: [122][24/25]	Time 0.337 (0.625)	Data 0.004 (0.029)	Loss 0.2345 (0.2375)	Acc@1 98.939 (98.692)	Acc@5 100.000 (99.986)

Epoch: [123 | 180] LR: 0.040000
Epoch: [123][0/25]	Time 0.645 (0.645)	Data 0.621 (0.621)	Loss 0.2242 (0.2242)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [123][1/25]	Time 0.664 (0.655)	Data 0.004 (0.313)	Loss 0.2332 (0.2287)	Acc@1 99.121 (99.170)	Acc@5 100.000 (100.000)
Epoch: [123][2/25]	Time 0.646 (0.652)	Data 0.007 (0.211)	Loss 0.2251 (0.2275)	Acc@1 98.926 (99.089)	Acc@5 100.000 (100.000)
Epoch: [123][3/25]	Time 0.615 (0.643)	Data 0.004 (0.159)	Loss 0.2358 (0.2296)	Acc@1 99.023 (99.072)	Acc@5 99.951 (99.988)
Epoch: [123][4/25]	Time 0.603 (0.635)	Data 0.007 (0.129)	Loss 0.2429 (0.2322)	Acc@1 98.535 (98.965)	Acc@5 100.000 (99.990)
Epoch: [123][5/25]	Time 0.659 (0.639)	Data 0.006 (0.108)	Loss 0.2274 (0.2314)	Acc@1 99.170 (98.999)	Acc@5 100.000 (99.992)
Epoch: [123][6/25]	Time 0.615 (0.635)	Data 0.007 (0.094)	Loss 0.2401 (0.2327)	Acc@1 98.340 (98.905)	Acc@5 100.000 (99.993)
Epoch: [123][7/25]	Time 0.643 (0.636)	Data 0.005 (0.083)	Loss 0.2405 (0.2337)	Acc@1 98.633 (98.871)	Acc@5 100.000 (99.994)
Epoch: [123][8/25]	Time 0.657 (0.639)	Data 0.008 (0.074)	Loss 0.2356 (0.2339)	Acc@1 99.268 (98.915)	Acc@5 99.951 (99.989)
Epoch: [123][9/25]	Time 0.640 (0.639)	Data 0.004 (0.067)	Loss 0.2406 (0.2345)	Acc@1 98.438 (98.867)	Acc@5 100.000 (99.990)
Epoch: [123][10/25]	Time 0.654 (0.640)	Data 0.005 (0.062)	Loss 0.2320 (0.2343)	Acc@1 98.828 (98.864)	Acc@5 100.000 (99.991)
Epoch: [123][11/25]	Time 0.623 (0.639)	Data 0.004 (0.057)	Loss 0.2263 (0.2336)	Acc@1 99.316 (98.901)	Acc@5 100.000 (99.992)
Epoch: [123][12/25]	Time 0.597 (0.636)	Data 0.006 (0.053)	Loss 0.2313 (0.2335)	Acc@1 98.975 (98.907)	Acc@5 100.000 (99.992)
Epoch: [123][13/25]	Time 0.634 (0.635)	Data 0.004 (0.049)	Loss 0.2244 (0.2328)	Acc@1 99.170 (98.926)	Acc@5 100.000 (99.993)
Epoch: [123][14/25]	Time 0.653 (0.637)	Data 0.007 (0.047)	Loss 0.2331 (0.2328)	Acc@1 98.975 (98.929)	Acc@5 100.000 (99.993)
Epoch: [123][15/25]	Time 0.627 (0.636)	Data 0.006 (0.044)	Loss 0.2270 (0.2325)	Acc@1 98.877 (98.926)	Acc@5 100.000 (99.994)
Epoch: [123][16/25]	Time 0.694 (0.640)	Data 0.007 (0.042)	Loss 0.2415 (0.2330)	Acc@1 98.877 (98.923)	Acc@5 99.951 (99.991)
Epoch: [123][17/25]	Time 0.660 (0.641)	Data 0.005 (0.040)	Loss 0.2273 (0.2327)	Acc@1 99.072 (98.931)	Acc@5 100.000 (99.992)
Epoch: [123][18/25]	Time 0.671 (0.642)	Data 0.004 (0.038)	Loss 0.2258 (0.2323)	Acc@1 99.219 (98.946)	Acc@5 100.000 (99.992)
Epoch: [123][19/25]	Time 0.660 (0.643)	Data 0.004 (0.036)	Loss 0.2300 (0.2322)	Acc@1 98.779 (98.938)	Acc@5 100.000 (99.993)
Epoch: [123][20/25]	Time 0.634 (0.643)	Data 0.005 (0.035)	Loss 0.2399 (0.2326)	Acc@1 98.291 (98.907)	Acc@5 100.000 (99.993)
Epoch: [123][21/25]	Time 0.634 (0.642)	Data 0.006 (0.033)	Loss 0.2346 (0.2327)	Acc@1 98.633 (98.895)	Acc@5 100.000 (99.993)
Epoch: [123][22/25]	Time 0.632 (0.642)	Data 0.006 (0.032)	Loss 0.2349 (0.2328)	Acc@1 98.828 (98.892)	Acc@5 100.000 (99.994)
Epoch: [123][23/25]	Time 0.667 (0.643)	Data 0.007 (0.031)	Loss 0.2354 (0.2329)	Acc@1 98.828 (98.889)	Acc@5 99.951 (99.992)
Epoch: [123][24/25]	Time 0.375 (0.632)	Data 0.006 (0.030)	Loss 0.2361 (0.2329)	Acc@1 98.821 (98.888)	Acc@5 100.000 (99.992)

Epoch: [124 | 180] LR: 0.040000
Epoch: [124][0/25]	Time 0.641 (0.641)	Data 0.593 (0.593)	Loss 0.2292 (0.2292)	Acc@1 98.975 (98.975)	Acc@5 100.000 (100.000)
Epoch: [124][1/25]	Time 0.678 (0.660)	Data 0.008 (0.301)	Loss 0.2357 (0.2324)	Acc@1 98.389 (98.682)	Acc@5 99.951 (99.976)
Epoch: [124][2/25]	Time 0.656 (0.659)	Data 0.005 (0.202)	Loss 0.2294 (0.2314)	Acc@1 99.023 (98.796)	Acc@5 100.000 (99.984)
Epoch: [124][3/25]	Time 0.609 (0.646)	Data 0.004 (0.153)	Loss 0.2352 (0.2324)	Acc@1 98.730 (98.779)	Acc@5 100.000 (99.988)
Epoch: [124][4/25]	Time 0.667 (0.650)	Data 0.005 (0.123)	Loss 0.2348 (0.2328)	Acc@1 98.877 (98.799)	Acc@5 99.951 (99.980)
Epoch: [124][5/25]	Time 0.643 (0.649)	Data 0.005 (0.103)	Loss 0.2384 (0.2338)	Acc@1 98.682 (98.779)	Acc@5 100.000 (99.984)
Epoch: [124][6/25]	Time 0.674 (0.653)	Data 0.007 (0.090)	Loss 0.2360 (0.2341)	Acc@1 98.682 (98.765)	Acc@5 100.000 (99.986)
Epoch: [124][7/25]	Time 0.640 (0.651)	Data 0.005 (0.079)	Loss 0.2422 (0.2351)	Acc@1 98.242 (98.700)	Acc@5 100.000 (99.988)
Epoch: [124][8/25]	Time 0.673 (0.654)	Data 0.004 (0.071)	Loss 0.2392 (0.2356)	Acc@1 98.340 (98.660)	Acc@5 100.000 (99.989)
Epoch: [124][9/25]	Time 0.664 (0.655)	Data 0.007 (0.064)	Loss 0.2253 (0.2345)	Acc@1 99.219 (98.716)	Acc@5 100.000 (99.990)
Epoch: [124][10/25]	Time 0.673 (0.656)	Data 0.008 (0.059)	Loss 0.2381 (0.2349)	Acc@1 98.633 (98.708)	Acc@5 100.000 (99.991)
Epoch: [124][11/25]	Time 0.595 (0.651)	Data 0.006 (0.055)	Loss 0.2397 (0.2353)	Acc@1 98.535 (98.694)	Acc@5 100.000 (99.992)
Epoch: [124][12/25]	Time 0.599 (0.647)	Data 0.007 (0.051)	Loss 0.2401 (0.2356)	Acc@1 98.486 (98.678)	Acc@5 100.000 (99.992)
Epoch: [124][13/25]	Time 0.603 (0.644)	Data 0.006 (0.048)	Loss 0.2351 (0.2356)	Acc@1 98.877 (98.692)	Acc@5 99.951 (99.990)
Epoch: [124][14/25]	Time 0.631 (0.643)	Data 0.007 (0.045)	Loss 0.2349 (0.2355)	Acc@1 98.730 (98.695)	Acc@5 100.000 (99.990)
Epoch: [124][15/25]	Time 0.651 (0.644)	Data 0.007 (0.043)	Loss 0.2396 (0.2358)	Acc@1 98.291 (98.669)	Acc@5 100.000 (99.991)
Epoch: [124][16/25]	Time 0.600 (0.641)	Data 0.008 (0.041)	Loss 0.2353 (0.2358)	Acc@1 98.438 (98.656)	Acc@5 100.000 (99.991)
Epoch: [124][17/25]	Time 0.643 (0.641)	Data 0.009 (0.039)	Loss 0.2229 (0.2351)	Acc@1 99.121 (98.682)	Acc@5 100.000 (99.992)
Epoch: [124][18/25]	Time 0.649 (0.642)	Data 0.005 (0.037)	Loss 0.2365 (0.2351)	Acc@1 98.291 (98.661)	Acc@5 100.000 (99.992)
Epoch: [124][19/25]	Time 0.620 (0.641)	Data 0.004 (0.036)	Loss 0.2459 (0.2357)	Acc@1 98.242 (98.640)	Acc@5 100.000 (99.993)
Epoch: [124][20/25]	Time 0.597 (0.638)	Data 0.005 (0.034)	Loss 0.2438 (0.2361)	Acc@1 98.438 (98.630)	Acc@5 99.951 (99.991)
Epoch: [124][21/25]	Time 0.650 (0.639)	Data 0.006 (0.033)	Loss 0.2350 (0.2360)	Acc@1 98.779 (98.637)	Acc@5 100.000 (99.991)
Epoch: [124][22/25]	Time 0.627 (0.638)	Data 0.006 (0.032)	Loss 0.2415 (0.2363)	Acc@1 98.047 (98.612)	Acc@5 100.000 (99.992)
Epoch: [124][23/25]	Time 0.637 (0.638)	Data 0.007 (0.031)	Loss 0.2307 (0.2360)	Acc@1 98.633 (98.612)	Acc@5 100.000 (99.992)
Epoch: [124][24/25]	Time 0.395 (0.629)	Data 0.008 (0.030)	Loss 0.2203 (0.2358)	Acc@1 99.292 (98.624)	Acc@5 100.000 (99.992)

Epoch: [125 | 180] LR: 0.040000
Epoch: [125][0/25]	Time 0.636 (0.636)	Data 0.631 (0.631)	Loss 0.2368 (0.2368)	Acc@1 98.486 (98.486)	Acc@5 100.000 (100.000)
Epoch: [125][1/25]	Time 0.671 (0.654)	Data 0.005 (0.318)	Loss 0.2256 (0.2312)	Acc@1 98.926 (98.706)	Acc@5 100.000 (100.000)
Epoch: [125][2/25]	Time 0.655 (0.654)	Data 0.007 (0.214)	Loss 0.2354 (0.2326)	Acc@1 98.877 (98.763)	Acc@5 100.000 (100.000)
Epoch: [125][3/25]	Time 0.648 (0.653)	Data 0.004 (0.162)	Loss 0.2357 (0.2334)	Acc@1 98.779 (98.767)	Acc@5 100.000 (100.000)
Epoch: [125][4/25]	Time 0.651 (0.652)	Data 0.008 (0.131)	Loss 0.2318 (0.2331)	Acc@1 98.828 (98.779)	Acc@5 100.000 (100.000)
Epoch: [125][5/25]	Time 0.663 (0.654)	Data 0.005 (0.110)	Loss 0.2325 (0.2330)	Acc@1 98.535 (98.739)	Acc@5 100.000 (100.000)
Epoch: [125][6/25]	Time 0.681 (0.658)	Data 0.008 (0.095)	Loss 0.2294 (0.2325)	Acc@1 98.682 (98.730)	Acc@5 100.000 (100.000)
Epoch: [125][7/25]	Time 0.568 (0.647)	Data 0.004 (0.084)	Loss 0.2317 (0.2324)	Acc@1 98.682 (98.724)	Acc@5 100.000 (100.000)
Epoch: [125][8/25]	Time 0.622 (0.644)	Data 0.004 (0.075)	Loss 0.2303 (0.2321)	Acc@1 98.682 (98.720)	Acc@5 100.000 (100.000)
Epoch: [125][9/25]	Time 0.606 (0.640)	Data 0.008 (0.068)	Loss 0.2235 (0.2313)	Acc@1 99.072 (98.755)	Acc@5 100.000 (100.000)
Epoch: [125][10/25]	Time 0.639 (0.640)	Data 0.005 (0.063)	Loss 0.2340 (0.2315)	Acc@1 98.633 (98.744)	Acc@5 100.000 (100.000)
Epoch: [125][11/25]	Time 0.665 (0.642)	Data 0.008 (0.058)	Loss 0.2324 (0.2316)	Acc@1 98.682 (98.739)	Acc@5 100.000 (100.000)
Epoch: [125][12/25]	Time 0.616 (0.640)	Data 0.007 (0.054)	Loss 0.2294 (0.2314)	Acc@1 98.975 (98.757)	Acc@5 100.000 (100.000)
Epoch: [125][13/25]	Time 0.651 (0.641)	Data 0.005 (0.051)	Loss 0.2359 (0.2317)	Acc@1 98.535 (98.741)	Acc@5 100.000 (100.000)
Epoch: [125][14/25]	Time 0.667 (0.643)	Data 0.004 (0.047)	Loss 0.2177 (0.2308)	Acc@1 99.219 (98.773)	Acc@5 100.000 (100.000)
Epoch: [125][15/25]	Time 0.667 (0.644)	Data 0.004 (0.045)	Loss 0.2308 (0.2308)	Acc@1 98.877 (98.779)	Acc@5 100.000 (100.000)
Epoch: [125][16/25]	Time 0.671 (0.646)	Data 0.005 (0.042)	Loss 0.2367 (0.2312)	Acc@1 98.486 (98.762)	Acc@5 100.000 (100.000)
Epoch: [125][17/25]	Time 0.678 (0.648)	Data 0.007 (0.040)	Loss 0.2266 (0.2309)	Acc@1 99.023 (98.777)	Acc@5 100.000 (100.000)
Epoch: [125][18/25]	Time 0.602 (0.645)	Data 0.004 (0.039)	Loss 0.2234 (0.2305)	Acc@1 99.072 (98.792)	Acc@5 100.000 (100.000)
Epoch: [125][19/25]	Time 0.618 (0.644)	Data 0.005 (0.037)	Loss 0.2323 (0.2306)	Acc@1 98.828 (98.794)	Acc@5 99.951 (99.998)
Epoch: [125][20/25]	Time 0.630 (0.643)	Data 0.005 (0.035)	Loss 0.2326 (0.2307)	Acc@1 98.486 (98.779)	Acc@5 100.000 (99.998)
Epoch: [125][21/25]	Time 0.677 (0.645)	Data 0.006 (0.034)	Loss 0.2372 (0.2310)	Acc@1 98.584 (98.770)	Acc@5 100.000 (99.998)
Epoch: [125][22/25]	Time 0.612 (0.643)	Data 0.004 (0.033)	Loss 0.2318 (0.2310)	Acc@1 98.535 (98.760)	Acc@5 99.951 (99.996)
Epoch: [125][23/25]	Time 0.635 (0.643)	Data 0.007 (0.032)	Loss 0.2321 (0.2311)	Acc@1 98.486 (98.749)	Acc@5 99.951 (99.994)
Epoch: [125][24/25]	Time 0.396 (0.633)	Data 0.004 (0.031)	Loss 0.2431 (0.2313)	Acc@1 97.877 (98.734)	Acc@5 100.000 (99.994)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [126 | 180] LR: 0.040000
Epoch: [126][0/25]	Time 0.696 (0.696)	Data 0.676 (0.676)	Loss 0.2258 (0.2258)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [126][1/25]	Time 0.717 (0.707)	Data 0.007 (0.341)	Loss 0.2347 (0.2302)	Acc@1 98.340 (98.584)	Acc@5 100.000 (100.000)
Epoch: [126][2/25]	Time 0.668 (0.694)	Data 0.005 (0.229)	Loss 0.2230 (0.2278)	Acc@1 98.975 (98.714)	Acc@5 100.000 (100.000)
Epoch: [126][3/25]	Time 0.603 (0.671)	Data 0.006 (0.173)	Loss 0.2210 (0.2261)	Acc@1 99.170 (98.828)	Acc@5 100.000 (100.000)
Epoch: [126][4/25]	Time 0.664 (0.670)	Data 0.008 (0.140)	Loss 0.2251 (0.2259)	Acc@1 99.072 (98.877)	Acc@5 100.000 (100.000)
Epoch: [126][5/25]	Time 0.572 (0.653)	Data 0.004 (0.118)	Loss 0.2287 (0.2264)	Acc@1 98.633 (98.836)	Acc@5 100.000 (100.000)
Epoch: [126][6/25]	Time 0.556 (0.639)	Data 0.004 (0.101)	Loss 0.2193 (0.2254)	Acc@1 99.121 (98.877)	Acc@5 100.000 (100.000)
Epoch: [126][7/25]	Time 0.605 (0.635)	Data 0.005 (0.089)	Loss 0.2244 (0.2252)	Acc@1 98.926 (98.883)	Acc@5 100.000 (100.000)
Epoch: [126][8/25]	Time 0.669 (0.639)	Data 0.005 (0.080)	Loss 0.2166 (0.2243)	Acc@1 99.463 (98.947)	Acc@5 100.000 (100.000)
Epoch: [126][9/25]	Time 0.664 (0.641)	Data 0.005 (0.072)	Loss 0.2186 (0.2237)	Acc@1 99.121 (98.965)	Acc@5 100.000 (100.000)
Epoch: [126][10/25]	Time 0.585 (0.636)	Data 0.007 (0.067)	Loss 0.2279 (0.2241)	Acc@1 98.779 (98.948)	Acc@5 100.000 (100.000)
Epoch: [126][11/25]	Time 0.618 (0.635)	Data 0.005 (0.061)	Loss 0.2266 (0.2243)	Acc@1 99.023 (98.954)	Acc@5 100.000 (100.000)
Epoch: [126][12/25]	Time 0.646 (0.636)	Data 0.005 (0.057)	Loss 0.2309 (0.2248)	Acc@1 98.779 (98.941)	Acc@5 100.000 (100.000)
Epoch: [126][13/25]	Time 0.650 (0.637)	Data 0.005 (0.053)	Loss 0.2290 (0.2251)	Acc@1 98.730 (98.926)	Acc@5 99.951 (99.997)
Epoch: [126][14/25]	Time 0.567 (0.632)	Data 0.006 (0.050)	Loss 0.2295 (0.2254)	Acc@1 98.584 (98.903)	Acc@5 100.000 (99.997)
Epoch: [126][15/25]	Time 0.627 (0.632)	Data 0.004 (0.047)	Loss 0.2210 (0.2251)	Acc@1 99.219 (98.923)	Acc@5 100.000 (99.997)
Epoch: [126][16/25]	Time 0.657 (0.633)	Data 0.006 (0.045)	Loss 0.2217 (0.2249)	Acc@1 98.975 (98.926)	Acc@5 100.000 (99.997)
Epoch: [126][17/25]	Time 0.637 (0.633)	Data 0.004 (0.043)	Loss 0.2252 (0.2249)	Acc@1 98.828 (98.920)	Acc@5 99.951 (99.995)
Epoch: [126][18/25]	Time 0.619 (0.633)	Data 0.008 (0.041)	Loss 0.2253 (0.2250)	Acc@1 98.926 (98.921)	Acc@5 100.000 (99.995)
Epoch: [126][19/25]	Time 0.682 (0.635)	Data 0.004 (0.039)	Loss 0.2332 (0.2254)	Acc@1 98.438 (98.896)	Acc@5 100.000 (99.995)
Epoch: [126][20/25]	Time 0.577 (0.632)	Data 0.005 (0.037)	Loss 0.2248 (0.2253)	Acc@1 99.023 (98.903)	Acc@5 100.000 (99.995)
Epoch: [126][21/25]	Time 0.613 (0.631)	Data 0.004 (0.036)	Loss 0.2252 (0.2253)	Acc@1 98.975 (98.906)	Acc@5 100.000 (99.996)
Epoch: [126][22/25]	Time 0.645 (0.632)	Data 0.005 (0.035)	Loss 0.2254 (0.2253)	Acc@1 98.926 (98.907)	Acc@5 100.000 (99.996)
Epoch: [126][23/25]	Time 0.637 (0.632)	Data 0.006 (0.033)	Loss 0.2230 (0.2252)	Acc@1 99.219 (98.920)	Acc@5 99.951 (99.994)
Epoch: [126][24/25]	Time 0.309 (0.619)	Data 0.005 (0.032)	Loss 0.2483 (0.2256)	Acc@1 97.995 (98.904)	Acc@5 100.000 (99.994)

Epoch: [127 | 180] LR: 0.040000
Epoch: [127][0/25]	Time 0.666 (0.666)	Data 0.620 (0.620)	Loss 0.2217 (0.2217)	Acc@1 99.121 (99.121)	Acc@5 100.000 (100.000)
Epoch: [127][1/25]	Time 0.587 (0.627)	Data 0.007 (0.314)	Loss 0.2250 (0.2233)	Acc@1 98.730 (98.926)	Acc@5 100.000 (100.000)
Epoch: [127][2/25]	Time 0.581 (0.611)	Data 0.008 (0.212)	Loss 0.2242 (0.2236)	Acc@1 98.730 (98.861)	Acc@5 100.000 (100.000)
Epoch: [127][3/25]	Time 0.611 (0.611)	Data 0.004 (0.160)	Loss 0.2227 (0.2234)	Acc@1 98.975 (98.889)	Acc@5 100.000 (100.000)
Epoch: [127][4/25]	Time 0.655 (0.620)	Data 0.006 (0.129)	Loss 0.2250 (0.2237)	Acc@1 98.828 (98.877)	Acc@5 100.000 (100.000)
Epoch: [127][5/25]	Time 0.612 (0.619)	Data 0.008 (0.109)	Loss 0.2170 (0.2226)	Acc@1 99.170 (98.926)	Acc@5 100.000 (100.000)
Epoch: [127][6/25]	Time 0.579 (0.613)	Data 0.005 (0.094)	Loss 0.2284 (0.2234)	Acc@1 98.779 (98.905)	Acc@5 100.000 (100.000)
Epoch: [127][7/25]	Time 0.657 (0.619)	Data 0.007 (0.083)	Loss 0.2272 (0.2239)	Acc@1 98.535 (98.859)	Acc@5 100.000 (100.000)
Epoch: [127][8/25]	Time 0.684 (0.626)	Data 0.007 (0.074)	Loss 0.2269 (0.2242)	Acc@1 98.926 (98.866)	Acc@5 100.000 (100.000)
Epoch: [127][9/25]	Time 0.549 (0.618)	Data 0.007 (0.068)	Loss 0.2339 (0.2252)	Acc@1 98.242 (98.804)	Acc@5 100.000 (100.000)
Epoch: [127][10/25]	Time 0.569 (0.614)	Data 0.007 (0.062)	Loss 0.2142 (0.2242)	Acc@1 99.268 (98.846)	Acc@5 100.000 (100.000)
Epoch: [127][11/25]	Time 0.608 (0.613)	Data 0.007 (0.058)	Loss 0.2276 (0.2245)	Acc@1 98.779 (98.840)	Acc@5 100.000 (100.000)
Epoch: [127][12/25]	Time 0.692 (0.619)	Data 0.009 (0.054)	Loss 0.2334 (0.2252)	Acc@1 98.584 (98.821)	Acc@5 99.951 (99.996)
Epoch: [127][13/25]	Time 0.612 (0.619)	Data 0.005 (0.050)	Loss 0.2163 (0.2245)	Acc@1 99.219 (98.849)	Acc@5 100.000 (99.997)
Epoch: [127][14/25]	Time 0.657 (0.621)	Data 0.007 (0.047)	Loss 0.2351 (0.2252)	Acc@1 98.730 (98.841)	Acc@5 100.000 (99.997)
Epoch: [127][15/25]	Time 0.655 (0.623)	Data 0.007 (0.045)	Loss 0.2314 (0.2256)	Acc@1 98.584 (98.825)	Acc@5 100.000 (99.997)
Epoch: [127][16/25]	Time 0.693 (0.627)	Data 0.005 (0.043)	Loss 0.2349 (0.2262)	Acc@1 98.682 (98.817)	Acc@5 100.000 (99.997)
Epoch: [127][17/25]	Time 0.641 (0.628)	Data 0.004 (0.040)	Loss 0.2227 (0.2260)	Acc@1 98.828 (98.817)	Acc@5 100.000 (99.997)
Epoch: [127][18/25]	Time 0.645 (0.629)	Data 0.004 (0.039)	Loss 0.2339 (0.2264)	Acc@1 98.389 (98.795)	Acc@5 100.000 (99.997)
Epoch: [127][19/25]	Time 0.637 (0.630)	Data 0.006 (0.037)	Loss 0.2253 (0.2263)	Acc@1 98.779 (98.794)	Acc@5 100.000 (99.998)
Epoch: [127][20/25]	Time 0.646 (0.630)	Data 0.005 (0.035)	Loss 0.2376 (0.2269)	Acc@1 98.535 (98.782)	Acc@5 100.000 (99.998)
Epoch: [127][21/25]	Time 0.664 (0.632)	Data 0.004 (0.034)	Loss 0.2332 (0.2272)	Acc@1 98.438 (98.766)	Acc@5 99.951 (99.996)
Epoch: [127][22/25]	Time 0.653 (0.633)	Data 0.004 (0.033)	Loss 0.2293 (0.2273)	Acc@1 98.389 (98.750)	Acc@5 100.000 (99.996)
Epoch: [127][23/25]	Time 0.610 (0.632)	Data 0.007 (0.032)	Loss 0.2267 (0.2272)	Acc@1 98.779 (98.751)	Acc@5 100.000 (99.996)
Epoch: [127][24/25]	Time 0.326 (0.620)	Data 0.006 (0.031)	Loss 0.2344 (0.2274)	Acc@1 98.585 (98.748)	Acc@5 100.000 (99.996)

Epoch: [128 | 180] LR: 0.040000
Epoch: [128][0/25]	Time 0.673 (0.673)	Data 0.592 (0.592)	Loss 0.2202 (0.2202)	Acc@1 99.072 (99.072)	Acc@5 100.000 (100.000)
Epoch: [128][1/25]	Time 0.670 (0.672)	Data 0.007 (0.300)	Loss 0.2268 (0.2235)	Acc@1 98.682 (98.877)	Acc@5 100.000 (100.000)
Epoch: [128][2/25]	Time 0.639 (0.661)	Data 0.004 (0.201)	Loss 0.2180 (0.2217)	Acc@1 99.170 (98.975)	Acc@5 100.000 (100.000)
Epoch: [128][3/25]	Time 0.616 (0.649)	Data 0.007 (0.153)	Loss 0.2272 (0.2231)	Acc@1 98.682 (98.901)	Acc@5 99.951 (99.988)
Epoch: [128][4/25]	Time 0.665 (0.653)	Data 0.008 (0.124)	Loss 0.2216 (0.2228)	Acc@1 99.121 (98.945)	Acc@5 100.000 (99.990)
Epoch: [128][5/25]	Time 0.633 (0.649)	Data 0.005 (0.104)	Loss 0.2200 (0.2223)	Acc@1 99.023 (98.958)	Acc@5 100.000 (99.992)
Epoch: [128][6/25]	Time 0.590 (0.641)	Data 0.005 (0.090)	Loss 0.2323 (0.2237)	Acc@1 98.389 (98.877)	Acc@5 100.000 (99.993)
Epoch: [128][7/25]	Time 0.648 (0.642)	Data 0.003 (0.079)	Loss 0.2218 (0.2235)	Acc@1 98.877 (98.877)	Acc@5 100.000 (99.994)
Epoch: [128][8/25]	Time 0.677 (0.646)	Data 0.004 (0.071)	Loss 0.2209 (0.2232)	Acc@1 98.682 (98.855)	Acc@5 100.000 (99.995)
Epoch: [128][9/25]	Time 0.592 (0.640)	Data 0.004 (0.064)	Loss 0.2244 (0.2233)	Acc@1 98.828 (98.853)	Acc@5 100.000 (99.995)
Epoch: [128][10/25]	Time 0.583 (0.635)	Data 0.005 (0.059)	Loss 0.2281 (0.2238)	Acc@1 98.633 (98.833)	Acc@5 100.000 (99.996)
Epoch: [128][11/25]	Time 0.658 (0.637)	Data 0.004 (0.054)	Loss 0.2218 (0.2236)	Acc@1 98.975 (98.844)	Acc@5 100.000 (99.996)
Epoch: [128][12/25]	Time 0.663 (0.639)	Data 0.007 (0.050)	Loss 0.2272 (0.2239)	Acc@1 98.730 (98.836)	Acc@5 100.000 (99.996)
Epoch: [128][13/25]	Time 0.549 (0.633)	Data 0.007 (0.047)	Loss 0.2255 (0.2240)	Acc@1 98.682 (98.825)	Acc@5 100.000 (99.997)
Epoch: [128][14/25]	Time 0.577 (0.629)	Data 0.006 (0.045)	Loss 0.2337 (0.2246)	Acc@1 98.096 (98.776)	Acc@5 100.000 (99.997)
Epoch: [128][15/25]	Time 0.768 (0.638)	Data 0.004 (0.042)	Loss 0.2304 (0.2250)	Acc@1 98.584 (98.764)	Acc@5 100.000 (99.997)
Epoch: [128][16/25]	Time 0.751 (0.644)	Data 0.006 (0.040)	Loss 0.2352 (0.2256)	Acc@1 98.340 (98.739)	Acc@5 100.000 (99.997)
Epoch: [128][17/25]	Time 0.661 (0.645)	Data 0.007 (0.038)	Loss 0.2206 (0.2253)	Acc@1 99.170 (98.763)	Acc@5 100.000 (99.997)
Epoch: [128][18/25]	Time 0.645 (0.645)	Data 0.006 (0.036)	Loss 0.2237 (0.2252)	Acc@1 98.633 (98.756)	Acc@5 100.000 (99.997)
Epoch: [128][19/25]	Time 0.654 (0.646)	Data 0.008 (0.035)	Loss 0.2298 (0.2255)	Acc@1 98.584 (98.748)	Acc@5 99.902 (99.993)
Epoch: [128][20/25]	Time 0.671 (0.647)	Data 0.004 (0.034)	Loss 0.2293 (0.2256)	Acc@1 98.584 (98.740)	Acc@5 100.000 (99.993)
Epoch: [128][21/25]	Time 0.653 (0.647)	Data 0.005 (0.032)	Loss 0.2265 (0.2257)	Acc@1 98.389 (98.724)	Acc@5 100.000 (99.993)
Epoch: [128][22/25]	Time 0.594 (0.645)	Data 0.004 (0.031)	Loss 0.2290 (0.2258)	Acc@1 98.779 (98.726)	Acc@5 100.000 (99.994)
Epoch: [128][23/25]	Time 0.605 (0.643)	Data 0.004 (0.030)	Loss 0.2339 (0.2262)	Acc@1 98.096 (98.700)	Acc@5 100.000 (99.994)
Epoch: [128][24/25]	Time 0.403 (0.634)	Data 0.006 (0.029)	Loss 0.2182 (0.2260)	Acc@1 99.175 (98.708)	Acc@5 100.000 (99.994)

Epoch: [129 | 180] LR: 0.040000
Epoch: [129][0/25]	Time 0.656 (0.656)	Data 0.759 (0.759)	Loss 0.2218 (0.2218)	Acc@1 99.023 (99.023)	Acc@5 100.000 (100.000)
Epoch: [129][1/25]	Time 0.617 (0.637)	Data 0.003 (0.381)	Loss 0.2224 (0.2221)	Acc@1 98.828 (98.926)	Acc@5 100.000 (100.000)
Epoch: [129][2/25]	Time 0.640 (0.638)	Data 0.005 (0.256)	Loss 0.2250 (0.2231)	Acc@1 98.584 (98.812)	Acc@5 100.000 (100.000)
Epoch: [129][3/25]	Time 0.626 (0.635)	Data 0.005 (0.193)	Loss 0.2290 (0.2245)	Acc@1 98.682 (98.779)	Acc@5 100.000 (100.000)
Epoch: [129][4/25]	Time 0.675 (0.643)	Data 0.006 (0.156)	Loss 0.2282 (0.2253)	Acc@1 98.730 (98.770)	Acc@5 100.000 (100.000)
Epoch: [129][5/25]	Time 0.619 (0.639)	Data 0.003 (0.130)	Loss 0.2315 (0.2263)	Acc@1 98.096 (98.657)	Acc@5 99.951 (99.992)
Epoch: [129][6/25]	Time 0.676 (0.644)	Data 0.004 (0.112)	Loss 0.2253 (0.2262)	Acc@1 98.828 (98.682)	Acc@5 100.000 (99.993)
Epoch: [129][7/25]	Time 0.651 (0.645)	Data 0.004 (0.099)	Loss 0.2208 (0.2255)	Acc@1 98.926 (98.712)	Acc@5 100.000 (99.994)
Epoch: [129][8/25]	Time 0.656 (0.646)	Data 0.004 (0.088)	Loss 0.2222 (0.2251)	Acc@1 99.023 (98.747)	Acc@5 99.951 (99.989)
Epoch: [129][9/25]	Time 0.609 (0.643)	Data 0.008 (0.080)	Loss 0.2206 (0.2247)	Acc@1 99.023 (98.774)	Acc@5 100.000 (99.990)
Epoch: [129][10/25]	Time 0.655 (0.644)	Data 0.009 (0.074)	Loss 0.2328 (0.2254)	Acc@1 98.633 (98.762)	Acc@5 100.000 (99.991)
Epoch: [129][11/25]	Time 0.577 (0.638)	Data 0.005 (0.068)	Loss 0.2216 (0.2251)	Acc@1 98.877 (98.771)	Acc@5 100.000 (99.992)
Epoch: [129][12/25]	Time 0.588 (0.634)	Data 0.006 (0.063)	Loss 0.2204 (0.2247)	Acc@1 98.828 (98.776)	Acc@5 100.000 (99.992)
Epoch: [129][13/25]	Time 0.585 (0.631)	Data 0.007 (0.059)	Loss 0.2251 (0.2248)	Acc@1 98.633 (98.765)	Acc@5 100.000 (99.993)
Epoch: [129][14/25]	Time 0.645 (0.632)	Data 0.007 (0.056)	Loss 0.2282 (0.2250)	Acc@1 98.682 (98.760)	Acc@5 100.000 (99.993)
Epoch: [129][15/25]	Time 0.647 (0.633)	Data 0.004 (0.053)	Loss 0.2263 (0.2251)	Acc@1 98.438 (98.740)	Acc@5 100.000 (99.994)
Epoch: [129][16/25]	Time 0.595 (0.631)	Data 0.008 (0.050)	Loss 0.2291 (0.2253)	Acc@1 98.633 (98.733)	Acc@5 100.000 (99.994)
Epoch: [129][17/25]	Time 0.595 (0.629)	Data 0.004 (0.047)	Loss 0.2268 (0.2254)	Acc@1 98.730 (98.733)	Acc@5 100.000 (99.995)
Epoch: [129][18/25]	Time 0.653 (0.630)	Data 0.004 (0.045)	Loss 0.2246 (0.2254)	Acc@1 98.828 (98.738)	Acc@5 100.000 (99.995)
Epoch: [129][19/25]	Time 0.661 (0.631)	Data 0.007 (0.043)	Loss 0.2308 (0.2256)	Acc@1 98.340 (98.718)	Acc@5 100.000 (99.995)
Epoch: [129][20/25]	Time 0.606 (0.630)	Data 0.006 (0.041)	Loss 0.2298 (0.2258)	Acc@1 98.389 (98.703)	Acc@5 99.951 (99.993)
Epoch: [129][21/25]	Time 0.578 (0.628)	Data 0.007 (0.040)	Loss 0.2176 (0.2255)	Acc@1 99.219 (98.726)	Acc@5 100.000 (99.993)
Epoch: [129][22/25]	Time 0.589 (0.626)	Data 0.007 (0.038)	Loss 0.2263 (0.2255)	Acc@1 98.584 (98.720)	Acc@5 100.000 (99.994)
Epoch: [129][23/25]	Time 0.663 (0.628)	Data 0.007 (0.037)	Loss 0.2307 (0.2257)	Acc@1 98.633 (98.716)	Acc@5 99.951 (99.992)
Epoch: [129][24/25]	Time 0.390 (0.618)	Data 0.007 (0.036)	Loss 0.2399 (0.2259)	Acc@1 98.349 (98.710)	Acc@5 100.000 (99.992)

Epoch: [130 | 180] LR: 0.040000
Epoch: [130][0/25]	Time 0.624 (0.624)	Data 0.605 (0.605)	Loss 0.2243 (0.2243)	Acc@1 98.486 (98.486)	Acc@5 100.000 (100.000)
Epoch: [130][1/25]	Time 0.614 (0.619)	Data 0.008 (0.306)	Loss 0.2302 (0.2272)	Acc@1 98.535 (98.511)	Acc@5 100.000 (100.000)
Epoch: [130][2/25]	Time 0.666 (0.635)	Data 0.004 (0.206)	Loss 0.2286 (0.2277)	Acc@1 98.389 (98.470)	Acc@5 100.000 (100.000)
Epoch: [130][3/25]	Time 0.642 (0.637)	Data 0.006 (0.156)	Loss 0.2281 (0.2278)	Acc@1 98.535 (98.486)	Acc@5 100.000 (100.000)
Epoch: [130][4/25]	Time 0.637 (0.637)	Data 0.007 (0.126)	Loss 0.2300 (0.2282)	Acc@1 98.828 (98.555)	Acc@5 100.000 (100.000)
Epoch: [130][5/25]	Time 0.603 (0.631)	Data 0.006 (0.106)	Loss 0.2157 (0.2261)	Acc@1 99.072 (98.641)	Acc@5 100.000 (100.000)
Epoch: [130][6/25]	Time 0.649 (0.634)	Data 0.004 (0.092)	Loss 0.2241 (0.2259)	Acc@1 98.633 (98.640)	Acc@5 100.000 (100.000)
Epoch: [130][7/25]	Time 0.670 (0.638)	Data 0.007 (0.081)	Loss 0.2248 (0.2257)	Acc@1 98.779 (98.657)	Acc@5 100.000 (100.000)
Epoch: [130][8/25]	Time 0.639 (0.638)	Data 0.007 (0.073)	Loss 0.2363 (0.2269)	Acc@1 98.291 (98.617)	Acc@5 100.000 (100.000)
Epoch: [130][9/25]	Time 0.649 (0.639)	Data 0.008 (0.066)	Loss 0.2246 (0.2267)	Acc@1 98.438 (98.599)	Acc@5 100.000 (100.000)
Epoch: [130][10/25]	Time 0.600 (0.636)	Data 0.005 (0.061)	Loss 0.2444 (0.2283)	Acc@1 98.242 (98.566)	Acc@5 99.951 (99.996)
Epoch: [130][11/25]	Time 0.600 (0.633)	Data 0.005 (0.056)	Loss 0.2278 (0.2282)	Acc@1 98.535 (98.564)	Acc@5 100.000 (99.996)
Epoch: [130][12/25]	Time 0.589 (0.629)	Data 0.005 (0.052)	Loss 0.2371 (0.2289)	Acc@1 98.340 (98.546)	Acc@5 100.000 (99.996)
Epoch: [130][13/25]	Time 0.637 (0.630)	Data 0.007 (0.049)	Loss 0.2293 (0.2290)	Acc@1 98.779 (98.563)	Acc@5 100.000 (99.997)
Epoch: [130][14/25]	Time 0.684 (0.634)	Data 0.005 (0.046)	Loss 0.2290 (0.2290)	Acc@1 98.779 (98.577)	Acc@5 100.000 (99.997)
Epoch: [130][15/25]	Time 0.574 (0.630)	Data 0.005 (0.043)	Loss 0.2196 (0.2284)	Acc@1 98.828 (98.593)	Acc@5 100.000 (99.997)
Epoch: [130][16/25]	Time 0.606 (0.628)	Data 0.009 (0.041)	Loss 0.2238 (0.2281)	Acc@1 98.779 (98.604)	Acc@5 100.000 (99.997)
Epoch: [130][17/25]	Time 0.662 (0.630)	Data 0.006 (0.039)	Loss 0.2181 (0.2275)	Acc@1 98.828 (98.617)	Acc@5 100.000 (99.997)
Epoch: [130][18/25]	Time 0.625 (0.630)	Data 0.004 (0.038)	Loss 0.2279 (0.2276)	Acc@1 98.340 (98.602)	Acc@5 100.000 (99.997)
Epoch: [130][19/25]	Time 0.647 (0.631)	Data 0.004 (0.036)	Loss 0.2331 (0.2278)	Acc@1 98.389 (98.591)	Acc@5 100.000 (99.998)
Epoch: [130][20/25]	Time 0.609 (0.630)	Data 0.006 (0.034)	Loss 0.2372 (0.2283)	Acc@1 98.291 (98.577)	Acc@5 100.000 (99.998)
Epoch: [130][21/25]	Time 0.605 (0.629)	Data 0.007 (0.033)	Loss 0.2339 (0.2285)	Acc@1 98.096 (98.555)	Acc@5 99.951 (99.996)
Epoch: [130][22/25]	Time 0.642 (0.629)	Data 0.004 (0.032)	Loss 0.2249 (0.2284)	Acc@1 98.486 (98.552)	Acc@5 100.000 (99.996)
Epoch: [130][23/25]	Time 0.635 (0.630)	Data 0.005 (0.031)	Loss 0.2318 (0.2285)	Acc@1 98.242 (98.539)	Acc@5 100.000 (99.996)
Epoch: [130][24/25]	Time 0.346 (0.618)	Data 0.004 (0.030)	Loss 0.2383 (0.2287)	Acc@1 97.877 (98.528)	Acc@5 100.000 (99.996)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 438262 ; 487386 ; 0.8992092509838198

Epoch: [131 | 180] LR: 0.040000
Epoch: [131][0/25]	Time 0.581 (0.581)	Data 0.609 (0.609)	Loss 0.2232 (0.2232)	Acc@1 98.779 (98.779)	Acc@5 100.000 (100.000)
Epoch: [131][1/25]	Time 0.551 (0.566)	Data 0.004 (0.306)	Loss 0.2261 (0.2247)	Acc@1 98.633 (98.706)	Acc@5 100.000 (100.000)
Epoch: [131][2/25]	Time 0.632 (0.588)	Data 0.006 (0.206)	Loss 0.2173 (0.2222)	Acc@1 98.975 (98.796)	Acc@5 100.000 (100.000)
Epoch: [131][3/25]	Time 0.655 (0.605)	Data 0.005 (0.156)	Loss 0.2079 (0.2186)	Acc@1 99.414 (98.950)	Acc@5 100.000 (100.000)
Epoch: [131][4/25]	Time 0.633 (0.610)	Data 0.005 (0.126)	Loss 0.2153 (0.2180)	Acc@1 99.121 (98.984)	Acc@5 99.951 (99.990)
Epoch: [131][5/25]	Time 0.694 (0.624)	Data 0.004 (0.105)	Loss 0.2153 (0.2175)	Acc@1 99.072 (98.999)	Acc@5 100.000 (99.992)
Epoch: [131][6/25]	Time 0.641 (0.627)	Data 0.005 (0.091)	Loss 0.2198 (0.2178)	Acc@1 98.926 (98.989)	Acc@5 100.000 (99.993)
Epoch: [131][7/25]	Time 0.623 (0.626)	Data 0.008 (0.081)	Loss 0.2095 (0.2168)	Acc@1 99.316 (99.030)	Acc@5 100.000 (99.994)
Epoch: [131][8/25]	Time 0.632 (0.627)	Data 0.006 (0.072)	Loss 0.2105 (0.2161)	Acc@1 99.072 (99.034)	Acc@5 100.000 (99.995)
Epoch: [131][9/25]	Time 0.586 (0.623)	Data 0.005 (0.066)	Loss 0.2114 (0.2156)	Acc@1 99.072 (99.038)	Acc@5 100.000 (99.995)
Epoch: [131][10/25]	Time 0.591 (0.620)	Data 0.004 (0.060)	Loss 0.2167 (0.2157)	Acc@1 98.779 (99.015)	Acc@5 100.000 (99.996)
Epoch: [131][11/25]	Time 0.644 (0.622)	Data 0.005 (0.056)	Loss 0.2109 (0.2153)	Acc@1 99.219 (99.032)	Acc@5 100.000 (99.996)
Epoch: [131][12/25]	Time 0.654 (0.624)	Data 0.007 (0.052)	Loss 0.2157 (0.2153)	Acc@1 98.926 (99.023)	Acc@5 100.000 (99.996)
Epoch: [131][13/25]	Time 0.658 (0.627)	Data 0.006 (0.049)	Loss 0.2159 (0.2154)	Acc@1 98.779 (99.006)	Acc@5 100.000 (99.997)
Epoch: [131][14/25]	Time 0.644 (0.628)	Data 0.006 (0.046)	Loss 0.2103 (0.2150)	Acc@1 99.365 (99.030)	Acc@5 100.000 (99.997)
Epoch: [131][15/25]	Time 0.640 (0.629)	Data 0.007 (0.043)	Loss 0.2131 (0.2149)	Acc@1 99.121 (99.036)	Acc@5 100.000 (99.997)
Epoch: [131][16/25]	Time 0.632 (0.629)	Data 0.004 (0.041)	Loss 0.2229 (0.2154)	Acc@1 98.926 (99.029)	Acc@5 100.000 (99.997)
Epoch: [131][17/25]	Time 0.618 (0.628)	Data 0.004 (0.039)	Loss 0.2066 (0.2149)	Acc@1 99.170 (99.037)	Acc@5 100.000 (99.997)
Epoch: [131][18/25]	Time 0.650 (0.629)	Data 0.006 (0.037)	Loss 0.2110 (0.2147)	Acc@1 99.219 (99.047)	Acc@5 100.000 (99.997)
Epoch: [131][19/25]	Time 0.668 (0.631)	Data 0.005 (0.036)	Loss 0.2135 (0.2146)	Acc@1 98.877 (99.038)	Acc@5 100.000 (99.998)
Epoch: [131][20/25]	Time 0.574 (0.629)	Data 0.005 (0.034)	Loss 0.2208 (0.2149)	Acc@1 98.730 (99.023)	Acc@5 100.000 (99.998)
Epoch: [131][21/25]	Time 0.661 (0.630)	Data 0.007 (0.033)	Loss 0.2082 (0.2146)	Acc@1 99.121 (99.028)	Acc@5 100.000 (99.998)
Epoch: [131][22/25]	Time 0.661 (0.631)	Data 0.006 (0.032)	Loss 0.2195 (0.2148)	Acc@1 98.730 (99.015)	Acc@5 100.000 (99.998)
Epoch: [131][23/25]	Time 0.553 (0.628)	Data 0.007 (0.031)	Loss 0.2119 (0.2147)	Acc@1 99.121 (99.019)	Acc@5 100.000 (99.998)
Epoch: [131][24/25]	Time 0.316 (0.616)	Data 0.006 (0.030)	Loss 0.2140 (0.2147)	Acc@1 99.057 (99.020)	Acc@5 100.000 (99.998)

Epoch: [132 | 180] LR: 0.040000
Epoch: [132][0/25]	Time 0.631 (0.631)	Data 0.596 (0.596)	Loss 0.2119 (0.2119)	Acc@1 99.072 (99.072)	Acc@5 100.000 (100.000)
Epoch: [132][1/25]	Time 0.626 (0.628)	Data 0.005 (0.301)	Loss 0.2134 (0.2126)	Acc@1 98.975 (99.023)	Acc@5 100.000 (100.000)
Epoch: [132][2/25]	Time 0.663 (0.640)	Data 0.006 (0.202)	Loss 0.2211 (0.2154)	Acc@1 98.730 (98.926)	Acc@5 100.000 (100.000)
Epoch: [132][3/25]	Time 0.618 (0.635)	Data 0.004 (0.153)	Loss 0.2215 (0.2170)	Acc@1 98.633 (98.853)	Acc@5 100.000 (100.000)
Epoch: [132][4/25]	Time 0.602 (0.628)	Data 0.007 (0.124)	Loss 0.2210 (0.2178)	Acc@1 98.486 (98.779)	Acc@5 100.000 (100.000)
Epoch: [132][5/25]	Time 0.658 (0.633)	Data 0.007 (0.104)	Loss 0.2135 (0.2171)	Acc@1 98.975 (98.812)	Acc@5 100.000 (100.000)
Epoch: [132][6/25]	Time 0.591 (0.627)	Data 0.007 (0.090)	Loss 0.2161 (0.2169)	Acc@1 99.219 (98.870)	Acc@5 100.000 (100.000)
Epoch: [132][7/25]	Time 0.547 (0.617)	Data 0.005 (0.080)	Loss 0.2097 (0.2160)	Acc@1 99.219 (98.914)	Acc@5 100.000 (100.000)
Epoch: [132][8/25]	Time 0.565 (0.611)	Data 0.005 (0.071)	Loss 0.2284 (0.2174)	Acc@1 98.438 (98.861)	Acc@5 100.000 (100.000)
Epoch: [132][9/25]	Time 0.598 (0.610)	Data 0.007 (0.065)	Loss 0.2197 (0.2176)	Acc@1 98.730 (98.848)	Acc@5 100.000 (100.000)
Epoch: [132][10/25]	Time 0.599 (0.609)	Data 0.004 (0.059)	Loss 0.2352 (0.2192)	Acc@1 98.291 (98.797)	Acc@5 100.000 (100.000)
Epoch: [132][11/25]	Time 0.554 (0.604)	Data 0.006 (0.055)	Loss 0.2286 (0.2200)	Acc@1 98.486 (98.771)	Acc@5 99.951 (99.996)
Epoch: [132][12/25]	Time 0.561 (0.601)	Data 0.007 (0.051)	Loss 0.2224 (0.2202)	Acc@1 98.633 (98.761)	Acc@5 100.000 (99.996)
Epoch: [132][13/25]	Time 0.614 (0.602)	Data 0.006 (0.048)	Loss 0.2199 (0.2202)	Acc@1 98.828 (98.765)	Acc@5 100.000 (99.997)
Epoch: [132][14/25]	Time 0.666 (0.606)	Data 0.008 (0.045)	Loss 0.2117 (0.2196)	Acc@1 98.926 (98.776)	Acc@5 100.000 (99.997)
Epoch: [132][15/25]	Time 0.645 (0.609)	Data 0.006 (0.043)	Loss 0.2146 (0.2193)	Acc@1 99.023 (98.792)	Acc@5 100.000 (99.997)
Epoch: [132][16/25]	Time 0.645 (0.611)	Data 0.008 (0.041)	Loss 0.2185 (0.2193)	Acc@1 98.828 (98.794)	Acc@5 100.000 (99.997)
Epoch: [132][17/25]	Time 0.661 (0.614)	Data 0.005 (0.039)	Loss 0.2246 (0.2196)	Acc@1 98.535 (98.779)	Acc@5 100.000 (99.997)
Epoch: [132][18/25]	Time 0.614 (0.614)	Data 0.007 (0.037)	Loss 0.2311 (0.2202)	Acc@1 98.096 (98.743)	Acc@5 100.000 (99.997)
Epoch: [132][19/25]	Time 0.665 (0.616)	Data 0.007 (0.036)	Loss 0.2365 (0.2210)	Acc@1 98.047 (98.708)	Acc@5 100.000 (99.998)
Epoch: [132][20/25]	Time 0.612 (0.616)	Data 0.004 (0.034)	Loss 0.2237 (0.2211)	Acc@1 98.730 (98.710)	Acc@5 100.000 (99.998)
Epoch: [132][21/25]	Time 0.641 (0.617)	Data 0.005 (0.033)	Loss 0.2145 (0.2208)	Acc@1 99.072 (98.726)	Acc@5 99.951 (99.996)
Epoch: [132][22/25]	Time 0.646 (0.618)	Data 0.007 (0.032)	Loss 0.2140 (0.2205)	Acc@1 98.730 (98.726)	Acc@5 100.000 (99.996)
Epoch: [132][23/25]	Time 0.579 (0.617)	Data 0.007 (0.031)	Loss 0.2191 (0.2205)	Acc@1 98.828 (98.730)	Acc@5 100.000 (99.996)
Epoch: [132][24/25]	Time 0.325 (0.605)	Data 0.005 (0.030)	Loss 0.2083 (0.2202)	Acc@1 99.292 (98.740)	Acc@5 100.000 (99.996)

Epoch: [133 | 180] LR: 0.040000
Epoch: [133][0/25]	Time 0.581 (0.581)	Data 0.592 (0.592)	Loss 0.2232 (0.2232)	Acc@1 98.535 (98.535)	Acc@5 100.000 (100.000)
Epoch: [133][1/25]	Time 0.674 (0.628)	Data 0.009 (0.300)	Loss 0.2098 (0.2165)	Acc@1 99.072 (98.804)	Acc@5 100.000 (100.000)
Epoch: [133][2/25]	Time 0.652 (0.636)	Data 0.007 (0.202)	Loss 0.2289 (0.2206)	Acc@1 98.193 (98.600)	Acc@5 100.000 (100.000)
Epoch: [133][3/25]	Time 0.583 (0.623)	Data 0.004 (0.153)	Loss 0.2141 (0.2190)	Acc@1 98.682 (98.621)	Acc@5 100.000 (100.000)
Epoch: [133][4/25]	Time 0.586 (0.615)	Data 0.007 (0.124)	Loss 0.2248 (0.2201)	Acc@1 98.389 (98.574)	Acc@5 100.000 (100.000)
Epoch: [133][5/25]	Time 0.560 (0.606)	Data 0.007 (0.104)	Loss 0.2151 (0.2193)	Acc@1 98.486 (98.560)	Acc@5 100.000 (100.000)
Epoch: [133][6/25]	Time 0.608 (0.606)	Data 0.006 (0.090)	Loss 0.2322 (0.2211)	Acc@1 98.096 (98.493)	Acc@5 100.000 (100.000)
Epoch: [133][7/25]	Time 0.685 (0.616)	Data 0.004 (0.079)	Loss 0.2097 (0.2197)	Acc@1 98.975 (98.553)	Acc@5 100.000 (100.000)
Epoch: [133][8/25]	Time 0.633 (0.618)	Data 0.007 (0.071)	Loss 0.2316 (0.2210)	Acc@1 98.242 (98.519)	Acc@5 99.951 (99.995)
Epoch: [133][9/25]	Time 0.706 (0.627)	Data 0.005 (0.065)	Loss 0.2238 (0.2213)	Acc@1 98.535 (98.521)	Acc@5 100.000 (99.995)
Epoch: [133][10/25]	Time 0.688 (0.632)	Data 0.007 (0.059)	Loss 0.2135 (0.2206)	Acc@1 99.023 (98.566)	Acc@5 100.000 (99.996)
Epoch: [133][11/25]	Time 0.638 (0.633)	Data 0.007 (0.055)	Loss 0.2202 (0.2206)	Acc@1 98.779 (98.584)	Acc@5 100.000 (99.996)
Epoch: [133][12/25]	Time 0.659 (0.635)	Data 0.006 (0.051)	Loss 0.2106 (0.2198)	Acc@1 98.877 (98.607)	Acc@5 100.000 (99.996)
Epoch: [133][13/25]	Time 0.713 (0.641)	Data 0.009 (0.048)	Loss 0.2337 (0.2208)	Acc@1 98.047 (98.567)	Acc@5 100.000 (99.997)
Epoch: [133][14/25]	Time 0.588 (0.637)	Data 0.005 (0.045)	Loss 0.2130 (0.2203)	Acc@1 98.682 (98.574)	Acc@5 100.000 (99.997)
Epoch: [133][15/25]	Time 0.639 (0.637)	Data 0.005 (0.043)	Loss 0.2199 (0.2202)	Acc@1 98.682 (98.581)	Acc@5 100.000 (99.997)
Epoch: [133][16/25]	Time 0.678 (0.640)	Data 0.007 (0.041)	Loss 0.2136 (0.2199)	Acc@1 98.926 (98.601)	Acc@5 100.000 (99.997)
Epoch: [133][17/25]	Time 0.566 (0.635)	Data 0.004 (0.039)	Loss 0.2317 (0.2205)	Acc@1 98.340 (98.587)	Acc@5 100.000 (99.997)
Epoch: [133][18/25]	Time 0.634 (0.635)	Data 0.006 (0.037)	Loss 0.2308 (0.2211)	Acc@1 98.193 (98.566)	Acc@5 100.000 (99.997)
Epoch: [133][19/25]	Time 0.668 (0.637)	Data 0.005 (0.035)	Loss 0.2158 (0.2208)	Acc@1 98.535 (98.564)	Acc@5 100.000 (99.998)
Epoch: [133][20/25]	Time 0.617 (0.636)	Data 0.006 (0.034)	Loss 0.2302 (0.2212)	Acc@1 98.096 (98.542)	Acc@5 100.000 (99.998)
Epoch: [133][21/25]	Time 0.669 (0.638)	Data 0.006 (0.033)	Loss 0.2265 (0.2215)	Acc@1 98.486 (98.540)	Acc@5 100.000 (99.998)
Epoch: [133][22/25]	Time 0.651 (0.638)	Data 0.006 (0.032)	Loss 0.2195 (0.2214)	Acc@1 98.486 (98.537)	Acc@5 100.000 (99.998)
Epoch: [133][23/25]	Time 0.671 (0.639)	Data 0.006 (0.031)	Loss 0.2212 (0.2214)	Acc@1 98.438 (98.533)	Acc@5 100.000 (99.998)
Epoch: [133][24/25]	Time 0.395 (0.630)	Data 0.004 (0.029)	Loss 0.2187 (0.2213)	Acc@1 98.467 (98.532)	Acc@5 100.000 (99.998)

Epoch: [134 | 180] LR: 0.040000
Epoch: [134][0/25]	Time 0.629 (0.629)	Data 0.628 (0.628)	Loss 0.2177 (0.2177)	Acc@1 98.633 (98.633)	Acc@5 99.951 (99.951)
Epoch: [134][1/25]	Time 0.569 (0.599)	Data 0.007 (0.317)	Loss 0.2111 (0.2144)	Acc@1 99.170 (98.901)	Acc@5 100.000 (99.976)
Epoch: [134][2/25]	Time 0.612 (0.603)	Data 0.004 (0.213)	Loss 0.2233 (0.2173)	Acc@1 98.389 (98.730)	Acc@5 100.000 (99.984)
Epoch: [134][3/25]	Time 0.649 (0.615)	Data 0.007 (0.162)	Loss 0.2147 (0.2167)	Acc@1 98.682 (98.718)	Acc@5 100.000 (99.988)
Epoch: [134][4/25]	Time 0.663 (0.624)	Data 0.005 (0.130)	Loss 0.2204 (0.2174)	Acc@1 98.633 (98.701)	Acc@5 100.000 (99.990)
Epoch: [134][5/25]	Time 0.588 (0.618)	Data 0.004 (0.109)	Loss 0.2143 (0.2169)	Acc@1 98.535 (98.674)	Acc@5 100.000 (99.992)
Epoch: [134][6/25]	Time 0.659 (0.624)	Data 0.009 (0.095)	Loss 0.2163 (0.2168)	Acc@1 98.584 (98.661)	Acc@5 100.000 (99.993)
Epoch: [134][7/25]	Time 0.646 (0.627)	Data 0.005 (0.084)	Loss 0.2249 (0.2178)	Acc@1 98.486 (98.639)	Acc@5 100.000 (99.994)
Epoch: [134][8/25]	Time 0.660 (0.631)	Data 0.005 (0.075)	Loss 0.2365 (0.2199)	Acc@1 98.242 (98.595)	Acc@5 100.000 (99.995)
Epoch: [134][9/25]	Time 0.620 (0.630)	Data 0.006 (0.068)	Loss 0.2233 (0.2202)	Acc@1 98.145 (98.550)	Acc@5 100.000 (99.995)
Epoch: [134][10/25]	Time 0.666 (0.633)	Data 0.004 (0.062)	Loss 0.2187 (0.2201)	Acc@1 98.926 (98.584)	Acc@5 100.000 (99.996)
Epoch: [134][11/25]	Time 0.561 (0.627)	Data 0.005 (0.057)	Loss 0.2125 (0.2195)	Acc@1 99.121 (98.629)	Acc@5 100.000 (99.996)
Epoch: [134][12/25]	Time 0.627 (0.627)	Data 0.008 (0.054)	Loss 0.2247 (0.2199)	Acc@1 98.193 (98.595)	Acc@5 100.000 (99.996)
Epoch: [134][13/25]	Time 0.645 (0.628)	Data 0.004 (0.050)	Loss 0.2235 (0.2201)	Acc@1 98.633 (98.598)	Acc@5 100.000 (99.997)
Epoch: [134][14/25]	Time 0.632 (0.628)	Data 0.004 (0.047)	Loss 0.2329 (0.2210)	Acc@1 98.096 (98.564)	Acc@5 100.000 (99.997)
Epoch: [134][15/25]	Time 0.686 (0.632)	Data 0.007 (0.045)	Loss 0.2283 (0.2214)	Acc@1 98.633 (98.569)	Acc@5 100.000 (99.997)
Epoch: [134][16/25]	Time 0.643 (0.633)	Data 0.009 (0.042)	Loss 0.2173 (0.2212)	Acc@1 98.682 (98.575)	Acc@5 100.000 (99.997)
Epoch: [134][17/25]	Time 0.613 (0.632)	Data 0.006 (0.040)	Loss 0.2186 (0.2210)	Acc@1 98.633 (98.579)	Acc@5 100.000 (99.997)
Epoch: [134][18/25]	Time 0.616 (0.631)	Data 0.007 (0.039)	Loss 0.2197 (0.2210)	Acc@1 98.828 (98.592)	Acc@5 100.000 (99.997)
Epoch: [134][19/25]	Time 0.678 (0.633)	Data 0.005 (0.037)	Loss 0.2204 (0.2209)	Acc@1 98.535 (98.589)	Acc@5 100.000 (99.998)
Epoch: [134][20/25]	Time 0.591 (0.631)	Data 0.005 (0.035)	Loss 0.2127 (0.2206)	Acc@1 99.121 (98.614)	Acc@5 100.000 (99.998)
Epoch: [134][21/25]	Time 0.594 (0.629)	Data 0.006 (0.034)	Loss 0.2237 (0.2207)	Acc@1 98.535 (98.611)	Acc@5 100.000 (99.998)
Epoch: [134][22/25]	Time 0.647 (0.630)	Data 0.007 (0.033)	Loss 0.2345 (0.2213)	Acc@1 98.047 (98.586)	Acc@5 100.000 (99.998)
Epoch: [134][23/25]	Time 0.663 (0.632)	Data 0.004 (0.032)	Loss 0.2247 (0.2214)	Acc@1 98.145 (98.568)	Acc@5 100.000 (99.998)
Epoch: [134][24/25]	Time 0.328 (0.619)	Data 0.005 (0.031)	Loss 0.2348 (0.2217)	Acc@1 97.877 (98.556)	Acc@5 100.000 (99.998)

Epoch: [135 | 180] LR: 0.040000
Epoch: [135][0/25]	Time 0.607 (0.607)	Data 0.624 (0.624)	Loss 0.2121 (0.2121)	Acc@1 98.779 (98.779)	Acc@5 100.000 (100.000)
Epoch: [135][1/25]	Time 0.711 (0.659)	Data 0.009 (0.317)	Loss 0.2169 (0.2145)	Acc@1 98.682 (98.730)	Acc@5 100.000 (100.000)
Epoch: [135][2/25]	Time 0.718 (0.678)	Data 0.006 (0.213)	Loss 0.2297 (0.2196)	Acc@1 98.242 (98.568)	Acc@5 100.000 (100.000)
Epoch: [135][3/25]	Time 0.733 (0.692)	Data 0.003 (0.161)	Loss 0.2144 (0.2183)	Acc@1 99.023 (98.682)	Acc@5 100.000 (100.000)
Epoch: [135][4/25]	Time 0.654 (0.684)	Data 0.006 (0.130)	Loss 0.2166 (0.2180)	Acc@1 98.828 (98.711)	Acc@5 100.000 (100.000)
Epoch: [135][5/25]	Time 0.613 (0.672)	Data 0.006 (0.109)	Loss 0.2259 (0.2193)	Acc@1 98.486 (98.674)	Acc@5 100.000 (100.000)
Epoch: [135][6/25]	Time 0.680 (0.674)	Data 0.006 (0.094)	Loss 0.2239 (0.2199)	Acc@1 98.145 (98.598)	Acc@5 100.000 (100.000)
Epoch: [135][7/25]	Time 0.602 (0.665)	Data 0.003 (0.083)	Loss 0.2356 (0.2219)	Acc@1 98.047 (98.529)	Acc@5 99.951 (99.994)
Epoch: [135][8/25]	Time 0.683 (0.667)	Data 0.007 (0.074)	Loss 0.2202 (0.2217)	Acc@1 98.584 (98.535)	Acc@5 100.000 (99.995)
Epoch: [135][9/25]	Time 0.654 (0.665)	Data 0.006 (0.068)	Loss 0.2322 (0.2228)	Acc@1 97.998 (98.481)	Acc@5 100.000 (99.995)
Epoch: [135][10/25]	Time 0.588 (0.658)	Data 0.004 (0.062)	Loss 0.2322 (0.2236)	Acc@1 97.900 (98.429)	Acc@5 100.000 (99.996)
Epoch: [135][11/25]	Time 0.635 (0.656)	Data 0.006 (0.057)	Loss 0.2246 (0.2237)	Acc@1 98.438 (98.429)	Acc@5 100.000 (99.996)
Epoch: [135][12/25]	Time 0.678 (0.658)	Data 0.009 (0.053)	Loss 0.2302 (0.2242)	Acc@1 98.242 (98.415)	Acc@5 100.000 (99.996)
Epoch: [135][13/25]	Time 0.674 (0.659)	Data 0.008 (0.050)	Loss 0.2202 (0.2239)	Acc@1 98.730 (98.438)	Acc@5 100.000 (99.997)
Epoch: [135][14/25]	Time 0.625 (0.657)	Data 0.007 (0.047)	Loss 0.2209 (0.2237)	Acc@1 98.145 (98.418)	Acc@5 100.000 (99.997)
Epoch: [135][15/25]	Time 0.655 (0.657)	Data 0.008 (0.045)	Loss 0.2346 (0.2244)	Acc@1 97.998 (98.392)	Acc@5 100.000 (99.997)
Epoch: [135][16/25]	Time 0.682 (0.658)	Data 0.006 (0.043)	Loss 0.2144 (0.2238)	Acc@1 98.877 (98.420)	Acc@5 100.000 (99.997)
Epoch: [135][17/25]	Time 0.640 (0.657)	Data 0.005 (0.040)	Loss 0.2198 (0.2236)	Acc@1 98.389 (98.419)	Acc@5 99.951 (99.995)
Epoch: [135][18/25]	Time 0.660 (0.657)	Data 0.007 (0.039)	Loss 0.2375 (0.2243)	Acc@1 97.656 (98.378)	Acc@5 100.000 (99.995)
Epoch: [135][19/25]	Time 0.696 (0.659)	Data 0.006 (0.037)	Loss 0.2217 (0.2242)	Acc@1 98.291 (98.374)	Acc@5 100.000 (99.995)
Epoch: [135][20/25]	Time 0.604 (0.657)	Data 0.005 (0.036)	Loss 0.2296 (0.2244)	Acc@1 98.389 (98.375)	Acc@5 100.000 (99.995)
Epoch: [135][21/25]	Time 0.575 (0.653)	Data 0.004 (0.034)	Loss 0.2304 (0.2247)	Acc@1 97.949 (98.355)	Acc@5 99.951 (99.993)
Epoch: [135][22/25]	Time 0.590 (0.650)	Data 0.006 (0.033)	Loss 0.2232 (0.2246)	Acc@1 98.486 (98.361)	Acc@5 100.000 (99.994)
Epoch: [135][23/25]	Time 0.634 (0.650)	Data 0.005 (0.032)	Loss 0.2272 (0.2247)	Acc@1 98.340 (98.360)	Acc@5 100.000 (99.994)
Epoch: [135][24/25]	Time 0.393 (0.639)	Data 0.008 (0.031)	Loss 0.2311 (0.2249)	Acc@1 98.231 (98.358)	Acc@5 100.000 (99.994)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [136 | 180] LR: 0.040000
Epoch: [136][0/25]	Time 0.597 (0.597)	Data 0.631 (0.631)	Loss 0.2131 (0.2131)	Acc@1 99.121 (99.121)	Acc@5 100.000 (100.000)
Epoch: [136][1/25]	Time 0.573 (0.585)	Data 0.005 (0.318)	Loss 0.2197 (0.2164)	Acc@1 98.486 (98.804)	Acc@5 100.000 (100.000)
Epoch: [136][2/25]	Time 0.561 (0.577)	Data 0.005 (0.214)	Loss 0.2131 (0.2153)	Acc@1 99.023 (98.877)	Acc@5 100.000 (100.000)
Epoch: [136][3/25]	Time 0.621 (0.588)	Data 0.005 (0.161)	Loss 0.2161 (0.2155)	Acc@1 98.584 (98.804)	Acc@5 100.000 (100.000)
Epoch: [136][4/25]	Time 0.656 (0.602)	Data 0.006 (0.130)	Loss 0.2043 (0.2133)	Acc@1 99.316 (98.906)	Acc@5 100.000 (100.000)
Epoch: [136][5/25]	Time 0.626 (0.606)	Data 0.005 (0.109)	Loss 0.2046 (0.2118)	Acc@1 99.072 (98.934)	Acc@5 100.000 (100.000)
Epoch: [136][6/25]	Time 0.660 (0.613)	Data 0.007 (0.095)	Loss 0.2030 (0.2106)	Acc@1 99.268 (98.982)	Acc@5 100.000 (100.000)
Epoch: [136][7/25]	Time 0.656 (0.619)	Data 0.007 (0.084)	Loss 0.2105 (0.2105)	Acc@1 98.926 (98.975)	Acc@5 100.000 (100.000)
Epoch: [136][8/25]	Time 0.629 (0.620)	Data 0.006 (0.075)	Loss 0.2196 (0.2116)	Acc@1 98.535 (98.926)	Acc@5 100.000 (100.000)
Epoch: [136][9/25]	Time 0.639 (0.622)	Data 0.005 (0.068)	Loss 0.2057 (0.2110)	Acc@1 99.023 (98.936)	Acc@5 100.000 (100.000)
Epoch: [136][10/25]	Time 0.628 (0.622)	Data 0.004 (0.062)	Loss 0.2136 (0.2112)	Acc@1 98.682 (98.912)	Acc@5 100.000 (100.000)
Epoch: [136][11/25]	Time 0.567 (0.618)	Data 0.007 (0.058)	Loss 0.2108 (0.2112)	Acc@1 99.121 (98.930)	Acc@5 100.000 (100.000)
Epoch: [136][12/25]	Time 0.659 (0.621)	Data 0.006 (0.054)	Loss 0.2179 (0.2117)	Acc@1 98.682 (98.911)	Acc@5 100.000 (100.000)
Epoch: [136][13/25]	Time 0.675 (0.625)	Data 0.007 (0.050)	Loss 0.2199 (0.2123)	Acc@1 98.633 (98.891)	Acc@5 99.951 (99.997)
Epoch: [136][14/25]	Time 0.583 (0.622)	Data 0.006 (0.047)	Loss 0.2073 (0.2119)	Acc@1 98.975 (98.896)	Acc@5 100.000 (99.997)
Epoch: [136][15/25]	Time 0.573 (0.619)	Data 0.006 (0.045)	Loss 0.2097 (0.2118)	Acc@1 99.023 (98.904)	Acc@5 100.000 (99.997)
Epoch: [136][16/25]	Time 0.631 (0.620)	Data 0.010 (0.043)	Loss 0.2117 (0.2118)	Acc@1 98.633 (98.888)	Acc@5 100.000 (99.997)
Epoch: [136][17/25]	Time 0.663 (0.622)	Data 0.006 (0.041)	Loss 0.2116 (0.2118)	Acc@1 98.926 (98.891)	Acc@5 100.000 (99.997)
Epoch: [136][18/25]	Time 0.567 (0.619)	Data 0.008 (0.039)	Loss 0.2158 (0.2120)	Acc@1 98.584 (98.874)	Acc@5 100.000 (99.997)
Epoch: [136][19/25]	Time 0.557 (0.616)	Data 0.003 (0.037)	Loss 0.2096 (0.2119)	Acc@1 98.926 (98.877)	Acc@5 100.000 (99.998)
Epoch: [136][20/25]	Time 0.594 (0.615)	Data 0.004 (0.036)	Loss 0.2173 (0.2121)	Acc@1 98.682 (98.868)	Acc@5 100.000 (99.998)
Epoch: [136][21/25]	Time 0.645 (0.616)	Data 0.006 (0.034)	Loss 0.2126 (0.2122)	Acc@1 98.877 (98.868)	Acc@5 100.000 (99.998)
Epoch: [136][22/25]	Time 0.645 (0.618)	Data 0.007 (0.033)	Loss 0.2180 (0.2124)	Acc@1 98.633 (98.858)	Acc@5 100.000 (99.998)
Epoch: [136][23/25]	Time 0.640 (0.618)	Data 0.006 (0.032)	Loss 0.2070 (0.2122)	Acc@1 98.975 (98.863)	Acc@5 100.000 (99.998)
Epoch: [136][24/25]	Time 0.340 (0.607)	Data 0.006 (0.031)	Loss 0.2095 (0.2121)	Acc@1 98.585 (98.858)	Acc@5 100.000 (99.998)

Epoch: [137 | 180] LR: 0.040000
Epoch: [137][0/25]	Time 0.608 (0.608)	Data 0.659 (0.659)	Loss 0.2132 (0.2132)	Acc@1 98.779 (98.779)	Acc@5 100.000 (100.000)
Epoch: [137][1/25]	Time 0.591 (0.600)	Data 0.005 (0.332)	Loss 0.2075 (0.2104)	Acc@1 98.975 (98.877)	Acc@5 100.000 (100.000)
Epoch: [137][2/25]	Time 0.646 (0.615)	Data 0.005 (0.223)	Loss 0.2129 (0.2112)	Acc@1 98.975 (98.910)	Acc@5 100.000 (100.000)
Epoch: [137][3/25]	Time 0.636 (0.620)	Data 0.005 (0.169)	Loss 0.2126 (0.2116)	Acc@1 98.682 (98.853)	Acc@5 100.000 (100.000)
Epoch: [137][4/25]	Time 0.620 (0.620)	Data 0.006 (0.136)	Loss 0.2039 (0.2100)	Acc@1 99.023 (98.887)	Acc@5 100.000 (100.000)
Epoch: [137][5/25]	Time 0.645 (0.624)	Data 0.006 (0.114)	Loss 0.2057 (0.2093)	Acc@1 98.730 (98.861)	Acc@5 100.000 (100.000)
Epoch: [137][6/25]	Time 0.641 (0.627)	Data 0.005 (0.099)	Loss 0.2064 (0.2089)	Acc@1 98.779 (98.849)	Acc@5 100.000 (100.000)
Epoch: [137][7/25]	Time 0.682 (0.634)	Data 0.004 (0.087)	Loss 0.2143 (0.2096)	Acc@1 98.584 (98.816)	Acc@5 99.951 (99.994)
Epoch: [137][8/25]	Time 0.658 (0.636)	Data 0.005 (0.078)	Loss 0.2041 (0.2089)	Acc@1 99.268 (98.866)	Acc@5 100.000 (99.995)
Epoch: [137][9/25]	Time 0.649 (0.638)	Data 0.006 (0.071)	Loss 0.2173 (0.2098)	Acc@1 98.486 (98.828)	Acc@5 100.000 (99.995)
Epoch: [137][10/25]	Time 0.627 (0.637)	Data 0.008 (0.065)	Loss 0.2020 (0.2091)	Acc@1 99.316 (98.873)	Acc@5 100.000 (99.996)
Epoch: [137][11/25]	Time 0.649 (0.638)	Data 0.006 (0.060)	Loss 0.2084 (0.2090)	Acc@1 98.975 (98.881)	Acc@5 100.000 (99.996)
Epoch: [137][12/25]	Time 0.792 (0.649)	Data 0.005 (0.056)	Loss 0.2105 (0.2091)	Acc@1 98.730 (98.869)	Acc@5 100.000 (99.996)
Epoch: [137][13/25]	Time 0.697 (0.653)	Data 0.008 (0.052)	Loss 0.2184 (0.2098)	Acc@1 98.486 (98.842)	Acc@5 100.000 (99.997)
Epoch: [137][14/25]	Time 0.607 (0.650)	Data 0.004 (0.049)	Loss 0.2205 (0.2105)	Acc@1 98.047 (98.789)	Acc@5 100.000 (99.997)
Epoch: [137][15/25]	Time 0.624 (0.648)	Data 0.005 (0.046)	Loss 0.2187 (0.2110)	Acc@1 98.389 (98.764)	Acc@5 100.000 (99.997)
Epoch: [137][16/25]	Time 0.644 (0.648)	Data 0.008 (0.044)	Loss 0.2113 (0.2110)	Acc@1 98.682 (98.759)	Acc@5 100.000 (99.997)
Epoch: [137][17/25]	Time 0.653 (0.648)	Data 0.004 (0.042)	Loss 0.2120 (0.2111)	Acc@1 98.828 (98.763)	Acc@5 99.951 (99.995)
Epoch: [137][18/25]	Time 0.632 (0.647)	Data 0.006 (0.040)	Loss 0.2229 (0.2117)	Acc@1 98.291 (98.738)	Acc@5 99.951 (99.992)
Epoch: [137][19/25]	Time 0.668 (0.648)	Data 0.005 (0.038)	Loss 0.2268 (0.2125)	Acc@1 98.242 (98.713)	Acc@5 100.000 (99.993)
Epoch: [137][20/25]	Time 0.621 (0.647)	Data 0.007 (0.037)	Loss 0.2077 (0.2122)	Acc@1 98.877 (98.721)	Acc@5 100.000 (99.993)
Epoch: [137][21/25]	Time 0.571 (0.644)	Data 0.004 (0.035)	Loss 0.2085 (0.2121)	Acc@1 98.828 (98.726)	Acc@5 99.951 (99.991)
Epoch: [137][22/25]	Time 0.598 (0.642)	Data 0.004 (0.034)	Loss 0.2258 (0.2127)	Acc@1 98.291 (98.707)	Acc@5 100.000 (99.992)
Epoch: [137][23/25]	Time 0.593 (0.640)	Data 0.006 (0.033)	Loss 0.2166 (0.2128)	Acc@1 98.535 (98.700)	Acc@5 100.000 (99.992)
Epoch: [137][24/25]	Time 0.354 (0.628)	Data 0.008 (0.032)	Loss 0.2386 (0.2133)	Acc@1 98.113 (98.690)	Acc@5 100.000 (99.992)

Epoch: [138 | 180] LR: 0.040000
Epoch: [138][0/25]	Time 0.582 (0.582)	Data 0.690 (0.690)	Loss 0.2139 (0.2139)	Acc@1 98.389 (98.389)	Acc@5 100.000 (100.000)
Epoch: [138][1/25]	Time 0.598 (0.590)	Data 0.005 (0.347)	Loss 0.2066 (0.2103)	Acc@1 99.023 (98.706)	Acc@5 100.000 (100.000)
Epoch: [138][2/25]	Time 0.589 (0.590)	Data 0.006 (0.233)	Loss 0.2078 (0.2094)	Acc@1 99.023 (98.812)	Acc@5 100.000 (100.000)
Epoch: [138][3/25]	Time 0.562 (0.583)	Data 0.007 (0.177)	Loss 0.2108 (0.2098)	Acc@1 98.828 (98.816)	Acc@5 100.000 (100.000)
Epoch: [138][4/25]	Time 0.586 (0.583)	Data 0.005 (0.142)	Loss 0.2143 (0.2107)	Acc@1 98.340 (98.721)	Acc@5 100.000 (100.000)
Epoch: [138][5/25]	Time 0.596 (0.585)	Data 0.006 (0.120)	Loss 0.2190 (0.2121)	Acc@1 98.438 (98.674)	Acc@5 100.000 (100.000)
Epoch: [138][6/25]	Time 0.528 (0.577)	Data 0.005 (0.103)	Loss 0.2089 (0.2116)	Acc@1 98.877 (98.703)	Acc@5 100.000 (100.000)
Epoch: [138][7/25]	Time 0.613 (0.582)	Data 0.007 (0.091)	Loss 0.2122 (0.2117)	Acc@1 98.828 (98.718)	Acc@5 100.000 (100.000)
Epoch: [138][8/25]	Time 0.555 (0.579)	Data 0.005 (0.082)	Loss 0.2132 (0.2119)	Acc@1 98.535 (98.698)	Acc@5 100.000 (100.000)
Epoch: [138][9/25]	Time 0.618 (0.583)	Data 0.006 (0.074)	Loss 0.2152 (0.2122)	Acc@1 98.340 (98.662)	Acc@5 100.000 (100.000)
Epoch: [138][10/25]	Time 0.551 (0.580)	Data 0.006 (0.068)	Loss 0.2071 (0.2117)	Acc@1 98.779 (98.673)	Acc@5 100.000 (100.000)
Epoch: [138][11/25]	Time 0.592 (0.581)	Data 0.006 (0.063)	Loss 0.2153 (0.2120)	Acc@1 98.584 (98.665)	Acc@5 100.000 (100.000)
Epoch: [138][12/25]	Time 0.568 (0.580)	Data 0.007 (0.058)	Loss 0.2099 (0.2119)	Acc@1 98.633 (98.663)	Acc@5 100.000 (100.000)
Epoch: [138][13/25]	Time 0.574 (0.579)	Data 0.004 (0.055)	Loss 0.2116 (0.2118)	Acc@1 98.779 (98.671)	Acc@5 100.000 (100.000)
Epoch: [138][14/25]	Time 0.570 (0.579)	Data 0.005 (0.051)	Loss 0.2134 (0.2120)	Acc@1 98.682 (98.672)	Acc@5 100.000 (100.000)
Epoch: [138][15/25]	Time 0.581 (0.579)	Data 0.007 (0.049)	Loss 0.2033 (0.2114)	Acc@1 98.975 (98.691)	Acc@5 100.000 (100.000)
Epoch: [138][16/25]	Time 0.534 (0.576)	Data 0.007 (0.046)	Loss 0.2147 (0.2116)	Acc@1 98.828 (98.699)	Acc@5 100.000 (100.000)
Epoch: [138][17/25]	Time 0.600 (0.578)	Data 0.006 (0.044)	Loss 0.2139 (0.2117)	Acc@1 98.438 (98.684)	Acc@5 100.000 (100.000)
Epoch: [138][18/25]	Time 0.537 (0.575)	Data 0.004 (0.042)	Loss 0.2088 (0.2116)	Acc@1 98.877 (98.694)	Acc@5 100.000 (100.000)
Epoch: [138][19/25]	Time 0.588 (0.576)	Data 0.008 (0.040)	Loss 0.2207 (0.2120)	Acc@1 98.389 (98.679)	Acc@5 99.951 (99.998)
Epoch: [138][20/25]	Time 0.582 (0.576)	Data 0.004 (0.038)	Loss 0.2173 (0.2123)	Acc@1 98.193 (98.656)	Acc@5 100.000 (99.998)
Epoch: [138][21/25]	Time 0.593 (0.577)	Data 0.004 (0.037)	Loss 0.2296 (0.2131)	Acc@1 97.852 (98.619)	Acc@5 100.000 (99.998)
Epoch: [138][22/25]	Time 0.570 (0.577)	Data 0.006 (0.036)	Loss 0.2204 (0.2134)	Acc@1 98.291 (98.605)	Acc@5 100.000 (99.998)
Epoch: [138][23/25]	Time 0.592 (0.577)	Data 0.008 (0.034)	Loss 0.2203 (0.2137)	Acc@1 98.438 (98.598)	Acc@5 99.951 (99.996)
Epoch: [138][24/25]	Time 0.326 (0.567)	Data 0.006 (0.033)	Loss 0.2280 (0.2139)	Acc@1 98.349 (98.594)	Acc@5 100.000 (99.996)

Epoch: [139 | 180] LR: 0.040000
Epoch: [139][0/25]	Time 0.601 (0.601)	Data 0.710 (0.710)	Loss 0.2174 (0.2174)	Acc@1 98.682 (98.682)	Acc@5 100.000 (100.000)
Epoch: [139][1/25]	Time 0.559 (0.580)	Data 0.006 (0.358)	Loss 0.2100 (0.2137)	Acc@1 98.828 (98.755)	Acc@5 100.000 (100.000)
Epoch: [139][2/25]	Time 0.570 (0.577)	Data 0.004 (0.240)	Loss 0.2216 (0.2163)	Acc@1 98.682 (98.730)	Acc@5 99.951 (99.984)
Epoch: [139][3/25]	Time 0.590 (0.580)	Data 0.004 (0.181)	Loss 0.2159 (0.2162)	Acc@1 98.438 (98.657)	Acc@5 100.000 (99.988)
Epoch: [139][4/25]	Time 0.572 (0.578)	Data 0.008 (0.147)	Loss 0.2098 (0.2149)	Acc@1 98.682 (98.662)	Acc@5 100.000 (99.990)
Epoch: [139][5/25]	Time 0.533 (0.571)	Data 0.004 (0.123)	Loss 0.2107 (0.2142)	Acc@1 98.877 (98.698)	Acc@5 100.000 (99.992)
Epoch: [139][6/25]	Time 0.585 (0.573)	Data 0.004 (0.106)	Loss 0.2223 (0.2154)	Acc@1 98.193 (98.626)	Acc@5 100.000 (99.993)
Epoch: [139][7/25]	Time 0.539 (0.569)	Data 0.005 (0.093)	Loss 0.2151 (0.2153)	Acc@1 98.730 (98.639)	Acc@5 100.000 (99.994)
Epoch: [139][8/25]	Time 0.593 (0.571)	Data 0.008 (0.084)	Loss 0.2213 (0.2160)	Acc@1 98.389 (98.611)	Acc@5 100.000 (99.995)
Epoch: [139][9/25]	Time 0.578 (0.572)	Data 0.005 (0.076)	Loss 0.2264 (0.2170)	Acc@1 97.803 (98.530)	Acc@5 100.000 (99.995)
Epoch: [139][10/25]	Time 0.579 (0.573)	Data 0.004 (0.069)	Loss 0.2129 (0.2167)	Acc@1 98.682 (98.544)	Acc@5 99.951 (99.991)
Epoch: [139][11/25]	Time 0.571 (0.572)	Data 0.005 (0.064)	Loss 0.2235 (0.2172)	Acc@1 98.096 (98.507)	Acc@5 100.000 (99.992)
Epoch: [139][12/25]	Time 0.586 (0.573)	Data 0.008 (0.060)	Loss 0.2233 (0.2177)	Acc@1 97.852 (98.456)	Acc@5 99.951 (99.989)
Epoch: [139][13/25]	Time 0.553 (0.572)	Data 0.005 (0.056)	Loss 0.2177 (0.2177)	Acc@1 98.486 (98.458)	Acc@5 100.000 (99.990)
Epoch: [139][14/25]	Time 0.588 (0.573)	Data 0.007 (0.053)	Loss 0.2200 (0.2179)	Acc@1 98.486 (98.460)	Acc@5 100.000 (99.990)
Epoch: [139][15/25]	Time 0.564 (0.572)	Data 0.006 (0.050)	Loss 0.2207 (0.2180)	Acc@1 98.145 (98.441)	Acc@5 100.000 (99.991)
Epoch: [139][16/25]	Time 0.573 (0.573)	Data 0.007 (0.047)	Loss 0.2150 (0.2179)	Acc@1 98.389 (98.438)	Acc@5 100.000 (99.991)
Epoch: [139][17/25]	Time 0.555 (0.572)	Data 0.004 (0.045)	Loss 0.2187 (0.2179)	Acc@1 98.535 (98.443)	Acc@5 100.000 (99.992)
Epoch: [139][18/25]	Time 0.604 (0.573)	Data 0.006 (0.043)	Loss 0.2251 (0.2183)	Acc@1 98.096 (98.425)	Acc@5 99.951 (99.990)
Epoch: [139][19/25]	Time 0.558 (0.573)	Data 0.004 (0.041)	Loss 0.2129 (0.2180)	Acc@1 98.438 (98.425)	Acc@5 100.000 (99.990)
Epoch: [139][20/25]	Time 0.579 (0.573)	Data 0.006 (0.039)	Loss 0.2240 (0.2183)	Acc@1 98.242 (98.417)	Acc@5 100.000 (99.991)
Epoch: [139][21/25]	Time 0.592 (0.574)	Data 0.004 (0.037)	Loss 0.2226 (0.2185)	Acc@1 98.193 (98.406)	Acc@5 100.000 (99.991)
Epoch: [139][22/25]	Time 0.582 (0.574)	Data 0.006 (0.036)	Loss 0.2210 (0.2186)	Acc@1 98.389 (98.406)	Acc@5 100.000 (99.992)
Epoch: [139][23/25]	Time 0.563 (0.574)	Data 0.006 (0.035)	Loss 0.2226 (0.2188)	Acc@1 98.096 (98.393)	Acc@5 100.000 (99.992)
Epoch: [139][24/25]	Time 0.337 (0.564)	Data 0.004 (0.034)	Loss 0.2348 (0.2190)	Acc@1 97.995 (98.386)	Acc@5 100.000 (99.992)

Epoch: [140 | 180] LR: 0.040000
Epoch: [140][0/25]	Time 0.596 (0.596)	Data 0.583 (0.583)	Loss 0.2115 (0.2115)	Acc@1 98.535 (98.535)	Acc@5 100.000 (100.000)
Epoch: [140][1/25]	Time 0.568 (0.582)	Data 0.009 (0.296)	Loss 0.2115 (0.2115)	Acc@1 98.730 (98.633)	Acc@5 100.000 (100.000)
Epoch: [140][2/25]	Time 0.571 (0.578)	Data 0.003 (0.198)	Loss 0.2095 (0.2108)	Acc@1 98.584 (98.617)	Acc@5 100.000 (100.000)
Epoch: [140][3/25]	Time 0.559 (0.574)	Data 0.006 (0.150)	Loss 0.2269 (0.2148)	Acc@1 98.145 (98.499)	Acc@5 100.000 (100.000)
Epoch: [140][4/25]	Time 0.584 (0.576)	Data 0.006 (0.121)	Loss 0.2144 (0.2148)	Acc@1 98.828 (98.564)	Acc@5 100.000 (100.000)
Epoch: [140][5/25]	Time 0.578 (0.576)	Data 0.008 (0.102)	Loss 0.2170 (0.2151)	Acc@1 98.486 (98.551)	Acc@5 100.000 (100.000)
Epoch: [140][6/25]	Time 0.552 (0.573)	Data 0.007 (0.089)	Loss 0.2105 (0.2145)	Acc@1 98.730 (98.577)	Acc@5 100.000 (100.000)
Epoch: [140][7/25]	Time 0.606 (0.577)	Data 0.003 (0.078)	Loss 0.2103 (0.2139)	Acc@1 98.486 (98.566)	Acc@5 100.000 (100.000)
Epoch: [140][8/25]	Time 0.564 (0.575)	Data 0.007 (0.070)	Loss 0.2043 (0.2129)	Acc@1 98.975 (98.611)	Acc@5 100.000 (100.000)
Epoch: [140][9/25]	Time 0.586 (0.576)	Data 0.005 (0.064)	Loss 0.2127 (0.2129)	Acc@1 98.682 (98.618)	Acc@5 99.951 (99.995)
Epoch: [140][10/25]	Time 0.598 (0.578)	Data 0.005 (0.058)	Loss 0.2091 (0.2125)	Acc@1 98.584 (98.615)	Acc@5 100.000 (99.996)
Epoch: [140][11/25]	Time 0.576 (0.578)	Data 0.006 (0.054)	Loss 0.2158 (0.2128)	Acc@1 98.242 (98.584)	Acc@5 100.000 (99.996)
Epoch: [140][12/25]	Time 0.590 (0.579)	Data 0.005 (0.050)	Loss 0.2083 (0.2124)	Acc@1 98.730 (98.595)	Acc@5 100.000 (99.996)
Epoch: [140][13/25]	Time 0.561 (0.578)	Data 0.008 (0.047)	Loss 0.2120 (0.2124)	Acc@1 98.633 (98.598)	Acc@5 100.000 (99.997)
Epoch: [140][14/25]	Time 0.591 (0.579)	Data 0.005 (0.044)	Loss 0.2220 (0.2131)	Acc@1 98.291 (98.577)	Acc@5 100.000 (99.997)
Epoch: [140][15/25]	Time 0.548 (0.577)	Data 0.005 (0.042)	Loss 0.2235 (0.2137)	Acc@1 98.242 (98.557)	Acc@5 100.000 (99.997)
Epoch: [140][16/25]	Time 0.601 (0.578)	Data 0.006 (0.040)	Loss 0.2176 (0.2139)	Acc@1 98.340 (98.544)	Acc@5 100.000 (99.997)
Epoch: [140][17/25]	Time 0.589 (0.579)	Data 0.007 (0.038)	Loss 0.2125 (0.2139)	Acc@1 98.486 (98.541)	Acc@5 99.951 (99.995)
Epoch: [140][18/25]	Time 0.553 (0.577)	Data 0.004 (0.036)	Loss 0.2247 (0.2144)	Acc@1 97.998 (98.512)	Acc@5 100.000 (99.995)
Epoch: [140][19/25]	Time 0.570 (0.577)	Data 0.004 (0.035)	Loss 0.2086 (0.2141)	Acc@1 98.975 (98.535)	Acc@5 100.000 (99.995)
Epoch: [140][20/25]	Time 0.570 (0.577)	Data 0.006 (0.033)	Loss 0.2182 (0.2143)	Acc@1 98.730 (98.544)	Acc@5 100.000 (99.995)
Epoch: [140][21/25]	Time 0.579 (0.577)	Data 0.009 (0.032)	Loss 0.2100 (0.2141)	Acc@1 98.682 (98.551)	Acc@5 100.000 (99.996)
Epoch: [140][22/25]	Time 0.546 (0.575)	Data 0.004 (0.031)	Loss 0.2163 (0.2142)	Acc@1 98.438 (98.546)	Acc@5 100.000 (99.996)
Epoch: [140][23/25]	Time 0.583 (0.576)	Data 0.004 (0.030)	Loss 0.2077 (0.2140)	Acc@1 98.828 (98.558)	Acc@5 100.000 (99.996)
Epoch: [140][24/25]	Time 0.319 (0.566)	Data 0.004 (0.029)	Loss 0.2322 (0.2143)	Acc@1 97.759 (98.544)	Acc@5 100.000 (99.996)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [141 | 180] LR: 0.040000
Epoch: [141][0/25]	Time 0.547 (0.547)	Data 0.760 (0.760)	Loss 0.2062 (0.2062)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [141][1/25]	Time 0.589 (0.568)	Data 0.005 (0.382)	Loss 0.2074 (0.2068)	Acc@1 98.926 (98.877)	Acc@5 100.000 (100.000)
Epoch: [141][2/25]	Time 0.541 (0.559)	Data 0.003 (0.256)	Loss 0.2079 (0.2072)	Acc@1 98.779 (98.844)	Acc@5 100.000 (100.000)
Epoch: [141][3/25]	Time 0.571 (0.562)	Data 0.007 (0.194)	Loss 0.1993 (0.2052)	Acc@1 99.365 (98.975)	Acc@5 100.000 (100.000)
Epoch: [141][4/25]	Time 0.554 (0.560)	Data 0.006 (0.156)	Loss 0.2033 (0.2048)	Acc@1 99.023 (98.984)	Acc@5 100.000 (100.000)
Epoch: [141][5/25]	Time 0.567 (0.562)	Data 0.007 (0.131)	Loss 0.1994 (0.2039)	Acc@1 99.072 (98.999)	Acc@5 100.000 (100.000)
Epoch: [141][6/25]	Time 0.571 (0.563)	Data 0.005 (0.113)	Loss 0.2035 (0.2039)	Acc@1 99.121 (99.016)	Acc@5 100.000 (100.000)
Epoch: [141][7/25]	Time 0.576 (0.565)	Data 0.005 (0.100)	Loss 0.2028 (0.2037)	Acc@1 98.730 (98.981)	Acc@5 100.000 (100.000)
Epoch: [141][8/25]	Time 0.596 (0.568)	Data 0.005 (0.089)	Loss 0.2052 (0.2039)	Acc@1 98.682 (98.947)	Acc@5 100.000 (100.000)
Epoch: [141][9/25]	Time 0.555 (0.567)	Data 0.005 (0.081)	Loss 0.2031 (0.2038)	Acc@1 98.975 (98.950)	Acc@5 100.000 (100.000)
Epoch: [141][10/25]	Time 0.596 (0.569)	Data 0.005 (0.074)	Loss 0.2056 (0.2040)	Acc@1 98.877 (98.944)	Acc@5 100.000 (100.000)
Epoch: [141][11/25]	Time 0.534 (0.566)	Data 0.006 (0.068)	Loss 0.1929 (0.2031)	Acc@1 99.463 (98.987)	Acc@5 100.000 (100.000)
Epoch: [141][12/25]	Time 0.604 (0.569)	Data 0.005 (0.063)	Loss 0.2099 (0.2036)	Acc@1 98.828 (98.975)	Acc@5 100.000 (100.000)
Epoch: [141][13/25]	Time 0.574 (0.570)	Data 0.006 (0.059)	Loss 0.2099 (0.2040)	Acc@1 98.682 (98.954)	Acc@5 99.951 (99.997)
Epoch: [141][14/25]	Time 0.531 (0.567)	Data 0.005 (0.056)	Loss 0.2033 (0.2040)	Acc@1 98.828 (98.945)	Acc@5 100.000 (99.997)
Epoch: [141][15/25]	Time 0.555 (0.566)	Data 0.007 (0.053)	Loss 0.1973 (0.2036)	Acc@1 99.170 (98.959)	Acc@5 100.000 (99.997)
Epoch: [141][16/25]	Time 0.587 (0.568)	Data 0.005 (0.050)	Loss 0.2111 (0.2040)	Acc@1 98.779 (98.949)	Acc@5 100.000 (99.997)
Epoch: [141][17/25]	Time 0.572 (0.568)	Data 0.005 (0.047)	Loss 0.2068 (0.2042)	Acc@1 99.023 (98.953)	Acc@5 100.000 (99.997)
Epoch: [141][18/25]	Time 0.577 (0.568)	Data 0.007 (0.045)	Loss 0.2135 (0.2047)	Acc@1 98.389 (98.923)	Acc@5 100.000 (99.997)
Epoch: [141][19/25]	Time 0.591 (0.569)	Data 0.005 (0.043)	Loss 0.1988 (0.2044)	Acc@1 99.121 (98.933)	Acc@5 100.000 (99.998)
Epoch: [141][20/25]	Time 0.580 (0.570)	Data 0.004 (0.041)	Loss 0.2090 (0.2046)	Acc@1 98.682 (98.921)	Acc@5 100.000 (99.998)
Epoch: [141][21/25]	Time 0.585 (0.571)	Data 0.007 (0.040)	Loss 0.2085 (0.2048)	Acc@1 98.926 (98.921)	Acc@5 100.000 (99.998)
Epoch: [141][22/25]	Time 0.595 (0.572)	Data 0.005 (0.038)	Loss 0.2108 (0.2050)	Acc@1 98.389 (98.898)	Acc@5 100.000 (99.998)
Epoch: [141][23/25]	Time 0.607 (0.573)	Data 0.007 (0.037)	Loss 0.2058 (0.2051)	Acc@1 98.779 (98.893)	Acc@5 100.000 (99.998)
Epoch: [141][24/25]	Time 0.384 (0.566)	Data 0.004 (0.036)	Loss 0.2108 (0.2052)	Acc@1 98.939 (98.894)	Acc@5 100.000 (99.998)

Epoch: [142 | 180] LR: 0.040000
Epoch: [142][0/25]	Time 0.597 (0.597)	Data 0.639 (0.639)	Loss 0.1964 (0.1964)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [142][1/25]	Time 0.563 (0.580)	Data 0.007 (0.323)	Loss 0.2068 (0.2016)	Acc@1 98.535 (98.877)	Acc@5 100.000 (100.000)
Epoch: [142][2/25]	Time 0.597 (0.586)	Data 0.006 (0.217)	Loss 0.2167 (0.2066)	Acc@1 98.291 (98.682)	Acc@5 100.000 (100.000)
Epoch: [142][3/25]	Time 0.564 (0.580)	Data 0.005 (0.164)	Loss 0.2057 (0.2064)	Acc@1 98.926 (98.743)	Acc@5 100.000 (100.000)
Epoch: [142][4/25]	Time 0.587 (0.582)	Data 0.005 (0.132)	Loss 0.2023 (0.2056)	Acc@1 98.779 (98.750)	Acc@5 100.000 (100.000)
Epoch: [142][5/25]	Time 0.549 (0.576)	Data 0.005 (0.111)	Loss 0.2053 (0.2055)	Acc@1 98.779 (98.755)	Acc@5 100.000 (100.000)
Epoch: [142][6/25]	Time 0.566 (0.575)	Data 0.005 (0.096)	Loss 0.2163 (0.2071)	Acc@1 98.438 (98.710)	Acc@5 100.000 (100.000)
Epoch: [142][7/25]	Time 0.563 (0.573)	Data 0.006 (0.085)	Loss 0.2062 (0.2070)	Acc@1 99.023 (98.749)	Acc@5 100.000 (100.000)
Epoch: [142][8/25]	Time 0.587 (0.575)	Data 0.005 (0.076)	Loss 0.2045 (0.2067)	Acc@1 98.730 (98.747)	Acc@5 100.000 (100.000)
Epoch: [142][9/25]	Time 0.554 (0.573)	Data 0.005 (0.069)	Loss 0.2012 (0.2061)	Acc@1 98.975 (98.770)	Acc@5 100.000 (100.000)
Epoch: [142][10/25]	Time 0.579 (0.573)	Data 0.005 (0.063)	Loss 0.2079 (0.2063)	Acc@1 98.877 (98.779)	Acc@5 100.000 (100.000)
Epoch: [142][11/25]	Time 0.539 (0.571)	Data 0.006 (0.058)	Loss 0.2231 (0.2077)	Acc@1 98.340 (98.743)	Acc@5 100.000 (100.000)
Epoch: [142][12/25]	Time 0.585 (0.572)	Data 0.007 (0.054)	Loss 0.2097 (0.2079)	Acc@1 98.633 (98.734)	Acc@5 100.000 (100.000)
Epoch: [142][13/25]	Time 0.575 (0.572)	Data 0.004 (0.051)	Loss 0.2065 (0.2078)	Acc@1 98.828 (98.741)	Acc@5 100.000 (100.000)
Epoch: [142][14/25]	Time 0.616 (0.575)	Data 0.005 (0.048)	Loss 0.2042 (0.2075)	Acc@1 98.633 (98.734)	Acc@5 100.000 (100.000)
Epoch: [142][15/25]	Time 0.558 (0.574)	Data 0.005 (0.045)	Loss 0.2151 (0.2080)	Acc@1 98.242 (98.703)	Acc@5 100.000 (100.000)
Epoch: [142][16/25]	Time 0.580 (0.574)	Data 0.007 (0.043)	Loss 0.2064 (0.2079)	Acc@1 98.682 (98.702)	Acc@5 100.000 (100.000)
Epoch: [142][17/25]	Time 0.597 (0.575)	Data 0.004 (0.041)	Loss 0.2093 (0.2080)	Acc@1 98.389 (98.684)	Acc@5 100.000 (100.000)
Epoch: [142][18/25]	Time 0.572 (0.575)	Data 0.005 (0.039)	Loss 0.2209 (0.2087)	Acc@1 98.145 (98.656)	Acc@5 100.000 (100.000)
Epoch: [142][19/25]	Time 0.560 (0.574)	Data 0.004 (0.037)	Loss 0.2199 (0.2092)	Acc@1 97.998 (98.623)	Acc@5 100.000 (100.000)
Epoch: [142][20/25]	Time 0.587 (0.575)	Data 0.005 (0.035)	Loss 0.2147 (0.2095)	Acc@1 98.486 (98.617)	Acc@5 100.000 (100.000)
Epoch: [142][21/25]	Time 0.597 (0.576)	Data 0.004 (0.034)	Loss 0.2104 (0.2095)	Acc@1 98.633 (98.617)	Acc@5 100.000 (100.000)
Epoch: [142][22/25]	Time 0.572 (0.576)	Data 0.004 (0.033)	Loss 0.2223 (0.2101)	Acc@1 98.242 (98.601)	Acc@5 100.000 (100.000)
Epoch: [142][23/25]	Time 0.549 (0.575)	Data 0.004 (0.032)	Loss 0.2149 (0.2103)	Acc@1 98.389 (98.592)	Acc@5 100.000 (100.000)
Epoch: [142][24/25]	Time 0.335 (0.565)	Data 0.006 (0.031)	Loss 0.2225 (0.2105)	Acc@1 98.349 (98.588)	Acc@5 100.000 (100.000)

Epoch: [143 | 180] LR: 0.040000
Epoch: [143][0/25]	Time 0.608 (0.608)	Data 0.621 (0.621)	Loss 0.2113 (0.2113)	Acc@1 98.682 (98.682)	Acc@5 100.000 (100.000)
Epoch: [143][1/25]	Time 0.558 (0.583)	Data 0.006 (0.313)	Loss 0.2094 (0.2104)	Acc@1 98.584 (98.633)	Acc@5 100.000 (100.000)
Epoch: [143][2/25]	Time 0.573 (0.579)	Data 0.005 (0.211)	Loss 0.2216 (0.2141)	Acc@1 98.291 (98.519)	Acc@5 100.000 (100.000)
Epoch: [143][3/25]	Time 0.577 (0.579)	Data 0.006 (0.159)	Loss 0.2129 (0.2138)	Acc@1 98.438 (98.499)	Acc@5 100.000 (100.000)
Epoch: [143][4/25]	Time 0.581 (0.579)	Data 0.005 (0.128)	Loss 0.2174 (0.2145)	Acc@1 98.389 (98.477)	Acc@5 100.000 (100.000)
Epoch: [143][5/25]	Time 0.531 (0.571)	Data 0.008 (0.108)	Loss 0.2102 (0.2138)	Acc@1 98.291 (98.446)	Acc@5 100.000 (100.000)
Epoch: [143][6/25]	Time 0.573 (0.571)	Data 0.007 (0.094)	Loss 0.2110 (0.2134)	Acc@1 98.047 (98.389)	Acc@5 100.000 (100.000)
Epoch: [143][7/25]	Time 0.568 (0.571)	Data 0.004 (0.083)	Loss 0.2206 (0.2143)	Acc@1 98.096 (98.352)	Acc@5 100.000 (100.000)
Epoch: [143][8/25]	Time 0.585 (0.573)	Data 0.005 (0.074)	Loss 0.2027 (0.2130)	Acc@1 99.268 (98.454)	Acc@5 99.951 (99.995)
Epoch: [143][9/25]	Time 0.544 (0.570)	Data 0.005 (0.067)	Loss 0.2052 (0.2122)	Acc@1 98.877 (98.496)	Acc@5 100.000 (99.995)
Epoch: [143][10/25]	Time 0.652 (0.577)	Data 0.006 (0.062)	Loss 0.2171 (0.2127)	Acc@1 98.242 (98.473)	Acc@5 100.000 (99.996)
Epoch: [143][11/25]	Time 0.638 (0.582)	Data 0.004 (0.057)	Loss 0.2200 (0.2133)	Acc@1 97.852 (98.421)	Acc@5 100.000 (99.996)
Epoch: [143][12/25]	Time 0.583 (0.582)	Data 0.007 (0.053)	Loss 0.2241 (0.2141)	Acc@1 97.949 (98.385)	Acc@5 100.000 (99.996)
Epoch: [143][13/25]	Time 0.596 (0.583)	Data 0.004 (0.050)	Loss 0.2181 (0.2144)	Acc@1 98.340 (98.382)	Acc@5 100.000 (99.997)
Epoch: [143][14/25]	Time 0.596 (0.584)	Data 0.008 (0.047)	Loss 0.2024 (0.2136)	Acc@1 98.877 (98.415)	Acc@5 99.951 (99.993)
Epoch: [143][15/25]	Time 0.581 (0.584)	Data 0.005 (0.044)	Loss 0.2206 (0.2140)	Acc@1 98.193 (98.401)	Acc@5 100.000 (99.994)
Epoch: [143][16/25]	Time 0.567 (0.583)	Data 0.007 (0.042)	Loss 0.2196 (0.2144)	Acc@1 98.389 (98.400)	Acc@5 100.000 (99.994)
Epoch: [143][17/25]	Time 0.576 (0.583)	Data 0.004 (0.040)	Loss 0.2136 (0.2143)	Acc@1 98.535 (98.408)	Acc@5 99.951 (99.992)
Epoch: [143][18/25]	Time 0.601 (0.584)	Data 0.006 (0.038)	Loss 0.2071 (0.2139)	Acc@1 98.730 (98.425)	Acc@5 100.000 (99.992)
Epoch: [143][19/25]	Time 0.593 (0.584)	Data 0.008 (0.037)	Loss 0.2193 (0.2142)	Acc@1 98.242 (98.416)	Acc@5 100.000 (99.993)
Epoch: [143][20/25]	Time 0.553 (0.583)	Data 0.007 (0.035)	Loss 0.2197 (0.2145)	Acc@1 98.340 (98.412)	Acc@5 100.000 (99.993)
Epoch: [143][21/25]	Time 0.581 (0.583)	Data 0.004 (0.034)	Loss 0.2225 (0.2148)	Acc@1 97.998 (98.393)	Acc@5 100.000 (99.993)
Epoch: [143][22/25]	Time 0.566 (0.582)	Data 0.005 (0.032)	Loss 0.2260 (0.2153)	Acc@1 97.803 (98.367)	Acc@5 99.951 (99.992)
Epoch: [143][23/25]	Time 0.579 (0.582)	Data 0.006 (0.031)	Loss 0.2194 (0.2155)	Acc@1 98.193 (98.360)	Acc@5 100.000 (99.992)
Epoch: [143][24/25]	Time 0.323 (0.571)	Data 0.006 (0.030)	Loss 0.2185 (0.2155)	Acc@1 98.349 (98.360)	Acc@5 100.000 (99.992)

Epoch: [144 | 180] LR: 0.040000
Epoch: [144][0/25]	Time 0.573 (0.573)	Data 0.612 (0.612)	Loss 0.2128 (0.2128)	Acc@1 98.584 (98.584)	Acc@5 100.000 (100.000)
Epoch: [144][1/25]	Time 0.575 (0.574)	Data 0.008 (0.310)	Loss 0.2110 (0.2119)	Acc@1 98.486 (98.535)	Acc@5 100.000 (100.000)
Epoch: [144][2/25]	Time 0.584 (0.577)	Data 0.007 (0.209)	Loss 0.2083 (0.2107)	Acc@1 98.486 (98.519)	Acc@5 100.000 (100.000)
Epoch: [144][3/25]	Time 0.547 (0.570)	Data 0.006 (0.158)	Loss 0.2083 (0.2101)	Acc@1 98.779 (98.584)	Acc@5 100.000 (100.000)
Epoch: [144][4/25]	Time 0.573 (0.570)	Data 0.009 (0.128)	Loss 0.2140 (0.2109)	Acc@1 98.340 (98.535)	Acc@5 99.951 (99.990)
Epoch: [144][5/25]	Time 0.588 (0.573)	Data 0.005 (0.108)	Loss 0.2078 (0.2104)	Acc@1 98.926 (98.600)	Acc@5 100.000 (99.992)
Epoch: [144][6/25]	Time 0.550 (0.570)	Data 0.004 (0.093)	Loss 0.2180 (0.2115)	Acc@1 98.486 (98.584)	Acc@5 100.000 (99.993)
Epoch: [144][7/25]	Time 0.592 (0.573)	Data 0.007 (0.082)	Loss 0.2148 (0.2119)	Acc@1 98.535 (98.578)	Acc@5 100.000 (99.994)
Epoch: [144][8/25]	Time 0.577 (0.573)	Data 0.004 (0.074)	Loss 0.2281 (0.2137)	Acc@1 97.998 (98.513)	Acc@5 100.000 (99.995)
Epoch: [144][9/25]	Time 0.586 (0.574)	Data 0.006 (0.067)	Loss 0.2141 (0.2137)	Acc@1 98.389 (98.501)	Acc@5 100.000 (99.995)
Epoch: [144][10/25]	Time 0.543 (0.572)	Data 0.008 (0.061)	Loss 0.2177 (0.2141)	Acc@1 98.242 (98.477)	Acc@5 100.000 (99.996)
Epoch: [144][11/25]	Time 0.592 (0.573)	Data 0.006 (0.057)	Loss 0.2170 (0.2143)	Acc@1 98.438 (98.474)	Acc@5 100.000 (99.996)
Epoch: [144][12/25]	Time 0.583 (0.574)	Data 0.005 (0.053)	Loss 0.2097 (0.2140)	Acc@1 98.584 (98.483)	Acc@5 100.000 (99.996)
Epoch: [144][13/25]	Time 0.552 (0.573)	Data 0.008 (0.050)	Loss 0.2197 (0.2144)	Acc@1 98.096 (98.455)	Acc@5 100.000 (99.997)
Epoch: [144][14/25]	Time 0.588 (0.574)	Data 0.005 (0.047)	Loss 0.2101 (0.2141)	Acc@1 98.682 (98.470)	Acc@5 100.000 (99.997)
Epoch: [144][15/25]	Time 0.569 (0.573)	Data 0.007 (0.044)	Loss 0.2060 (0.2136)	Acc@1 98.730 (98.486)	Acc@5 100.000 (99.997)
Epoch: [144][16/25]	Time 0.562 (0.573)	Data 0.005 (0.042)	Loss 0.2086 (0.2133)	Acc@1 98.389 (98.481)	Acc@5 100.000 (99.997)
Epoch: [144][17/25]	Time 0.567 (0.572)	Data 0.006 (0.040)	Loss 0.2156 (0.2134)	Acc@1 98.096 (98.459)	Acc@5 100.000 (99.997)
Epoch: [144][18/25]	Time 0.568 (0.572)	Data 0.006 (0.038)	Loss 0.2143 (0.2135)	Acc@1 98.193 (98.445)	Acc@5 100.000 (99.997)
Epoch: [144][19/25]	Time 0.554 (0.571)	Data 0.004 (0.036)	Loss 0.2224 (0.2139)	Acc@1 97.998 (98.423)	Acc@5 100.000 (99.998)
Epoch: [144][20/25]	Time 0.574 (0.571)	Data 0.005 (0.035)	Loss 0.2268 (0.2145)	Acc@1 98.047 (98.405)	Acc@5 99.951 (99.995)
Epoch: [144][21/25]	Time 0.549 (0.570)	Data 0.004 (0.033)	Loss 0.2225 (0.2149)	Acc@1 97.998 (98.386)	Acc@5 100.000 (99.996)
Epoch: [144][22/25]	Time 0.572 (0.570)	Data 0.006 (0.032)	Loss 0.2205 (0.2151)	Acc@1 98.047 (98.372)	Acc@5 100.000 (99.996)
Epoch: [144][23/25]	Time 0.562 (0.570)	Data 0.007 (0.031)	Loss 0.2113 (0.2150)	Acc@1 98.389 (98.372)	Acc@5 100.000 (99.996)
Epoch: [144][24/25]	Time 0.338 (0.561)	Data 0.006 (0.030)	Loss 0.2203 (0.2151)	Acc@1 97.995 (98.366)	Acc@5 100.000 (99.996)

Epoch: [145 | 180] LR: 0.040000
Epoch: [145][0/25]	Time 0.597 (0.597)	Data 0.663 (0.663)	Loss 0.2038 (0.2038)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [145][1/25]	Time 0.583 (0.590)	Data 0.006 (0.335)	Loss 0.2169 (0.2103)	Acc@1 98.291 (98.560)	Acc@5 100.000 (100.000)
Epoch: [145][2/25]	Time 0.598 (0.592)	Data 0.005 (0.225)	Loss 0.2088 (0.2098)	Acc@1 98.584 (98.568)	Acc@5 100.000 (100.000)
Epoch: [145][3/25]	Time 0.562 (0.585)	Data 0.005 (0.170)	Loss 0.2227 (0.2130)	Acc@1 98.242 (98.486)	Acc@5 100.000 (100.000)
Epoch: [145][4/25]	Time 0.595 (0.587)	Data 0.008 (0.138)	Loss 0.2159 (0.2136)	Acc@1 98.242 (98.438)	Acc@5 100.000 (100.000)
Epoch: [145][5/25]	Time 0.586 (0.587)	Data 0.005 (0.115)	Loss 0.2091 (0.2128)	Acc@1 98.389 (98.429)	Acc@5 100.000 (100.000)
Epoch: [145][6/25]	Time 0.554 (0.582)	Data 0.005 (0.100)	Loss 0.2150 (0.2132)	Acc@1 98.242 (98.403)	Acc@5 100.000 (100.000)
Epoch: [145][7/25]	Time 0.588 (0.583)	Data 0.005 (0.088)	Loss 0.2099 (0.2127)	Acc@1 98.633 (98.431)	Acc@5 100.000 (100.000)
Epoch: [145][8/25]	Time 0.580 (0.582)	Data 0.007 (0.079)	Loss 0.2148 (0.2130)	Acc@1 98.389 (98.427)	Acc@5 100.000 (100.000)
Epoch: [145][9/25]	Time 0.539 (0.578)	Data 0.005 (0.071)	Loss 0.2191 (0.2136)	Acc@1 98.438 (98.428)	Acc@5 99.854 (99.985)
Epoch: [145][10/25]	Time 0.588 (0.579)	Data 0.005 (0.065)	Loss 0.2160 (0.2138)	Acc@1 98.486 (98.433)	Acc@5 100.000 (99.987)
Epoch: [145][11/25]	Time 0.535 (0.575)	Data 0.007 (0.061)	Loss 0.2081 (0.2133)	Acc@1 98.535 (98.442)	Acc@5 100.000 (99.988)
Epoch: [145][12/25]	Time 0.599 (0.577)	Data 0.009 (0.057)	Loss 0.2072 (0.2129)	Acc@1 98.682 (98.460)	Acc@5 100.000 (99.989)
Epoch: [145][13/25]	Time 0.576 (0.577)	Data 0.006 (0.053)	Loss 0.2155 (0.2131)	Acc@1 98.291 (98.448)	Acc@5 100.000 (99.990)
Epoch: [145][14/25]	Time 0.577 (0.577)	Data 0.007 (0.050)	Loss 0.2114 (0.2129)	Acc@1 98.730 (98.467)	Acc@5 100.000 (99.990)
Epoch: [145][15/25]	Time 0.580 (0.577)	Data 0.005 (0.047)	Loss 0.2122 (0.2129)	Acc@1 98.389 (98.462)	Acc@5 100.000 (99.991)
Epoch: [145][16/25]	Time 0.540 (0.575)	Data 0.009 (0.045)	Loss 0.2235 (0.2135)	Acc@1 97.900 (98.429)	Acc@5 100.000 (99.991)
Epoch: [145][17/25]	Time 0.606 (0.577)	Data 0.005 (0.043)	Loss 0.2199 (0.2139)	Acc@1 97.900 (98.400)	Acc@5 100.000 (99.992)
Epoch: [145][18/25]	Time 0.587 (0.577)	Data 0.006 (0.041)	Loss 0.2019 (0.2132)	Acc@1 98.926 (98.427)	Acc@5 100.000 (99.992)
Epoch: [145][19/25]	Time 0.593 (0.578)	Data 0.005 (0.039)	Loss 0.2104 (0.2131)	Acc@1 98.535 (98.433)	Acc@5 100.000 (99.993)
Epoch: [145][20/25]	Time 0.567 (0.578)	Data 0.006 (0.037)	Loss 0.2137 (0.2131)	Acc@1 98.486 (98.435)	Acc@5 100.000 (99.993)
Epoch: [145][21/25]	Time 0.574 (0.577)	Data 0.008 (0.036)	Loss 0.2197 (0.2134)	Acc@1 98.193 (98.424)	Acc@5 100.000 (99.993)
Epoch: [145][22/25]	Time 0.572 (0.577)	Data 0.005 (0.035)	Loss 0.2119 (0.2134)	Acc@1 98.438 (98.425)	Acc@5 100.000 (99.994)
Epoch: [145][23/25]	Time 0.587 (0.578)	Data 0.007 (0.034)	Loss 0.2195 (0.2136)	Acc@1 98.193 (98.415)	Acc@5 100.000 (99.994)
Epoch: [145][24/25]	Time 0.321 (0.567)	Data 0.005 (0.032)	Loss 0.2034 (0.2134)	Acc@1 98.821 (98.422)	Acc@5 100.000 (99.994)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 437972 ; 487386 ; 0.8986142400479291

Epoch: [146 | 180] LR: 0.040000
Epoch: [146][0/25]	Time 0.597 (0.597)	Data 0.581 (0.581)	Loss 0.2091 (0.2091)	Acc@1 98.389 (98.389)	Acc@5 99.951 (99.951)
Epoch: [146][1/25]	Time 0.590 (0.593)	Data 0.008 (0.295)	Loss 0.2024 (0.2058)	Acc@1 98.877 (98.633)	Acc@5 100.000 (99.976)
Epoch: [146][2/25]	Time 0.532 (0.573)	Data 0.006 (0.199)	Loss 0.2116 (0.2077)	Acc@1 98.193 (98.486)	Acc@5 100.000 (99.984)
Epoch: [146][3/25]	Time 0.581 (0.575)	Data 0.009 (0.151)	Loss 0.1980 (0.2053)	Acc@1 99.121 (98.645)	Acc@5 100.000 (99.988)
Epoch: [146][4/25]	Time 0.565 (0.573)	Data 0.006 (0.122)	Loss 0.1952 (0.2033)	Acc@1 99.072 (98.730)	Acc@5 100.000 (99.990)
Epoch: [146][5/25]	Time 0.577 (0.574)	Data 0.008 (0.103)	Loss 0.1954 (0.2019)	Acc@1 98.975 (98.771)	Acc@5 100.000 (99.992)
Epoch: [146][6/25]	Time 0.585 (0.575)	Data 0.006 (0.089)	Loss 0.2042 (0.2023)	Acc@1 98.828 (98.779)	Acc@5 100.000 (99.993)
Epoch: [146][7/25]	Time 0.597 (0.578)	Data 0.006 (0.079)	Loss 0.1990 (0.2019)	Acc@1 98.877 (98.792)	Acc@5 100.000 (99.994)
Epoch: [146][8/25]	Time 0.576 (0.578)	Data 0.005 (0.071)	Loss 0.1995 (0.2016)	Acc@1 98.877 (98.801)	Acc@5 100.000 (99.995)
Epoch: [146][9/25]	Time 0.568 (0.577)	Data 0.006 (0.064)	Loss 0.1917 (0.2006)	Acc@1 99.414 (98.862)	Acc@5 100.000 (99.995)
Epoch: [146][10/25]	Time 0.563 (0.576)	Data 0.008 (0.059)	Loss 0.2070 (0.2012)	Acc@1 98.486 (98.828)	Acc@5 100.000 (99.996)
Epoch: [146][11/25]	Time 0.580 (0.576)	Data 0.007 (0.055)	Loss 0.2060 (0.2016)	Acc@1 98.633 (98.812)	Acc@5 100.000 (99.996)
Epoch: [146][12/25]	Time 0.575 (0.576)	Data 0.005 (0.051)	Loss 0.1967 (0.2012)	Acc@1 99.023 (98.828)	Acc@5 100.000 (99.996)
Epoch: [146][13/25]	Time 0.569 (0.575)	Data 0.007 (0.048)	Loss 0.2026 (0.2013)	Acc@1 98.877 (98.832)	Acc@5 100.000 (99.997)
Epoch: [146][14/25]	Time 0.526 (0.572)	Data 0.006 (0.045)	Loss 0.2068 (0.2017)	Acc@1 98.633 (98.818)	Acc@5 100.000 (99.997)
Epoch: [146][15/25]	Time 0.565 (0.572)	Data 0.007 (0.043)	Loss 0.1971 (0.2014)	Acc@1 99.121 (98.837)	Acc@5 100.000 (99.997)
Epoch: [146][16/25]	Time 0.578 (0.572)	Data 0.005 (0.040)	Loss 0.2008 (0.2013)	Acc@1 98.877 (98.840)	Acc@5 99.951 (99.994)
Epoch: [146][17/25]	Time 0.582 (0.573)	Data 0.007 (0.038)	Loss 0.2030 (0.2014)	Acc@1 98.535 (98.823)	Acc@5 100.000 (99.995)
Epoch: [146][18/25]	Time 0.543 (0.571)	Data 0.006 (0.037)	Loss 0.1964 (0.2012)	Acc@1 99.121 (98.838)	Acc@5 100.000 (99.995)
Epoch: [146][19/25]	Time 0.599 (0.572)	Data 0.006 (0.035)	Loss 0.2047 (0.2013)	Acc@1 98.584 (98.826)	Acc@5 100.000 (99.995)
Epoch: [146][20/25]	Time 0.565 (0.572)	Data 0.004 (0.034)	Loss 0.1993 (0.2012)	Acc@1 99.023 (98.835)	Acc@5 100.000 (99.995)
Epoch: [146][21/25]	Time 0.585 (0.573)	Data 0.004 (0.032)	Loss 0.2030 (0.2013)	Acc@1 99.072 (98.846)	Acc@5 100.000 (99.996)
Epoch: [146][22/25]	Time 0.534 (0.571)	Data 0.007 (0.031)	Loss 0.1979 (0.2012)	Acc@1 99.121 (98.858)	Acc@5 100.000 (99.996)
Epoch: [146][23/25]	Time 0.585 (0.572)	Data 0.004 (0.030)	Loss 0.2135 (0.2017)	Acc@1 98.242 (98.832)	Acc@5 100.000 (99.996)
Epoch: [146][24/25]	Time 0.316 (0.561)	Data 0.004 (0.029)	Loss 0.2035 (0.2017)	Acc@1 98.821 (98.832)	Acc@5 100.000 (99.996)

Epoch: [147 | 180] LR: 0.040000
Epoch: [147][0/25]	Time 0.595 (0.595)	Data 0.625 (0.625)	Loss 0.2030 (0.2030)	Acc@1 98.975 (98.975)	Acc@5 100.000 (100.000)
Epoch: [147][1/25]	Time 0.582 (0.589)	Data 0.005 (0.315)	Loss 0.2086 (0.2058)	Acc@1 98.193 (98.584)	Acc@5 100.000 (100.000)
Epoch: [147][2/25]	Time 0.593 (0.590)	Data 0.007 (0.212)	Loss 0.2029 (0.2048)	Acc@1 98.877 (98.682)	Acc@5 100.000 (100.000)
Epoch: [147][3/25]	Time 0.555 (0.581)	Data 0.007 (0.161)	Loss 0.2105 (0.2063)	Acc@1 98.438 (98.621)	Acc@5 100.000 (100.000)
Epoch: [147][4/25]	Time 0.573 (0.580)	Data 0.005 (0.130)	Loss 0.2104 (0.2071)	Acc@1 98.340 (98.564)	Acc@5 99.951 (99.990)
Epoch: [147][5/25]	Time 0.571 (0.578)	Data 0.004 (0.109)	Loss 0.2129 (0.2081)	Acc@1 98.535 (98.560)	Acc@5 100.000 (99.992)
Epoch: [147][6/25]	Time 0.596 (0.581)	Data 0.008 (0.094)	Loss 0.1921 (0.2058)	Acc@1 99.121 (98.640)	Acc@5 100.000 (99.993)
Epoch: [147][7/25]	Time 0.587 (0.581)	Data 0.005 (0.083)	Loss 0.2022 (0.2053)	Acc@1 98.828 (98.663)	Acc@5 99.951 (99.988)
Epoch: [147][8/25]	Time 0.532 (0.576)	Data 0.007 (0.075)	Loss 0.2121 (0.2061)	Acc@1 98.486 (98.644)	Acc@5 100.000 (99.989)
Epoch: [147][9/25]	Time 0.557 (0.574)	Data 0.007 (0.068)	Loss 0.2143 (0.2069)	Acc@1 98.096 (98.589)	Acc@5 100.000 (99.990)
Epoch: [147][10/25]	Time 0.553 (0.572)	Data 0.008 (0.063)	Loss 0.2091 (0.2071)	Acc@1 98.291 (98.562)	Acc@5 100.000 (99.991)
Epoch: [147][11/25]	Time 0.575 (0.572)	Data 0.009 (0.058)	Loss 0.2096 (0.2073)	Acc@1 98.340 (98.543)	Acc@5 100.000 (99.992)
Epoch: [147][12/25]	Time 0.597 (0.574)	Data 0.008 (0.054)	Loss 0.2096 (0.2075)	Acc@1 98.633 (98.550)	Acc@5 100.000 (99.992)
Epoch: [147][13/25]	Time 0.588 (0.575)	Data 0.004 (0.051)	Loss 0.2063 (0.2074)	Acc@1 98.340 (98.535)	Acc@5 100.000 (99.993)
Epoch: [147][14/25]	Time 0.564 (0.574)	Data 0.004 (0.048)	Loss 0.2092 (0.2075)	Acc@1 98.486 (98.532)	Acc@5 100.000 (99.993)
Epoch: [147][15/25]	Time 0.586 (0.575)	Data 0.004 (0.045)	Loss 0.2107 (0.2077)	Acc@1 98.291 (98.517)	Acc@5 100.000 (99.994)
Epoch: [147][16/25]	Time 0.549 (0.574)	Data 0.005 (0.042)	Loss 0.2232 (0.2086)	Acc@1 97.852 (98.478)	Acc@5 100.000 (99.994)
Epoch: [147][17/25]	Time 0.578 (0.574)	Data 0.004 (0.040)	Loss 0.2070 (0.2085)	Acc@1 98.779 (98.494)	Acc@5 100.000 (99.995)
Epoch: [147][18/25]	Time 0.566 (0.573)	Data 0.006 (0.039)	Loss 0.2100 (0.2086)	Acc@1 98.535 (98.497)	Acc@5 100.000 (99.995)
Epoch: [147][19/25]	Time 0.596 (0.575)	Data 0.005 (0.037)	Loss 0.2095 (0.2087)	Acc@1 98.682 (98.506)	Acc@5 100.000 (99.995)
Epoch: [147][20/25]	Time 0.577 (0.575)	Data 0.004 (0.035)	Loss 0.2179 (0.2091)	Acc@1 98.242 (98.493)	Acc@5 100.000 (99.995)
Epoch: [147][21/25]	Time 0.580 (0.575)	Data 0.008 (0.034)	Loss 0.2159 (0.2094)	Acc@1 98.096 (98.475)	Acc@5 100.000 (99.996)
Epoch: [147][22/25]	Time 0.595 (0.576)	Data 0.004 (0.033)	Loss 0.2123 (0.2095)	Acc@1 98.389 (98.471)	Acc@5 99.951 (99.994)
Epoch: [147][23/25]	Time 0.591 (0.576)	Data 0.004 (0.032)	Loss 0.2077 (0.2095)	Acc@1 98.389 (98.468)	Acc@5 100.000 (99.994)
Epoch: [147][24/25]	Time 0.333 (0.567)	Data 0.006 (0.031)	Loss 0.1988 (0.2093)	Acc@1 99.175 (98.480)	Acc@5 100.000 (99.994)

Epoch: [148 | 180] LR: 0.040000
Epoch: [148][0/25]	Time 0.568 (0.568)	Data 0.580 (0.580)	Loss 0.2058 (0.2058)	Acc@1 98.730 (98.730)	Acc@5 100.000 (100.000)
Epoch: [148][1/25]	Time 0.554 (0.561)	Data 0.009 (0.294)	Loss 0.2154 (0.2106)	Acc@1 98.193 (98.462)	Acc@5 100.000 (100.000)
Epoch: [148][2/25]	Time 0.573 (0.565)	Data 0.005 (0.198)	Loss 0.2116 (0.2109)	Acc@1 98.340 (98.421)	Acc@5 100.000 (100.000)
Epoch: [148][3/25]	Time 0.578 (0.568)	Data 0.004 (0.150)	Loss 0.2092 (0.2105)	Acc@1 98.438 (98.425)	Acc@5 100.000 (100.000)
Epoch: [148][4/25]	Time 0.589 (0.572)	Data 0.006 (0.121)	Loss 0.2102 (0.2104)	Acc@1 98.145 (98.369)	Acc@5 100.000 (100.000)
Epoch: [148][5/25]	Time 0.569 (0.572)	Data 0.008 (0.102)	Loss 0.2052 (0.2096)	Acc@1 98.535 (98.397)	Acc@5 100.000 (100.000)
Epoch: [148][6/25]	Time 0.600 (0.576)	Data 0.004 (0.088)	Loss 0.2021 (0.2085)	Acc@1 98.828 (98.458)	Acc@5 100.000 (100.000)
Epoch: [148][7/25]	Time 0.552 (0.573)	Data 0.004 (0.078)	Loss 0.2020 (0.2077)	Acc@1 98.828 (98.505)	Acc@5 99.951 (99.994)
Epoch: [148][8/25]	Time 0.569 (0.572)	Data 0.007 (0.070)	Loss 0.1987 (0.2067)	Acc@1 98.779 (98.535)	Acc@5 100.000 (99.995)
Epoch: [148][9/25]	Time 0.578 (0.573)	Data 0.007 (0.064)	Loss 0.2270 (0.2087)	Acc@1 97.656 (98.447)	Acc@5 100.000 (99.995)
Epoch: [148][10/25]	Time 0.565 (0.572)	Data 0.006 (0.058)	Loss 0.2089 (0.2087)	Acc@1 98.389 (98.442)	Acc@5 100.000 (99.996)
Epoch: [148][11/25]	Time 0.579 (0.573)	Data 0.004 (0.054)	Loss 0.2124 (0.2090)	Acc@1 98.096 (98.413)	Acc@5 100.000 (99.996)
Epoch: [148][12/25]	Time 0.538 (0.570)	Data 0.007 (0.050)	Loss 0.2027 (0.2085)	Acc@1 98.828 (98.445)	Acc@5 100.000 (99.996)
Epoch: [148][13/25]	Time 0.562 (0.570)	Data 0.005 (0.047)	Loss 0.2042 (0.2082)	Acc@1 98.486 (98.448)	Acc@5 100.000 (99.997)
Epoch: [148][14/25]	Time 0.565 (0.569)	Data 0.006 (0.044)	Loss 0.1982 (0.2076)	Acc@1 99.023 (98.486)	Acc@5 100.000 (99.997)
Epoch: [148][15/25]	Time 0.582 (0.570)	Data 0.005 (0.042)	Loss 0.2060 (0.2075)	Acc@1 98.633 (98.495)	Acc@5 100.000 (99.997)
Epoch: [148][16/25]	Time 0.598 (0.572)	Data 0.007 (0.040)	Loss 0.2063 (0.2074)	Acc@1 98.730 (98.509)	Acc@5 100.000 (99.997)
Epoch: [148][17/25]	Time 0.603 (0.573)	Data 0.004 (0.038)	Loss 0.2140 (0.2078)	Acc@1 98.242 (98.494)	Acc@5 100.000 (99.997)
Epoch: [148][18/25]	Time 0.557 (0.573)	Data 0.006 (0.036)	Loss 0.2155 (0.2082)	Acc@1 98.145 (98.476)	Acc@5 99.951 (99.995)
Epoch: [148][19/25]	Time 0.581 (0.573)	Data 0.007 (0.035)	Loss 0.2189 (0.2087)	Acc@1 98.291 (98.467)	Acc@5 100.000 (99.995)
Epoch: [148][20/25]	Time 0.603 (0.574)	Data 0.006 (0.033)	Loss 0.2220 (0.2093)	Acc@1 97.949 (98.442)	Acc@5 100.000 (99.995)
Epoch: [148][21/25]	Time 0.559 (0.574)	Data 0.006 (0.032)	Loss 0.2060 (0.2092)	Acc@1 98.730 (98.455)	Acc@5 100.000 (99.996)
Epoch: [148][22/25]	Time 0.589 (0.574)	Data 0.004 (0.031)	Loss 0.2098 (0.2092)	Acc@1 98.438 (98.454)	Acc@5 100.000 (99.996)
Epoch: [148][23/25]	Time 0.572 (0.574)	Data 0.005 (0.030)	Loss 0.2139 (0.2094)	Acc@1 98.389 (98.452)	Acc@5 99.951 (99.994)
Epoch: [148][24/25]	Time 0.342 (0.565)	Data 0.007 (0.029)	Loss 0.2041 (0.2093)	Acc@1 98.821 (98.458)	Acc@5 100.000 (99.994)

Epoch: [149 | 180] LR: 0.040000
Epoch: [149][0/25]	Time 0.588 (0.588)	Data 0.645 (0.645)	Loss 0.2035 (0.2035)	Acc@1 98.779 (98.779)	Acc@5 100.000 (100.000)
Epoch: [149][1/25]	Time 0.604 (0.596)	Data 0.006 (0.325)	Loss 0.2124 (0.2080)	Acc@1 98.145 (98.462)	Acc@5 100.000 (100.000)
Epoch: [149][2/25]	Time 0.563 (0.585)	Data 0.004 (0.218)	Loss 0.2077 (0.2079)	Acc@1 98.486 (98.470)	Acc@5 100.000 (100.000)
Epoch: [149][3/25]	Time 0.584 (0.585)	Data 0.007 (0.166)	Loss 0.2033 (0.2068)	Acc@1 98.535 (98.486)	Acc@5 100.000 (100.000)
Epoch: [149][4/25]	Time 0.536 (0.575)	Data 0.006 (0.134)	Loss 0.2126 (0.2079)	Acc@1 98.535 (98.496)	Acc@5 100.000 (100.000)
Epoch: [149][5/25]	Time 0.586 (0.577)	Data 0.007 (0.113)	Loss 0.2199 (0.2099)	Acc@1 97.998 (98.413)	Acc@5 100.000 (100.000)
Epoch: [149][6/25]	Time 0.598 (0.580)	Data 0.006 (0.097)	Loss 0.2247 (0.2120)	Acc@1 97.998 (98.354)	Acc@5 100.000 (100.000)
Epoch: [149][7/25]	Time 0.573 (0.579)	Data 0.005 (0.086)	Loss 0.2045 (0.2111)	Acc@1 98.584 (98.383)	Acc@5 100.000 (100.000)
Epoch: [149][8/25]	Time 0.545 (0.575)	Data 0.005 (0.077)	Loss 0.2027 (0.2102)	Acc@1 98.486 (98.394)	Acc@5 100.000 (100.000)
Epoch: [149][9/25]	Time 0.576 (0.575)	Data 0.007 (0.070)	Loss 0.2066 (0.2098)	Acc@1 98.389 (98.394)	Acc@5 100.000 (100.000)
Epoch: [149][10/25]	Time 0.563 (0.574)	Data 0.004 (0.064)	Loss 0.2119 (0.2100)	Acc@1 98.389 (98.393)	Acc@5 100.000 (100.000)
Epoch: [149][11/25]	Time 0.556 (0.573)	Data 0.005 (0.059)	Loss 0.2211 (0.2109)	Acc@1 98.096 (98.368)	Acc@5 100.000 (100.000)
Epoch: [149][12/25]	Time 0.564 (0.572)	Data 0.007 (0.055)	Loss 0.2190 (0.2115)	Acc@1 98.096 (98.347)	Acc@5 100.000 (100.000)
Epoch: [149][13/25]	Time 0.574 (0.572)	Data 0.006 (0.051)	Loss 0.2122 (0.2116)	Acc@1 98.291 (98.343)	Acc@5 100.000 (100.000)
Epoch: [149][14/25]	Time 0.583 (0.573)	Data 0.005 (0.048)	Loss 0.2135 (0.2117)	Acc@1 98.193 (98.333)	Acc@5 100.000 (100.000)
Epoch: [149][15/25]	Time 0.545 (0.571)	Data 0.007 (0.046)	Loss 0.2090 (0.2115)	Acc@1 98.486 (98.343)	Acc@5 100.000 (100.000)
Epoch: [149][16/25]	Time 0.606 (0.573)	Data 0.005 (0.043)	Loss 0.2098 (0.2114)	Acc@1 98.145 (98.331)	Acc@5 100.000 (100.000)
Epoch: [149][17/25]	Time 0.608 (0.575)	Data 0.005 (0.041)	Loss 0.2153 (0.2117)	Acc@1 98.242 (98.326)	Acc@5 100.000 (100.000)
Epoch: [149][18/25]	Time 0.557 (0.574)	Data 0.004 (0.039)	Loss 0.2181 (0.2120)	Acc@1 98.047 (98.312)	Acc@5 99.951 (99.997)
Epoch: [149][19/25]	Time 0.601 (0.576)	Data 0.005 (0.038)	Loss 0.2124 (0.2120)	Acc@1 98.438 (98.318)	Acc@5 100.000 (99.998)
Epoch: [149][20/25]	Time 0.534 (0.574)	Data 0.006 (0.036)	Loss 0.2188 (0.2123)	Acc@1 98.096 (98.307)	Acc@5 100.000 (99.998)
Epoch: [149][21/25]	Time 0.589 (0.574)	Data 0.005 (0.035)	Loss 0.2278 (0.2130)	Acc@1 97.607 (98.275)	Acc@5 100.000 (99.998)
Epoch: [149][22/25]	Time 0.544 (0.573)	Data 0.007 (0.033)	Loss 0.2233 (0.2135)	Acc@1 97.559 (98.244)	Acc@5 100.000 (99.998)
Epoch: [149][23/25]	Time 0.580 (0.573)	Data 0.007 (0.032)	Loss 0.2185 (0.2137)	Acc@1 98.145 (98.240)	Acc@5 100.000 (99.998)
Epoch: [149][24/25]	Time 0.315 (0.563)	Data 0.004 (0.031)	Loss 0.2169 (0.2138)	Acc@1 98.113 (98.238)	Acc@5 100.000 (99.998)

Epoch: [150 | 180] LR: 0.000400
Epoch: [150][0/25]	Time 0.590 (0.590)	Data 0.640 (0.640)	Loss 0.2148 (0.2148)	Acc@1 98.145 (98.145)	Acc@5 100.000 (100.000)
Epoch: [150][1/25]	Time 0.586 (0.588)	Data 0.003 (0.322)	Loss 0.2115 (0.2132)	Acc@1 98.633 (98.389)	Acc@5 100.000 (100.000)
Epoch: [150][2/25]	Time 0.590 (0.589)	Data 0.008 (0.217)	Loss 0.2100 (0.2121)	Acc@1 98.389 (98.389)	Acc@5 100.000 (100.000)
Epoch: [150][3/25]	Time 0.574 (0.585)	Data 0.005 (0.164)	Loss 0.1943 (0.2077)	Acc@1 99.023 (98.547)	Acc@5 100.000 (100.000)
Epoch: [150][4/25]	Time 0.591 (0.586)	Data 0.007 (0.133)	Loss 0.2073 (0.2076)	Acc@1 98.535 (98.545)	Acc@5 99.951 (99.990)
Epoch: [150][5/25]	Time 0.568 (0.583)	Data 0.007 (0.112)	Loss 0.2178 (0.2093)	Acc@1 97.803 (98.421)	Acc@5 100.000 (99.992)
Epoch: [150][6/25]	Time 0.525 (0.575)	Data 0.008 (0.097)	Loss 0.2000 (0.2080)	Acc@1 98.730 (98.465)	Acc@5 100.000 (99.993)
Epoch: [150][7/25]	Time 0.586 (0.576)	Data 0.007 (0.086)	Loss 0.2019 (0.2072)	Acc@1 98.877 (98.517)	Acc@5 100.000 (99.994)
Epoch: [150][8/25]	Time 0.588 (0.578)	Data 0.006 (0.077)	Loss 0.2045 (0.2069)	Acc@1 98.682 (98.535)	Acc@5 100.000 (99.995)
Epoch: [150][9/25]	Time 0.574 (0.577)	Data 0.007 (0.070)	Loss 0.1935 (0.2056)	Acc@1 99.121 (98.594)	Acc@5 100.000 (99.995)
Epoch: [150][10/25]	Time 0.538 (0.574)	Data 0.005 (0.064)	Loss 0.2003 (0.2051)	Acc@1 98.926 (98.624)	Acc@5 100.000 (99.996)
Epoch: [150][11/25]	Time 0.586 (0.575)	Data 0.004 (0.059)	Loss 0.2082 (0.2053)	Acc@1 98.682 (98.629)	Acc@5 100.000 (99.996)
Epoch: [150][12/25]	Time 0.567 (0.574)	Data 0.005 (0.055)	Loss 0.2028 (0.2052)	Acc@1 98.779 (98.640)	Acc@5 100.000 (99.996)
Epoch: [150][13/25]	Time 0.592 (0.575)	Data 0.007 (0.051)	Loss 0.1966 (0.2045)	Acc@1 99.023 (98.668)	Acc@5 100.000 (99.997)
Epoch: [150][14/25]	Time 0.554 (0.574)	Data 0.004 (0.048)	Loss 0.2102 (0.2049)	Acc@1 98.291 (98.643)	Acc@5 100.000 (99.997)
Epoch: [150][15/25]	Time 0.580 (0.574)	Data 0.007 (0.046)	Loss 0.2117 (0.2053)	Acc@1 98.242 (98.618)	Acc@5 100.000 (99.997)
Epoch: [150][16/25]	Time 0.537 (0.572)	Data 0.005 (0.043)	Loss 0.2088 (0.2055)	Acc@1 98.340 (98.601)	Acc@5 100.000 (99.997)
Epoch: [150][17/25]	Time 0.595 (0.573)	Data 0.005 (0.041)	Loss 0.2070 (0.2056)	Acc@1 98.242 (98.581)	Acc@5 100.000 (99.997)
Epoch: [150][18/25]	Time 0.585 (0.574)	Data 0.005 (0.039)	Loss 0.2033 (0.2055)	Acc@1 98.584 (98.581)	Acc@5 100.000 (99.997)
Epoch: [150][19/25]	Time 0.600 (0.575)	Data 0.006 (0.038)	Loss 0.1996 (0.2052)	Acc@1 98.779 (98.591)	Acc@5 100.000 (99.998)
Epoch: [150][20/25]	Time 0.590 (0.576)	Data 0.007 (0.036)	Loss 0.2018 (0.2050)	Acc@1 98.877 (98.605)	Acc@5 100.000 (99.998)
Epoch: [150][21/25]	Time 0.575 (0.576)	Data 0.006 (0.035)	Loss 0.2022 (0.2049)	Acc@1 98.730 (98.611)	Acc@5 100.000 (99.998)
Epoch: [150][22/25]	Time 0.577 (0.576)	Data 0.008 (0.034)	Loss 0.2029 (0.2048)	Acc@1 98.633 (98.612)	Acc@5 100.000 (99.998)
Epoch: [150][23/25]	Time 0.556 (0.575)	Data 0.004 (0.032)	Loss 0.2069 (0.2049)	Acc@1 98.340 (98.600)	Acc@5 100.000 (99.998)
Epoch: [150][24/25]	Time 0.332 (0.565)	Data 0.006 (0.031)	Loss 0.1960 (0.2048)	Acc@1 98.821 (98.604)	Acc@5 100.000 (99.998)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [151 | 180] LR: 0.000400
Epoch: [151][0/25]	Time 0.553 (0.553)	Data 0.619 (0.619)	Loss 0.2169 (0.2169)	Acc@1 98.096 (98.096)	Acc@5 100.000 (100.000)
Epoch: [151][1/25]	Time 0.597 (0.575)	Data 0.006 (0.312)	Loss 0.1990 (0.2079)	Acc@1 98.535 (98.315)	Acc@5 100.000 (100.000)
Epoch: [151][2/25]	Time 0.538 (0.562)	Data 0.006 (0.210)	Loss 0.1912 (0.2023)	Acc@1 98.975 (98.535)	Acc@5 100.000 (100.000)
Epoch: [151][3/25]	Time 0.542 (0.557)	Data 0.005 (0.159)	Loss 0.2084 (0.2039)	Acc@1 98.438 (98.511)	Acc@5 100.000 (100.000)
Epoch: [151][4/25]	Time 0.666 (0.579)	Data 0.008 (0.129)	Loss 0.1969 (0.2025)	Acc@1 98.779 (98.564)	Acc@5 100.000 (100.000)
Epoch: [151][5/25]	Time 0.667 (0.594)	Data 0.004 (0.108)	Loss 0.1999 (0.2020)	Acc@1 98.730 (98.592)	Acc@5 100.000 (100.000)
Epoch: [151][6/25]	Time 0.554 (0.588)	Data 0.008 (0.094)	Loss 0.2059 (0.2026)	Acc@1 98.438 (98.570)	Acc@5 100.000 (100.000)
Epoch: [151][7/25]	Time 0.589 (0.588)	Data 0.004 (0.082)	Loss 0.2046 (0.2029)	Acc@1 98.486 (98.560)	Acc@5 100.000 (100.000)
Epoch: [151][8/25]	Time 0.535 (0.582)	Data 0.005 (0.074)	Loss 0.1980 (0.2023)	Acc@1 98.975 (98.606)	Acc@5 100.000 (100.000)
Epoch: [151][9/25]	Time 0.581 (0.582)	Data 0.006 (0.067)	Loss 0.1937 (0.2015)	Acc@1 99.023 (98.647)	Acc@5 100.000 (100.000)
Epoch: [151][10/25]	Time 0.562 (0.580)	Data 0.007 (0.062)	Loss 0.1937 (0.2008)	Acc@1 98.877 (98.668)	Acc@5 100.000 (100.000)
Epoch: [151][11/25]	Time 0.575 (0.580)	Data 0.006 (0.057)	Loss 0.1959 (0.2004)	Acc@1 98.877 (98.686)	Acc@5 100.000 (100.000)
Epoch: [151][12/25]	Time 0.578 (0.580)	Data 0.006 (0.053)	Loss 0.2002 (0.2003)	Acc@1 98.438 (98.667)	Acc@5 100.000 (100.000)
Epoch: [151][13/25]	Time 0.599 (0.581)	Data 0.007 (0.050)	Loss 0.1965 (0.2001)	Acc@1 98.779 (98.675)	Acc@5 100.000 (100.000)
Epoch: [151][14/25]	Time 0.567 (0.580)	Data 0.004 (0.047)	Loss 0.1976 (0.1999)	Acc@1 98.730 (98.678)	Acc@5 100.000 (100.000)
Epoch: [151][15/25]	Time 0.567 (0.579)	Data 0.008 (0.044)	Loss 0.1973 (0.1997)	Acc@1 98.828 (98.688)	Acc@5 100.000 (100.000)
Epoch: [151][16/25]	Time 0.536 (0.577)	Data 0.005 (0.042)	Loss 0.1951 (0.1995)	Acc@1 99.023 (98.707)	Acc@5 100.000 (100.000)
Epoch: [151][17/25]	Time 0.582 (0.577)	Data 0.007 (0.040)	Loss 0.1975 (0.1994)	Acc@1 98.584 (98.701)	Acc@5 100.000 (100.000)
Epoch: [151][18/25]	Time 0.545 (0.575)	Data 0.005 (0.038)	Loss 0.2002 (0.1994)	Acc@1 98.584 (98.694)	Acc@5 100.000 (100.000)
Epoch: [151][19/25]	Time 0.582 (0.576)	Data 0.005 (0.037)	Loss 0.1981 (0.1993)	Acc@1 99.072 (98.713)	Acc@5 100.000 (100.000)
Epoch: [151][20/25]	Time 0.564 (0.575)	Data 0.005 (0.035)	Loss 0.1963 (0.1992)	Acc@1 99.023 (98.728)	Acc@5 100.000 (100.000)
Epoch: [151][21/25]	Time 0.570 (0.575)	Data 0.006 (0.034)	Loss 0.1963 (0.1991)	Acc@1 98.877 (98.735)	Acc@5 100.000 (100.000)
Epoch: [151][22/25]	Time 0.564 (0.574)	Data 0.004 (0.032)	Loss 0.2000 (0.1991)	Acc@1 98.535 (98.726)	Acc@5 100.000 (100.000)
Epoch: [151][23/25]	Time 0.583 (0.575)	Data 0.006 (0.031)	Loss 0.2078 (0.1995)	Acc@1 98.730 (98.726)	Acc@5 100.000 (100.000)
Epoch: [151][24/25]	Time 0.344 (0.566)	Data 0.006 (0.030)	Loss 0.2060 (0.1996)	Acc@1 98.349 (98.720)	Acc@5 100.000 (100.000)

Epoch: [152 | 180] LR: 0.000400
Epoch: [152][0/25]	Time 0.598 (0.598)	Data 0.589 (0.589)	Loss 0.1991 (0.1991)	Acc@1 99.023 (99.023)	Acc@5 100.000 (100.000)
Epoch: [152][1/25]	Time 0.580 (0.589)	Data 0.008 (0.298)	Loss 0.1997 (0.1994)	Acc@1 98.877 (98.950)	Acc@5 100.000 (100.000)
Epoch: [152][2/25]	Time 0.590 (0.589)	Data 0.003 (0.200)	Loss 0.2031 (0.2006)	Acc@1 98.682 (98.861)	Acc@5 100.000 (100.000)
Epoch: [152][3/25]	Time 0.549 (0.579)	Data 0.005 (0.151)	Loss 0.2008 (0.2007)	Acc@1 98.633 (98.804)	Acc@5 100.000 (100.000)
Epoch: [152][4/25]	Time 0.556 (0.575)	Data 0.006 (0.122)	Loss 0.1988 (0.2003)	Acc@1 98.779 (98.799)	Acc@5 100.000 (100.000)
Epoch: [152][5/25]	Time 0.583 (0.576)	Data 0.007 (0.103)	Loss 0.1983 (0.2000)	Acc@1 98.779 (98.796)	Acc@5 100.000 (100.000)
Epoch: [152][6/25]	Time 0.552 (0.572)	Data 0.008 (0.089)	Loss 0.1960 (0.1994)	Acc@1 98.828 (98.800)	Acc@5 100.000 (100.000)
Epoch: [152][7/25]	Time 0.577 (0.573)	Data 0.005 (0.079)	Loss 0.2017 (0.1997)	Acc@1 98.682 (98.785)	Acc@5 100.000 (100.000)
Epoch: [152][8/25]	Time 0.573 (0.573)	Data 0.007 (0.071)	Loss 0.1969 (0.1994)	Acc@1 98.730 (98.779)	Acc@5 100.000 (100.000)
Epoch: [152][9/25]	Time 0.569 (0.573)	Data 0.007 (0.065)	Loss 0.1965 (0.1991)	Acc@1 98.730 (98.774)	Acc@5 100.000 (100.000)
Epoch: [152][10/25]	Time 0.574 (0.573)	Data 0.007 (0.059)	Loss 0.1995 (0.1991)	Acc@1 98.682 (98.766)	Acc@5 100.000 (100.000)
Epoch: [152][11/25]	Time 0.615 (0.576)	Data 0.004 (0.055)	Loss 0.1959 (0.1989)	Acc@1 98.682 (98.759)	Acc@5 100.000 (100.000)
Epoch: [152][12/25]	Time 0.585 (0.577)	Data 0.008 (0.051)	Loss 0.1965 (0.1987)	Acc@1 98.926 (98.772)	Acc@5 99.951 (99.996)
Epoch: [152][13/25]	Time 0.579 (0.577)	Data 0.009 (0.048)	Loss 0.1935 (0.1983)	Acc@1 99.170 (98.800)	Acc@5 100.000 (99.997)
Epoch: [152][14/25]	Time 0.550 (0.575)	Data 0.004 (0.045)	Loss 0.1966 (0.1982)	Acc@1 98.975 (98.812)	Acc@5 100.000 (99.997)
Epoch: [152][15/25]	Time 0.588 (0.576)	Data 0.005 (0.043)	Loss 0.1906 (0.1977)	Acc@1 99.023 (98.825)	Acc@5 100.000 (99.997)
Epoch: [152][16/25]	Time 0.586 (0.577)	Data 0.007 (0.040)	Loss 0.1961 (0.1976)	Acc@1 98.877 (98.828)	Acc@5 100.000 (99.997)
Epoch: [152][17/25]	Time 0.537 (0.575)	Data 0.005 (0.038)	Loss 0.1886 (0.1971)	Acc@1 99.268 (98.853)	Acc@5 100.000 (99.997)
Epoch: [152][18/25]	Time 0.577 (0.575)	Data 0.004 (0.037)	Loss 0.1916 (0.1968)	Acc@1 99.316 (98.877)	Acc@5 100.000 (99.997)
Epoch: [152][19/25]	Time 0.583 (0.575)	Data 0.007 (0.035)	Loss 0.1973 (0.1969)	Acc@1 98.926 (98.879)	Acc@5 100.000 (99.998)
Epoch: [152][20/25]	Time 0.588 (0.576)	Data 0.006 (0.034)	Loss 0.1900 (0.1965)	Acc@1 99.072 (98.889)	Acc@5 100.000 (99.998)
Epoch: [152][21/25]	Time 0.593 (0.576)	Data 0.006 (0.033)	Loss 0.1911 (0.1963)	Acc@1 99.170 (98.901)	Acc@5 99.951 (99.996)
Epoch: [152][22/25]	Time 0.578 (0.577)	Data 0.004 (0.031)	Loss 0.2053 (0.1967)	Acc@1 98.926 (98.902)	Acc@5 99.951 (99.994)
Epoch: [152][23/25]	Time 0.557 (0.576)	Data 0.005 (0.030)	Loss 0.1941 (0.1966)	Acc@1 98.926 (98.903)	Acc@5 100.000 (99.994)
Epoch: [152][24/25]	Time 0.343 (0.566)	Data 0.006 (0.029)	Loss 0.1971 (0.1966)	Acc@1 99.175 (98.908)	Acc@5 100.000 (99.994)

Epoch: [153 | 180] LR: 0.000400
Epoch: [153][0/25]	Time 0.593 (0.593)	Data 0.614 (0.614)	Loss 0.1855 (0.1855)	Acc@1 99.512 (99.512)	Acc@5 100.000 (100.000)
Epoch: [153][1/25]	Time 0.596 (0.595)	Data 0.007 (0.310)	Loss 0.1972 (0.1914)	Acc@1 98.535 (99.023)	Acc@5 100.000 (100.000)
Epoch: [153][2/25]	Time 0.569 (0.586)	Data 0.006 (0.209)	Loss 0.1938 (0.1922)	Acc@1 98.926 (98.991)	Acc@5 100.000 (100.000)
Epoch: [153][3/25]	Time 0.593 (0.588)	Data 0.006 (0.158)	Loss 0.1922 (0.1922)	Acc@1 99.170 (99.036)	Acc@5 100.000 (100.000)
Epoch: [153][4/25]	Time 0.574 (0.585)	Data 0.005 (0.127)	Loss 0.1911 (0.1920)	Acc@1 99.023 (99.033)	Acc@5 100.000 (100.000)
Epoch: [153][5/25]	Time 0.580 (0.584)	Data 0.008 (0.108)	Loss 0.1966 (0.1927)	Acc@1 99.072 (99.040)	Acc@5 100.000 (100.000)
Epoch: [153][6/25]	Time 0.584 (0.584)	Data 0.005 (0.093)	Loss 0.1956 (0.1931)	Acc@1 98.828 (99.009)	Acc@5 100.000 (100.000)
Epoch: [153][7/25]	Time 0.569 (0.582)	Data 0.005 (0.082)	Loss 0.1978 (0.1937)	Acc@1 98.877 (98.993)	Acc@5 100.000 (100.000)
Epoch: [153][8/25]	Time 0.576 (0.582)	Data 0.006 (0.073)	Loss 0.1937 (0.1937)	Acc@1 99.121 (99.007)	Acc@5 100.000 (100.000)
Epoch: [153][9/25]	Time 0.589 (0.582)	Data 0.006 (0.067)	Loss 0.1877 (0.1931)	Acc@1 99.365 (99.043)	Acc@5 100.000 (100.000)
Epoch: [153][10/25]	Time 0.587 (0.583)	Data 0.005 (0.061)	Loss 0.1874 (0.1926)	Acc@1 99.316 (99.068)	Acc@5 100.000 (100.000)
Epoch: [153][11/25]	Time 0.581 (0.583)	Data 0.007 (0.057)	Loss 0.1945 (0.1928)	Acc@1 98.828 (99.048)	Acc@5 100.000 (100.000)
Epoch: [153][12/25]	Time 0.592 (0.583)	Data 0.009 (0.053)	Loss 0.1870 (0.1923)	Acc@1 99.365 (99.072)	Acc@5 100.000 (100.000)
Epoch: [153][13/25]	Time 0.575 (0.583)	Data 0.006 (0.050)	Loss 0.1914 (0.1923)	Acc@1 98.926 (99.062)	Acc@5 100.000 (100.000)
Epoch: [153][14/25]	Time 0.564 (0.582)	Data 0.005 (0.047)	Loss 0.1881 (0.1920)	Acc@1 99.170 (99.069)	Acc@5 100.000 (100.000)
Epoch: [153][15/25]	Time 0.590 (0.582)	Data 0.009 (0.044)	Loss 0.1921 (0.1920)	Acc@1 99.121 (99.072)	Acc@5 100.000 (100.000)
Epoch: [153][16/25]	Time 0.603 (0.583)	Data 0.005 (0.042)	Loss 0.1982 (0.1923)	Acc@1 98.877 (99.061)	Acc@5 100.000 (100.000)
Epoch: [153][17/25]	Time 0.596 (0.584)	Data 0.004 (0.040)	Loss 0.1905 (0.1922)	Acc@1 99.121 (99.064)	Acc@5 100.000 (100.000)
Epoch: [153][18/25]	Time 0.570 (0.583)	Data 0.006 (0.038)	Loss 0.1889 (0.1921)	Acc@1 99.219 (99.072)	Acc@5 100.000 (100.000)
Epoch: [153][19/25]	Time 0.599 (0.584)	Data 0.005 (0.036)	Loss 0.1821 (0.1916)	Acc@1 99.463 (99.092)	Acc@5 100.000 (100.000)
Epoch: [153][20/25]	Time 0.586 (0.584)	Data 0.005 (0.035)	Loss 0.1913 (0.1916)	Acc@1 99.121 (99.093)	Acc@5 100.000 (100.000)
Epoch: [153][21/25]	Time 0.555 (0.583)	Data 0.006 (0.034)	Loss 0.1938 (0.1917)	Acc@1 98.926 (99.086)	Acc@5 100.000 (100.000)
Epoch: [153][22/25]	Time 0.583 (0.583)	Data 0.004 (0.032)	Loss 0.1885 (0.1915)	Acc@1 99.170 (99.089)	Acc@5 100.000 (100.000)
Epoch: [153][23/25]	Time 0.563 (0.582)	Data 0.004 (0.031)	Loss 0.1907 (0.1915)	Acc@1 99.268 (99.097)	Acc@5 100.000 (100.000)
Epoch: [153][24/25]	Time 0.343 (0.572)	Data 0.004 (0.030)	Loss 0.1951 (0.1916)	Acc@1 99.175 (99.098)	Acc@5 100.000 (100.000)

Epoch: [154 | 180] LR: 0.000400
Epoch: [154][0/25]	Time 0.585 (0.585)	Data 0.659 (0.659)	Loss 0.1886 (0.1886)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [154][1/25]	Time 0.562 (0.573)	Data 0.005 (0.332)	Loss 0.1936 (0.1911)	Acc@1 99.023 (99.121)	Acc@5 100.000 (100.000)
Epoch: [154][2/25]	Time 0.524 (0.557)	Data 0.006 (0.223)	Loss 0.1930 (0.1917)	Acc@1 99.219 (99.154)	Acc@5 100.000 (100.000)
Epoch: [154][3/25]	Time 0.523 (0.548)	Data 0.007 (0.169)	Loss 0.1940 (0.1923)	Acc@1 98.877 (99.084)	Acc@5 100.000 (100.000)
Epoch: [154][4/25]	Time 0.581 (0.555)	Data 0.008 (0.137)	Loss 0.1950 (0.1928)	Acc@1 99.023 (99.072)	Acc@5 100.000 (100.000)
Epoch: [154][5/25]	Time 0.544 (0.553)	Data 0.007 (0.115)	Loss 0.1839 (0.1914)	Acc@1 99.609 (99.162)	Acc@5 100.000 (100.000)
Epoch: [154][6/25]	Time 0.595 (0.559)	Data 0.003 (0.099)	Loss 0.1893 (0.1911)	Acc@1 99.219 (99.170)	Acc@5 100.000 (100.000)
Epoch: [154][7/25]	Time 0.628 (0.568)	Data 0.005 (0.088)	Loss 0.1966 (0.1918)	Acc@1 98.633 (99.103)	Acc@5 100.000 (100.000)
Epoch: [154][8/25]	Time 0.571 (0.568)	Data 0.007 (0.079)	Loss 0.1905 (0.1916)	Acc@1 99.072 (99.099)	Acc@5 100.000 (100.000)
Epoch: [154][9/25]	Time 0.592 (0.570)	Data 0.006 (0.071)	Loss 0.1841 (0.1909)	Acc@1 99.463 (99.136)	Acc@5 100.000 (100.000)
Epoch: [154][10/25]	Time 0.572 (0.571)	Data 0.005 (0.065)	Loss 0.1910 (0.1909)	Acc@1 98.926 (99.117)	Acc@5 100.000 (100.000)
Epoch: [154][11/25]	Time 0.566 (0.570)	Data 0.007 (0.060)	Loss 0.1946 (0.1912)	Acc@1 98.926 (99.101)	Acc@5 100.000 (100.000)
Epoch: [154][12/25]	Time 0.558 (0.569)	Data 0.004 (0.056)	Loss 0.1861 (0.1908)	Acc@1 99.463 (99.129)	Acc@5 100.000 (100.000)
Epoch: [154][13/25]	Time 0.560 (0.569)	Data 0.006 (0.053)	Loss 0.1952 (0.1911)	Acc@1 98.975 (99.118)	Acc@5 100.000 (100.000)
Epoch: [154][14/25]	Time 0.588 (0.570)	Data 0.005 (0.049)	Loss 0.1947 (0.1913)	Acc@1 98.682 (99.089)	Acc@5 100.000 (100.000)
Epoch: [154][15/25]	Time 0.608 (0.572)	Data 0.007 (0.047)	Loss 0.1871 (0.1911)	Acc@1 99.219 (99.097)	Acc@5 100.000 (100.000)
Epoch: [154][16/25]	Time 0.580 (0.573)	Data 0.006 (0.044)	Loss 0.1922 (0.1911)	Acc@1 99.316 (99.110)	Acc@5 100.000 (100.000)
Epoch: [154][17/25]	Time 0.563 (0.572)	Data 0.006 (0.042)	Loss 0.1896 (0.1911)	Acc@1 99.072 (99.108)	Acc@5 100.000 (100.000)
Epoch: [154][18/25]	Time 0.584 (0.573)	Data 0.005 (0.040)	Loss 0.1960 (0.1913)	Acc@1 99.170 (99.111)	Acc@5 100.000 (100.000)
Epoch: [154][19/25]	Time 0.596 (0.574)	Data 0.009 (0.039)	Loss 0.1884 (0.1912)	Acc@1 99.219 (99.116)	Acc@5 100.000 (100.000)
Epoch: [154][20/25]	Time 0.563 (0.573)	Data 0.005 (0.037)	Loss 0.1943 (0.1913)	Acc@1 98.877 (99.105)	Acc@5 100.000 (100.000)
Epoch: [154][21/25]	Time 0.603 (0.575)	Data 0.007 (0.036)	Loss 0.1902 (0.1913)	Acc@1 99.268 (99.112)	Acc@5 100.000 (100.000)
Epoch: [154][22/25]	Time 0.593 (0.576)	Data 0.004 (0.034)	Loss 0.1905 (0.1912)	Acc@1 99.023 (99.108)	Acc@5 100.000 (100.000)
Epoch: [154][23/25]	Time 0.583 (0.576)	Data 0.004 (0.033)	Loss 0.1891 (0.1912)	Acc@1 99.170 (99.111)	Acc@5 100.000 (100.000)
Epoch: [154][24/25]	Time 0.328 (0.566)	Data 0.006 (0.032)	Loss 0.1916 (0.1912)	Acc@1 98.821 (99.106)	Acc@5 100.000 (100.000)

Epoch: [155 | 180] LR: 0.000400
Epoch: [155][0/25]	Time 0.575 (0.575)	Data 0.643 (0.643)	Loss 0.1783 (0.1783)	Acc@1 99.658 (99.658)	Acc@5 100.000 (100.000)
Epoch: [155][1/25]	Time 0.586 (0.580)	Data 0.012 (0.328)	Loss 0.1884 (0.1834)	Acc@1 99.219 (99.438)	Acc@5 100.000 (100.000)
Epoch: [155][2/25]	Time 0.587 (0.583)	Data 0.006 (0.221)	Loss 0.1968 (0.1879)	Acc@1 98.779 (99.219)	Acc@5 100.000 (100.000)
Epoch: [155][3/25]	Time 0.546 (0.574)	Data 0.006 (0.167)	Loss 0.1909 (0.1886)	Acc@1 99.072 (99.182)	Acc@5 100.000 (100.000)
Epoch: [155][4/25]	Time 0.576 (0.574)	Data 0.008 (0.135)	Loss 0.1910 (0.1891)	Acc@1 99.023 (99.150)	Acc@5 100.000 (100.000)
Epoch: [155][5/25]	Time 0.570 (0.573)	Data 0.005 (0.113)	Loss 0.1827 (0.1880)	Acc@1 99.316 (99.178)	Acc@5 100.000 (100.000)
Epoch: [155][6/25]	Time 0.579 (0.574)	Data 0.006 (0.098)	Loss 0.1954 (0.1891)	Acc@1 98.828 (99.128)	Acc@5 100.000 (100.000)
Epoch: [155][7/25]	Time 0.579 (0.575)	Data 0.004 (0.086)	Loss 0.1908 (0.1893)	Acc@1 98.975 (99.109)	Acc@5 100.000 (100.000)
Epoch: [155][8/25]	Time 0.588 (0.576)	Data 0.004 (0.077)	Loss 0.1883 (0.1892)	Acc@1 99.170 (99.116)	Acc@5 100.000 (100.000)
Epoch: [155][9/25]	Time 0.570 (0.576)	Data 0.007 (0.070)	Loss 0.1808 (0.1884)	Acc@1 99.609 (99.165)	Acc@5 100.000 (100.000)
Epoch: [155][10/25]	Time 0.598 (0.578)	Data 0.007 (0.064)	Loss 0.1957 (0.1890)	Acc@1 98.877 (99.139)	Acc@5 99.951 (99.996)
Epoch: [155][11/25]	Time 0.595 (0.579)	Data 0.006 (0.060)	Loss 0.1884 (0.1890)	Acc@1 99.219 (99.146)	Acc@5 100.000 (99.996)
Epoch: [155][12/25]	Time 0.554 (0.577)	Data 0.007 (0.056)	Loss 0.1844 (0.1886)	Acc@1 99.268 (99.155)	Acc@5 100.000 (99.996)
Epoch: [155][13/25]	Time 0.573 (0.577)	Data 0.005 (0.052)	Loss 0.1900 (0.1887)	Acc@1 99.316 (99.166)	Acc@5 100.000 (99.997)
Epoch: [155][14/25]	Time 0.583 (0.577)	Data 0.005 (0.049)	Loss 0.1947 (0.1891)	Acc@1 98.828 (99.144)	Acc@5 100.000 (99.997)
Epoch: [155][15/25]	Time 0.556 (0.576)	Data 0.006 (0.046)	Loss 0.1846 (0.1888)	Acc@1 99.512 (99.167)	Acc@5 100.000 (99.997)
Epoch: [155][16/25]	Time 0.593 (0.577)	Data 0.005 (0.044)	Loss 0.1958 (0.1892)	Acc@1 99.023 (99.158)	Acc@5 100.000 (99.997)
Epoch: [155][17/25]	Time 0.597 (0.578)	Data 0.004 (0.042)	Loss 0.1890 (0.1892)	Acc@1 99.268 (99.164)	Acc@5 100.000 (99.997)
Epoch: [155][18/25]	Time 0.603 (0.579)	Data 0.005 (0.040)	Loss 0.1868 (0.1891)	Acc@1 99.365 (99.175)	Acc@5 100.000 (99.997)
Epoch: [155][19/25]	Time 0.595 (0.580)	Data 0.008 (0.038)	Loss 0.1858 (0.1889)	Acc@1 99.121 (99.172)	Acc@5 100.000 (99.998)
Epoch: [155][20/25]	Time 0.551 (0.579)	Data 0.004 (0.036)	Loss 0.1915 (0.1891)	Acc@1 98.926 (99.161)	Acc@5 100.000 (99.998)
Epoch: [155][21/25]	Time 0.578 (0.579)	Data 0.007 (0.035)	Loss 0.1899 (0.1891)	Acc@1 99.072 (99.157)	Acc@5 100.000 (99.998)
Epoch: [155][22/25]	Time 0.577 (0.579)	Data 0.004 (0.034)	Loss 0.1925 (0.1893)	Acc@1 99.121 (99.155)	Acc@5 100.000 (99.998)
Epoch: [155][23/25]	Time 0.578 (0.579)	Data 0.005 (0.033)	Loss 0.1862 (0.1891)	Acc@1 99.268 (99.160)	Acc@5 100.000 (99.998)
Epoch: [155][24/25]	Time 0.332 (0.569)	Data 0.006 (0.032)	Loss 0.1934 (0.1892)	Acc@1 98.939 (99.156)	Acc@5 100.000 (99.998)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [156 | 180] LR: 0.000400
Epoch: [156][0/25]	Time 0.588 (0.588)	Data 0.681 (0.681)	Loss 0.1863 (0.1863)	Acc@1 99.414 (99.414)	Acc@5 100.000 (100.000)
Epoch: [156][1/25]	Time 0.545 (0.567)	Data 0.005 (0.343)	Loss 0.1868 (0.1865)	Acc@1 99.219 (99.316)	Acc@5 100.000 (100.000)
Epoch: [156][2/25]	Time 0.580 (0.571)	Data 0.007 (0.231)	Loss 0.1815 (0.1849)	Acc@1 99.512 (99.382)	Acc@5 100.000 (100.000)
Epoch: [156][3/25]	Time 0.531 (0.561)	Data 0.005 (0.174)	Loss 0.1854 (0.1850)	Acc@1 99.268 (99.353)	Acc@5 100.000 (100.000)
Epoch: [156][4/25]	Time 0.597 (0.568)	Data 0.008 (0.141)	Loss 0.1885 (0.1857)	Acc@1 99.023 (99.287)	Acc@5 100.000 (100.000)
Epoch: [156][5/25]	Time 0.552 (0.566)	Data 0.007 (0.119)	Loss 0.1844 (0.1855)	Acc@1 99.414 (99.308)	Acc@5 100.000 (100.000)
Epoch: [156][6/25]	Time 0.564 (0.565)	Data 0.004 (0.102)	Loss 0.1892 (0.1860)	Acc@1 99.219 (99.295)	Acc@5 100.000 (100.000)
Epoch: [156][7/25]	Time 0.583 (0.568)	Data 0.003 (0.090)	Loss 0.1863 (0.1860)	Acc@1 99.316 (99.298)	Acc@5 100.000 (100.000)
Epoch: [156][8/25]	Time 0.569 (0.568)	Data 0.006 (0.081)	Loss 0.1754 (0.1849)	Acc@1 99.756 (99.349)	Acc@5 100.000 (100.000)
Epoch: [156][9/25]	Time 0.567 (0.568)	Data 0.005 (0.073)	Loss 0.1895 (0.1853)	Acc@1 99.219 (99.336)	Acc@5 100.000 (100.000)
Epoch: [156][10/25]	Time 0.590 (0.570)	Data 0.004 (0.067)	Loss 0.1829 (0.1851)	Acc@1 99.414 (99.343)	Acc@5 100.000 (100.000)
Epoch: [156][11/25]	Time 0.552 (0.568)	Data 0.007 (0.062)	Loss 0.1855 (0.1851)	Acc@1 99.512 (99.357)	Acc@5 100.000 (100.000)
Epoch: [156][12/25]	Time 0.559 (0.568)	Data 0.007 (0.058)	Loss 0.1842 (0.1851)	Acc@1 99.414 (99.361)	Acc@5 100.000 (100.000)
Epoch: [156][13/25]	Time 0.543 (0.566)	Data 0.008 (0.054)	Loss 0.1867 (0.1852)	Acc@1 99.414 (99.365)	Acc@5 99.951 (99.997)
Epoch: [156][14/25]	Time 0.572 (0.566)	Data 0.006 (0.051)	Loss 0.1902 (0.1855)	Acc@1 99.023 (99.342)	Acc@5 100.000 (99.997)
Epoch: [156][15/25]	Time 0.569 (0.566)	Data 0.005 (0.048)	Loss 0.1867 (0.1856)	Acc@1 99.561 (99.356)	Acc@5 100.000 (99.997)
Epoch: [156][16/25]	Time 0.578 (0.567)	Data 0.007 (0.045)	Loss 0.1956 (0.1862)	Acc@1 98.682 (99.316)	Acc@5 100.000 (99.997)
Epoch: [156][17/25]	Time 0.565 (0.567)	Data 0.004 (0.043)	Loss 0.1905 (0.1864)	Acc@1 98.975 (99.297)	Acc@5 100.000 (99.997)
Epoch: [156][18/25]	Time 0.559 (0.567)	Data 0.005 (0.041)	Loss 0.1841 (0.1863)	Acc@1 99.414 (99.304)	Acc@5 100.000 (99.997)
Epoch: [156][19/25]	Time 0.595 (0.568)	Data 0.007 (0.039)	Loss 0.1815 (0.1861)	Acc@1 99.463 (99.312)	Acc@5 100.000 (99.998)
Epoch: [156][20/25]	Time 0.570 (0.568)	Data 0.006 (0.038)	Loss 0.1866 (0.1861)	Acc@1 99.170 (99.305)	Acc@5 100.000 (99.998)
Epoch: [156][21/25]	Time 0.591 (0.569)	Data 0.005 (0.036)	Loss 0.1895 (0.1862)	Acc@1 99.365 (99.308)	Acc@5 100.000 (99.998)
Epoch: [156][22/25]	Time 0.579 (0.570)	Data 0.004 (0.035)	Loss 0.1860 (0.1862)	Acc@1 99.268 (99.306)	Acc@5 100.000 (99.998)
Epoch: [156][23/25]	Time 0.579 (0.570)	Data 0.008 (0.034)	Loss 0.1855 (0.1862)	Acc@1 99.219 (99.302)	Acc@5 100.000 (99.998)
Epoch: [156][24/25]	Time 0.353 (0.561)	Data 0.007 (0.033)	Loss 0.1903 (0.1863)	Acc@1 99.292 (99.302)	Acc@5 100.000 (99.998)

Epoch: [157 | 180] LR: 0.000400
Epoch: [157][0/25]	Time 0.577 (0.577)	Data 0.610 (0.610)	Loss 0.1875 (0.1875)	Acc@1 99.463 (99.463)	Acc@5 100.000 (100.000)
Epoch: [157][1/25]	Time 0.594 (0.585)	Data 0.005 (0.308)	Loss 0.1834 (0.1854)	Acc@1 99.463 (99.463)	Acc@5 100.000 (100.000)
Epoch: [157][2/25]	Time 0.563 (0.578)	Data 0.007 (0.207)	Loss 0.1851 (0.1853)	Acc@1 99.268 (99.398)	Acc@5 100.000 (100.000)
Epoch: [157][3/25]	Time 0.574 (0.577)	Data 0.006 (0.157)	Loss 0.1821 (0.1845)	Acc@1 99.414 (99.402)	Acc@5 100.000 (100.000)
Epoch: [157][4/25]	Time 0.577 (0.577)	Data 0.006 (0.127)	Loss 0.1855 (0.1847)	Acc@1 99.268 (99.375)	Acc@5 100.000 (100.000)
Epoch: [157][5/25]	Time 0.568 (0.576)	Data 0.005 (0.107)	Loss 0.1866 (0.1850)	Acc@1 99.121 (99.333)	Acc@5 100.000 (100.000)
Epoch: [157][6/25]	Time 0.571 (0.575)	Data 0.007 (0.092)	Loss 0.1873 (0.1854)	Acc@1 99.316 (99.330)	Acc@5 100.000 (100.000)
Epoch: [157][7/25]	Time 0.576 (0.575)	Data 0.006 (0.081)	Loss 0.1877 (0.1857)	Acc@1 99.365 (99.335)	Acc@5 100.000 (100.000)
Epoch: [157][8/25]	Time 0.529 (0.570)	Data 0.006 (0.073)	Loss 0.1875 (0.1859)	Acc@1 99.219 (99.322)	Acc@5 100.000 (100.000)
Epoch: [157][9/25]	Time 0.596 (0.573)	Data 0.004 (0.066)	Loss 0.1859 (0.1859)	Acc@1 99.268 (99.316)	Acc@5 100.000 (100.000)
Epoch: [157][10/25]	Time 0.570 (0.572)	Data 0.005 (0.061)	Loss 0.1879 (0.1861)	Acc@1 99.268 (99.312)	Acc@5 100.000 (100.000)
Epoch: [157][11/25]	Time 0.601 (0.575)	Data 0.006 (0.056)	Loss 0.1934 (0.1867)	Acc@1 98.877 (99.276)	Acc@5 100.000 (100.000)
Epoch: [157][12/25]	Time 0.586 (0.576)	Data 0.006 (0.052)	Loss 0.1826 (0.1864)	Acc@1 99.609 (99.301)	Acc@5 100.000 (100.000)
Epoch: [157][13/25]	Time 0.577 (0.576)	Data 0.005 (0.049)	Loss 0.1897 (0.1866)	Acc@1 99.268 (99.299)	Acc@5 100.000 (100.000)
Epoch: [157][14/25]	Time 0.558 (0.574)	Data 0.004 (0.046)	Loss 0.1848 (0.1865)	Acc@1 99.463 (99.310)	Acc@5 100.000 (100.000)
Epoch: [157][15/25]	Time 0.585 (0.575)	Data 0.006 (0.043)	Loss 0.1866 (0.1865)	Acc@1 99.268 (99.307)	Acc@5 100.000 (100.000)
Epoch: [157][16/25]	Time 0.561 (0.574)	Data 0.006 (0.041)	Loss 0.1827 (0.1863)	Acc@1 99.561 (99.322)	Acc@5 100.000 (100.000)
Epoch: [157][17/25]	Time 0.588 (0.575)	Data 0.006 (0.039)	Loss 0.1865 (0.1863)	Acc@1 99.219 (99.316)	Acc@5 100.000 (100.000)
Epoch: [157][18/25]	Time 0.575 (0.575)	Data 0.005 (0.037)	Loss 0.1847 (0.1862)	Acc@1 99.463 (99.324)	Acc@5 99.951 (99.997)
Epoch: [157][19/25]	Time 0.564 (0.575)	Data 0.005 (0.036)	Loss 0.1855 (0.1862)	Acc@1 99.268 (99.321)	Acc@5 100.000 (99.998)
Epoch: [157][20/25]	Time 0.572 (0.574)	Data 0.005 (0.034)	Loss 0.1852 (0.1861)	Acc@1 99.463 (99.328)	Acc@5 100.000 (99.998)
Epoch: [157][21/25]	Time 0.552 (0.573)	Data 0.004 (0.033)	Loss 0.1822 (0.1859)	Acc@1 99.512 (99.336)	Acc@5 100.000 (99.998)
Epoch: [157][22/25]	Time 0.579 (0.574)	Data 0.008 (0.032)	Loss 0.1960 (0.1864)	Acc@1 99.023 (99.323)	Acc@5 100.000 (99.998)
Epoch: [157][23/25]	Time 0.568 (0.573)	Data 0.005 (0.031)	Loss 0.1953 (0.1867)	Acc@1 98.926 (99.306)	Acc@5 99.951 (99.996)
Epoch: [157][24/25]	Time 0.340 (0.564)	Data 0.004 (0.030)	Loss 0.1752 (0.1865)	Acc@1 99.764 (99.314)	Acc@5 100.000 (99.996)

Epoch: [158 | 180] LR: 0.000400
Epoch: [158][0/25]	Time 0.577 (0.577)	Data 0.633 (0.633)	Loss 0.1894 (0.1894)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [158][1/25]	Time 0.616 (0.596)	Data 0.005 (0.319)	Loss 0.1838 (0.1866)	Acc@1 99.316 (99.268)	Acc@5 100.000 (100.000)
Epoch: [158][2/25]	Time 0.552 (0.582)	Data 0.005 (0.214)	Loss 0.1892 (0.1874)	Acc@1 99.414 (99.316)	Acc@5 100.000 (100.000)
Epoch: [158][3/25]	Time 0.590 (0.584)	Data 0.008 (0.163)	Loss 0.1901 (0.1881)	Acc@1 99.170 (99.280)	Acc@5 100.000 (100.000)
Epoch: [158][4/25]	Time 0.556 (0.578)	Data 0.009 (0.132)	Loss 0.1853 (0.1876)	Acc@1 99.316 (99.287)	Acc@5 100.000 (100.000)
Epoch: [158][5/25]	Time 0.559 (0.575)	Data 0.004 (0.111)	Loss 0.1841 (0.1870)	Acc@1 99.365 (99.300)	Acc@5 100.000 (100.000)
Epoch: [158][6/25]	Time 0.548 (0.571)	Data 0.005 (0.095)	Loss 0.1843 (0.1866)	Acc@1 99.512 (99.330)	Acc@5 100.000 (100.000)
Epoch: [158][7/25]	Time 0.584 (0.573)	Data 0.003 (0.084)	Loss 0.1826 (0.1861)	Acc@1 99.463 (99.347)	Acc@5 100.000 (100.000)
Epoch: [158][8/25]	Time 0.552 (0.570)	Data 0.005 (0.075)	Loss 0.1785 (0.1853)	Acc@1 99.658 (99.382)	Acc@5 100.000 (100.000)
Epoch: [158][9/25]	Time 0.601 (0.573)	Data 0.007 (0.068)	Loss 0.1857 (0.1853)	Acc@1 99.316 (99.375)	Acc@5 100.000 (100.000)
Epoch: [158][10/25]	Time 0.571 (0.573)	Data 0.005 (0.063)	Loss 0.1825 (0.1850)	Acc@1 99.463 (99.383)	Acc@5 100.000 (100.000)
Epoch: [158][11/25]	Time 0.576 (0.573)	Data 0.004 (0.058)	Loss 0.1849 (0.1850)	Acc@1 99.512 (99.394)	Acc@5 100.000 (100.000)
Epoch: [158][12/25]	Time 0.579 (0.574)	Data 0.005 (0.054)	Loss 0.1829 (0.1849)	Acc@1 99.170 (99.377)	Acc@5 100.000 (100.000)
Epoch: [158][13/25]	Time 0.572 (0.574)	Data 0.006 (0.050)	Loss 0.1849 (0.1849)	Acc@1 99.512 (99.386)	Acc@5 100.000 (100.000)
Epoch: [158][14/25]	Time 0.592 (0.575)	Data 0.004 (0.047)	Loss 0.1907 (0.1853)	Acc@1 99.072 (99.365)	Acc@5 99.951 (99.997)
Epoch: [158][15/25]	Time 0.558 (0.574)	Data 0.007 (0.045)	Loss 0.1816 (0.1850)	Acc@1 99.512 (99.374)	Acc@5 100.000 (99.997)
Epoch: [158][16/25]	Time 0.570 (0.574)	Data 0.005 (0.042)	Loss 0.1852 (0.1850)	Acc@1 99.268 (99.368)	Acc@5 100.000 (99.997)
Epoch: [158][17/25]	Time 0.565 (0.573)	Data 0.004 (0.040)	Loss 0.1848 (0.1850)	Acc@1 99.463 (99.373)	Acc@5 100.000 (99.997)
Epoch: [158][18/25]	Time 0.589 (0.574)	Data 0.008 (0.038)	Loss 0.1938 (0.1855)	Acc@1 98.779 (99.342)	Acc@5 100.000 (99.997)
Epoch: [158][19/25]	Time 0.554 (0.573)	Data 0.004 (0.037)	Loss 0.1855 (0.1855)	Acc@1 99.365 (99.343)	Acc@5 100.000 (99.998)
Epoch: [158][20/25]	Time 0.581 (0.573)	Data 0.004 (0.035)	Loss 0.1879 (0.1856)	Acc@1 99.170 (99.335)	Acc@5 100.000 (99.998)
Epoch: [158][21/25]	Time 0.578 (0.574)	Data 0.007 (0.034)	Loss 0.1780 (0.1853)	Acc@1 99.609 (99.347)	Acc@5 100.000 (99.998)
Epoch: [158][22/25]	Time 0.575 (0.574)	Data 0.006 (0.033)	Loss 0.1857 (0.1853)	Acc@1 99.365 (99.348)	Acc@5 100.000 (99.998)
Epoch: [158][23/25]	Time 0.570 (0.573)	Data 0.006 (0.032)	Loss 0.1877 (0.1854)	Acc@1 99.219 (99.343)	Acc@5 100.000 (99.998)
Epoch: [158][24/25]	Time 0.371 (0.565)	Data 0.004 (0.030)	Loss 0.1907 (0.1855)	Acc@1 99.175 (99.340)	Acc@5 100.000 (99.998)

Epoch: [159 | 180] LR: 0.000400
Epoch: [159][0/25]	Time 0.565 (0.565)	Data 0.602 (0.602)	Loss 0.1896 (0.1896)	Acc@1 99.121 (99.121)	Acc@5 100.000 (100.000)
Epoch: [159][1/25]	Time 0.601 (0.583)	Data 0.005 (0.303)	Loss 0.1873 (0.1884)	Acc@1 99.219 (99.170)	Acc@5 100.000 (100.000)
Epoch: [159][2/25]	Time 0.559 (0.575)	Data 0.006 (0.204)	Loss 0.1860 (0.1876)	Acc@1 99.365 (99.235)	Acc@5 100.000 (100.000)
Epoch: [159][3/25]	Time 0.599 (0.581)	Data 0.004 (0.154)	Loss 0.1898 (0.1882)	Acc@1 99.121 (99.207)	Acc@5 100.000 (100.000)
Epoch: [159][4/25]	Time 0.571 (0.579)	Data 0.005 (0.124)	Loss 0.1866 (0.1878)	Acc@1 99.268 (99.219)	Acc@5 100.000 (100.000)
Epoch: [159][5/25]	Time 0.585 (0.580)	Data 0.003 (0.104)	Loss 0.1841 (0.1872)	Acc@1 99.365 (99.243)	Acc@5 100.000 (100.000)
Epoch: [159][6/25]	Time 0.560 (0.577)	Data 0.006 (0.090)	Loss 0.1857 (0.1870)	Acc@1 99.219 (99.240)	Acc@5 100.000 (100.000)
Epoch: [159][7/25]	Time 0.588 (0.579)	Data 0.007 (0.080)	Loss 0.1816 (0.1863)	Acc@1 99.365 (99.255)	Acc@5 100.000 (100.000)
Epoch: [159][8/25]	Time 0.511 (0.571)	Data 0.004 (0.071)	Loss 0.1904 (0.1868)	Acc@1 98.975 (99.224)	Acc@5 100.000 (100.000)
Epoch: [159][9/25]	Time 0.653 (0.579)	Data 0.004 (0.065)	Loss 0.1815 (0.1863)	Acc@1 99.561 (99.258)	Acc@5 99.951 (99.995)
Epoch: [159][10/25]	Time 0.655 (0.586)	Data 0.007 (0.059)	Loss 0.1840 (0.1861)	Acc@1 99.268 (99.259)	Acc@5 100.000 (99.996)
Epoch: [159][11/25]	Time 0.579 (0.586)	Data 0.005 (0.055)	Loss 0.1848 (0.1860)	Acc@1 99.414 (99.272)	Acc@5 100.000 (99.996)
Epoch: [159][12/25]	Time 0.583 (0.585)	Data 0.006 (0.051)	Loss 0.1814 (0.1856)	Acc@1 99.561 (99.294)	Acc@5 100.000 (99.996)
Epoch: [159][13/25]	Time 0.579 (0.585)	Data 0.007 (0.048)	Loss 0.1888 (0.1858)	Acc@1 99.268 (99.292)	Acc@5 100.000 (99.997)
Epoch: [159][14/25]	Time 0.582 (0.585)	Data 0.007 (0.045)	Loss 0.1794 (0.1854)	Acc@1 99.609 (99.313)	Acc@5 100.000 (99.997)
Epoch: [159][15/25]	Time 0.541 (0.582)	Data 0.004 (0.043)	Loss 0.1834 (0.1853)	Acc@1 99.219 (99.307)	Acc@5 100.000 (99.997)
Epoch: [159][16/25]	Time 0.585 (0.582)	Data 0.008 (0.041)	Loss 0.1902 (0.1856)	Acc@1 99.316 (99.308)	Acc@5 100.000 (99.997)
Epoch: [159][17/25]	Time 0.583 (0.582)	Data 0.007 (0.039)	Loss 0.1861 (0.1856)	Acc@1 99.170 (99.300)	Acc@5 100.000 (99.997)
Epoch: [159][18/25]	Time 0.580 (0.582)	Data 0.004 (0.037)	Loss 0.1806 (0.1853)	Acc@1 99.463 (99.309)	Acc@5 100.000 (99.997)
Epoch: [159][19/25]	Time 0.537 (0.580)	Data 0.007 (0.035)	Loss 0.1850 (0.1853)	Acc@1 99.414 (99.314)	Acc@5 100.000 (99.998)
Epoch: [159][20/25]	Time 0.592 (0.580)	Data 0.008 (0.034)	Loss 0.1859 (0.1854)	Acc@1 99.121 (99.305)	Acc@5 100.000 (99.998)
Epoch: [159][21/25]	Time 0.570 (0.580)	Data 0.004 (0.033)	Loss 0.1860 (0.1854)	Acc@1 99.268 (99.303)	Acc@5 100.000 (99.998)
Epoch: [159][22/25]	Time 0.561 (0.579)	Data 0.006 (0.032)	Loss 0.1902 (0.1856)	Acc@1 98.975 (99.289)	Acc@5 100.000 (99.998)
Epoch: [159][23/25]	Time 0.575 (0.579)	Data 0.007 (0.031)	Loss 0.1831 (0.1855)	Acc@1 99.512 (99.298)	Acc@5 100.000 (99.998)
Epoch: [159][24/25]	Time 0.315 (0.568)	Data 0.006 (0.030)	Loss 0.1841 (0.1855)	Acc@1 99.410 (99.300)	Acc@5 100.000 (99.998)

Epoch: [160 | 180] LR: 0.000400
Epoch: [160][0/25]	Time 0.559 (0.559)	Data 0.606 (0.606)	Loss 0.1902 (0.1902)	Acc@1 99.365 (99.365)	Acc@5 100.000 (100.000)
Epoch: [160][1/25]	Time 0.577 (0.568)	Data 0.007 (0.306)	Loss 0.1889 (0.1895)	Acc@1 99.170 (99.268)	Acc@5 100.000 (100.000)
Epoch: [160][2/25]	Time 0.589 (0.575)	Data 0.009 (0.207)	Loss 0.1893 (0.1895)	Acc@1 99.219 (99.251)	Acc@5 99.951 (99.984)
Epoch: [160][3/25]	Time 0.544 (0.567)	Data 0.008 (0.157)	Loss 0.1824 (0.1877)	Acc@1 99.512 (99.316)	Acc@5 100.000 (99.988)
Epoch: [160][4/25]	Time 0.579 (0.570)	Data 0.005 (0.127)	Loss 0.1848 (0.1871)	Acc@1 99.365 (99.326)	Acc@5 100.000 (99.990)
Epoch: [160][5/25]	Time 0.543 (0.565)	Data 0.004 (0.106)	Loss 0.1853 (0.1868)	Acc@1 99.170 (99.300)	Acc@5 100.000 (99.992)
Epoch: [160][6/25]	Time 0.609 (0.571)	Data 0.009 (0.092)	Loss 0.1840 (0.1864)	Acc@1 99.365 (99.309)	Acc@5 100.000 (99.993)
Epoch: [160][7/25]	Time 0.587 (0.573)	Data 0.005 (0.082)	Loss 0.1851 (0.1863)	Acc@1 99.414 (99.323)	Acc@5 100.000 (99.994)
Epoch: [160][8/25]	Time 0.546 (0.570)	Data 0.005 (0.073)	Loss 0.1902 (0.1867)	Acc@1 98.828 (99.268)	Acc@5 100.000 (99.995)
Epoch: [160][9/25]	Time 0.579 (0.571)	Data 0.007 (0.066)	Loss 0.1811 (0.1861)	Acc@1 99.561 (99.297)	Acc@5 100.000 (99.995)
Epoch: [160][10/25]	Time 0.569 (0.571)	Data 0.005 (0.061)	Loss 0.1860 (0.1861)	Acc@1 99.219 (99.290)	Acc@5 100.000 (99.996)
Epoch: [160][11/25]	Time 0.576 (0.571)	Data 0.007 (0.056)	Loss 0.1910 (0.1865)	Acc@1 99.170 (99.280)	Acc@5 99.951 (99.992)
Epoch: [160][12/25]	Time 0.544 (0.569)	Data 0.007 (0.052)	Loss 0.1851 (0.1864)	Acc@1 99.316 (99.283)	Acc@5 100.000 (99.992)
Epoch: [160][13/25]	Time 0.565 (0.569)	Data 0.008 (0.049)	Loss 0.1805 (0.1860)	Acc@1 99.463 (99.295)	Acc@5 100.000 (99.993)
Epoch: [160][14/25]	Time 0.537 (0.567)	Data 0.007 (0.046)	Loss 0.1834 (0.1858)	Acc@1 99.365 (99.300)	Acc@5 100.000 (99.993)
Epoch: [160][15/25]	Time 0.520 (0.564)	Data 0.007 (0.044)	Loss 0.1837 (0.1857)	Acc@1 99.512 (99.313)	Acc@5 100.000 (99.994)
Epoch: [160][16/25]	Time 0.551 (0.563)	Data 0.005 (0.042)	Loss 0.1819 (0.1855)	Acc@1 99.512 (99.325)	Acc@5 100.000 (99.994)
Epoch: [160][17/25]	Time 0.582 (0.564)	Data 0.004 (0.040)	Loss 0.1855 (0.1855)	Acc@1 99.316 (99.325)	Acc@5 100.000 (99.995)
Epoch: [160][18/25]	Time 0.574 (0.565)	Data 0.004 (0.038)	Loss 0.1865 (0.1855)	Acc@1 99.170 (99.316)	Acc@5 100.000 (99.995)
Epoch: [160][19/25]	Time 0.568 (0.565)	Data 0.009 (0.036)	Loss 0.1903 (0.1858)	Acc@1 99.121 (99.307)	Acc@5 100.000 (99.995)
Epoch: [160][20/25]	Time 0.576 (0.565)	Data 0.007 (0.035)	Loss 0.1819 (0.1856)	Acc@1 99.463 (99.314)	Acc@5 100.000 (99.995)
Epoch: [160][21/25]	Time 0.571 (0.566)	Data 0.004 (0.034)	Loss 0.1752 (0.1851)	Acc@1 99.756 (99.334)	Acc@5 100.000 (99.996)
Epoch: [160][22/25]	Time 0.562 (0.565)	Data 0.006 (0.032)	Loss 0.1835 (0.1850)	Acc@1 99.463 (99.340)	Acc@5 100.000 (99.996)
Epoch: [160][23/25]	Time 0.586 (0.566)	Data 0.006 (0.031)	Loss 0.1852 (0.1850)	Acc@1 99.414 (99.343)	Acc@5 100.000 (99.996)
Epoch: [160][24/25]	Time 0.331 (0.557)	Data 0.004 (0.030)	Loss 0.1784 (0.1849)	Acc@1 99.646 (99.348)	Acc@5 100.000 (99.996)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [161 | 180] LR: 0.000400
Epoch: [161][0/25]	Time 0.577 (0.577)	Data 0.669 (0.669)	Loss 0.1880 (0.1880)	Acc@1 99.170 (99.170)	Acc@5 100.000 (100.000)
Epoch: [161][1/25]	Time 0.571 (0.574)	Data 0.005 (0.337)	Loss 0.1875 (0.1878)	Acc@1 99.170 (99.170)	Acc@5 100.000 (100.000)
Epoch: [161][2/25]	Time 0.570 (0.573)	Data 0.006 (0.227)	Loss 0.1864 (0.1873)	Acc@1 99.219 (99.186)	Acc@5 100.000 (100.000)
Epoch: [161][3/25]	Time 0.578 (0.574)	Data 0.004 (0.171)	Loss 0.1806 (0.1856)	Acc@1 99.561 (99.280)	Acc@5 100.000 (100.000)
Epoch: [161][4/25]	Time 0.543 (0.568)	Data 0.007 (0.138)	Loss 0.1824 (0.1850)	Acc@1 99.512 (99.326)	Acc@5 100.000 (100.000)
Epoch: [161][5/25]	Time 0.565 (0.567)	Data 0.003 (0.116)	Loss 0.1891 (0.1857)	Acc@1 99.121 (99.292)	Acc@5 100.000 (100.000)
Epoch: [161][6/25]	Time 0.578 (0.569)	Data 0.005 (0.100)	Loss 0.1850 (0.1856)	Acc@1 99.268 (99.289)	Acc@5 100.000 (100.000)
Epoch: [161][7/25]	Time 0.568 (0.569)	Data 0.006 (0.088)	Loss 0.1825 (0.1852)	Acc@1 99.365 (99.298)	Acc@5 100.000 (100.000)
Epoch: [161][8/25]	Time 0.575 (0.569)	Data 0.005 (0.079)	Loss 0.1821 (0.1849)	Acc@1 99.316 (99.300)	Acc@5 100.000 (100.000)
Epoch: [161][9/25]	Time 0.561 (0.569)	Data 0.005 (0.072)	Loss 0.1871 (0.1851)	Acc@1 99.170 (99.287)	Acc@5 100.000 (100.000)
Epoch: [161][10/25]	Time 0.597 (0.571)	Data 0.006 (0.066)	Loss 0.1804 (0.1847)	Acc@1 99.561 (99.312)	Acc@5 100.000 (100.000)
Epoch: [161][11/25]	Time 0.562 (0.570)	Data 0.006 (0.061)	Loss 0.1830 (0.1845)	Acc@1 99.316 (99.312)	Acc@5 100.000 (100.000)
Epoch: [161][12/25]	Time 0.600 (0.573)	Data 0.006 (0.056)	Loss 0.1817 (0.1843)	Acc@1 99.463 (99.324)	Acc@5 99.951 (99.996)
Epoch: [161][13/25]	Time 0.578 (0.573)	Data 0.005 (0.053)	Loss 0.1814 (0.1841)	Acc@1 99.561 (99.341)	Acc@5 100.000 (99.997)
Epoch: [161][14/25]	Time 0.579 (0.573)	Data 0.004 (0.049)	Loss 0.1809 (0.1839)	Acc@1 99.414 (99.346)	Acc@5 100.000 (99.997)
Epoch: [161][15/25]	Time 0.566 (0.573)	Data 0.005 (0.047)	Loss 0.1870 (0.1841)	Acc@1 99.365 (99.347)	Acc@5 100.000 (99.997)
Epoch: [161][16/25]	Time 0.590 (0.574)	Data 0.005 (0.044)	Loss 0.1863 (0.1842)	Acc@1 99.316 (99.345)	Acc@5 100.000 (99.997)
Epoch: [161][17/25]	Time 0.588 (0.575)	Data 0.005 (0.042)	Loss 0.1806 (0.1840)	Acc@1 99.512 (99.354)	Acc@5 100.000 (99.997)
Epoch: [161][18/25]	Time 0.598 (0.576)	Data 0.007 (0.040)	Loss 0.1815 (0.1839)	Acc@1 99.463 (99.360)	Acc@5 100.000 (99.997)
Epoch: [161][19/25]	Time 0.573 (0.576)	Data 0.004 (0.038)	Loss 0.1841 (0.1839)	Acc@1 99.414 (99.363)	Acc@5 100.000 (99.998)
Epoch: [161][20/25]	Time 0.567 (0.575)	Data 0.005 (0.037)	Loss 0.1812 (0.1838)	Acc@1 99.365 (99.363)	Acc@5 100.000 (99.998)
Epoch: [161][21/25]	Time 0.572 (0.575)	Data 0.007 (0.035)	Loss 0.1869 (0.1839)	Acc@1 99.170 (99.354)	Acc@5 100.000 (99.998)
Epoch: [161][22/25]	Time 0.547 (0.574)	Data 0.005 (0.034)	Loss 0.1797 (0.1837)	Acc@1 99.707 (99.369)	Acc@5 100.000 (99.998)
Epoch: [161][23/25]	Time 0.591 (0.575)	Data 0.005 (0.033)	Loss 0.1792 (0.1835)	Acc@1 99.463 (99.373)	Acc@5 100.000 (99.998)
Epoch: [161][24/25]	Time 0.337 (0.565)	Data 0.004 (0.032)	Loss 0.1883 (0.1836)	Acc@1 99.175 (99.370)	Acc@5 100.000 (99.998)

Epoch: [162 | 180] LR: 0.000400
Epoch: [162][0/25]	Time 0.553 (0.553)	Data 0.774 (0.774)	Loss 0.1853 (0.1853)	Acc@1 99.365 (99.365)	Acc@5 100.000 (100.000)
Epoch: [162][1/25]	Time 0.596 (0.575)	Data 0.005 (0.389)	Loss 0.1810 (0.1831)	Acc@1 99.512 (99.438)	Acc@5 100.000 (100.000)
Epoch: [162][2/25]	Time 0.556 (0.568)	Data 0.004 (0.261)	Loss 0.1768 (0.1810)	Acc@1 99.707 (99.528)	Acc@5 100.000 (100.000)
Epoch: [162][3/25]	Time 0.579 (0.571)	Data 0.005 (0.197)	Loss 0.1852 (0.1821)	Acc@1 99.268 (99.463)	Acc@5 100.000 (100.000)
Epoch: [162][4/25]	Time 0.560 (0.569)	Data 0.007 (0.159)	Loss 0.1835 (0.1823)	Acc@1 99.512 (99.473)	Acc@5 100.000 (100.000)
Epoch: [162][5/25]	Time 0.603 (0.574)	Data 0.004 (0.133)	Loss 0.1830 (0.1824)	Acc@1 99.365 (99.455)	Acc@5 100.000 (100.000)
Epoch: [162][6/25]	Time 0.577 (0.575)	Data 0.006 (0.115)	Loss 0.1829 (0.1825)	Acc@1 99.414 (99.449)	Acc@5 100.000 (100.000)
Epoch: [162][7/25]	Time 0.558 (0.573)	Data 0.005 (0.101)	Loss 0.1834 (0.1826)	Acc@1 99.219 (99.420)	Acc@5 100.000 (100.000)
Epoch: [162][8/25]	Time 0.570 (0.572)	Data 0.004 (0.090)	Loss 0.1782 (0.1821)	Acc@1 99.756 (99.457)	Acc@5 100.000 (100.000)
Epoch: [162][9/25]	Time 0.582 (0.573)	Data 0.006 (0.082)	Loss 0.1847 (0.1824)	Acc@1 99.268 (99.438)	Acc@5 100.000 (100.000)
Epoch: [162][10/25]	Time 0.554 (0.572)	Data 0.004 (0.075)	Loss 0.1813 (0.1823)	Acc@1 99.561 (99.450)	Acc@5 100.000 (100.000)
Epoch: [162][11/25]	Time 0.592 (0.573)	Data 0.008 (0.069)	Loss 0.1863 (0.1826)	Acc@1 99.170 (99.426)	Acc@5 100.000 (100.000)
Epoch: [162][12/25]	Time 0.532 (0.570)	Data 0.004 (0.064)	Loss 0.1787 (0.1823)	Acc@1 99.414 (99.425)	Acc@5 100.000 (100.000)
Epoch: [162][13/25]	Time 0.561 (0.569)	Data 0.004 (0.060)	Loss 0.1786 (0.1821)	Acc@1 99.609 (99.438)	Acc@5 100.000 (100.000)
Epoch: [162][14/25]	Time 0.571 (0.570)	Data 0.006 (0.056)	Loss 0.1815 (0.1820)	Acc@1 99.609 (99.450)	Acc@5 100.000 (100.000)
Epoch: [162][15/25]	Time 0.561 (0.569)	Data 0.004 (0.053)	Loss 0.1824 (0.1820)	Acc@1 99.561 (99.457)	Acc@5 100.000 (100.000)
Epoch: [162][16/25]	Time 0.578 (0.570)	Data 0.009 (0.051)	Loss 0.1855 (0.1822)	Acc@1 99.316 (99.449)	Acc@5 100.000 (100.000)
Epoch: [162][17/25]	Time 0.584 (0.570)	Data 0.005 (0.048)	Loss 0.1808 (0.1822)	Acc@1 99.414 (99.447)	Acc@5 100.000 (100.000)
Epoch: [162][18/25]	Time 0.598 (0.572)	Data 0.007 (0.046)	Loss 0.1840 (0.1823)	Acc@1 99.561 (99.453)	Acc@5 100.000 (100.000)
Epoch: [162][19/25]	Time 0.560 (0.571)	Data 0.006 (0.044)	Loss 0.1836 (0.1823)	Acc@1 99.463 (99.453)	Acc@5 100.000 (100.000)
Epoch: [162][20/25]	Time 0.600 (0.573)	Data 0.007 (0.042)	Loss 0.1866 (0.1825)	Acc@1 99.365 (99.449)	Acc@5 100.000 (100.000)
Epoch: [162][21/25]	Time 0.599 (0.574)	Data 0.004 (0.040)	Loss 0.1833 (0.1826)	Acc@1 99.365 (99.445)	Acc@5 100.000 (100.000)
Epoch: [162][22/25]	Time 0.616 (0.576)	Data 0.008 (0.039)	Loss 0.1804 (0.1825)	Acc@1 99.512 (99.448)	Acc@5 100.000 (100.000)
Epoch: [162][23/25]	Time 0.586 (0.576)	Data 0.007 (0.038)	Loss 0.1852 (0.1826)	Acc@1 99.268 (99.441)	Acc@5 100.000 (100.000)
Epoch: [162][24/25]	Time 0.371 (0.568)	Data 0.005 (0.036)	Loss 0.1887 (0.1827)	Acc@1 99.057 (99.434)	Acc@5 100.000 (100.000)

Epoch: [163 | 180] LR: 0.000400
Epoch: [163][0/25]	Time 0.577 (0.577)	Data 0.681 (0.681)	Loss 0.1876 (0.1876)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [163][1/25]	Time 0.597 (0.587)	Data 0.005 (0.343)	Loss 0.1867 (0.1871)	Acc@1 99.316 (99.268)	Acc@5 100.000 (100.000)
Epoch: [163][2/25]	Time 0.565 (0.580)	Data 0.005 (0.230)	Loss 0.1840 (0.1861)	Acc@1 99.463 (99.333)	Acc@5 100.000 (100.000)
Epoch: [163][3/25]	Time 0.589 (0.582)	Data 0.004 (0.174)	Loss 0.1853 (0.1859)	Acc@1 99.316 (99.329)	Acc@5 100.000 (100.000)
Epoch: [163][4/25]	Time 0.574 (0.581)	Data 0.007 (0.140)	Loss 0.1838 (0.1855)	Acc@1 99.463 (99.355)	Acc@5 100.000 (100.000)
Epoch: [163][5/25]	Time 0.580 (0.581)	Data 0.003 (0.118)	Loss 0.1830 (0.1851)	Acc@1 99.414 (99.365)	Acc@5 100.000 (100.000)
Epoch: [163][6/25]	Time 0.544 (0.575)	Data 0.005 (0.102)	Loss 0.1839 (0.1849)	Acc@1 99.512 (99.386)	Acc@5 100.000 (100.000)
Epoch: [163][7/25]	Time 0.610 (0.580)	Data 0.004 (0.089)	Loss 0.1848 (0.1849)	Acc@1 99.316 (99.377)	Acc@5 100.000 (100.000)
Epoch: [163][8/25]	Time 0.570 (0.579)	Data 0.005 (0.080)	Loss 0.1812 (0.1845)	Acc@1 99.658 (99.409)	Acc@5 100.000 (100.000)
Epoch: [163][9/25]	Time 0.521 (0.573)	Data 0.005 (0.072)	Loss 0.1851 (0.1845)	Acc@1 99.365 (99.404)	Acc@5 100.000 (100.000)
Epoch: [163][10/25]	Time 0.602 (0.576)	Data 0.005 (0.066)	Loss 0.1831 (0.1844)	Acc@1 99.414 (99.405)	Acc@5 99.951 (99.996)
Epoch: [163][11/25]	Time 0.593 (0.577)	Data 0.005 (0.061)	Loss 0.1864 (0.1846)	Acc@1 99.170 (99.386)	Acc@5 100.000 (99.996)
Epoch: [163][12/25]	Time 0.592 (0.578)	Data 0.005 (0.057)	Loss 0.1861 (0.1847)	Acc@1 99.316 (99.380)	Acc@5 100.000 (99.996)
Epoch: [163][13/25]	Time 0.571 (0.578)	Data 0.005 (0.053)	Loss 0.1855 (0.1847)	Acc@1 99.268 (99.372)	Acc@5 100.000 (99.997)
Epoch: [163][14/25]	Time 0.614 (0.580)	Data 0.004 (0.050)	Loss 0.1779 (0.1843)	Acc@1 99.805 (99.401)	Acc@5 100.000 (99.997)
Epoch: [163][15/25]	Time 0.566 (0.579)	Data 0.007 (0.047)	Loss 0.1821 (0.1842)	Acc@1 99.658 (99.417)	Acc@5 100.000 (99.997)
Epoch: [163][16/25]	Time 0.567 (0.578)	Data 0.007 (0.045)	Loss 0.1845 (0.1842)	Acc@1 99.365 (99.414)	Acc@5 100.000 (99.997)
Epoch: [163][17/25]	Time 0.576 (0.578)	Data 0.005 (0.043)	Loss 0.1811 (0.1840)	Acc@1 99.512 (99.419)	Acc@5 100.000 (99.997)
Epoch: [163][18/25]	Time 0.582 (0.579)	Data 0.005 (0.041)	Loss 0.1835 (0.1840)	Acc@1 99.463 (99.422)	Acc@5 100.000 (99.997)
Epoch: [163][19/25]	Time 0.606 (0.580)	Data 0.003 (0.039)	Loss 0.1811 (0.1838)	Acc@1 99.414 (99.421)	Acc@5 100.000 (99.998)
Epoch: [163][20/25]	Time 0.608 (0.581)	Data 0.004 (0.037)	Loss 0.1825 (0.1838)	Acc@1 99.316 (99.416)	Acc@5 100.000 (99.998)
Epoch: [163][21/25]	Time 0.572 (0.581)	Data 0.005 (0.036)	Loss 0.1851 (0.1838)	Acc@1 99.463 (99.419)	Acc@5 100.000 (99.998)
Epoch: [163][22/25]	Time 0.576 (0.581)	Data 0.005 (0.034)	Loss 0.1811 (0.1837)	Acc@1 99.512 (99.423)	Acc@5 100.000 (99.998)
Epoch: [163][23/25]	Time 0.594 (0.581)	Data 0.006 (0.033)	Loss 0.1833 (0.1837)	Acc@1 99.316 (99.418)	Acc@5 100.000 (99.998)
Epoch: [163][24/25]	Time 0.337 (0.571)	Data 0.005 (0.032)	Loss 0.1858 (0.1837)	Acc@1 98.939 (99.410)	Acc@5 100.000 (99.998)

Epoch: [164 | 180] LR: 0.000400
Epoch: [164][0/25]	Time 0.545 (0.545)	Data 0.599 (0.599)	Loss 0.1830 (0.1830)	Acc@1 99.365 (99.365)	Acc@5 100.000 (100.000)
Epoch: [164][1/25]	Time 0.595 (0.570)	Data 0.007 (0.303)	Loss 0.1837 (0.1834)	Acc@1 99.316 (99.341)	Acc@5 100.000 (100.000)
Epoch: [164][2/25]	Time 0.585 (0.575)	Data 0.005 (0.204)	Loss 0.1816 (0.1828)	Acc@1 99.512 (99.398)	Acc@5 100.000 (100.000)
Epoch: [164][3/25]	Time 0.596 (0.580)	Data 0.006 (0.154)	Loss 0.1788 (0.1818)	Acc@1 99.463 (99.414)	Acc@5 100.000 (100.000)
Epoch: [164][4/25]	Time 0.579 (0.580)	Data 0.006 (0.125)	Loss 0.1855 (0.1825)	Acc@1 99.170 (99.365)	Acc@5 100.000 (100.000)
Epoch: [164][5/25]	Time 0.612 (0.585)	Data 0.006 (0.105)	Loss 0.1790 (0.1819)	Acc@1 99.561 (99.398)	Acc@5 100.000 (100.000)
Epoch: [164][6/25]	Time 0.565 (0.583)	Data 0.005 (0.091)	Loss 0.1808 (0.1818)	Acc@1 99.512 (99.414)	Acc@5 100.000 (100.000)
Epoch: [164][7/25]	Time 0.573 (0.581)	Data 0.003 (0.080)	Loss 0.1855 (0.1822)	Acc@1 99.561 (99.432)	Acc@5 99.951 (99.994)
Epoch: [164][8/25]	Time 0.565 (0.580)	Data 0.005 (0.071)	Loss 0.1786 (0.1818)	Acc@1 99.561 (99.447)	Acc@5 100.000 (99.995)
Epoch: [164][9/25]	Time 0.574 (0.579)	Data 0.008 (0.065)	Loss 0.1837 (0.1820)	Acc@1 99.414 (99.443)	Acc@5 100.000 (99.995)
Epoch: [164][10/25]	Time 0.584 (0.579)	Data 0.006 (0.060)	Loss 0.1798 (0.1818)	Acc@1 99.512 (99.450)	Acc@5 100.000 (99.996)
Epoch: [164][11/25]	Time 0.572 (0.579)	Data 0.005 (0.055)	Loss 0.1815 (0.1818)	Acc@1 99.609 (99.463)	Acc@5 100.000 (99.996)
Epoch: [164][12/25]	Time 0.592 (0.580)	Data 0.006 (0.051)	Loss 0.1825 (0.1819)	Acc@1 99.316 (99.452)	Acc@5 100.000 (99.996)
Epoch: [164][13/25]	Time 0.564 (0.579)	Data 0.005 (0.048)	Loss 0.1812 (0.1818)	Acc@1 99.463 (99.452)	Acc@5 100.000 (99.997)
Epoch: [164][14/25]	Time 0.576 (0.578)	Data 0.006 (0.045)	Loss 0.1863 (0.1821)	Acc@1 99.365 (99.447)	Acc@5 100.000 (99.997)
Epoch: [164][15/25]	Time 0.570 (0.578)	Data 0.005 (0.043)	Loss 0.1833 (0.1822)	Acc@1 99.512 (99.451)	Acc@5 100.000 (99.997)
Epoch: [164][16/25]	Time 0.587 (0.578)	Data 0.006 (0.040)	Loss 0.1831 (0.1822)	Acc@1 99.316 (99.443)	Acc@5 100.000 (99.997)
Epoch: [164][17/25]	Time 0.583 (0.579)	Data 0.005 (0.038)	Loss 0.1802 (0.1821)	Acc@1 99.463 (99.444)	Acc@5 100.000 (99.997)
Epoch: [164][18/25]	Time 0.583 (0.579)	Data 0.004 (0.037)	Loss 0.1799 (0.1820)	Acc@1 99.512 (99.447)	Acc@5 100.000 (99.997)
Epoch: [164][19/25]	Time 0.584 (0.579)	Data 0.008 (0.035)	Loss 0.1797 (0.1819)	Acc@1 99.756 (99.463)	Acc@5 100.000 (99.998)
Epoch: [164][20/25]	Time 0.561 (0.578)	Data 0.005 (0.034)	Loss 0.1848 (0.1820)	Acc@1 99.463 (99.463)	Acc@5 100.000 (99.998)
Epoch: [164][21/25]	Time 0.591 (0.579)	Data 0.007 (0.033)	Loss 0.1814 (0.1820)	Acc@1 99.561 (99.467)	Acc@5 100.000 (99.998)
Epoch: [164][22/25]	Time 0.551 (0.578)	Data 0.005 (0.031)	Loss 0.1838 (0.1821)	Acc@1 99.561 (99.471)	Acc@5 100.000 (99.998)
Epoch: [164][23/25]	Time 0.588 (0.578)	Data 0.006 (0.030)	Loss 0.1862 (0.1822)	Acc@1 99.219 (99.461)	Acc@5 100.000 (99.998)
Epoch: [164][24/25]	Time 0.307 (0.567)	Data 0.004 (0.029)	Loss 0.1799 (0.1822)	Acc@1 99.528 (99.462)	Acc@5 100.000 (99.998)

Epoch: [165 | 180] LR: 0.000400
Epoch: [165][0/25]	Time 0.525 (0.525)	Data 0.786 (0.786)	Loss 0.1784 (0.1784)	Acc@1 99.658 (99.658)	Acc@5 100.000 (100.000)
Epoch: [165][1/25]	Time 0.541 (0.533)	Data 0.005 (0.396)	Loss 0.1832 (0.1808)	Acc@1 99.463 (99.561)	Acc@5 100.000 (100.000)
Epoch: [165][2/25]	Time 0.586 (0.551)	Data 0.004 (0.265)	Loss 0.1847 (0.1821)	Acc@1 99.365 (99.495)	Acc@5 100.000 (100.000)
Epoch: [165][3/25]	Time 0.546 (0.550)	Data 0.003 (0.200)	Loss 0.1891 (0.1838)	Acc@1 99.072 (99.390)	Acc@5 99.951 (99.988)
Epoch: [165][4/25]	Time 0.602 (0.560)	Data 0.009 (0.162)	Loss 0.1813 (0.1833)	Acc@1 99.414 (99.395)	Acc@5 100.000 (99.990)
Epoch: [165][5/25]	Time 0.571 (0.562)	Data 0.004 (0.135)	Loss 0.1800 (0.1828)	Acc@1 99.658 (99.438)	Acc@5 100.000 (99.992)
Epoch: [165][6/25]	Time 0.583 (0.565)	Data 0.003 (0.116)	Loss 0.1866 (0.1833)	Acc@1 99.316 (99.421)	Acc@5 100.000 (99.993)
Epoch: [165][7/25]	Time 0.572 (0.566)	Data 0.006 (0.103)	Loss 0.1873 (0.1838)	Acc@1 99.268 (99.402)	Acc@5 100.000 (99.994)
Epoch: [165][8/25]	Time 0.588 (0.568)	Data 0.005 (0.092)	Loss 0.1836 (0.1838)	Acc@1 99.268 (99.387)	Acc@5 100.000 (99.995)
Epoch: [165][9/25]	Time 0.547 (0.566)	Data 0.006 (0.083)	Loss 0.1848 (0.1839)	Acc@1 99.512 (99.399)	Acc@5 100.000 (99.995)
Epoch: [165][10/25]	Time 0.592 (0.568)	Data 0.005 (0.076)	Loss 0.1791 (0.1835)	Acc@1 99.365 (99.396)	Acc@5 100.000 (99.996)
Epoch: [165][11/25]	Time 0.613 (0.572)	Data 0.005 (0.070)	Loss 0.1839 (0.1835)	Acc@1 99.512 (99.406)	Acc@5 100.000 (99.996)
Epoch: [165][12/25]	Time 0.606 (0.575)	Data 0.007 (0.065)	Loss 0.1810 (0.1833)	Acc@1 99.316 (99.399)	Acc@5 100.000 (99.996)
Epoch: [165][13/25]	Time 0.531 (0.572)	Data 0.004 (0.061)	Loss 0.1790 (0.1830)	Acc@1 99.512 (99.407)	Acc@5 100.000 (99.997)
Epoch: [165][14/25]	Time 0.590 (0.573)	Data 0.004 (0.057)	Loss 0.1760 (0.1825)	Acc@1 99.707 (99.427)	Acc@5 100.000 (99.997)
Epoch: [165][15/25]	Time 0.546 (0.571)	Data 0.004 (0.054)	Loss 0.1755 (0.1821)	Acc@1 99.658 (99.442)	Acc@5 100.000 (99.997)
Epoch: [165][16/25]	Time 0.569 (0.571)	Data 0.007 (0.051)	Loss 0.1810 (0.1820)	Acc@1 99.561 (99.449)	Acc@5 100.000 (99.997)
Epoch: [165][17/25]	Time 0.597 (0.573)	Data 0.006 (0.049)	Loss 0.1801 (0.1819)	Acc@1 99.561 (99.455)	Acc@5 100.000 (99.997)
Epoch: [165][18/25]	Time 0.581 (0.573)	Data 0.005 (0.046)	Loss 0.1792 (0.1818)	Acc@1 99.414 (99.453)	Acc@5 100.000 (99.997)
Epoch: [165][19/25]	Time 0.569 (0.573)	Data 0.007 (0.044)	Loss 0.1769 (0.1815)	Acc@1 99.707 (99.465)	Acc@5 100.000 (99.998)
Epoch: [165][20/25]	Time 0.597 (0.574)	Data 0.007 (0.043)	Loss 0.1859 (0.1817)	Acc@1 99.121 (99.449)	Acc@5 100.000 (99.998)
Epoch: [165][21/25]	Time 0.575 (0.574)	Data 0.007 (0.041)	Loss 0.1823 (0.1818)	Acc@1 99.170 (99.436)	Acc@5 100.000 (99.998)
Epoch: [165][22/25]	Time 0.577 (0.574)	Data 0.006 (0.040)	Loss 0.1798 (0.1817)	Acc@1 99.561 (99.442)	Acc@5 100.000 (99.998)
Epoch: [165][23/25]	Time 0.602 (0.575)	Data 0.007 (0.038)	Loss 0.1844 (0.1818)	Acc@1 99.170 (99.430)	Acc@5 100.000 (99.998)
Epoch: [165][24/25]	Time 0.335 (0.566)	Data 0.006 (0.037)	Loss 0.1857 (0.1819)	Acc@1 99.175 (99.426)	Acc@5 100.000 (99.998)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [166 | 180] LR: 0.000400
Epoch: [166][0/25]	Time 0.586 (0.586)	Data 0.681 (0.681)	Loss 0.1787 (0.1787)	Acc@1 99.707 (99.707)	Acc@5 100.000 (100.000)
Epoch: [166][1/25]	Time 0.562 (0.574)	Data 0.006 (0.343)	Loss 0.1783 (0.1785)	Acc@1 99.707 (99.707)	Acc@5 100.000 (100.000)
Epoch: [166][2/25]	Time 0.538 (0.562)	Data 0.006 (0.231)	Loss 0.1815 (0.1795)	Acc@1 99.561 (99.658)	Acc@5 100.000 (100.000)
Epoch: [166][3/25]	Time 0.569 (0.564)	Data 0.003 (0.174)	Loss 0.1802 (0.1797)	Acc@1 99.512 (99.622)	Acc@5 100.000 (100.000)
Epoch: [166][4/25]	Time 0.559 (0.563)	Data 0.007 (0.141)	Loss 0.1847 (0.1807)	Acc@1 99.316 (99.561)	Acc@5 100.000 (100.000)
Epoch: [166][5/25]	Time 0.576 (0.565)	Data 0.006 (0.118)	Loss 0.1782 (0.1803)	Acc@1 99.609 (99.569)	Acc@5 100.000 (100.000)
Epoch: [166][6/25]	Time 0.542 (0.562)	Data 0.005 (0.102)	Loss 0.1816 (0.1804)	Acc@1 99.268 (99.526)	Acc@5 100.000 (100.000)
Epoch: [166][7/25]	Time 0.590 (0.565)	Data 0.006 (0.090)	Loss 0.1780 (0.1801)	Acc@1 99.463 (99.518)	Acc@5 100.000 (100.000)
Epoch: [166][8/25]	Time 0.527 (0.561)	Data 0.004 (0.080)	Loss 0.1853 (0.1807)	Acc@1 99.414 (99.506)	Acc@5 100.000 (100.000)
Epoch: [166][9/25]	Time 0.576 (0.563)	Data 0.006 (0.073)	Loss 0.1836 (0.1810)	Acc@1 99.463 (99.502)	Acc@5 100.000 (100.000)
Epoch: [166][10/25]	Time 0.582 (0.564)	Data 0.005 (0.067)	Loss 0.1775 (0.1807)	Acc@1 99.658 (99.516)	Acc@5 100.000 (100.000)
Epoch: [166][11/25]	Time 0.570 (0.565)	Data 0.005 (0.062)	Loss 0.1804 (0.1807)	Acc@1 99.316 (99.500)	Acc@5 100.000 (100.000)
Epoch: [166][12/25]	Time 0.569 (0.565)	Data 0.007 (0.057)	Loss 0.1767 (0.1804)	Acc@1 99.756 (99.519)	Acc@5 100.000 (100.000)
Epoch: [166][13/25]	Time 0.582 (0.566)	Data 0.007 (0.054)	Loss 0.1814 (0.1804)	Acc@1 99.463 (99.515)	Acc@5 100.000 (100.000)
Epoch: [166][14/25]	Time 0.586 (0.568)	Data 0.008 (0.051)	Loss 0.1752 (0.1801)	Acc@1 99.658 (99.525)	Acc@5 100.000 (100.000)
Epoch: [166][15/25]	Time 0.576 (0.568)	Data 0.008 (0.048)	Loss 0.1755 (0.1798)	Acc@1 99.658 (99.533)	Acc@5 100.000 (100.000)
Epoch: [166][16/25]	Time 0.572 (0.568)	Data 0.007 (0.046)	Loss 0.1804 (0.1798)	Acc@1 99.463 (99.529)	Acc@5 100.000 (100.000)
Epoch: [166][17/25]	Time 0.572 (0.569)	Data 0.006 (0.043)	Loss 0.1799 (0.1798)	Acc@1 99.658 (99.536)	Acc@5 100.000 (100.000)
Epoch: [166][18/25]	Time 0.579 (0.569)	Data 0.005 (0.041)	Loss 0.1880 (0.1803)	Acc@1 99.268 (99.522)	Acc@5 100.000 (100.000)
Epoch: [166][19/25]	Time 0.548 (0.568)	Data 0.005 (0.039)	Loss 0.1833 (0.1804)	Acc@1 99.512 (99.521)	Acc@5 100.000 (100.000)
Epoch: [166][20/25]	Time 0.576 (0.568)	Data 0.007 (0.038)	Loss 0.1803 (0.1804)	Acc@1 99.414 (99.516)	Acc@5 100.000 (100.000)
Epoch: [166][21/25]	Time 0.568 (0.568)	Data 0.004 (0.036)	Loss 0.1770 (0.1803)	Acc@1 99.707 (99.525)	Acc@5 100.000 (100.000)
Epoch: [166][22/25]	Time 0.576 (0.569)	Data 0.007 (0.035)	Loss 0.1793 (0.1802)	Acc@1 99.707 (99.533)	Acc@5 100.000 (100.000)
Epoch: [166][23/25]	Time 0.579 (0.569)	Data 0.004 (0.034)	Loss 0.1824 (0.1803)	Acc@1 99.561 (99.534)	Acc@5 100.000 (100.000)
Epoch: [166][24/25]	Time 0.349 (0.560)	Data 0.009 (0.033)	Loss 0.1819 (0.1803)	Acc@1 99.410 (99.532)	Acc@5 100.000 (100.000)

Epoch: [167 | 180] LR: 0.000400
Epoch: [167][0/25]	Time 0.593 (0.593)	Data 0.663 (0.663)	Loss 0.1792 (0.1792)	Acc@1 99.365 (99.365)	Acc@5 100.000 (100.000)
Epoch: [167][1/25]	Time 0.545 (0.569)	Data 0.007 (0.335)	Loss 0.1758 (0.1775)	Acc@1 99.658 (99.512)	Acc@5 100.000 (100.000)
Epoch: [167][2/25]	Time 0.607 (0.581)	Data 0.003 (0.224)	Loss 0.1792 (0.1781)	Acc@1 99.658 (99.561)	Acc@5 99.951 (99.984)
Epoch: [167][3/25]	Time 0.544 (0.572)	Data 0.004 (0.169)	Loss 0.1780 (0.1780)	Acc@1 99.658 (99.585)	Acc@5 100.000 (99.988)
Epoch: [167][4/25]	Time 0.605 (0.579)	Data 0.008 (0.137)	Loss 0.1809 (0.1786)	Acc@1 99.512 (99.570)	Acc@5 100.000 (99.990)
Epoch: [167][5/25]	Time 0.512 (0.567)	Data 0.006 (0.115)	Loss 0.1783 (0.1786)	Acc@1 99.561 (99.569)	Acc@5 100.000 (99.992)
Epoch: [167][6/25]	Time 0.647 (0.579)	Data 0.003 (0.099)	Loss 0.1810 (0.1789)	Acc@1 99.609 (99.574)	Acc@5 100.000 (99.993)
Epoch: [167][7/25]	Time 0.664 (0.589)	Data 0.005 (0.087)	Loss 0.1774 (0.1787)	Acc@1 99.561 (99.573)	Acc@5 100.000 (99.994)
Epoch: [167][8/25]	Time 0.588 (0.589)	Data 0.005 (0.078)	Loss 0.1799 (0.1788)	Acc@1 99.512 (99.566)	Acc@5 100.000 (99.995)
Epoch: [167][9/25]	Time 0.584 (0.589)	Data 0.006 (0.071)	Loss 0.1835 (0.1793)	Acc@1 99.463 (99.556)	Acc@5 100.000 (99.995)
Epoch: [167][10/25]	Time 0.578 (0.588)	Data 0.006 (0.065)	Loss 0.1829 (0.1796)	Acc@1 99.414 (99.543)	Acc@5 100.000 (99.996)
Epoch: [167][11/25]	Time 0.553 (0.585)	Data 0.007 (0.060)	Loss 0.1822 (0.1798)	Acc@1 99.463 (99.536)	Acc@5 100.000 (99.996)
Epoch: [167][12/25]	Time 0.573 (0.584)	Data 0.007 (0.056)	Loss 0.1778 (0.1797)	Acc@1 99.609 (99.542)	Acc@5 100.000 (99.996)
Epoch: [167][13/25]	Time 0.575 (0.583)	Data 0.006 (0.053)	Loss 0.1838 (0.1800)	Acc@1 99.316 (99.526)	Acc@5 100.000 (99.997)
Epoch: [167][14/25]	Time 0.582 (0.583)	Data 0.006 (0.049)	Loss 0.1804 (0.1800)	Acc@1 99.707 (99.538)	Acc@5 100.000 (99.997)
Epoch: [167][15/25]	Time 0.567 (0.582)	Data 0.004 (0.047)	Loss 0.1829 (0.1802)	Acc@1 99.316 (99.524)	Acc@5 100.000 (99.997)
Epoch: [167][16/25]	Time 0.564 (0.581)	Data 0.009 (0.044)	Loss 0.1876 (0.1806)	Acc@1 98.975 (99.492)	Acc@5 100.000 (99.997)
Epoch: [167][17/25]	Time 0.574 (0.581)	Data 0.007 (0.042)	Loss 0.1829 (0.1808)	Acc@1 99.316 (99.482)	Acc@5 100.000 (99.997)
Epoch: [167][18/25]	Time 0.563 (0.580)	Data 0.005 (0.040)	Loss 0.1819 (0.1808)	Acc@1 99.414 (99.478)	Acc@5 100.000 (99.997)
Epoch: [167][19/25]	Time 0.606 (0.581)	Data 0.004 (0.038)	Loss 0.1800 (0.1808)	Acc@1 99.316 (99.470)	Acc@5 100.000 (99.998)
Epoch: [167][20/25]	Time 0.587 (0.581)	Data 0.007 (0.037)	Loss 0.1789 (0.1807)	Acc@1 99.609 (99.477)	Acc@5 100.000 (99.998)
Epoch: [167][21/25]	Time 0.582 (0.581)	Data 0.005 (0.036)	Loss 0.1804 (0.1807)	Acc@1 99.512 (99.478)	Acc@5 100.000 (99.998)
Epoch: [167][22/25]	Time 0.567 (0.581)	Data 0.004 (0.034)	Loss 0.1880 (0.1810)	Acc@1 99.121 (99.463)	Acc@5 100.000 (99.998)
Epoch: [167][23/25]	Time 0.579 (0.581)	Data 0.007 (0.033)	Loss 0.1861 (0.1812)	Acc@1 99.219 (99.453)	Acc@5 100.000 (99.998)
Epoch: [167][24/25]	Time 0.309 (0.570)	Data 0.004 (0.032)	Loss 0.1860 (0.1813)	Acc@1 99.057 (99.446)	Acc@5 100.000 (99.998)

Epoch: [168 | 180] LR: 0.000400
Epoch: [168][0/25]	Time 0.590 (0.590)	Data 0.618 (0.618)	Loss 0.1862 (0.1862)	Acc@1 99.414 (99.414)	Acc@5 100.000 (100.000)
Epoch: [168][1/25]	Time 0.538 (0.564)	Data 0.006 (0.312)	Loss 0.1795 (0.1828)	Acc@1 99.512 (99.463)	Acc@5 100.000 (100.000)
Epoch: [168][2/25]	Time 0.577 (0.568)	Data 0.006 (0.210)	Loss 0.1762 (0.1806)	Acc@1 99.561 (99.495)	Acc@5 100.000 (100.000)
Epoch: [168][3/25]	Time 0.583 (0.572)	Data 0.007 (0.159)	Loss 0.1851 (0.1818)	Acc@1 99.463 (99.487)	Acc@5 100.000 (100.000)
Epoch: [168][4/25]	Time 0.593 (0.576)	Data 0.007 (0.129)	Loss 0.1752 (0.1804)	Acc@1 99.854 (99.561)	Acc@5 100.000 (100.000)
Epoch: [168][5/25]	Time 0.547 (0.571)	Data 0.003 (0.108)	Loss 0.1750 (0.1795)	Acc@1 99.756 (99.593)	Acc@5 100.000 (100.000)
Epoch: [168][6/25]	Time 0.590 (0.574)	Data 0.003 (0.093)	Loss 0.1808 (0.1797)	Acc@1 99.609 (99.595)	Acc@5 100.000 (100.000)
Epoch: [168][7/25]	Time 0.564 (0.573)	Data 0.005 (0.082)	Loss 0.1799 (0.1797)	Acc@1 99.707 (99.609)	Acc@5 100.000 (100.000)
Epoch: [168][8/25]	Time 0.574 (0.573)	Data 0.006 (0.073)	Loss 0.1843 (0.1802)	Acc@1 99.561 (99.604)	Acc@5 100.000 (100.000)
Epoch: [168][9/25]	Time 0.559 (0.572)	Data 0.003 (0.066)	Loss 0.1764 (0.1799)	Acc@1 99.707 (99.614)	Acc@5 100.000 (100.000)
Epoch: [168][10/25]	Time 0.574 (0.572)	Data 0.004 (0.061)	Loss 0.1854 (0.1804)	Acc@1 99.072 (99.565)	Acc@5 100.000 (100.000)
Epoch: [168][11/25]	Time 0.536 (0.569)	Data 0.006 (0.056)	Loss 0.1797 (0.1803)	Acc@1 99.316 (99.544)	Acc@5 100.000 (100.000)
Epoch: [168][12/25]	Time 0.580 (0.570)	Data 0.005 (0.052)	Loss 0.1765 (0.1800)	Acc@1 99.658 (99.553)	Acc@5 100.000 (100.000)
Epoch: [168][13/25]	Time 0.589 (0.571)	Data 0.004 (0.049)	Loss 0.1843 (0.1803)	Acc@1 99.219 (99.529)	Acc@5 100.000 (100.000)
Epoch: [168][14/25]	Time 0.592 (0.572)	Data 0.006 (0.046)	Loss 0.1813 (0.1804)	Acc@1 99.512 (99.528)	Acc@5 100.000 (100.000)
Epoch: [168][15/25]	Time 0.566 (0.572)	Data 0.006 (0.044)	Loss 0.1816 (0.1805)	Acc@1 99.463 (99.524)	Acc@5 100.000 (100.000)
Epoch: [168][16/25]	Time 0.585 (0.573)	Data 0.010 (0.042)	Loss 0.1820 (0.1805)	Acc@1 99.365 (99.515)	Acc@5 100.000 (100.000)
Epoch: [168][17/25]	Time 0.581 (0.573)	Data 0.007 (0.040)	Loss 0.1761 (0.1803)	Acc@1 99.658 (99.523)	Acc@5 100.000 (100.000)
Epoch: [168][18/25]	Time 0.610 (0.575)	Data 0.004 (0.038)	Loss 0.1803 (0.1803)	Acc@1 99.512 (99.522)	Acc@5 100.000 (100.000)
Epoch: [168][19/25]	Time 0.613 (0.577)	Data 0.005 (0.036)	Loss 0.1856 (0.1806)	Acc@1 99.268 (99.509)	Acc@5 100.000 (100.000)
Epoch: [168][20/25]	Time 0.583 (0.577)	Data 0.004 (0.035)	Loss 0.1803 (0.1806)	Acc@1 99.609 (99.514)	Acc@5 100.000 (100.000)
Epoch: [168][21/25]	Time 0.592 (0.578)	Data 0.007 (0.033)	Loss 0.1822 (0.1806)	Acc@1 99.121 (99.496)	Acc@5 100.000 (100.000)
Epoch: [168][22/25]	Time 0.572 (0.578)	Data 0.004 (0.032)	Loss 0.1860 (0.1809)	Acc@1 99.219 (99.484)	Acc@5 100.000 (100.000)
Epoch: [168][23/25]	Time 0.546 (0.576)	Data 0.004 (0.031)	Loss 0.1813 (0.1809)	Acc@1 99.463 (99.483)	Acc@5 100.000 (100.000)
Epoch: [168][24/25]	Time 0.333 (0.567)	Data 0.008 (0.030)	Loss 0.1826 (0.1809)	Acc@1 99.175 (99.478)	Acc@5 100.000 (100.000)

Epoch: [169 | 180] LR: 0.000400
Epoch: [169][0/25]	Time 0.583 (0.583)	Data 0.802 (0.802)	Loss 0.1795 (0.1795)	Acc@1 99.365 (99.365)	Acc@5 100.000 (100.000)
Epoch: [169][1/25]	Time 0.561 (0.572)	Data 0.005 (0.403)	Loss 0.1782 (0.1789)	Acc@1 99.561 (99.463)	Acc@5 100.000 (100.000)
Epoch: [169][2/25]	Time 0.596 (0.580)	Data 0.006 (0.271)	Loss 0.1750 (0.1776)	Acc@1 99.756 (99.561)	Acc@5 100.000 (100.000)
Epoch: [169][3/25]	Time 0.568 (0.577)	Data 0.005 (0.204)	Loss 0.1822 (0.1788)	Acc@1 99.414 (99.524)	Acc@5 100.000 (100.000)
Epoch: [169][4/25]	Time 0.578 (0.577)	Data 0.006 (0.165)	Loss 0.1806 (0.1791)	Acc@1 99.512 (99.521)	Acc@5 100.000 (100.000)
Epoch: [169][5/25]	Time 0.575 (0.577)	Data 0.007 (0.138)	Loss 0.1795 (0.1792)	Acc@1 99.561 (99.528)	Acc@5 100.000 (100.000)
Epoch: [169][6/25]	Time 0.529 (0.570)	Data 0.004 (0.119)	Loss 0.1813 (0.1795)	Acc@1 99.365 (99.505)	Acc@5 100.000 (100.000)
Epoch: [169][7/25]	Time 0.559 (0.569)	Data 0.005 (0.105)	Loss 0.1808 (0.1797)	Acc@1 99.609 (99.518)	Acc@5 100.000 (100.000)
Epoch: [169][8/25]	Time 0.603 (0.573)	Data 0.004 (0.094)	Loss 0.1782 (0.1795)	Acc@1 99.609 (99.528)	Acc@5 100.000 (100.000)
Epoch: [169][9/25]	Time 0.532 (0.569)	Data 0.004 (0.085)	Loss 0.1807 (0.1796)	Acc@1 99.463 (99.521)	Acc@5 100.000 (100.000)
Epoch: [169][10/25]	Time 0.603 (0.572)	Data 0.005 (0.077)	Loss 0.1759 (0.1793)	Acc@1 99.707 (99.538)	Acc@5 100.000 (100.000)
Epoch: [169][11/25]	Time 0.603 (0.574)	Data 0.006 (0.071)	Loss 0.1777 (0.1791)	Acc@1 99.658 (99.548)	Acc@5 100.000 (100.000)
Epoch: [169][12/25]	Time 0.583 (0.575)	Data 0.006 (0.066)	Loss 0.1785 (0.1791)	Acc@1 99.609 (99.553)	Acc@5 100.000 (100.000)
Epoch: [169][13/25]	Time 0.606 (0.577)	Data 0.006 (0.062)	Loss 0.1835 (0.1794)	Acc@1 99.463 (99.547)	Acc@5 100.000 (100.000)
Epoch: [169][14/25]	Time 0.587 (0.578)	Data 0.008 (0.058)	Loss 0.1762 (0.1792)	Acc@1 99.658 (99.554)	Acc@5 100.000 (100.000)
Epoch: [169][15/25]	Time 0.607 (0.580)	Data 0.008 (0.055)	Loss 0.1833 (0.1795)	Acc@1 99.414 (99.545)	Acc@5 100.000 (100.000)
Epoch: [169][16/25]	Time 0.574 (0.579)	Data 0.007 (0.052)	Loss 0.1829 (0.1797)	Acc@1 99.463 (99.540)	Acc@5 100.000 (100.000)
Epoch: [169][17/25]	Time 0.582 (0.579)	Data 0.004 (0.050)	Loss 0.1784 (0.1796)	Acc@1 99.658 (99.547)	Acc@5 100.000 (100.000)
Epoch: [169][18/25]	Time 0.590 (0.580)	Data 0.007 (0.048)	Loss 0.1806 (0.1796)	Acc@1 99.707 (99.555)	Acc@5 100.000 (100.000)
Epoch: [169][19/25]	Time 0.579 (0.580)	Data 0.006 (0.045)	Loss 0.1770 (0.1795)	Acc@1 99.658 (99.561)	Acc@5 100.000 (100.000)
Epoch: [169][20/25]	Time 0.591 (0.580)	Data 0.007 (0.044)	Loss 0.1775 (0.1794)	Acc@1 99.756 (99.570)	Acc@5 100.000 (100.000)
Epoch: [169][21/25]	Time 0.557 (0.579)	Data 0.005 (0.042)	Loss 0.1848 (0.1797)	Acc@1 99.121 (99.549)	Acc@5 100.000 (100.000)
Epoch: [169][22/25]	Time 0.590 (0.580)	Data 0.009 (0.040)	Loss 0.1789 (0.1796)	Acc@1 99.463 (99.546)	Acc@5 100.000 (100.000)
Epoch: [169][23/25]	Time 0.600 (0.581)	Data 0.005 (0.039)	Loss 0.1792 (0.1796)	Acc@1 99.561 (99.546)	Acc@5 100.000 (100.000)
Epoch: [169][24/25]	Time 0.339 (0.571)	Data 0.009 (0.038)	Loss 0.1817 (0.1796)	Acc@1 99.528 (99.546)	Acc@5 100.000 (100.000)

Epoch: [170 | 180] LR: 0.000400
Epoch: [170][0/25]	Time 0.585 (0.585)	Data 0.606 (0.606)	Loss 0.1792 (0.1792)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [170][1/25]	Time 0.595 (0.590)	Data 0.007 (0.306)	Loss 0.1869 (0.1830)	Acc@1 99.268 (99.438)	Acc@5 100.000 (100.000)
Epoch: [170][2/25]	Time 0.539 (0.573)	Data 0.005 (0.206)	Loss 0.1817 (0.1826)	Acc@1 99.365 (99.414)	Acc@5 100.000 (100.000)
Epoch: [170][3/25]	Time 0.580 (0.575)	Data 0.007 (0.156)	Loss 0.1828 (0.1826)	Acc@1 99.414 (99.414)	Acc@5 100.000 (100.000)
Epoch: [170][4/25]	Time 0.594 (0.579)	Data 0.008 (0.127)	Loss 0.1773 (0.1816)	Acc@1 99.707 (99.473)	Acc@5 100.000 (100.000)
Epoch: [170][5/25]	Time 0.565 (0.576)	Data 0.005 (0.106)	Loss 0.1773 (0.1809)	Acc@1 99.658 (99.504)	Acc@5 100.000 (100.000)
Epoch: [170][6/25]	Time 0.567 (0.575)	Data 0.004 (0.092)	Loss 0.1769 (0.1803)	Acc@1 99.658 (99.526)	Acc@5 100.000 (100.000)
Epoch: [170][7/25]	Time 0.572 (0.575)	Data 0.005 (0.081)	Loss 0.1772 (0.1799)	Acc@1 99.561 (99.530)	Acc@5 100.000 (100.000)
Epoch: [170][8/25]	Time 0.569 (0.574)	Data 0.006 (0.072)	Loss 0.1882 (0.1808)	Acc@1 99.365 (99.512)	Acc@5 99.951 (99.995)
Epoch: [170][9/25]	Time 0.572 (0.574)	Data 0.007 (0.066)	Loss 0.1817 (0.1809)	Acc@1 99.365 (99.497)	Acc@5 100.000 (99.995)
Epoch: [170][10/25]	Time 0.604 (0.576)	Data 0.005 (0.060)	Loss 0.1830 (0.1811)	Acc@1 99.512 (99.498)	Acc@5 100.000 (99.996)
Epoch: [170][11/25]	Time 0.570 (0.576)	Data 0.006 (0.056)	Loss 0.1815 (0.1811)	Acc@1 99.414 (99.491)	Acc@5 100.000 (99.996)
Epoch: [170][12/25]	Time 0.585 (0.577)	Data 0.005 (0.052)	Loss 0.1771 (0.1808)	Acc@1 99.561 (99.497)	Acc@5 100.000 (99.996)
Epoch: [170][13/25]	Time 0.561 (0.575)	Data 0.005 (0.049)	Loss 0.1821 (0.1809)	Acc@1 99.414 (99.491)	Acc@5 100.000 (99.997)
Epoch: [170][14/25]	Time 0.589 (0.576)	Data 0.004 (0.046)	Loss 0.1753 (0.1805)	Acc@1 99.854 (99.515)	Acc@5 100.000 (99.997)
Epoch: [170][15/25]	Time 0.578 (0.577)	Data 0.005 (0.043)	Loss 0.1792 (0.1805)	Acc@1 99.512 (99.515)	Acc@5 100.000 (99.997)
Epoch: [170][16/25]	Time 0.587 (0.577)	Data 0.006 (0.041)	Loss 0.1807 (0.1805)	Acc@1 99.463 (99.512)	Acc@5 100.000 (99.997)
Epoch: [170][17/25]	Time 0.567 (0.577)	Data 0.005 (0.039)	Loss 0.1820 (0.1806)	Acc@1 99.561 (99.514)	Acc@5 100.000 (99.997)
Epoch: [170][18/25]	Time 0.581 (0.577)	Data 0.006 (0.037)	Loss 0.1854 (0.1808)	Acc@1 99.219 (99.499)	Acc@5 100.000 (99.997)
Epoch: [170][19/25]	Time 0.567 (0.576)	Data 0.006 (0.036)	Loss 0.1780 (0.1807)	Acc@1 99.707 (99.509)	Acc@5 100.000 (99.998)
Epoch: [170][20/25]	Time 0.578 (0.576)	Data 0.006 (0.034)	Loss 0.1800 (0.1806)	Acc@1 99.414 (99.505)	Acc@5 100.000 (99.998)
Epoch: [170][21/25]	Time 0.561 (0.576)	Data 0.004 (0.033)	Loss 0.1777 (0.1805)	Acc@1 99.512 (99.505)	Acc@5 100.000 (99.998)
Epoch: [170][22/25]	Time 0.587 (0.576)	Data 0.005 (0.032)	Loss 0.1808 (0.1805)	Acc@1 99.365 (99.499)	Acc@5 100.000 (99.998)
Epoch: [170][23/25]	Time 0.568 (0.576)	Data 0.004 (0.030)	Loss 0.1761 (0.1803)	Acc@1 99.756 (99.510)	Acc@5 100.000 (99.998)
Epoch: [170][24/25]	Time 0.356 (0.567)	Data 0.004 (0.029)	Loss 0.1863 (0.1804)	Acc@1 99.410 (99.508)	Acc@5 100.000 (99.998)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [171 | 180] LR: 0.000400
Epoch: [171][0/25]	Time 0.520 (0.520)	Data 0.599 (0.599)	Loss 0.1830 (0.1830)	Acc@1 99.463 (99.463)	Acc@5 100.000 (100.000)
Epoch: [171][1/25]	Time 0.556 (0.538)	Data 0.004 (0.302)	Loss 0.1752 (0.1791)	Acc@1 99.658 (99.561)	Acc@5 100.000 (100.000)
Epoch: [171][2/25]	Time 0.603 (0.560)	Data 0.005 (0.203)	Loss 0.1786 (0.1789)	Acc@1 99.658 (99.593)	Acc@5 100.000 (100.000)
Epoch: [171][3/25]	Time 0.551 (0.558)	Data 0.006 (0.154)	Loss 0.1779 (0.1787)	Acc@1 99.512 (99.573)	Acc@5 100.000 (100.000)
Epoch: [171][4/25]	Time 0.606 (0.567)	Data 0.008 (0.124)	Loss 0.1825 (0.1795)	Acc@1 99.512 (99.561)	Acc@5 100.000 (100.000)
Epoch: [171][5/25]	Time 0.580 (0.569)	Data 0.006 (0.105)	Loss 0.1831 (0.1801)	Acc@1 99.268 (99.512)	Acc@5 100.000 (100.000)
Epoch: [171][6/25]	Time 0.572 (0.570)	Data 0.006 (0.091)	Loss 0.1784 (0.1798)	Acc@1 99.561 (99.519)	Acc@5 100.000 (100.000)
Epoch: [171][7/25]	Time 0.587 (0.572)	Data 0.005 (0.080)	Loss 0.1793 (0.1798)	Acc@1 99.658 (99.536)	Acc@5 100.000 (100.000)
Epoch: [171][8/25]	Time 0.535 (0.568)	Data 0.005 (0.072)	Loss 0.1785 (0.1796)	Acc@1 99.609 (99.544)	Acc@5 100.000 (100.000)
Epoch: [171][9/25]	Time 0.583 (0.569)	Data 0.003 (0.065)	Loss 0.1799 (0.1796)	Acc@1 99.512 (99.541)	Acc@5 100.000 (100.000)
Epoch: [171][10/25]	Time 0.541 (0.567)	Data 0.006 (0.059)	Loss 0.1788 (0.1796)	Acc@1 99.414 (99.529)	Acc@5 100.000 (100.000)
Epoch: [171][11/25]	Time 0.588 (0.569)	Data 0.006 (0.055)	Loss 0.1762 (0.1793)	Acc@1 99.756 (99.548)	Acc@5 100.000 (100.000)
Epoch: [171][12/25]	Time 0.529 (0.566)	Data 0.006 (0.051)	Loss 0.1784 (0.1792)	Acc@1 99.463 (99.542)	Acc@5 100.000 (100.000)
Epoch: [171][13/25]	Time 0.614 (0.569)	Data 0.006 (0.048)	Loss 0.1785 (0.1792)	Acc@1 99.658 (99.550)	Acc@5 100.000 (100.000)
Epoch: [171][14/25]	Time 0.565 (0.569)	Data 0.006 (0.045)	Loss 0.1792 (0.1792)	Acc@1 99.512 (99.548)	Acc@5 100.000 (100.000)
Epoch: [171][15/25]	Time 0.565 (0.568)	Data 0.004 (0.043)	Loss 0.1810 (0.1793)	Acc@1 99.512 (99.545)	Acc@5 100.000 (100.000)
Epoch: [171][16/25]	Time 0.590 (0.570)	Data 0.006 (0.041)	Loss 0.1798 (0.1793)	Acc@1 99.512 (99.543)	Acc@5 100.000 (100.000)
Epoch: [171][17/25]	Time 0.593 (0.571)	Data 0.006 (0.039)	Loss 0.1830 (0.1795)	Acc@1 99.365 (99.533)	Acc@5 100.000 (100.000)
Epoch: [171][18/25]	Time 0.553 (0.570)	Data 0.004 (0.037)	Loss 0.1820 (0.1796)	Acc@1 99.316 (99.522)	Acc@5 100.000 (100.000)
Epoch: [171][19/25]	Time 0.582 (0.571)	Data 0.008 (0.035)	Loss 0.1774 (0.1795)	Acc@1 99.707 (99.531)	Acc@5 100.000 (100.000)
Epoch: [171][20/25]	Time 0.575 (0.571)	Data 0.007 (0.034)	Loss 0.1823 (0.1797)	Acc@1 99.512 (99.530)	Acc@5 99.951 (99.998)
Epoch: [171][21/25]	Time 0.576 (0.571)	Data 0.005 (0.033)	Loss 0.1752 (0.1795)	Acc@1 99.756 (99.541)	Acc@5 100.000 (99.998)
Epoch: [171][22/25]	Time 0.614 (0.573)	Data 0.006 (0.032)	Loss 0.1786 (0.1794)	Acc@1 99.707 (99.548)	Acc@5 100.000 (99.998)
Epoch: [171][23/25]	Time 0.572 (0.573)	Data 0.007 (0.030)	Loss 0.1790 (0.1794)	Acc@1 99.658 (99.552)	Acc@5 100.000 (99.998)
Epoch: [171][24/25]	Time 0.321 (0.563)	Data 0.007 (0.030)	Loss 0.1750 (0.1793)	Acc@1 99.764 (99.556)	Acc@5 100.000 (99.998)

Epoch: [172 | 180] LR: 0.000400
Epoch: [172][0/25]	Time 0.545 (0.545)	Data 0.587 (0.587)	Loss 0.1779 (0.1779)	Acc@1 99.463 (99.463)	Acc@5 100.000 (100.000)
Epoch: [172][1/25]	Time 0.605 (0.575)	Data 0.009 (0.298)	Loss 0.1840 (0.1810)	Acc@1 99.219 (99.341)	Acc@5 100.000 (100.000)
Epoch: [172][2/25]	Time 0.591 (0.580)	Data 0.005 (0.200)	Loss 0.1767 (0.1796)	Acc@1 99.707 (99.463)	Acc@5 100.000 (100.000)
Epoch: [172][3/25]	Time 0.573 (0.578)	Data 0.006 (0.152)	Loss 0.1793 (0.1795)	Acc@1 99.561 (99.487)	Acc@5 100.000 (100.000)
Epoch: [172][4/25]	Time 0.579 (0.579)	Data 0.007 (0.123)	Loss 0.1825 (0.1801)	Acc@1 99.609 (99.512)	Acc@5 100.000 (100.000)
Epoch: [172][5/25]	Time 0.591 (0.581)	Data 0.005 (0.103)	Loss 0.1779 (0.1797)	Acc@1 99.414 (99.495)	Acc@5 100.000 (100.000)
Epoch: [172][6/25]	Time 0.578 (0.580)	Data 0.004 (0.089)	Loss 0.1790 (0.1796)	Acc@1 99.707 (99.526)	Acc@5 99.951 (99.993)
Epoch: [172][7/25]	Time 0.566 (0.578)	Data 0.007 (0.079)	Loss 0.1766 (0.1792)	Acc@1 99.609 (99.536)	Acc@5 100.000 (99.994)
Epoch: [172][8/25]	Time 0.612 (0.582)	Data 0.005 (0.071)	Loss 0.1839 (0.1798)	Acc@1 99.365 (99.517)	Acc@5 100.000 (99.995)
Epoch: [172][9/25]	Time 0.575 (0.581)	Data 0.005 (0.064)	Loss 0.1730 (0.1791)	Acc@1 99.854 (99.551)	Acc@5 100.000 (99.995)
Epoch: [172][10/25]	Time 0.577 (0.581)	Data 0.005 (0.059)	Loss 0.1794 (0.1791)	Acc@1 99.609 (99.556)	Acc@5 100.000 (99.996)
Epoch: [172][11/25]	Time 0.578 (0.581)	Data 0.006 (0.054)	Loss 0.1788 (0.1791)	Acc@1 99.512 (99.552)	Acc@5 100.000 (99.996)
Epoch: [172][12/25]	Time 0.582 (0.581)	Data 0.005 (0.050)	Loss 0.1768 (0.1789)	Acc@1 99.658 (99.561)	Acc@5 100.000 (99.996)
Epoch: [172][13/25]	Time 0.578 (0.581)	Data 0.008 (0.047)	Loss 0.1757 (0.1787)	Acc@1 99.854 (99.581)	Acc@5 100.000 (99.997)
Epoch: [172][14/25]	Time 0.572 (0.580)	Data 0.004 (0.044)	Loss 0.1814 (0.1789)	Acc@1 99.463 (99.574)	Acc@5 100.000 (99.997)
Epoch: [172][15/25]	Time 0.604 (0.582)	Data 0.007 (0.042)	Loss 0.1845 (0.1792)	Acc@1 99.121 (99.545)	Acc@5 100.000 (99.997)
Epoch: [172][16/25]	Time 0.597 (0.582)	Data 0.008 (0.040)	Loss 0.1826 (0.1794)	Acc@1 99.414 (99.538)	Acc@5 100.000 (99.997)
Epoch: [172][17/25]	Time 0.579 (0.582)	Data 0.006 (0.038)	Loss 0.1804 (0.1795)	Acc@1 99.414 (99.531)	Acc@5 100.000 (99.997)
Epoch: [172][18/25]	Time 0.588 (0.583)	Data 0.006 (0.037)	Loss 0.1762 (0.1793)	Acc@1 99.658 (99.537)	Acc@5 100.000 (99.997)
Epoch: [172][19/25]	Time 0.590 (0.583)	Data 0.004 (0.035)	Loss 0.1789 (0.1793)	Acc@1 99.707 (99.546)	Acc@5 100.000 (99.998)
Epoch: [172][20/25]	Time 0.557 (0.582)	Data 0.004 (0.033)	Loss 0.1760 (0.1791)	Acc@1 99.658 (99.551)	Acc@5 100.000 (99.998)
Epoch: [172][21/25]	Time 0.594 (0.582)	Data 0.006 (0.032)	Loss 0.1840 (0.1793)	Acc@1 99.268 (99.538)	Acc@5 100.000 (99.998)
Epoch: [172][22/25]	Time 0.567 (0.582)	Data 0.004 (0.031)	Loss 0.1821 (0.1795)	Acc@1 99.268 (99.527)	Acc@5 100.000 (99.998)
Epoch: [172][23/25]	Time 0.601 (0.582)	Data 0.005 (0.030)	Loss 0.1758 (0.1793)	Acc@1 99.805 (99.538)	Acc@5 100.000 (99.998)
Epoch: [172][24/25]	Time 0.311 (0.572)	Data 0.004 (0.029)	Loss 0.1813 (0.1793)	Acc@1 99.292 (99.534)	Acc@5 100.000 (99.998)

Epoch: [173 | 180] LR: 0.000400
Epoch: [173][0/25]	Time 0.578 (0.578)	Data 0.618 (0.618)	Loss 0.1812 (0.1812)	Acc@1 99.658 (99.658)	Acc@5 100.000 (100.000)
Epoch: [173][1/25]	Time 0.614 (0.596)	Data 0.005 (0.311)	Loss 0.1761 (0.1786)	Acc@1 99.658 (99.658)	Acc@5 100.000 (100.000)
Epoch: [173][2/25]	Time 0.543 (0.578)	Data 0.007 (0.210)	Loss 0.1815 (0.1796)	Acc@1 99.707 (99.674)	Acc@5 100.000 (100.000)
Epoch: [173][3/25]	Time 0.608 (0.586)	Data 0.006 (0.159)	Loss 0.1773 (0.1790)	Acc@1 99.512 (99.634)	Acc@5 100.000 (100.000)
Epoch: [173][4/25]	Time 0.566 (0.582)	Data 0.005 (0.128)	Loss 0.1807 (0.1794)	Acc@1 99.561 (99.619)	Acc@5 100.000 (100.000)
Epoch: [173][5/25]	Time 0.573 (0.580)	Data 0.004 (0.107)	Loss 0.1814 (0.1797)	Acc@1 99.512 (99.601)	Acc@5 99.951 (99.992)
Epoch: [173][6/25]	Time 0.535 (0.574)	Data 0.007 (0.093)	Loss 0.1827 (0.1801)	Acc@1 99.365 (99.568)	Acc@5 100.000 (99.993)
Epoch: [173][7/25]	Time 0.591 (0.576)	Data 0.007 (0.082)	Loss 0.1823 (0.1804)	Acc@1 99.365 (99.542)	Acc@5 100.000 (99.994)
Epoch: [173][8/25]	Time 0.539 (0.572)	Data 0.005 (0.074)	Loss 0.1769 (0.1800)	Acc@1 99.609 (99.550)	Acc@5 100.000 (99.995)
Epoch: [173][9/25]	Time 0.559 (0.571)	Data 0.007 (0.067)	Loss 0.1758 (0.1796)	Acc@1 99.805 (99.575)	Acc@5 100.000 (99.995)
Epoch: [173][10/25]	Time 0.545 (0.568)	Data 0.004 (0.061)	Loss 0.1786 (0.1795)	Acc@1 99.512 (99.569)	Acc@5 100.000 (99.996)
Epoch: [173][11/25]	Time 0.565 (0.568)	Data 0.005 (0.057)	Loss 0.1779 (0.1794)	Acc@1 99.756 (99.585)	Acc@5 100.000 (99.996)
Epoch: [173][12/25]	Time 0.581 (0.569)	Data 0.006 (0.053)	Loss 0.1792 (0.1794)	Acc@1 99.609 (99.587)	Acc@5 100.000 (99.996)
Epoch: [173][13/25]	Time 0.556 (0.568)	Data 0.005 (0.049)	Loss 0.1791 (0.1793)	Acc@1 99.561 (99.585)	Acc@5 100.000 (99.997)
Epoch: [173][14/25]	Time 0.586 (0.569)	Data 0.008 (0.046)	Loss 0.1757 (0.1791)	Acc@1 99.854 (99.603)	Acc@5 100.000 (99.997)
Epoch: [173][15/25]	Time 0.605 (0.571)	Data 0.005 (0.044)	Loss 0.1744 (0.1788)	Acc@1 99.854 (99.619)	Acc@5 100.000 (99.997)
Epoch: [173][16/25]	Time 0.569 (0.571)	Data 0.007 (0.042)	Loss 0.1753 (0.1786)	Acc@1 99.707 (99.624)	Acc@5 100.000 (99.997)
Epoch: [173][17/25]	Time 0.595 (0.573)	Data 0.007 (0.040)	Loss 0.1807 (0.1787)	Acc@1 99.512 (99.618)	Acc@5 100.000 (99.997)
Epoch: [173][18/25]	Time 0.586 (0.573)	Data 0.004 (0.038)	Loss 0.1795 (0.1788)	Acc@1 99.658 (99.620)	Acc@5 100.000 (99.997)
Epoch: [173][19/25]	Time 0.572 (0.573)	Data 0.007 (0.036)	Loss 0.1784 (0.1787)	Acc@1 99.609 (99.619)	Acc@5 100.000 (99.998)
Epoch: [173][20/25]	Time 0.596 (0.574)	Data 0.007 (0.035)	Loss 0.1791 (0.1788)	Acc@1 99.658 (99.621)	Acc@5 100.000 (99.998)
Epoch: [173][21/25]	Time 0.579 (0.575)	Data 0.008 (0.034)	Loss 0.1795 (0.1788)	Acc@1 99.414 (99.612)	Acc@5 100.000 (99.998)
Epoch: [173][22/25]	Time 0.579 (0.575)	Data 0.004 (0.032)	Loss 0.1811 (0.1789)	Acc@1 99.561 (99.609)	Acc@5 99.951 (99.996)
Epoch: [173][23/25]	Time 0.576 (0.575)	Data 0.006 (0.031)	Loss 0.1785 (0.1789)	Acc@1 99.707 (99.613)	Acc@5 100.000 (99.996)
Epoch: [173][24/25]	Time 0.332 (0.565)	Data 0.004 (0.030)	Loss 0.1798 (0.1789)	Acc@1 99.410 (99.610)	Acc@5 100.000 (99.996)

Epoch: [174 | 180] LR: 0.000400
Epoch: [174][0/25]	Time 0.593 (0.593)	Data 0.805 (0.805)	Loss 0.1772 (0.1772)	Acc@1 99.463 (99.463)	Acc@5 100.000 (100.000)
Epoch: [174][1/25]	Time 0.569 (0.581)	Data 0.005 (0.405)	Loss 0.1798 (0.1785)	Acc@1 99.512 (99.487)	Acc@5 100.000 (100.000)
Epoch: [174][2/25]	Time 0.571 (0.578)	Data 0.004 (0.271)	Loss 0.1775 (0.1781)	Acc@1 99.561 (99.512)	Acc@5 100.000 (100.000)
Epoch: [174][3/25]	Time 0.578 (0.578)	Data 0.005 (0.205)	Loss 0.1834 (0.1795)	Acc@1 99.316 (99.463)	Acc@5 100.000 (100.000)
Epoch: [174][4/25]	Time 0.520 (0.566)	Data 0.006 (0.165)	Loss 0.1753 (0.1786)	Acc@1 99.609 (99.492)	Acc@5 100.000 (100.000)
Epoch: [174][5/25]	Time 0.564 (0.566)	Data 0.004 (0.138)	Loss 0.1786 (0.1786)	Acc@1 99.561 (99.504)	Acc@5 100.000 (100.000)
Epoch: [174][6/25]	Time 0.577 (0.567)	Data 0.006 (0.119)	Loss 0.1777 (0.1785)	Acc@1 99.609 (99.519)	Acc@5 100.000 (100.000)
Epoch: [174][7/25]	Time 0.587 (0.570)	Data 0.003 (0.105)	Loss 0.1807 (0.1788)	Acc@1 99.463 (99.512)	Acc@5 100.000 (100.000)
Epoch: [174][8/25]	Time 0.544 (0.567)	Data 0.005 (0.094)	Loss 0.1825 (0.1792)	Acc@1 99.365 (99.495)	Acc@5 100.000 (100.000)
Epoch: [174][9/25]	Time 0.581 (0.568)	Data 0.005 (0.085)	Loss 0.1740 (0.1787)	Acc@1 99.707 (99.517)	Acc@5 100.000 (100.000)
Epoch: [174][10/25]	Time 0.576 (0.569)	Data 0.005 (0.077)	Loss 0.1835 (0.1791)	Acc@1 99.463 (99.512)	Acc@5 100.000 (100.000)
Epoch: [174][11/25]	Time 0.572 (0.569)	Data 0.004 (0.071)	Loss 0.1768 (0.1789)	Acc@1 99.609 (99.520)	Acc@5 100.000 (100.000)
Epoch: [174][12/25]	Time 0.561 (0.569)	Data 0.008 (0.066)	Loss 0.1811 (0.1791)	Acc@1 99.365 (99.508)	Acc@5 100.000 (100.000)
Epoch: [174][13/25]	Time 0.565 (0.568)	Data 0.005 (0.062)	Loss 0.1806 (0.1792)	Acc@1 99.561 (99.512)	Acc@5 100.000 (100.000)
Epoch: [174][14/25]	Time 0.580 (0.569)	Data 0.005 (0.058)	Loss 0.1737 (0.1788)	Acc@1 99.756 (99.528)	Acc@5 100.000 (100.000)
Epoch: [174][15/25]	Time 0.583 (0.570)	Data 0.005 (0.055)	Loss 0.1761 (0.1786)	Acc@1 99.658 (99.536)	Acc@5 100.000 (100.000)
Epoch: [174][16/25]	Time 0.554 (0.569)	Data 0.005 (0.052)	Loss 0.1761 (0.1785)	Acc@1 99.707 (99.546)	Acc@5 100.000 (100.000)
Epoch: [174][17/25]	Time 0.576 (0.569)	Data 0.006 (0.049)	Loss 0.1783 (0.1785)	Acc@1 99.316 (99.533)	Acc@5 100.000 (100.000)
Epoch: [174][18/25]	Time 0.554 (0.569)	Data 0.004 (0.047)	Loss 0.1841 (0.1788)	Acc@1 99.512 (99.532)	Acc@5 99.951 (99.997)
Epoch: [174][19/25]	Time 0.615 (0.571)	Data 0.004 (0.045)	Loss 0.1772 (0.1787)	Acc@1 99.609 (99.536)	Acc@5 100.000 (99.998)
Epoch: [174][20/25]	Time 0.580 (0.571)	Data 0.004 (0.043)	Loss 0.1786 (0.1787)	Acc@1 99.463 (99.533)	Acc@5 100.000 (99.998)
Epoch: [174][21/25]	Time 0.599 (0.573)	Data 0.006 (0.041)	Loss 0.1774 (0.1786)	Acc@1 99.609 (99.536)	Acc@5 100.000 (99.998)
Epoch: [174][22/25]	Time 0.585 (0.573)	Data 0.006 (0.040)	Loss 0.1811 (0.1787)	Acc@1 99.316 (99.527)	Acc@5 100.000 (99.998)
Epoch: [174][23/25]	Time 0.593 (0.574)	Data 0.004 (0.038)	Loss 0.1838 (0.1790)	Acc@1 99.365 (99.520)	Acc@5 100.000 (99.998)
Epoch: [174][24/25]	Time 0.311 (0.563)	Data 0.006 (0.037)	Loss 0.1796 (0.1790)	Acc@1 99.882 (99.526)	Acc@5 100.000 (99.998)

Epoch: [175 | 180] LR: 0.000400
Epoch: [175][0/25]	Time 0.589 (0.589)	Data 0.667 (0.667)	Loss 0.1763 (0.1763)	Acc@1 99.756 (99.756)	Acc@5 100.000 (100.000)
Epoch: [175][1/25]	Time 0.559 (0.574)	Data 0.005 (0.336)	Loss 0.1797 (0.1780)	Acc@1 99.463 (99.609)	Acc@5 100.000 (100.000)
Epoch: [175][2/25]	Time 0.595 (0.581)	Data 0.006 (0.226)	Loss 0.1759 (0.1773)	Acc@1 99.756 (99.658)	Acc@5 100.000 (100.000)
Epoch: [175][3/25]	Time 0.552 (0.574)	Data 0.007 (0.171)	Loss 0.1835 (0.1789)	Acc@1 99.268 (99.561)	Acc@5 100.000 (100.000)
Epoch: [175][4/25]	Time 0.595 (0.578)	Data 0.009 (0.139)	Loss 0.1755 (0.1782)	Acc@1 99.756 (99.600)	Acc@5 100.000 (100.000)
Epoch: [175][5/25]	Time 0.590 (0.580)	Data 0.005 (0.116)	Loss 0.1824 (0.1789)	Acc@1 99.365 (99.561)	Acc@5 100.000 (100.000)
Epoch: [175][6/25]	Time 0.561 (0.577)	Data 0.006 (0.100)	Loss 0.1803 (0.1791)	Acc@1 99.658 (99.574)	Acc@5 100.000 (100.000)
Epoch: [175][7/25]	Time 0.589 (0.579)	Data 0.007 (0.089)	Loss 0.1753 (0.1786)	Acc@1 99.707 (99.591)	Acc@5 100.000 (100.000)
Epoch: [175][8/25]	Time 0.513 (0.572)	Data 0.005 (0.079)	Loss 0.1768 (0.1784)	Acc@1 99.756 (99.609)	Acc@5 100.000 (100.000)
Epoch: [175][9/25]	Time 0.643 (0.579)	Data 0.004 (0.072)	Loss 0.1833 (0.1789)	Acc@1 99.414 (99.590)	Acc@5 100.000 (100.000)
Epoch: [175][10/25]	Time 0.664 (0.586)	Data 0.004 (0.066)	Loss 0.1780 (0.1788)	Acc@1 99.658 (99.596)	Acc@5 100.000 (100.000)
Epoch: [175][11/25]	Time 0.596 (0.587)	Data 0.004 (0.061)	Loss 0.1807 (0.1790)	Acc@1 99.512 (99.589)	Acc@5 100.000 (100.000)
Epoch: [175][12/25]	Time 0.586 (0.587)	Data 0.004 (0.056)	Loss 0.1815 (0.1792)	Acc@1 99.365 (99.572)	Acc@5 100.000 (100.000)
Epoch: [175][13/25]	Time 0.567 (0.586)	Data 0.006 (0.053)	Loss 0.1806 (0.1793)	Acc@1 99.561 (99.571)	Acc@5 100.000 (100.000)
Epoch: [175][14/25]	Time 0.595 (0.586)	Data 0.006 (0.049)	Loss 0.1814 (0.1794)	Acc@1 99.561 (99.570)	Acc@5 100.000 (100.000)
Epoch: [175][15/25]	Time 0.596 (0.587)	Data 0.006 (0.047)	Loss 0.1745 (0.1791)	Acc@1 99.854 (99.588)	Acc@5 100.000 (100.000)
Epoch: [175][16/25]	Time 0.590 (0.587)	Data 0.005 (0.044)	Loss 0.1790 (0.1791)	Acc@1 99.561 (99.586)	Acc@5 100.000 (100.000)
Epoch: [175][17/25]	Time 0.507 (0.583)	Data 0.004 (0.042)	Loss 0.1755 (0.1789)	Acc@1 99.561 (99.585)	Acc@5 100.000 (100.000)
Epoch: [175][18/25]	Time 0.550 (0.581)	Data 0.005 (0.040)	Loss 0.1820 (0.1791)	Acc@1 99.414 (99.576)	Acc@5 100.000 (100.000)
Epoch: [175][19/25]	Time 0.592 (0.582)	Data 0.006 (0.038)	Loss 0.1762 (0.1789)	Acc@1 99.805 (99.587)	Acc@5 100.000 (100.000)
Epoch: [175][20/25]	Time 0.539 (0.579)	Data 0.007 (0.037)	Loss 0.1807 (0.1790)	Acc@1 99.463 (99.581)	Acc@5 100.000 (100.000)
Epoch: [175][21/25]	Time 0.587 (0.580)	Data 0.004 (0.035)	Loss 0.1770 (0.1789)	Acc@1 99.512 (99.578)	Acc@5 100.000 (100.000)
Epoch: [175][22/25]	Time 0.561 (0.579)	Data 0.007 (0.034)	Loss 0.1791 (0.1789)	Acc@1 99.512 (99.575)	Acc@5 100.000 (100.000)
Epoch: [175][23/25]	Time 0.579 (0.579)	Data 0.007 (0.033)	Loss 0.1772 (0.1788)	Acc@1 99.609 (99.577)	Acc@5 100.000 (100.000)
Epoch: [175][24/25]	Time 0.316 (0.568)	Data 0.004 (0.032)	Loss 0.1724 (0.1787)	Acc@1 100.000 (99.584)	Acc@5 100.000 (100.000)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(7, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [176 | 180] LR: 0.000400
Epoch: [176][0/25]	Time 0.582 (0.582)	Data 0.744 (0.744)	Loss 0.1791 (0.1791)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [176][1/25]	Time 0.545 (0.564)	Data 0.005 (0.374)	Loss 0.1774 (0.1782)	Acc@1 99.561 (99.585)	Acc@5 100.000 (100.000)
Epoch: [176][2/25]	Time 0.571 (0.566)	Data 0.003 (0.251)	Loss 0.1784 (0.1783)	Acc@1 99.512 (99.561)	Acc@5 100.000 (100.000)
Epoch: [176][3/25]	Time 0.607 (0.576)	Data 0.004 (0.189)	Loss 0.1825 (0.1793)	Acc@1 99.268 (99.487)	Acc@5 100.000 (100.000)
Epoch: [176][4/25]	Time 0.583 (0.578)	Data 0.007 (0.153)	Loss 0.1768 (0.1788)	Acc@1 99.609 (99.512)	Acc@5 100.000 (100.000)
Epoch: [176][5/25]	Time 0.544 (0.572)	Data 0.004 (0.128)	Loss 0.1789 (0.1788)	Acc@1 99.561 (99.520)	Acc@5 100.000 (100.000)
Epoch: [176][6/25]	Time 0.563 (0.571)	Data 0.004 (0.110)	Loss 0.1749 (0.1783)	Acc@1 99.854 (99.568)	Acc@5 100.000 (100.000)
Epoch: [176][7/25]	Time 0.528 (0.565)	Data 0.006 (0.097)	Loss 0.1848 (0.1791)	Acc@1 99.268 (99.530)	Acc@5 100.000 (100.000)
Epoch: [176][8/25]	Time 0.597 (0.569)	Data 0.005 (0.087)	Loss 0.1814 (0.1793)	Acc@1 99.365 (99.512)	Acc@5 100.000 (100.000)
Epoch: [176][9/25]	Time 0.520 (0.564)	Data 0.004 (0.079)	Loss 0.1788 (0.1793)	Acc@1 99.365 (99.497)	Acc@5 100.000 (100.000)
Epoch: [176][10/25]	Time 0.577 (0.565)	Data 0.009 (0.072)	Loss 0.1800 (0.1793)	Acc@1 99.512 (99.498)	Acc@5 100.000 (100.000)
Epoch: [176][11/25]	Time 0.561 (0.565)	Data 0.006 (0.067)	Loss 0.1733 (0.1788)	Acc@1 99.805 (99.524)	Acc@5 100.000 (100.000)
Epoch: [176][12/25]	Time 0.575 (0.566)	Data 0.005 (0.062)	Loss 0.1772 (0.1787)	Acc@1 99.609 (99.530)	Acc@5 100.000 (100.000)
Epoch: [176][13/25]	Time 0.563 (0.565)	Data 0.004 (0.058)	Loss 0.1756 (0.1785)	Acc@1 99.609 (99.536)	Acc@5 100.000 (100.000)
Epoch: [176][14/25]	Time 0.586 (0.567)	Data 0.005 (0.054)	Loss 0.1764 (0.1784)	Acc@1 99.609 (99.541)	Acc@5 100.000 (100.000)
Epoch: [176][15/25]	Time 0.587 (0.568)	Data 0.005 (0.051)	Loss 0.1803 (0.1785)	Acc@1 99.268 (99.524)	Acc@5 100.000 (100.000)
Epoch: [176][16/25]	Time 0.579 (0.569)	Data 0.006 (0.049)	Loss 0.1795 (0.1785)	Acc@1 99.512 (99.523)	Acc@5 100.000 (100.000)
Epoch: [176][17/25]	Time 0.555 (0.568)	Data 0.007 (0.046)	Loss 0.1779 (0.1785)	Acc@1 99.707 (99.533)	Acc@5 100.000 (100.000)
Epoch: [176][18/25]	Time 0.583 (0.569)	Data 0.007 (0.044)	Loss 0.1776 (0.1785)	Acc@1 99.609 (99.537)	Acc@5 100.000 (100.000)
Epoch: [176][19/25]	Time 0.589 (0.570)	Data 0.004 (0.042)	Loss 0.1808 (0.1786)	Acc@1 99.561 (99.539)	Acc@5 100.000 (100.000)
Epoch: [176][20/25]	Time 0.573 (0.570)	Data 0.009 (0.041)	Loss 0.1758 (0.1784)	Acc@1 99.609 (99.542)	Acc@5 100.000 (100.000)
Epoch: [176][21/25]	Time 0.570 (0.570)	Data 0.004 (0.039)	Loss 0.1764 (0.1783)	Acc@1 99.707 (99.549)	Acc@5 100.000 (100.000)
Epoch: [176][22/25]	Time 0.577 (0.570)	Data 0.007 (0.038)	Loss 0.1831 (0.1786)	Acc@1 99.316 (99.539)	Acc@5 100.000 (100.000)
Epoch: [176][23/25]	Time 0.601 (0.571)	Data 0.005 (0.036)	Loss 0.1756 (0.1784)	Acc@1 99.707 (99.546)	Acc@5 100.000 (100.000)
Epoch: [176][24/25]	Time 0.354 (0.563)	Data 0.005 (0.035)	Loss 0.1785 (0.1784)	Acc@1 99.646 (99.548)	Acc@5 100.000 (100.000)

Epoch: [177 | 180] LR: 0.000400
Epoch: [177][0/25]	Time 0.562 (0.562)	Data 0.611 (0.611)	Loss 0.1798 (0.1798)	Acc@1 99.414 (99.414)	Acc@5 100.000 (100.000)
Epoch: [177][1/25]	Time 0.555 (0.559)	Data 0.005 (0.308)	Loss 0.1762 (0.1780)	Acc@1 99.609 (99.512)	Acc@5 100.000 (100.000)
Epoch: [177][2/25]	Time 0.570 (0.562)	Data 0.004 (0.207)	Loss 0.1790 (0.1783)	Acc@1 99.561 (99.528)	Acc@5 100.000 (100.000)
Epoch: [177][3/25]	Time 0.577 (0.566)	Data 0.005 (0.156)	Loss 0.1761 (0.1778)	Acc@1 99.609 (99.548)	Acc@5 100.000 (100.000)
Epoch: [177][4/25]	Time 0.605 (0.574)	Data 0.005 (0.126)	Loss 0.1773 (0.1777)	Acc@1 99.658 (99.570)	Acc@5 100.000 (100.000)
Epoch: [177][5/25]	Time 0.590 (0.577)	Data 0.005 (0.106)	Loss 0.1803 (0.1781)	Acc@1 99.365 (99.536)	Acc@5 100.000 (100.000)
Epoch: [177][6/25]	Time 0.592 (0.579)	Data 0.003 (0.091)	Loss 0.1777 (0.1781)	Acc@1 99.561 (99.540)	Acc@5 100.000 (100.000)
Epoch: [177][7/25]	Time 0.596 (0.581)	Data 0.007 (0.081)	Loss 0.1784 (0.1781)	Acc@1 99.512 (99.536)	Acc@5 100.000 (100.000)
Epoch: [177][8/25]	Time 0.553 (0.578)	Data 0.007 (0.072)	Loss 0.1817 (0.1785)	Acc@1 99.316 (99.512)	Acc@5 100.000 (100.000)
Epoch: [177][9/25]	Time 0.596 (0.580)	Data 0.006 (0.066)	Loss 0.1766 (0.1783)	Acc@1 99.707 (99.531)	Acc@5 100.000 (100.000)
Epoch: [177][10/25]	Time 0.593 (0.581)	Data 0.004 (0.060)	Loss 0.1821 (0.1787)	Acc@1 99.561 (99.534)	Acc@5 100.000 (100.000)
Epoch: [177][11/25]	Time 0.543 (0.578)	Data 0.005 (0.056)	Loss 0.1811 (0.1789)	Acc@1 99.414 (99.524)	Acc@5 100.000 (100.000)
Epoch: [177][12/25]	Time 0.607 (0.580)	Data 0.004 (0.052)	Loss 0.1783 (0.1788)	Acc@1 99.658 (99.534)	Acc@5 100.000 (100.000)
Epoch: [177][13/25]	Time 0.582 (0.580)	Data 0.008 (0.048)	Loss 0.1726 (0.1784)	Acc@1 99.805 (99.554)	Acc@5 100.000 (100.000)
Epoch: [177][14/25]	Time 0.566 (0.579)	Data 0.005 (0.046)	Loss 0.1799 (0.1785)	Acc@1 99.512 (99.551)	Acc@5 100.000 (100.000)
Epoch: [177][15/25]	Time 0.571 (0.579)	Data 0.007 (0.043)	Loss 0.1801 (0.1786)	Acc@1 99.463 (99.545)	Acc@5 100.000 (100.000)
Epoch: [177][16/25]	Time 0.560 (0.578)	Data 0.007 (0.041)	Loss 0.1818 (0.1788)	Acc@1 99.463 (99.540)	Acc@5 100.000 (100.000)
Epoch: [177][17/25]	Time 0.580 (0.578)	Data 0.006 (0.039)	Loss 0.1767 (0.1787)	Acc@1 99.609 (99.544)	Acc@5 100.000 (100.000)
Epoch: [177][18/25]	Time 0.597 (0.579)	Data 0.005 (0.037)	Loss 0.1779 (0.1786)	Acc@1 99.658 (99.550)	Acc@5 100.000 (100.000)
Epoch: [177][19/25]	Time 0.592 (0.579)	Data 0.004 (0.036)	Loss 0.1778 (0.1786)	Acc@1 99.658 (99.556)	Acc@5 100.000 (100.000)
Epoch: [177][20/25]	Time 0.560 (0.578)	Data 0.005 (0.034)	Loss 0.1756 (0.1784)	Acc@1 99.658 (99.561)	Acc@5 100.000 (100.000)
Epoch: [177][21/25]	Time 0.589 (0.579)	Data 0.006 (0.033)	Loss 0.1742 (0.1782)	Acc@1 99.609 (99.563)	Acc@5 100.000 (100.000)
Epoch: [177][22/25]	Time 0.577 (0.579)	Data 0.004 (0.032)	Loss 0.1757 (0.1781)	Acc@1 99.756 (99.571)	Acc@5 100.000 (100.000)
Epoch: [177][23/25]	Time 0.594 (0.580)	Data 0.007 (0.031)	Loss 0.1781 (0.1781)	Acc@1 99.609 (99.573)	Acc@5 100.000 (100.000)
Epoch: [177][24/25]	Time 0.341 (0.570)	Data 0.005 (0.030)	Loss 0.1756 (0.1781)	Acc@1 99.882 (99.578)	Acc@5 100.000 (100.000)

Epoch: [178 | 180] LR: 0.000400
Epoch: [178][0/25]	Time 0.532 (0.532)	Data 0.594 (0.594)	Loss 0.1782 (0.1782)	Acc@1 99.463 (99.463)	Acc@5 100.000 (100.000)
Epoch: [178][1/25]	Time 0.570 (0.551)	Data 0.007 (0.300)	Loss 0.1768 (0.1775)	Acc@1 99.561 (99.512)	Acc@5 100.000 (100.000)
Epoch: [178][2/25]	Time 0.576 (0.559)	Data 0.004 (0.201)	Loss 0.1780 (0.1777)	Acc@1 99.512 (99.512)	Acc@5 100.000 (100.000)
Epoch: [178][3/25]	Time 0.526 (0.551)	Data 0.004 (0.152)	Loss 0.1791 (0.1780)	Acc@1 99.512 (99.512)	Acc@5 100.000 (100.000)
Epoch: [178][4/25]	Time 0.579 (0.556)	Data 0.006 (0.123)	Loss 0.1746 (0.1774)	Acc@1 99.707 (99.551)	Acc@5 100.000 (100.000)
Epoch: [178][5/25]	Time 0.540 (0.554)	Data 0.008 (0.104)	Loss 0.1769 (0.1773)	Acc@1 99.365 (99.520)	Acc@5 100.000 (100.000)
Epoch: [178][6/25]	Time 0.604 (0.561)	Data 0.004 (0.089)	Loss 0.1767 (0.1772)	Acc@1 99.707 (99.547)	Acc@5 100.000 (100.000)
Epoch: [178][7/25]	Time 0.604 (0.566)	Data 0.006 (0.079)	Loss 0.1808 (0.1776)	Acc@1 99.756 (99.573)	Acc@5 100.000 (100.000)
Epoch: [178][8/25]	Time 0.574 (0.567)	Data 0.005 (0.071)	Loss 0.1857 (0.1785)	Acc@1 99.316 (99.544)	Acc@5 100.000 (100.000)
Epoch: [178][9/25]	Time 0.607 (0.571)	Data 0.004 (0.064)	Loss 0.1755 (0.1782)	Acc@1 99.609 (99.551)	Acc@5 100.000 (100.000)
Epoch: [178][10/25]	Time 0.585 (0.572)	Data 0.005 (0.059)	Loss 0.1779 (0.1782)	Acc@1 99.609 (99.556)	Acc@5 100.000 (100.000)
Epoch: [178][11/25]	Time 0.604 (0.575)	Data 0.007 (0.054)	Loss 0.1786 (0.1782)	Acc@1 99.561 (99.556)	Acc@5 100.000 (100.000)
Epoch: [178][12/25]	Time 0.574 (0.575)	Data 0.007 (0.051)	Loss 0.1811 (0.1785)	Acc@1 99.365 (99.542)	Acc@5 100.000 (100.000)
Epoch: [178][13/25]	Time 0.578 (0.575)	Data 0.004 (0.047)	Loss 0.1750 (0.1782)	Acc@1 99.658 (99.550)	Acc@5 100.000 (100.000)
Epoch: [178][14/25]	Time 0.570 (0.575)	Data 0.005 (0.045)	Loss 0.1733 (0.1779)	Acc@1 99.756 (99.564)	Acc@5 100.000 (100.000)
Epoch: [178][15/25]	Time 0.586 (0.576)	Data 0.007 (0.042)	Loss 0.1730 (0.1776)	Acc@1 99.707 (99.573)	Acc@5 100.000 (100.000)
Epoch: [178][16/25]	Time 0.588 (0.576)	Data 0.007 (0.040)	Loss 0.1776 (0.1776)	Acc@1 99.561 (99.572)	Acc@5 100.000 (100.000)
Epoch: [178][17/25]	Time 0.614 (0.578)	Data 0.007 (0.038)	Loss 0.1802 (0.1777)	Acc@1 99.658 (99.577)	Acc@5 100.000 (100.000)
Epoch: [178][18/25]	Time 0.592 (0.579)	Data 0.006 (0.037)	Loss 0.1817 (0.1779)	Acc@1 99.414 (99.568)	Acc@5 100.000 (100.000)
Epoch: [178][19/25]	Time 0.568 (0.579)	Data 0.003 (0.035)	Loss 0.1740 (0.1777)	Acc@1 99.805 (99.580)	Acc@5 100.000 (100.000)
Epoch: [178][20/25]	Time 0.572 (0.578)	Data 0.006 (0.034)	Loss 0.1775 (0.1777)	Acc@1 99.658 (99.584)	Acc@5 99.951 (99.998)
Epoch: [178][21/25]	Time 0.578 (0.578)	Data 0.007 (0.032)	Loss 0.1754 (0.1776)	Acc@1 99.756 (99.592)	Acc@5 100.000 (99.998)
Epoch: [178][22/25]	Time 0.608 (0.580)	Data 0.006 (0.031)	Loss 0.1764 (0.1776)	Acc@1 99.805 (99.601)	Acc@5 100.000 (99.998)
Epoch: [178][23/25]	Time 0.583 (0.580)	Data 0.004 (0.030)	Loss 0.1812 (0.1777)	Acc@1 99.463 (99.595)	Acc@5 100.000 (99.998)
Epoch: [178][24/25]	Time 0.316 (0.569)	Data 0.006 (0.029)	Loss 0.1739 (0.1777)	Acc@1 99.764 (99.598)	Acc@5 100.000 (99.998)

Epoch: [179 | 180] LR: 0.000400
Epoch: [179][0/25]	Time 0.549 (0.549)	Data 0.801 (0.801)	Loss 0.1785 (0.1785)	Acc@1 99.561 (99.561)	Acc@5 100.000 (100.000)
Epoch: [179][1/25]	Time 0.555 (0.552)	Data 0.005 (0.403)	Loss 0.1773 (0.1779)	Acc@1 99.805 (99.683)	Acc@5 100.000 (100.000)
Epoch: [179][2/25]	Time 0.583 (0.562)	Data 0.004 (0.270)	Loss 0.1776 (0.1778)	Acc@1 99.365 (99.577)	Acc@5 100.000 (100.000)
Epoch: [179][3/25]	Time 0.608 (0.573)	Data 0.005 (0.204)	Loss 0.1767 (0.1775)	Acc@1 99.658 (99.597)	Acc@5 100.000 (100.000)
Epoch: [179][4/25]	Time 0.586 (0.576)	Data 0.006 (0.164)	Loss 0.1759 (0.1772)	Acc@1 99.561 (99.590)	Acc@5 100.000 (100.000)
Epoch: [179][5/25]	Time 0.558 (0.573)	Data 0.003 (0.137)	Loss 0.1745 (0.1768)	Acc@1 99.658 (99.601)	Acc@5 100.000 (100.000)
Epoch: [179][6/25]	Time 0.582 (0.574)	Data 0.005 (0.118)	Loss 0.1764 (0.1767)	Acc@1 99.658 (99.609)	Acc@5 100.000 (100.000)
Epoch: [179][7/25]	Time 0.579 (0.575)	Data 0.007 (0.105)	Loss 0.1776 (0.1768)	Acc@1 99.512 (99.597)	Acc@5 100.000 (100.000)
Epoch: [179][8/25]	Time 0.609 (0.578)	Data 0.006 (0.094)	Loss 0.1796 (0.1771)	Acc@1 99.463 (99.582)	Acc@5 100.000 (100.000)
Epoch: [179][9/25]	Time 0.553 (0.576)	Data 0.005 (0.085)	Loss 0.1779 (0.1772)	Acc@1 99.609 (99.585)	Acc@5 100.000 (100.000)
Epoch: [179][10/25]	Time 0.598 (0.578)	Data 0.004 (0.077)	Loss 0.1781 (0.1773)	Acc@1 99.512 (99.578)	Acc@5 100.000 (100.000)
Epoch: [179][11/25]	Time 0.556 (0.576)	Data 0.005 (0.071)	Loss 0.1745 (0.1770)	Acc@1 99.756 (99.593)	Acc@5 100.000 (100.000)
Epoch: [179][12/25]	Time 0.588 (0.577)	Data 0.004 (0.066)	Loss 0.1813 (0.1774)	Acc@1 99.512 (99.587)	Acc@5 100.000 (100.000)
Epoch: [179][13/25]	Time 0.588 (0.578)	Data 0.007 (0.062)	Loss 0.1792 (0.1775)	Acc@1 99.463 (99.578)	Acc@5 100.000 (100.000)
Epoch: [179][14/25]	Time 0.567 (0.577)	Data 0.006 (0.058)	Loss 0.1770 (0.1775)	Acc@1 99.463 (99.570)	Acc@5 100.000 (100.000)
Epoch: [179][15/25]	Time 0.554 (0.576)	Data 0.006 (0.055)	Loss 0.1752 (0.1773)	Acc@1 99.707 (99.579)	Acc@5 100.000 (100.000)
Epoch: [179][16/25]	Time 0.598 (0.577)	Data 0.005 (0.052)	Loss 0.1797 (0.1775)	Acc@1 99.609 (99.581)	Acc@5 100.000 (100.000)
Epoch: [179][17/25]	Time 0.581 (0.577)	Data 0.006 (0.049)	Loss 0.1769 (0.1774)	Acc@1 99.609 (99.582)	Acc@5 100.000 (100.000)
Epoch: [179][18/25]	Time 0.573 (0.577)	Data 0.005 (0.047)	Loss 0.1831 (0.1777)	Acc@1 99.219 (99.563)	Acc@5 100.000 (100.000)
Epoch: [179][19/25]	Time 0.587 (0.577)	Data 0.004 (0.045)	Loss 0.1800 (0.1778)	Acc@1 99.414 (99.556)	Acc@5 100.000 (100.000)
Epoch: [179][20/25]	Time 0.579 (0.578)	Data 0.005 (0.043)	Loss 0.1753 (0.1777)	Acc@1 99.561 (99.556)	Acc@5 100.000 (100.000)
Epoch: [179][21/25]	Time 0.571 (0.577)	Data 0.007 (0.041)	Loss 0.1738 (0.1775)	Acc@1 99.854 (99.569)	Acc@5 100.000 (100.000)
Epoch: [179][22/25]	Time 0.531 (0.575)	Data 0.004 (0.040)	Loss 0.1776 (0.1776)	Acc@1 99.707 (99.575)	Acc@5 100.000 (100.000)
Epoch: [179][23/25]	Time 0.622 (0.577)	Data 0.006 (0.038)	Loss 0.1825 (0.1778)	Acc@1 99.463 (99.571)	Acc@5 99.951 (99.998)
Epoch: [179][24/25]	Time 0.307 (0.566)	Data 0.006 (0.037)	Loss 0.1754 (0.1777)	Acc@1 99.764 (99.574)	Acc@5 100.000 (99.998)

Epoch: [180 | 180] LR: 0.000400
Epoch: [180][0/25]	Time 0.589 (0.589)	Data 0.602 (0.602)	Loss 0.1793 (0.1793)	Acc@1 99.561 (99.561)	Acc@5 100.000 (100.000)
Epoch: [180][1/25]	Time 0.601 (0.595)	Data 0.007 (0.304)	Loss 0.1748 (0.1771)	Acc@1 99.707 (99.634)	Acc@5 100.000 (100.000)
Epoch: [180][2/25]	Time 0.553 (0.581)	Data 0.005 (0.205)	Loss 0.1755 (0.1766)	Acc@1 99.707 (99.658)	Acc@5 100.000 (100.000)
Epoch: [180][3/25]	Time 0.585 (0.582)	Data 0.005 (0.155)	Loss 0.1726 (0.1756)	Acc@1 99.805 (99.695)	Acc@5 100.000 (100.000)
Epoch: [180][4/25]	Time 0.580 (0.582)	Data 0.008 (0.125)	Loss 0.1782 (0.1761)	Acc@1 99.609 (99.678)	Acc@5 100.000 (100.000)
Epoch: [180][5/25]	Time 0.577 (0.581)	Data 0.005 (0.105)	Loss 0.1758 (0.1760)	Acc@1 99.658 (99.674)	Acc@5 100.000 (100.000)
Epoch: [180][6/25]	Time 0.568 (0.579)	Data 0.006 (0.091)	Loss 0.1813 (0.1768)	Acc@1 99.365 (99.630)	Acc@5 100.000 (100.000)
Epoch: [180][7/25]	Time 0.595 (0.581)	Data 0.003 (0.080)	Loss 0.1785 (0.1770)	Acc@1 99.756 (99.646)	Acc@5 100.000 (100.000)
Epoch: [180][8/25]	Time 0.613 (0.585)	Data 0.006 (0.072)	Loss 0.1787 (0.1772)	Acc@1 99.463 (99.626)	Acc@5 100.000 (100.000)
Epoch: [180][9/25]	Time 0.525 (0.579)	Data 0.007 (0.065)	Loss 0.1792 (0.1774)	Acc@1 99.561 (99.619)	Acc@5 100.000 (100.000)
Epoch: [180][10/25]	Time 0.568 (0.578)	Data 0.008 (0.060)	Loss 0.1748 (0.1772)	Acc@1 99.658 (99.623)	Acc@5 100.000 (100.000)
Epoch: [180][11/25]	Time 0.553 (0.576)	Data 0.004 (0.055)	Loss 0.1794 (0.1773)	Acc@1 99.561 (99.618)	Acc@5 100.000 (100.000)
Epoch: [180][12/25]	Time 0.601 (0.578)	Data 0.008 (0.052)	Loss 0.1750 (0.1772)	Acc@1 99.756 (99.628)	Acc@5 100.000 (100.000)
Epoch: [180][13/25]	Time 0.580 (0.578)	Data 0.006 (0.049)	Loss 0.1763 (0.1771)	Acc@1 99.658 (99.630)	Acc@5 100.000 (100.000)
Epoch: [180][14/25]	Time 0.560 (0.577)	Data 0.006 (0.046)	Loss 0.1792 (0.1772)	Acc@1 99.463 (99.619)	Acc@5 100.000 (100.000)
Epoch: [180][15/25]	Time 0.563 (0.576)	Data 0.004 (0.043)	Loss 0.1768 (0.1772)	Acc@1 99.561 (99.615)	Acc@5 100.000 (100.000)
Epoch: [180][16/25]	Time 0.621 (0.578)	Data 0.008 (0.041)	Loss 0.1741 (0.1770)	Acc@1 99.707 (99.621)	Acc@5 100.000 (100.000)
Epoch: [180][17/25]	Time 0.583 (0.579)	Data 0.006 (0.039)	Loss 0.1791 (0.1771)	Acc@1 99.756 (99.628)	Acc@5 100.000 (100.000)
Epoch: [180][18/25]	Time 0.583 (0.579)	Data 0.005 (0.037)	Loss 0.1773 (0.1771)	Acc@1 99.658 (99.630)	Acc@5 100.000 (100.000)
Epoch: [180][19/25]	Time 0.576 (0.579)	Data 0.004 (0.036)	Loss 0.1767 (0.1771)	Acc@1 99.609 (99.629)	Acc@5 100.000 (100.000)
Epoch: [180][20/25]	Time 0.590 (0.579)	Data 0.005 (0.034)	Loss 0.1767 (0.1771)	Acc@1 99.609 (99.628)	Acc@5 100.000 (100.000)
Epoch: [180][21/25]	Time 0.598 (0.580)	Data 0.004 (0.033)	Loss 0.1804 (0.1773)	Acc@1 99.561 (99.625)	Acc@5 99.951 (99.998)
Epoch: [180][22/25]	Time 0.596 (0.581)	Data 0.005 (0.032)	Loss 0.1754 (0.1772)	Acc@1 99.561 (99.622)	Acc@5 100.000 (99.998)
Epoch: [180][23/25]	Time 0.588 (0.581)	Data 0.004 (0.030)	Loss 0.1771 (0.1772)	Acc@1 99.658 (99.624)	Acc@5 100.000 (99.998)
Epoch: [180][24/25]	Time 0.368 (0.573)	Data 0.007 (0.030)	Loss 0.1770 (0.1772)	Acc@1 99.646 (99.624)	Acc@5 100.000 (99.998)

  2048
  92.76
 15.102s  no display found. Using non-interactive Agg backend
[5, 5, 5]
gpu id:0
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 6736
used     : 4283


Cifar10: True; cifar100: False
False
Files already downloaded and verified

Arch Num:  [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/25]	Time 0.688 (0.688)	Data 0.763 (0.763)	Loss 3.2414 (3.2414)	Acc@1 10.645 (10.645)	Acc@5 50.146 (50.146)
Epoch: [1][1/25]	Time 0.611 (0.649)	Data 0.004 (0.383)	Loss 4.4264 (3.8339)	Acc@1 9.766 (10.205)	Acc@5 50.293 (50.220)
Epoch: [1][2/25]	Time 0.605 (0.634)	Data 0.005 (0.257)	Loss 5.3974 (4.3551)	Acc@1 10.840 (10.417)	Acc@5 50.879 (50.439)
Epoch: [1][3/25]	Time 0.586 (0.622)	Data 0.004 (0.194)	Loss 4.8417 (4.4767)	Acc@1 8.789 (10.010)	Acc@5 50.439 (50.439)
Epoch: [1][4/25]	Time 0.599 (0.618)	Data 0.006 (0.156)	Loss 4.2529 (4.4320)	Acc@1 10.547 (10.117)	Acc@5 50.439 (50.439)
Epoch: [1][5/25]	Time 0.598 (0.614)	Data 0.004 (0.131)	Loss 4.4142 (4.4290)	Acc@1 9.766 (10.059)	Acc@5 48.486 (50.114)
Epoch: [1][6/25]	Time 0.599 (0.612)	Data 0.005 (0.113)	Loss 3.5451 (4.3027)	Acc@1 8.252 (9.801)	Acc@5 48.877 (49.937)
Epoch: [1][7/25]	Time 0.602 (0.611)	Data 0.005 (0.099)	Loss 3.3300 (4.1811)	Acc@1 9.619 (9.778)	Acc@5 49.609 (49.896)
Epoch: [1][8/25]	Time 0.532 (0.602)	Data 0.008 (0.089)	Loss 3.2701 (4.0799)	Acc@1 11.816 (10.004)	Acc@5 50.195 (49.929)
Epoch: [1][9/25]	Time 0.598 (0.602)	Data 0.005 (0.081)	Loss 3.3615 (4.0081)	Acc@1 9.619 (9.966)	Acc@5 48.535 (49.790)
Epoch: [1][10/25]	Time 0.581 (0.600)	Data 0.008 (0.074)	Loss 3.1036 (3.9258)	Acc@1 10.449 (10.010)	Acc@5 50.439 (49.849)
Epoch: [1][11/25]	Time 0.604 (0.600)	Data 0.006 (0.068)	Loss 3.1109 (3.8579)	Acc@1 9.766 (9.989)	Acc@5 50.684 (49.919)
Epoch: [1][12/25]	Time 0.596 (0.600)	Data 0.007 (0.064)	Loss 3.0511 (3.7959)	Acc@1 10.303 (10.014)	Acc@5 49.268 (49.869)
Epoch: [1][13/25]	Time 0.609 (0.601)	Data 0.004 (0.059)	Loss 3.0087 (3.7396)	Acc@1 9.570 (9.982)	Acc@5 51.953 (50.017)
Epoch: [1][14/25]	Time 0.578 (0.599)	Data 0.006 (0.056)	Loss 3.0410 (3.6931)	Acc@1 10.889 (10.042)	Acc@5 49.463 (49.980)
Epoch: [1][15/25]	Time 0.615 (0.600)	Data 0.008 (0.053)	Loss 3.0308 (3.6517)	Acc@1 10.449 (10.068)	Acc@5 50.928 (50.040)
Epoch: [1][16/25]	Time 0.596 (0.600)	Data 0.005 (0.050)	Loss 3.0051 (3.6136)	Acc@1 10.010 (10.064)	Acc@5 51.074 (50.101)
Epoch: [1][17/25]	Time 0.612 (0.601)	Data 0.005 (0.048)	Loss 3.0069 (3.5799)	Acc@1 10.742 (10.102)	Acc@5 51.953 (50.203)
Epoch: [1][18/25]	Time 0.599 (0.600)	Data 0.005 (0.045)	Loss 2.9948 (3.5491)	Acc@1 9.766 (10.084)	Acc@5 52.051 (50.301)
Epoch: [1][19/25]	Time 0.605 (0.601)	Data 0.003 (0.043)	Loss 2.9833 (3.5208)	Acc@1 9.961 (10.078)	Acc@5 51.074 (50.339)
Epoch: [1][20/25]	Time 0.610 (0.601)	Data 0.008 (0.042)	Loss 2.9778 (3.4950)	Acc@1 10.400 (10.093)	Acc@5 51.807 (50.409)
Epoch: [1][21/25]	Time 0.580 (0.600)	Data 0.006 (0.040)	Loss 2.9763 (3.4714)	Acc@1 12.744 (10.214)	Acc@5 50.732 (50.424)
Epoch: [1][22/25]	Time 0.607 (0.600)	Data 0.004 (0.038)	Loss 2.9702 (3.4496)	Acc@1 10.986 (10.248)	Acc@5 54.980 (50.622)
Epoch: [1][23/25]	Time 0.588 (0.600)	Data 0.006 (0.037)	Loss 2.9687 (3.4296)	Acc@1 12.842 (10.356)	Acc@5 58.887 (50.966)
Epoch: [1][24/25]	Time 0.369 (0.591)	Data 0.007 (0.036)	Loss 2.9497 (3.4214)	Acc@1 14.033 (10.418)	Acc@5 59.316 (51.108)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/25]	Time 0.593 (0.593)	Data 0.626 (0.626)	Loss 2.9639 (2.9639)	Acc@1 12.402 (12.402)	Acc@5 58.887 (58.887)
Epoch: [2][1/25]	Time 0.575 (0.584)	Data 0.007 (0.316)	Loss 2.9521 (2.9580)	Acc@1 13.086 (12.744)	Acc@5 58.496 (58.691)
Epoch: [2][2/25]	Time 0.589 (0.586)	Data 0.004 (0.212)	Loss 2.9536 (2.9565)	Acc@1 13.135 (12.874)	Acc@5 58.691 (58.691)
Epoch: [2][3/25]	Time 0.620 (0.594)	Data 0.004 (0.160)	Loss 2.9463 (2.9540)	Acc@1 12.988 (12.903)	Acc@5 58.740 (58.704)
Epoch: [2][4/25]	Time 0.573 (0.590)	Data 0.008 (0.130)	Loss 2.9254 (2.9483)	Acc@1 14.355 (13.193)	Acc@5 61.816 (59.326)
Epoch: [2][5/25]	Time 0.599 (0.591)	Data 0.004 (0.109)	Loss 2.9222 (2.9439)	Acc@1 14.307 (13.379)	Acc@5 65.137 (60.295)
Epoch: [2][6/25]	Time 0.585 (0.591)	Data 0.005 (0.094)	Loss 2.9216 (2.9407)	Acc@1 13.623 (13.414)	Acc@5 64.990 (60.965)
Epoch: [2][7/25]	Time 0.595 (0.591)	Data 0.005 (0.083)	Loss 2.9132 (2.9373)	Acc@1 15.332 (13.654)	Acc@5 66.016 (61.597)
Epoch: [2][8/25]	Time 0.597 (0.592)	Data 0.006 (0.074)	Loss 2.8901 (2.9320)	Acc@1 15.088 (13.813)	Acc@5 66.162 (62.104)
Epoch: [2][9/25]	Time 0.631 (0.596)	Data 0.004 (0.067)	Loss 2.8859 (2.9274)	Acc@1 14.355 (13.867)	Acc@5 67.822 (62.676)
Epoch: [2][10/25]	Time 0.585 (0.595)	Data 0.006 (0.062)	Loss 2.8773 (2.9229)	Acc@1 14.111 (13.889)	Acc@5 67.822 (63.144)
Epoch: [2][11/25]	Time 0.595 (0.595)	Data 0.007 (0.057)	Loss 2.8660 (2.9181)	Acc@1 15.771 (14.046)	Acc@5 70.801 (63.782)
Epoch: [2][12/25]	Time 0.580 (0.594)	Data 0.007 (0.053)	Loss 2.8620 (2.9138)	Acc@1 15.918 (14.190)	Acc@5 66.748 (64.010)
Epoch: [2][13/25]	Time 0.631 (0.596)	Data 0.004 (0.050)	Loss 2.8593 (2.9099)	Acc@1 16.650 (14.366)	Acc@5 66.699 (64.202)
Epoch: [2][14/25]	Time 0.601 (0.597)	Data 0.005 (0.047)	Loss 2.8491 (2.9059)	Acc@1 15.576 (14.447)	Acc@5 65.723 (64.303)
Epoch: [2][15/25]	Time 0.576 (0.595)	Data 0.007 (0.044)	Loss 2.8203 (2.9005)	Acc@1 16.260 (14.560)	Acc@5 72.852 (64.838)
Epoch: [2][16/25]	Time 0.632 (0.597)	Data 0.007 (0.042)	Loss 2.8251 (2.8961)	Acc@1 15.820 (14.634)	Acc@5 70.557 (65.174)
Epoch: [2][17/25]	Time 0.573 (0.596)	Data 0.004 (0.040)	Loss 2.8130 (2.8915)	Acc@1 15.918 (14.705)	Acc@5 71.777 (65.541)
Epoch: [2][18/25]	Time 0.618 (0.597)	Data 0.007 (0.038)	Loss 2.7965 (2.8865)	Acc@1 17.822 (14.869)	Acc@5 73.242 (65.946)
Epoch: [2][19/25]	Time 0.610 (0.598)	Data 0.003 (0.037)	Loss 2.7944 (2.8819)	Acc@1 18.311 (15.042)	Acc@5 71.338 (66.216)
Epoch: [2][20/25]	Time 0.609 (0.598)	Data 0.006 (0.035)	Loss 2.7922 (2.8776)	Acc@1 16.553 (15.113)	Acc@5 72.559 (66.518)
Epoch: [2][21/25]	Time 0.591 (0.598)	Data 0.007 (0.034)	Loss 2.7945 (2.8738)	Acc@1 17.383 (15.217)	Acc@5 71.777 (66.757)
Epoch: [2][22/25]	Time 0.587 (0.598)	Data 0.008 (0.033)	Loss 2.7988 (2.8706)	Acc@1 16.602 (15.277)	Acc@5 72.998 (67.028)
Epoch: [2][23/25]	Time 0.598 (0.598)	Data 0.004 (0.031)	Loss 2.7633 (2.8661)	Acc@1 18.311 (15.403)	Acc@5 73.828 (67.312)
Epoch: [2][24/25]	Time 0.360 (0.588)	Data 0.007 (0.030)	Loss 2.7610 (2.8643)	Acc@1 18.160 (15.450)	Acc@5 73.113 (67.410)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/25]	Time 0.616 (0.616)	Data 0.679 (0.679)	Loss 2.7706 (2.7706)	Acc@1 17.139 (17.139)	Acc@5 74.072 (74.072)
Epoch: [3][1/25]	Time 0.605 (0.611)	Data 0.003 (0.341)	Loss 2.7673 (2.7690)	Acc@1 19.678 (18.408)	Acc@5 72.754 (73.413)
Epoch: [3][2/25]	Time 0.579 (0.600)	Data 0.006 (0.229)	Loss 2.7367 (2.7582)	Acc@1 20.312 (19.043)	Acc@5 74.365 (73.730)
Epoch: [3][3/25]	Time 0.615 (0.604)	Data 0.007 (0.174)	Loss 2.7332 (2.7520)	Acc@1 18.066 (18.799)	Acc@5 74.561 (73.938)
Epoch: [3][4/25]	Time 0.695 (0.622)	Data 0.006 (0.140)	Loss 2.7444 (2.7505)	Acc@1 21.191 (19.277)	Acc@5 74.316 (74.014)
Epoch: [3][5/25]	Time 0.701 (0.635)	Data 0.006 (0.118)	Loss 2.7700 (2.7537)	Acc@1 22.314 (19.784)	Acc@5 75.684 (74.292)
Epoch: [3][6/25]	Time 0.564 (0.625)	Data 0.004 (0.102)	Loss 2.7098 (2.7474)	Acc@1 23.096 (20.257)	Acc@5 74.365 (74.302)
Epoch: [3][7/25]	Time 0.594 (0.621)	Data 0.003 (0.089)	Loss 2.7234 (2.7444)	Acc@1 22.217 (20.502)	Acc@5 74.902 (74.377)
Epoch: [3][8/25]	Time 0.557 (0.614)	Data 0.005 (0.080)	Loss 2.7130 (2.7409)	Acc@1 22.900 (20.768)	Acc@5 74.072 (74.344)
Epoch: [3][9/25]	Time 0.595 (0.612)	Data 0.004 (0.072)	Loss 2.6967 (2.7365)	Acc@1 21.777 (20.869)	Acc@5 75.781 (74.487)
Epoch: [3][10/25]	Time 0.604 (0.611)	Data 0.004 (0.066)	Loss 2.7107 (2.7342)	Acc@1 23.047 (21.067)	Acc@5 74.707 (74.507)
Epoch: [3][11/25]	Time 0.631 (0.613)	Data 0.006 (0.061)	Loss 2.6930 (2.7307)	Acc@1 23.535 (21.273)	Acc@5 76.221 (74.650)
Epoch: [3][12/25]	Time 0.632 (0.615)	Data 0.005 (0.057)	Loss 2.6859 (2.7273)	Acc@1 23.242 (21.424)	Acc@5 75.293 (74.700)
Epoch: [3][13/25]	Time 0.615 (0.615)	Data 0.008 (0.053)	Loss 2.7056 (2.7257)	Acc@1 22.803 (21.523)	Acc@5 75.439 (74.752)
Epoch: [3][14/25]	Time 0.639 (0.616)	Data 0.007 (0.050)	Loss 2.6883 (2.7232)	Acc@1 22.119 (21.562)	Acc@5 77.588 (74.941)
Epoch: [3][15/25]	Time 0.619 (0.616)	Data 0.005 (0.047)	Loss 2.6733 (2.7201)	Acc@1 21.143 (21.536)	Acc@5 78.125 (75.140)
Epoch: [3][16/25]	Time 0.575 (0.614)	Data 0.008 (0.045)	Loss 2.6586 (2.7165)	Acc@1 24.023 (21.683)	Acc@5 77.734 (75.293)
Epoch: [3][17/25]	Time 0.619 (0.614)	Data 0.006 (0.043)	Loss 2.6305 (2.7117)	Acc@1 25.342 (21.886)	Acc@5 78.906 (75.494)
Epoch: [3][18/25]	Time 0.617 (0.614)	Data 0.008 (0.041)	Loss 2.6258 (2.7072)	Acc@1 25.586 (22.081)	Acc@5 78.027 (75.627)
Epoch: [3][19/25]	Time 0.597 (0.613)	Data 0.006 (0.039)	Loss 2.6523 (2.7045)	Acc@1 23.926 (22.173)	Acc@5 77.734 (75.732)
Epoch: [3][20/25]	Time 0.616 (0.614)	Data 0.008 (0.038)	Loss 2.6466 (2.7017)	Acc@1 23.584 (22.240)	Acc@5 76.709 (75.779)
Epoch: [3][21/25]	Time 0.623 (0.614)	Data 0.004 (0.036)	Loss 2.6439 (2.6991)	Acc@1 22.852 (22.268)	Acc@5 77.686 (75.866)
Epoch: [3][22/25]	Time 0.599 (0.613)	Data 0.007 (0.035)	Loss 2.6382 (2.6964)	Acc@1 24.365 (22.359)	Acc@5 77.588 (75.940)
Epoch: [3][23/25]	Time 0.565 (0.611)	Data 0.007 (0.034)	Loss 2.6479 (2.6944)	Acc@1 24.268 (22.439)	Acc@5 77.539 (76.007)
Epoch: [3][24/25]	Time 0.353 (0.601)	Data 0.008 (0.033)	Loss 2.6242 (2.6932)	Acc@1 22.759 (22.444)	Acc@5 79.599 (76.068)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/25]	Time 0.597 (0.597)	Data 0.597 (0.597)	Loss 2.6001 (2.6001)	Acc@1 26.758 (26.758)	Acc@5 79.150 (79.150)
Epoch: [4][1/25]	Time 0.620 (0.609)	Data 0.007 (0.302)	Loss 2.6240 (2.6120)	Acc@1 24.707 (25.732)	Acc@5 78.613 (78.882)
Epoch: [4][2/25]	Time 0.633 (0.617)	Data 0.007 (0.204)	Loss 2.6096 (2.6112)	Acc@1 26.172 (25.879)	Acc@5 78.955 (78.906)
Epoch: [4][3/25]	Time 0.585 (0.609)	Data 0.004 (0.154)	Loss 2.6238 (2.6144)	Acc@1 25.293 (25.732)	Acc@5 77.100 (78.455)
Epoch: [4][4/25]	Time 0.594 (0.606)	Data 0.007 (0.125)	Loss 2.6132 (2.6141)	Acc@1 24.365 (25.459)	Acc@5 78.369 (78.438)
Epoch: [4][5/25]	Time 0.598 (0.605)	Data 0.007 (0.105)	Loss 2.6085 (2.6132)	Acc@1 24.170 (25.244)	Acc@5 79.932 (78.687)
Epoch: [4][6/25]	Time 0.595 (0.603)	Data 0.006 (0.091)	Loss 2.5747 (2.6077)	Acc@1 26.123 (25.370)	Acc@5 81.836 (79.136)
Epoch: [4][7/25]	Time 0.578 (0.600)	Data 0.007 (0.080)	Loss 2.5798 (2.6042)	Acc@1 25.439 (25.378)	Acc@5 79.443 (79.175)
Epoch: [4][8/25]	Time 0.607 (0.601)	Data 0.008 (0.072)	Loss 2.5550 (2.5987)	Acc@1 27.100 (25.570)	Acc@5 80.469 (79.319)
Epoch: [4][9/25]	Time 0.609 (0.602)	Data 0.008 (0.066)	Loss 2.5588 (2.5947)	Acc@1 27.100 (25.723)	Acc@5 80.029 (79.390)
Epoch: [4][10/25]	Time 0.612 (0.603)	Data 0.004 (0.060)	Loss 2.5752 (2.5930)	Acc@1 25.586 (25.710)	Acc@5 78.467 (79.306)
Epoch: [4][11/25]	Time 0.597 (0.602)	Data 0.005 (0.056)	Loss 2.5767 (2.5916)	Acc@1 26.123 (25.745)	Acc@5 79.346 (79.309)
Epoch: [4][12/25]	Time 0.632 (0.604)	Data 0.004 (0.052)	Loss 2.5684 (2.5898)	Acc@1 24.561 (25.654)	Acc@5 79.102 (79.293)
Epoch: [4][13/25]	Time 0.584 (0.603)	Data 0.006 (0.048)	Loss 2.5644 (2.5880)	Acc@1 25.146 (25.617)	Acc@5 78.906 (79.265)
Epoch: [4][14/25]	Time 0.634 (0.605)	Data 0.004 (0.045)	Loss 2.5745 (2.5871)	Acc@1 24.805 (25.563)	Acc@5 79.102 (79.255)
Epoch: [4][15/25]	Time 0.594 (0.604)	Data 0.006 (0.043)	Loss 2.5490 (2.5847)	Acc@1 26.123 (25.598)	Acc@5 79.199 (79.251)
Epoch: [4][16/25]	Time 0.593 (0.604)	Data 0.007 (0.041)	Loss 2.5605 (2.5833)	Acc@1 25.928 (25.618)	Acc@5 79.785 (79.283)
Epoch: [4][17/25]	Time 0.621 (0.605)	Data 0.004 (0.039)	Loss 2.5161 (2.5796)	Acc@1 28.223 (25.762)	Acc@5 81.787 (79.422)
Epoch: [4][18/25]	Time 0.628 (0.606)	Data 0.005 (0.037)	Loss 2.5460 (2.5778)	Acc@1 27.441 (25.851)	Acc@5 79.883 (79.446)
Epoch: [4][19/25]	Time 0.573 (0.604)	Data 0.005 (0.035)	Loss 2.5390 (2.5759)	Acc@1 27.588 (25.938)	Acc@5 80.859 (79.517)
Epoch: [4][20/25]	Time 0.612 (0.605)	Data 0.007 (0.034)	Loss 2.5167 (2.5730)	Acc@1 27.832 (26.028)	Acc@5 81.885 (79.629)
Epoch: [4][21/25]	Time 0.598 (0.604)	Data 0.007 (0.033)	Loss 2.5007 (2.5698)	Acc@1 26.562 (26.052)	Acc@5 81.836 (79.730)
Epoch: [4][22/25]	Time 0.603 (0.604)	Data 0.004 (0.032)	Loss 2.4972 (2.5666)	Acc@1 27.637 (26.121)	Acc@5 81.201 (79.794)
Epoch: [4][23/25]	Time 0.605 (0.604)	Data 0.004 (0.030)	Loss 2.4726 (2.5627)	Acc@1 29.541 (26.263)	Acc@5 83.154 (79.934)
Epoch: [4][24/25]	Time 0.374 (0.595)	Data 0.004 (0.029)	Loss 2.4558 (2.5609)	Acc@1 29.363 (26.316)	Acc@5 83.491 (79.994)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/25]	Time 0.611 (0.611)	Data 0.608 (0.608)	Loss 2.4729 (2.4729)	Acc@1 30.273 (30.273)	Acc@5 82.520 (82.520)
Epoch: [5][1/25]	Time 0.638 (0.625)	Data 0.006 (0.307)	Loss 2.4704 (2.4716)	Acc@1 28.369 (29.321)	Acc@5 83.105 (82.812)
Epoch: [5][2/25]	Time 0.633 (0.627)	Data 0.006 (0.206)	Loss 2.4994 (2.4809)	Acc@1 27.686 (28.776)	Acc@5 81.934 (82.520)
Epoch: [5][3/25]	Time 0.612 (0.624)	Data 0.003 (0.156)	Loss 2.4614 (2.4760)	Acc@1 27.637 (28.491)	Acc@5 82.373 (82.483)
Epoch: [5][4/25]	Time 0.633 (0.625)	Data 0.007 (0.126)	Loss 2.4139 (2.4636)	Acc@1 30.811 (28.955)	Acc@5 84.766 (82.939)
Epoch: [5][5/25]	Time 0.594 (0.620)	Data 0.004 (0.106)	Loss 2.4550 (2.4621)	Acc@1 28.516 (28.882)	Acc@5 82.764 (82.910)
Epoch: [5][6/25]	Time 0.631 (0.622)	Data 0.004 (0.091)	Loss 2.4787 (2.4645)	Acc@1 28.271 (28.795)	Acc@5 83.789 (83.036)
Epoch: [5][7/25]	Time 0.604 (0.620)	Data 0.004 (0.080)	Loss 2.4616 (2.4641)	Acc@1 27.100 (28.583)	Acc@5 81.494 (82.843)
Epoch: [5][8/25]	Time 0.584 (0.616)	Data 0.006 (0.072)	Loss 2.4404 (2.4615)	Acc@1 29.346 (28.668)	Acc@5 82.959 (82.856)
Epoch: [5][9/25]	Time 0.585 (0.613)	Data 0.008 (0.066)	Loss 2.4398 (2.4593)	Acc@1 29.443 (28.745)	Acc@5 83.789 (82.949)
Epoch: [5][10/25]	Time 0.626 (0.614)	Data 0.008 (0.060)	Loss 2.4261 (2.4563)	Acc@1 29.150 (28.782)	Acc@5 83.203 (82.972)
Epoch: [5][11/25]	Time 0.608 (0.613)	Data 0.006 (0.056)	Loss 2.4357 (2.4546)	Acc@1 29.688 (28.857)	Acc@5 83.203 (82.992)
Epoch: [5][12/25]	Time 0.587 (0.611)	Data 0.007 (0.052)	Loss 2.4014 (2.4505)	Acc@1 30.127 (28.955)	Acc@5 83.984 (83.068)
Epoch: [5][13/25]	Time 0.621 (0.612)	Data 0.006 (0.049)	Loss 2.4178 (2.4482)	Acc@1 30.664 (29.077)	Acc@5 83.545 (83.102)
Epoch: [5][14/25]	Time 0.579 (0.610)	Data 0.007 (0.046)	Loss 2.4203 (2.4463)	Acc@1 29.395 (29.098)	Acc@5 83.594 (83.135)
Epoch: [5][15/25]	Time 0.594 (0.609)	Data 0.005 (0.043)	Loss 2.3586 (2.4408)	Acc@1 32.129 (29.288)	Acc@5 84.766 (83.237)
Epoch: [5][16/25]	Time 0.634 (0.610)	Data 0.007 (0.041)	Loss 2.3708 (2.4367)	Acc@1 33.398 (29.530)	Acc@5 84.863 (83.332)
Epoch: [5][17/25]	Time 0.602 (0.610)	Data 0.004 (0.039)	Loss 2.3841 (2.4338)	Acc@1 30.029 (29.557)	Acc@5 85.254 (83.439)
Epoch: [5][18/25]	Time 0.610 (0.610)	Data 0.004 (0.037)	Loss 2.3861 (2.4313)	Acc@1 33.594 (29.770)	Acc@5 83.154 (83.424)
Epoch: [5][19/25]	Time 0.609 (0.610)	Data 0.006 (0.036)	Loss 2.3915 (2.4293)	Acc@1 30.762 (29.819)	Acc@5 83.594 (83.433)
Epoch: [5][20/25]	Time 0.602 (0.609)	Data 0.005 (0.034)	Loss 2.3762 (2.4268)	Acc@1 31.348 (29.892)	Acc@5 84.277 (83.473)
Epoch: [5][21/25]	Time 0.629 (0.610)	Data 0.004 (0.033)	Loss 2.3726 (2.4243)	Acc@1 31.055 (29.945)	Acc@5 84.912 (83.538)
Epoch: [5][22/25]	Time 0.590 (0.609)	Data 0.004 (0.032)	Loss 2.3794 (2.4223)	Acc@1 30.811 (29.983)	Acc@5 83.594 (83.541)
Epoch: [5][23/25]	Time 0.628 (0.610)	Data 0.007 (0.031)	Loss 2.3383 (2.4188)	Acc@1 31.689 (30.054)	Acc@5 86.475 (83.663)
Epoch: [5][24/25]	Time 0.337 (0.599)	Data 0.008 (0.030)	Loss 2.4165 (2.4188)	Acc@1 29.127 (30.038)	Acc@5 83.491 (83.660)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/25]	Time 0.613 (0.613)	Data 0.605 (0.605)	Loss 2.4330 (2.4330)	Acc@1 29.102 (29.102)	Acc@5 83.057 (83.057)
Epoch: [6][1/25]	Time 0.566 (0.590)	Data 0.004 (0.305)	Loss 2.3964 (2.4147)	Acc@1 31.641 (30.371)	Acc@5 83.008 (83.032)
Epoch: [6][2/25]	Time 0.613 (0.597)	Data 0.006 (0.205)	Loss 2.3778 (2.4024)	Acc@1 31.787 (30.843)	Acc@5 83.643 (83.236)
Epoch: [6][3/25]	Time 0.559 (0.588)	Data 0.007 (0.156)	Loss 2.3590 (2.3915)	Acc@1 31.396 (30.981)	Acc@5 83.545 (83.313)
Epoch: [6][4/25]	Time 0.627 (0.596)	Data 0.005 (0.125)	Loss 2.3946 (2.3922)	Acc@1 30.908 (30.967)	Acc@5 83.154 (83.281)
Epoch: [6][5/25]	Time 0.609 (0.598)	Data 0.004 (0.105)	Loss 2.3662 (2.3878)	Acc@1 32.617 (31.242)	Acc@5 83.887 (83.382)
Epoch: [6][6/25]	Time 0.627 (0.602)	Data 0.006 (0.091)	Loss 2.3931 (2.3886)	Acc@1 30.713 (31.166)	Acc@5 83.447 (83.391)
Epoch: [6][7/25]	Time 0.597 (0.601)	Data 0.004 (0.080)	Loss 2.4049 (2.3906)	Acc@1 30.420 (31.073)	Acc@5 83.105 (83.356)
Epoch: [6][8/25]	Time 0.624 (0.604)	Data 0.004 (0.072)	Loss 2.3762 (2.3890)	Acc@1 32.178 (31.196)	Acc@5 84.277 (83.458)
Epoch: [6][9/25]	Time 0.600 (0.604)	Data 0.008 (0.065)	Loss 2.3447 (2.3846)	Acc@1 29.688 (31.045)	Acc@5 85.303 (83.643)
Epoch: [6][10/25]	Time 0.627 (0.606)	Data 0.004 (0.060)	Loss 2.3615 (2.3825)	Acc@1 30.908 (31.032)	Acc@5 82.910 (83.576)
Epoch: [6][11/25]	Time 0.594 (0.605)	Data 0.007 (0.055)	Loss 2.3379 (2.3788)	Acc@1 32.471 (31.152)	Acc@5 85.742 (83.757)
Epoch: [6][12/25]	Time 0.590 (0.603)	Data 0.005 (0.051)	Loss 2.3384 (2.3757)	Acc@1 32.764 (31.276)	Acc@5 84.570 (83.819)
Epoch: [6][13/25]	Time 0.640 (0.606)	Data 0.007 (0.048)	Loss 2.3620 (2.3747)	Acc@1 31.104 (31.264)	Acc@5 83.447 (83.793)
Epoch: [6][14/25]	Time 0.596 (0.605)	Data 0.006 (0.045)	Loss 2.3548 (2.3734)	Acc@1 31.152 (31.257)	Acc@5 84.473 (83.838)
Epoch: [6][15/25]	Time 0.614 (0.606)	Data 0.004 (0.043)	Loss 2.3260 (2.3704)	Acc@1 32.910 (31.360)	Acc@5 84.277 (83.865)
Epoch: [6][16/25]	Time 0.608 (0.606)	Data 0.005 (0.041)	Loss 2.3461 (2.3690)	Acc@1 29.346 (31.241)	Acc@5 85.156 (83.941)
Epoch: [6][17/25]	Time 0.564 (0.604)	Data 0.004 (0.039)	Loss 2.3024 (2.3653)	Acc@1 32.178 (31.293)	Acc@5 86.377 (84.077)
Epoch: [6][18/25]	Time 0.631 (0.605)	Data 0.007 (0.037)	Loss 2.3023 (2.3620)	Acc@1 32.764 (31.371)	Acc@5 85.840 (84.169)
Epoch: [6][19/25]	Time 0.616 (0.606)	Data 0.005 (0.035)	Loss 2.3140 (2.3596)	Acc@1 33.301 (31.467)	Acc@5 85.742 (84.248)
Epoch: [6][20/25]	Time 0.625 (0.607)	Data 0.005 (0.034)	Loss 2.3094 (2.3572)	Acc@1 34.814 (31.627)	Acc@5 84.668 (84.268)
Epoch: [6][21/25]	Time 0.597 (0.606)	Data 0.004 (0.033)	Loss 2.2784 (2.3536)	Acc@1 34.229 (31.745)	Acc@5 86.182 (84.355)
Epoch: [6][22/25]	Time 0.585 (0.605)	Data 0.005 (0.031)	Loss 2.3082 (2.3516)	Acc@1 32.031 (31.757)	Acc@5 85.352 (84.398)
Epoch: [6][23/25]	Time 0.640 (0.607)	Data 0.008 (0.030)	Loss 2.2802 (2.3486)	Acc@1 32.959 (31.807)	Acc@5 86.475 (84.485)
Epoch: [6][24/25]	Time 0.322 (0.595)	Data 0.004 (0.029)	Loss 2.2504 (2.3470)	Acc@1 32.901 (31.826)	Acc@5 88.090 (84.546)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/25]	Time 0.587 (0.587)	Data 0.631 (0.631)	Loss 2.2517 (2.2517)	Acc@1 34.033 (34.033)	Acc@5 85.693 (85.693)
Epoch: [7][1/25]	Time 0.600 (0.593)	Data 0.006 (0.319)	Loss 2.2621 (2.2569)	Acc@1 34.180 (34.106)	Acc@5 86.572 (86.133)
Epoch: [7][2/25]	Time 0.610 (0.599)	Data 0.006 (0.215)	Loss 2.2540 (2.2559)	Acc@1 35.791 (34.668)	Acc@5 86.670 (86.312)
Epoch: [7][3/25]	Time 0.590 (0.597)	Data 0.004 (0.162)	Loss 2.2450 (2.2532)	Acc@1 34.766 (34.692)	Acc@5 86.572 (86.377)
Epoch: [7][4/25]	Time 0.624 (0.602)	Data 0.006 (0.131)	Loss 2.2555 (2.2537)	Acc@1 33.252 (34.404)	Acc@5 87.500 (86.602)
Epoch: [7][5/25]	Time 0.606 (0.603)	Data 0.005 (0.110)	Loss 2.2356 (2.2507)	Acc@1 33.447 (34.245)	Acc@5 86.475 (86.580)
Epoch: [7][6/25]	Time 0.617 (0.605)	Data 0.006 (0.095)	Loss 2.2415 (2.2493)	Acc@1 34.277 (34.249)	Acc@5 86.572 (86.579)
Epoch: [7][7/25]	Time 0.624 (0.607)	Data 0.006 (0.084)	Loss 2.1892 (2.2418)	Acc@1 36.621 (34.546)	Acc@5 87.891 (86.743)
Epoch: [7][8/25]	Time 0.598 (0.606)	Data 0.004 (0.075)	Loss 2.2080 (2.2381)	Acc@1 35.156 (34.614)	Acc@5 87.598 (86.838)
Epoch: [7][9/25]	Time 0.602 (0.606)	Data 0.006 (0.068)	Loss 2.2254 (2.2368)	Acc@1 34.082 (34.561)	Acc@5 86.035 (86.758)
Epoch: [7][10/25]	Time 0.607 (0.606)	Data 0.005 (0.062)	Loss 2.2204 (2.2353)	Acc@1 34.961 (34.597)	Acc@5 87.061 (86.785)
Epoch: [7][11/25]	Time 0.602 (0.606)	Data 0.007 (0.058)	Loss 2.1736 (2.2302)	Acc@1 37.842 (34.867)	Acc@5 88.330 (86.914)
Epoch: [7][12/25]	Time 0.612 (0.606)	Data 0.007 (0.054)	Loss 2.1909 (2.2271)	Acc@1 35.352 (34.905)	Acc@5 87.988 (86.997)
Epoch: [7][13/25]	Time 0.614 (0.607)	Data 0.005 (0.050)	Loss 2.2060 (2.2256)	Acc@1 36.133 (34.992)	Acc@5 87.939 (87.064)
Epoch: [7][14/25]	Time 0.563 (0.604)	Data 0.005 (0.047)	Loss 2.1942 (2.2235)	Acc@1 36.182 (35.072)	Acc@5 87.061 (87.064)
Epoch: [7][15/25]	Time 0.642 (0.606)	Data 0.008 (0.045)	Loss 2.1562 (2.2193)	Acc@1 37.451 (35.220)	Acc@5 88.281 (87.140)
Epoch: [7][16/25]	Time 0.641 (0.608)	Data 0.007 (0.043)	Loss 2.2330 (2.2201)	Acc@1 35.693 (35.248)	Acc@5 86.719 (87.115)
Epoch: [7][17/25]	Time 0.593 (0.607)	Data 0.006 (0.041)	Loss 2.1708 (2.2174)	Acc@1 36.768 (35.333)	Acc@5 87.695 (87.147)
Epoch: [7][18/25]	Time 0.593 (0.607)	Data 0.004 (0.039)	Loss 2.1966 (2.2163)	Acc@1 34.619 (35.295)	Acc@5 86.816 (87.130)
Epoch: [7][19/25]	Time 0.619 (0.607)	Data 0.006 (0.037)	Loss 2.1783 (2.2144)	Acc@1 34.814 (35.271)	Acc@5 87.988 (87.173)
Epoch: [7][20/25]	Time 0.606 (0.607)	Data 0.006 (0.036)	Loss 2.1898 (2.2132)	Acc@1 35.352 (35.275)	Acc@5 87.256 (87.177)
Epoch: [7][21/25]	Time 0.580 (0.606)	Data 0.004 (0.034)	Loss 2.1975 (2.2125)	Acc@1 35.840 (35.301)	Acc@5 86.914 (87.165)
Epoch: [7][22/25]	Time 0.626 (0.607)	Data 0.004 (0.033)	Loss 2.1867 (2.2114)	Acc@1 37.158 (35.381)	Acc@5 86.230 (87.124)
Epoch: [7][23/25]	Time 0.609 (0.607)	Data 0.005 (0.032)	Loss 2.1548 (2.2090)	Acc@1 36.719 (35.437)	Acc@5 88.770 (87.193)
Epoch: [7][24/25]	Time 0.355 (0.597)	Data 0.005 (0.031)	Loss 2.1344 (2.2078)	Acc@1 38.443 (35.488)	Acc@5 87.382 (87.196)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/25]	Time 0.588 (0.588)	Data 0.621 (0.621)	Loss 2.1534 (2.1534)	Acc@1 36.182 (36.182)	Acc@5 88.281 (88.281)
Epoch: [8][1/25]	Time 0.609 (0.599)	Data 0.006 (0.313)	Loss 2.1466 (2.1500)	Acc@1 38.086 (37.134)	Acc@5 87.842 (88.062)
Epoch: [8][2/25]	Time 0.558 (0.585)	Data 0.005 (0.210)	Loss 2.1524 (2.1508)	Acc@1 37.598 (37.288)	Acc@5 88.330 (88.151)
Epoch: [8][3/25]	Time 0.593 (0.587)	Data 0.004 (0.159)	Loss 2.1466 (2.1497)	Acc@1 34.033 (36.475)	Acc@5 89.551 (88.501)
Epoch: [8][4/25]	Time 0.613 (0.592)	Data 0.005 (0.128)	Loss 2.1672 (2.1532)	Acc@1 35.889 (36.357)	Acc@5 88.623 (88.525)
Epoch: [8][5/25]	Time 0.594 (0.592)	Data 0.005 (0.107)	Loss 2.1320 (2.1497)	Acc@1 37.402 (36.532)	Acc@5 88.867 (88.582)
Epoch: [8][6/25]	Time 0.599 (0.593)	Data 0.004 (0.093)	Loss 2.0949 (2.1419)	Acc@1 37.158 (36.621)	Acc@5 89.404 (88.700)
Epoch: [8][7/25]	Time 0.584 (0.592)	Data 0.006 (0.082)	Loss 2.1997 (2.1491)	Acc@1 35.693 (36.505)	Acc@5 86.768 (88.458)
Epoch: [8][8/25]	Time 0.630 (0.596)	Data 0.005 (0.073)	Loss 2.2355 (2.1587)	Acc@1 34.180 (36.247)	Acc@5 85.742 (88.156)
Epoch: [8][9/25]	Time 0.596 (0.596)	Data 0.007 (0.067)	Loss 2.2582 (2.1686)	Acc@1 32.715 (35.894)	Acc@5 86.230 (87.964)
Epoch: [8][10/25]	Time 0.609 (0.597)	Data 0.004 (0.061)	Loss 2.2128 (2.1727)	Acc@1 33.105 (35.640)	Acc@5 86.426 (87.824)
Epoch: [8][11/25]	Time 0.598 (0.597)	Data 0.006 (0.056)	Loss 2.2163 (2.1763)	Acc@1 32.568 (35.384)	Acc@5 86.475 (87.712)
Epoch: [8][12/25]	Time 0.635 (0.600)	Data 0.006 (0.052)	Loss 2.2309 (2.1805)	Acc@1 33.252 (35.220)	Acc@5 84.131 (87.436)
Epoch: [8][13/25]	Time 0.588 (0.599)	Data 0.004 (0.049)	Loss 2.2426 (2.1849)	Acc@1 33.447 (35.093)	Acc@5 85.205 (87.277)
Epoch: [8][14/25]	Time 0.634 (0.602)	Data 0.005 (0.046)	Loss 2.2160 (2.1870)	Acc@1 33.545 (34.990)	Acc@5 86.182 (87.204)
Epoch: [8][15/25]	Time 0.586 (0.601)	Data 0.007 (0.044)	Loss 2.1910 (2.1873)	Acc@1 34.912 (34.985)	Acc@5 87.451 (87.219)
Epoch: [8][16/25]	Time 0.619 (0.602)	Data 0.006 (0.041)	Loss 2.2016 (2.1881)	Acc@1 33.008 (34.869)	Acc@5 86.377 (87.170)
Epoch: [8][17/25]	Time 0.615 (0.603)	Data 0.005 (0.039)	Loss 2.1556 (2.1863)	Acc@1 37.988 (35.042)	Acc@5 87.109 (87.166)
Epoch: [8][18/25]	Time 0.619 (0.603)	Data 0.005 (0.038)	Loss 2.1751 (2.1857)	Acc@1 36.426 (35.115)	Acc@5 86.719 (87.143)
Epoch: [8][19/25]	Time 0.596 (0.603)	Data 0.003 (0.036)	Loss 2.1668 (2.1848)	Acc@1 35.254 (35.122)	Acc@5 88.379 (87.205)
Epoch: [8][20/25]	Time 0.625 (0.604)	Data 0.003 (0.034)	Loss 2.1300 (2.1822)	Acc@1 34.814 (35.107)	Acc@5 88.135 (87.249)
Epoch: [8][21/25]	Time 0.600 (0.604)	Data 0.006 (0.033)	Loss 2.1151 (2.1791)	Acc@1 38.428 (35.258)	Acc@5 88.135 (87.289)
Epoch: [8][22/25]	Time 0.631 (0.605)	Data 0.005 (0.032)	Loss 2.1749 (2.1789)	Acc@1 37.402 (35.352)	Acc@5 85.840 (87.226)
Epoch: [8][23/25]	Time 0.575 (0.604)	Data 0.004 (0.031)	Loss 2.1592 (2.1781)	Acc@1 35.010 (35.337)	Acc@5 87.744 (87.248)
Epoch: [8][24/25]	Time 0.356 (0.594)	Data 0.005 (0.030)	Loss 2.1080 (2.1769)	Acc@1 36.203 (35.352)	Acc@5 88.797 (87.274)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/25]	Time 0.615 (0.615)	Data 0.727 (0.727)	Loss 2.0991 (2.0991)	Acc@1 38.428 (38.428)	Acc@5 87.402 (87.402)
Epoch: [9][1/25]	Time 0.618 (0.616)	Data 0.004 (0.366)	Loss 2.0804 (2.0897)	Acc@1 38.135 (38.281)	Acc@5 89.355 (88.379)
Epoch: [9][2/25]	Time 0.594 (0.609)	Data 0.003 (0.245)	Loss 2.1016 (2.0937)	Acc@1 38.574 (38.379)	Acc@5 89.111 (88.623)
Epoch: [9][3/25]	Time 0.606 (0.608)	Data 0.003 (0.184)	Loss 2.0612 (2.0856)	Acc@1 39.209 (38.586)	Acc@5 89.258 (88.782)
Epoch: [9][4/25]	Time 0.558 (0.598)	Data 0.005 (0.148)	Loss 2.0656 (2.0816)	Acc@1 39.697 (38.809)	Acc@5 89.746 (88.975)
Epoch: [9][5/25]	Time 0.612 (0.600)	Data 0.005 (0.125)	Loss 2.0598 (2.0780)	Acc@1 38.086 (38.688)	Acc@5 89.258 (89.022)
Epoch: [9][6/25]	Time 0.609 (0.602)	Data 0.003 (0.107)	Loss 2.0853 (2.0790)	Acc@1 35.791 (38.274)	Acc@5 88.525 (88.951)
Epoch: [9][7/25]	Time 0.581 (0.599)	Data 0.005 (0.094)	Loss 2.0868 (2.0800)	Acc@1 38.916 (38.354)	Acc@5 89.258 (88.989)
Epoch: [9][8/25]	Time 0.600 (0.599)	Data 0.004 (0.084)	Loss 2.0581 (2.0775)	Acc@1 38.867 (38.411)	Acc@5 88.721 (88.959)
Epoch: [9][9/25]	Time 0.633 (0.603)	Data 0.004 (0.076)	Loss 2.0335 (2.0731)	Acc@1 39.453 (38.516)	Acc@5 90.430 (89.106)
Epoch: [9][10/25]	Time 0.615 (0.604)	Data 0.005 (0.070)	Loss 2.0480 (2.0708)	Acc@1 39.746 (38.627)	Acc@5 89.697 (89.160)
Epoch: [9][11/25]	Time 0.601 (0.604)	Data 0.004 (0.064)	Loss 2.0483 (2.0690)	Acc@1 39.404 (38.692)	Acc@5 88.818 (89.132)
Epoch: [9][12/25]	Time 0.602 (0.603)	Data 0.004 (0.060)	Loss 2.0138 (2.0647)	Acc@1 40.527 (38.833)	Acc@5 89.844 (89.186)
Epoch: [9][13/25]	Time 0.629 (0.605)	Data 0.004 (0.056)	Loss 2.0693 (2.0651)	Acc@1 37.842 (38.763)	Acc@5 89.160 (89.185)
Epoch: [9][14/25]	Time 0.607 (0.605)	Data 0.005 (0.052)	Loss 2.0315 (2.0628)	Acc@1 38.525 (38.747)	Acc@5 88.477 (89.137)
Epoch: [9][15/25]	Time 0.611 (0.606)	Data 0.007 (0.050)	Loss 1.9973 (2.0587)	Acc@1 41.943 (38.947)	Acc@5 89.990 (89.191)
Epoch: [9][16/25]	Time 0.594 (0.605)	Data 0.006 (0.047)	Loss 2.0065 (2.0557)	Acc@1 40.625 (39.045)	Acc@5 89.795 (89.226)
Epoch: [9][17/25]	Time 0.634 (0.607)	Data 0.005 (0.045)	Loss 1.9787 (2.0514)	Acc@1 41.064 (39.157)	Acc@5 90.674 (89.307)
Epoch: [9][18/25]	Time 0.602 (0.606)	Data 0.003 (0.042)	Loss 2.0122 (2.0493)	Acc@1 41.260 (39.268)	Acc@5 89.307 (89.307)
Epoch: [9][19/25]	Time 0.613 (0.607)	Data 0.004 (0.041)	Loss 1.9888 (2.0463)	Acc@1 40.820 (39.346)	Acc@5 90.625 (89.373)
Epoch: [9][20/25]	Time 0.611 (0.607)	Data 0.005 (0.039)	Loss 1.9852 (2.0434)	Acc@1 41.260 (39.437)	Acc@5 90.381 (89.421)
Epoch: [9][21/25]	Time 0.595 (0.606)	Data 0.005 (0.037)	Loss 1.9436 (2.0388)	Acc@1 42.725 (39.586)	Acc@5 90.869 (89.486)
Epoch: [9][22/25]	Time 0.608 (0.606)	Data 0.004 (0.036)	Loss 1.9360 (2.0344)	Acc@1 43.701 (39.765)	Acc@5 90.625 (89.536)
Epoch: [9][23/25]	Time 0.618 (0.607)	Data 0.003 (0.035)	Loss 1.9804 (2.0321)	Acc@1 40.332 (39.789)	Acc@5 90.576 (89.579)
Epoch: [9][24/25]	Time 0.379 (0.598)	Data 0.006 (0.033)	Loss 1.9654 (2.0310)	Acc@1 41.627 (39.820)	Acc@5 90.684 (89.598)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/25]	Time 0.594 (0.594)	Data 0.705 (0.705)	Loss 1.9359 (1.9359)	Acc@1 42.480 (42.480)	Acc@5 91.016 (91.016)
Epoch: [10][1/25]	Time 0.590 (0.592)	Data 0.004 (0.354)	Loss 1.8991 (1.9175)	Acc@1 42.334 (42.407)	Acc@5 91.943 (91.479)
Epoch: [10][2/25]	Time 0.618 (0.601)	Data 0.005 (0.238)	Loss 1.9277 (1.9209)	Acc@1 43.164 (42.660)	Acc@5 92.676 (91.878)
Epoch: [10][3/25]	Time 0.580 (0.596)	Data 0.005 (0.180)	Loss 1.8899 (1.9131)	Acc@1 44.336 (43.079)	Acc@5 92.432 (92.017)
Epoch: [10][4/25]	Time 0.658 (0.608)	Data 0.008 (0.145)	Loss 1.9205 (1.9146)	Acc@1 43.457 (43.154)	Acc@5 90.918 (91.797)
Epoch: [10][5/25]	Time 0.586 (0.604)	Data 0.006 (0.122)	Loss 1.8659 (1.9065)	Acc@1 46.533 (43.717)	Acc@5 91.602 (91.764)
Epoch: [10][6/25]	Time 0.598 (0.604)	Data 0.003 (0.105)	Loss 1.8982 (1.9053)	Acc@1 44.141 (43.778)	Acc@5 91.162 (91.678)
Epoch: [10][7/25]	Time 0.623 (0.606)	Data 0.003 (0.092)	Loss 1.9139 (1.9064)	Acc@1 43.555 (43.750)	Acc@5 91.846 (91.699)
Epoch: [10][8/25]	Time 0.595 (0.605)	Data 0.006 (0.083)	Loss 1.8550 (1.9007)	Acc@1 46.094 (44.010)	Acc@5 92.236 (91.759)
Epoch: [10][9/25]	Time 0.631 (0.607)	Data 0.004 (0.075)	Loss 1.8583 (1.8964)	Acc@1 45.361 (44.146)	Acc@5 91.943 (91.777)
Epoch: [10][10/25]	Time 0.641 (0.610)	Data 0.007 (0.069)	Loss 1.8535 (1.8925)	Acc@1 42.920 (44.034)	Acc@5 91.553 (91.757)
Epoch: [10][11/25]	Time 0.606 (0.610)	Data 0.006 (0.064)	Loss 1.8647 (1.8902)	Acc@1 44.873 (44.104)	Acc@5 91.650 (91.748)
Epoch: [10][12/25]	Time 0.646 (0.613)	Data 0.004 (0.059)	Loss 1.8570 (1.8877)	Acc@1 46.289 (44.272)	Acc@5 92.188 (91.782)
Epoch: [10][13/25]	Time 0.615 (0.613)	Data 0.005 (0.055)	Loss 1.8259 (1.8832)	Acc@1 45.605 (44.367)	Acc@5 92.676 (91.846)
Epoch: [10][14/25]	Time 0.613 (0.613)	Data 0.005 (0.052)	Loss 1.8848 (1.8833)	Acc@1 43.115 (44.284)	Acc@5 91.943 (91.852)
Epoch: [10][15/25]	Time 0.610 (0.613)	Data 0.004 (0.049)	Loss 1.8912 (1.8838)	Acc@1 43.311 (44.223)	Acc@5 92.529 (91.895)
Epoch: [10][16/25]	Time 0.613 (0.613)	Data 0.006 (0.046)	Loss 1.8866 (1.8840)	Acc@1 43.652 (44.189)	Acc@5 92.041 (91.903)
Epoch: [10][17/25]	Time 0.590 (0.612)	Data 0.004 (0.044)	Loss 1.8918 (1.8844)	Acc@1 43.604 (44.157)	Acc@5 90.918 (91.848)
Epoch: [10][18/25]	Time 0.622 (0.612)	Data 0.005 (0.042)	Loss 1.8317 (1.8817)	Acc@1 47.266 (44.321)	Acc@5 91.797 (91.846)
Epoch: [10][19/25]	Time 0.612 (0.612)	Data 0.007 (0.040)	Loss 1.7992 (1.8775)	Acc@1 48.779 (44.543)	Acc@5 93.213 (91.914)
Epoch: [10][20/25]	Time 0.591 (0.611)	Data 0.004 (0.038)	Loss 1.8512 (1.8763)	Acc@1 46.143 (44.620)	Acc@5 91.748 (91.906)
Epoch: [10][21/25]	Time 0.620 (0.612)	Data 0.004 (0.037)	Loss 1.8507 (1.8751)	Acc@1 46.973 (44.727)	Acc@5 92.432 (91.930)
Epoch: [10][22/25]	Time 0.553 (0.609)	Data 0.005 (0.035)	Loss 1.8291 (1.8731)	Acc@1 45.801 (44.773)	Acc@5 92.822 (91.969)
Epoch: [10][23/25]	Time 0.619 (0.609)	Data 0.005 (0.034)	Loss 1.7760 (1.8691)	Acc@1 48.438 (44.926)	Acc@5 93.262 (92.023)
Epoch: [10][24/25]	Time 0.321 (0.598)	Data 0.004 (0.033)	Loss 1.8087 (1.8680)	Acc@1 49.057 (44.996)	Acc@5 91.981 (92.022)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/25]	Time 0.609 (0.609)	Data 0.807 (0.807)	Loss 1.7944 (1.7944)	Acc@1 46.875 (46.875)	Acc@5 91.943 (91.943)
Epoch: [11][1/25]	Time 0.601 (0.605)	Data 0.003 (0.405)	Loss 1.7786 (1.7865)	Acc@1 49.219 (48.047)	Acc@5 92.090 (92.017)
Epoch: [11][2/25]	Time 0.549 (0.586)	Data 0.008 (0.273)	Loss 1.7219 (1.7650)	Acc@1 50.049 (48.714)	Acc@5 93.945 (92.660)
Epoch: [11][3/25]	Time 0.593 (0.588)	Data 0.003 (0.205)	Loss 1.7488 (1.7609)	Acc@1 49.805 (48.987)	Acc@5 92.480 (92.615)
Epoch: [11][4/25]	Time 0.605 (0.591)	Data 0.008 (0.166)	Loss 1.7761 (1.7640)	Acc@1 48.779 (48.945)	Acc@5 93.359 (92.764)
Epoch: [11][5/25]	Time 0.593 (0.591)	Data 0.005 (0.139)	Loss 1.7588 (1.7631)	Acc@1 47.168 (48.649)	Acc@5 93.555 (92.896)
Epoch: [11][6/25]	Time 0.605 (0.593)	Data 0.003 (0.119)	Loss 1.7730 (1.7645)	Acc@1 48.682 (48.654)	Acc@5 93.408 (92.969)
Epoch: [11][7/25]	Time 0.596 (0.594)	Data 0.005 (0.105)	Loss 1.7442 (1.7620)	Acc@1 50.293 (48.859)	Acc@5 93.164 (92.993)
Epoch: [11][8/25]	Time 0.607 (0.595)	Data 0.008 (0.094)	Loss 1.6868 (1.7536)	Acc@1 50.781 (49.072)	Acc@5 95.020 (93.218)
Epoch: [11][9/25]	Time 0.581 (0.594)	Data 0.005 (0.085)	Loss 1.7466 (1.7529)	Acc@1 49.365 (49.102)	Acc@5 93.506 (93.247)
Epoch: [11][10/25]	Time 0.603 (0.595)	Data 0.004 (0.078)	Loss 1.7334 (1.7511)	Acc@1 50.732 (49.250)	Acc@5 92.871 (93.213)
Epoch: [11][11/25]	Time 0.599 (0.595)	Data 0.004 (0.072)	Loss 1.6952 (1.7465)	Acc@1 51.025 (49.398)	Acc@5 94.287 (93.302)
Epoch: [11][12/25]	Time 0.590 (0.595)	Data 0.006 (0.067)	Loss 1.7670 (1.7481)	Acc@1 49.609 (49.414)	Acc@5 92.627 (93.250)
Epoch: [11][13/25]	Time 0.580 (0.593)	Data 0.007 (0.062)	Loss 1.7303 (1.7468)	Acc@1 50.195 (49.470)	Acc@5 92.969 (93.230)
Epoch: [11][14/25]	Time 0.594 (0.594)	Data 0.004 (0.059)	Loss 1.6415 (1.7398)	Acc@1 54.785 (49.824)	Acc@5 93.311 (93.236)
Epoch: [11][15/25]	Time 0.549 (0.591)	Data 0.006 (0.055)	Loss 1.7327 (1.7393)	Acc@1 49.170 (49.783)	Acc@5 93.652 (93.262)
Epoch: [11][16/25]	Time 0.591 (0.591)	Data 0.008 (0.052)	Loss 1.7450 (1.7397)	Acc@1 50.391 (49.819)	Acc@5 93.848 (93.296)
Epoch: [11][17/25]	Time 0.603 (0.591)	Data 0.006 (0.050)	Loss 1.7323 (1.7392)	Acc@1 49.756 (49.816)	Acc@5 93.945 (93.332)
Epoch: [11][18/25]	Time 0.578 (0.591)	Data 0.004 (0.047)	Loss 1.6988 (1.7371)	Acc@1 49.365 (49.792)	Acc@5 94.629 (93.400)
Epoch: [11][19/25]	Time 0.596 (0.591)	Data 0.003 (0.045)	Loss 1.6596 (1.7332)	Acc@1 51.758 (49.890)	Acc@5 94.287 (93.445)
Epoch: [11][20/25]	Time 0.598 (0.591)	Data 0.005 (0.043)	Loss 1.6773 (1.7306)	Acc@1 52.100 (49.995)	Acc@5 93.994 (93.471)
Epoch: [11][21/25]	Time 0.578 (0.591)	Data 0.006 (0.042)	Loss 1.6759 (1.7281)	Acc@1 50.146 (50.002)	Acc@5 94.678 (93.526)
Epoch: [11][22/25]	Time 0.626 (0.592)	Data 0.004 (0.040)	Loss 1.6641 (1.7253)	Acc@1 51.953 (50.087)	Acc@5 93.848 (93.540)
Epoch: [11][23/25]	Time 0.600 (0.593)	Data 0.007 (0.039)	Loss 1.6752 (1.7232)	Acc@1 52.393 (50.183)	Acc@5 94.531 (93.581)
Epoch: [11][24/25]	Time 0.325 (0.582)	Data 0.005 (0.037)	Loss 1.6271 (1.7216)	Acc@1 54.009 (50.248)	Acc@5 95.047 (93.606)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/25]	Time 0.633 (0.633)	Data 0.648 (0.648)	Loss 1.6420 (1.6420)	Acc@1 51.904 (51.904)	Acc@5 94.336 (94.336)
Epoch: [12][1/25]	Time 0.620 (0.627)	Data 0.006 (0.327)	Loss 1.6542 (1.6481)	Acc@1 51.855 (51.880)	Acc@5 94.141 (94.238)
Epoch: [12][2/25]	Time 0.553 (0.602)	Data 0.003 (0.219)	Loss 1.6249 (1.6404)	Acc@1 54.150 (52.637)	Acc@5 94.482 (94.320)
Epoch: [12][3/25]	Time 0.575 (0.595)	Data 0.004 (0.165)	Loss 1.6292 (1.6376)	Acc@1 54.346 (53.064)	Acc@5 94.336 (94.324)
Epoch: [12][4/25]	Time 0.602 (0.596)	Data 0.006 (0.133)	Loss 1.5872 (1.6275)	Acc@1 54.639 (53.379)	Acc@5 95.557 (94.570)
Epoch: [12][5/25]	Time 0.601 (0.597)	Data 0.007 (0.112)	Loss 1.5839 (1.6202)	Acc@1 54.590 (53.581)	Acc@5 95.898 (94.792)
Epoch: [12][6/25]	Time 0.588 (0.596)	Data 0.003 (0.097)	Loss 1.6160 (1.6196)	Acc@1 54.004 (53.641)	Acc@5 94.971 (94.817)
Epoch: [12][7/25]	Time 0.642 (0.602)	Data 0.004 (0.085)	Loss 1.5918 (1.6162)	Acc@1 55.664 (53.894)	Acc@5 94.580 (94.788)
Epoch: [12][8/25]	Time 0.615 (0.603)	Data 0.005 (0.076)	Loss 1.5249 (1.6060)	Acc@1 57.617 (54.308)	Acc@5 95.557 (94.873)
Epoch: [12][9/25]	Time 0.590 (0.602)	Data 0.004 (0.069)	Loss 1.5836 (1.6038)	Acc@1 55.420 (54.419)	Acc@5 95.068 (94.893)
Epoch: [12][10/25]	Time 0.608 (0.602)	Data 0.009 (0.064)	Loss 1.5910 (1.6026)	Acc@1 56.104 (54.572)	Acc@5 94.922 (94.895)
Epoch: [12][11/25]	Time 0.613 (0.603)	Data 0.005 (0.059)	Loss 1.5218 (1.5959)	Acc@1 58.008 (54.858)	Acc@5 95.020 (94.906)
Epoch: [12][12/25]	Time 0.546 (0.599)	Data 0.004 (0.054)	Loss 1.5653 (1.5935)	Acc@1 56.543 (54.988)	Acc@5 95.166 (94.926)
Epoch: [12][13/25]	Time 0.609 (0.600)	Data 0.004 (0.051)	Loss 1.5677 (1.5917)	Acc@1 55.811 (55.047)	Acc@5 95.410 (94.960)
Epoch: [12][14/25]	Time 0.614 (0.600)	Data 0.005 (0.048)	Loss 1.5718 (1.5903)	Acc@1 54.785 (55.029)	Acc@5 95.898 (95.023)
Epoch: [12][15/25]	Time 0.606 (0.601)	Data 0.007 (0.045)	Loss 1.5969 (1.5908)	Acc@1 55.713 (55.072)	Acc@5 95.215 (95.035)
Epoch: [12][16/25]	Time 0.617 (0.602)	Data 0.004 (0.043)	Loss 1.5422 (1.5879)	Acc@1 56.592 (55.161)	Acc@5 95.215 (95.045)
Epoch: [12][17/25]	Time 0.606 (0.602)	Data 0.003 (0.041)	Loss 1.5321 (1.5848)	Acc@1 56.250 (55.222)	Acc@5 94.971 (95.041)
Epoch: [12][18/25]	Time 0.591 (0.601)	Data 0.004 (0.039)	Loss 1.6295 (1.5871)	Acc@1 53.174 (55.114)	Acc@5 94.385 (95.007)
Epoch: [12][19/25]	Time 0.577 (0.600)	Data 0.004 (0.037)	Loss 1.5373 (1.5847)	Acc@1 55.762 (55.146)	Acc@5 95.264 (95.020)
Epoch: [12][20/25]	Time 0.611 (0.601)	Data 0.004 (0.035)	Loss 1.5614 (1.5836)	Acc@1 56.445 (55.208)	Acc@5 95.264 (95.031)
Epoch: [12][21/25]	Time 0.623 (0.602)	Data 0.004 (0.034)	Loss 1.5281 (1.5810)	Acc@1 57.324 (55.305)	Acc@5 94.775 (95.020)
Epoch: [12][22/25]	Time 0.586 (0.601)	Data 0.008 (0.033)	Loss 1.4990 (1.5775)	Acc@1 57.715 (55.409)	Acc@5 96.045 (95.064)
Epoch: [12][23/25]	Time 0.593 (0.601)	Data 0.006 (0.032)	Loss 1.5486 (1.5763)	Acc@1 55.811 (55.426)	Acc@5 95.020 (95.062)
Epoch: [12][24/25]	Time 0.335 (0.590)	Data 0.006 (0.031)	Loss 1.5183 (1.5753)	Acc@1 57.429 (55.460)	Acc@5 95.283 (95.066)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/25]	Time 0.572 (0.572)	Data 0.646 (0.646)	Loss 1.5126 (1.5126)	Acc@1 55.908 (55.908)	Acc@5 96.045 (96.045)
Epoch: [13][1/25]	Time 0.592 (0.582)	Data 0.003 (0.324)	Loss 1.5135 (1.5130)	Acc@1 57.666 (56.787)	Acc@5 95.166 (95.605)
Epoch: [13][2/25]	Time 0.642 (0.602)	Data 0.006 (0.218)	Loss 1.4772 (1.5011)	Acc@1 58.936 (57.503)	Acc@5 95.605 (95.605)
Epoch: [13][3/25]	Time 0.607 (0.603)	Data 0.006 (0.165)	Loss 1.4843 (1.4969)	Acc@1 58.789 (57.825)	Acc@5 95.703 (95.630)
Epoch: [13][4/25]	Time 0.603 (0.603)	Data 0.006 (0.134)	Loss 1.4395 (1.4854)	Acc@1 60.352 (58.330)	Acc@5 95.654 (95.635)
Epoch: [13][5/25]	Time 0.594 (0.602)	Data 0.007 (0.112)	Loss 1.4606 (1.4813)	Acc@1 60.254 (58.651)	Acc@5 96.289 (95.744)
Epoch: [13][6/25]	Time 0.636 (0.607)	Data 0.004 (0.097)	Loss 1.4019 (1.4699)	Acc@1 60.303 (58.887)	Acc@5 96.875 (95.905)
Epoch: [13][7/25]	Time 0.569 (0.602)	Data 0.004 (0.085)	Loss 1.4120 (1.4627)	Acc@1 61.914 (59.265)	Acc@5 95.752 (95.886)
Epoch: [13][8/25]	Time 0.632 (0.605)	Data 0.004 (0.076)	Loss 1.4060 (1.4564)	Acc@1 60.986 (59.456)	Acc@5 96.973 (96.007)
Epoch: [13][9/25]	Time 0.618 (0.606)	Data 0.004 (0.069)	Loss 1.3993 (1.4507)	Acc@1 61.768 (59.688)	Acc@5 96.289 (96.035)
Epoch: [13][10/25]	Time 0.612 (0.607)	Data 0.004 (0.063)	Loss 1.3656 (1.4430)	Acc@1 62.695 (59.961)	Acc@5 96.191 (96.049)
Epoch: [13][11/25]	Time 0.625 (0.609)	Data 0.007 (0.059)	Loss 1.3885 (1.4384)	Acc@1 62.305 (60.156)	Acc@5 96.387 (96.077)
Epoch: [13][12/25]	Time 0.618 (0.609)	Data 0.008 (0.055)	Loss 1.3804 (1.4339)	Acc@1 62.354 (60.325)	Acc@5 95.898 (96.064)
Epoch: [13][13/25]	Time 0.586 (0.608)	Data 0.005 (0.051)	Loss 1.3326 (1.4267)	Acc@1 64.111 (60.596)	Acc@5 95.801 (96.045)
Epoch: [13][14/25]	Time 0.612 (0.608)	Data 0.005 (0.048)	Loss 1.4000 (1.4249)	Acc@1 61.523 (60.658)	Acc@5 96.191 (96.055)
Epoch: [13][15/25]	Time 0.625 (0.609)	Data 0.005 (0.045)	Loss 1.3438 (1.4199)	Acc@1 64.355 (60.889)	Acc@5 96.777 (96.100)
Epoch: [13][16/25]	Time 0.571 (0.607)	Data 0.005 (0.043)	Loss 1.3878 (1.4180)	Acc@1 60.254 (60.851)	Acc@5 96.338 (96.114)
Epoch: [13][17/25]	Time 0.630 (0.608)	Data 0.004 (0.041)	Loss 1.3153 (1.4123)	Acc@1 64.453 (61.051)	Acc@5 96.582 (96.140)
Epoch: [13][18/25]	Time 0.598 (0.608)	Data 0.006 (0.039)	Loss 1.3396 (1.4084)	Acc@1 63.672 (61.189)	Acc@5 96.436 (96.155)
Epoch: [13][19/25]	Time 0.578 (0.606)	Data 0.005 (0.037)	Loss 1.3923 (1.4076)	Acc@1 62.061 (61.233)	Acc@5 96.289 (96.162)
Epoch: [13][20/25]	Time 0.620 (0.607)	Data 0.004 (0.036)	Loss 1.3438 (1.4046)	Acc@1 63.477 (61.340)	Acc@5 96.533 (96.180)
Epoch: [13][21/25]	Time 0.597 (0.606)	Data 0.005 (0.034)	Loss 1.3183 (1.4007)	Acc@1 65.234 (61.517)	Acc@5 96.484 (96.194)
Epoch: [13][22/25]	Time 0.635 (0.608)	Data 0.004 (0.033)	Loss 1.3964 (1.4005)	Acc@1 60.840 (61.487)	Acc@5 96.045 (96.187)
Epoch: [13][23/25]	Time 0.591 (0.607)	Data 0.004 (0.032)	Loss 1.3234 (1.3973)	Acc@1 63.867 (61.587)	Acc@5 97.021 (96.222)
Epoch: [13][24/25]	Time 0.344 (0.596)	Data 0.005 (0.031)	Loss 1.3972 (1.3973)	Acc@1 62.146 (61.596)	Acc@5 96.226 (96.222)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/25]	Time 0.601 (0.601)	Data 0.674 (0.674)	Loss 1.2921 (1.2921)	Acc@1 64.941 (64.941)	Acc@5 97.266 (97.266)
Epoch: [14][1/25]	Time 0.601 (0.601)	Data 0.005 (0.339)	Loss 1.3238 (1.3079)	Acc@1 63.623 (64.282)	Acc@5 97.217 (97.241)
Epoch: [14][2/25]	Time 0.596 (0.599)	Data 0.004 (0.227)	Loss 1.3204 (1.3121)	Acc@1 63.184 (63.916)	Acc@5 97.070 (97.184)
Epoch: [14][3/25]	Time 0.613 (0.603)	Data 0.005 (0.172)	Loss 1.2936 (1.3075)	Acc@1 65.820 (64.392)	Acc@5 96.777 (97.083)
Epoch: [14][4/25]	Time 0.606 (0.604)	Data 0.005 (0.138)	Loss 1.3018 (1.3063)	Acc@1 64.258 (64.365)	Acc@5 96.973 (97.061)
Epoch: [14][5/25]	Time 0.558 (0.596)	Data 0.004 (0.116)	Loss 1.2363 (1.2947)	Acc@1 66.748 (64.762)	Acc@5 97.949 (97.209)
Epoch: [14][6/25]	Time 0.619 (0.599)	Data 0.004 (0.100)	Loss 1.2698 (1.2911)	Acc@1 65.869 (64.920)	Acc@5 96.973 (97.175)
Epoch: [14][7/25]	Time 0.593 (0.598)	Data 0.003 (0.088)	Loss 1.3133 (1.2939)	Acc@1 64.307 (64.844)	Acc@5 96.729 (97.119)
Epoch: [14][8/25]	Time 0.599 (0.598)	Data 0.005 (0.079)	Loss 1.2295 (1.2867)	Acc@1 66.748 (65.055)	Acc@5 97.314 (97.141)
Epoch: [14][9/25]	Time 0.594 (0.598)	Data 0.004 (0.071)	Loss 1.2048 (1.2785)	Acc@1 68.604 (65.410)	Acc@5 97.363 (97.163)
Epoch: [14][10/25]	Time 0.616 (0.600)	Data 0.004 (0.065)	Loss 1.1924 (1.2707)	Acc@1 69.141 (65.749)	Acc@5 98.145 (97.252)
Epoch: [14][11/25]	Time 0.624 (0.602)	Data 0.004 (0.060)	Loss 1.2463 (1.2687)	Acc@1 66.992 (65.853)	Acc@5 97.070 (97.237)
Epoch: [14][12/25]	Time 0.617 (0.603)	Data 0.007 (0.056)	Loss 1.2062 (1.2639)	Acc@1 67.432 (65.974)	Acc@5 97.607 (97.266)
Epoch: [14][13/25]	Time 0.578 (0.601)	Data 0.004 (0.052)	Loss 1.1684 (1.2570)	Acc@1 70.020 (66.263)	Acc@5 97.705 (97.297)
Epoch: [14][14/25]	Time 0.620 (0.602)	Data 0.005 (0.049)	Loss 1.2458 (1.2563)	Acc@1 66.504 (66.279)	Acc@5 97.656 (97.321)
Epoch: [14][15/25]	Time 0.614 (0.603)	Data 0.005 (0.046)	Loss 1.2001 (1.2528)	Acc@1 69.141 (66.458)	Acc@5 97.363 (97.324)
Epoch: [14][16/25]	Time 0.591 (0.602)	Data 0.006 (0.044)	Loss 1.1926 (1.2492)	Acc@1 69.971 (66.665)	Acc@5 97.461 (97.332)
Epoch: [14][17/25]	Time 0.622 (0.603)	Data 0.003 (0.042)	Loss 1.2012 (1.2466)	Acc@1 69.434 (66.819)	Acc@5 97.559 (97.344)
Epoch: [14][18/25]	Time 0.631 (0.605)	Data 0.004 (0.040)	Loss 1.2210 (1.2452)	Acc@1 67.041 (66.830)	Acc@5 97.314 (97.343)
Epoch: [14][19/25]	Time 0.576 (0.603)	Data 0.007 (0.038)	Loss 1.2054 (1.2432)	Acc@1 67.725 (66.875)	Acc@5 97.070 (97.329)
Epoch: [14][20/25]	Time 0.619 (0.604)	Data 0.004 (0.036)	Loss 1.2340 (1.2428)	Acc@1 67.236 (66.892)	Acc@5 97.754 (97.349)
Epoch: [14][21/25]	Time 0.613 (0.605)	Data 0.008 (0.035)	Loss 1.2153 (1.2416)	Acc@1 68.311 (66.957)	Acc@5 97.217 (97.343)
Epoch: [14][22/25]	Time 0.584 (0.604)	Data 0.003 (0.034)	Loss 1.1937 (1.2395)	Acc@1 68.018 (67.003)	Acc@5 97.021 (97.329)
Epoch: [14][23/25]	Time 0.593 (0.603)	Data 0.005 (0.033)	Loss 1.1702 (1.2366)	Acc@1 70.166 (67.135)	Acc@5 97.119 (97.321)
Epoch: [14][24/25]	Time 0.373 (0.594)	Data 0.004 (0.031)	Loss 1.1608 (1.2353)	Acc@1 70.637 (67.194)	Acc@5 97.052 (97.316)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/25]	Time 0.577 (0.577)	Data 0.642 (0.642)	Loss 1.1198 (1.1198)	Acc@1 70.996 (70.996)	Acc@5 97.900 (97.900)
Epoch: [15][1/25]	Time 0.607 (0.592)	Data 0.007 (0.325)	Loss 1.1771 (1.1485)	Acc@1 69.043 (70.020)	Acc@5 97.656 (97.778)
Epoch: [15][2/25]	Time 0.610 (0.598)	Data 0.004 (0.218)	Loss 1.1452 (1.1474)	Acc@1 70.703 (70.247)	Acc@5 97.754 (97.770)
Epoch: [15][3/25]	Time 0.586 (0.595)	Data 0.004 (0.164)	Loss 1.0930 (1.1338)	Acc@1 73.047 (70.947)	Acc@5 98.584 (97.974)
Epoch: [15][4/25]	Time 0.632 (0.602)	Data 0.005 (0.133)	Loss 1.1422 (1.1355)	Acc@1 69.531 (70.664)	Acc@5 97.559 (97.891)
Epoch: [15][5/25]	Time 0.614 (0.604)	Data 0.005 (0.111)	Loss 1.0831 (1.1267)	Acc@1 72.656 (70.996)	Acc@5 98.242 (97.949)
Epoch: [15][6/25]	Time 0.601 (0.604)	Data 0.005 (0.096)	Loss 1.1637 (1.1320)	Acc@1 69.727 (70.815)	Acc@5 97.705 (97.914)
Epoch: [15][7/25]	Time 0.625 (0.606)	Data 0.004 (0.085)	Loss 1.1490 (1.1341)	Acc@1 70.801 (70.813)	Acc@5 97.803 (97.900)
Epoch: [15][8/25]	Time 0.600 (0.606)	Data 0.005 (0.076)	Loss 1.1018 (1.1305)	Acc@1 72.705 (71.023)	Acc@5 97.705 (97.879)
Epoch: [15][9/25]	Time 0.594 (0.605)	Data 0.004 (0.069)	Loss 1.0728 (1.1248)	Acc@1 72.461 (71.167)	Acc@5 98.145 (97.905)
Epoch: [15][10/25]	Time 0.636 (0.607)	Data 0.005 (0.063)	Loss 1.1337 (1.1256)	Acc@1 70.654 (71.120)	Acc@5 97.949 (97.909)
Epoch: [15][11/25]	Time 0.588 (0.606)	Data 0.006 (0.058)	Loss 1.1139 (1.1246)	Acc@1 71.143 (71.122)	Acc@5 98.486 (97.957)
Epoch: [15][12/25]	Time 0.598 (0.605)	Data 0.005 (0.054)	Loss 1.0904 (1.1220)	Acc@1 71.631 (71.161)	Acc@5 97.607 (97.930)
Epoch: [15][13/25]	Time 0.636 (0.607)	Data 0.005 (0.050)	Loss 1.1297 (1.1225)	Acc@1 71.436 (71.181)	Acc@5 97.607 (97.907)
Epoch: [15][14/25]	Time 0.596 (0.607)	Data 0.004 (0.047)	Loss 1.1231 (1.1226)	Acc@1 71.533 (71.204)	Acc@5 97.510 (97.881)
Epoch: [15][15/25]	Time 0.620 (0.608)	Data 0.004 (0.045)	Loss 1.0685 (1.1192)	Acc@1 74.072 (71.384)	Acc@5 98.242 (97.903)
Epoch: [15][16/25]	Time 0.587 (0.606)	Data 0.007 (0.042)	Loss 1.0435 (1.1147)	Acc@1 73.535 (71.510)	Acc@5 98.242 (97.923)
Epoch: [15][17/25]	Time 0.618 (0.607)	Data 0.003 (0.040)	Loss 1.0700 (1.1122)	Acc@1 73.486 (71.620)	Acc@5 98.242 (97.941)
Epoch: [15][18/25]	Time 0.608 (0.607)	Data 0.004 (0.038)	Loss 1.0460 (1.1088)	Acc@1 72.949 (71.690)	Acc@5 98.193 (97.954)
Epoch: [15][19/25]	Time 0.601 (0.607)	Data 0.005 (0.037)	Loss 1.0862 (1.1076)	Acc@1 73.389 (71.775)	Acc@5 97.559 (97.935)
Epoch: [15][20/25]	Time 0.606 (0.607)	Data 0.008 (0.035)	Loss 1.0403 (1.1044)	Acc@1 73.340 (71.849)	Acc@5 98.340 (97.954)
Epoch: [15][21/25]	Time 0.630 (0.608)	Data 0.004 (0.034)	Loss 1.0778 (1.1032)	Acc@1 72.363 (71.873)	Acc@5 97.949 (97.954)
Epoch: [15][22/25]	Time 0.607 (0.608)	Data 0.005 (0.033)	Loss 1.0244 (1.0998)	Acc@1 72.900 (71.917)	Acc@5 98.779 (97.990)
Epoch: [15][23/25]	Time 0.601 (0.607)	Data 0.004 (0.031)	Loss 1.0660 (1.0984)	Acc@1 72.168 (71.928)	Acc@5 97.803 (97.982)
Epoch: [15][24/25]	Time 0.315 (0.596)	Data 0.006 (0.030)	Loss 1.0258 (1.0971)	Acc@1 72.406 (71.936)	Acc@5 98.821 (97.996)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/25]	Time 0.575 (0.575)	Data 0.621 (0.621)	Loss 1.0335 (1.0335)	Acc@1 74.365 (74.365)	Acc@5 98.389 (98.389)
Epoch: [16][1/25]	Time 0.589 (0.582)	Data 0.007 (0.314)	Loss 0.9747 (1.0041)	Acc@1 76.123 (75.244)	Acc@5 98.535 (98.462)
Epoch: [16][2/25]	Time 0.594 (0.586)	Data 0.009 (0.212)	Loss 0.9826 (0.9969)	Acc@1 76.221 (75.570)	Acc@5 97.949 (98.291)
Epoch: [16][3/25]	Time 0.594 (0.588)	Data 0.003 (0.160)	Loss 0.9857 (0.9941)	Acc@1 75.830 (75.635)	Acc@5 98.535 (98.352)
Epoch: [16][4/25]	Time 0.602 (0.591)	Data 0.007 (0.129)	Loss 0.9714 (0.9896)	Acc@1 76.807 (75.869)	Acc@5 98.779 (98.438)
Epoch: [16][5/25]	Time 0.591 (0.591)	Data 0.004 (0.108)	Loss 0.9855 (0.9889)	Acc@1 76.807 (76.025)	Acc@5 98.340 (98.421)
Epoch: [16][6/25]	Time 0.620 (0.595)	Data 0.007 (0.094)	Loss 1.0261 (0.9942)	Acc@1 74.268 (75.774)	Acc@5 98.633 (98.451)
Epoch: [16][7/25]	Time 0.591 (0.595)	Data 0.005 (0.083)	Loss 0.9965 (0.9945)	Acc@1 75.244 (75.708)	Acc@5 98.242 (98.425)
Epoch: [16][8/25]	Time 0.600 (0.595)	Data 0.006 (0.074)	Loss 1.0008 (0.9952)	Acc@1 74.854 (75.613)	Acc@5 98.291 (98.410)
Epoch: [16][9/25]	Time 0.555 (0.591)	Data 0.004 (0.067)	Loss 0.9774 (0.9934)	Acc@1 75.830 (75.635)	Acc@5 98.340 (98.403)
Epoch: [16][10/25]	Time 0.601 (0.592)	Data 0.005 (0.062)	Loss 1.0048 (0.9944)	Acc@1 75.977 (75.666)	Acc@5 98.096 (98.375)
Epoch: [16][11/25]	Time 0.578 (0.591)	Data 0.007 (0.057)	Loss 1.0181 (0.9964)	Acc@1 73.926 (75.521)	Acc@5 98.389 (98.376)
Epoch: [16][12/25]	Time 0.596 (0.591)	Data 0.008 (0.053)	Loss 1.0071 (0.9972)	Acc@1 75.635 (75.530)	Acc@5 98.242 (98.366)
Epoch: [16][13/25]	Time 0.594 (0.591)	Data 0.004 (0.050)	Loss 1.0023 (0.9976)	Acc@1 74.463 (75.453)	Acc@5 98.242 (98.357)
Epoch: [16][14/25]	Time 0.615 (0.593)	Data 0.005 (0.047)	Loss 1.0071 (0.9982)	Acc@1 75.146 (75.433)	Acc@5 98.193 (98.346)
Epoch: [16][15/25]	Time 0.600 (0.593)	Data 0.006 (0.044)	Loss 0.9848 (0.9974)	Acc@1 75.244 (75.421)	Acc@5 98.584 (98.361)
Epoch: [16][16/25]	Time 0.626 (0.595)	Data 0.006 (0.042)	Loss 1.0190 (0.9987)	Acc@1 73.145 (75.287)	Acc@5 98.291 (98.357)
Epoch: [16][17/25]	Time 0.585 (0.595)	Data 0.007 (0.040)	Loss 0.9704 (0.9971)	Acc@1 75.977 (75.326)	Acc@5 98.486 (98.364)
Epoch: [16][18/25]	Time 0.617 (0.596)	Data 0.006 (0.038)	Loss 1.0198 (0.9983)	Acc@1 73.584 (75.234)	Acc@5 98.291 (98.360)
Epoch: [16][19/25]	Time 0.603 (0.596)	Data 0.003 (0.036)	Loss 1.0131 (0.9990)	Acc@1 73.975 (75.171)	Acc@5 98.291 (98.357)
Epoch: [16][20/25]	Time 0.600 (0.596)	Data 0.003 (0.035)	Loss 1.0274 (1.0004)	Acc@1 73.535 (75.093)	Acc@5 98.633 (98.370)
Epoch: [16][21/25]	Time 0.614 (0.597)	Data 0.003 (0.033)	Loss 1.0545 (1.0028)	Acc@1 72.656 (74.982)	Acc@5 98.242 (98.364)
Epoch: [16][22/25]	Time 0.611 (0.598)	Data 0.006 (0.032)	Loss 0.9833 (1.0020)	Acc@1 76.221 (75.036)	Acc@5 98.389 (98.365)
Epoch: [16][23/25]	Time 0.608 (0.598)	Data 0.003 (0.031)	Loss 1.0314 (1.0032)	Acc@1 74.512 (75.014)	Acc@5 97.314 (98.322)
Epoch: [16][24/25]	Time 0.317 (0.587)	Data 0.004 (0.030)	Loss 0.9375 (1.0021)	Acc@1 78.538 (75.074)	Acc@5 98.939 (98.332)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/25]	Time 0.635 (0.635)	Data 0.622 (0.622)	Loss 0.9719 (0.9719)	Acc@1 76.318 (76.318)	Acc@5 98.389 (98.389)
Epoch: [17][1/25]	Time 0.588 (0.611)	Data 0.004 (0.313)	Loss 0.9678 (0.9699)	Acc@1 77.197 (76.758)	Acc@5 98.486 (98.438)
Epoch: [17][2/25]	Time 0.601 (0.608)	Data 0.003 (0.210)	Loss 0.9903 (0.9767)	Acc@1 75.488 (76.335)	Acc@5 98.535 (98.470)
Epoch: [17][3/25]	Time 0.609 (0.608)	Data 0.006 (0.159)	Loss 0.9826 (0.9782)	Acc@1 76.367 (76.343)	Acc@5 98.340 (98.438)
Epoch: [17][4/25]	Time 0.597 (0.606)	Data 0.008 (0.129)	Loss 0.9829 (0.9791)	Acc@1 75.537 (76.182)	Acc@5 98.535 (98.457)
Epoch: [17][5/25]	Time 0.605 (0.606)	Data 0.003 (0.108)	Loss 0.9791 (0.9791)	Acc@1 75.537 (76.074)	Acc@5 98.389 (98.446)
Epoch: [17][6/25]	Time 0.606 (0.606)	Data 0.003 (0.093)	Loss 0.9560 (0.9758)	Acc@1 76.172 (76.088)	Acc@5 98.535 (98.458)
Epoch: [17][7/25]	Time 0.596 (0.605)	Data 0.006 (0.082)	Loss 0.9894 (0.9775)	Acc@1 74.365 (75.873)	Acc@5 98.633 (98.480)
Epoch: [17][8/25]	Time 0.593 (0.603)	Data 0.005 (0.073)	Loss 1.0210 (0.9823)	Acc@1 74.072 (75.673)	Acc@5 98.584 (98.492)
Epoch: [17][9/25]	Time 0.604 (0.603)	Data 0.007 (0.067)	Loss 0.9609 (0.9802)	Acc@1 76.660 (75.771)	Acc@5 98.584 (98.501)
Epoch: [17][10/25]	Time 0.595 (0.603)	Data 0.005 (0.061)	Loss 0.9853 (0.9807)	Acc@1 75.098 (75.710)	Acc@5 98.682 (98.517)
Epoch: [17][11/25]	Time 0.629 (0.605)	Data 0.007 (0.057)	Loss 0.9323 (0.9766)	Acc@1 77.393 (75.850)	Acc@5 98.926 (98.551)
Epoch: [17][12/25]	Time 0.583 (0.603)	Data 0.008 (0.053)	Loss 0.9448 (0.9742)	Acc@1 77.930 (76.010)	Acc@5 98.486 (98.546)
Epoch: [17][13/25]	Time 0.634 (0.605)	Data 0.004 (0.049)	Loss 0.9984 (0.9759)	Acc@1 76.074 (76.015)	Acc@5 98.242 (98.525)
Epoch: [17][14/25]	Time 0.581 (0.604)	Data 0.004 (0.046)	Loss 0.9473 (0.9740)	Acc@1 75.732 (75.996)	Acc@5 98.779 (98.542)
Epoch: [17][15/25]	Time 0.644 (0.606)	Data 0.006 (0.044)	Loss 0.9927 (0.9752)	Acc@1 75.488 (75.964)	Acc@5 98.096 (98.514)
Epoch: [17][16/25]	Time 0.597 (0.606)	Data 0.005 (0.042)	Loss 0.9465 (0.9735)	Acc@1 77.832 (76.074)	Acc@5 98.438 (98.509)
Epoch: [17][17/25]	Time 0.606 (0.606)	Data 0.006 (0.040)	Loss 0.9427 (0.9718)	Acc@1 77.393 (76.147)	Acc@5 98.633 (98.516)
Epoch: [17][18/25]	Time 0.614 (0.606)	Data 0.004 (0.038)	Loss 0.9237 (0.9692)	Acc@1 77.686 (76.228)	Acc@5 98.877 (98.535)
Epoch: [17][19/25]	Time 0.614 (0.607)	Data 0.004 (0.036)	Loss 0.9641 (0.9690)	Acc@1 76.025 (76.218)	Acc@5 98.535 (98.535)
Epoch: [17][20/25]	Time 0.614 (0.607)	Data 0.004 (0.034)	Loss 0.9573 (0.9684)	Acc@1 76.465 (76.230)	Acc@5 98.926 (98.554)
Epoch: [17][21/25]	Time 0.607 (0.607)	Data 0.005 (0.033)	Loss 0.9428 (0.9673)	Acc@1 77.148 (76.272)	Acc@5 98.779 (98.564)
Epoch: [17][22/25]	Time 0.570 (0.605)	Data 0.005 (0.032)	Loss 0.9537 (0.9667)	Acc@1 76.758 (76.293)	Acc@5 98.486 (98.561)
Epoch: [17][23/25]	Time 0.640 (0.607)	Data 0.004 (0.031)	Loss 0.8907 (0.9635)	Acc@1 80.029 (76.449)	Acc@5 98.633 (98.564)
Epoch: [17][24/25]	Time 0.323 (0.595)	Data 0.006 (0.030)	Loss 0.9752 (0.9637)	Acc@1 74.882 (76.422)	Acc@5 98.585 (98.564)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/25]	Time 0.608 (0.608)	Data 0.606 (0.606)	Loss 0.9165 (0.9165)	Acc@1 77.637 (77.637)	Acc@5 98.633 (98.633)
Epoch: [18][1/25]	Time 0.588 (0.598)	Data 0.008 (0.307)	Loss 0.9289 (0.9227)	Acc@1 76.904 (77.271)	Acc@5 98.682 (98.657)
Epoch: [18][2/25]	Time 0.619 (0.605)	Data 0.004 (0.206)	Loss 0.9283 (0.9246)	Acc@1 76.270 (76.937)	Acc@5 98.730 (98.682)
Epoch: [18][3/25]	Time 0.576 (0.598)	Data 0.006 (0.156)	Loss 0.9199 (0.9234)	Acc@1 78.223 (77.258)	Acc@5 98.975 (98.755)
Epoch: [18][4/25]	Time 0.631 (0.604)	Data 0.005 (0.126)	Loss 0.9196 (0.9226)	Acc@1 77.148 (77.236)	Acc@5 98.877 (98.779)
Epoch: [18][5/25]	Time 0.586 (0.601)	Data 0.006 (0.106)	Loss 0.9165 (0.9216)	Acc@1 78.809 (77.498)	Acc@5 98.145 (98.674)
Epoch: [18][6/25]	Time 0.605 (0.602)	Data 0.003 (0.091)	Loss 0.9365 (0.9237)	Acc@1 77.051 (77.434)	Acc@5 98.291 (98.619)
Epoch: [18][7/25]	Time 0.629 (0.605)	Data 0.006 (0.081)	Loss 0.9149 (0.9226)	Acc@1 77.441 (77.435)	Acc@5 98.828 (98.645)
Epoch: [18][8/25]	Time 0.610 (0.606)	Data 0.006 (0.072)	Loss 0.8971 (0.9198)	Acc@1 78.369 (77.539)	Acc@5 98.682 (98.649)
Epoch: [18][9/25]	Time 0.611 (0.606)	Data 0.005 (0.065)	Loss 0.9206 (0.9199)	Acc@1 77.881 (77.573)	Acc@5 98.682 (98.652)
Epoch: [18][10/25]	Time 0.594 (0.605)	Data 0.005 (0.060)	Loss 0.9506 (0.9227)	Acc@1 75.586 (77.393)	Acc@5 98.291 (98.619)
Epoch: [18][11/25]	Time 0.594 (0.604)	Data 0.007 (0.056)	Loss 0.8811 (0.9192)	Acc@1 79.297 (77.551)	Acc@5 99.023 (98.653)
Epoch: [18][12/25]	Time 0.621 (0.606)	Data 0.006 (0.052)	Loss 0.9080 (0.9183)	Acc@1 78.223 (77.603)	Acc@5 98.535 (98.644)
Epoch: [18][13/25]	Time 0.589 (0.604)	Data 0.007 (0.049)	Loss 0.8828 (0.9158)	Acc@1 78.076 (77.637)	Acc@5 98.828 (98.657)
Epoch: [18][14/25]	Time 0.644 (0.607)	Data 0.008 (0.046)	Loss 0.9009 (0.9148)	Acc@1 77.930 (77.656)	Acc@5 98.779 (98.665)
Epoch: [18][15/25]	Time 0.609 (0.607)	Data 0.006 (0.043)	Loss 0.8913 (0.9133)	Acc@1 78.369 (77.701)	Acc@5 98.779 (98.672)
Epoch: [18][16/25]	Time 0.592 (0.606)	Data 0.005 (0.041)	Loss 0.9252 (0.9140)	Acc@1 76.807 (77.648)	Acc@5 98.926 (98.687)
Epoch: [18][17/25]	Time 0.619 (0.607)	Data 0.006 (0.039)	Loss 0.9082 (0.9137)	Acc@1 77.637 (77.648)	Acc@5 98.389 (98.671)
Epoch: [18][18/25]	Time 0.586 (0.606)	Data 0.004 (0.037)	Loss 0.9032 (0.9132)	Acc@1 78.760 (77.706)	Acc@5 98.877 (98.682)
Epoch: [18][19/25]	Time 0.637 (0.607)	Data 0.005 (0.036)	Loss 0.9230 (0.9137)	Acc@1 77.637 (77.703)	Acc@5 98.730 (98.684)
Epoch: [18][20/25]	Time 0.589 (0.606)	Data 0.007 (0.034)	Loss 0.9166 (0.9138)	Acc@1 78.320 (77.732)	Acc@5 98.682 (98.684)
Epoch: [18][21/25]	Time 0.579 (0.605)	Data 0.004 (0.033)	Loss 0.8645 (0.9116)	Acc@1 79.053 (77.792)	Acc@5 98.633 (98.682)
Epoch: [18][22/25]	Time 0.594 (0.605)	Data 0.003 (0.032)	Loss 0.8824 (0.9103)	Acc@1 78.320 (77.815)	Acc@5 98.828 (98.688)
Epoch: [18][23/25]	Time 0.592 (0.604)	Data 0.004 (0.030)	Loss 0.8656 (0.9084)	Acc@1 79.443 (77.883)	Acc@5 98.877 (98.696)
Epoch: [18][24/25]	Time 0.361 (0.595)	Data 0.006 (0.029)	Loss 0.8617 (0.9076)	Acc@1 79.009 (77.902)	Acc@5 98.821 (98.698)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/25]	Time 0.688 (0.688)	Data 0.763 (0.763)	Loss 0.8844 (0.8844)	Acc@1 78.613 (78.613)	Acc@5 99.023 (99.023)
Epoch: [19][1/25]	Time 0.634 (0.661)	Data 0.003 (0.383)	Loss 0.9304 (0.9074)	Acc@1 76.758 (77.686)	Acc@5 98.877 (98.950)
Epoch: [19][2/25]	Time 0.561 (0.627)	Data 0.003 (0.256)	Loss 0.8703 (0.8950)	Acc@1 78.857 (78.076)	Acc@5 98.584 (98.828)
Epoch: [19][3/25]	Time 0.598 (0.620)	Data 0.004 (0.193)	Loss 0.8788 (0.8910)	Acc@1 78.125 (78.088)	Acc@5 98.633 (98.779)
Epoch: [19][4/25]	Time 0.610 (0.618)	Data 0.006 (0.156)	Loss 0.9002 (0.8928)	Acc@1 78.076 (78.086)	Acc@5 98.975 (98.818)
Epoch: [19][5/25]	Time 0.601 (0.615)	Data 0.006 (0.131)	Loss 0.8708 (0.8891)	Acc@1 78.906 (78.223)	Acc@5 98.535 (98.771)
Epoch: [19][6/25]	Time 0.638 (0.618)	Data 0.005 (0.113)	Loss 0.8778 (0.8875)	Acc@1 78.906 (78.320)	Acc@5 98.340 (98.710)
Epoch: [19][7/25]	Time 0.576 (0.613)	Data 0.003 (0.099)	Loss 0.8518 (0.8831)	Acc@1 81.445 (78.711)	Acc@5 99.170 (98.767)
Epoch: [19][8/25]	Time 0.601 (0.612)	Data 0.004 (0.088)	Loss 0.8810 (0.8828)	Acc@1 79.199 (78.765)	Acc@5 98.975 (98.790)
Epoch: [19][9/25]	Time 0.607 (0.611)	Data 0.008 (0.080)	Loss 0.8712 (0.8817)	Acc@1 78.955 (78.784)	Acc@5 98.779 (98.789)
Epoch: [19][10/25]	Time 0.579 (0.608)	Data 0.006 (0.074)	Loss 0.8492 (0.8787)	Acc@1 79.883 (78.884)	Acc@5 98.779 (98.788)
Epoch: [19][11/25]	Time 0.613 (0.609)	Data 0.005 (0.068)	Loss 0.8083 (0.8728)	Acc@1 81.152 (79.073)	Acc@5 98.730 (98.783)
Epoch: [19][12/25]	Time 0.600 (0.608)	Data 0.005 (0.063)	Loss 0.8345 (0.8699)	Acc@1 81.104 (79.229)	Acc@5 98.877 (98.791)
Epoch: [19][13/25]	Time 0.599 (0.607)	Data 0.005 (0.059)	Loss 0.8287 (0.8670)	Acc@1 80.518 (79.321)	Acc@5 99.316 (98.828)
Epoch: [19][14/25]	Time 0.618 (0.608)	Data 0.005 (0.055)	Loss 0.8429 (0.8654)	Acc@1 79.590 (79.339)	Acc@5 99.072 (98.844)
Epoch: [19][15/25]	Time 0.599 (0.607)	Data 0.005 (0.052)	Loss 0.8234 (0.8627)	Acc@1 81.201 (79.456)	Acc@5 98.682 (98.834)
Epoch: [19][16/25]	Time 0.608 (0.607)	Data 0.005 (0.049)	Loss 0.8629 (0.8627)	Acc@1 80.127 (79.495)	Acc@5 99.072 (98.848)
Epoch: [19][17/25]	Time 0.601 (0.607)	Data 0.004 (0.047)	Loss 0.8038 (0.8595)	Acc@1 80.957 (79.576)	Acc@5 98.828 (98.847)
Epoch: [19][18/25]	Time 0.645 (0.609)	Data 0.004 (0.045)	Loss 0.8603 (0.8595)	Acc@1 78.027 (79.495)	Acc@5 99.023 (98.856)
Epoch: [19][19/25]	Time 0.612 (0.609)	Data 0.005 (0.043)	Loss 0.8757 (0.8603)	Acc@1 78.516 (79.446)	Acc@5 98.975 (98.862)
Epoch: [19][20/25]	Time 0.629 (0.610)	Data 0.005 (0.041)	Loss 0.8460 (0.8596)	Acc@1 80.518 (79.497)	Acc@5 98.730 (98.856)
Epoch: [19][21/25]	Time 0.627 (0.611)	Data 0.004 (0.039)	Loss 0.8036 (0.8571)	Acc@1 81.055 (79.568)	Acc@5 99.219 (98.873)
Epoch: [19][22/25]	Time 0.632 (0.612)	Data 0.005 (0.038)	Loss 0.8182 (0.8554)	Acc@1 80.615 (79.613)	Acc@5 99.170 (98.885)
Epoch: [19][23/25]	Time 0.613 (0.612)	Data 0.004 (0.036)	Loss 0.8088 (0.8535)	Acc@1 82.422 (79.730)	Acc@5 99.023 (98.891)
Epoch: [19][24/25]	Time 0.324 (0.600)	Data 0.003 (0.035)	Loss 0.8118 (0.8528)	Acc@1 83.608 (79.796)	Acc@5 98.939 (98.892)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/25]	Time 0.608 (0.608)	Data 0.678 (0.678)	Loss 0.8583 (0.8583)	Acc@1 78.271 (78.271)	Acc@5 99.121 (99.121)
Epoch: [20][1/25]	Time 0.626 (0.617)	Data 0.004 (0.341)	Loss 0.8553 (0.8568)	Acc@1 78.662 (78.467)	Acc@5 99.023 (99.072)
Epoch: [20][2/25]	Time 0.571 (0.602)	Data 0.006 (0.230)	Loss 0.8501 (0.8546)	Acc@1 80.176 (79.036)	Acc@5 98.975 (99.040)
Epoch: [20][3/25]	Time 0.593 (0.600)	Data 0.005 (0.173)	Loss 0.8688 (0.8581)	Acc@1 78.955 (79.016)	Acc@5 98.730 (98.962)
Epoch: [20][4/25]	Time 0.600 (0.600)	Data 0.008 (0.140)	Loss 0.8399 (0.8545)	Acc@1 79.736 (79.160)	Acc@5 99.463 (99.062)
Epoch: [20][5/25]	Time 0.606 (0.601)	Data 0.004 (0.118)	Loss 0.8407 (0.8522)	Acc@1 79.541 (79.224)	Acc@5 99.121 (99.072)
Epoch: [20][6/25]	Time 0.618 (0.603)	Data 0.003 (0.101)	Loss 0.8224 (0.8479)	Acc@1 81.201 (79.506)	Acc@5 98.828 (99.037)
Epoch: [20][7/25]	Time 0.613 (0.604)	Data 0.006 (0.089)	Loss 0.8293 (0.8456)	Acc@1 80.469 (79.626)	Acc@5 99.268 (99.066)
Epoch: [20][8/25]	Time 0.608 (0.605)	Data 0.006 (0.080)	Loss 0.7957 (0.8401)	Acc@1 82.568 (79.953)	Acc@5 99.023 (99.061)
Epoch: [20][9/25]	Time 0.617 (0.606)	Data 0.005 (0.073)	Loss 0.7946 (0.8355)	Acc@1 82.373 (80.195)	Acc@5 98.877 (99.043)
Epoch: [20][10/25]	Time 0.612 (0.607)	Data 0.004 (0.066)	Loss 0.8313 (0.8351)	Acc@1 79.053 (80.091)	Acc@5 98.584 (99.001)
Epoch: [20][11/25]	Time 0.567 (0.603)	Data 0.004 (0.061)	Loss 0.8471 (0.8361)	Acc@1 79.736 (80.062)	Acc@5 98.779 (98.983)
Epoch: [20][12/25]	Time 0.634 (0.606)	Data 0.007 (0.057)	Loss 0.8216 (0.8350)	Acc@1 81.299 (80.157)	Acc@5 99.268 (99.005)
Epoch: [20][13/25]	Time 0.569 (0.603)	Data 0.004 (0.053)	Loss 0.8030 (0.8327)	Acc@1 81.104 (80.225)	Acc@5 99.023 (99.006)
Epoch: [20][14/25]	Time 0.634 (0.605)	Data 0.004 (0.050)	Loss 0.7987 (0.8305)	Acc@1 82.178 (80.355)	Acc@5 99.072 (99.010)
Epoch: [20][15/25]	Time 0.645 (0.608)	Data 0.005 (0.047)	Loss 0.7835 (0.8275)	Acc@1 81.885 (80.450)	Acc@5 99.121 (99.017)
Epoch: [20][16/25]	Time 0.610 (0.608)	Data 0.004 (0.045)	Loss 0.8422 (0.8284)	Acc@1 80.127 (80.431)	Acc@5 99.023 (99.018)
Epoch: [20][17/25]	Time 0.661 (0.611)	Data 0.005 (0.042)	Loss 0.7975 (0.8267)	Acc@1 81.738 (80.504)	Acc@5 98.633 (98.996)
Epoch: [20][18/25]	Time 0.572 (0.609)	Data 0.005 (0.040)	Loss 0.8361 (0.8272)	Acc@1 80.566 (80.507)	Acc@5 98.730 (98.982)
Epoch: [20][19/25]	Time 0.655 (0.611)	Data 0.005 (0.039)	Loss 0.7968 (0.8256)	Acc@1 82.812 (80.623)	Acc@5 98.682 (98.967)
Epoch: [20][20/25]	Time 0.563 (0.609)	Data 0.004 (0.037)	Loss 0.7789 (0.8234)	Acc@1 82.568 (80.715)	Acc@5 98.730 (98.956)
Epoch: [20][21/25]	Time 0.613 (0.609)	Data 0.006 (0.036)	Loss 0.8446 (0.8244)	Acc@1 80.420 (80.702)	Acc@5 98.877 (98.952)
Epoch: [20][22/25]	Time 0.568 (0.607)	Data 0.007 (0.034)	Loss 0.8025 (0.8234)	Acc@1 81.494 (80.736)	Acc@5 99.365 (98.970)
Epoch: [20][23/25]	Time 0.630 (0.608)	Data 0.005 (0.033)	Loss 0.7875 (0.8219)	Acc@1 81.885 (80.784)	Acc@5 98.682 (98.958)
Epoch: [20][24/25]	Time 0.351 (0.598)	Data 0.005 (0.032)	Loss 0.8148 (0.8218)	Acc@1 80.896 (80.786)	Acc@5 99.410 (98.966)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/25]	Time 0.615 (0.615)	Data 0.791 (0.791)	Loss 0.7493 (0.7493)	Acc@1 82.959 (82.959)	Acc@5 99.365 (99.365)
Epoch: [21][1/25]	Time 0.566 (0.590)	Data 0.004 (0.398)	Loss 0.7712 (0.7603)	Acc@1 81.885 (82.422)	Acc@5 98.535 (98.950)
Epoch: [21][2/25]	Time 0.625 (0.602)	Data 0.005 (0.267)	Loss 0.7357 (0.7521)	Acc@1 84.082 (82.975)	Acc@5 99.414 (99.105)
Epoch: [21][3/25]	Time 0.607 (0.603)	Data 0.005 (0.201)	Loss 0.7938 (0.7625)	Acc@1 82.080 (82.751)	Acc@5 99.072 (99.097)
Epoch: [21][4/25]	Time 0.622 (0.607)	Data 0.006 (0.162)	Loss 0.7704 (0.7641)	Acc@1 82.812 (82.764)	Acc@5 99.170 (99.111)
Epoch: [21][5/25]	Time 0.593 (0.605)	Data 0.006 (0.136)	Loss 0.7177 (0.7564)	Acc@1 84.619 (83.073)	Acc@5 99.268 (99.137)
Epoch: [21][6/25]	Time 0.591 (0.603)	Data 0.003 (0.117)	Loss 0.7503 (0.7555)	Acc@1 82.959 (83.057)	Acc@5 99.121 (99.135)
Epoch: [21][7/25]	Time 0.641 (0.608)	Data 0.003 (0.103)	Loss 0.7102 (0.7498)	Acc@1 84.326 (83.215)	Acc@5 99.219 (99.146)
Epoch: [21][8/25]	Time 0.619 (0.609)	Data 0.006 (0.092)	Loss 0.7582 (0.7508)	Acc@1 82.520 (83.138)	Acc@5 99.463 (99.181)
Epoch: [21][9/25]	Time 0.593 (0.607)	Data 0.005 (0.083)	Loss 0.7677 (0.7525)	Acc@1 82.031 (83.027)	Acc@5 99.170 (99.180)
Epoch: [21][10/25]	Time 0.620 (0.608)	Data 0.006 (0.076)	Loss 0.7467 (0.7519)	Acc@1 83.398 (83.061)	Acc@5 98.975 (99.161)
Epoch: [21][11/25]	Time 0.618 (0.609)	Data 0.006 (0.070)	Loss 0.7585 (0.7525)	Acc@1 81.885 (82.963)	Acc@5 99.170 (99.162)
Epoch: [21][12/25]	Time 0.627 (0.611)	Data 0.005 (0.065)	Loss 0.7977 (0.7560)	Acc@1 81.445 (82.846)	Acc@5 99.023 (99.151)
Epoch: [21][13/25]	Time 0.599 (0.610)	Data 0.006 (0.061)	Loss 0.7488 (0.7554)	Acc@1 83.105 (82.865)	Acc@5 99.121 (99.149)
Epoch: [21][14/25]	Time 0.621 (0.611)	Data 0.006 (0.058)	Loss 0.8003 (0.7584)	Acc@1 81.348 (82.764)	Acc@5 99.023 (99.141)
Epoch: [21][15/25]	Time 0.617 (0.611)	Data 0.006 (0.054)	Loss 0.7723 (0.7593)	Acc@1 82.129 (82.724)	Acc@5 99.170 (99.142)
Epoch: [21][16/25]	Time 0.624 (0.612)	Data 0.004 (0.051)	Loss 0.7986 (0.7616)	Acc@1 81.201 (82.634)	Acc@5 98.730 (99.118)
Epoch: [21][17/25]	Time 0.567 (0.609)	Data 0.005 (0.049)	Loss 0.7688 (0.7620)	Acc@1 82.324 (82.617)	Acc@5 99.316 (99.129)
Epoch: [21][18/25]	Time 0.595 (0.608)	Data 0.006 (0.046)	Loss 0.7687 (0.7624)	Acc@1 82.080 (82.589)	Acc@5 98.926 (99.119)
Epoch: [21][19/25]	Time 0.605 (0.608)	Data 0.003 (0.044)	Loss 0.7982 (0.7642)	Acc@1 81.396 (82.529)	Acc@5 99.072 (99.116)
Epoch: [21][20/25]	Time 0.585 (0.607)	Data 0.004 (0.042)	Loss 0.7771 (0.7648)	Acc@1 83.154 (82.559)	Acc@5 98.926 (99.107)
Epoch: [21][21/25]	Time 0.544 (0.604)	Data 0.005 (0.041)	Loss 0.7452 (0.7639)	Acc@1 82.568 (82.559)	Acc@5 99.072 (99.106)
Epoch: [21][22/25]	Time 0.646 (0.606)	Data 0.004 (0.039)	Loss 0.8081 (0.7658)	Acc@1 80.762 (82.481)	Acc@5 98.828 (99.093)
Epoch: [21][23/25]	Time 0.593 (0.606)	Data 0.005 (0.038)	Loss 0.7581 (0.7655)	Acc@1 82.861 (82.497)	Acc@5 99.121 (99.095)
Epoch: [21][24/25]	Time 0.399 (0.597)	Data 0.005 (0.036)	Loss 0.7766 (0.7657)	Acc@1 81.958 (82.488)	Acc@5 98.821 (99.090)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/25]	Time 0.629 (0.629)	Data 0.797 (0.797)	Loss 0.7590 (0.7590)	Acc@1 83.496 (83.496)	Acc@5 99.121 (99.121)
Epoch: [22][1/25]	Time 0.578 (0.604)	Data 0.004 (0.401)	Loss 0.7562 (0.7576)	Acc@1 83.057 (83.276)	Acc@5 99.365 (99.243)
Epoch: [22][2/25]	Time 0.631 (0.613)	Data 0.004 (0.268)	Loss 0.7644 (0.7599)	Acc@1 82.031 (82.861)	Acc@5 99.268 (99.251)
Epoch: [22][3/25]	Time 0.603 (0.610)	Data 0.006 (0.203)	Loss 0.7835 (0.7658)	Acc@1 81.885 (82.617)	Acc@5 99.170 (99.231)
Epoch: [22][4/25]	Time 0.621 (0.612)	Data 0.009 (0.164)	Loss 0.7446 (0.7615)	Acc@1 82.617 (82.617)	Acc@5 99.365 (99.258)
Epoch: [22][5/25]	Time 0.624 (0.614)	Data 0.003 (0.137)	Loss 0.7755 (0.7639)	Acc@1 82.764 (82.642)	Acc@5 99.121 (99.235)
Epoch: [22][6/25]	Time 0.570 (0.608)	Data 0.004 (0.118)	Loss 0.8022 (0.7693)	Acc@1 81.348 (82.457)	Acc@5 99.316 (99.247)
Epoch: [22][7/25]	Time 0.606 (0.608)	Data 0.004 (0.104)	Loss 0.7488 (0.7668)	Acc@1 83.008 (82.526)	Acc@5 99.268 (99.249)
Epoch: [22][8/25]	Time 0.628 (0.610)	Data 0.006 (0.093)	Loss 0.7651 (0.7666)	Acc@1 82.666 (82.541)	Acc@5 98.975 (99.219)
Epoch: [22][9/25]	Time 0.580 (0.607)	Data 0.004 (0.084)	Loss 0.7633 (0.7663)	Acc@1 82.861 (82.573)	Acc@5 99.268 (99.224)
Epoch: [22][10/25]	Time 0.650 (0.611)	Data 0.006 (0.077)	Loss 0.7383 (0.7637)	Acc@1 83.350 (82.644)	Acc@5 99.121 (99.214)
Epoch: [22][11/25]	Time 0.619 (0.612)	Data 0.005 (0.071)	Loss 0.7951 (0.7663)	Acc@1 81.494 (82.548)	Acc@5 99.170 (99.211)
Epoch: [22][12/25]	Time 0.597 (0.611)	Data 0.006 (0.066)	Loss 0.7862 (0.7679)	Acc@1 81.836 (82.493)	Acc@5 98.926 (99.189)
Epoch: [22][13/25]	Time 0.602 (0.610)	Data 0.006 (0.062)	Loss 0.7419 (0.7660)	Acc@1 83.496 (82.565)	Acc@5 99.561 (99.215)
Epoch: [22][14/25]	Time 0.630 (0.611)	Data 0.006 (0.058)	Loss 0.7878 (0.7675)	Acc@1 82.275 (82.546)	Acc@5 98.779 (99.186)
Epoch: [22][15/25]	Time 0.630 (0.612)	Data 0.005 (0.055)	Loss 0.7473 (0.7662)	Acc@1 83.594 (82.611)	Acc@5 99.121 (99.182)
Epoch: [22][16/25]	Time 0.623 (0.613)	Data 0.006 (0.052)	Loss 0.7851 (0.7673)	Acc@1 82.227 (82.588)	Acc@5 98.926 (99.167)
Epoch: [22][17/25]	Time 0.570 (0.611)	Data 0.003 (0.049)	Loss 0.7480 (0.7662)	Acc@1 82.861 (82.604)	Acc@5 98.926 (99.154)
Epoch: [22][18/25]	Time 0.623 (0.611)	Data 0.005 (0.047)	Loss 0.7332 (0.7645)	Acc@1 83.936 (82.674)	Acc@5 98.779 (99.134)
Epoch: [22][19/25]	Time 0.578 (0.610)	Data 0.003 (0.045)	Loss 0.8291 (0.7677)	Acc@1 80.566 (82.568)	Acc@5 98.584 (99.106)
Epoch: [22][20/25]	Time 0.602 (0.609)	Data 0.005 (0.043)	Loss 0.7935 (0.7690)	Acc@1 81.836 (82.533)	Acc@5 98.926 (99.098)
Epoch: [22][21/25]	Time 0.620 (0.610)	Data 0.003 (0.041)	Loss 0.7515 (0.7682)	Acc@1 83.301 (82.568)	Acc@5 99.023 (99.094)
Epoch: [22][22/25]	Time 0.583 (0.609)	Data 0.004 (0.039)	Loss 0.7915 (0.7692)	Acc@1 81.836 (82.537)	Acc@5 98.828 (99.083)
Epoch: [22][23/25]	Time 0.564 (0.607)	Data 0.005 (0.038)	Loss 0.7700 (0.7692)	Acc@1 82.715 (82.544)	Acc@5 99.316 (99.093)
Epoch: [22][24/25]	Time 0.374 (0.598)	Data 0.005 (0.037)	Loss 0.7427 (0.7688)	Acc@1 82.665 (82.546)	Acc@5 99.646 (99.102)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/25]	Time 0.624 (0.624)	Data 0.801 (0.801)	Loss 0.7675 (0.7675)	Acc@1 81.738 (81.738)	Acc@5 99.219 (99.219)
Epoch: [23][1/25]	Time 0.603 (0.613)	Data 0.005 (0.403)	Loss 0.7665 (0.7670)	Acc@1 82.275 (82.007)	Acc@5 99.170 (99.194)
Epoch: [23][2/25]	Time 0.640 (0.622)	Data 0.005 (0.270)	Loss 0.7956 (0.7765)	Acc@1 81.641 (81.885)	Acc@5 99.023 (99.137)
Epoch: [23][3/25]	Time 0.581 (0.612)	Data 0.006 (0.204)	Loss 0.7501 (0.7699)	Acc@1 83.594 (82.312)	Acc@5 99.316 (99.182)
Epoch: [23][4/25]	Time 0.628 (0.615)	Data 0.007 (0.165)	Loss 0.7275 (0.7614)	Acc@1 84.277 (82.705)	Acc@5 99.512 (99.248)
Epoch: [23][5/25]	Time 0.573 (0.608)	Data 0.003 (0.138)	Loss 0.7747 (0.7637)	Acc@1 82.764 (82.715)	Acc@5 99.023 (99.211)
Epoch: [23][6/25]	Time 0.636 (0.612)	Data 0.005 (0.119)	Loss 0.7281 (0.7586)	Acc@1 83.936 (82.889)	Acc@5 99.268 (99.219)
Epoch: [23][7/25]	Time 0.568 (0.607)	Data 0.004 (0.104)	Loss 0.7573 (0.7584)	Acc@1 83.105 (82.916)	Acc@5 99.072 (99.200)
Epoch: [23][8/25]	Time 0.594 (0.605)	Data 0.004 (0.093)	Loss 0.7545 (0.7580)	Acc@1 82.812 (82.905)	Acc@5 99.072 (99.186)
Epoch: [23][9/25]	Time 0.631 (0.608)	Data 0.006 (0.084)	Loss 0.7475 (0.7569)	Acc@1 82.031 (82.817)	Acc@5 99.219 (99.189)
Epoch: [23][10/25]	Time 0.569 (0.604)	Data 0.005 (0.077)	Loss 0.7431 (0.7557)	Acc@1 83.105 (82.844)	Acc@5 99.121 (99.183)
Epoch: [23][11/25]	Time 0.642 (0.607)	Data 0.006 (0.071)	Loss 0.7581 (0.7559)	Acc@1 82.568 (82.821)	Acc@5 99.414 (99.202)
Epoch: [23][12/25]	Time 0.569 (0.604)	Data 0.007 (0.066)	Loss 0.7522 (0.7556)	Acc@1 82.812 (82.820)	Acc@5 99.316 (99.211)
Epoch: [23][13/25]	Time 0.662 (0.609)	Data 0.006 (0.062)	Loss 0.7426 (0.7547)	Acc@1 83.594 (82.875)	Acc@5 99.316 (99.219)
Epoch: [23][14/25]	Time 0.627 (0.610)	Data 0.005 (0.058)	Loss 0.7283 (0.7529)	Acc@1 83.545 (82.920)	Acc@5 99.316 (99.225)
Epoch: [23][15/25]	Time 0.623 (0.611)	Data 0.005 (0.055)	Loss 0.7612 (0.7534)	Acc@1 82.324 (82.883)	Acc@5 98.828 (99.200)
Epoch: [23][16/25]	Time 0.604 (0.610)	Data 0.007 (0.052)	Loss 0.7354 (0.7524)	Acc@1 83.350 (82.910)	Acc@5 99.072 (99.193)
Epoch: [23][17/25]	Time 0.653 (0.613)	Data 0.006 (0.049)	Loss 0.8021 (0.7551)	Acc@1 80.664 (82.785)	Acc@5 99.463 (99.208)
Epoch: [23][18/25]	Time 0.635 (0.614)	Data 0.006 (0.047)	Loss 0.7840 (0.7566)	Acc@1 81.787 (82.733)	Acc@5 99.414 (99.219)
Epoch: [23][19/25]	Time 0.630 (0.615)	Data 0.005 (0.045)	Loss 0.7633 (0.7570)	Acc@1 82.715 (82.732)	Acc@5 98.975 (99.207)
Epoch: [23][20/25]	Time 0.587 (0.613)	Data 0.006 (0.043)	Loss 0.7829 (0.7582)	Acc@1 82.666 (82.729)	Acc@5 99.316 (99.212)
Epoch: [23][21/25]	Time 0.594 (0.612)	Data 0.003 (0.041)	Loss 0.7717 (0.7588)	Acc@1 81.885 (82.690)	Acc@5 99.121 (99.208)
Epoch: [23][22/25]	Time 0.624 (0.613)	Data 0.005 (0.040)	Loss 0.7519 (0.7585)	Acc@1 82.520 (82.683)	Acc@5 99.023 (99.200)
Epoch: [23][23/25]	Time 0.593 (0.612)	Data 0.005 (0.038)	Loss 0.7676 (0.7589)	Acc@1 81.689 (82.642)	Acc@5 98.877 (99.186)
Epoch: [23][24/25]	Time 0.385 (0.603)	Data 0.006 (0.037)	Loss 0.8277 (0.7601)	Acc@1 81.958 (82.630)	Acc@5 99.175 (99.186)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/25]	Time 0.622 (0.622)	Data 0.748 (0.748)	Loss 0.7212 (0.7212)	Acc@1 83.496 (83.496)	Acc@5 99.316 (99.316)
Epoch: [24][1/25]	Time 0.616 (0.619)	Data 0.003 (0.376)	Loss 0.7369 (0.7291)	Acc@1 83.545 (83.521)	Acc@5 99.268 (99.292)
Epoch: [24][2/25]	Time 0.583 (0.607)	Data 0.005 (0.252)	Loss 0.7223 (0.7268)	Acc@1 84.375 (83.805)	Acc@5 99.170 (99.251)
Epoch: [24][3/25]	Time 0.619 (0.610)	Data 0.005 (0.190)	Loss 0.7622 (0.7356)	Acc@1 81.885 (83.325)	Acc@5 99.316 (99.268)
Epoch: [24][4/25]	Time 0.596 (0.607)	Data 0.013 (0.155)	Loss 0.7366 (0.7358)	Acc@1 83.691 (83.398)	Acc@5 99.023 (99.219)
Epoch: [24][5/25]	Time 0.615 (0.608)	Data 0.004 (0.130)	Loss 0.7021 (0.7302)	Acc@1 84.473 (83.577)	Acc@5 99.512 (99.268)
Epoch: [24][6/25]	Time 0.618 (0.610)	Data 0.005 (0.112)	Loss 0.7397 (0.7316)	Acc@1 82.910 (83.482)	Acc@5 99.023 (99.233)
Epoch: [24][7/25]	Time 0.598 (0.608)	Data 0.005 (0.098)	Loss 0.7250 (0.7307)	Acc@1 83.984 (83.545)	Acc@5 99.219 (99.231)
Epoch: [24][8/25]	Time 0.608 (0.608)	Data 0.006 (0.088)	Loss 0.7317 (0.7308)	Acc@1 83.740 (83.567)	Acc@5 99.268 (99.235)
Epoch: [24][9/25]	Time 0.583 (0.606)	Data 0.005 (0.080)	Loss 0.7155 (0.7293)	Acc@1 84.473 (83.657)	Acc@5 99.414 (99.253)
Epoch: [24][10/25]	Time 0.657 (0.610)	Data 0.008 (0.073)	Loss 0.7479 (0.7310)	Acc@1 83.301 (83.625)	Acc@5 99.170 (99.245)
Epoch: [24][11/25]	Time 0.610 (0.610)	Data 0.006 (0.068)	Loss 0.7355 (0.7314)	Acc@1 83.838 (83.643)	Acc@5 99.023 (99.227)
Epoch: [24][12/25]	Time 0.630 (0.612)	Data 0.007 (0.063)	Loss 0.7772 (0.7349)	Acc@1 82.861 (83.582)	Acc@5 99.170 (99.223)
Epoch: [24][13/25]	Time 0.668 (0.616)	Data 0.006 (0.059)	Loss 0.7186 (0.7337)	Acc@1 83.496 (83.576)	Acc@5 99.609 (99.250)
Epoch: [24][14/25]	Time 0.612 (0.616)	Data 0.006 (0.055)	Loss 0.7281 (0.7334)	Acc@1 83.057 (83.542)	Acc@5 99.463 (99.264)
Epoch: [24][15/25]	Time 0.641 (0.617)	Data 0.005 (0.052)	Loss 0.7304 (0.7332)	Acc@1 83.447 (83.536)	Acc@5 99.609 (99.286)
Epoch: [24][16/25]	Time 0.586 (0.615)	Data 0.006 (0.049)	Loss 0.7090 (0.7318)	Acc@1 85.059 (83.625)	Acc@5 99.316 (99.288)
Epoch: [24][17/25]	Time 0.631 (0.616)	Data 0.006 (0.047)	Loss 0.7211 (0.7312)	Acc@1 83.740 (83.632)	Acc@5 99.121 (99.278)
Epoch: [24][18/25]	Time 0.619 (0.616)	Data 0.006 (0.045)	Loss 0.7537 (0.7323)	Acc@1 83.398 (83.619)	Acc@5 99.268 (99.278)
Epoch: [24][19/25]	Time 0.599 (0.616)	Data 0.005 (0.043)	Loss 0.7431 (0.7329)	Acc@1 83.936 (83.635)	Acc@5 99.219 (99.275)
Epoch: [24][20/25]	Time 0.640 (0.617)	Data 0.005 (0.041)	Loss 0.7564 (0.7340)	Acc@1 83.154 (83.612)	Acc@5 98.828 (99.254)
Epoch: [24][21/25]	Time 0.633 (0.617)	Data 0.005 (0.039)	Loss 0.7396 (0.7343)	Acc@1 84.082 (83.634)	Acc@5 99.121 (99.248)
Epoch: [24][22/25]	Time 0.610 (0.617)	Data 0.004 (0.038)	Loss 0.7120 (0.7333)	Acc@1 84.570 (83.674)	Acc@5 99.561 (99.261)
Epoch: [24][23/25]	Time 0.579 (0.616)	Data 0.005 (0.037)	Loss 0.7467 (0.7338)	Acc@1 82.812 (83.639)	Acc@5 99.170 (99.257)
Epoch: [24][24/25]	Time 0.383 (0.606)	Data 0.006 (0.035)	Loss 0.6963 (0.7332)	Acc@1 84.670 (83.656)	Acc@5 98.939 (99.252)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/25]	Time 0.631 (0.631)	Data 0.778 (0.778)	Loss 0.7786 (0.7786)	Acc@1 81.543 (81.543)	Acc@5 98.730 (98.730)
Epoch: [25][1/25]	Time 0.589 (0.610)	Data 0.004 (0.391)	Loss 0.7182 (0.7484)	Acc@1 83.203 (82.373)	Acc@5 99.316 (99.023)
Epoch: [25][2/25]	Time 0.608 (0.609)	Data 0.003 (0.262)	Loss 0.7095 (0.7354)	Acc@1 83.447 (82.731)	Acc@5 98.975 (99.007)
Epoch: [25][3/25]	Time 0.629 (0.614)	Data 0.004 (0.197)	Loss 0.7127 (0.7297)	Acc@1 83.984 (83.044)	Acc@5 99.121 (99.036)
Epoch: [25][4/25]	Time 0.597 (0.611)	Data 0.008 (0.159)	Loss 0.7069 (0.7252)	Acc@1 85.400 (83.516)	Acc@5 99.219 (99.072)
Epoch: [25][5/25]	Time 0.631 (0.614)	Data 0.004 (0.134)	Loss 0.7099 (0.7226)	Acc@1 84.863 (83.740)	Acc@5 99.170 (99.089)
Epoch: [25][6/25]	Time 0.653 (0.620)	Data 0.004 (0.115)	Loss 0.7595 (0.7279)	Acc@1 83.398 (83.691)	Acc@5 98.975 (99.072)
Epoch: [25][7/25]	Time 0.613 (0.619)	Data 0.004 (0.101)	Loss 0.6938 (0.7236)	Acc@1 85.107 (83.868)	Acc@5 99.414 (99.115)
Epoch: [25][8/25]	Time 0.640 (0.621)	Data 0.006 (0.091)	Loss 0.7080 (0.7219)	Acc@1 84.180 (83.903)	Acc@5 99.561 (99.164)
Epoch: [25][9/25]	Time 0.598 (0.619)	Data 0.005 (0.082)	Loss 0.7186 (0.7216)	Acc@1 83.984 (83.911)	Acc@5 99.219 (99.170)
Epoch: [25][10/25]	Time 0.646 (0.621)	Data 0.006 (0.075)	Loss 0.7038 (0.7200)	Acc@1 84.814 (83.993)	Acc@5 99.561 (99.205)
Epoch: [25][11/25]	Time 0.625 (0.622)	Data 0.005 (0.069)	Loss 0.7072 (0.7189)	Acc@1 84.131 (84.005)	Acc@5 99.170 (99.202)
Epoch: [25][12/25]	Time 0.617 (0.621)	Data 0.005 (0.064)	Loss 0.6705 (0.7152)	Acc@1 85.449 (84.116)	Acc@5 99.658 (99.238)
Epoch: [25][13/25]	Time 0.600 (0.620)	Data 0.006 (0.060)	Loss 0.7324 (0.7164)	Acc@1 83.643 (84.082)	Acc@5 99.170 (99.233)
Epoch: [25][14/25]	Time 0.584 (0.617)	Data 0.005 (0.057)	Loss 0.7042 (0.7156)	Acc@1 84.717 (84.124)	Acc@5 99.561 (99.255)
Epoch: [25][15/25]	Time 0.623 (0.618)	Data 0.004 (0.053)	Loss 0.7058 (0.7150)	Acc@1 84.619 (84.155)	Acc@5 99.268 (99.255)
Epoch: [25][16/25]	Time 0.591 (0.616)	Data 0.005 (0.050)	Loss 0.7260 (0.7156)	Acc@1 83.496 (84.116)	Acc@5 99.170 (99.250)
Epoch: [25][17/25]	Time 0.608 (0.616)	Data 0.005 (0.048)	Loss 0.7240 (0.7161)	Acc@1 83.691 (84.093)	Acc@5 99.170 (99.246)
Epoch: [25][18/25]	Time 0.565 (0.613)	Data 0.005 (0.046)	Loss 0.6768 (0.7140)	Acc@1 85.889 (84.187)	Acc@5 99.609 (99.265)
Epoch: [25][19/25]	Time 0.643 (0.615)	Data 0.005 (0.044)	Loss 0.6864 (0.7126)	Acc@1 84.814 (84.219)	Acc@5 99.512 (99.277)
Epoch: [25][20/25]	Time 0.591 (0.613)	Data 0.006 (0.042)	Loss 0.7005 (0.7121)	Acc@1 84.424 (84.229)	Acc@5 99.316 (99.279)
Epoch: [25][21/25]	Time 0.634 (0.614)	Data 0.006 (0.040)	Loss 0.7079 (0.7119)	Acc@1 84.668 (84.248)	Acc@5 99.072 (99.270)
Epoch: [25][22/25]	Time 0.618 (0.615)	Data 0.005 (0.039)	Loss 0.7159 (0.7121)	Acc@1 83.301 (84.207)	Acc@5 99.512 (99.280)
Epoch: [25][23/25]	Time 0.595 (0.614)	Data 0.007 (0.037)	Loss 0.6923 (0.7112)	Acc@1 84.766 (84.231)	Acc@5 99.268 (99.280)
Epoch: [25][24/25]	Time 0.333 (0.602)	Data 0.005 (0.036)	Loss 0.6970 (0.7110)	Acc@1 84.198 (84.230)	Acc@5 99.175 (99.278)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [26 | 180] LR: 0.100000
Epoch: [26][0/25]	Time 0.575 (0.575)	Data 0.671 (0.671)	Loss 0.7301 (0.7301)	Acc@1 83.887 (83.887)	Acc@5 99.365 (99.365)
Epoch: [26][1/25]	Time 0.611 (0.593)	Data 0.006 (0.338)	Loss 0.6403 (0.6852)	Acc@1 86.621 (85.254)	Acc@5 99.268 (99.316)
Epoch: [26][2/25]	Time 0.603 (0.596)	Data 0.003 (0.227)	Loss 0.7172 (0.6959)	Acc@1 84.082 (84.863)	Acc@5 99.121 (99.251)
Epoch: [26][3/25]	Time 0.566 (0.589)	Data 0.005 (0.171)	Loss 0.6655 (0.6883)	Acc@1 85.400 (84.998)	Acc@5 99.561 (99.329)
Epoch: [26][4/25]	Time 0.627 (0.596)	Data 0.006 (0.138)	Loss 0.6718 (0.6850)	Acc@1 85.693 (85.137)	Acc@5 99.121 (99.287)
Epoch: [26][5/25]	Time 0.577 (0.593)	Data 0.006 (0.116)	Loss 0.6535 (0.6797)	Acc@1 86.572 (85.376)	Acc@5 99.316 (99.292)
Epoch: [26][6/25]	Time 0.624 (0.598)	Data 0.004 (0.100)	Loss 0.6788 (0.6796)	Acc@1 85.986 (85.463)	Acc@5 99.268 (99.289)
Epoch: [26][7/25]	Time 0.627 (0.601)	Data 0.006 (0.088)	Loss 0.6683 (0.6782)	Acc@1 85.400 (85.455)	Acc@5 99.561 (99.323)
Epoch: [26][8/25]	Time 0.586 (0.599)	Data 0.006 (0.079)	Loss 0.6586 (0.6760)	Acc@1 86.182 (85.536)	Acc@5 99.463 (99.338)
Epoch: [26][9/25]	Time 0.613 (0.601)	Data 0.004 (0.072)	Loss 0.6793 (0.6763)	Acc@1 85.742 (85.557)	Acc@5 99.121 (99.316)
Epoch: [26][10/25]	Time 0.633 (0.604)	Data 0.006 (0.066)	Loss 0.6826 (0.6769)	Acc@1 84.570 (85.467)	Acc@5 99.463 (99.330)
Epoch: [26][11/25]	Time 0.574 (0.601)	Data 0.006 (0.061)	Loss 0.6808 (0.6772)	Acc@1 85.205 (85.445)	Acc@5 99.512 (99.345)
Epoch: [26][12/25]	Time 0.623 (0.603)	Data 0.006 (0.057)	Loss 0.7190 (0.6804)	Acc@1 85.156 (85.423)	Acc@5 98.926 (99.313)
Epoch: [26][13/25]	Time 0.613 (0.604)	Data 0.004 (0.053)	Loss 0.6498 (0.6782)	Acc@1 86.719 (85.515)	Acc@5 99.316 (99.313)
Epoch: [26][14/25]	Time 0.610 (0.604)	Data 0.006 (0.050)	Loss 0.6699 (0.6777)	Acc@1 86.328 (85.570)	Acc@5 99.023 (99.294)
Epoch: [26][15/25]	Time 0.619 (0.605)	Data 0.006 (0.047)	Loss 0.6895 (0.6784)	Acc@1 85.449 (85.562)	Acc@5 99.365 (99.298)
Epoch: [26][16/25]	Time 0.620 (0.606)	Data 0.006 (0.044)	Loss 0.7132 (0.6805)	Acc@1 83.789 (85.458)	Acc@5 99.707 (99.322)
Epoch: [26][17/25]	Time 0.589 (0.605)	Data 0.004 (0.042)	Loss 0.6914 (0.6811)	Acc@1 84.766 (85.419)	Acc@5 99.365 (99.325)
Epoch: [26][18/25]	Time 0.551 (0.602)	Data 0.006 (0.040)	Loss 0.6554 (0.6797)	Acc@1 86.182 (85.459)	Acc@5 99.463 (99.332)
Epoch: [26][19/25]	Time 0.642 (0.604)	Data 0.005 (0.039)	Loss 0.6728 (0.6794)	Acc@1 84.912 (85.432)	Acc@5 99.512 (99.341)
Epoch: [26][20/25]	Time 0.625 (0.605)	Data 0.005 (0.037)	Loss 0.6712 (0.6790)	Acc@1 85.693 (85.445)	Acc@5 99.463 (99.347)
Epoch: [26][21/25]	Time 0.567 (0.603)	Data 0.004 (0.035)	Loss 0.7017 (0.6800)	Acc@1 84.717 (85.411)	Acc@5 99.121 (99.336)
Epoch: [26][22/25]	Time 0.560 (0.601)	Data 0.006 (0.034)	Loss 0.7279 (0.6821)	Acc@1 83.545 (85.330)	Acc@5 99.414 (99.340)
Epoch: [26][23/25]	Time 0.685 (0.605)	Data 0.004 (0.033)	Loss 0.6730 (0.6817)	Acc@1 84.473 (85.295)	Acc@5 99.658 (99.353)
Epoch: [26][24/25]	Time 0.397 (0.597)	Data 0.005 (0.032)	Loss 0.7027 (0.6821)	Acc@1 84.198 (85.276)	Acc@5 99.528 (99.356)

Epoch: [27 | 180] LR: 0.100000
Epoch: [27][0/25]	Time 0.629 (0.629)	Data 0.712 (0.712)	Loss 0.7272 (0.7272)	Acc@1 83.301 (83.301)	Acc@5 99.365 (99.365)
Epoch: [27][1/25]	Time 0.609 (0.619)	Data 0.007 (0.359)	Loss 0.6923 (0.7097)	Acc@1 84.326 (83.813)	Acc@5 99.414 (99.390)
Epoch: [27][2/25]	Time 0.627 (0.622)	Data 0.006 (0.242)	Loss 0.6986 (0.7060)	Acc@1 84.961 (84.196)	Acc@5 99.121 (99.300)
Epoch: [27][3/25]	Time 0.598 (0.616)	Data 0.006 (0.183)	Loss 0.7196 (0.7094)	Acc@1 83.447 (84.009)	Acc@5 99.463 (99.341)
Epoch: [27][4/25]	Time 0.619 (0.617)	Data 0.006 (0.147)	Loss 0.6921 (0.7060)	Acc@1 85.205 (84.248)	Acc@5 99.463 (99.365)
Epoch: [27][5/25]	Time 0.554 (0.606)	Data 0.009 (0.124)	Loss 0.6814 (0.7019)	Acc@1 84.961 (84.367)	Acc@5 99.170 (99.333)
Epoch: [27][6/25]	Time 0.564 (0.600)	Data 0.006 (0.107)	Loss 0.7064 (0.7025)	Acc@1 84.863 (84.438)	Acc@5 99.268 (99.323)
Epoch: [27][7/25]	Time 0.617 (0.602)	Data 0.005 (0.095)	Loss 0.6561 (0.6967)	Acc@1 86.768 (84.729)	Acc@5 99.561 (99.353)
Epoch: [27][8/25]	Time 0.582 (0.600)	Data 0.007 (0.085)	Loss 0.7063 (0.6978)	Acc@1 83.545 (84.597)	Acc@5 99.512 (99.371)
Epoch: [27][9/25]	Time 0.593 (0.599)	Data 0.004 (0.077)	Loss 0.6805 (0.6961)	Acc@1 85.156 (84.653)	Acc@5 99.121 (99.346)
Epoch: [27][10/25]	Time 0.623 (0.601)	Data 0.005 (0.070)	Loss 0.6663 (0.6934)	Acc@1 86.475 (84.819)	Acc@5 99.072 (99.321)
Epoch: [27][11/25]	Time 0.626 (0.604)	Data 0.006 (0.065)	Loss 0.6769 (0.6920)	Acc@1 85.449 (84.871)	Acc@5 99.365 (99.325)
Epoch: [27][12/25]	Time 0.578 (0.602)	Data 0.005 (0.060)	Loss 0.7143 (0.6937)	Acc@1 84.912 (84.875)	Acc@5 99.121 (99.309)
Epoch: [27][13/25]	Time 0.675 (0.607)	Data 0.007 (0.056)	Loss 0.6652 (0.6917)	Acc@1 85.352 (84.909)	Acc@5 99.707 (99.337)
Epoch: [27][14/25]	Time 0.600 (0.606)	Data 0.005 (0.053)	Loss 0.7124 (0.6930)	Acc@1 84.766 (84.899)	Acc@5 99.365 (99.339)
Epoch: [27][15/25]	Time 0.623 (0.607)	Data 0.004 (0.050)	Loss 0.6617 (0.6911)	Acc@1 85.840 (84.958)	Acc@5 99.512 (99.350)
Epoch: [27][16/25]	Time 0.621 (0.608)	Data 0.004 (0.047)	Loss 0.7267 (0.6932)	Acc@1 83.643 (84.881)	Acc@5 99.463 (99.357)
Epoch: [27][17/25]	Time 0.611 (0.608)	Data 0.004 (0.045)	Loss 0.6858 (0.6928)	Acc@1 85.010 (84.888)	Acc@5 99.609 (99.371)
Epoch: [27][18/25]	Time 0.603 (0.608)	Data 0.004 (0.043)	Loss 0.6924 (0.6928)	Acc@1 84.424 (84.863)	Acc@5 99.170 (99.360)
Epoch: [27][19/25]	Time 0.611 (0.608)	Data 0.005 (0.041)	Loss 0.7037 (0.6933)	Acc@1 84.229 (84.832)	Acc@5 99.268 (99.355)
Epoch: [27][20/25]	Time 0.597 (0.608)	Data 0.006 (0.039)	Loss 0.6916 (0.6932)	Acc@1 84.570 (84.819)	Acc@5 99.658 (99.370)
Epoch: [27][21/25]	Time 0.654 (0.610)	Data 0.005 (0.038)	Loss 0.7058 (0.6938)	Acc@1 85.205 (84.837)	Acc@5 99.121 (99.359)
Epoch: [27][22/25]	Time 0.612 (0.610)	Data 0.005 (0.036)	Loss 0.6809 (0.6932)	Acc@1 85.254 (84.855)	Acc@5 98.926 (99.340)
Epoch: [27][23/25]	Time 0.583 (0.609)	Data 0.004 (0.035)	Loss 0.7164 (0.6942)	Acc@1 84.375 (84.835)	Acc@5 98.975 (99.325)
Epoch: [27][24/25]	Time 0.342 (0.598)	Data 0.006 (0.034)	Loss 0.7209 (0.6946)	Acc@1 83.137 (84.806)	Acc@5 99.410 (99.326)

Epoch: [28 | 180] LR: 0.100000
Epoch: [28][0/25]	Time 0.608 (0.608)	Data 0.833 (0.833)	Loss 0.7154 (0.7154)	Acc@1 83.301 (83.301)	Acc@5 99.414 (99.414)
Epoch: [28][1/25]	Time 0.626 (0.617)	Data 0.005 (0.419)	Loss 0.6719 (0.6937)	Acc@1 85.449 (84.375)	Acc@5 99.219 (99.316)
Epoch: [28][2/25]	Time 0.608 (0.614)	Data 0.005 (0.281)	Loss 0.7054 (0.6976)	Acc@1 84.668 (84.473)	Acc@5 99.463 (99.365)
Epoch: [28][3/25]	Time 0.564 (0.602)	Data 0.006 (0.212)	Loss 0.6947 (0.6969)	Acc@1 84.473 (84.473)	Acc@5 99.268 (99.341)
Epoch: [28][4/25]	Time 0.661 (0.614)	Data 0.009 (0.171)	Loss 0.7045 (0.6984)	Acc@1 84.131 (84.404)	Acc@5 99.268 (99.326)
Epoch: [28][5/25]	Time 0.555 (0.604)	Data 0.005 (0.144)	Loss 0.7093 (0.7002)	Acc@1 85.156 (84.530)	Acc@5 99.170 (99.300)
Epoch: [28][6/25]	Time 0.648 (0.610)	Data 0.006 (0.124)	Loss 0.6926 (0.6991)	Acc@1 85.156 (84.619)	Acc@5 99.219 (99.289)
Epoch: [28][7/25]	Time 0.583 (0.607)	Data 0.004 (0.109)	Loss 0.6833 (0.6971)	Acc@1 85.791 (84.766)	Acc@5 99.268 (99.286)
Epoch: [28][8/25]	Time 0.633 (0.610)	Data 0.004 (0.097)	Loss 0.6704 (0.6942)	Acc@1 84.619 (84.749)	Acc@5 99.170 (99.273)
Epoch: [28][9/25]	Time 0.637 (0.612)	Data 0.005 (0.088)	Loss 0.6727 (0.6920)	Acc@1 85.107 (84.785)	Acc@5 99.414 (99.287)
Epoch: [28][10/25]	Time 0.642 (0.615)	Data 0.006 (0.081)	Loss 0.6947 (0.6923)	Acc@1 85.400 (84.841)	Acc@5 98.975 (99.259)
Epoch: [28][11/25]	Time 0.623 (0.616)	Data 0.007 (0.075)	Loss 0.7100 (0.6937)	Acc@1 83.984 (84.770)	Acc@5 99.316 (99.264)
Epoch: [28][12/25]	Time 0.591 (0.614)	Data 0.008 (0.069)	Loss 0.7355 (0.6970)	Acc@1 82.666 (84.608)	Acc@5 99.219 (99.260)
Epoch: [28][13/25]	Time 0.614 (0.614)	Data 0.005 (0.065)	Loss 0.6448 (0.6932)	Acc@1 86.572 (84.748)	Acc@5 99.658 (99.289)
Epoch: [28][14/25]	Time 0.611 (0.614)	Data 0.006 (0.061)	Loss 0.7117 (0.6945)	Acc@1 83.789 (84.684)	Acc@5 99.316 (99.290)
Epoch: [28][15/25]	Time 0.628 (0.615)	Data 0.004 (0.057)	Loss 0.6512 (0.6918)	Acc@1 85.547 (84.738)	Acc@5 99.316 (99.292)
Epoch: [28][16/25]	Time 0.577 (0.612)	Data 0.007 (0.054)	Loss 0.7174 (0.6933)	Acc@1 84.277 (84.711)	Acc@5 98.975 (99.273)
Epoch: [28][17/25]	Time 0.637 (0.614)	Data 0.004 (0.052)	Loss 0.6790 (0.6925)	Acc@1 84.717 (84.711)	Acc@5 99.609 (99.292)
Epoch: [28][18/25]	Time 0.593 (0.613)	Data 0.006 (0.049)	Loss 0.6992 (0.6928)	Acc@1 84.717 (84.712)	Acc@5 98.975 (99.275)
Epoch: [28][19/25]	Time 0.580 (0.611)	Data 0.003 (0.047)	Loss 0.6903 (0.6927)	Acc@1 85.254 (84.739)	Acc@5 99.414 (99.282)
Epoch: [28][20/25]	Time 0.660 (0.613)	Data 0.005 (0.045)	Loss 0.6865 (0.6924)	Acc@1 84.131 (84.710)	Acc@5 99.463 (99.291)
Epoch: [28][21/25]	Time 0.576 (0.612)	Data 0.004 (0.043)	Loss 0.7074 (0.6931)	Acc@1 84.229 (84.688)	Acc@5 99.414 (99.296)
Epoch: [28][22/25]	Time 0.631 (0.613)	Data 0.005 (0.041)	Loss 0.7057 (0.6936)	Acc@1 84.082 (84.662)	Acc@5 99.463 (99.304)
Epoch: [28][23/25]	Time 0.635 (0.614)	Data 0.005 (0.040)	Loss 0.6516 (0.6919)	Acc@1 85.596 (84.701)	Acc@5 99.512 (99.312)
Epoch: [28][24/25]	Time 0.352 (0.603)	Data 0.005 (0.038)	Loss 0.7691 (0.6932)	Acc@1 82.547 (84.664)	Acc@5 98.703 (99.302)

Epoch: [29 | 180] LR: 0.100000
Epoch: [29][0/25]	Time 0.595 (0.595)	Data 0.805 (0.805)	Loss 0.7195 (0.7195)	Acc@1 83.643 (83.643)	Acc@5 99.561 (99.561)
Epoch: [29][1/25]	Time 0.630 (0.612)	Data 0.005 (0.405)	Loss 0.6697 (0.6946)	Acc@1 85.840 (84.741)	Acc@5 99.561 (99.561)
Epoch: [29][2/25]	Time 0.642 (0.622)	Data 0.005 (0.272)	Loss 0.6929 (0.6940)	Acc@1 84.863 (84.782)	Acc@5 99.316 (99.479)
Epoch: [29][3/25]	Time 0.649 (0.629)	Data 0.005 (0.205)	Loss 0.7059 (0.6970)	Acc@1 84.912 (84.814)	Acc@5 99.414 (99.463)
Epoch: [29][4/25]	Time 0.605 (0.624)	Data 0.006 (0.165)	Loss 0.6792 (0.6934)	Acc@1 85.547 (84.961)	Acc@5 99.512 (99.473)
Epoch: [29][5/25]	Time 0.597 (0.620)	Data 0.007 (0.139)	Loss 0.7109 (0.6964)	Acc@1 84.619 (84.904)	Acc@5 99.561 (99.487)
Epoch: [29][6/25]	Time 0.604 (0.617)	Data 0.006 (0.120)	Loss 0.7060 (0.6977)	Acc@1 84.082 (84.787)	Acc@5 99.121 (99.435)
Epoch: [29][7/25]	Time 0.613 (0.617)	Data 0.005 (0.105)	Loss 0.6975 (0.6977)	Acc@1 84.619 (84.766)	Acc@5 99.316 (99.420)
Epoch: [29][8/25]	Time 0.632 (0.619)	Data 0.006 (0.094)	Loss 0.6896 (0.6968)	Acc@1 84.961 (84.787)	Acc@5 99.316 (99.409)
Epoch: [29][9/25]	Time 0.608 (0.617)	Data 0.004 (0.085)	Loss 0.6816 (0.6953)	Acc@1 85.059 (84.814)	Acc@5 99.023 (99.370)
Epoch: [29][10/25]	Time 0.609 (0.617)	Data 0.004 (0.078)	Loss 0.6985 (0.6956)	Acc@1 84.521 (84.788)	Acc@5 99.219 (99.356)
Epoch: [29][11/25]	Time 0.588 (0.614)	Data 0.006 (0.072)	Loss 0.6958 (0.6956)	Acc@1 84.668 (84.778)	Acc@5 99.609 (99.377)
Epoch: [29][12/25]	Time 0.647 (0.617)	Data 0.006 (0.067)	Loss 0.6896 (0.6951)	Acc@1 85.205 (84.811)	Acc@5 99.512 (99.388)
Epoch: [29][13/25]	Time 0.568 (0.613)	Data 0.005 (0.062)	Loss 0.6723 (0.6935)	Acc@1 85.352 (84.849)	Acc@5 99.414 (99.390)
Epoch: [29][14/25]	Time 0.621 (0.614)	Data 0.004 (0.059)	Loss 0.6526 (0.6908)	Acc@1 86.377 (84.951)	Acc@5 99.512 (99.398)
Epoch: [29][15/25]	Time 0.583 (0.612)	Data 0.007 (0.055)	Loss 0.6930 (0.6909)	Acc@1 85.156 (84.964)	Acc@5 99.414 (99.399)
Epoch: [29][16/25]	Time 0.656 (0.615)	Data 0.006 (0.052)	Loss 0.6653 (0.6894)	Acc@1 85.791 (85.013)	Acc@5 99.512 (99.405)
Epoch: [29][17/25]	Time 0.621 (0.615)	Data 0.005 (0.050)	Loss 0.6854 (0.6892)	Acc@1 84.717 (84.996)	Acc@5 99.707 (99.422)
Epoch: [29][18/25]	Time 0.624 (0.615)	Data 0.007 (0.048)	Loss 0.7435 (0.6921)	Acc@1 82.471 (84.863)	Acc@5 99.121 (99.406)
Epoch: [29][19/25]	Time 0.562 (0.613)	Data 0.007 (0.046)	Loss 0.6978 (0.6923)	Acc@1 84.668 (84.854)	Acc@5 99.658 (99.419)
Epoch: [29][20/25]	Time 0.611 (0.613)	Data 0.005 (0.044)	Loss 0.6595 (0.6908)	Acc@1 86.377 (84.926)	Acc@5 99.268 (99.412)
Epoch: [29][21/25]	Time 0.575 (0.611)	Data 0.005 (0.042)	Loss 0.6798 (0.6903)	Acc@1 85.205 (84.939)	Acc@5 99.756 (99.427)
Epoch: [29][22/25]	Time 0.638 (0.612)	Data 0.004 (0.040)	Loss 0.6693 (0.6894)	Acc@1 85.205 (84.950)	Acc@5 99.463 (99.429)
Epoch: [29][23/25]	Time 0.622 (0.613)	Data 0.005 (0.039)	Loss 0.6877 (0.6893)	Acc@1 85.254 (84.963)	Acc@5 99.316 (99.424)
Epoch: [29][24/25]	Time 0.383 (0.603)	Data 0.006 (0.037)	Loss 0.7250 (0.6899)	Acc@1 84.080 (84.948)	Acc@5 99.646 (99.428)

Epoch: [30 | 180] LR: 0.100000
Epoch: [30][0/25]	Time 0.635 (0.635)	Data 0.818 (0.818)	Loss 0.6591 (0.6591)	Acc@1 86.328 (86.328)	Acc@5 99.316 (99.316)
Epoch: [30][1/25]	Time 0.605 (0.620)	Data 0.004 (0.411)	Loss 0.6949 (0.6770)	Acc@1 84.912 (85.620)	Acc@5 99.512 (99.414)
Epoch: [30][2/25]	Time 0.587 (0.609)	Data 0.005 (0.276)	Loss 0.6722 (0.6754)	Acc@1 85.498 (85.579)	Acc@5 99.268 (99.365)
Epoch: [30][3/25]	Time 0.623 (0.613)	Data 0.005 (0.208)	Loss 0.7166 (0.6857)	Acc@1 84.375 (85.278)	Acc@5 99.365 (99.365)
Epoch: [30][4/25]	Time 0.611 (0.612)	Data 0.005 (0.168)	Loss 0.6718 (0.6829)	Acc@1 86.230 (85.469)	Acc@5 99.463 (99.385)
Epoch: [30][5/25]	Time 0.597 (0.610)	Data 0.004 (0.140)	Loss 0.6316 (0.6744)	Acc@1 86.230 (85.596)	Acc@5 99.414 (99.390)
Epoch: [30][6/25]	Time 0.628 (0.612)	Data 0.004 (0.121)	Loss 0.6847 (0.6758)	Acc@1 86.133 (85.672)	Acc@5 99.561 (99.414)
Epoch: [30][7/25]	Time 0.627 (0.614)	Data 0.006 (0.106)	Loss 0.6947 (0.6782)	Acc@1 85.498 (85.651)	Acc@5 99.170 (99.384)
Epoch: [30][8/25]	Time 0.609 (0.613)	Data 0.005 (0.095)	Loss 0.6550 (0.6756)	Acc@1 86.426 (85.737)	Acc@5 99.561 (99.403)
Epoch: [30][9/25]	Time 0.580 (0.610)	Data 0.003 (0.086)	Loss 0.6722 (0.6753)	Acc@1 85.840 (85.747)	Acc@5 99.316 (99.395)
Epoch: [30][10/25]	Time 0.614 (0.611)	Data 0.006 (0.079)	Loss 0.6816 (0.6759)	Acc@1 85.986 (85.769)	Acc@5 99.268 (99.383)
Epoch: [30][11/25]	Time 0.592 (0.609)	Data 0.005 (0.073)	Loss 0.6744 (0.6757)	Acc@1 86.475 (85.828)	Acc@5 99.463 (99.390)
Epoch: [30][12/25]	Time 0.620 (0.610)	Data 0.004 (0.067)	Loss 0.6798 (0.6760)	Acc@1 84.814 (85.750)	Acc@5 99.512 (99.399)
Epoch: [30][13/25]	Time 0.629 (0.611)	Data 0.004 (0.063)	Loss 0.7069 (0.6783)	Acc@1 84.863 (85.686)	Acc@5 99.512 (99.407)
Epoch: [30][14/25]	Time 0.629 (0.612)	Data 0.004 (0.059)	Loss 0.6694 (0.6777)	Acc@1 86.572 (85.745)	Acc@5 99.121 (99.388)
Epoch: [30][15/25]	Time 0.619 (0.613)	Data 0.007 (0.056)	Loss 0.6880 (0.6783)	Acc@1 85.205 (85.712)	Acc@5 98.975 (99.362)
Epoch: [30][16/25]	Time 0.571 (0.610)	Data 0.008 (0.053)	Loss 0.6585 (0.6771)	Acc@1 85.938 (85.725)	Acc@5 99.170 (99.351)
Epoch: [30][17/25]	Time 0.630 (0.611)	Data 0.005 (0.050)	Loss 0.6380 (0.6750)	Acc@1 86.865 (85.788)	Acc@5 99.365 (99.352)
Epoch: [30][18/25]	Time 0.596 (0.611)	Data 0.005 (0.048)	Loss 0.7039 (0.6765)	Acc@1 84.766 (85.734)	Acc@5 99.463 (99.358)
Epoch: [30][19/25]	Time 0.620 (0.611)	Data 0.005 (0.046)	Loss 0.6989 (0.6776)	Acc@1 84.863 (85.691)	Acc@5 99.609 (99.370)
Epoch: [30][20/25]	Time 0.560 (0.609)	Data 0.005 (0.044)	Loss 0.6567 (0.6766)	Acc@1 86.084 (85.710)	Acc@5 99.365 (99.370)
Epoch: [30][21/25]	Time 0.634 (0.610)	Data 0.005 (0.042)	Loss 0.6271 (0.6744)	Acc@1 87.451 (85.789)	Acc@5 99.072 (99.356)
Epoch: [30][22/25]	Time 0.638 (0.611)	Data 0.005 (0.040)	Loss 0.6736 (0.6743)	Acc@1 84.912 (85.751)	Acc@5 99.512 (99.363)
Epoch: [30][23/25]	Time 0.622 (0.611)	Data 0.006 (0.039)	Loss 0.6940 (0.6752)	Acc@1 84.082 (85.681)	Acc@5 99.219 (99.357)
Epoch: [30][24/25]	Time 0.321 (0.600)	Data 0.005 (0.037)	Loss 0.7148 (0.6758)	Acc@1 85.377 (85.676)	Acc@5 99.292 (99.356)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoch: [31 | 180] LR: 0.100000
Epoch: [31][0/25]	Time 0.595 (0.595)	Data 0.790 (0.790)	Loss 0.6523 (0.6523)	Acc@1 86.621 (86.621)	Acc@5 99.707 (99.707)
Epoch: [31][1/25]	Time 0.543 (0.569)	Data 0.003 (0.397)	Loss 0.5894 (0.6208)	Acc@1 88.037 (87.329)	Acc@5 99.658 (99.683)
Epoch: [31][2/25]	Time 0.626 (0.588)	Data 0.005 (0.266)	Loss 0.5940 (0.6119)	Acc@1 88.721 (87.793)	Acc@5 99.805 (99.723)
Epoch: [31][3/25]	Time 0.603 (0.591)	Data 0.004 (0.201)	Loss 0.5486 (0.5961)	Acc@1 90.479 (88.464)	Acc@5 99.609 (99.695)
Epoch: [31][4/25]	Time 0.586 (0.590)	Data 0.007 (0.162)	Loss 0.6034 (0.5975)	Acc@1 88.037 (88.379)	Acc@5 99.561 (99.668)
Epoch: [31][5/25]	Time 0.613 (0.594)	Data 0.006 (0.136)	Loss 0.6257 (0.6022)	Acc@1 86.963 (88.143)	Acc@5 99.561 (99.650)
Epoch: [31][6/25]	Time 0.601 (0.595)	Data 0.005 (0.117)	Loss 0.6153 (0.6041)	Acc@1 86.768 (87.946)	Acc@5 99.756 (99.665)
Epoch: [31][7/25]	Time 0.609 (0.597)	Data 0.004 (0.103)	Loss 0.6045 (0.6042)	Acc@1 88.525 (88.019)	Acc@5 99.609 (99.658)
Epoch: [31][8/25]	Time 0.594 (0.597)	Data 0.006 (0.092)	Loss 0.6090 (0.6047)	Acc@1 87.549 (87.967)	Acc@5 99.512 (99.642)
Epoch: [31][9/25]	Time 0.579 (0.595)	Data 0.006 (0.084)	Loss 0.6453 (0.6087)	Acc@1 86.914 (87.861)	Acc@5 99.365 (99.614)
Epoch: [31][10/25]	Time 0.618 (0.597)	Data 0.004 (0.076)	Loss 0.6177 (0.6096)	Acc@1 86.865 (87.771)	Acc@5 99.658 (99.618)
Epoch: [31][11/25]	Time 0.604 (0.598)	Data 0.006 (0.070)	Loss 0.5941 (0.6083)	Acc@1 87.939 (87.785)	Acc@5 99.658 (99.622)
Epoch: [31][12/25]	Time 0.597 (0.597)	Data 0.006 (0.065)	Loss 0.6259 (0.6096)	Acc@1 87.061 (87.729)	Acc@5 99.707 (99.628)
Epoch: [31][13/25]	Time 0.597 (0.597)	Data 0.006 (0.061)	Loss 0.6689 (0.6139)	Acc@1 85.791 (87.591)	Acc@5 99.316 (99.606)
Epoch: [31][14/25]	Time 0.641 (0.600)	Data 0.006 (0.058)	Loss 0.6271 (0.6147)	Acc@1 87.549 (87.588)	Acc@5 99.316 (99.587)
Epoch: [31][15/25]	Time 0.604 (0.601)	Data 0.006 (0.054)	Loss 0.6547 (0.6172)	Acc@1 86.279 (87.506)	Acc@5 99.463 (99.579)
Epoch: [31][16/25]	Time 0.567 (0.599)	Data 0.006 (0.052)	Loss 0.6288 (0.6179)	Acc@1 86.963 (87.474)	Acc@5 99.609 (99.581)
Epoch: [31][17/25]	Time 0.615 (0.600)	Data 0.004 (0.049)	Loss 0.6326 (0.6187)	Acc@1 87.012 (87.448)	Acc@5 99.414 (99.571)
Epoch: [31][18/25]	Time 0.621 (0.601)	Data 0.005 (0.047)	Loss 0.6452 (0.6201)	Acc@1 86.670 (87.407)	Acc@5 99.219 (99.553)
Epoch: [31][19/25]	Time 0.569 (0.599)	Data 0.004 (0.044)	Loss 0.6553 (0.6219)	Acc@1 85.986 (87.336)	Acc@5 99.463 (99.548)
Epoch: [31][20/25]	Time 0.597 (0.599)	Data 0.003 (0.042)	Loss 0.6562 (0.6235)	Acc@1 86.572 (87.300)	Acc@5 99.414 (99.542)
Epoch: [31][21/25]	Time 0.599 (0.599)	Data 0.005 (0.041)	Loss 0.6761 (0.6259)	Acc@1 85.889 (87.236)	Acc@5 99.170 (99.525)
Epoch: [31][22/25]	Time 0.640 (0.601)	Data 0.005 (0.039)	Loss 0.6770 (0.6281)	Acc@1 85.449 (87.158)	Acc@5 98.926 (99.499)
Epoch: [31][23/25]	Time 0.622 (0.602)	Data 0.004 (0.038)	Loss 0.6511 (0.6291)	Acc@1 86.719 (87.140)	Acc@5 99.463 (99.497)
Epoch: [31][24/25]	Time 0.348 (0.591)	Data 0.005 (0.036)	Loss 0.6044 (0.6287)	Acc@1 87.736 (87.150)	Acc@5 99.528 (99.498)

Epoch: [32 | 180] LR: 0.100000
Epoch: [32][0/25]	Time 0.590 (0.590)	Data 0.668 (0.668)	Loss 0.6574 (0.6574)	Acc@1 86.621 (86.621)	Acc@5 99.463 (99.463)
Epoch: [32][1/25]	Time 0.648 (0.619)	Data 0.009 (0.339)	Loss 0.6525 (0.6549)	Acc@1 86.816 (86.719)	Acc@5 99.512 (99.487)
Epoch: [32][2/25]	Time 0.569 (0.602)	Data 0.006 (0.228)	Loss 0.6681 (0.6593)	Acc@1 85.938 (86.458)	Acc@5 99.414 (99.463)
Epoch: [32][3/25]	Time 0.630 (0.609)	Data 0.006 (0.172)	Loss 0.6524 (0.6576)	Acc@1 86.377 (86.438)	Acc@5 99.170 (99.390)
Epoch: [32][4/25]	Time 0.592 (0.606)	Data 0.004 (0.139)	Loss 0.6391 (0.6539)	Acc@1 86.523 (86.455)	Acc@5 99.658 (99.443)
Epoch: [32][5/25]	Time 0.645 (0.612)	Data 0.007 (0.117)	Loss 0.6386 (0.6513)	Acc@1 86.865 (86.523)	Acc@5 99.316 (99.422)
Epoch: [32][6/25]	Time 0.584 (0.608)	Data 0.003 (0.100)	Loss 0.6743 (0.6546)	Acc@1 85.498 (86.377)	Acc@5 99.365 (99.414)
Epoch: [32][7/25]	Time 0.630 (0.611)	Data 0.005 (0.088)	Loss 0.6775 (0.6575)	Acc@1 84.961 (86.200)	Acc@5 99.512 (99.426)
Epoch: [32][8/25]	Time 0.614 (0.611)	Data 0.005 (0.079)	Loss 0.6491 (0.6565)	Acc@1 85.986 (86.176)	Acc@5 99.414 (99.425)
Epoch: [32][9/25]	Time 0.617 (0.612)	Data 0.004 (0.072)	Loss 0.6608 (0.6570)	Acc@1 86.084 (86.167)	Acc@5 99.609 (99.443)
Epoch: [32][10/25]	Time 0.585 (0.609)	Data 0.007 (0.066)	Loss 0.6683 (0.6580)	Acc@1 85.791 (86.133)	Acc@5 99.121 (99.414)
Epoch: [32][11/25]	Time 0.568 (0.606)	Data 0.004 (0.061)	Loss 0.6657 (0.6586)	Acc@1 85.498 (86.080)	Acc@5 99.414 (99.414)
Epoch: [32][12/25]	Time 0.611 (0.606)	Data 0.004 (0.056)	Loss 0.6122 (0.6551)	Acc@1 87.793 (86.212)	Acc@5 99.609 (99.429)
Epoch: [32][13/25]	Time 0.551 (0.602)	Data 0.004 (0.053)	Loss 0.6605 (0.6555)	Acc@1 85.352 (86.150)	Acc@5 99.414 (99.428)
Epoch: [32][14/25]	Time 0.573 (0.600)	Data 0.006 (0.049)	Loss 0.6198 (0.6531)	Acc@1 86.816 (86.195)	Acc@5 99.561 (99.437)
Epoch: [32][15/25]	Time 0.609 (0.601)	Data 0.006 (0.047)	Loss 0.6744 (0.6544)	Acc@1 84.668 (86.099)	Acc@5 99.268 (99.426)
Epoch: [32][16/25]	Time 0.579 (0.600)	Data 0.005 (0.044)	Loss 0.6213 (0.6525)	Acc@1 86.426 (86.118)	Acc@5 99.561 (99.434)
Epoch: [32][17/25]	Time 0.625 (0.601)	Data 0.005 (0.042)	Loss 0.6434 (0.6520)	Acc@1 86.719 (86.152)	Acc@5 99.658 (99.447)
Epoch: [32][18/25]	Time 0.634 (0.603)	Data 0.005 (0.040)	Loss 0.6514 (0.6519)	Acc@1 87.012 (86.197)	Acc@5 99.365 (99.442)
Epoch: [32][19/25]	Time 0.582 (0.602)	Data 0.005 (0.038)	Loss 0.6592 (0.6523)	Acc@1 86.426 (86.208)	Acc@5 99.561 (99.448)
Epoch: [32][20/25]	Time 0.601 (0.602)	Data 0.005 (0.037)	Loss 0.6407 (0.6517)	Acc@1 86.963 (86.244)	Acc@5 99.609 (99.456)
Epoch: [32][21/25]	Time 0.627 (0.603)	Data 0.005 (0.035)	Loss 0.6742 (0.6528)	Acc@1 85.498 (86.210)	Acc@5 99.414 (99.454)
Epoch: [32][22/25]	Time 0.609 (0.603)	Data 0.006 (0.034)	Loss 0.6709 (0.6536)	Acc@1 85.303 (86.171)	Acc@5 99.463 (99.454)
Epoch: [32][23/25]	Time 0.604 (0.603)	Data 0.005 (0.033)	Loss 0.6632 (0.6540)	Acc@1 85.303 (86.135)	Acc@5 99.316 (99.449)
Epoch: [32][24/25]	Time 0.334 (0.592)	Data 0.007 (0.032)	Loss 0.6996 (0.6547)	Acc@1 84.552 (86.108)	Acc@5 99.175 (99.444)

Epoch: [33 | 180] LR: 0.100000
Epoch: [33][0/25]	Time 0.691 (0.691)	Data 0.842 (0.842)	Loss 0.6360 (0.6360)	Acc@1 86.084 (86.084)	Acc@5 99.609 (99.609)
Epoch: [33][1/25]	Time 0.616 (0.653)	Data 0.003 (0.422)	Loss 0.6848 (0.6604)	Acc@1 85.205 (85.645)	Acc@5 99.170 (99.390)
Epoch: [33][2/25]	Time 0.633 (0.646)	Data 0.004 (0.283)	Loss 0.6608 (0.6606)	Acc@1 85.889 (85.726)	Acc@5 99.512 (99.430)
Epoch: [33][3/25]	Time 0.630 (0.642)	Data 0.003 (0.213)	Loss 0.6826 (0.6661)	Acc@1 85.352 (85.632)	Acc@5 99.268 (99.390)
Epoch: [33][4/25]	Time 0.625 (0.639)	Data 0.008 (0.172)	Loss 0.6771 (0.6683)	Acc@1 85.498 (85.605)	Acc@5 99.414 (99.395)
Epoch: [33][5/25]	Time 0.610 (0.634)	Data 0.005 (0.144)	Loss 0.6702 (0.6686)	Acc@1 84.961 (85.498)	Acc@5 99.561 (99.422)
Epoch: [33][6/25]	Time 0.577 (0.626)	Data 0.004 (0.124)	Loss 0.6584 (0.6671)	Acc@1 86.377 (85.624)	Acc@5 99.072 (99.372)
Epoch: [33][7/25]	Time 0.620 (0.625)	Data 0.003 (0.109)	Loss 0.6693 (0.6674)	Acc@1 85.352 (85.590)	Acc@5 99.463 (99.384)
Epoch: [33][8/25]	Time 0.610 (0.623)	Data 0.008 (0.098)	Loss 0.6780 (0.6686)	Acc@1 85.156 (85.541)	Acc@5 99.365 (99.382)
Epoch: [33][9/25]	Time 0.573 (0.618)	Data 0.005 (0.088)	Loss 0.6635 (0.6681)	Acc@1 85.938 (85.581)	Acc@5 99.316 (99.375)
Epoch: [33][10/25]	Time 0.589 (0.616)	Data 0.004 (0.081)	Loss 0.6618 (0.6675)	Acc@1 85.303 (85.556)	Acc@5 99.561 (99.392)
Epoch: [33][11/25]	Time 0.639 (0.618)	Data 0.006 (0.075)	Loss 0.6750 (0.6681)	Acc@1 85.645 (85.563)	Acc@5 99.658 (99.414)
Epoch: [33][12/25]	Time 0.605 (0.617)	Data 0.006 (0.069)	Loss 0.6632 (0.6677)	Acc@1 86.230 (85.614)	Acc@5 99.365 (99.410)
Epoch: [33][13/25]	Time 0.610 (0.616)	Data 0.007 (0.065)	Loss 0.6568 (0.6670)	Acc@1 85.889 (85.634)	Acc@5 99.316 (99.404)
Epoch: [33][14/25]	Time 0.587 (0.614)	Data 0.005 (0.061)	Loss 0.6577 (0.6663)	Acc@1 86.279 (85.677)	Acc@5 99.219 (99.391)
Epoch: [33][15/25]	Time 0.645 (0.616)	Data 0.006 (0.057)	Loss 0.6761 (0.6670)	Acc@1 86.768 (85.745)	Acc@5 99.170 (99.377)
Epoch: [33][16/25]	Time 0.624 (0.617)	Data 0.006 (0.054)	Loss 0.6796 (0.6677)	Acc@1 85.938 (85.757)	Acc@5 99.414 (99.380)
Epoch: [33][17/25]	Time 0.563 (0.614)	Data 0.006 (0.052)	Loss 0.7146 (0.6703)	Acc@1 84.473 (85.685)	Acc@5 99.414 (99.382)
Epoch: [33][18/25]	Time 0.626 (0.614)	Data 0.005 (0.049)	Loss 0.6768 (0.6706)	Acc@1 85.156 (85.657)	Acc@5 99.268 (99.376)
Epoch: [33][19/25]	Time 0.611 (0.614)	Data 0.005 (0.047)	Loss 0.6622 (0.6702)	Acc@1 85.938 (85.671)	Acc@5 99.609 (99.387)
Epoch: [33][20/25]	Time 0.573 (0.612)	Data 0.005 (0.045)	Loss 0.6730 (0.6704)	Acc@1 85.596 (85.668)	Acc@5 99.463 (99.391)
Epoch: [33][21/25]	Time 0.631 (0.613)	Data 0.005 (0.043)	Loss 0.6445 (0.6692)	Acc@1 86.719 (85.716)	Acc@5 99.023 (99.374)
Epoch: [33][22/25]	Time 0.645 (0.614)	Data 0.005 (0.041)	Loss 0.6644 (0.6690)	Acc@1 85.596 (85.710)	Acc@5 99.609 (99.384)
Epoch: [33][23/25]	Time 0.599 (0.614)	Data 0.006 (0.040)	Loss 0.7148 (0.6709)	Acc@1 84.082 (85.642)	Acc@5 99.268 (99.379)
Epoch: [33][24/25]	Time 0.316 (0.602)	Data 0.005 (0.039)	Loss 0.6672 (0.6708)	Acc@1 85.024 (85.632)	Acc@5 99.410 (99.380)

Epoch: [34 | 180] LR: 0.100000
Epoch: [34][0/25]	Time 0.591 (0.591)	Data 0.781 (0.781)	Loss 0.6236 (0.6236)	Acc@1 87.158 (87.158)	Acc@5 99.561 (99.561)
Epoch: [34][1/25]	Time 0.649 (0.620)	Data 0.005 (0.393)	Loss 0.6535 (0.6385)	Acc@1 86.670 (86.914)	Acc@5 99.609 (99.585)
Epoch: [34][2/25]	Time 0.630 (0.623)	Data 0.006 (0.264)	Loss 0.6594 (0.6455)	Acc@1 86.279 (86.702)	Acc@5 99.268 (99.479)
Epoch: [34][3/25]	Time 0.599 (0.617)	Data 0.006 (0.199)	Loss 0.6800 (0.6541)	Acc@1 84.326 (86.108)	Acc@5 99.512 (99.487)
Epoch: [34][4/25]	Time 0.593 (0.612)	Data 0.007 (0.161)	Loss 0.6847 (0.6602)	Acc@1 85.205 (85.928)	Acc@5 99.609 (99.512)
Epoch: [34][5/25]	Time 0.563 (0.604)	Data 0.003 (0.135)	Loss 0.6344 (0.6559)	Acc@1 86.621 (86.043)	Acc@5 99.512 (99.512)
Epoch: [34][6/25]	Time 0.600 (0.603)	Data 0.004 (0.116)	Loss 0.6158 (0.6502)	Acc@1 87.158 (86.203)	Acc@5 99.561 (99.519)
Epoch: [34][7/25]	Time 0.610 (0.604)	Data 0.005 (0.102)	Loss 0.6845 (0.6545)	Acc@1 85.791 (86.151)	Acc@5 99.072 (99.463)
Epoch: [34][8/25]	Time 0.629 (0.607)	Data 0.007 (0.092)	Loss 0.6408 (0.6530)	Acc@1 86.719 (86.214)	Acc@5 99.609 (99.479)
Epoch: [34][9/25]	Time 0.645 (0.611)	Data 0.005 (0.083)	Loss 0.6721 (0.6549)	Acc@1 85.059 (86.099)	Acc@5 99.268 (99.458)
Epoch: [34][10/25]	Time 0.626 (0.612)	Data 0.005 (0.076)	Loss 0.6927 (0.6583)	Acc@1 86.133 (86.102)	Acc@5 99.268 (99.441)
Epoch: [34][11/25]	Time 0.605 (0.612)	Data 0.004 (0.070)	Loss 0.6246 (0.6555)	Acc@1 87.354 (86.206)	Acc@5 99.756 (99.467)
Epoch: [34][12/25]	Time 0.615 (0.612)	Data 0.004 (0.065)	Loss 0.6718 (0.6568)	Acc@1 86.084 (86.197)	Acc@5 99.072 (99.437)
Epoch: [34][13/25]	Time 0.563 (0.608)	Data 0.005 (0.061)	Loss 0.6404 (0.6556)	Acc@1 86.377 (86.210)	Acc@5 99.512 (99.442)
Epoch: [34][14/25]	Time 0.610 (0.608)	Data 0.006 (0.057)	Loss 0.6831 (0.6574)	Acc@1 85.645 (86.172)	Acc@5 99.463 (99.443)
Epoch: [34][15/25]	Time 0.607 (0.608)	Data 0.004 (0.054)	Loss 0.6405 (0.6564)	Acc@1 86.719 (86.206)	Acc@5 99.658 (99.457)
Epoch: [34][16/25]	Time 0.632 (0.610)	Data 0.006 (0.051)	Loss 0.6633 (0.6568)	Acc@1 86.621 (86.230)	Acc@5 99.463 (99.457)
Epoch: [34][17/25]	Time 0.584 (0.608)	Data 0.005 (0.048)	Loss 0.6823 (0.6582)	Acc@1 85.254 (86.176)	Acc@5 99.170 (99.441)
Epoch: [34][18/25]	Time 0.642 (0.610)	Data 0.005 (0.046)	Loss 0.6701 (0.6588)	Acc@1 86.035 (86.169)	Acc@5 99.170 (99.427)
Epoch: [34][19/25]	Time 0.575 (0.608)	Data 0.004 (0.044)	Loss 0.6672 (0.6592)	Acc@1 86.426 (86.182)	Acc@5 99.365 (99.424)
Epoch: [34][20/25]	Time 0.633 (0.609)	Data 0.005 (0.042)	Loss 0.6660 (0.6596)	Acc@1 86.426 (86.193)	Acc@5 99.268 (99.416)
Epoch: [34][21/25]	Time 0.560 (0.607)	Data 0.005 (0.040)	Loss 0.6554 (0.6594)	Acc@1 87.012 (86.230)	Acc@5 99.609 (99.425)
Epoch: [34][22/25]	Time 0.728 (0.612)	Data 0.005 (0.039)	Loss 0.6415 (0.6586)	Acc@1 87.109 (86.269)	Acc@5 99.658 (99.435)
Epoch: [34][23/25]	Time 0.671 (0.615)	Data 0.005 (0.037)	Loss 0.6806 (0.6595)	Acc@1 85.693 (86.245)	Acc@5 99.268 (99.428)
Epoch: [34][24/25]	Time 0.388 (0.606)	Data 0.005 (0.036)	Loss 0.7217 (0.6606)	Acc@1 84.198 (86.210)	Acc@5 99.528 (99.430)

Epoch: [35 | 180] LR: 0.100000
Epoch: [35][0/25]	Time 0.627 (0.627)	Data 0.833 (0.833)	Loss 0.6616 (0.6616)	Acc@1 86.084 (86.084)	Acc@5 99.561 (99.561)
Epoch: [35][1/25]	Time 0.591 (0.609)	Data 0.005 (0.419)	Loss 0.6569 (0.6593)	Acc@1 85.596 (85.840)	Acc@5 99.365 (99.463)
Epoch: [35][2/25]	Time 0.533 (0.584)	Data 0.003 (0.280)	Loss 0.6530 (0.6572)	Acc@1 86.377 (86.019)	Acc@5 99.268 (99.398)
Epoch: [35][3/25]	Time 0.623 (0.594)	Data 0.005 (0.212)	Loss 0.6898 (0.6653)	Acc@1 84.473 (85.632)	Acc@5 99.414 (99.402)
Epoch: [35][4/25]	Time 0.553 (0.585)	Data 0.006 (0.170)	Loss 0.6286 (0.6580)	Acc@1 87.451 (85.996)	Acc@5 99.463 (99.414)
Epoch: [35][5/25]	Time 0.621 (0.591)	Data 0.008 (0.143)	Loss 0.6523 (0.6570)	Acc@1 86.816 (86.133)	Acc@5 99.512 (99.430)
Epoch: [35][6/25]	Time 0.574 (0.589)	Data 0.005 (0.123)	Loss 0.6636 (0.6580)	Acc@1 86.279 (86.154)	Acc@5 99.268 (99.407)
Epoch: [35][7/25]	Time 0.620 (0.593)	Data 0.005 (0.109)	Loss 0.6474 (0.6566)	Acc@1 86.865 (86.243)	Acc@5 99.609 (99.432)
Epoch: [35][8/25]	Time 0.574 (0.591)	Data 0.004 (0.097)	Loss 0.6560 (0.6566)	Acc@1 86.670 (86.290)	Acc@5 99.463 (99.436)
Epoch: [35][9/25]	Time 0.609 (0.593)	Data 0.004 (0.088)	Loss 0.6344 (0.6544)	Acc@1 87.500 (86.411)	Acc@5 99.414 (99.434)
Epoch: [35][10/25]	Time 0.612 (0.594)	Data 0.005 (0.080)	Loss 0.6584 (0.6547)	Acc@1 86.572 (86.426)	Acc@5 99.268 (99.419)
Epoch: [35][11/25]	Time 0.571 (0.592)	Data 0.004 (0.074)	Loss 0.6773 (0.6566)	Acc@1 85.498 (86.348)	Acc@5 99.463 (99.422)
Epoch: [35][12/25]	Time 0.578 (0.591)	Data 0.007 (0.069)	Loss 0.6529 (0.6563)	Acc@1 85.449 (86.279)	Acc@5 99.463 (99.425)
Epoch: [35][13/25]	Time 0.631 (0.594)	Data 0.006 (0.064)	Loss 0.6674 (0.6571)	Acc@1 85.596 (86.230)	Acc@5 99.609 (99.438)
Epoch: [35][14/25]	Time 0.561 (0.592)	Data 0.006 (0.060)	Loss 0.6404 (0.6560)	Acc@1 86.621 (86.257)	Acc@5 99.316 (99.430)
Epoch: [35][15/25]	Time 0.590 (0.592)	Data 0.007 (0.057)	Loss 0.6445 (0.6553)	Acc@1 86.279 (86.258)	Acc@5 99.561 (99.438)
Epoch: [35][16/25]	Time 0.612 (0.593)	Data 0.006 (0.054)	Loss 0.6596 (0.6555)	Acc@1 86.621 (86.279)	Acc@5 99.414 (99.437)
Epoch: [35][17/25]	Time 0.560 (0.591)	Data 0.004 (0.051)	Loss 0.6514 (0.6553)	Acc@1 86.377 (86.285)	Acc@5 99.609 (99.447)
Epoch: [35][18/25]	Time 0.607 (0.592)	Data 0.005 (0.049)	Loss 0.6549 (0.6553)	Acc@1 86.865 (86.315)	Acc@5 99.170 (99.432)
Epoch: [35][19/25]	Time 0.568 (0.591)	Data 0.005 (0.047)	Loss 0.6680 (0.6559)	Acc@1 86.328 (86.316)	Acc@5 99.512 (99.436)
Epoch: [35][20/25]	Time 0.605 (0.592)	Data 0.008 (0.045)	Loss 0.6751 (0.6568)	Acc@1 85.010 (86.254)	Acc@5 99.658 (99.447)
Epoch: [35][21/25]	Time 0.642 (0.594)	Data 0.005 (0.043)	Loss 0.6679 (0.6573)	Acc@1 86.963 (86.286)	Acc@5 99.365 (99.443)
Epoch: [35][22/25]	Time 0.573 (0.593)	Data 0.007 (0.041)	Loss 0.6396 (0.6566)	Acc@1 87.207 (86.326)	Acc@5 99.463 (99.444)
Epoch: [35][23/25]	Time 0.612 (0.594)	Data 0.004 (0.040)	Loss 0.6397 (0.6559)	Acc@1 86.865 (86.348)	Acc@5 99.463 (99.445)
Epoch: [35][24/25]	Time 0.357 (0.584)	Data 0.004 (0.038)	Loss 0.6388 (0.6556)	Acc@1 87.854 (86.374)	Acc@5 99.292 (99.442)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 486232 ; 487386 ; 0.9976322668275248

Epoch: [36 | 180] LR: 0.100000
Epoch: [36][0/25]	Time 0.617 (0.617)	Data 0.820 (0.820)	Loss 0.6254 (0.6254)	Acc@1 87.158 (87.158)	Acc@5 99.658 (99.658)
Epoch: [36][1/25]	Time 0.610 (0.614)	Data 0.006 (0.413)	Loss 0.6002 (0.6128)	Acc@1 88.477 (87.817)	Acc@5 99.414 (99.536)
Epoch: [36][2/25]	Time 0.596 (0.608)	Data 0.004 (0.277)	Loss 0.6032 (0.6096)	Acc@1 87.598 (87.744)	Acc@5 99.609 (99.561)
Epoch: [36][3/25]	Time 0.600 (0.606)	Data 0.005 (0.209)	Loss 0.5795 (0.6021)	Acc@1 89.209 (88.110)	Acc@5 99.658 (99.585)
Epoch: [36][4/25]	Time 0.562 (0.597)	Data 0.010 (0.169)	Loss 0.6120 (0.6040)	Acc@1 87.646 (88.018)	Acc@5 99.365 (99.541)
Epoch: [36][5/25]	Time 0.587 (0.595)	Data 0.005 (0.142)	Loss 0.5849 (0.6008)	Acc@1 89.404 (88.249)	Acc@5 99.561 (99.544)
Epoch: [36][6/25]	Time 0.565 (0.591)	Data 0.003 (0.122)	Loss 0.5821 (0.5982)	Acc@1 89.258 (88.393)	Acc@5 99.756 (99.574)
Epoch: [36][7/25]	Time 0.574 (0.589)	Data 0.005 (0.107)	Loss 0.5831 (0.5963)	Acc@1 88.818 (88.446)	Acc@5 99.707 (99.591)
Epoch: [36][8/25]	Time 0.611 (0.591)	Data 0.006 (0.096)	Loss 0.6165 (0.5985)	Acc@1 88.477 (88.449)	Acc@5 99.707 (99.604)
Epoch: [36][9/25]	Time 0.614 (0.594)	Data 0.006 (0.087)	Loss 0.6020 (0.5989)	Acc@1 87.842 (88.389)	Acc@5 99.561 (99.600)
Epoch: [36][10/25]	Time 0.583 (0.593)	Data 0.007 (0.080)	Loss 0.5831 (0.5974)	Acc@1 89.258 (88.468)	Acc@5 99.658 (99.605)
Epoch: [36][11/25]	Time 0.625 (0.595)	Data 0.006 (0.074)	Loss 0.6409 (0.6011)	Acc@1 86.719 (88.322)	Acc@5 99.463 (99.593)
Epoch: [36][12/25]	Time 0.598 (0.596)	Data 0.007 (0.069)	Loss 0.5934 (0.6005)	Acc@1 88.281 (88.319)	Acc@5 99.707 (99.602)
Epoch: [36][13/25]	Time 0.623 (0.598)	Data 0.006 (0.064)	Loss 0.6296 (0.6026)	Acc@1 86.768 (88.208)	Acc@5 99.609 (99.602)
Epoch: [36][14/25]	Time 0.624 (0.599)	Data 0.006 (0.060)	Loss 0.5962 (0.6021)	Acc@1 88.281 (88.213)	Acc@5 99.805 (99.616)
Epoch: [36][15/25]	Time 0.593 (0.599)	Data 0.004 (0.057)	Loss 0.5800 (0.6008)	Acc@1 89.600 (88.300)	Acc@5 99.463 (99.606)
Epoch: [36][16/25]	Time 0.623 (0.600)	Data 0.007 (0.054)	Loss 0.6264 (0.6023)	Acc@1 87.500 (88.253)	Acc@5 99.414 (99.595)
Epoch: [36][17/25]	Time 0.620 (0.601)	Data 0.006 (0.051)	Loss 0.6370 (0.6042)	Acc@1 87.451 (88.208)	Acc@5 99.512 (99.590)
Epoch: [36][18/25]	Time 0.627 (0.603)	Data 0.005 (0.049)	Loss 0.6252 (0.6053)	Acc@1 87.646 (88.178)	Acc@5 99.561 (99.589)
Epoch: [36][19/25]	Time 0.582 (0.602)	Data 0.004 (0.046)	Loss 0.6402 (0.6070)	Acc@1 86.670 (88.103)	Acc@5 99.561 (99.587)
Epoch: [36][20/25]	Time 0.603 (0.602)	Data 0.005 (0.044)	Loss 0.6348 (0.6084)	Acc@1 87.256 (88.063)	Acc@5 99.268 (99.572)
Epoch: [36][21/25]	Time 0.599 (0.602)	Data 0.006 (0.043)	Loss 0.6511 (0.6103)	Acc@1 86.328 (87.984)	Acc@5 99.316 (99.561)
Epoch: [36][22/25]	Time 0.559 (0.600)	Data 0.004 (0.041)	Loss 0.6231 (0.6109)	Acc@1 87.158 (87.948)	Acc@5 99.414 (99.554)
Epoch: [36][23/25]	Time 0.618 (0.601)	Data 0.006 (0.040)	Loss 0.5859 (0.6098)	Acc@1 89.355 (88.007)	Acc@5 99.609 (99.556)
Epoch: [36][24/25]	Time 0.373 (0.592)	Data 0.006 (0.038)	Loss 0.6366 (0.6103)	Acc@1 86.321 (87.978)	Acc@5 99.764 (99.560)

Epoch: [37 | 180] LR: 0.100000
Epoch: [37][0/25]	Time 0.600 (0.600)	Data 0.806 (0.806)	Loss 0.5792 (0.5792)	Acc@1 88.770 (88.770)	Acc@5 99.707 (99.707)
Epoch: [37][1/25]	Time 0.619 (0.609)	Data 0.005 (0.405)	Loss 0.6115 (0.5953)	Acc@1 87.451 (88.110)	Acc@5 99.512 (99.609)
Epoch: [37][2/25]	Time 0.601 (0.607)	Data 0.005 (0.272)	Loss 0.5903 (0.5937)	Acc@1 88.623 (88.281)	Acc@5 99.609 (99.609)
Epoch: [37][3/25]	Time 0.549 (0.592)	Data 0.005 (0.205)	Loss 0.6039 (0.5962)	Acc@1 88.281 (88.281)	Acc@5 99.316 (99.536)
Epoch: [37][4/25]	Time 0.608 (0.595)	Data 0.010 (0.166)	Loss 0.6130 (0.5996)	Acc@1 87.988 (88.223)	Acc@5 99.170 (99.463)
Epoch: [37][5/25]	Time 0.595 (0.595)	Data 0.003 (0.139)	Loss 0.6106 (0.6014)	Acc@1 86.670 (87.964)	Acc@5 99.512 (99.471)
Epoch: [37][6/25]	Time 0.621 (0.599)	Data 0.005 (0.120)	Loss 0.6606 (0.6099)	Acc@1 86.572 (87.765)	Acc@5 99.561 (99.484)
Epoch: [37][7/25]	Time 0.603 (0.599)	Data 0.005 (0.105)	Loss 0.6406 (0.6137)	Acc@1 86.572 (87.616)	Acc@5 99.512 (99.487)
Epoch: [37][8/25]	Time 0.639 (0.604)	Data 0.005 (0.094)	Loss 0.6372 (0.6163)	Acc@1 87.061 (87.554)	Acc@5 99.561 (99.495)
Epoch: [37][9/25]	Time 0.611 (0.604)	Data 0.008 (0.085)	Loss 0.6733 (0.6220)	Acc@1 85.840 (87.383)	Acc@5 99.365 (99.482)
Epoch: [37][10/25]	Time 0.622 (0.606)	Data 0.006 (0.078)	Loss 0.6113 (0.6210)	Acc@1 87.500 (87.393)	Acc@5 99.658 (99.498)
Epoch: [37][11/25]	Time 0.622 (0.607)	Data 0.006 (0.072)	Loss 0.5979 (0.6191)	Acc@1 88.281 (87.467)	Acc@5 99.561 (99.504)
Epoch: [37][12/25]	Time 0.626 (0.609)	Data 0.007 (0.067)	Loss 0.6309 (0.6200)	Acc@1 86.572 (87.399)	Acc@5 99.463 (99.500)
Epoch: [37][13/25]	Time 0.619 (0.610)	Data 0.006 (0.063)	Loss 0.6356 (0.6211)	Acc@1 86.572 (87.340)	Acc@5 99.561 (99.505)
Epoch: [37][14/25]	Time 0.598 (0.609)	Data 0.004 (0.059)	Loss 0.6410 (0.6225)	Acc@1 86.328 (87.272)	Acc@5 99.561 (99.508)
Epoch: [37][15/25]	Time 0.610 (0.609)	Data 0.007 (0.056)	Loss 0.5961 (0.6208)	Acc@1 87.695 (87.299)	Acc@5 99.756 (99.524)
Epoch: [37][16/25]	Time 0.612 (0.609)	Data 0.007 (0.053)	Loss 0.6373 (0.6218)	Acc@1 87.158 (87.290)	Acc@5 99.365 (99.515)
Epoch: [37][17/25]	Time 0.630 (0.610)	Data 0.005 (0.050)	Loss 0.6490 (0.6233)	Acc@1 86.914 (87.269)	Acc@5 99.414 (99.509)
Epoch: [37][18/25]	Time 0.598 (0.609)	Data 0.005 (0.048)	Loss 0.6254 (0.6234)	Acc@1 87.744 (87.294)	Acc@5 99.463 (99.507)
Epoch: [37][19/25]	Time 0.602 (0.609)	Data 0.006 (0.046)	Loss 0.6275 (0.6236)	Acc@1 87.402 (87.300)	Acc@5 99.512 (99.507)
Epoch: [37][20/25]	Time 0.600 (0.609)	Data 0.004 (0.044)	Loss 0.6123 (0.6231)	Acc@1 87.402 (87.305)	Acc@5 99.512 (99.507)
Epoch: [37][21/25]	Time 0.618 (0.609)	Data 0.005 (0.042)	Loss 0.6825 (0.6258)	Acc@1 85.254 (87.211)	Acc@5 99.512 (99.507)
Epoch: [37][22/25]	Time 0.613 (0.609)	Data 0.004 (0.040)	Loss 0.6176 (0.6254)	Acc@1 87.402 (87.220)	Acc@5 99.756 (99.518)
Epoch: [37][23/25]	Time 0.608 (0.609)	Data 0.005 (0.039)	Loss 0.6412 (0.6261)	Acc@1 86.621 (87.195)	Acc@5 99.609 (99.522)
Epoch: [37][24/25]	Time 0.367 (0.599)	Data 0.005 (0.037)	Loss 0.6978 (0.6273)	Acc@1 85.259 (87.162)	Acc@5 99.528 (99.522)

Epoch: [38 | 180] LR: 0.100000
Epoch: [38][0/25]	Time 0.623 (0.623)	Data 0.806 (0.806)	Loss 0.6248 (0.6248)	Acc@1 87.354 (87.354)	Acc@5 99.414 (99.414)
Epoch: [38][1/25]	Time 0.643 (0.633)	Data 0.011 (0.408)	Loss 0.6294 (0.6271)	Acc@1 87.012 (87.183)	Acc@5 99.463 (99.438)
Epoch: [38][2/25]	Time 0.631 (0.632)	Data 0.005 (0.274)	Loss 0.6074 (0.6205)	Acc@1 87.207 (87.191)	Acc@5 99.512 (99.463)
Epoch: [38][3/25]	Time 0.607 (0.626)	Data 0.005 (0.207)	Loss 0.6476 (0.6273)	Acc@1 87.012 (87.146)	Acc@5 99.414 (99.451)
Epoch: [38][4/25]	Time 0.573 (0.616)	Data 0.006 (0.167)	Loss 0.6181 (0.6255)	Acc@1 87.402 (87.197)	Acc@5 99.707 (99.502)
Epoch: [38][5/25]	Time 0.559 (0.606)	Data 0.006 (0.140)	Loss 0.6420 (0.6282)	Acc@1 86.182 (87.028)	Acc@5 99.561 (99.512)
Epoch: [38][6/25]	Time 0.623 (0.609)	Data 0.006 (0.121)	Loss 0.6211 (0.6272)	Acc@1 87.354 (87.074)	Acc@5 99.365 (99.491)
Epoch: [38][7/25]	Time 0.568 (0.603)	Data 0.004 (0.106)	Loss 0.6477 (0.6298)	Acc@1 86.768 (87.036)	Acc@5 99.561 (99.500)
Epoch: [38][8/25]	Time 0.575 (0.600)	Data 0.004 (0.095)	Loss 0.6353 (0.6304)	Acc@1 87.158 (87.050)	Acc@5 99.609 (99.512)
Epoch: [38][9/25]	Time 0.607 (0.601)	Data 0.005 (0.086)	Loss 0.5947 (0.6268)	Acc@1 88.818 (87.227)	Acc@5 99.756 (99.536)
Epoch: [38][10/25]	Time 0.624 (0.603)	Data 0.006 (0.079)	Loss 0.6429 (0.6283)	Acc@1 86.279 (87.140)	Acc@5 99.268 (99.512)
Epoch: [38][11/25]	Time 0.640 (0.606)	Data 0.007 (0.073)	Loss 0.6692 (0.6317)	Acc@1 85.254 (86.983)	Acc@5 99.268 (99.491)
Epoch: [38][12/25]	Time 0.578 (0.604)	Data 0.004 (0.067)	Loss 0.5929 (0.6287)	Acc@1 88.525 (87.102)	Acc@5 99.609 (99.500)
Epoch: [38][13/25]	Time 0.606 (0.604)	Data 0.006 (0.063)	Loss 0.6677 (0.6315)	Acc@1 85.693 (87.001)	Acc@5 99.854 (99.526)
Epoch: [38][14/25]	Time 0.626 (0.606)	Data 0.006 (0.059)	Loss 0.6837 (0.6350)	Acc@1 85.400 (86.895)	Acc@5 99.365 (99.515)
Epoch: [38][15/25]	Time 0.609 (0.606)	Data 0.004 (0.056)	Loss 0.5900 (0.6322)	Acc@1 89.014 (87.027)	Acc@5 99.463 (99.512)
Epoch: [38][16/25]	Time 0.601 (0.606)	Data 0.006 (0.053)	Loss 0.6500 (0.6332)	Acc@1 86.084 (86.972)	Acc@5 99.072 (99.486)
Epoch: [38][17/25]	Time 0.613 (0.606)	Data 0.006 (0.050)	Loss 0.6682 (0.6352)	Acc@1 85.889 (86.911)	Acc@5 99.170 (99.468)
Epoch: [38][18/25]	Time 0.570 (0.604)	Data 0.005 (0.048)	Loss 0.6600 (0.6365)	Acc@1 86.377 (86.883)	Acc@5 99.512 (99.471)
Epoch: [38][19/25]	Time 0.622 (0.605)	Data 0.005 (0.046)	Loss 0.6467 (0.6370)	Acc@1 86.865 (86.882)	Acc@5 99.316 (99.463)
Epoch: [38][20/25]	Time 0.592 (0.604)	Data 0.005 (0.044)	Loss 0.6794 (0.6390)	Acc@1 85.205 (86.802)	Acc@5 99.658 (99.472)
Epoch: [38][21/25]	Time 0.629 (0.605)	Data 0.003 (0.042)	Loss 0.7051 (0.6420)	Acc@1 84.814 (86.712)	Acc@5 99.268 (99.463)
Epoch: [38][22/25]	Time 0.586 (0.605)	Data 0.004 (0.040)	Loss 0.6373 (0.6418)	Acc@1 87.207 (86.734)	Acc@5 99.512 (99.465)
Epoch: [38][23/25]	Time 0.650 (0.606)	Data 0.006 (0.039)	Loss 0.6258 (0.6411)	Acc@1 87.012 (86.745)	Acc@5 99.707 (99.475)
Epoch: [38][24/25]	Time 0.333 (0.596)	Data 0.005 (0.037)	Loss 0.6503 (0.6413)	Acc@1 87.028 (86.750)	Acc@5 99.528 (99.476)

Epoch: [39 | 180] LR: 0.100000
Epoch: [39][0/25]	Time 0.647 (0.647)	Data 0.822 (0.822)	Loss 0.6182 (0.6182)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [39][1/25]	Time 0.623 (0.635)	Data 0.006 (0.414)	Loss 0.6265 (0.6223)	Acc@1 87.842 (87.866)	Acc@5 99.414 (99.512)
Epoch: [39][2/25]	Time 0.631 (0.634)	Data 0.005 (0.278)	Loss 0.6216 (0.6221)	Acc@1 87.109 (87.614)	Acc@5 99.561 (99.528)
Epoch: [39][3/25]	Time 0.597 (0.624)	Data 0.005 (0.209)	Loss 0.6506 (0.6292)	Acc@1 86.865 (87.427)	Acc@5 99.658 (99.561)
Epoch: [39][4/25]	Time 0.623 (0.624)	Data 0.006 (0.169)	Loss 0.5788 (0.6191)	Acc@1 88.330 (87.607)	Acc@5 99.512 (99.551)
Epoch: [39][5/25]	Time 0.649 (0.628)	Data 0.006 (0.142)	Loss 0.6399 (0.6226)	Acc@1 86.768 (87.467)	Acc@5 99.365 (99.520)
Epoch: [39][6/25]	Time 0.632 (0.629)	Data 0.006 (0.122)	Loss 0.6019 (0.6196)	Acc@1 88.281 (87.584)	Acc@5 99.609 (99.533)
Epoch: [39][7/25]	Time 0.597 (0.625)	Data 0.005 (0.108)	Loss 0.6131 (0.6188)	Acc@1 88.135 (87.653)	Acc@5 99.316 (99.506)
Epoch: [39][8/25]	Time 0.600 (0.622)	Data 0.006 (0.096)	Loss 0.6485 (0.6221)	Acc@1 86.914 (87.571)	Acc@5 99.561 (99.512)
Epoch: [39][9/25]	Time 0.600 (0.620)	Data 0.004 (0.087)	Loss 0.6009 (0.6200)	Acc@1 88.672 (87.681)	Acc@5 99.756 (99.536)
Epoch: [39][10/25]	Time 0.599 (0.618)	Data 0.007 (0.080)	Loss 0.6794 (0.6254)	Acc@1 85.498 (87.482)	Acc@5 99.512 (99.534)
Epoch: [39][11/25]	Time 0.581 (0.615)	Data 0.005 (0.074)	Loss 0.6726 (0.6293)	Acc@1 86.523 (87.402)	Acc@5 99.512 (99.532)
Epoch: [39][12/25]	Time 0.610 (0.614)	Data 0.007 (0.069)	Loss 0.6543 (0.6313)	Acc@1 86.523 (87.335)	Acc@5 99.316 (99.515)
Epoch: [39][13/25]	Time 0.579 (0.612)	Data 0.006 (0.064)	Loss 0.6299 (0.6312)	Acc@1 87.451 (87.343)	Acc@5 99.756 (99.533)
Epoch: [39][14/25]	Time 0.602 (0.611)	Data 0.007 (0.060)	Loss 0.6492 (0.6324)	Acc@1 87.695 (87.367)	Acc@5 99.512 (99.531)
Epoch: [39][15/25]	Time 0.627 (0.612)	Data 0.006 (0.057)	Loss 0.6645 (0.6344)	Acc@1 85.742 (87.265)	Acc@5 99.414 (99.524)
Epoch: [39][16/25]	Time 0.589 (0.611)	Data 0.007 (0.054)	Loss 0.6160 (0.6333)	Acc@1 88.232 (87.322)	Acc@5 99.365 (99.515)
Epoch: [39][17/25]	Time 0.620 (0.611)	Data 0.006 (0.051)	Loss 0.6369 (0.6335)	Acc@1 86.523 (87.278)	Acc@5 99.365 (99.506)
Epoch: [39][18/25]	Time 0.622 (0.612)	Data 0.006 (0.049)	Loss 0.6637 (0.6351)	Acc@1 86.572 (87.240)	Acc@5 99.365 (99.499)
Epoch: [39][19/25]	Time 0.575 (0.610)	Data 0.004 (0.047)	Loss 0.6487 (0.6358)	Acc@1 86.084 (87.183)	Acc@5 99.609 (99.504)
Epoch: [39][20/25]	Time 0.644 (0.612)	Data 0.005 (0.045)	Loss 0.6518 (0.6365)	Acc@1 85.791 (87.116)	Acc@5 99.414 (99.500)
Epoch: [39][21/25]	Time 0.596 (0.611)	Data 0.004 (0.043)	Loss 0.6575 (0.6375)	Acc@1 86.768 (87.100)	Acc@5 99.121 (99.483)
Epoch: [39][22/25]	Time 0.611 (0.611)	Data 0.004 (0.041)	Loss 0.6930 (0.6399)	Acc@1 85.693 (87.039)	Acc@5 99.023 (99.463)
Epoch: [39][23/25]	Time 0.607 (0.611)	Data 0.003 (0.040)	Loss 0.6087 (0.6386)	Acc@1 89.062 (87.124)	Acc@5 99.756 (99.475)
Epoch: [39][24/25]	Time 0.317 (0.599)	Data 0.004 (0.038)	Loss 0.6481 (0.6388)	Acc@1 86.792 (87.118)	Acc@5 99.646 (99.478)

Epoch: [40 | 180] LR: 0.100000
Epoch: [40][0/25]	Time 0.605 (0.605)	Data 0.704 (0.704)	Loss 0.6134 (0.6134)	Acc@1 87.305 (87.305)	Acc@5 99.463 (99.463)
Epoch: [40][1/25]	Time 0.625 (0.615)	Data 0.005 (0.355)	Loss 0.6306 (0.6220)	Acc@1 87.158 (87.231)	Acc@5 99.561 (99.512)
Epoch: [40][2/25]	Time 0.585 (0.605)	Data 0.007 (0.239)	Loss 0.6422 (0.6287)	Acc@1 88.135 (87.533)	Acc@5 99.316 (99.447)
Epoch: [40][3/25]	Time 0.633 (0.612)	Data 0.005 (0.180)	Loss 0.6507 (0.6342)	Acc@1 86.670 (87.317)	Acc@5 99.414 (99.438)
Epoch: [40][4/25]	Time 0.596 (0.609)	Data 0.007 (0.146)	Loss 0.6487 (0.6371)	Acc@1 86.816 (87.217)	Acc@5 99.609 (99.473)
Epoch: [40][5/25]	Time 0.593 (0.606)	Data 0.005 (0.122)	Loss 0.6355 (0.6368)	Acc@1 87.695 (87.297)	Acc@5 99.658 (99.504)
Epoch: [40][6/25]	Time 0.607 (0.607)	Data 0.004 (0.105)	Loss 0.6638 (0.6407)	Acc@1 85.742 (87.074)	Acc@5 99.268 (99.470)
Epoch: [40][7/25]	Time 0.606 (0.606)	Data 0.006 (0.093)	Loss 0.6270 (0.6390)	Acc@1 87.012 (87.067)	Acc@5 99.561 (99.481)
Epoch: [40][8/25]	Time 0.568 (0.602)	Data 0.005 (0.083)	Loss 0.6469 (0.6399)	Acc@1 87.109 (87.071)	Acc@5 99.268 (99.457)
Epoch: [40][9/25]	Time 0.570 (0.599)	Data 0.003 (0.075)	Loss 0.6402 (0.6399)	Acc@1 85.693 (86.934)	Acc@5 99.463 (99.458)
Epoch: [40][10/25]	Time 0.605 (0.599)	Data 0.005 (0.069)	Loss 0.6404 (0.6399)	Acc@1 87.646 (86.998)	Acc@5 99.316 (99.445)
Epoch: [40][11/25]	Time 0.626 (0.602)	Data 0.005 (0.064)	Loss 0.6582 (0.6415)	Acc@1 85.986 (86.914)	Acc@5 99.365 (99.438)
Epoch: [40][12/25]	Time 0.637 (0.604)	Data 0.008 (0.059)	Loss 0.6010 (0.6384)	Acc@1 88.281 (87.019)	Acc@5 99.658 (99.455)
Epoch: [40][13/25]	Time 0.602 (0.604)	Data 0.006 (0.055)	Loss 0.6305 (0.6378)	Acc@1 87.939 (87.085)	Acc@5 99.463 (99.456)
Epoch: [40][14/25]	Time 0.591 (0.603)	Data 0.011 (0.052)	Loss 0.5967 (0.6350)	Acc@1 89.014 (87.214)	Acc@5 99.561 (99.463)
Epoch: [40][15/25]	Time 0.618 (0.604)	Data 0.006 (0.050)	Loss 0.6493 (0.6359)	Acc@1 87.061 (87.204)	Acc@5 99.463 (99.463)
Epoch: [40][16/25]	Time 0.601 (0.604)	Data 0.008 (0.047)	Loss 0.6142 (0.6347)	Acc@1 88.281 (87.267)	Acc@5 99.512 (99.466)
Epoch: [40][17/25]	Time 0.586 (0.603)	Data 0.006 (0.045)	Loss 0.6162 (0.6336)	Acc@1 87.939 (87.305)	Acc@5 99.512 (99.468)
Epoch: [40][18/25]	Time 0.595 (0.603)	Data 0.005 (0.043)	Loss 0.6057 (0.6322)	Acc@1 88.379 (87.361)	Acc@5 99.463 (99.468)
Epoch: [40][19/25]	Time 0.637 (0.604)	Data 0.005 (0.041)	Loss 0.6122 (0.6312)	Acc@1 88.135 (87.400)	Acc@5 99.365 (99.463)
Epoch: [40][20/25]	Time 0.617 (0.605)	Data 0.004 (0.039)	Loss 0.6402 (0.6316)	Acc@1 87.549 (87.407)	Acc@5 99.414 (99.461)
Epoch: [40][21/25]	Time 0.592 (0.604)	Data 0.004 (0.038)	Loss 0.5997 (0.6301)	Acc@1 88.184 (87.442)	Acc@5 99.658 (99.470)
Epoch: [40][22/25]	Time 0.560 (0.602)	Data 0.007 (0.036)	Loss 0.6309 (0.6302)	Acc@1 88.037 (87.468)	Acc@5 99.219 (99.459)
Epoch: [40][23/25]	Time 0.653 (0.605)	Data 0.005 (0.035)	Loss 0.6566 (0.6313)	Acc@1 86.572 (87.431)	Acc@5 99.219 (99.449)
Epoch: [40][24/25]	Time 0.362 (0.595)	Data 0.004 (0.034)	Loss 0.6666 (0.6319)	Acc@1 85.142 (87.392)	Acc@5 99.528 (99.450)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 485078 ; 487386 ; 0.9952645336550496

Epoch: [41 | 180] LR: 0.100000
Epoch: [41][0/25]	Time 0.606 (0.606)	Data 0.815 (0.815)	Loss 0.6143 (0.6143)	Acc@1 88.184 (88.184)	Acc@5 99.609 (99.609)
Epoch: [41][1/25]	Time 0.593 (0.600)	Data 0.005 (0.410)	Loss 0.5667 (0.5905)	Acc@1 89.404 (88.794)	Acc@5 99.805 (99.707)
Epoch: [41][2/25]	Time 0.605 (0.601)	Data 0.006 (0.275)	Loss 0.6050 (0.5953)	Acc@1 88.623 (88.737)	Acc@5 99.756 (99.723)
Epoch: [41][3/25]	Time 0.605 (0.602)	Data 0.008 (0.208)	Loss 0.5591 (0.5863)	Acc@1 89.795 (89.001)	Acc@5 99.805 (99.744)
Epoch: [41][4/25]	Time 0.587 (0.599)	Data 0.006 (0.168)	Loss 0.5745 (0.5839)	Acc@1 89.600 (89.121)	Acc@5 99.658 (99.727)
Epoch: [41][5/25]	Time 0.615 (0.602)	Data 0.007 (0.141)	Loss 0.5831 (0.5838)	Acc@1 88.672 (89.046)	Acc@5 99.365 (99.666)
Epoch: [41][6/25]	Time 0.540 (0.593)	Data 0.004 (0.121)	Loss 0.5755 (0.5826)	Acc@1 89.355 (89.090)	Acc@5 99.512 (99.644)
Epoch: [41][7/25]	Time 0.614 (0.596)	Data 0.005 (0.107)	Loss 0.5540 (0.5790)	Acc@1 90.527 (89.270)	Acc@5 99.561 (99.634)
Epoch: [41][8/25]	Time 0.555 (0.591)	Data 0.006 (0.096)	Loss 0.5905 (0.5803)	Acc@1 88.086 (89.138)	Acc@5 99.756 (99.647)
Epoch: [41][9/25]	Time 0.596 (0.592)	Data 0.004 (0.086)	Loss 0.6007 (0.5823)	Acc@1 88.184 (89.043)	Acc@5 99.805 (99.663)
Epoch: [41][10/25]	Time 0.579 (0.590)	Data 0.007 (0.079)	Loss 0.5875 (0.5828)	Acc@1 88.623 (89.005)	Acc@5 99.561 (99.654)
Epoch: [41][11/25]	Time 0.616 (0.593)	Data 0.008 (0.073)	Loss 0.6129 (0.5853)	Acc@1 87.842 (88.908)	Acc@5 99.609 (99.650)
Epoch: [41][12/25]	Time 0.566 (0.591)	Data 0.004 (0.068)	Loss 0.5958 (0.5861)	Acc@1 87.939 (88.833)	Acc@5 99.463 (99.636)
Epoch: [41][13/25]	Time 0.578 (0.590)	Data 0.006 (0.063)	Loss 0.5983 (0.5870)	Acc@1 88.623 (88.818)	Acc@5 99.463 (99.623)
Epoch: [41][14/25]	Time 0.622 (0.592)	Data 0.005 (0.060)	Loss 0.5856 (0.5869)	Acc@1 88.916 (88.825)	Acc@5 99.707 (99.629)
Epoch: [41][15/25]	Time 0.557 (0.590)	Data 0.006 (0.056)	Loss 0.6475 (0.5907)	Acc@1 86.523 (88.681)	Acc@5 99.414 (99.615)
Epoch: [41][16/25]	Time 0.625 (0.592)	Data 0.006 (0.053)	Loss 0.6581 (0.5947)	Acc@1 86.133 (88.531)	Acc@5 99.463 (99.607)
Epoch: [41][17/25]	Time 0.607 (0.593)	Data 0.006 (0.051)	Loss 0.6122 (0.5956)	Acc@1 88.379 (88.523)	Acc@5 99.365 (99.593)
Epoch: [41][18/25]	Time 0.592 (0.593)	Data 0.005 (0.048)	Loss 0.6121 (0.5965)	Acc@1 87.793 (88.484)	Acc@5 99.512 (99.589)
Epoch: [41][19/25]	Time 0.589 (0.592)	Data 0.004 (0.046)	Loss 0.6066 (0.5970)	Acc@1 88.281 (88.474)	Acc@5 99.463 (99.583)
Epoch: [41][20/25]	Time 0.597 (0.593)	Data 0.005 (0.044)	Loss 0.5939 (0.5969)	Acc@1 87.598 (88.432)	Acc@5 99.854 (99.595)
Epoch: [41][21/25]	Time 0.560 (0.591)	Data 0.004 (0.042)	Loss 0.6338 (0.5985)	Acc@1 87.061 (88.370)	Acc@5 99.512 (99.592)
Epoch: [41][22/25]	Time 0.599 (0.591)	Data 0.005 (0.041)	Loss 0.6104 (0.5990)	Acc@1 88.086 (88.358)	Acc@5 99.316 (99.580)
Epoch: [41][23/25]	Time 0.613 (0.592)	Data 0.003 (0.039)	Loss 0.6481 (0.6011)	Acc@1 86.865 (88.295)	Acc@5 99.609 (99.581)
Epoch: [41][24/25]	Time 0.303 (0.581)	Data 0.005 (0.038)	Loss 0.6123 (0.6013)	Acc@1 88.561 (88.300)	Acc@5 99.410 (99.578)

Epoch: [42 | 180] LR: 0.100000
Epoch: [42][0/25]	Time 0.580 (0.580)	Data 0.840 (0.840)	Loss 0.6477 (0.6477)	Acc@1 86.816 (86.816)	Acc@5 99.414 (99.414)
Epoch: [42][1/25]	Time 0.628 (0.604)	Data 0.006 (0.423)	Loss 0.6230 (0.6354)	Acc@1 87.988 (87.402)	Acc@5 99.316 (99.365)
Epoch: [42][2/25]	Time 0.620 (0.609)	Data 0.005 (0.284)	Loss 0.6483 (0.6397)	Acc@1 86.377 (87.061)	Acc@5 99.512 (99.414)
Epoch: [42][3/25]	Time 0.571 (0.600)	Data 0.006 (0.214)	Loss 0.6289 (0.6370)	Acc@1 87.207 (87.097)	Acc@5 99.512 (99.438)
Epoch: [42][4/25]	Time 0.612 (0.602)	Data 0.009 (0.173)	Loss 0.6241 (0.6344)	Acc@1 87.500 (87.178)	Acc@5 99.316 (99.414)
Epoch: [42][5/25]	Time 0.594 (0.601)	Data 0.006 (0.145)	Loss 0.6080 (0.6300)	Acc@1 88.623 (87.419)	Acc@5 99.512 (99.430)
Epoch: [42][6/25]	Time 0.614 (0.603)	Data 0.003 (0.125)	Loss 0.6225 (0.6289)	Acc@1 87.012 (87.360)	Acc@5 99.609 (99.456)
Epoch: [42][7/25]	Time 0.589 (0.601)	Data 0.004 (0.110)	Loss 0.6128 (0.6269)	Acc@1 88.867 (87.549)	Acc@5 99.463 (99.457)
Epoch: [42][8/25]	Time 0.634 (0.605)	Data 0.004 (0.098)	Loss 0.6281 (0.6271)	Acc@1 87.500 (87.543)	Acc@5 99.658 (99.479)
Epoch: [42][9/25]	Time 0.615 (0.606)	Data 0.006 (0.089)	Loss 0.6172 (0.6261)	Acc@1 88.428 (87.632)	Acc@5 99.414 (99.473)
Epoch: [42][10/25]	Time 0.598 (0.605)	Data 0.006 (0.081)	Loss 0.6322 (0.6266)	Acc@1 87.109 (87.584)	Acc@5 99.512 (99.476)
Epoch: [42][11/25]	Time 0.629 (0.607)	Data 0.006 (0.075)	Loss 0.6063 (0.6249)	Acc@1 87.695 (87.594)	Acc@5 99.756 (99.500)
Epoch: [42][12/25]	Time 0.602 (0.607)	Data 0.006 (0.070)	Loss 0.6094 (0.6237)	Acc@1 88.965 (87.699)	Acc@5 99.463 (99.497)
Epoch: [42][13/25]	Time 0.603 (0.606)	Data 0.004 (0.065)	Loss 0.6252 (0.6238)	Acc@1 87.646 (87.695)	Acc@5 99.561 (99.501)
Epoch: [42][14/25]	Time 0.584 (0.605)	Data 0.004 (0.061)	Loss 0.6431 (0.6251)	Acc@1 86.865 (87.640)	Acc@5 99.365 (99.492)
Epoch: [42][15/25]	Time 0.648 (0.608)	Data 0.005 (0.058)	Loss 0.6391 (0.6260)	Acc@1 86.914 (87.595)	Acc@5 99.365 (99.484)
Epoch: [42][16/25]	Time 0.589 (0.606)	Data 0.004 (0.054)	Loss 0.6051 (0.6248)	Acc@1 89.404 (87.701)	Acc@5 99.512 (99.486)
Epoch: [42][17/25]	Time 0.624 (0.607)	Data 0.005 (0.052)	Loss 0.6047 (0.6236)	Acc@1 87.891 (87.712)	Acc@5 99.707 (99.498)
Epoch: [42][18/25]	Time 0.541 (0.604)	Data 0.004 (0.049)	Loss 0.6267 (0.6238)	Acc@1 87.207 (87.685)	Acc@5 99.756 (99.512)
Epoch: [42][19/25]	Time 0.572 (0.602)	Data 0.004 (0.047)	Loss 0.6387 (0.6246)	Acc@1 86.523 (87.627)	Acc@5 99.658 (99.519)
Epoch: [42][20/25]	Time 0.602 (0.602)	Data 0.004 (0.045)	Loss 0.5858 (0.6227)	Acc@1 88.672 (87.677)	Acc@5 99.561 (99.521)
Epoch: [42][21/25]	Time 0.590 (0.602)	Data 0.005 (0.043)	Loss 0.5901 (0.6212)	Acc@1 88.770 (87.726)	Acc@5 99.512 (99.521)
Epoch: [42][22/25]	Time 0.595 (0.602)	Data 0.005 (0.041)	Loss 0.6291 (0.6216)	Acc@1 87.744 (87.727)	Acc@5 99.609 (99.524)
Epoch: [42][23/25]	Time 0.558 (0.600)	Data 0.005 (0.040)	Loss 0.6176 (0.6214)	Acc@1 87.451 (87.716)	Acc@5 99.805 (99.536)
Epoch: [42][24/25]	Time 0.339 (0.589)	Data 0.005 (0.038)	Loss 0.6464 (0.6218)	Acc@1 87.146 (87.706)	Acc@5 99.175 (99.530)

Epoch: [43 | 180] LR: 0.100000
Epoch: [43][0/25]	Time 0.693 (0.693)	Data 0.825 (0.825)	Loss 0.5804 (0.5804)	Acc@1 89.209 (89.209)	Acc@5 99.756 (99.756)
Epoch: [43][1/25]	Time 0.556 (0.625)	Data 0.012 (0.418)	Loss 0.6259 (0.6032)	Acc@1 87.695 (88.452)	Acc@5 99.805 (99.780)
Epoch: [43][2/25]	Time 0.544 (0.598)	Data 0.004 (0.280)	Loss 0.6083 (0.6049)	Acc@1 87.744 (88.216)	Acc@5 99.756 (99.772)
Epoch: [43][3/25]	Time 0.626 (0.605)	Data 0.004 (0.211)	Loss 0.6335 (0.6120)	Acc@1 87.305 (87.988)	Acc@5 99.316 (99.658)
Epoch: [43][4/25]	Time 0.597 (0.603)	Data 0.009 (0.171)	Loss 0.6059 (0.6108)	Acc@1 88.477 (88.086)	Acc@5 99.463 (99.619)
Epoch: [43][5/25]	Time 0.567 (0.597)	Data 0.005 (0.143)	Loss 0.6109 (0.6108)	Acc@1 87.354 (87.964)	Acc@5 99.805 (99.650)
Epoch: [43][6/25]	Time 0.603 (0.598)	Data 0.004 (0.123)	Loss 0.6513 (0.6166)	Acc@1 86.816 (87.800)	Acc@5 99.512 (99.630)
Epoch: [43][7/25]	Time 0.620 (0.601)	Data 0.004 (0.108)	Loss 0.6608 (0.6221)	Acc@1 86.084 (87.585)	Acc@5 99.414 (99.603)
Epoch: [43][8/25]	Time 0.594 (0.600)	Data 0.004 (0.097)	Loss 0.5852 (0.6180)	Acc@1 89.307 (87.777)	Acc@5 99.463 (99.588)
Epoch: [43][9/25]	Time 0.609 (0.601)	Data 0.006 (0.088)	Loss 0.5993 (0.6162)	Acc@1 88.330 (87.832)	Acc@5 99.512 (99.580)
Epoch: [43][10/25]	Time 0.629 (0.604)	Data 0.004 (0.080)	Loss 0.6688 (0.6209)	Acc@1 86.279 (87.691)	Acc@5 99.414 (99.565)
Epoch: [43][11/25]	Time 0.606 (0.604)	Data 0.004 (0.074)	Loss 0.6411 (0.6226)	Acc@1 87.402 (87.667)	Acc@5 99.658 (99.573)
Epoch: [43][12/25]	Time 0.605 (0.604)	Data 0.010 (0.069)	Loss 0.6325 (0.6234)	Acc@1 87.256 (87.635)	Acc@5 99.316 (99.553)
Epoch: [43][13/25]	Time 0.610 (0.604)	Data 0.004 (0.064)	Loss 0.6233 (0.6234)	Acc@1 87.988 (87.660)	Acc@5 99.561 (99.554)
Epoch: [43][14/25]	Time 0.590 (0.603)	Data 0.005 (0.060)	Loss 0.6115 (0.6226)	Acc@1 88.232 (87.699)	Acc@5 99.561 (99.554)
Epoch: [43][15/25]	Time 0.610 (0.604)	Data 0.006 (0.057)	Loss 0.6325 (0.6232)	Acc@1 86.914 (87.650)	Acc@5 99.854 (99.573)
Epoch: [43][16/25]	Time 0.624 (0.605)	Data 0.008 (0.054)	Loss 0.6674 (0.6258)	Acc@1 86.377 (87.575)	Acc@5 99.414 (99.563)
Epoch: [43][17/25]	Time 0.618 (0.606)	Data 0.006 (0.051)	Loss 0.6571 (0.6275)	Acc@1 86.377 (87.508)	Acc@5 99.268 (99.547)
Epoch: [43][18/25]	Time 0.607 (0.606)	Data 0.004 (0.049)	Loss 0.6447 (0.6284)	Acc@1 87.109 (87.487)	Acc@5 99.414 (99.540)
Epoch: [43][19/25]	Time 0.574 (0.604)	Data 0.004 (0.047)	Loss 0.6354 (0.6288)	Acc@1 86.572 (87.441)	Acc@5 99.512 (99.539)
Epoch: [43][20/25]	Time 0.623 (0.605)	Data 0.005 (0.045)	Loss 0.6523 (0.6299)	Acc@1 86.084 (87.377)	Acc@5 99.609 (99.542)
Epoch: [43][21/25]	Time 0.567 (0.603)	Data 0.005 (0.043)	Loss 0.6596 (0.6313)	Acc@1 86.426 (87.334)	Acc@5 99.268 (99.529)
Epoch: [43][22/25]	Time 0.640 (0.605)	Data 0.006 (0.041)	Loss 0.6609 (0.6326)	Acc@1 86.523 (87.298)	Acc@5 99.170 (99.514)
Epoch: [43][23/25]	Time 0.592 (0.604)	Data 0.006 (0.040)	Loss 0.6863 (0.6348)	Acc@1 85.156 (87.209)	Acc@5 99.463 (99.512)
Epoch: [43][24/25]	Time 0.348 (0.594)	Data 0.005 (0.038)	Loss 0.6084 (0.6343)	Acc@1 87.500 (87.214)	Acc@5 99.764 (99.516)

Epoch: [44 | 180] LR: 0.100000
Epoch: [44][0/25]	Time 0.617 (0.617)	Data 0.740 (0.740)	Loss 0.6341 (0.6341)	Acc@1 87.402 (87.402)	Acc@5 99.756 (99.756)
Epoch: [44][1/25]	Time 0.594 (0.605)	Data 0.006 (0.373)	Loss 0.6699 (0.6520)	Acc@1 85.986 (86.694)	Acc@5 99.609 (99.683)
Epoch: [44][2/25]	Time 0.622 (0.611)	Data 0.006 (0.251)	Loss 0.6717 (0.6586)	Acc@1 85.938 (86.442)	Acc@5 99.463 (99.609)
Epoch: [44][3/25]	Time 0.564 (0.599)	Data 0.004 (0.189)	Loss 0.6450 (0.6552)	Acc@1 87.256 (86.646)	Acc@5 99.609 (99.609)
Epoch: [44][4/25]	Time 0.605 (0.600)	Data 0.006 (0.152)	Loss 0.6679 (0.6578)	Acc@1 85.791 (86.475)	Acc@5 99.561 (99.600)
Epoch: [44][5/25]	Time 0.617 (0.603)	Data 0.007 (0.128)	Loss 0.6475 (0.6560)	Acc@1 87.402 (86.629)	Acc@5 99.414 (99.569)
Epoch: [44][6/25]	Time 0.559 (0.597)	Data 0.004 (0.110)	Loss 0.6248 (0.6516)	Acc@1 87.549 (86.761)	Acc@5 99.561 (99.568)
Epoch: [44][7/25]	Time 0.628 (0.601)	Data 0.006 (0.097)	Loss 0.6383 (0.6499)	Acc@1 87.500 (86.853)	Acc@5 99.414 (99.548)
Epoch: [44][8/25]	Time 0.618 (0.603)	Data 0.006 (0.087)	Loss 0.5969 (0.6440)	Acc@1 88.672 (87.055)	Acc@5 99.805 (99.577)
Epoch: [44][9/25]	Time 0.570 (0.599)	Data 0.005 (0.079)	Loss 0.6030 (0.6399)	Acc@1 88.184 (87.168)	Acc@5 99.609 (99.580)
Epoch: [44][10/25]	Time 0.643 (0.603)	Data 0.006 (0.072)	Loss 0.6092 (0.6371)	Acc@1 88.232 (87.265)	Acc@5 99.463 (99.569)
Epoch: [44][11/25]	Time 0.611 (0.604)	Data 0.005 (0.067)	Loss 0.6303 (0.6366)	Acc@1 87.646 (87.297)	Acc@5 99.707 (99.581)
Epoch: [44][12/25]	Time 0.643 (0.607)	Data 0.006 (0.062)	Loss 0.6148 (0.6349)	Acc@1 88.037 (87.354)	Acc@5 99.609 (99.583)
Epoch: [44][13/25]	Time 0.601 (0.607)	Data 0.006 (0.058)	Loss 0.5967 (0.6322)	Acc@1 89.111 (87.479)	Acc@5 99.512 (99.578)
Epoch: [44][14/25]	Time 0.604 (0.606)	Data 0.004 (0.054)	Loss 0.6163 (0.6311)	Acc@1 88.672 (87.559)	Acc@5 99.658 (99.583)
Epoch: [44][15/25]	Time 0.560 (0.604)	Data 0.005 (0.051)	Loss 0.5919 (0.6286)	Acc@1 88.916 (87.643)	Acc@5 99.561 (99.582)
Epoch: [44][16/25]	Time 0.637 (0.606)	Data 0.007 (0.049)	Loss 0.6342 (0.6290)	Acc@1 88.086 (87.669)	Acc@5 99.463 (99.575)
Epoch: [44][17/25]	Time 0.624 (0.607)	Data 0.005 (0.046)	Loss 0.6180 (0.6284)	Acc@1 87.744 (87.674)	Acc@5 99.365 (99.563)
Epoch: [44][18/25]	Time 0.627 (0.608)	Data 0.005 (0.044)	Loss 0.6186 (0.6279)	Acc@1 87.695 (87.675)	Acc@5 99.609 (99.566)
Epoch: [44][19/25]	Time 0.630 (0.609)	Data 0.005 (0.042)	Loss 0.6467 (0.6288)	Acc@1 86.182 (87.600)	Acc@5 99.609 (99.568)
Epoch: [44][20/25]	Time 0.618 (0.609)	Data 0.005 (0.040)	Loss 0.6200 (0.6284)	Acc@1 87.988 (87.619)	Acc@5 99.609 (99.570)
Epoch: [44][21/25]	Time 0.606 (0.609)	Data 0.005 (0.039)	Loss 0.6546 (0.6296)	Acc@1 87.109 (87.595)	Acc@5 99.365 (99.561)
Epoch: [44][22/25]	Time 0.596 (0.608)	Data 0.005 (0.037)	Loss 0.6303 (0.6296)	Acc@1 87.207 (87.579)	Acc@5 99.658 (99.565)
Epoch: [44][23/25]	Time 0.582 (0.607)	Data 0.004 (0.036)	Loss 0.6472 (0.6303)	Acc@1 85.791 (87.504)	Acc@5 99.658 (99.569)
Epoch: [44][24/25]	Time 0.380 (0.598)	Data 0.006 (0.035)	Loss 0.6438 (0.6306)	Acc@1 85.967 (87.478)	Acc@5 99.646 (99.570)

Epoch: [45 | 180] LR: 0.100000
Epoch: [45][0/25]	Time 0.622 (0.622)	Data 0.718 (0.718)	Loss 0.6105 (0.6105)	Acc@1 88.330 (88.330)	Acc@5 99.512 (99.512)
Epoch: [45][1/25]	Time 0.611 (0.617)	Data 0.008 (0.363)	Loss 0.6384 (0.6245)	Acc@1 87.207 (87.769)	Acc@5 99.609 (99.561)
Epoch: [45][2/25]	Time 0.567 (0.600)	Data 0.003 (0.243)	Loss 0.6296 (0.6262)	Acc@1 87.256 (87.598)	Acc@5 99.902 (99.674)
Epoch: [45][3/25]	Time 0.636 (0.609)	Data 0.005 (0.184)	Loss 0.6188 (0.6243)	Acc@1 88.623 (87.854)	Acc@5 99.463 (99.622)
Epoch: [45][4/25]	Time 0.608 (0.609)	Data 0.004 (0.148)	Loss 0.6739 (0.6343)	Acc@1 85.547 (87.393)	Acc@5 99.561 (99.609)
Epoch: [45][5/25]	Time 0.620 (0.611)	Data 0.006 (0.124)	Loss 0.6275 (0.6331)	Acc@1 87.305 (87.378)	Acc@5 99.561 (99.601)
Epoch: [45][6/25]	Time 0.619 (0.612)	Data 0.006 (0.107)	Loss 0.6095 (0.6298)	Acc@1 88.574 (87.549)	Acc@5 99.414 (99.574)
Epoch: [45][7/25]	Time 0.618 (0.613)	Data 0.007 (0.095)	Loss 0.6222 (0.6288)	Acc@1 87.793 (87.579)	Acc@5 99.609 (99.579)
Epoch: [45][8/25]	Time 0.635 (0.615)	Data 0.005 (0.085)	Loss 0.6327 (0.6293)	Acc@1 87.500 (87.571)	Acc@5 99.561 (99.577)
Epoch: [45][9/25]	Time 0.606 (0.614)	Data 0.004 (0.077)	Loss 0.6067 (0.6270)	Acc@1 88.232 (87.637)	Acc@5 99.658 (99.585)
Epoch: [45][10/25]	Time 0.605 (0.613)	Data 0.008 (0.070)	Loss 0.6127 (0.6257)	Acc@1 89.062 (87.766)	Acc@5 99.707 (99.596)
Epoch: [45][11/25]	Time 0.615 (0.614)	Data 0.005 (0.065)	Loss 0.6251 (0.6256)	Acc@1 87.549 (87.748)	Acc@5 99.707 (99.605)
Epoch: [45][12/25]	Time 0.587 (0.611)	Data 0.007 (0.060)	Loss 0.5775 (0.6219)	Acc@1 89.209 (87.861)	Acc@5 99.658 (99.609)
Epoch: [45][13/25]	Time 0.574 (0.609)	Data 0.004 (0.056)	Loss 0.6115 (0.6212)	Acc@1 87.256 (87.817)	Acc@5 99.756 (99.620)
Epoch: [45][14/25]	Time 0.630 (0.610)	Data 0.007 (0.053)	Loss 0.6226 (0.6213)	Acc@1 88.037 (87.832)	Acc@5 99.707 (99.626)
Epoch: [45][15/25]	Time 0.552 (0.607)	Data 0.007 (0.050)	Loss 0.5805 (0.6187)	Acc@1 88.623 (87.881)	Acc@5 99.854 (99.640)
Epoch: [45][16/25]	Time 0.652 (0.609)	Data 0.006 (0.048)	Loss 0.6388 (0.6199)	Acc@1 87.598 (87.865)	Acc@5 99.365 (99.624)
Epoch: [45][17/25]	Time 0.602 (0.609)	Data 0.005 (0.045)	Loss 0.5889 (0.6182)	Acc@1 88.818 (87.918)	Acc@5 99.707 (99.628)
Epoch: [45][18/25]	Time 0.586 (0.608)	Data 0.006 (0.043)	Loss 0.6327 (0.6190)	Acc@1 87.988 (87.921)	Acc@5 99.805 (99.638)
Epoch: [45][19/25]	Time 0.639 (0.609)	Data 0.005 (0.041)	Loss 0.6253 (0.6193)	Acc@1 87.891 (87.920)	Acc@5 99.561 (99.634)
Epoch: [45][20/25]	Time 0.627 (0.610)	Data 0.005 (0.040)	Loss 0.6244 (0.6195)	Acc@1 88.379 (87.942)	Acc@5 99.414 (99.623)
Epoch: [45][21/25]	Time 0.566 (0.608)	Data 0.004 (0.038)	Loss 0.6775 (0.6222)	Acc@1 85.596 (87.835)	Acc@5 99.561 (99.620)
Epoch: [45][22/25]	Time 0.643 (0.610)	Data 0.005 (0.036)	Loss 0.6257 (0.6223)	Acc@1 87.842 (87.835)	Acc@5 99.756 (99.626)
Epoch: [45][23/25]	Time 0.591 (0.609)	Data 0.005 (0.035)	Loss 0.6452 (0.6233)	Acc@1 87.207 (87.809)	Acc@5 99.414 (99.618)
Epoch: [45][24/25]	Time 0.364 (0.599)	Data 0.006 (0.034)	Loss 0.6027 (0.6229)	Acc@1 87.618 (87.806)	Acc@5 100.000 (99.624)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 482478 ; 487386 ; 0.9899299528505128

Epoch: [46 | 180] LR: 0.100000
Epoch: [46][0/25]	Time 0.613 (0.613)	Data 0.910 (0.910)	Loss 0.6003 (0.6003)	Acc@1 88.574 (88.574)	Acc@5 99.805 (99.805)
Epoch: [46][1/25]	Time 0.623 (0.618)	Data 0.006 (0.458)	Loss 0.5731 (0.5867)	Acc@1 89.502 (89.038)	Acc@5 99.658 (99.731)
Epoch: [46][2/25]	Time 0.561 (0.599)	Data 0.005 (0.307)	Loss 0.5703 (0.5812)	Acc@1 89.697 (89.258)	Acc@5 99.609 (99.691)
Epoch: [46][3/25]	Time 0.620 (0.604)	Data 0.005 (0.231)	Loss 0.5767 (0.5801)	Acc@1 89.746 (89.380)	Acc@5 99.609 (99.670)
Epoch: [46][4/25]	Time 0.563 (0.596)	Data 0.008 (0.187)	Loss 0.5446 (0.5730)	Acc@1 90.283 (89.561)	Acc@5 99.561 (99.648)
Epoch: [46][5/25]	Time 0.624 (0.600)	Data 0.005 (0.156)	Loss 0.5114 (0.5627)	Acc@1 91.650 (89.909)	Acc@5 99.756 (99.666)
Epoch: [46][6/25]	Time 0.617 (0.603)	Data 0.005 (0.135)	Loss 0.5389 (0.5593)	Acc@1 90.381 (89.976)	Acc@5 99.658 (99.665)
Epoch: [46][7/25]	Time 0.615 (0.604)	Data 0.005 (0.118)	Loss 0.5518 (0.5584)	Acc@1 90.137 (89.996)	Acc@5 99.756 (99.677)
Epoch: [46][8/25]	Time 0.594 (0.603)	Data 0.007 (0.106)	Loss 0.6081 (0.5639)	Acc@1 88.135 (89.789)	Acc@5 99.756 (99.685)
Epoch: [46][9/25]	Time 0.588 (0.602)	Data 0.004 (0.096)	Loss 0.5486 (0.5624)	Acc@1 89.355 (89.746)	Acc@5 99.902 (99.707)
Epoch: [46][10/25]	Time 0.574 (0.599)	Data 0.004 (0.088)	Loss 0.5637 (0.5625)	Acc@1 90.186 (89.786)	Acc@5 99.805 (99.716)
Epoch: [46][11/25]	Time 0.610 (0.600)	Data 0.007 (0.081)	Loss 0.5497 (0.5614)	Acc@1 90.674 (89.860)	Acc@5 99.756 (99.719)
Epoch: [46][12/25]	Time 0.593 (0.600)	Data 0.004 (0.075)	Loss 0.6102 (0.5652)	Acc@1 88.232 (89.735)	Acc@5 99.561 (99.707)
Epoch: [46][13/25]	Time 0.601 (0.600)	Data 0.008 (0.070)	Loss 0.5707 (0.5656)	Acc@1 89.160 (89.694)	Acc@5 99.707 (99.707)
Epoch: [46][14/25]	Time 0.617 (0.601)	Data 0.005 (0.066)	Loss 0.5682 (0.5658)	Acc@1 89.258 (89.665)	Acc@5 99.609 (99.701)
Epoch: [46][15/25]	Time 0.591 (0.600)	Data 0.007 (0.062)	Loss 0.5582 (0.5653)	Acc@1 90.625 (89.725)	Acc@5 99.707 (99.701)
Epoch: [46][16/25]	Time 0.613 (0.601)	Data 0.006 (0.059)	Loss 0.6154 (0.5682)	Acc@1 87.549 (89.597)	Acc@5 99.512 (99.690)
Epoch: [46][17/25]	Time 0.602 (0.601)	Data 0.005 (0.056)	Loss 0.5883 (0.5694)	Acc@1 89.160 (89.572)	Acc@5 99.756 (99.693)
Epoch: [46][18/25]	Time 0.603 (0.601)	Data 0.005 (0.053)	Loss 0.6312 (0.5726)	Acc@1 87.451 (89.461)	Acc@5 99.512 (99.684)
Epoch: [46][19/25]	Time 0.604 (0.601)	Data 0.005 (0.051)	Loss 0.6117 (0.5746)	Acc@1 88.037 (89.390)	Acc@5 99.316 (99.666)
Epoch: [46][20/25]	Time 0.580 (0.600)	Data 0.006 (0.049)	Loss 0.5931 (0.5754)	Acc@1 88.965 (89.369)	Acc@5 99.609 (99.663)
Epoch: [46][21/25]	Time 0.636 (0.602)	Data 0.004 (0.047)	Loss 0.6145 (0.5772)	Acc@1 87.695 (89.293)	Acc@5 99.609 (99.660)
Epoch: [46][22/25]	Time 0.593 (0.601)	Data 0.005 (0.045)	Loss 0.6091 (0.5786)	Acc@1 87.695 (89.224)	Acc@5 99.561 (99.656)
Epoch: [46][23/25]	Time 0.630 (0.603)	Data 0.005 (0.043)	Loss 0.5729 (0.5784)	Acc@1 89.209 (89.223)	Acc@5 99.707 (99.658)
Epoch: [46][24/25]	Time 0.314 (0.591)	Data 0.005 (0.042)	Loss 0.5983 (0.5787)	Acc@1 89.269 (89.224)	Acc@5 99.410 (99.654)

Epoch: [47 | 180] LR: 0.100000
Epoch: [47][0/25]	Time 0.615 (0.615)	Data 0.803 (0.803)	Loss 0.5752 (0.5752)	Acc@1 89.307 (89.307)	Acc@5 99.609 (99.609)
Epoch: [47][1/25]	Time 0.562 (0.589)	Data 0.004 (0.403)	Loss 0.5994 (0.5873)	Acc@1 88.135 (88.721)	Acc@5 99.658 (99.634)
Epoch: [47][2/25]	Time 0.620 (0.599)	Data 0.005 (0.270)	Loss 0.6135 (0.5960)	Acc@1 86.816 (88.086)	Acc@5 99.707 (99.658)
Epoch: [47][3/25]	Time 0.572 (0.592)	Data 0.007 (0.205)	Loss 0.5997 (0.5970)	Acc@1 88.623 (88.220)	Acc@5 99.658 (99.658)
Epoch: [47][4/25]	Time 0.582 (0.590)	Data 0.006 (0.165)	Loss 0.6278 (0.6031)	Acc@1 87.256 (88.027)	Acc@5 99.609 (99.648)
Epoch: [47][5/25]	Time 0.608 (0.593)	Data 0.005 (0.138)	Loss 0.6123 (0.6046)	Acc@1 88.037 (88.029)	Acc@5 99.463 (99.618)
Epoch: [47][6/25]	Time 0.602 (0.595)	Data 0.003 (0.119)	Loss 0.6251 (0.6076)	Acc@1 87.402 (87.939)	Acc@5 99.463 (99.595)
Epoch: [47][7/25]	Time 0.569 (0.591)	Data 0.006 (0.105)	Loss 0.6143 (0.6084)	Acc@1 88.477 (88.007)	Acc@5 99.268 (99.554)
Epoch: [47][8/25]	Time 0.639 (0.597)	Data 0.006 (0.094)	Loss 0.5723 (0.6044)	Acc@1 90.137 (88.243)	Acc@5 99.854 (99.588)
Epoch: [47][9/25]	Time 0.634 (0.600)	Data 0.006 (0.085)	Loss 0.6039 (0.6043)	Acc@1 88.135 (88.232)	Acc@5 99.512 (99.580)
Epoch: [47][10/25]	Time 0.591 (0.600)	Data 0.004 (0.078)	Loss 0.6194 (0.6057)	Acc@1 88.281 (88.237)	Acc@5 99.219 (99.547)
Epoch: [47][11/25]	Time 0.627 (0.602)	Data 0.006 (0.072)	Loss 0.6022 (0.6054)	Acc@1 88.379 (88.249)	Acc@5 99.756 (99.565)
Epoch: [47][12/25]	Time 0.571 (0.599)	Data 0.007 (0.067)	Loss 0.6088 (0.6057)	Acc@1 88.135 (88.240)	Acc@5 99.561 (99.564)
Epoch: [47][13/25]	Time 0.618 (0.601)	Data 0.006 (0.062)	Loss 0.6072 (0.6058)	Acc@1 88.232 (88.239)	Acc@5 99.658 (99.571)
Epoch: [47][14/25]	Time 0.632 (0.603)	Data 0.006 (0.059)	Loss 0.6504 (0.6088)	Acc@1 87.549 (88.193)	Acc@5 99.268 (99.551)
Epoch: [47][15/25]	Time 0.547 (0.599)	Data 0.004 (0.055)	Loss 0.6269 (0.6099)	Acc@1 88.037 (88.184)	Acc@5 99.365 (99.539)
Epoch: [47][16/25]	Time 0.597 (0.599)	Data 0.005 (0.052)	Loss 0.6519 (0.6124)	Acc@1 87.012 (88.115)	Acc@5 99.561 (99.540)
Epoch: [47][17/25]	Time 0.648 (0.602)	Data 0.005 (0.050)	Loss 0.5909 (0.6112)	Acc@1 88.330 (88.127)	Acc@5 99.561 (99.542)
Epoch: [47][18/25]	Time 0.587 (0.601)	Data 0.004 (0.047)	Loss 0.5949 (0.6103)	Acc@1 88.672 (88.155)	Acc@5 99.707 (99.550)
Epoch: [47][19/25]	Time 0.602 (0.601)	Data 0.004 (0.045)	Loss 0.5913 (0.6094)	Acc@1 88.623 (88.179)	Acc@5 99.658 (99.556)
Epoch: [47][20/25]	Time 0.612 (0.602)	Data 0.005 (0.043)	Loss 0.6050 (0.6092)	Acc@1 87.891 (88.165)	Acc@5 99.561 (99.556)
Epoch: [47][21/25]	Time 0.597 (0.601)	Data 0.004 (0.041)	Loss 0.6223 (0.6098)	Acc@1 87.793 (88.148)	Acc@5 99.512 (99.554)
Epoch: [47][22/25]	Time 0.628 (0.603)	Data 0.005 (0.040)	Loss 0.6105 (0.6098)	Acc@1 88.916 (88.181)	Acc@5 99.707 (99.561)
Epoch: [47][23/25]	Time 0.605 (0.603)	Data 0.005 (0.038)	Loss 0.6001 (0.6094)	Acc@1 88.135 (88.180)	Acc@5 100.000 (99.579)
Epoch: [47][24/25]	Time 0.372 (0.593)	Data 0.005 (0.037)	Loss 0.6033 (0.6093)	Acc@1 87.264 (88.164)	Acc@5 99.882 (99.584)

Epoch: [48 | 180] LR: 0.100000
Epoch: [48][0/25]	Time 0.608 (0.608)	Data 0.669 (0.669)	Loss 0.5942 (0.5942)	Acc@1 89.355 (89.355)	Acc@5 99.561 (99.561)
Epoch: [48][1/25]	Time 0.645 (0.626)	Data 0.008 (0.339)	Loss 0.6110 (0.6026)	Acc@1 87.549 (88.452)	Acc@5 99.609 (99.585)
Epoch: [48][2/25]	Time 0.603 (0.618)	Data 0.004 (0.227)	Loss 0.6032 (0.6028)	Acc@1 88.818 (88.574)	Acc@5 99.365 (99.512)
Epoch: [48][3/25]	Time 0.613 (0.617)	Data 0.006 (0.172)	Loss 0.6009 (0.6023)	Acc@1 87.695 (88.354)	Acc@5 99.463 (99.500)
Epoch: [48][4/25]	Time 0.620 (0.618)	Data 0.006 (0.139)	Loss 0.6355 (0.6090)	Acc@1 87.793 (88.242)	Acc@5 99.414 (99.482)
Epoch: [48][5/25]	Time 0.559 (0.608)	Data 0.008 (0.117)	Loss 0.5991 (0.6073)	Acc@1 88.965 (88.363)	Acc@5 99.365 (99.463)
Epoch: [48][6/25]	Time 0.578 (0.604)	Data 0.009 (0.101)	Loss 0.6332 (0.6110)	Acc@1 86.963 (88.163)	Acc@5 99.658 (99.491)
Epoch: [48][7/25]	Time 0.632 (0.607)	Data 0.004 (0.089)	Loss 0.6154 (0.6116)	Acc@1 88.184 (88.165)	Acc@5 99.658 (99.512)
Epoch: [48][8/25]	Time 0.599 (0.606)	Data 0.006 (0.080)	Loss 0.6388 (0.6146)	Acc@1 87.500 (88.091)	Acc@5 99.707 (99.533)
Epoch: [48][9/25]	Time 0.618 (0.607)	Data 0.006 (0.073)	Loss 0.6173 (0.6149)	Acc@1 87.402 (88.022)	Acc@5 99.609 (99.541)
Epoch: [48][10/25]	Time 0.612 (0.608)	Data 0.004 (0.066)	Loss 0.6093 (0.6144)	Acc@1 88.818 (88.095)	Acc@5 99.512 (99.538)
Epoch: [48][11/25]	Time 0.620 (0.609)	Data 0.004 (0.061)	Loss 0.6322 (0.6158)	Acc@1 87.402 (88.037)	Acc@5 99.414 (99.528)
Epoch: [48][12/25]	Time 0.555 (0.605)	Data 0.007 (0.057)	Loss 0.6028 (0.6148)	Acc@1 88.867 (88.101)	Acc@5 99.365 (99.515)
Epoch: [48][13/25]	Time 0.629 (0.606)	Data 0.005 (0.053)	Loss 0.6107 (0.6145)	Acc@1 87.939 (88.089)	Acc@5 99.414 (99.508)
Epoch: [48][14/25]	Time 0.612 (0.607)	Data 0.008 (0.050)	Loss 0.6094 (0.6142)	Acc@1 88.281 (88.102)	Acc@5 99.512 (99.508)
Epoch: [48][15/25]	Time 0.574 (0.605)	Data 0.008 (0.048)	Loss 0.6433 (0.6160)	Acc@1 86.768 (88.019)	Acc@5 99.365 (99.500)
Epoch: [48][16/25]	Time 0.656 (0.608)	Data 0.006 (0.045)	Loss 0.6061 (0.6154)	Acc@1 88.281 (88.034)	Acc@5 99.658 (99.509)
Epoch: [48][17/25]	Time 0.602 (0.607)	Data 0.005 (0.043)	Loss 0.6059 (0.6149)	Acc@1 88.818 (88.078)	Acc@5 99.609 (99.514)
Epoch: [48][18/25]	Time 0.596 (0.607)	Data 0.003 (0.041)	Loss 0.6182 (0.6151)	Acc@1 87.793 (88.063)	Acc@5 99.512 (99.514)
Epoch: [48][19/25]	Time 0.633 (0.608)	Data 0.004 (0.039)	Loss 0.6268 (0.6157)	Acc@1 87.891 (88.054)	Acc@5 99.561 (99.517)
Epoch: [48][20/25]	Time 0.599 (0.608)	Data 0.003 (0.037)	Loss 0.6213 (0.6159)	Acc@1 87.256 (88.016)	Acc@5 99.658 (99.523)
Epoch: [48][21/25]	Time 0.637 (0.609)	Data 0.003 (0.036)	Loss 0.6090 (0.6156)	Acc@1 88.281 (88.028)	Acc@5 99.609 (99.527)
Epoch: [48][22/25]	Time 0.597 (0.609)	Data 0.005 (0.034)	Loss 0.6220 (0.6159)	Acc@1 87.744 (88.016)	Acc@5 99.609 (99.531)
Epoch: [48][23/25]	Time 0.608 (0.609)	Data 0.007 (0.033)	Loss 0.6162 (0.6159)	Acc@1 87.646 (88.000)	Acc@5 99.707 (99.538)
Epoch: [48][24/25]	Time 0.336 (0.598)	Data 0.004 (0.032)	Loss 0.7124 (0.6175)	Acc@1 85.024 (87.950)	Acc@5 99.528 (99.538)

Epoch: [49 | 180] LR: 0.100000
Epoch: [49][0/25]	Time 0.613 (0.613)	Data 0.674 (0.674)	Loss 0.6596 (0.6596)	Acc@1 86.328 (86.328)	Acc@5 99.072 (99.072)
Epoch: [49][1/25]	Time 0.600 (0.607)	Data 0.008 (0.341)	Loss 0.6405 (0.6500)	Acc@1 87.305 (86.816)	Acc@5 99.658 (99.365)
Epoch: [49][2/25]	Time 0.616 (0.610)	Data 0.005 (0.229)	Loss 0.6345 (0.6449)	Acc@1 87.598 (87.077)	Acc@5 99.561 (99.430)
Epoch: [49][3/25]	Time 0.606 (0.609)	Data 0.005 (0.173)	Loss 0.6413 (0.6440)	Acc@1 86.719 (86.987)	Acc@5 99.707 (99.500)
Epoch: [49][4/25]	Time 0.588 (0.605)	Data 0.006 (0.140)	Loss 0.6379 (0.6428)	Acc@1 88.135 (87.217)	Acc@5 99.316 (99.463)
Epoch: [49][5/25]	Time 0.598 (0.603)	Data 0.007 (0.117)	Loss 0.6207 (0.6391)	Acc@1 87.988 (87.345)	Acc@5 99.658 (99.495)
Epoch: [49][6/25]	Time 0.578 (0.600)	Data 0.005 (0.101)	Loss 0.6409 (0.6393)	Acc@1 87.305 (87.340)	Acc@5 99.365 (99.477)
Epoch: [49][7/25]	Time 0.626 (0.603)	Data 0.004 (0.089)	Loss 0.6085 (0.6355)	Acc@1 88.330 (87.463)	Acc@5 99.707 (99.506)
Epoch: [49][8/25]	Time 0.590 (0.602)	Data 0.005 (0.080)	Loss 0.6160 (0.6333)	Acc@1 88.477 (87.576)	Acc@5 99.658 (99.523)
Epoch: [49][9/25]	Time 0.596 (0.601)	Data 0.006 (0.072)	Loss 0.6148 (0.6315)	Acc@1 87.793 (87.598)	Acc@5 99.707 (99.541)
Epoch: [49][10/25]	Time 0.545 (0.596)	Data 0.006 (0.066)	Loss 0.6217 (0.6306)	Acc@1 88.428 (87.673)	Acc@5 99.609 (99.547)
Epoch: [49][11/25]	Time 0.606 (0.597)	Data 0.007 (0.061)	Loss 0.6125 (0.6291)	Acc@1 87.891 (87.691)	Acc@5 99.512 (99.544)
Epoch: [49][12/25]	Time 0.646 (0.601)	Data 0.006 (0.057)	Loss 0.6076 (0.6274)	Acc@1 88.184 (87.729)	Acc@5 99.609 (99.549)
Epoch: [49][13/25]	Time 0.637 (0.603)	Data 0.006 (0.053)	Loss 0.6248 (0.6272)	Acc@1 87.305 (87.699)	Acc@5 99.365 (99.536)
Epoch: [49][14/25]	Time 0.617 (0.604)	Data 0.004 (0.050)	Loss 0.6159 (0.6265)	Acc@1 88.477 (87.751)	Acc@5 99.316 (99.521)
Epoch: [49][15/25]	Time 0.656 (0.607)	Data 0.004 (0.047)	Loss 0.6321 (0.6268)	Acc@1 87.549 (87.738)	Acc@5 99.609 (99.527)
Epoch: [49][16/25]	Time 0.599 (0.607)	Data 0.004 (0.045)	Loss 0.6148 (0.6261)	Acc@1 88.672 (87.793)	Acc@5 99.707 (99.538)
Epoch: [49][17/25]	Time 0.582 (0.606)	Data 0.007 (0.043)	Loss 0.6322 (0.6265)	Acc@1 87.061 (87.752)	Acc@5 99.902 (99.558)
Epoch: [49][18/25]	Time 0.603 (0.605)	Data 0.004 (0.041)	Loss 0.6145 (0.6258)	Acc@1 87.451 (87.736)	Acc@5 99.756 (99.568)
Epoch: [49][19/25]	Time 0.550 (0.603)	Data 0.007 (0.039)	Loss 0.6251 (0.6258)	Acc@1 88.525 (87.776)	Acc@5 99.463 (99.563)
Epoch: [49][20/25]	Time 0.636 (0.604)	Data 0.005 (0.037)	Loss 0.5998 (0.6246)	Acc@1 88.574 (87.814)	Acc@5 99.512 (99.561)
Epoch: [49][21/25]	Time 0.590 (0.604)	Data 0.004 (0.036)	Loss 0.6334 (0.6250)	Acc@1 87.598 (87.804)	Acc@5 99.658 (99.565)
Epoch: [49][22/25]	Time 0.642 (0.605)	Data 0.005 (0.034)	Loss 0.6411 (0.6257)	Acc@1 87.793 (87.804)	Acc@5 99.756 (99.573)
Epoch: [49][23/25]	Time 0.617 (0.606)	Data 0.005 (0.033)	Loss 0.6490 (0.6266)	Acc@1 86.865 (87.764)	Acc@5 99.414 (99.567)
Epoch: [49][24/25]	Time 0.386 (0.597)	Data 0.004 (0.032)	Loss 0.6103 (0.6264)	Acc@1 88.090 (87.770)	Acc@5 99.646 (99.568)

Epoch: [50 | 180] LR: 0.100000
Epoch: [50][0/25]	Time 0.570 (0.570)	Data 0.904 (0.904)	Loss 0.6158 (0.6158)	Acc@1 87.988 (87.988)	Acc@5 99.561 (99.561)
Epoch: [50][1/25]	Time 0.643 (0.607)	Data 0.005 (0.455)	Loss 0.6049 (0.6104)	Acc@1 88.330 (88.159)	Acc@5 99.658 (99.609)
Epoch: [50][2/25]	Time 0.602 (0.605)	Data 0.005 (0.305)	Loss 0.6036 (0.6081)	Acc@1 88.818 (88.379)	Acc@5 99.512 (99.577)
Epoch: [50][3/25]	Time 0.626 (0.610)	Data 0.003 (0.229)	Loss 0.6347 (0.6148)	Acc@1 87.793 (88.232)	Acc@5 99.512 (99.561)
Epoch: [50][4/25]	Time 0.620 (0.612)	Data 0.008 (0.185)	Loss 0.6271 (0.6172)	Acc@1 86.914 (87.969)	Acc@5 99.561 (99.561)
Epoch: [50][5/25]	Time 0.624 (0.614)	Data 0.004 (0.155)	Loss 0.6064 (0.6154)	Acc@1 88.281 (88.021)	Acc@5 99.609 (99.569)
Epoch: [50][6/25]	Time 0.626 (0.616)	Data 0.004 (0.133)	Loss 0.6027 (0.6136)	Acc@1 88.379 (88.072)	Acc@5 99.512 (99.561)
Epoch: [50][7/25]	Time 0.617 (0.616)	Data 0.005 (0.117)	Loss 0.5450 (0.6050)	Acc@1 91.357 (88.483)	Acc@5 99.609 (99.567)
Epoch: [50][8/25]	Time 0.624 (0.617)	Data 0.007 (0.105)	Loss 0.5995 (0.6044)	Acc@1 89.502 (88.596)	Acc@5 99.658 (99.577)
Epoch: [50][9/25]	Time 0.603 (0.615)	Data 0.003 (0.095)	Loss 0.5957 (0.6035)	Acc@1 88.770 (88.613)	Acc@5 99.609 (99.580)
Epoch: [50][10/25]	Time 0.608 (0.615)	Data 0.006 (0.087)	Loss 0.6098 (0.6041)	Acc@1 88.721 (88.623)	Acc@5 99.561 (99.578)
Epoch: [50][11/25]	Time 0.610 (0.614)	Data 0.006 (0.080)	Loss 0.6375 (0.6069)	Acc@1 86.963 (88.485)	Acc@5 99.512 (99.573)
Epoch: [50][12/25]	Time 0.597 (0.613)	Data 0.005 (0.074)	Loss 0.5782 (0.6047)	Acc@1 88.477 (88.484)	Acc@5 99.756 (99.587)
Epoch: [50][13/25]	Time 0.625 (0.614)	Data 0.006 (0.069)	Loss 0.5992 (0.6043)	Acc@1 88.477 (88.484)	Acc@5 99.756 (99.599)
Epoch: [50][14/25]	Time 0.573 (0.611)	Data 0.008 (0.065)	Loss 0.5896 (0.6033)	Acc@1 88.965 (88.516)	Acc@5 99.463 (99.590)
Epoch: [50][15/25]	Time 0.629 (0.612)	Data 0.007 (0.062)	Loss 0.6438 (0.6058)	Acc@1 86.426 (88.385)	Acc@5 99.463 (99.582)
Epoch: [50][16/25]	Time 0.584 (0.611)	Data 0.008 (0.058)	Loss 0.6363 (0.6076)	Acc@1 87.939 (88.359)	Acc@5 99.268 (99.563)
Epoch: [50][17/25]	Time 0.627 (0.612)	Data 0.005 (0.055)	Loss 0.5897 (0.6066)	Acc@1 88.721 (88.379)	Acc@5 99.512 (99.561)
Epoch: [50][18/25]	Time 0.590 (0.610)	Data 0.005 (0.053)	Loss 0.5898 (0.6057)	Acc@1 88.867 (88.405)	Acc@5 99.805 (99.573)
Epoch: [50][19/25]	Time 0.650 (0.612)	Data 0.005 (0.050)	Loss 0.6178 (0.6063)	Acc@1 87.939 (88.381)	Acc@5 99.365 (99.563)
Epoch: [50][20/25]	Time 0.581 (0.611)	Data 0.006 (0.048)	Loss 0.6241 (0.6072)	Acc@1 88.379 (88.381)	Acc@5 99.512 (99.561)
Epoch: [50][21/25]	Time 0.627 (0.612)	Data 0.005 (0.046)	Loss 0.6177 (0.6077)	Acc@1 87.891 (88.359)	Acc@5 99.414 (99.554)
Epoch: [50][22/25]	Time 0.631 (0.612)	Data 0.007 (0.045)	Loss 0.6137 (0.6079)	Acc@1 88.330 (88.358)	Acc@5 99.561 (99.554)
Epoch: [50][23/25]	Time 0.600 (0.612)	Data 0.004 (0.043)	Loss 0.5804 (0.6068)	Acc@1 89.404 (88.401)	Acc@5 99.756 (99.563)
Epoch: [50][24/25]	Time 0.345 (0.601)	Data 0.005 (0.041)	Loss 0.6045 (0.6067)	Acc@1 88.679 (88.406)	Acc@5 99.410 (99.560)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(56, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 474972 ; 487386 ; 0.9745294284201844

Epoch: [51 | 180] LR: 0.100000
Epoch: [51][0/25]	Time 0.619 (0.619)	Data 0.796 (0.796)	Loss 0.5866 (0.5866)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [51][1/25]	Time 0.575 (0.597)	Data 0.004 (0.400)	Loss 0.5454 (0.5660)	Acc@1 90.576 (90.015)	Acc@5 99.756 (99.683)
Epoch: [51][2/25]	Time 0.589 (0.594)	Data 0.003 (0.268)	Loss 0.5400 (0.5573)	Acc@1 90.381 (90.137)	Acc@5 99.756 (99.707)
Epoch: [51][3/25]	Time 0.589 (0.593)	Data 0.007 (0.203)	Loss 0.5750 (0.5618)	Acc@1 90.234 (90.161)	Acc@5 99.561 (99.670)
Epoch: [51][4/25]	Time 0.554 (0.585)	Data 0.005 (0.163)	Loss 0.5504 (0.5595)	Acc@1 90.332 (90.195)	Acc@5 99.561 (99.648)
Epoch: [51][5/25]	Time 0.569 (0.583)	Data 0.007 (0.137)	Loss 0.5222 (0.5533)	Acc@1 91.455 (90.405)	Acc@5 99.902 (99.691)
Epoch: [51][6/25]	Time 0.594 (0.584)	Data 0.003 (0.118)	Loss 0.5440 (0.5519)	Acc@1 90.137 (90.367)	Acc@5 99.756 (99.700)
Epoch: [51][7/25]	Time 0.594 (0.586)	Data 0.005 (0.104)	Loss 0.5401 (0.5505)	Acc@1 90.674 (90.405)	Acc@5 99.609 (99.689)
Epoch: [51][8/25]	Time 0.585 (0.586)	Data 0.006 (0.093)	Loss 0.5794 (0.5537)	Acc@1 89.795 (90.337)	Acc@5 99.756 (99.696)
Epoch: [51][9/25]	Time 0.593 (0.586)	Data 0.005 (0.084)	Loss 0.5638 (0.5547)	Acc@1 89.746 (90.278)	Acc@5 99.805 (99.707)
Epoch: [51][10/25]	Time 0.623 (0.590)	Data 0.005 (0.077)	Loss 0.5767 (0.5567)	Acc@1 89.307 (90.190)	Acc@5 99.707 (99.707)
Epoch: [51][11/25]	Time 0.592 (0.590)	Data 0.004 (0.071)	Loss 0.5334 (0.5548)	Acc@1 89.893 (90.165)	Acc@5 99.707 (99.707)
Epoch: [51][12/25]	Time 0.583 (0.589)	Data 0.008 (0.066)	Loss 0.5515 (0.5545)	Acc@1 89.893 (90.144)	Acc@5 99.902 (99.722)
Epoch: [51][13/25]	Time 0.603 (0.590)	Data 0.004 (0.062)	Loss 0.5451 (0.5538)	Acc@1 89.697 (90.112)	Acc@5 99.805 (99.728)
Epoch: [51][14/25]	Time 0.595 (0.591)	Data 0.006 (0.058)	Loss 0.5739 (0.5552)	Acc@1 89.600 (90.078)	Acc@5 99.561 (99.717)
Epoch: [51][15/25]	Time 0.604 (0.591)	Data 0.006 (0.055)	Loss 0.5543 (0.5551)	Acc@1 90.332 (90.094)	Acc@5 99.609 (99.710)
Epoch: [51][16/25]	Time 0.611 (0.593)	Data 0.005 (0.052)	Loss 0.5523 (0.5549)	Acc@1 90.186 (90.099)	Acc@5 99.658 (99.707)
Epoch: [51][17/25]	Time 0.621 (0.594)	Data 0.005 (0.049)	Loss 0.5847 (0.5566)	Acc@1 88.477 (90.009)	Acc@5 99.707 (99.707)
Epoch: [51][18/25]	Time 0.619 (0.595)	Data 0.005 (0.047)	Loss 0.5829 (0.5580)	Acc@1 88.672 (89.939)	Acc@5 99.658 (99.704)
Epoch: [51][19/25]	Time 0.584 (0.595)	Data 0.006 (0.045)	Loss 0.5561 (0.5579)	Acc@1 90.088 (89.946)	Acc@5 99.707 (99.705)
Epoch: [51][20/25]	Time 0.613 (0.596)	Data 0.005 (0.043)	Loss 0.5729 (0.5586)	Acc@1 89.307 (89.916)	Acc@5 99.609 (99.700)
Epoch: [51][21/25]	Time 0.605 (0.596)	Data 0.005 (0.041)	Loss 0.6181 (0.5613)	Acc@1 87.646 (89.813)	Acc@5 99.658 (99.698)
Epoch: [51][22/25]	Time 0.606 (0.597)	Data 0.005 (0.040)	Loss 0.5842 (0.5623)	Acc@1 89.307 (89.791)	Acc@5 99.756 (99.701)
Epoch: [51][23/25]	Time 0.560 (0.595)	Data 0.004 (0.038)	Loss 0.5939 (0.5636)	Acc@1 88.184 (89.724)	Acc@5 99.805 (99.705)
Epoch: [51][24/25]	Time 0.338 (0.585)	Data 0.006 (0.037)	Loss 0.5789 (0.5639)	Acc@1 89.858 (89.726)	Acc@5 99.528 (99.702)

Epoch: [52 | 180] LR: 0.100000
Epoch: [52][0/25]	Time 0.613 (0.613)	Data 0.823 (0.823)	Loss 0.5820 (0.5820)	Acc@1 88.916 (88.916)	Acc@5 99.561 (99.561)
Epoch: [52][1/25]	Time 0.604 (0.609)	Data 0.005 (0.414)	Loss 0.5850 (0.5835)	Acc@1 88.330 (88.623)	Acc@5 99.658 (99.609)
Epoch: [52][2/25]	Time 0.626 (0.614)	Data 0.005 (0.278)	Loss 0.5665 (0.5778)	Acc@1 89.355 (88.867)	Acc@5 99.756 (99.658)
Epoch: [52][3/25]	Time 0.596 (0.610)	Data 0.005 (0.209)	Loss 0.6099 (0.5859)	Acc@1 87.646 (88.562)	Acc@5 99.365 (99.585)
Epoch: [52][4/25]	Time 0.575 (0.603)	Data 0.008 (0.169)	Loss 0.5458 (0.5778)	Acc@1 90.430 (88.936)	Acc@5 99.707 (99.609)
Epoch: [52][5/25]	Time 0.603 (0.603)	Data 0.003 (0.141)	Loss 0.6038 (0.5822)	Acc@1 88.086 (88.794)	Acc@5 99.609 (99.609)
Epoch: [52][6/25]	Time 0.559 (0.597)	Data 0.005 (0.122)	Loss 0.5648 (0.5797)	Acc@1 89.209 (88.853)	Acc@5 99.756 (99.630)
Epoch: [52][7/25]	Time 0.562 (0.592)	Data 0.005 (0.107)	Loss 0.6152 (0.5841)	Acc@1 88.379 (88.794)	Acc@5 99.561 (99.622)
Epoch: [52][8/25]	Time 0.614 (0.595)	Data 0.005 (0.096)	Loss 0.5906 (0.5848)	Acc@1 88.281 (88.737)	Acc@5 99.707 (99.631)
Epoch: [52][9/25]	Time 0.584 (0.594)	Data 0.005 (0.087)	Loss 0.6078 (0.5871)	Acc@1 88.037 (88.667)	Acc@5 99.561 (99.624)
Epoch: [52][10/25]	Time 0.663 (0.600)	Data 0.007 (0.080)	Loss 0.5598 (0.5846)	Acc@1 89.648 (88.756)	Acc@5 99.805 (99.640)
Epoch: [52][11/25]	Time 0.595 (0.599)	Data 0.006 (0.074)	Loss 0.6013 (0.5860)	Acc@1 87.646 (88.664)	Acc@5 99.609 (99.638)
Epoch: [52][12/25]	Time 0.576 (0.598)	Data 0.005 (0.068)	Loss 0.6123 (0.5881)	Acc@1 87.744 (88.593)	Acc@5 99.561 (99.632)
Epoch: [52][13/25]	Time 0.645 (0.601)	Data 0.006 (0.064)	Loss 0.5990 (0.5888)	Acc@1 87.842 (88.539)	Acc@5 99.414 (99.616)
Epoch: [52][14/25]	Time 0.622 (0.602)	Data 0.006 (0.060)	Loss 0.6209 (0.5910)	Acc@1 86.963 (88.434)	Acc@5 99.658 (99.619)
Epoch: [52][15/25]	Time 0.643 (0.605)	Data 0.006 (0.057)	Loss 0.5670 (0.5895)	Acc@1 88.818 (88.458)	Acc@5 99.658 (99.622)
Epoch: [52][16/25]	Time 0.594 (0.604)	Data 0.005 (0.054)	Loss 0.6292 (0.5918)	Acc@1 87.402 (88.396)	Acc@5 99.902 (99.638)
Epoch: [52][17/25]	Time 0.626 (0.606)	Data 0.006 (0.051)	Loss 0.5672 (0.5904)	Acc@1 89.502 (88.458)	Acc@5 99.463 (99.628)
Epoch: [52][18/25]	Time 0.608 (0.606)	Data 0.008 (0.049)	Loss 0.6012 (0.5910)	Acc@1 88.379 (88.453)	Acc@5 99.658 (99.630)
Epoch: [52][19/25]	Time 0.601 (0.605)	Data 0.006 (0.046)	Loss 0.6196 (0.5924)	Acc@1 88.135 (88.438)	Acc@5 99.561 (99.626)
Epoch: [52][20/25]	Time 0.608 (0.606)	Data 0.005 (0.044)	Loss 0.5726 (0.5915)	Acc@1 89.160 (88.472)	Acc@5 99.512 (99.621)
Epoch: [52][21/25]	Time 0.596 (0.605)	Data 0.003 (0.043)	Loss 0.6030 (0.5920)	Acc@1 88.574 (88.477)	Acc@5 99.707 (99.625)
Epoch: [52][22/25]	Time 0.611 (0.605)	Data 0.005 (0.041)	Loss 0.6311 (0.5937)	Acc@1 87.891 (88.451)	Acc@5 99.463 (99.618)
Epoch: [52][23/25]	Time 0.587 (0.605)	Data 0.006 (0.040)	Loss 0.6244 (0.5950)	Acc@1 88.281 (88.444)	Acc@5 99.561 (99.615)
Epoch: [52][24/25]	Time 0.336 (0.594)	Data 0.004 (0.038)	Loss 0.5953 (0.5950)	Acc@1 88.679 (88.448)	Acc@5 99.646 (99.616)

Epoch: [53 | 180] LR: 0.100000
Epoch: [53][0/25]	Time 0.592 (0.592)	Data 0.784 (0.784)	Loss 0.6130 (0.6130)	Acc@1 87.891 (87.891)	Acc@5 99.561 (99.561)
Epoch: [53][1/25]	Time 0.597 (0.594)	Data 0.006 (0.395)	Loss 0.6130 (0.6130)	Acc@1 87.842 (87.866)	Acc@5 99.756 (99.658)
Epoch: [53][2/25]	Time 0.622 (0.604)	Data 0.005 (0.265)	Loss 0.6182 (0.6147)	Acc@1 86.914 (87.549)	Acc@5 99.854 (99.723)
Epoch: [53][3/25]	Time 0.595 (0.601)	Data 0.004 (0.200)	Loss 0.6070 (0.6128)	Acc@1 87.695 (87.585)	Acc@5 99.561 (99.683)
Epoch: [53][4/25]	Time 0.607 (0.603)	Data 0.006 (0.161)	Loss 0.6290 (0.6161)	Acc@1 87.598 (87.588)	Acc@5 99.609 (99.668)
Epoch: [53][5/25]	Time 0.610 (0.604)	Data 0.005 (0.135)	Loss 0.5966 (0.6128)	Acc@1 88.135 (87.679)	Acc@5 99.561 (99.650)
Epoch: [53][6/25]	Time 0.632 (0.608)	Data 0.006 (0.117)	Loss 0.5942 (0.6102)	Acc@1 88.574 (87.807)	Acc@5 99.512 (99.630)
Epoch: [53][7/25]	Time 0.632 (0.611)	Data 0.006 (0.103)	Loss 0.6216 (0.6116)	Acc@1 87.207 (87.732)	Acc@5 99.609 (99.628)
Epoch: [53][8/25]	Time 0.619 (0.612)	Data 0.006 (0.092)	Loss 0.6237 (0.6129)	Acc@1 86.914 (87.641)	Acc@5 99.561 (99.620)
Epoch: [53][9/25]	Time 0.581 (0.609)	Data 0.006 (0.083)	Loss 0.5855 (0.6102)	Acc@1 89.258 (87.803)	Acc@5 99.316 (99.590)
Epoch: [53][10/25]	Time 0.546 (0.603)	Data 0.006 (0.076)	Loss 0.6052 (0.6097)	Acc@1 88.232 (87.842)	Acc@5 99.609 (99.592)
Epoch: [53][11/25]	Time 0.614 (0.604)	Data 0.007 (0.071)	Loss 0.6005 (0.6090)	Acc@1 88.525 (87.899)	Acc@5 99.512 (99.585)
Epoch: [53][12/25]	Time 0.589 (0.603)	Data 0.006 (0.066)	Loss 0.6173 (0.6096)	Acc@1 87.695 (87.883)	Acc@5 99.512 (99.579)
Epoch: [53][13/25]	Time 0.601 (0.603)	Data 0.006 (0.061)	Loss 0.6355 (0.6115)	Acc@1 87.061 (87.824)	Acc@5 99.512 (99.574)
Epoch: [53][14/25]	Time 0.631 (0.605)	Data 0.005 (0.058)	Loss 0.6034 (0.6109)	Acc@1 88.770 (87.887)	Acc@5 99.756 (99.587)
Epoch: [53][15/25]	Time 0.597 (0.604)	Data 0.006 (0.054)	Loss 0.6151 (0.6112)	Acc@1 88.232 (87.909)	Acc@5 99.707 (99.594)
Epoch: [53][16/25]	Time 0.552 (0.601)	Data 0.006 (0.051)	Loss 0.6169 (0.6115)	Acc@1 87.158 (87.865)	Acc@5 99.756 (99.604)
Epoch: [53][17/25]	Time 0.606 (0.601)	Data 0.005 (0.049)	Loss 0.6456 (0.6134)	Acc@1 87.500 (87.845)	Acc@5 99.268 (99.585)
Epoch: [53][18/25]	Time 0.606 (0.602)	Data 0.006 (0.047)	Loss 0.6250 (0.6140)	Acc@1 87.598 (87.832)	Acc@5 99.609 (99.586)
Epoch: [53][19/25]	Time 0.565 (0.600)	Data 0.006 (0.045)	Loss 0.5689 (0.6118)	Acc@1 89.600 (87.920)	Acc@5 99.707 (99.592)
Epoch: [53][20/25]	Time 0.615 (0.601)	Data 0.005 (0.043)	Loss 0.5888 (0.6107)	Acc@1 88.672 (87.956)	Acc@5 99.707 (99.598)
Epoch: [53][21/25]	Time 0.661 (0.603)	Data 0.005 (0.041)	Loss 0.6250 (0.6113)	Acc@1 88.623 (87.986)	Acc@5 99.414 (99.589)
Epoch: [53][22/25]	Time 0.622 (0.604)	Data 0.005 (0.039)	Loss 0.6246 (0.6119)	Acc@1 87.549 (87.967)	Acc@5 99.707 (99.595)
Epoch: [53][23/25]	Time 0.595 (0.604)	Data 0.005 (0.038)	Loss 0.6090 (0.6118)	Acc@1 88.184 (87.976)	Acc@5 99.512 (99.591)
Epoch: [53][24/25]	Time 0.316 (0.592)	Data 0.004 (0.037)	Loss 0.6636 (0.6127)	Acc@1 85.967 (87.942)	Acc@5 99.057 (99.582)

Epoch: [54 | 180] LR: 0.100000
Epoch: [54][0/25]	Time 0.604 (0.604)	Data 0.829 (0.829)	Loss 0.6022 (0.6022)	Acc@1 88.818 (88.818)	Acc@5 99.268 (99.268)
Epoch: [54][1/25]	Time 0.603 (0.604)	Data 0.006 (0.417)	Loss 0.6100 (0.6061)	Acc@1 88.477 (88.647)	Acc@5 99.756 (99.512)
Epoch: [54][2/25]	Time 0.633 (0.614)	Data 0.006 (0.280)	Loss 0.6381 (0.6168)	Acc@1 87.012 (88.102)	Acc@5 99.707 (99.577)
Epoch: [54][3/25]	Time 0.607 (0.612)	Data 0.004 (0.211)	Loss 0.6127 (0.6157)	Acc@1 88.232 (88.135)	Acc@5 99.561 (99.573)
Epoch: [54][4/25]	Time 0.622 (0.614)	Data 0.007 (0.170)	Loss 0.6178 (0.6162)	Acc@1 88.965 (88.301)	Acc@5 99.512 (99.561)
Epoch: [54][5/25]	Time 0.609 (0.613)	Data 0.004 (0.143)	Loss 0.6012 (0.6137)	Acc@1 88.623 (88.354)	Acc@5 99.609 (99.569)
Epoch: [54][6/25]	Time 0.628 (0.615)	Data 0.007 (0.123)	Loss 0.5950 (0.6110)	Acc@1 89.551 (88.525)	Acc@5 99.756 (99.595)
Epoch: [54][7/25]	Time 0.580 (0.611)	Data 0.006 (0.109)	Loss 0.5862 (0.6079)	Acc@1 88.965 (88.580)	Acc@5 99.561 (99.591)
Epoch: [54][8/25]	Time 0.612 (0.611)	Data 0.007 (0.097)	Loss 0.6210 (0.6093)	Acc@1 87.402 (88.449)	Acc@5 99.609 (99.593)
Epoch: [54][9/25]	Time 0.605 (0.610)	Data 0.004 (0.088)	Loss 0.5811 (0.6065)	Acc@1 89.307 (88.535)	Acc@5 99.902 (99.624)
Epoch: [54][10/25]	Time 0.588 (0.608)	Data 0.006 (0.080)	Loss 0.6329 (0.6089)	Acc@1 87.744 (88.463)	Acc@5 99.561 (99.618)
Epoch: [54][11/25]	Time 0.624 (0.610)	Data 0.004 (0.074)	Loss 0.5942 (0.6077)	Acc@1 89.062 (88.513)	Acc@5 99.658 (99.622)
Epoch: [54][12/25]	Time 0.616 (0.610)	Data 0.004 (0.069)	Loss 0.5941 (0.6066)	Acc@1 88.672 (88.525)	Acc@5 99.707 (99.628)
Epoch: [54][13/25]	Time 0.621 (0.611)	Data 0.006 (0.064)	Loss 0.6094 (0.6068)	Acc@1 88.281 (88.508)	Acc@5 99.756 (99.637)
Epoch: [54][14/25]	Time 0.541 (0.606)	Data 0.006 (0.060)	Loss 0.6011 (0.6065)	Acc@1 88.281 (88.493)	Acc@5 99.609 (99.635)
Epoch: [54][15/25]	Time 0.600 (0.606)	Data 0.005 (0.057)	Loss 0.5879 (0.6053)	Acc@1 88.721 (88.507)	Acc@5 99.805 (99.646)
Epoch: [54][16/25]	Time 0.620 (0.607)	Data 0.006 (0.054)	Loss 0.5761 (0.6036)	Acc@1 89.746 (88.580)	Acc@5 99.658 (99.647)
Epoch: [54][17/25]	Time 0.561 (0.604)	Data 0.005 (0.051)	Loss 0.5815 (0.6024)	Acc@1 88.672 (88.585)	Acc@5 99.805 (99.655)
Epoch: [54][18/25]	Time 0.588 (0.603)	Data 0.006 (0.049)	Loss 0.6062 (0.6026)	Acc@1 87.744 (88.541)	Acc@5 99.316 (99.638)
Epoch: [54][19/25]	Time 0.588 (0.603)	Data 0.004 (0.047)	Loss 0.6464 (0.6048)	Acc@1 86.719 (88.450)	Acc@5 99.609 (99.636)
Epoch: [54][20/25]	Time 0.591 (0.602)	Data 0.005 (0.045)	Loss 0.6134 (0.6052)	Acc@1 88.184 (88.437)	Acc@5 99.756 (99.642)
Epoch: [54][21/25]	Time 0.605 (0.602)	Data 0.005 (0.043)	Loss 0.6038 (0.6051)	Acc@1 87.842 (88.410)	Acc@5 99.609 (99.640)
Epoch: [54][22/25]	Time 0.607 (0.602)	Data 0.005 (0.041)	Loss 0.6315 (0.6062)	Acc@1 87.354 (88.364)	Acc@5 99.365 (99.628)
Epoch: [54][23/25]	Time 0.606 (0.603)	Data 0.004 (0.040)	Loss 0.6012 (0.6060)	Acc@1 88.916 (88.387)	Acc@5 99.658 (99.630)
Epoch: [54][24/25]	Time 0.320 (0.591)	Data 0.005 (0.038)	Loss 0.6079 (0.6061)	Acc@1 87.736 (88.376)	Acc@5 99.528 (99.628)

Epoch: [55 | 180] LR: 0.100000
Epoch: [55][0/25]	Time 0.669 (0.669)	Data 0.965 (0.965)	Loss 0.6023 (0.6023)	Acc@1 88.086 (88.086)	Acc@5 99.609 (99.609)
Epoch: [55][1/25]	Time 0.542 (0.606)	Data 0.004 (0.484)	Loss 0.6209 (0.6116)	Acc@1 88.184 (88.135)	Acc@5 99.854 (99.731)
Epoch: [55][2/25]	Time 0.626 (0.612)	Data 0.005 (0.324)	Loss 0.6142 (0.6124)	Acc@1 87.402 (87.891)	Acc@5 99.658 (99.707)
Epoch: [55][3/25]	Time 0.629 (0.617)	Data 0.007 (0.245)	Loss 0.6084 (0.6114)	Acc@1 87.939 (87.903)	Acc@5 99.658 (99.695)
Epoch: [55][4/25]	Time 0.641 (0.622)	Data 0.008 (0.198)	Loss 0.6513 (0.6194)	Acc@1 86.865 (87.695)	Acc@5 99.414 (99.639)
Epoch: [55][5/25]	Time 0.577 (0.614)	Data 0.006 (0.166)	Loss 0.5758 (0.6121)	Acc@1 89.014 (87.915)	Acc@5 99.902 (99.683)
Epoch: [55][6/25]	Time 0.651 (0.619)	Data 0.004 (0.143)	Loss 0.6246 (0.6139)	Acc@1 86.865 (87.765)	Acc@5 99.756 (99.693)
Epoch: [55][7/25]	Time 0.616 (0.619)	Data 0.004 (0.125)	Loss 0.6235 (0.6151)	Acc@1 87.939 (87.787)	Acc@5 99.609 (99.683)
Epoch: [55][8/25]	Time 0.556 (0.612)	Data 0.006 (0.112)	Loss 0.5887 (0.6122)	Acc@1 88.428 (87.858)	Acc@5 99.805 (99.696)
Epoch: [55][9/25]	Time 0.583 (0.609)	Data 0.006 (0.101)	Loss 0.5748 (0.6084)	Acc@1 89.795 (88.052)	Acc@5 99.463 (99.673)
Epoch: [55][10/25]	Time 0.616 (0.610)	Data 0.007 (0.093)	Loss 0.6189 (0.6094)	Acc@1 88.232 (88.068)	Acc@5 99.609 (99.667)
Epoch: [55][11/25]	Time 0.591 (0.608)	Data 0.006 (0.086)	Loss 0.6186 (0.6101)	Acc@1 87.744 (88.041)	Acc@5 99.658 (99.666)
Epoch: [55][12/25]	Time 0.585 (0.606)	Data 0.004 (0.079)	Loss 0.5971 (0.6091)	Acc@1 88.428 (88.071)	Acc@5 99.414 (99.647)
Epoch: [55][13/25]	Time 0.596 (0.606)	Data 0.007 (0.074)	Loss 0.6066 (0.6090)	Acc@1 87.598 (88.037)	Acc@5 99.561 (99.641)
Epoch: [55][14/25]	Time 0.567 (0.603)	Data 0.005 (0.070)	Loss 0.6104 (0.6091)	Acc@1 88.135 (88.044)	Acc@5 99.658 (99.642)
Epoch: [55][15/25]	Time 0.613 (0.604)	Data 0.004 (0.065)	Loss 0.5837 (0.6075)	Acc@1 88.867 (88.095)	Acc@5 99.805 (99.652)
Epoch: [55][16/25]	Time 0.599 (0.603)	Data 0.006 (0.062)	Loss 0.5732 (0.6055)	Acc@1 89.795 (88.195)	Acc@5 99.609 (99.650)
Epoch: [55][17/25]	Time 0.601 (0.603)	Data 0.005 (0.059)	Loss 0.6228 (0.6064)	Acc@1 87.451 (88.154)	Acc@5 99.561 (99.645)
Epoch: [55][18/25]	Time 0.590 (0.603)	Data 0.004 (0.056)	Loss 0.6114 (0.6067)	Acc@1 89.258 (88.212)	Acc@5 99.512 (99.638)
Epoch: [55][19/25]	Time 0.611 (0.603)	Data 0.003 (0.053)	Loss 0.6224 (0.6075)	Acc@1 88.428 (88.223)	Acc@5 99.707 (99.641)
Epoch: [55][20/25]	Time 0.607 (0.603)	Data 0.003 (0.051)	Loss 0.5998 (0.6071)	Acc@1 88.623 (88.242)	Acc@5 99.268 (99.623)
Epoch: [55][21/25]	Time 0.622 (0.604)	Data 0.005 (0.049)	Loss 0.6178 (0.6076)	Acc@1 88.184 (88.239)	Acc@5 99.512 (99.618)
Epoch: [55][22/25]	Time 0.586 (0.603)	Data 0.007 (0.047)	Loss 0.5858 (0.6066)	Acc@1 88.525 (88.252)	Acc@5 99.609 (99.618)
Epoch: [55][23/25]	Time 0.636 (0.605)	Data 0.005 (0.045)	Loss 0.6185 (0.6071)	Acc@1 87.842 (88.234)	Acc@5 99.658 (99.620)
Epoch: [55][24/25]	Time 0.357 (0.595)	Data 0.006 (0.044)	Loss 0.5791 (0.6067)	Acc@1 88.915 (88.246)	Acc@5 100.000 (99.626)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(52, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 466310 ; 487386 ; 0.956757067293685

Epoch: [56 | 180] LR: 0.100000
Epoch: [56][0/25]	Time 0.620 (0.620)	Data 0.742 (0.742)	Loss 0.6138 (0.6138)	Acc@1 88.525 (88.525)	Acc@5 99.854 (99.854)
Epoch: [56][1/25]	Time 0.609 (0.614)	Data 0.011 (0.376)	Loss 0.5542 (0.5840)	Acc@1 90.088 (89.307)	Acc@5 99.707 (99.780)
Epoch: [56][2/25]	Time 0.571 (0.600)	Data 0.006 (0.253)	Loss 0.5849 (0.5843)	Acc@1 88.672 (89.095)	Acc@5 99.658 (99.740)
Epoch: [56][3/25]	Time 0.594 (0.598)	Data 0.003 (0.190)	Loss 0.5468 (0.5749)	Acc@1 90.381 (89.417)	Acc@5 99.805 (99.756)
Epoch: [56][4/25]	Time 0.592 (0.597)	Data 0.009 (0.154)	Loss 0.5328 (0.5665)	Acc@1 91.064 (89.746)	Acc@5 99.609 (99.727)
Epoch: [56][5/25]	Time 0.611 (0.599)	Data 0.006 (0.129)	Loss 0.5387 (0.5619)	Acc@1 90.820 (89.925)	Acc@5 99.707 (99.723)
Epoch: [56][6/25]	Time 0.540 (0.591)	Data 0.005 (0.112)	Loss 0.5234 (0.5564)	Acc@1 91.211 (90.109)	Acc@5 99.756 (99.728)
Epoch: [56][7/25]	Time 0.613 (0.594)	Data 0.005 (0.098)	Loss 0.5302 (0.5531)	Acc@1 90.479 (90.155)	Acc@5 99.854 (99.744)
Epoch: [56][8/25]	Time 0.633 (0.598)	Data 0.007 (0.088)	Loss 0.5368 (0.5513)	Acc@1 90.576 (90.202)	Acc@5 99.707 (99.740)
Epoch: [56][9/25]	Time 0.590 (0.597)	Data 0.004 (0.080)	Loss 0.5431 (0.5505)	Acc@1 90.234 (90.205)	Acc@5 99.756 (99.741)
Epoch: [56][10/25]	Time 0.609 (0.598)	Data 0.004 (0.073)	Loss 0.5613 (0.5514)	Acc@1 90.039 (90.190)	Acc@5 99.658 (99.734)
Epoch: [56][11/25]	Time 0.600 (0.598)	Data 0.006 (0.067)	Loss 0.5528 (0.5516)	Acc@1 90.332 (90.202)	Acc@5 99.707 (99.731)
Epoch: [56][12/25]	Time 0.616 (0.600)	Data 0.004 (0.062)	Loss 0.5621 (0.5524)	Acc@1 89.746 (90.167)	Acc@5 99.609 (99.722)
Epoch: [56][13/25]	Time 0.578 (0.598)	Data 0.004 (0.058)	Loss 0.5391 (0.5514)	Acc@1 90.283 (90.175)	Acc@5 99.707 (99.721)
Epoch: [56][14/25]	Time 0.597 (0.598)	Data 0.006 (0.055)	Loss 0.5583 (0.5519)	Acc@1 89.844 (90.153)	Acc@5 99.414 (99.701)
Epoch: [56][15/25]	Time 0.582 (0.597)	Data 0.005 (0.052)	Loss 0.5371 (0.5510)	Acc@1 90.918 (90.201)	Acc@5 99.805 (99.707)
Epoch: [56][16/25]	Time 0.590 (0.597)	Data 0.005 (0.049)	Loss 0.5567 (0.5513)	Acc@1 90.088 (90.194)	Acc@5 99.463 (99.693)
Epoch: [56][17/25]	Time 0.620 (0.598)	Data 0.005 (0.047)	Loss 0.5594 (0.5518)	Acc@1 90.234 (90.196)	Acc@5 99.756 (99.696)
Epoch: [56][18/25]	Time 0.592 (0.598)	Data 0.005 (0.044)	Loss 0.5677 (0.5526)	Acc@1 89.453 (90.157)	Acc@5 99.805 (99.702)
Epoch: [56][19/25]	Time 0.596 (0.598)	Data 0.005 (0.042)	Loss 0.5683 (0.5534)	Acc@1 90.039 (90.151)	Acc@5 99.561 (99.695)
Epoch: [56][20/25]	Time 0.636 (0.599)	Data 0.006 (0.041)	Loss 0.5542 (0.5534)	Acc@1 89.355 (90.113)	Acc@5 99.609 (99.691)
Epoch: [56][21/25]	Time 0.553 (0.597)	Data 0.005 (0.039)	Loss 0.5899 (0.5551)	Acc@1 88.867 (90.057)	Acc@5 99.512 (99.683)
Epoch: [56][22/25]	Time 0.648 (0.600)	Data 0.005 (0.038)	Loss 0.5794 (0.5561)	Acc@1 88.867 (90.005)	Acc@5 99.658 (99.682)
Epoch: [56][23/25]	Time 0.605 (0.600)	Data 0.005 (0.036)	Loss 0.5650 (0.5565)	Acc@1 89.941 (90.002)	Acc@5 99.512 (99.674)
Epoch: [56][24/25]	Time 0.380 (0.591)	Data 0.005 (0.035)	Loss 0.6087 (0.5574)	Acc@1 88.090 (89.970)	Acc@5 99.410 (99.670)

Epoch: [57 | 180] LR: 0.100000
Epoch: [57][0/25]	Time 0.607 (0.607)	Data 0.876 (0.876)	Loss 0.5899 (0.5899)	Acc@1 88.867 (88.867)	Acc@5 99.561 (99.561)
Epoch: [57][1/25]	Time 0.611 (0.609)	Data 0.004 (0.440)	Loss 0.5860 (0.5880)	Acc@1 88.672 (88.770)	Acc@5 99.658 (99.609)
Epoch: [57][2/25]	Time 0.531 (0.583)	Data 0.004 (0.295)	Loss 0.5406 (0.5722)	Acc@1 89.844 (89.128)	Acc@5 99.805 (99.674)
Epoch: [57][3/25]	Time 0.604 (0.588)	Data 0.005 (0.222)	Loss 0.5604 (0.5692)	Acc@1 89.307 (89.172)	Acc@5 99.658 (99.670)
Epoch: [57][4/25]	Time 0.604 (0.591)	Data 0.007 (0.179)	Loss 0.5751 (0.5704)	Acc@1 90.625 (89.463)	Acc@5 99.609 (99.658)
Epoch: [57][5/25]	Time 0.605 (0.594)	Data 0.006 (0.150)	Loss 0.5691 (0.5702)	Acc@1 88.721 (89.339)	Acc@5 99.805 (99.683)
Epoch: [57][6/25]	Time 0.599 (0.595)	Data 0.005 (0.130)	Loss 0.5976 (0.5741)	Acc@1 89.111 (89.307)	Acc@5 99.121 (99.602)
Epoch: [57][7/25]	Time 0.601 (0.595)	Data 0.006 (0.114)	Loss 0.5934 (0.5765)	Acc@1 88.916 (89.258)	Acc@5 99.609 (99.603)
Epoch: [57][8/25]	Time 0.621 (0.598)	Data 0.006 (0.102)	Loss 0.5871 (0.5777)	Acc@1 88.623 (89.187)	Acc@5 99.561 (99.599)
Epoch: [57][9/25]	Time 0.622 (0.601)	Data 0.007 (0.093)	Loss 0.6112 (0.5811)	Acc@1 88.086 (89.077)	Acc@5 99.512 (99.590)
Epoch: [57][10/25]	Time 0.612 (0.602)	Data 0.005 (0.085)	Loss 0.6152 (0.5842)	Acc@1 88.037 (88.983)	Acc@5 99.365 (99.569)
Epoch: [57][11/25]	Time 0.550 (0.597)	Data 0.005 (0.078)	Loss 0.6176 (0.5870)	Acc@1 87.891 (88.892)	Acc@5 99.365 (99.552)
Epoch: [57][12/25]	Time 0.617 (0.599)	Data 0.004 (0.072)	Loss 0.5963 (0.5877)	Acc@1 88.525 (88.863)	Acc@5 99.658 (99.561)
Epoch: [57][13/25]	Time 0.584 (0.598)	Data 0.006 (0.068)	Loss 0.6066 (0.5890)	Acc@1 88.037 (88.804)	Acc@5 99.219 (99.536)
Epoch: [57][14/25]	Time 0.624 (0.600)	Data 0.006 (0.063)	Loss 0.6092 (0.5904)	Acc@1 88.135 (88.760)	Acc@5 99.365 (99.525)
Epoch: [57][15/25]	Time 0.560 (0.597)	Data 0.004 (0.060)	Loss 0.6034 (0.5912)	Acc@1 88.965 (88.773)	Acc@5 99.609 (99.530)
Epoch: [57][16/25]	Time 0.620 (0.598)	Data 0.007 (0.057)	Loss 0.6102 (0.5923)	Acc@1 87.939 (88.724)	Acc@5 99.658 (99.538)
Epoch: [57][17/25]	Time 0.570 (0.597)	Data 0.005 (0.054)	Loss 0.6008 (0.5928)	Acc@1 88.135 (88.691)	Acc@5 99.658 (99.544)
Epoch: [57][18/25]	Time 0.606 (0.597)	Data 0.005 (0.051)	Loss 0.6038 (0.5934)	Acc@1 88.037 (88.656)	Acc@5 99.365 (99.535)
Epoch: [57][19/25]	Time 0.583 (0.597)	Data 0.003 (0.049)	Loss 0.6090 (0.5941)	Acc@1 87.793 (88.613)	Acc@5 99.512 (99.534)
Epoch: [57][20/25]	Time 0.618 (0.598)	Data 0.005 (0.047)	Loss 0.5984 (0.5943)	Acc@1 88.818 (88.623)	Acc@5 99.561 (99.535)
Epoch: [57][21/25]	Time 0.652 (0.600)	Data 0.008 (0.045)	Loss 0.5959 (0.5944)	Acc@1 88.623 (88.623)	Acc@5 99.609 (99.538)
Epoch: [57][22/25]	Time 0.533 (0.597)	Data 0.005 (0.043)	Loss 0.6175 (0.5954)	Acc@1 87.598 (88.578)	Acc@5 99.561 (99.539)
Epoch: [57][23/25]	Time 0.598 (0.597)	Data 0.005 (0.042)	Loss 0.6185 (0.5964)	Acc@1 87.939 (88.552)	Acc@5 99.658 (99.544)
Epoch: [57][24/25]	Time 0.310 (0.586)	Data 0.004 (0.040)	Loss 0.6222 (0.5968)	Acc@1 86.910 (88.524)	Acc@5 99.292 (99.540)

Epoch: [58 | 180] LR: 0.100000
Epoch: [58][0/25]	Time 0.583 (0.583)	Data 0.712 (0.712)	Loss 0.6056 (0.6056)	Acc@1 88.672 (88.672)	Acc@5 99.365 (99.365)
Epoch: [58][1/25]	Time 0.579 (0.581)	Data 0.004 (0.358)	Loss 0.6226 (0.6141)	Acc@1 88.184 (88.428)	Acc@5 99.463 (99.414)
Epoch: [58][2/25]	Time 0.588 (0.583)	Data 0.006 (0.241)	Loss 0.6524 (0.6269)	Acc@1 87.207 (88.021)	Acc@5 99.414 (99.414)
Epoch: [58][3/25]	Time 0.563 (0.578)	Data 0.006 (0.182)	Loss 0.5759 (0.6141)	Acc@1 89.648 (88.428)	Acc@5 99.707 (99.487)
Epoch: [58][4/25]	Time 0.601 (0.583)	Data 0.007 (0.147)	Loss 0.6123 (0.6138)	Acc@1 87.891 (88.320)	Acc@5 99.756 (99.541)
Epoch: [58][5/25]	Time 0.569 (0.581)	Data 0.003 (0.123)	Loss 0.6115 (0.6134)	Acc@1 88.086 (88.281)	Acc@5 99.512 (99.536)
Epoch: [58][6/25]	Time 0.633 (0.588)	Data 0.004 (0.106)	Loss 0.6033 (0.6119)	Acc@1 88.818 (88.358)	Acc@5 99.805 (99.574)
Epoch: [58][7/25]	Time 0.626 (0.593)	Data 0.007 (0.094)	Loss 0.5923 (0.6095)	Acc@1 89.404 (88.489)	Acc@5 99.658 (99.585)
Epoch: [58][8/25]	Time 0.650 (0.599)	Data 0.007 (0.084)	Loss 0.5733 (0.6055)	Acc@1 88.867 (88.531)	Acc@5 99.756 (99.604)
Epoch: [58][9/25]	Time 0.619 (0.601)	Data 0.003 (0.076)	Loss 0.6149 (0.6064)	Acc@1 87.695 (88.447)	Acc@5 99.707 (99.614)
Epoch: [58][10/25]	Time 0.571 (0.598)	Data 0.007 (0.070)	Loss 0.5969 (0.6055)	Acc@1 88.770 (88.477)	Acc@5 99.512 (99.605)
Epoch: [58][11/25]	Time 0.624 (0.601)	Data 0.006 (0.064)	Loss 0.6376 (0.6082)	Acc@1 86.523 (88.314)	Acc@5 99.609 (99.605)
Epoch: [58][12/25]	Time 0.591 (0.600)	Data 0.004 (0.060)	Loss 0.6070 (0.6081)	Acc@1 88.477 (88.326)	Acc@5 99.561 (99.602)
Epoch: [58][13/25]	Time 0.599 (0.600)	Data 0.006 (0.056)	Loss 0.5717 (0.6055)	Acc@1 89.209 (88.389)	Acc@5 99.707 (99.609)
Epoch: [58][14/25]	Time 0.643 (0.603)	Data 0.006 (0.053)	Loss 0.5755 (0.6035)	Acc@1 88.770 (88.415)	Acc@5 99.609 (99.609)
Epoch: [58][15/25]	Time 0.603 (0.603)	Data 0.006 (0.050)	Loss 0.5807 (0.6021)	Acc@1 89.258 (88.467)	Acc@5 99.414 (99.597)
Epoch: [58][16/25]	Time 0.632 (0.604)	Data 0.007 (0.047)	Loss 0.5932 (0.6016)	Acc@1 89.014 (88.500)	Acc@5 99.658 (99.601)
Epoch: [58][17/25]	Time 0.589 (0.604)	Data 0.005 (0.045)	Loss 0.5844 (0.6006)	Acc@1 88.721 (88.512)	Acc@5 99.561 (99.599)
Epoch: [58][18/25]	Time 0.615 (0.604)	Data 0.005 (0.043)	Loss 0.6018 (0.6007)	Acc@1 89.014 (88.538)	Acc@5 99.512 (99.594)
Epoch: [58][19/25]	Time 0.612 (0.605)	Data 0.005 (0.041)	Loss 0.5655 (0.5989)	Acc@1 90.527 (88.638)	Acc@5 99.512 (99.590)
Epoch: [58][20/25]	Time 0.619 (0.605)	Data 0.006 (0.039)	Loss 0.5858 (0.5983)	Acc@1 89.404 (88.674)	Acc@5 99.365 (99.579)
Epoch: [58][21/25]	Time 0.542 (0.602)	Data 0.003 (0.038)	Loss 0.6014 (0.5984)	Acc@1 88.721 (88.676)	Acc@5 99.609 (99.581)
Epoch: [58][22/25]	Time 0.617 (0.603)	Data 0.005 (0.036)	Loss 0.6446 (0.6004)	Acc@1 86.816 (88.595)	Acc@5 99.512 (99.578)
Epoch: [58][23/25]	Time 0.582 (0.602)	Data 0.005 (0.035)	Loss 0.6288 (0.6016)	Acc@1 87.744 (88.560)	Acc@5 99.561 (99.577)
Epoch: [58][24/25]	Time 0.329 (0.591)	Data 0.005 (0.034)	Loss 0.6046 (0.6017)	Acc@1 87.382 (88.540)	Acc@5 99.646 (99.578)

Epoch: [59 | 180] LR: 0.100000
Epoch: [59][0/25]	Time 0.561 (0.561)	Data 0.791 (0.791)	Loss 0.5439 (0.5439)	Acc@1 90.771 (90.771)	Acc@5 99.805 (99.805)
Epoch: [59][1/25]	Time 0.667 (0.614)	Data 0.010 (0.400)	Loss 0.5924 (0.5682)	Acc@1 88.721 (89.746)	Acc@5 99.756 (99.780)
Epoch: [59][2/25]	Time 0.680 (0.636)	Data 0.004 (0.268)	Loss 0.5955 (0.5773)	Acc@1 88.525 (89.339)	Acc@5 99.561 (99.707)
Epoch: [59][3/25]	Time 0.681 (0.647)	Data 0.006 (0.203)	Loss 0.6056 (0.5844)	Acc@1 88.135 (89.038)	Acc@5 99.463 (99.646)
Epoch: [59][4/25]	Time 0.529 (0.623)	Data 0.006 (0.164)	Loss 0.5951 (0.5865)	Acc@1 88.770 (88.984)	Acc@5 99.609 (99.639)
Epoch: [59][5/25]	Time 0.645 (0.627)	Data 0.007 (0.137)	Loss 0.6062 (0.5898)	Acc@1 88.086 (88.835)	Acc@5 99.658 (99.642)
Epoch: [59][6/25]	Time 0.560 (0.617)	Data 0.004 (0.118)	Loss 0.6005 (0.5913)	Acc@1 88.867 (88.839)	Acc@5 99.756 (99.658)
Epoch: [59][7/25]	Time 0.588 (0.614)	Data 0.007 (0.105)	Loss 0.5925 (0.5915)	Acc@1 88.672 (88.818)	Acc@5 99.658 (99.658)
Epoch: [59][8/25]	Time 0.598 (0.612)	Data 0.007 (0.094)	Loss 0.5900 (0.5913)	Acc@1 88.672 (88.802)	Acc@5 99.561 (99.647)
Epoch: [59][9/25]	Time 0.597 (0.610)	Data 0.003 (0.085)	Loss 0.6231 (0.5945)	Acc@1 87.061 (88.628)	Acc@5 99.512 (99.634)
Epoch: [59][10/25]	Time 0.559 (0.606)	Data 0.004 (0.077)	Loss 0.6125 (0.5961)	Acc@1 88.281 (88.596)	Acc@5 99.463 (99.618)
Epoch: [59][11/25]	Time 0.566 (0.602)	Data 0.008 (0.072)	Loss 0.5913 (0.5957)	Acc@1 88.525 (88.590)	Acc@5 99.463 (99.605)
Epoch: [59][12/25]	Time 0.615 (0.603)	Data 0.006 (0.067)	Loss 0.5787 (0.5944)	Acc@1 89.600 (88.668)	Acc@5 99.756 (99.617)
Epoch: [59][13/25]	Time 0.616 (0.604)	Data 0.006 (0.062)	Loss 0.6182 (0.5961)	Acc@1 87.988 (88.620)	Acc@5 99.512 (99.609)
Epoch: [59][14/25]	Time 0.594 (0.604)	Data 0.006 (0.058)	Loss 0.6033 (0.5966)	Acc@1 88.770 (88.630)	Acc@5 99.512 (99.603)
Epoch: [59][15/25]	Time 0.616 (0.604)	Data 0.006 (0.055)	Loss 0.6705 (0.6012)	Acc@1 86.377 (88.489)	Acc@5 99.072 (99.570)
Epoch: [59][16/25]	Time 0.553 (0.601)	Data 0.005 (0.052)	Loss 0.6201 (0.6023)	Acc@1 87.549 (88.433)	Acc@5 99.512 (99.566)
Epoch: [59][17/25]	Time 0.607 (0.602)	Data 0.004 (0.050)	Loss 0.5909 (0.6017)	Acc@1 89.014 (88.466)	Acc@5 99.658 (99.571)
Epoch: [59][18/25]	Time 0.634 (0.603)	Data 0.006 (0.047)	Loss 0.5746 (0.6003)	Acc@1 89.551 (88.523)	Acc@5 99.805 (99.584)
Epoch: [59][19/25]	Time 0.583 (0.602)	Data 0.006 (0.045)	Loss 0.6213 (0.6013)	Acc@1 87.988 (88.496)	Acc@5 99.561 (99.583)
Epoch: [59][20/25]	Time 0.630 (0.604)	Data 0.005 (0.043)	Loss 0.6255 (0.6025)	Acc@1 87.305 (88.439)	Acc@5 99.463 (99.577)
Epoch: [59][21/25]	Time 0.573 (0.602)	Data 0.006 (0.042)	Loss 0.6165 (0.6031)	Acc@1 87.842 (88.412)	Acc@5 99.707 (99.583)
Epoch: [59][22/25]	Time 0.594 (0.602)	Data 0.006 (0.040)	Loss 0.5668 (0.6015)	Acc@1 89.844 (88.474)	Acc@5 99.707 (99.588)
Epoch: [59][23/25]	Time 0.630 (0.603)	Data 0.005 (0.039)	Loss 0.6103 (0.6019)	Acc@1 88.525 (88.477)	Acc@5 99.658 (99.591)
Epoch: [59][24/25]	Time 0.314 (0.592)	Data 0.005 (0.037)	Loss 0.5719 (0.6014)	Acc@1 88.679 (88.480)	Acc@5 99.528 (99.590)

Epoch: [60 | 180] LR: 0.100000
Epoch: [60][0/25]	Time 0.608 (0.608)	Data 0.833 (0.833)	Loss 0.5861 (0.5861)	Acc@1 89.600 (89.600)	Acc@5 99.512 (99.512)
Epoch: [60][1/25]	Time 0.563 (0.585)	Data 0.004 (0.419)	Loss 0.5664 (0.5762)	Acc@1 89.697 (89.648)	Acc@5 99.561 (99.536)
Epoch: [60][2/25]	Time 0.579 (0.583)	Data 0.004 (0.281)	Loss 0.6016 (0.5847)	Acc@1 88.672 (89.323)	Acc@5 99.609 (99.561)
Epoch: [60][3/25]	Time 0.612 (0.591)	Data 0.005 (0.212)	Loss 0.5907 (0.5862)	Acc@1 88.135 (89.026)	Acc@5 99.609 (99.573)
Epoch: [60][4/25]	Time 0.603 (0.593)	Data 0.009 (0.171)	Loss 0.5839 (0.5857)	Acc@1 88.574 (88.936)	Acc@5 99.609 (99.580)
Epoch: [60][5/25]	Time 0.621 (0.598)	Data 0.005 (0.143)	Loss 0.5845 (0.5855)	Acc@1 88.428 (88.851)	Acc@5 99.609 (99.585)
Epoch: [60][6/25]	Time 0.567 (0.593)	Data 0.005 (0.124)	Loss 0.5747 (0.5840)	Acc@1 90.039 (89.021)	Acc@5 99.658 (99.595)
Epoch: [60][7/25]	Time 0.606 (0.595)	Data 0.005 (0.109)	Loss 0.6127 (0.5876)	Acc@1 88.525 (88.959)	Acc@5 99.414 (99.573)
Epoch: [60][8/25]	Time 0.585 (0.594)	Data 0.006 (0.097)	Loss 0.5786 (0.5866)	Acc@1 89.111 (88.976)	Acc@5 99.805 (99.599)
Epoch: [60][9/25]	Time 0.605 (0.595)	Data 0.006 (0.088)	Loss 0.5973 (0.5877)	Acc@1 88.428 (88.921)	Acc@5 99.756 (99.614)
Epoch: [60][10/25]	Time 0.617 (0.597)	Data 0.006 (0.081)	Loss 0.5923 (0.5881)	Acc@1 88.574 (88.889)	Acc@5 99.805 (99.632)
Epoch: [60][11/25]	Time 0.576 (0.595)	Data 0.008 (0.075)	Loss 0.5970 (0.5888)	Acc@1 88.965 (88.896)	Acc@5 99.756 (99.642)
Epoch: [60][12/25]	Time 0.623 (0.597)	Data 0.006 (0.069)	Loss 0.6101 (0.5905)	Acc@1 87.305 (88.773)	Acc@5 99.902 (99.662)
Epoch: [60][13/25]	Time 0.601 (0.598)	Data 0.005 (0.065)	Loss 0.5771 (0.5895)	Acc@1 89.453 (88.822)	Acc@5 99.365 (99.641)
Epoch: [60][14/25]	Time 0.625 (0.599)	Data 0.006 (0.061)	Loss 0.6170 (0.5913)	Acc@1 88.525 (88.802)	Acc@5 99.512 (99.632)
Epoch: [60][15/25]	Time 0.598 (0.599)	Data 0.004 (0.057)	Loss 0.5868 (0.5911)	Acc@1 89.502 (88.846)	Acc@5 99.756 (99.640)
Epoch: [60][16/25]	Time 0.633 (0.601)	Data 0.006 (0.054)	Loss 0.6127 (0.5923)	Acc@1 88.184 (88.807)	Acc@5 99.561 (99.635)
Epoch: [60][17/25]	Time 0.582 (0.600)	Data 0.004 (0.052)	Loss 0.5940 (0.5924)	Acc@1 87.793 (88.751)	Acc@5 99.805 (99.645)
Epoch: [60][18/25]	Time 0.626 (0.602)	Data 0.006 (0.049)	Loss 0.6086 (0.5933)	Acc@1 88.086 (88.716)	Acc@5 99.854 (99.656)
Epoch: [60][19/25]	Time 0.630 (0.603)	Data 0.005 (0.047)	Loss 0.6058 (0.5939)	Acc@1 87.744 (88.667)	Acc@5 99.561 (99.651)
Epoch: [60][20/25]	Time 0.586 (0.602)	Data 0.005 (0.045)	Loss 0.5871 (0.5936)	Acc@1 89.404 (88.702)	Acc@5 99.658 (99.651)
Epoch: [60][21/25]	Time 0.607 (0.602)	Data 0.005 (0.043)	Loss 0.5942 (0.5936)	Acc@1 88.525 (88.694)	Acc@5 99.561 (99.647)
Epoch: [60][22/25]	Time 0.619 (0.603)	Data 0.005 (0.042)	Loss 0.5832 (0.5932)	Acc@1 89.307 (88.721)	Acc@5 99.756 (99.652)
Epoch: [60][23/25]	Time 0.605 (0.603)	Data 0.006 (0.040)	Loss 0.6029 (0.5936)	Acc@1 88.770 (88.723)	Acc@5 99.512 (99.646)
Epoch: [60][24/25]	Time 0.342 (0.593)	Data 0.004 (0.039)	Loss 0.6369 (0.5943)	Acc@1 87.736 (88.706)	Acc@5 99.528 (99.644)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 461116 ; 487386 ; 0.9461002162556987

Epoch: [61 | 180] LR: 0.100000
Epoch: [61][0/25]	Time 0.584 (0.584)	Data 0.805 (0.805)	Loss 0.5782 (0.5782)	Acc@1 88.916 (88.916)	Acc@5 99.658 (99.658)
Epoch: [61][1/25]	Time 0.594 (0.589)	Data 0.003 (0.404)	Loss 0.5331 (0.5556)	Acc@1 91.211 (90.063)	Acc@5 99.707 (99.683)
Epoch: [61][2/25]	Time 0.578 (0.585)	Data 0.004 (0.271)	Loss 0.5407 (0.5506)	Acc@1 90.918 (90.348)	Acc@5 99.805 (99.723)
Epoch: [61][3/25]	Time 0.602 (0.589)	Data 0.005 (0.204)	Loss 0.5310 (0.5457)	Acc@1 91.309 (90.588)	Acc@5 99.658 (99.707)
Epoch: [61][4/25]	Time 0.546 (0.580)	Data 0.006 (0.165)	Loss 0.5206 (0.5407)	Acc@1 91.162 (90.703)	Acc@5 99.854 (99.736)
Epoch: [61][5/25]	Time 0.618 (0.587)	Data 0.004 (0.138)	Loss 0.5502 (0.5423)	Acc@1 90.918 (90.739)	Acc@5 99.561 (99.707)
Epoch: [61][6/25]	Time 0.616 (0.591)	Data 0.011 (0.120)	Loss 0.5246 (0.5398)	Acc@1 90.918 (90.765)	Acc@5 99.951 (99.742)
Epoch: [61][7/25]	Time 0.616 (0.594)	Data 0.005 (0.105)	Loss 0.5109 (0.5361)	Acc@1 91.748 (90.887)	Acc@5 99.756 (99.744)
Epoch: [61][8/25]	Time 0.555 (0.590)	Data 0.005 (0.094)	Loss 0.5440 (0.5370)	Acc@1 90.381 (90.831)	Acc@5 99.609 (99.729)
Epoch: [61][9/25]	Time 0.600 (0.591)	Data 0.007 (0.085)	Loss 0.5602 (0.5393)	Acc@1 90.576 (90.806)	Acc@5 99.658 (99.722)
Epoch: [61][10/25]	Time 0.573 (0.589)	Data 0.004 (0.078)	Loss 0.5565 (0.5409)	Acc@1 89.893 (90.723)	Acc@5 99.707 (99.720)
Epoch: [61][11/25]	Time 0.574 (0.588)	Data 0.006 (0.072)	Loss 0.5673 (0.5431)	Acc@1 89.355 (90.609)	Acc@5 99.512 (99.703)
Epoch: [61][12/25]	Time 0.611 (0.590)	Data 0.006 (0.067)	Loss 0.5372 (0.5426)	Acc@1 90.137 (90.572)	Acc@5 99.805 (99.711)
Epoch: [61][13/25]	Time 0.607 (0.591)	Data 0.004 (0.062)	Loss 0.5364 (0.5422)	Acc@1 89.844 (90.520)	Acc@5 99.658 (99.707)
Epoch: [61][14/25]	Time 0.545 (0.588)	Data 0.006 (0.059)	Loss 0.5546 (0.5430)	Acc@1 90.088 (90.492)	Acc@5 99.658 (99.704)
Epoch: [61][15/25]	Time 0.631 (0.591)	Data 0.004 (0.055)	Loss 0.5507 (0.5435)	Acc@1 89.697 (90.442)	Acc@5 99.902 (99.716)
Epoch: [61][16/25]	Time 0.587 (0.590)	Data 0.007 (0.052)	Loss 0.5322 (0.5428)	Acc@1 91.504 (90.504)	Acc@5 99.707 (99.716)
Epoch: [61][17/25]	Time 0.629 (0.592)	Data 0.005 (0.050)	Loss 0.5603 (0.5438)	Acc@1 89.844 (90.468)	Acc@5 99.756 (99.718)
Epoch: [61][18/25]	Time 0.563 (0.591)	Data 0.005 (0.047)	Loss 0.5648 (0.5449)	Acc@1 89.502 (90.417)	Acc@5 99.658 (99.715)
Epoch: [61][19/25]	Time 0.610 (0.592)	Data 0.005 (0.045)	Loss 0.5291 (0.5441)	Acc@1 90.967 (90.444)	Acc@5 99.902 (99.724)
Epoch: [61][20/25]	Time 0.574 (0.591)	Data 0.004 (0.043)	Loss 0.5962 (0.5466)	Acc@1 88.770 (90.365)	Acc@5 99.609 (99.719)
Epoch: [61][21/25]	Time 0.622 (0.592)	Data 0.006 (0.042)	Loss 0.5678 (0.5476)	Acc@1 90.039 (90.350)	Acc@5 99.561 (99.711)
Epoch: [61][22/25]	Time 0.535 (0.590)	Data 0.004 (0.040)	Loss 0.5812 (0.5490)	Acc@1 89.404 (90.309)	Acc@5 99.756 (99.713)
Epoch: [61][23/25]	Time 0.589 (0.590)	Data 0.006 (0.039)	Loss 0.5838 (0.5505)	Acc@1 88.184 (90.220)	Acc@5 99.561 (99.707)
Epoch: [61][24/25]	Time 0.316 (0.579)	Data 0.003 (0.037)	Loss 0.5799 (0.5510)	Acc@1 89.387 (90.206)	Acc@5 99.528 (99.704)

Epoch: [62 | 180] LR: 0.100000
Epoch: [62][0/25]	Time 0.561 (0.561)	Data 0.775 (0.775)	Loss 0.5627 (0.5627)	Acc@1 89.551 (89.551)	Acc@5 99.707 (99.707)
Epoch: [62][1/25]	Time 0.610 (0.586)	Data 0.005 (0.390)	Loss 0.5658 (0.5643)	Acc@1 90.039 (89.795)	Acc@5 99.512 (99.609)
Epoch: [62][2/25]	Time 0.619 (0.597)	Data 0.005 (0.262)	Loss 0.5786 (0.5691)	Acc@1 89.355 (89.648)	Acc@5 99.658 (99.626)
Epoch: [62][3/25]	Time 0.583 (0.593)	Data 0.005 (0.198)	Loss 0.5847 (0.5730)	Acc@1 88.574 (89.380)	Acc@5 99.756 (99.658)
Epoch: [62][4/25]	Time 0.603 (0.595)	Data 0.007 (0.160)	Loss 0.5429 (0.5670)	Acc@1 90.723 (89.648)	Acc@5 99.707 (99.668)
Epoch: [62][5/25]	Time 0.583 (0.593)	Data 0.005 (0.134)	Loss 0.6043 (0.5732)	Acc@1 88.477 (89.453)	Acc@5 99.561 (99.650)
Epoch: [62][6/25]	Time 0.580 (0.591)	Data 0.006 (0.116)	Loss 0.5504 (0.5699)	Acc@1 89.746 (89.495)	Acc@5 99.609 (99.644)
Epoch: [62][7/25]	Time 0.614 (0.594)	Data 0.005 (0.102)	Loss 0.5663 (0.5695)	Acc@1 89.551 (89.502)	Acc@5 99.512 (99.628)
Epoch: [62][8/25]	Time 0.570 (0.591)	Data 0.006 (0.091)	Loss 0.5823 (0.5709)	Acc@1 88.770 (89.421)	Acc@5 99.512 (99.615)
Epoch: [62][9/25]	Time 0.629 (0.595)	Data 0.004 (0.082)	Loss 0.6004 (0.5738)	Acc@1 88.574 (89.336)	Acc@5 99.854 (99.639)
Epoch: [62][10/25]	Time 0.636 (0.599)	Data 0.006 (0.075)	Loss 0.5848 (0.5748)	Acc@1 88.965 (89.302)	Acc@5 99.756 (99.649)
Epoch: [62][11/25]	Time 0.609 (0.600)	Data 0.005 (0.070)	Loss 0.5667 (0.5742)	Acc@1 89.355 (89.307)	Acc@5 99.756 (99.658)
Epoch: [62][12/25]	Time 0.616 (0.601)	Data 0.006 (0.065)	Loss 0.6253 (0.5781)	Acc@1 88.086 (89.213)	Acc@5 99.316 (99.632)
Epoch: [62][13/25]	Time 0.620 (0.602)	Data 0.007 (0.061)	Loss 0.6090 (0.5803)	Acc@1 88.232 (89.143)	Acc@5 99.609 (99.630)
Epoch: [62][14/25]	Time 0.545 (0.599)	Data 0.004 (0.057)	Loss 0.6073 (0.5821)	Acc@1 87.988 (89.066)	Acc@5 99.365 (99.613)
Epoch: [62][15/25]	Time 0.631 (0.601)	Data 0.007 (0.054)	Loss 0.5830 (0.5822)	Acc@1 89.453 (89.090)	Acc@5 99.805 (99.625)
Epoch: [62][16/25]	Time 0.612 (0.601)	Data 0.007 (0.051)	Loss 0.6057 (0.5835)	Acc@1 88.281 (89.042)	Acc@5 99.414 (99.612)
Epoch: [62][17/25]	Time 0.631 (0.603)	Data 0.004 (0.048)	Loss 0.5992 (0.5844)	Acc@1 87.842 (88.976)	Acc@5 99.707 (99.618)
Epoch: [62][18/25]	Time 0.606 (0.603)	Data 0.005 (0.046)	Loss 0.5965 (0.5851)	Acc@1 88.135 (88.931)	Acc@5 99.658 (99.620)
Epoch: [62][19/25]	Time 0.612 (0.604)	Data 0.004 (0.044)	Loss 0.6250 (0.5871)	Acc@1 87.695 (88.870)	Acc@5 99.609 (99.619)
Epoch: [62][20/25]	Time 0.610 (0.604)	Data 0.005 (0.042)	Loss 0.5939 (0.5874)	Acc@1 89.062 (88.879)	Acc@5 99.561 (99.616)
Epoch: [62][21/25]	Time 0.588 (0.603)	Data 0.005 (0.040)	Loss 0.6280 (0.5892)	Acc@1 87.158 (88.801)	Acc@5 99.512 (99.612)
Epoch: [62][22/25]	Time 0.633 (0.604)	Data 0.005 (0.039)	Loss 0.5803 (0.5888)	Acc@1 89.453 (88.829)	Acc@5 99.512 (99.607)
Epoch: [62][23/25]	Time 0.549 (0.602)	Data 0.007 (0.038)	Loss 0.5916 (0.5889)	Acc@1 89.404 (88.853)	Acc@5 99.463 (99.601)
Epoch: [62][24/25]	Time 0.349 (0.592)	Data 0.004 (0.036)	Loss 0.6347 (0.5897)	Acc@1 86.557 (88.814)	Acc@5 99.646 (99.602)

Epoch: [63 | 180] LR: 0.100000
Epoch: [63][0/25]	Time 0.608 (0.608)	Data 0.767 (0.767)	Loss 0.5999 (0.5999)	Acc@1 88.525 (88.525)	Acc@5 99.561 (99.561)
Epoch: [63][1/25]	Time 0.548 (0.578)	Data 0.003 (0.385)	Loss 0.5987 (0.5993)	Acc@1 88.574 (88.550)	Acc@5 99.609 (99.585)
Epoch: [63][2/25]	Time 0.585 (0.580)	Data 0.007 (0.259)	Loss 0.6038 (0.6008)	Acc@1 88.281 (88.460)	Acc@5 99.707 (99.626)
Epoch: [63][3/25]	Time 0.617 (0.589)	Data 0.005 (0.196)	Loss 0.5980 (0.6001)	Acc@1 88.770 (88.538)	Acc@5 99.365 (99.561)
Epoch: [63][4/25]	Time 0.556 (0.583)	Data 0.006 (0.158)	Loss 0.5671 (0.5935)	Acc@1 89.502 (88.730)	Acc@5 99.561 (99.561)
Epoch: [63][5/25]	Time 0.613 (0.588)	Data 0.006 (0.132)	Loss 0.5930 (0.5934)	Acc@1 88.184 (88.639)	Acc@5 99.561 (99.561)
Epoch: [63][6/25]	Time 0.578 (0.586)	Data 0.006 (0.114)	Loss 0.5895 (0.5929)	Acc@1 88.770 (88.658)	Acc@5 99.756 (99.588)
Epoch: [63][7/25]	Time 0.643 (0.594)	Data 0.006 (0.101)	Loss 0.6027 (0.5941)	Acc@1 88.623 (88.654)	Acc@5 99.609 (99.591)
Epoch: [63][8/25]	Time 0.592 (0.593)	Data 0.006 (0.090)	Loss 0.5814 (0.5927)	Acc@1 89.600 (88.759)	Acc@5 99.951 (99.631)
Epoch: [63][9/25]	Time 0.613 (0.595)	Data 0.004 (0.081)	Loss 0.6070 (0.5941)	Acc@1 88.477 (88.730)	Acc@5 99.414 (99.609)
Epoch: [63][10/25]	Time 0.591 (0.595)	Data 0.006 (0.075)	Loss 0.5934 (0.5941)	Acc@1 88.232 (88.685)	Acc@5 99.561 (99.605)
Epoch: [63][11/25]	Time 0.630 (0.598)	Data 0.006 (0.069)	Loss 0.6092 (0.5953)	Acc@1 88.623 (88.680)	Acc@5 99.561 (99.601)
Epoch: [63][12/25]	Time 0.571 (0.596)	Data 0.004 (0.064)	Loss 0.5937 (0.5952)	Acc@1 89.160 (88.717)	Acc@5 99.512 (99.594)
Epoch: [63][13/25]	Time 0.616 (0.597)	Data 0.004 (0.060)	Loss 0.5558 (0.5924)	Acc@1 89.990 (88.808)	Acc@5 99.707 (99.602)
Epoch: [63][14/25]	Time 0.629 (0.600)	Data 0.007 (0.056)	Loss 0.6103 (0.5936)	Acc@1 88.721 (88.802)	Acc@5 99.756 (99.613)
Epoch: [63][15/25]	Time 0.631 (0.602)	Data 0.004 (0.053)	Loss 0.5925 (0.5935)	Acc@1 88.428 (88.779)	Acc@5 99.707 (99.619)
Epoch: [63][16/25]	Time 0.583 (0.600)	Data 0.006 (0.050)	Loss 0.5703 (0.5921)	Acc@1 89.795 (88.838)	Acc@5 99.756 (99.627)
Epoch: [63][17/25]	Time 0.607 (0.601)	Data 0.005 (0.048)	Loss 0.5604 (0.5904)	Acc@1 90.039 (88.905)	Acc@5 99.707 (99.631)
Epoch: [63][18/25]	Time 0.576 (0.599)	Data 0.005 (0.045)	Loss 0.5783 (0.5897)	Acc@1 89.209 (88.921)	Acc@5 99.707 (99.635)
Epoch: [63][19/25]	Time 0.587 (0.599)	Data 0.005 (0.043)	Loss 0.5887 (0.5897)	Acc@1 88.232 (88.887)	Acc@5 99.707 (99.639)
Epoch: [63][20/25]	Time 0.578 (0.598)	Data 0.004 (0.042)	Loss 0.6214 (0.5912)	Acc@1 88.037 (88.846)	Acc@5 99.707 (99.642)
Epoch: [63][21/25]	Time 0.557 (0.596)	Data 0.005 (0.040)	Loss 0.6300 (0.5930)	Acc@1 88.184 (88.816)	Acc@5 99.561 (99.638)
Epoch: [63][22/25]	Time 0.580 (0.595)	Data 0.005 (0.038)	Loss 0.5786 (0.5923)	Acc@1 89.453 (88.844)	Acc@5 99.805 (99.645)
Epoch: [63][23/25]	Time 0.585 (0.595)	Data 0.004 (0.037)	Loss 0.6191 (0.5935)	Acc@1 87.793 (88.800)	Acc@5 99.707 (99.648)
Epoch: [63][24/25]	Time 0.331 (0.584)	Data 0.004 (0.036)	Loss 0.5549 (0.5928)	Acc@1 90.330 (88.826)	Acc@5 99.882 (99.652)

Epoch: [64 | 180] LR: 0.100000
Epoch: [64][0/25]	Time 0.634 (0.634)	Data 0.974 (0.974)	Loss 0.5598 (0.5598)	Acc@1 90.137 (90.137)	Acc@5 99.756 (99.756)
Epoch: [64][1/25]	Time 0.603 (0.619)	Data 0.007 (0.490)	Loss 0.6125 (0.5861)	Acc@1 88.525 (89.331)	Acc@5 99.707 (99.731)
Epoch: [64][2/25]	Time 0.541 (0.593)	Data 0.004 (0.328)	Loss 0.6106 (0.5943)	Acc@1 88.232 (88.965)	Acc@5 99.707 (99.723)
Epoch: [64][3/25]	Time 0.612 (0.598)	Data 0.005 (0.247)	Loss 0.6004 (0.5958)	Acc@1 88.770 (88.916)	Acc@5 99.609 (99.695)
Epoch: [64][4/25]	Time 0.576 (0.593)	Data 0.008 (0.199)	Loss 0.6194 (0.6005)	Acc@1 87.891 (88.711)	Acc@5 99.463 (99.648)
Epoch: [64][5/25]	Time 0.564 (0.589)	Data 0.004 (0.167)	Loss 0.5782 (0.5968)	Acc@1 89.990 (88.924)	Acc@5 99.658 (99.650)
Epoch: [64][6/25]	Time 0.614 (0.592)	Data 0.005 (0.144)	Loss 0.5916 (0.5961)	Acc@1 88.281 (88.832)	Acc@5 99.756 (99.665)
Epoch: [64][7/25]	Time 0.575 (0.590)	Data 0.006 (0.127)	Loss 0.6003 (0.5966)	Acc@1 88.770 (88.824)	Acc@5 99.561 (99.652)
Epoch: [64][8/25]	Time 0.621 (0.593)	Data 0.007 (0.113)	Loss 0.5729 (0.5940)	Acc@1 89.307 (88.878)	Acc@5 99.805 (99.669)
Epoch: [64][9/25]	Time 0.540 (0.588)	Data 0.007 (0.103)	Loss 0.5831 (0.5929)	Acc@1 88.672 (88.857)	Acc@5 99.756 (99.678)
Epoch: [64][10/25]	Time 0.571 (0.586)	Data 0.005 (0.094)	Loss 0.5937 (0.5930)	Acc@1 89.111 (88.881)	Acc@5 99.463 (99.658)
Epoch: [64][11/25]	Time 0.610 (0.588)	Data 0.005 (0.086)	Loss 0.6032 (0.5938)	Acc@1 87.842 (88.794)	Acc@5 99.707 (99.662)
Epoch: [64][12/25]	Time 0.585 (0.588)	Data 0.006 (0.080)	Loss 0.6049 (0.5947)	Acc@1 87.988 (88.732)	Acc@5 99.463 (99.647)
Epoch: [64][13/25]	Time 0.621 (0.590)	Data 0.006 (0.075)	Loss 0.5820 (0.5938)	Acc@1 89.160 (88.763)	Acc@5 99.609 (99.644)
Epoch: [64][14/25]	Time 0.590 (0.590)	Data 0.006 (0.070)	Loss 0.6143 (0.5951)	Acc@1 88.330 (88.734)	Acc@5 99.561 (99.639)
Epoch: [64][15/25]	Time 0.568 (0.589)	Data 0.004 (0.066)	Loss 0.5984 (0.5953)	Acc@1 89.160 (88.760)	Acc@5 99.707 (99.643)
Epoch: [64][16/25]	Time 0.590 (0.589)	Data 0.009 (0.063)	Loss 0.5900 (0.5950)	Acc@1 89.014 (88.775)	Acc@5 99.561 (99.638)
Epoch: [64][17/25]	Time 0.597 (0.590)	Data 0.006 (0.060)	Loss 0.6195 (0.5964)	Acc@1 88.037 (88.734)	Acc@5 99.463 (99.628)
Epoch: [64][18/25]	Time 0.618 (0.591)	Data 0.005 (0.057)	Loss 0.5951 (0.5963)	Acc@1 88.330 (88.713)	Acc@5 99.658 (99.630)
Epoch: [64][19/25]	Time 0.608 (0.592)	Data 0.005 (0.054)	Loss 0.5744 (0.5952)	Acc@1 88.379 (88.696)	Acc@5 99.756 (99.636)
Epoch: [64][20/25]	Time 0.550 (0.590)	Data 0.005 (0.052)	Loss 0.6011 (0.5955)	Acc@1 89.209 (88.721)	Acc@5 99.609 (99.635)
Epoch: [64][21/25]	Time 0.615 (0.591)	Data 0.004 (0.050)	Loss 0.6106 (0.5962)	Acc@1 87.549 (88.667)	Acc@5 99.707 (99.638)
Epoch: [64][22/25]	Time 0.585 (0.591)	Data 0.004 (0.048)	Loss 0.6077 (0.5967)	Acc@1 88.428 (88.657)	Acc@5 99.609 (99.637)
Epoch: [64][23/25]	Time 0.607 (0.592)	Data 0.005 (0.046)	Loss 0.5854 (0.5962)	Acc@1 89.209 (88.680)	Acc@5 99.658 (99.638)
Epoch: [64][24/25]	Time 0.354 (0.582)	Data 0.004 (0.044)	Loss 0.6186 (0.5966)	Acc@1 86.439 (88.642)	Acc@5 99.528 (99.636)

Epoch: [65 | 180] LR: 0.100000
Epoch: [65][0/25]	Time 0.608 (0.608)	Data 0.703 (0.703)	Loss 0.5849 (0.5849)	Acc@1 89.697 (89.697)	Acc@5 99.561 (99.561)
Epoch: [65][1/25]	Time 0.604 (0.606)	Data 0.008 (0.356)	Loss 0.6264 (0.6056)	Acc@1 86.621 (88.159)	Acc@5 99.805 (99.683)
Epoch: [65][2/25]	Time 0.615 (0.609)	Data 0.006 (0.239)	Loss 0.5736 (0.5950)	Acc@1 89.502 (88.607)	Acc@5 99.609 (99.658)
Epoch: [65][3/25]	Time 0.553 (0.595)	Data 0.006 (0.181)	Loss 0.6071 (0.5980)	Acc@1 88.330 (88.538)	Acc@5 99.414 (99.597)
Epoch: [65][4/25]	Time 0.558 (0.588)	Data 0.006 (0.146)	Loss 0.6135 (0.6011)	Acc@1 87.061 (88.242)	Acc@5 99.707 (99.619)
Epoch: [65][5/25]	Time 0.597 (0.589)	Data 0.004 (0.122)	Loss 0.5850 (0.5984)	Acc@1 89.453 (88.444)	Acc@5 99.805 (99.650)
Epoch: [65][6/25]	Time 0.561 (0.585)	Data 0.006 (0.105)	Loss 0.6241 (0.6021)	Acc@1 87.891 (88.365)	Acc@5 99.609 (99.644)
Epoch: [65][7/25]	Time 0.611 (0.588)	Data 0.005 (0.093)	Loss 0.6070 (0.6027)	Acc@1 88.477 (88.379)	Acc@5 99.658 (99.646)
Epoch: [65][8/25]	Time 0.555 (0.585)	Data 0.006 (0.083)	Loss 0.6342 (0.6062)	Acc@1 86.914 (88.216)	Acc@5 99.609 (99.642)
Epoch: [65][9/25]	Time 0.588 (0.585)	Data 0.005 (0.075)	Loss 0.6124 (0.6068)	Acc@1 89.062 (88.301)	Acc@5 99.561 (99.634)
Epoch: [65][10/25]	Time 0.587 (0.585)	Data 0.006 (0.069)	Loss 0.6201 (0.6080)	Acc@1 88.281 (88.299)	Acc@5 99.609 (99.632)
Epoch: [65][11/25]	Time 0.609 (0.587)	Data 0.004 (0.064)	Loss 0.6210 (0.6091)	Acc@1 88.672 (88.330)	Acc@5 99.707 (99.638)
Epoch: [65][12/25]	Time 0.620 (0.590)	Data 0.007 (0.059)	Loss 0.5775 (0.6067)	Acc@1 89.502 (88.420)	Acc@5 99.658 (99.639)
Epoch: [65][13/25]	Time 0.593 (0.590)	Data 0.005 (0.055)	Loss 0.5829 (0.6050)	Acc@1 89.258 (88.480)	Acc@5 99.658 (99.641)
Epoch: [65][14/25]	Time 0.577 (0.589)	Data 0.006 (0.052)	Loss 0.6075 (0.6052)	Acc@1 88.379 (88.473)	Acc@5 99.512 (99.632)
Epoch: [65][15/25]	Time 0.614 (0.591)	Data 0.005 (0.049)	Loss 0.6364 (0.6071)	Acc@1 87.646 (88.422)	Acc@5 99.609 (99.631)
Epoch: [65][16/25]	Time 0.595 (0.591)	Data 0.006 (0.047)	Loss 0.6240 (0.6081)	Acc@1 88.330 (88.416)	Acc@5 99.268 (99.609)
Epoch: [65][17/25]	Time 0.586 (0.591)	Data 0.005 (0.044)	Loss 0.5937 (0.6073)	Acc@1 88.672 (88.430)	Acc@5 99.658 (99.612)
Epoch: [65][18/25]	Time 0.546 (0.588)	Data 0.005 (0.042)	Loss 0.6095 (0.6074)	Acc@1 88.672 (88.443)	Acc@5 99.512 (99.607)
Epoch: [65][19/25]	Time 0.608 (0.589)	Data 0.005 (0.040)	Loss 0.6255 (0.6083)	Acc@1 87.451 (88.394)	Acc@5 99.365 (99.595)
Epoch: [65][20/25]	Time 0.593 (0.589)	Data 0.005 (0.039)	Loss 0.5869 (0.6073)	Acc@1 88.818 (88.414)	Acc@5 99.854 (99.607)
Epoch: [65][21/25]	Time 0.612 (0.590)	Data 0.004 (0.037)	Loss 0.6370 (0.6086)	Acc@1 87.500 (88.372)	Acc@5 99.512 (99.603)
Epoch: [65][22/25]	Time 0.546 (0.589)	Data 0.004 (0.036)	Loss 0.5960 (0.6081)	Acc@1 88.672 (88.385)	Acc@5 99.756 (99.609)
Epoch: [65][23/25]	Time 0.609 (0.589)	Data 0.005 (0.034)	Loss 0.5624 (0.6062)	Acc@1 90.137 (88.458)	Acc@5 99.561 (99.607)
Epoch: [65][24/25]	Time 0.354 (0.580)	Data 0.004 (0.033)	Loss 0.6017 (0.6061)	Acc@1 89.033 (88.468)	Acc@5 99.764 (99.610)
[INFO] Force the sparse filters to zero...

Same Node:  [['module.conv2.weight', 'module.conv3.weight'], ['module.conv4.weight', 'module.conv5.weight'], ['module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight'], ['module.conv10.weight', 'module.conv11.weight'], ['module.conv12.weight', 'module.conv13.weight'], ['module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight'], ['module.conv19.weight', 'module.conv20.weight'], ['module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight'], ['module.conv26.weight', 'module.conv27.weight'], ['module.conv28.weight', 'module.conv29.weight'], ['module.conv30.weight', 'module.conv31.weight'], ['module.conv32.weight', 'module.conv33.weight']]
[INFO] Squeezing the sparse model to dense one...
Archnums: [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(22, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(45, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 453030 ; 487386 ; 0.9295096699535892

Epoch: [66 | 180] LR: 0.100000
Epoch: [66][0/25]	Time 0.599 (0.599)	Data 0.822 (0.822)	Loss 0.5832 (0.5832)	Acc@1 89.209 (89.209)	Acc@5 99.756 (99.756)
Epoch: [66][1/25]	Time 0.603 (0.601)	Data 0.004 (0.413)	Loss 0.5347 (0.5589)	Acc@1 91.016 (90.112)	Acc@5 99.854 (99.805)
Epoch: [66][2/25]	Time 0.561 (0.588)	Data 0.003 (0.276)	Loss 0.5088 (0.5422)	Acc@1 91.797 (90.674)	Acc@5 100.000 (99.870)
Epoch: [66][3/25]	Time 0.615 (0.594)	Data 0.005 (0.208)	Loss 0.5465 (0.5433)	Acc@1 90.527 (90.637)	Acc@5 99.658 (99.817)
Epoch: [66][4/25]	Time 0.594 (0.594)	Data 0.009 (0.169)	Loss 0.5304 (0.5407)	Acc@1 91.504 (90.811)	Acc@5 99.512 (99.756)
Epoch: [66][5/25]	Time 0.596 (0.595)	Data 0.006 (0.141)	Loss 0.5329 (0.5394)	Acc@1 90.918 (90.828)	Acc@5 99.805 (99.764)
Epoch: [66][6/25]	Time 0.581 (0.593)	Data 0.005 (0.122)	Loss 0.5360 (0.5389)	Acc@1 91.113 (90.869)	Acc@5 99.756 (99.763)
Epoch: [66][7/25]	Time 0.611 (0.595)	Data 0.005 (0.107)	Loss 0.5513 (0.5405)	Acc@1 90.186 (90.784)	Acc@5 99.756 (99.762)
Epoch: [66][8/25]	Time 0.560 (0.591)	Data 0.005 (0.096)	Loss 0.5471 (0.5412)	Acc@1 90.479 (90.750)	Acc@5 99.512 (99.734)
Epoch: [66][9/25]	Time 0.589 (0.591)	Data 0.004 (0.087)	Loss 0.5430 (0.5414)	Acc@1 90.527 (90.728)	Acc@5 99.609 (99.722)
Epoch: [66][10/25]	Time 0.597 (0.591)	Data 0.004 (0.079)	Loss 0.5587 (0.5430)	Acc@1 89.697 (90.634)	Acc@5 99.707 (99.720)
Epoch: [66][11/25]	Time 0.582 (0.591)	Data 0.006 (0.073)	Loss 0.5318 (0.5420)	Acc@1 90.869 (90.653)	Acc@5 99.707 (99.719)
Epoch: [66][12/25]	Time 0.589 (0.591)	Data 0.004 (0.068)	Loss 0.5182 (0.5402)	Acc@1 91.309 (90.704)	Acc@5 99.707 (99.718)
Epoch: [66][13/25]	Time 0.575 (0.589)	Data 0.006 (0.063)	Loss 0.5398 (0.5402)	Acc@1 90.967 (90.723)	Acc@5 99.707 (99.717)
Epoch: [66][14/25]	Time 0.589 (0.589)	Data 0.006 (0.060)	Loss 0.5785 (0.5427)	Acc@1 89.307 (90.628)	Acc@5 99.707 (99.717)
Epoch: [66][15/25]	Time 0.617 (0.591)	Data 0.007 (0.056)	Loss 0.5396 (0.5425)	Acc@1 90.820 (90.640)	Acc@5 99.756 (99.719)
Epoch: [66][16/25]	Time 0.585 (0.591)	Data 0.005 (0.053)	Loss 0.5566 (0.5434)	Acc@1 89.844 (90.593)	Acc@5 99.805 (99.724)
Epoch: [66][17/25]	Time 0.597 (0.591)	Data 0.005 (0.051)	Loss 0.5527 (0.5439)	Acc@1 89.990 (90.560)	Acc@5 99.707 (99.723)
Epoch: [66][18/25]	Time 0.530 (0.588)	Data 0.005 (0.048)	Loss 0.5535 (0.5444)	Acc@1 90.137 (90.538)	Acc@5 99.658 (99.720)
Epoch: [66][19/25]	Time 0.570 (0.587)	Data 0.004 (0.046)	Loss 0.5798 (0.5462)	Acc@1 89.307 (90.476)	Acc@5 99.512 (99.709)
Epoch: [66][20/25]	Time 0.605 (0.588)	Data 0.006 (0.044)	Loss 0.5517 (0.5464)	Acc@1 90.381 (90.472)	Acc@5 99.561 (99.702)
Epoch: [66][21/25]	Time 0.547 (0.586)	Data 0.004 (0.042)	Loss 0.5550 (0.5468)	Acc@1 89.746 (90.439)	Acc@5 99.707 (99.703)
Epoch: [66][22/25]	Time 0.598 (0.587)	Data 0.005 (0.041)	Loss 0.5931 (0.5488)	Acc@1 88.672 (90.362)	Acc@5 99.609 (99.699)
Epoch: [66][23/25]	Time 0.603 (0.587)	Data 0.004 (0.039)	Loss 0.5625 (0.5494)	Acc@1 89.502 (90.326)	Acc@5 99.561 (99.693)
Epoch: [66][24/25]	Time 0.314 (0.576)	Data 0.004 (0.038)	Loss 0.5606 (0.5496)	Acc@1 90.566 (90.330)	Acc@5 99.646 (99.692)

Epoch: [67 | 180] LR: 0.100000
Epoch: [67][0/25]	Time 0.673 (0.673)	Data 0.878 (0.878)	Loss 0.5589 (0.5589)	Acc@1 89.941 (89.941)	Acc@5 99.756 (99.756)
Epoch: [67][1/25]	Time 0.662 (0.667)	Data 0.003 (0.441)	Loss 0.5807 (0.5698)	Acc@1 88.965 (89.453)	Acc@5 99.609 (99.683)
Epoch: [67][2/25]	Time 0.529 (0.621)	Data 0.008 (0.296)	Loss 0.5721 (0.5706)	Acc@1 89.209 (89.372)	Acc@5 99.707 (99.691)
Epoch: [67][3/25]	Time 0.609 (0.618)	Data 0.004 (0.223)	Loss 0.5577 (0.5673)	Acc@1 89.209 (89.331)	Acc@5 99.756 (99.707)
Epoch: [67][4/25]	Time 0.620 (0.618)	Data 0.006 (0.180)	Loss 0.5902 (0.5719)	Acc@1 89.209 (89.307)	Acc@5 99.658 (99.697)
Epoch: [67][5/25]	Time 0.564 (0.609)	Data 0.006 (0.151)	Loss 0.5832 (0.5738)	Acc@1 88.867 (89.233)	Acc@5 99.658 (99.691)
Epoch: [67][6/25]	Time 0.619 (0.611)	Data 0.004 (0.130)	Loss 0.6180 (0.5801)	Acc@1 87.598 (89.000)	Acc@5 99.609 (99.679)
Epoch: [67][7/25]	Time 0.567 (0.605)	Data 0.004 (0.114)	Loss 0.5618 (0.5778)	Acc@1 89.502 (89.062)	Acc@5 99.805 (99.695)
Epoch: [67][8/25]	Time 0.572 (0.602)	Data 0.005 (0.102)	Loss 0.5675 (0.5767)	Acc@1 89.600 (89.122)	Acc@5 99.707 (99.696)
Epoch: [67][9/25]	Time 0.621 (0.603)	Data 0.005 (0.092)	Loss 0.6016 (0.5792)	Acc@1 88.428 (89.053)	Acc@5 99.463 (99.673)
Epoch: [67][10/25]	Time 0.567 (0.600)	Data 0.006 (0.085)	Loss 0.5862 (0.5798)	Acc@1 88.428 (88.996)	Acc@5 99.756 (99.680)
Epoch: [67][11/25]	Time 0.614 (0.601)	Data 0.004 (0.078)	Loss 0.5699 (0.5790)	Acc@1 89.404 (89.030)	Acc@5 99.512 (99.666)
Epoch: [67][12/25]	Time 0.565 (0.599)	Data 0.007 (0.072)	Loss 0.5878 (0.5797)	Acc@1 88.232 (88.969)	Acc@5 99.609 (99.662)
Traceback (most recent call last):
  File "main.py", line 721, in <module>
    main()
  File "main.py", line 314, in main
    use_gpu_num)
  File "main.py", line 406, in train
    total, use_before_forward, free = checkmem(use_gpu_num)
  File "main.py", line 712, in checkmem
    ).read().split('\n')[use_gpu].split(',')
ValueError: not enough values to unpack (expected 3, got 1)
no display found. Using non-interactive Agg backend
[5, 5, 5]
gpu id:0
Device Name: GeForce GTX 1080 Ti
Traceback (most recent call last):
  File "main.py", line 721, in <module>
    main()
  File "main.py", line 180, in main
    total, used, free = checkmem(gpu_id)
  File "main.py", line 712, in checkmem
    ).read().split('\n')[use_gpu].split(',')
ValueError: not enough values to unpack (expected 3, got 1)
no display found. Using non-interactive Agg backend
[5, 5, 5]
gpu id:0
Device Name: GeForce GTX 1080 Ti
Traceback (most recent call last):
  File "main.py", line 721, in <module>
    main()
  File "main.py", line 180, in main
    total, used, free = checkmem(gpu_id)
  File "main.py", line 712, in checkmem
    ).read().split('\n')[use_gpu].split(',')
ValueError: not enough values to unpack (expected 3, got 1)
no display found. Using non-interactive Agg backend
[5, 5, 5]
gpu id:0
Device Name: GeForce GTX 1080 Ti
Traceback (most recent call last):
  File "main.py", line 721, in <module>
    main()
  File "main.py", line 180, in main
    total, used, free = checkmem(gpu_id)
  File "main.py", line 712, in checkmem
    ).read().split('\n')[use_gpu].split(',')
ValueError: not enough values to unpack (expected 3, got 1)
no display found. Using non-interactive Agg backend
[5, 5, 5]
gpu id:0
Device Name: GeForce GTX 1080 Ti
Traceback (most recent call last):
  File "main.py", line 721, in <module>
    main()
  File "main.py", line 180, in main
    total, used, free = checkmem(gpu_id)
  File "main.py", line 712, in checkmem
    ).read().split('\n')[use_gpu].split(',')
ValueError: not enough values to unpack (expected 3, got 1)
no display found. Using non-interactive Agg backend
[5, 5, 5]
gpu id:0
Device Name: GeForce GTX 1080 Ti
Traceback (most recent call last):
  File "main.py", line 721, in <module>
    main()
  File "main.py", line 180, in main
    total, used, free = checkmem(gpu_id)
  File "main.py", line 712, in checkmem
    ).read().split('\n')[use_gpu].split(',')
ValueError: not enough values to unpack (expected 3, got 1)
no display found. Using non-interactive Agg backend
[5, 5, 5]
gpu id:0
Device Name: GeForce GTX 1080 Ti
Traceback (most recent call last):
  File "main.py", line 721, in <module>
    main()
  File "main.py", line 180, in main
    total, used, free = checkmem(gpu_id)
  File "main.py", line 712, in checkmem
    ).read().split('\n')[use_gpu].split(',')
ValueError: not enough values to unpack (expected 3, got 1)
no display found. Using non-interactive Agg backend
[5, 5, 5]
gpu id:0
Device Name: GeForce GTX 1080 Ti
Traceback (most recent call last):
  File "main.py", line 721, in <module>
    main()
  File "main.py", line 180, in main
    total, used, free = checkmem(gpu_id)
  File "main.py", line 712, in checkmem
    ).read().split('\n')[use_gpu].split(',')
ValueError: not enough values to unpack (expected 3, got 1)
