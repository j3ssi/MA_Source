no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.572 (0.572)	Data 0.167 (0.167)	Loss 3.1350 (3.1350)	Acc@1 6.250 (6.250)	Acc@5 50.781 (50.781)
Epoch: [1][64/391]	Time 0.506 (0.503)	Data 0.006 (0.004)	Loss 2.4621 (2.7481)	Acc@1 26.562 (20.685)	Acc@5 81.250 (74.675)
Epoch: [1][128/391]	Time 0.505 (0.504)	Data 0.001 (0.003)	Loss 2.3555 (2.5860)	Acc@1 31.250 (24.812)	Acc@5 82.812 (79.942)
Epoch: [1][192/391]	Time 0.467 (0.508)	Data 0.002 (0.003)	Loss 2.1895 (2.4884)	Acc@1 36.719 (27.421)	Acc@5 90.625 (82.760)
Epoch: [1][256/391]	Time 0.525 (0.510)	Data 0.002 (0.003)	Loss 2.1681 (2.4177)	Acc@1 35.938 (29.593)	Acc@5 91.406 (84.515)
Epoch: [1][320/391]	Time 0.563 (0.511)	Data 0.002 (0.002)	Loss 1.9880 (2.3545)	Acc@1 50.000 (31.744)	Acc@5 93.750 (85.809)
Epoch: [1][384/391]	Time 0.488 (0.510)	Data 0.002 (0.002)	Loss 1.7501 (2.2927)	Acc@1 53.125 (33.920)	Acc@5 96.094 (86.893)
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 323, in main
    use_gpu)
  File "main.py", line 545, in test
    outputs = model(inputs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 471, in forward
    _x = self.module_list[j](_x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 398.86 MiB already allocated; 13.94 MiB free; 426.00 MiB reserved in total by PyTorch)
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.579 (0.579)	Data 0.218 (0.218)	Loss 3.3711 (3.3711)	Acc@1 7.812 (7.812)	Acc@5 41.406 (41.406)
Epoch: [1][64/391]	Time 0.530 (0.509)	Data 0.002 (0.005)	Loss 2.7659 (2.9073)	Acc@1 15.625 (15.986)	Acc@5 75.781 (66.587)
Epoch: [1][128/391]	Time 0.477 (0.510)	Data 0.002 (0.004)	Loss 2.5699 (2.7242)	Acc@1 24.219 (20.246)	Acc@5 81.250 (74.945)
Epoch: [1][192/391]	Time 0.511 (0.514)	Data 0.002 (0.003)	Loss 2.2975 (2.6250)	Acc@1 34.375 (23.332)	Acc@5 86.719 (78.477)
Epoch: [1][256/391]	Time 0.505 (0.516)	Data 0.002 (0.003)	Loss 2.1195 (2.5421)	Acc@1 44.531 (25.894)	Acc@5 92.969 (80.946)
Epoch: [1][320/391]	Time 0.502 (0.517)	Data 0.002 (0.003)	Loss 2.1856 (2.4741)	Acc@1 40.625 (28.147)	Acc@5 91.406 (82.710)
Epoch: [1][384/391]	Time 0.467 (0.515)	Data 0.002 (0.002)	Loss 2.0907 (2.4088)	Acc@1 44.531 (30.507)	Acc@5 92.969 (84.103)
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 323, in main
    use_gpu)
  File "main.py", line 545, in test
    outputs = model(inputs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 471, in forward
    _x = self.module_list[j](_x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 398.86 MiB already allocated; 13.94 MiB free; 426.00 MiB reserved in total by PyTorch)
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.555 (0.555)	Data 0.208 (0.208)	Loss 3.3772 (3.3772)	Acc@1 10.938 (10.938)	Acc@5 48.438 (48.438)
Epoch: [1][64/391]	Time 0.513 (0.516)	Data 0.002 (0.005)	Loss 2.5856 (2.7317)	Acc@1 27.344 (23.317)	Acc@5 81.250 (75.962)
Epoch: [1][128/391]	Time 0.511 (0.516)	Data 0.002 (0.003)	Loss 2.2921 (2.5923)	Acc@1 36.719 (27.580)	Acc@5 87.500 (80.905)
Epoch: [1][192/391]	Time 0.498 (0.516)	Data 0.002 (0.003)	Loss 2.3199 (2.4944)	Acc@1 35.938 (30.538)	Acc@5 86.719 (83.541)
Epoch: [1][256/391]	Time 0.535 (0.516)	Data 0.002 (0.003)	Loss 2.2587 (2.4235)	Acc@1 40.625 (32.639)	Acc@5 82.812 (85.019)
Epoch: [1][320/391]	Time 0.505 (0.517)	Data 0.002 (0.003)	Loss 2.0056 (2.3588)	Acc@1 46.094 (34.650)	Acc@5 91.406 (86.181)
Epoch: [1][384/391]	Time 0.461 (0.517)	Data 0.002 (0.002)	Loss 1.9309 (2.2922)	Acc@1 50.781 (36.763)	Acc@5 96.094 (87.354)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.487 (0.487)	Data 0.251 (0.251)	Loss 2.1606 (2.1606)	Acc@1 39.062 (39.062)	Acc@5 87.500 (87.500)
Epoch: [2][64/391]	Time 0.451 (0.457)	Data 0.002 (0.006)	Loss 1.8191 (1.8202)	Acc@1 56.250 (52.175)	Acc@5 95.312 (94.014)
Epoch: [2][128/391]	Time 0.449 (0.457)	Data 0.002 (0.004)	Loss 1.7024 (1.7721)	Acc@1 57.812 (53.876)	Acc@5 95.312 (94.543)
Epoch: [2][192/391]	Time 0.457 (0.459)	Data 0.002 (0.003)	Loss 1.5170 (1.7311)	Acc@1 64.062 (55.396)	Acc@5 96.875 (94.802)
Epoch: [2][256/391]	Time 0.442 (0.458)	Data 0.001 (0.003)	Loss 1.4376 (1.6961)	Acc@1 64.062 (56.505)	Acc@5 96.094 (94.984)
Epoch: [2][320/391]	Time 0.422 (0.458)	Data 0.002 (0.003)	Loss 1.4120 (1.6617)	Acc@1 67.969 (57.423)	Acc@5 95.312 (95.242)
Epoch: [2][384/391]	Time 0.478 (0.459)	Data 0.002 (0.003)	Loss 1.1716 (1.6268)	Acc@1 71.094 (58.561)	Acc@5 98.438 (95.436)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.490 (0.490)	Data 0.273 (0.273)	Loss 1.3238 (1.3238)	Acc@1 69.531 (69.531)	Acc@5 99.219 (99.219)
Epoch: [3][64/391]	Time 0.456 (0.459)	Data 0.002 (0.006)	Loss 1.2781 (1.4486)	Acc@1 70.312 (63.486)	Acc@5 97.656 (96.346)
Epoch: [3][128/391]	Time 0.458 (0.459)	Data 0.002 (0.004)	Loss 1.1851 (1.4087)	Acc@1 67.188 (64.832)	Acc@5 97.656 (96.548)
Epoch: [3][192/391]	Time 0.451 (0.460)	Data 0.002 (0.003)	Loss 1.3040 (1.3784)	Acc@1 65.625 (65.625)	Acc@5 97.656 (96.745)
Epoch: [3][256/391]	Time 0.464 (0.460)	Data 0.002 (0.003)	Loss 1.1129 (1.3556)	Acc@1 75.781 (66.227)	Acc@5 100.000 (96.863)
Epoch: [3][320/391]	Time 0.455 (0.461)	Data 0.002 (0.003)	Loss 1.2499 (1.3371)	Acc@1 64.844 (66.608)	Acc@5 98.438 (97.087)
Epoch: [3][384/391]	Time 0.431 (0.461)	Data 0.001 (0.003)	Loss 1.0562 (1.3187)	Acc@1 75.781 (67.121)	Acc@5 98.438 (97.192)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.489 (0.489)	Data 0.197 (0.197)	Loss 1.3043 (1.3043)	Acc@1 67.969 (67.969)	Acc@5 96.875 (96.875)
Epoch: [4][64/391]	Time 0.451 (0.458)	Data 0.002 (0.005)	Loss 1.0317 (1.1945)	Acc@1 77.344 (70.841)	Acc@5 99.219 (97.957)
Epoch: [4][128/391]	Time 0.483 (0.457)	Data 0.002 (0.003)	Loss 1.1677 (1.1891)	Acc@1 69.531 (70.779)	Acc@5 98.438 (97.892)
Epoch: [4][192/391]	Time 0.475 (0.460)	Data 0.002 (0.003)	Loss 1.2582 (1.1736)	Acc@1 67.969 (71.017)	Acc@5 96.094 (97.927)
Epoch: [4][256/391]	Time 0.468 (0.461)	Data 0.002 (0.003)	Loss 1.3022 (1.1669)	Acc@1 69.531 (71.115)	Acc@5 95.312 (97.893)
Epoch: [4][320/391]	Time 0.440 (0.461)	Data 0.002 (0.003)	Loss 0.9421 (1.1601)	Acc@1 81.250 (71.323)	Acc@5 99.219 (97.934)
Epoch: [4][384/391]	Time 0.449 (0.463)	Data 0.002 (0.002)	Loss 1.1669 (1.1490)	Acc@1 71.094 (71.654)	Acc@5 99.219 (97.934)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.429 (0.429)	Data 0.197 (0.197)	Loss 1.0469 (1.0469)	Acc@1 73.438 (73.438)	Acc@5 100.000 (100.000)
Epoch: [5][64/391]	Time 0.481 (0.458)	Data 0.002 (0.005)	Loss 1.0093 (1.0807)	Acc@1 75.781 (73.942)	Acc@5 98.438 (98.089)
Epoch: [5][128/391]	Time 0.440 (0.459)	Data 0.002 (0.004)	Loss 1.1025 (1.0781)	Acc@1 73.438 (73.874)	Acc@5 100.000 (98.177)
Epoch: [5][192/391]	Time 0.510 (0.459)	Data 0.002 (0.003)	Loss 0.9005 (1.0695)	Acc@1 80.469 (74.130)	Acc@5 97.656 (98.097)
Epoch: [5][256/391]	Time 0.442 (0.461)	Data 0.001 (0.003)	Loss 1.0396 (1.0669)	Acc@1 76.562 (74.191)	Acc@5 94.531 (98.118)
Epoch: [5][320/391]	Time 0.445 (0.461)	Data 0.002 (0.003)	Loss 0.9545 (1.0567)	Acc@1 80.469 (74.491)	Acc@5 97.656 (98.148)
Epoch: [5][384/391]	Time 0.461 (0.462)	Data 0.002 (0.003)	Loss 1.0047 (1.0516)	Acc@1 73.438 (74.596)	Acc@5 99.219 (98.145)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.494 (0.494)	Data 0.182 (0.182)	Loss 1.0648 (1.0648)	Acc@1 73.438 (73.438)	Acc@5 95.312 (95.312)
Epoch: [6][64/391]	Time 0.453 (0.461)	Data 0.002 (0.005)	Loss 0.8626 (0.9941)	Acc@1 84.375 (76.070)	Acc@5 98.438 (98.450)
Epoch: [6][128/391]	Time 0.454 (0.459)	Data 0.001 (0.003)	Loss 1.1544 (1.0080)	Acc@1 73.438 (75.563)	Acc@5 98.438 (98.419)
Epoch: [6][192/391]	Time 0.540 (0.462)	Data 0.002 (0.003)	Loss 0.9192 (1.0129)	Acc@1 78.906 (75.644)	Acc@5 99.219 (98.340)
Epoch: [6][256/391]	Time 0.487 (0.462)	Data 0.002 (0.003)	Loss 1.0007 (1.0109)	Acc@1 77.344 (75.681)	Acc@5 96.875 (98.419)
Epoch: [6][320/391]	Time 0.519 (0.463)	Data 0.002 (0.003)	Loss 0.8054 (1.0069)	Acc@1 82.812 (75.825)	Acc@5 98.438 (98.450)
Epoch: [6][384/391]	Time 0.490 (0.463)	Data 0.002 (0.002)	Loss 0.9654 (1.0049)	Acc@1 76.562 (75.919)	Acc@5 97.656 (98.462)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.471 (0.471)	Data 0.222 (0.222)	Loss 0.9400 (0.9400)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [7][64/391]	Time 0.461 (0.464)	Data 0.002 (0.005)	Loss 0.9821 (0.9572)	Acc@1 78.125 (77.163)	Acc@5 99.219 (98.702)
Epoch: [7][128/391]	Time 0.460 (0.461)	Data 0.002 (0.004)	Loss 0.7946 (0.9661)	Acc@1 84.375 (77.247)	Acc@5 100.000 (98.565)
Epoch: [7][192/391]	Time 0.488 (0.461)	Data 0.002 (0.003)	Loss 1.1538 (0.9663)	Acc@1 74.219 (77.048)	Acc@5 100.000 (98.644)
Epoch: [7][256/391]	Time 0.455 (0.462)	Data 0.002 (0.003)	Loss 0.9112 (0.9739)	Acc@1 81.250 (76.882)	Acc@5 100.000 (98.565)
Epoch: [7][320/391]	Time 0.461 (0.462)	Data 0.002 (0.003)	Loss 0.9635 (0.9738)	Acc@1 76.562 (76.893)	Acc@5 99.219 (98.559)
Epoch: [7][384/391]	Time 0.453 (0.462)	Data 0.001 (0.003)	Loss 0.9558 (0.9724)	Acc@1 79.688 (77.003)	Acc@5 98.438 (98.553)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.511 (0.511)	Data 0.217 (0.217)	Loss 0.7942 (0.7942)	Acc@1 85.156 (85.156)	Acc@5 98.438 (98.438)
Epoch: [8][64/391]	Time 0.460 (0.462)	Data 0.002 (0.005)	Loss 1.0200 (0.9457)	Acc@1 75.781 (78.113)	Acc@5 99.219 (98.510)
Epoch: [8][128/391]	Time 0.452 (0.460)	Data 0.002 (0.004)	Loss 0.9624 (0.9525)	Acc@1 78.125 (77.786)	Acc@5 97.656 (98.625)
Epoch: [8][192/391]	Time 0.569 (0.460)	Data 0.002 (0.003)	Loss 0.9141 (0.9505)	Acc@1 77.344 (77.906)	Acc@5 97.656 (98.595)
Epoch: [8][256/391]	Time 0.461 (0.462)	Data 0.002 (0.003)	Loss 0.8709 (0.9497)	Acc@1 80.469 (77.882)	Acc@5 99.219 (98.626)
Epoch: [8][320/391]	Time 0.483 (0.461)	Data 0.002 (0.003)	Loss 0.9182 (0.9518)	Acc@1 78.906 (77.816)	Acc@5 98.438 (98.647)
Epoch: [8][384/391]	Time 0.462 (0.462)	Data 0.002 (0.003)	Loss 0.9701 (0.9493)	Acc@1 77.344 (77.914)	Acc@5 100.000 (98.677)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.446 (0.446)	Data 0.278 (0.278)	Loss 0.7859 (0.7859)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [9][64/391]	Time 0.468 (0.460)	Data 0.002 (0.006)	Loss 1.0349 (0.9461)	Acc@1 76.562 (77.740)	Acc@5 99.219 (98.702)
Epoch: [9][128/391]	Time 0.478 (0.460)	Data 0.002 (0.004)	Loss 1.1133 (0.9514)	Acc@1 75.781 (77.792)	Acc@5 96.875 (98.674)
Epoch: [9][192/391]	Time 0.478 (0.459)	Data 0.002 (0.003)	Loss 0.9660 (0.9444)	Acc@1 76.562 (78.040)	Acc@5 100.000 (98.656)
Epoch: [9][256/391]	Time 0.460 (0.459)	Data 0.002 (0.003)	Loss 0.8469 (0.9467)	Acc@1 79.688 (77.949)	Acc@5 97.656 (98.656)
Epoch: [9][320/391]	Time 0.501 (0.460)	Data 0.001 (0.003)	Loss 0.8858 (0.9422)	Acc@1 82.812 (78.113)	Acc@5 97.656 (98.708)
Epoch: [9][384/391]	Time 0.478 (0.459)	Data 0.002 (0.003)	Loss 0.8982 (0.9399)	Acc@1 76.562 (78.228)	Acc@5 98.438 (98.707)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.418 (0.418)	Data 0.245 (0.245)	Loss 1.0389 (1.0389)	Acc@1 74.219 (74.219)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.448 (0.457)	Data 0.002 (0.006)	Loss 0.7174 (0.9390)	Acc@1 89.062 (77.957)	Acc@5 100.000 (98.762)
Epoch: [10][128/391]	Time 0.445 (0.460)	Data 0.002 (0.004)	Loss 0.9131 (0.9373)	Acc@1 82.031 (78.252)	Acc@5 98.438 (98.625)
Epoch: [10][192/391]	Time 0.471 (0.459)	Data 0.002 (0.003)	Loss 0.7599 (0.9281)	Acc@1 79.688 (78.615)	Acc@5 100.000 (98.737)
Epoch: [10][256/391]	Time 0.462 (0.460)	Data 0.002 (0.003)	Loss 0.8665 (0.9278)	Acc@1 78.906 (78.724)	Acc@5 99.219 (98.790)
Epoch: [10][320/391]	Time 0.528 (0.460)	Data 0.002 (0.003)	Loss 0.9847 (0.9268)	Acc@1 77.344 (78.743)	Acc@5 100.000 (98.812)
Epoch: [10][384/391]	Time 0.454 (0.461)	Data 0.002 (0.003)	Loss 0.8292 (0.9265)	Acc@1 83.594 (78.774)	Acc@5 98.438 (98.829)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 477858 ; 487386 ; 0.9804508131132204

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.474 (0.474)	Data 0.197 (0.197)	Loss 0.9082 (0.9082)	Acc@1 78.906 (78.906)	Acc@5 97.656 (97.656)
Epoch: [11][64/391]	Time 0.459 (0.456)	Data 0.002 (0.005)	Loss 1.1032 (0.9082)	Acc@1 71.094 (78.750)	Acc@5 97.656 (98.882)
Epoch: [11][128/391]	Time 0.484 (0.458)	Data 0.002 (0.003)	Loss 1.1250 (0.9291)	Acc@1 74.219 (78.452)	Acc@5 98.438 (98.789)
Epoch: [11][192/391]	Time 0.451 (0.458)	Data 0.002 (0.003)	Loss 0.9214 (0.9199)	Acc@1 77.344 (78.991)	Acc@5 100.000 (98.802)
Epoch: [11][256/391]	Time 0.464 (0.459)	Data 0.002 (0.003)	Loss 0.9160 (0.9207)	Acc@1 78.125 (79.007)	Acc@5 98.438 (98.793)
Epoch: [11][320/391]	Time 0.489 (0.460)	Data 0.002 (0.003)	Loss 0.7955 (0.9201)	Acc@1 82.812 (79.045)	Acc@5 99.219 (98.812)
Epoch: [11][384/391]	Time 0.483 (0.460)	Data 0.002 (0.002)	Loss 0.9967 (0.9209)	Acc@1 76.562 (79.048)	Acc@5 99.219 (98.793)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.472 (0.472)	Data 0.197 (0.197)	Loss 0.9766 (0.9766)	Acc@1 77.344 (77.344)	Acc@5 99.219 (99.219)
Epoch: [12][64/391]	Time 0.427 (0.458)	Data 0.002 (0.005)	Loss 0.9405 (0.9338)	Acc@1 78.125 (78.618)	Acc@5 96.875 (98.738)
Epoch: [12][128/391]	Time 0.453 (0.458)	Data 0.002 (0.003)	Loss 0.9304 (0.9171)	Acc@1 75.000 (78.943)	Acc@5 98.438 (98.758)
Epoch: [12][192/391]	Time 0.450 (0.457)	Data 0.002 (0.003)	Loss 0.9626 (0.9078)	Acc@1 80.469 (79.275)	Acc@5 97.656 (98.818)
Epoch: [12][256/391]	Time 0.454 (0.460)	Data 0.002 (0.003)	Loss 0.9539 (0.9109)	Acc@1 74.219 (79.083)	Acc@5 96.875 (98.799)
Epoch: [12][320/391]	Time 0.476 (0.459)	Data 0.002 (0.003)	Loss 0.9262 (0.9083)	Acc@1 78.906 (79.220)	Acc@5 98.438 (98.798)
Epoch: [12][384/391]	Time 0.440 (0.459)	Data 0.002 (0.002)	Loss 0.9525 (0.9085)	Acc@1 75.781 (79.192)	Acc@5 100.000 (98.833)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.480 (0.480)	Data 0.166 (0.166)	Loss 0.8664 (0.8664)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [13][64/391]	Time 0.473 (0.464)	Data 0.002 (0.005)	Loss 0.9028 (0.8967)	Acc@1 77.344 (79.567)	Acc@5 98.438 (98.954)
Epoch: [13][128/391]	Time 0.462 (0.460)	Data 0.002 (0.003)	Loss 0.8015 (0.8933)	Acc@1 83.594 (79.403)	Acc@5 100.000 (98.946)
Epoch: [13][192/391]	Time 0.472 (0.460)	Data 0.002 (0.003)	Loss 0.7716 (0.8948)	Acc@1 85.156 (79.420)	Acc@5 99.219 (99.008)
Epoch: [13][256/391]	Time 0.451 (0.462)	Data 0.002 (0.003)	Loss 0.9840 (0.8984)	Acc@1 76.562 (79.420)	Acc@5 97.656 (98.884)
Epoch: [13][320/391]	Time 0.461 (0.459)	Data 0.002 (0.003)	Loss 0.8700 (0.8976)	Acc@1 79.688 (79.568)	Acc@5 100.000 (98.902)
Epoch: [13][384/391]	Time 0.477 (0.458)	Data 0.002 (0.002)	Loss 1.0484 (0.9015)	Acc@1 75.000 (79.478)	Acc@5 98.438 (98.904)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.473 (0.473)	Data 0.205 (0.205)	Loss 0.8916 (0.8916)	Acc@1 79.688 (79.688)	Acc@5 97.656 (97.656)
Epoch: [14][64/391]	Time 0.465 (0.458)	Data 0.002 (0.005)	Loss 0.8823 (0.8900)	Acc@1 81.250 (80.505)	Acc@5 99.219 (98.918)
Epoch: [14][128/391]	Time 0.454 (0.457)	Data 0.001 (0.004)	Loss 0.7319 (0.8960)	Acc@1 86.719 (80.269)	Acc@5 100.000 (98.892)
Epoch: [14][192/391]	Time 0.459 (0.455)	Data 0.002 (0.003)	Loss 0.9186 (0.8973)	Acc@1 78.125 (79.967)	Acc@5 100.000 (98.854)
Epoch: [14][256/391]	Time 0.452 (0.457)	Data 0.002 (0.003)	Loss 0.8620 (0.9002)	Acc@1 81.250 (79.821)	Acc@5 99.219 (98.821)
Epoch: [14][320/391]	Time 0.431 (0.457)	Data 0.002 (0.003)	Loss 0.8226 (0.9028)	Acc@1 84.375 (79.702)	Acc@5 99.219 (98.788)
Epoch: [14][384/391]	Time 0.453 (0.458)	Data 0.002 (0.002)	Loss 0.9548 (0.9010)	Acc@1 78.906 (79.801)	Acc@5 97.656 (98.795)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.476 (0.476)	Data 0.233 (0.233)	Loss 0.8644 (0.8644)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [15][64/391]	Time 0.446 (0.456)	Data 0.002 (0.006)	Loss 0.8994 (0.8734)	Acc@1 78.125 (80.637)	Acc@5 99.219 (98.834)
Epoch: [15][128/391]	Time 0.450 (0.455)	Data 0.002 (0.004)	Loss 0.9473 (0.8814)	Acc@1 75.781 (80.105)	Acc@5 98.438 (98.892)
Epoch: [15][192/391]	Time 0.456 (0.455)	Data 0.002 (0.003)	Loss 0.8289 (0.8831)	Acc@1 79.688 (80.295)	Acc@5 100.000 (98.883)
Epoch: [15][256/391]	Time 0.456 (0.455)	Data 0.002 (0.003)	Loss 0.9209 (0.8890)	Acc@1 76.562 (80.064)	Acc@5 99.219 (98.887)
Epoch: [15][320/391]	Time 0.458 (0.455)	Data 0.002 (0.003)	Loss 0.9823 (0.8883)	Acc@1 74.219 (79.963)	Acc@5 97.656 (98.907)
Epoch: [15][384/391]	Time 0.499 (0.456)	Data 0.002 (0.003)	Loss 1.0054 (0.8882)	Acc@1 78.125 (79.982)	Acc@5 99.219 (98.892)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 404832 ; 487386 ; 0.8306188524085633

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.450 (0.450)	Data 0.198 (0.198)	Loss 0.7592 (0.7592)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [16][64/391]	Time 0.459 (0.453)	Data 0.002 (0.005)	Loss 1.0611 (0.8599)	Acc@1 72.656 (80.697)	Acc@5 99.219 (99.183)
Epoch: [16][128/391]	Time 0.467 (0.455)	Data 0.002 (0.004)	Loss 0.9486 (0.8691)	Acc@1 82.031 (80.572)	Acc@5 97.656 (98.977)
Epoch: [16][192/391]	Time 0.443 (0.455)	Data 0.002 (0.003)	Loss 0.8531 (0.8697)	Acc@1 80.469 (80.481)	Acc@5 98.438 (98.964)
Epoch: [16][256/391]	Time 0.471 (0.456)	Data 0.002 (0.003)	Loss 0.7942 (0.8776)	Acc@1 85.938 (80.271)	Acc@5 99.219 (98.960)
Epoch: [16][320/391]	Time 0.449 (0.456)	Data 0.002 (0.003)	Loss 0.8629 (0.8772)	Acc@1 80.469 (80.330)	Acc@5 98.438 (98.951)
Epoch: [16][384/391]	Time 0.420 (0.456)	Data 0.003 (0.002)	Loss 0.9587 (0.8797)	Acc@1 76.562 (80.262)	Acc@5 99.219 (98.953)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.465 (0.465)	Data 0.209 (0.209)	Loss 0.8858 (0.8858)	Acc@1 75.000 (75.000)	Acc@5 99.219 (99.219)
Epoch: [17][64/391]	Time 0.445 (0.459)	Data 0.002 (0.005)	Loss 0.7245 (0.8627)	Acc@1 84.375 (80.950)	Acc@5 99.219 (98.822)
Epoch: [17][128/391]	Time 0.442 (0.460)	Data 0.002 (0.004)	Loss 0.8333 (0.8813)	Acc@1 82.812 (80.529)	Acc@5 99.219 (98.758)
Epoch: [17][192/391]	Time 0.473 (0.458)	Data 0.002 (0.003)	Loss 0.8918 (0.8851)	Acc@1 82.031 (80.319)	Acc@5 99.219 (98.769)
Epoch: [17][256/391]	Time 0.434 (0.458)	Data 0.002 (0.003)	Loss 0.9000 (0.8799)	Acc@1 82.031 (80.414)	Acc@5 99.219 (98.833)
Epoch: [17][320/391]	Time 0.467 (0.458)	Data 0.002 (0.003)	Loss 0.8681 (0.8811)	Acc@1 80.469 (80.369)	Acc@5 98.438 (98.856)
Epoch: [17][384/391]	Time 0.467 (0.458)	Data 0.002 (0.003)	Loss 0.8843 (0.8810)	Acc@1 78.906 (80.270)	Acc@5 99.219 (98.870)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.455 (0.455)	Data 0.235 (0.235)	Loss 0.9261 (0.9261)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [18][64/391]	Time 0.469 (0.456)	Data 0.002 (0.006)	Loss 0.8863 (0.8735)	Acc@1 78.906 (80.264)	Acc@5 98.438 (99.147)
Epoch: [18][128/391]	Time 0.460 (0.454)	Data 0.002 (0.004)	Loss 0.8301 (0.8762)	Acc@1 82.031 (80.154)	Acc@5 98.438 (99.013)
Epoch: [18][192/391]	Time 0.468 (0.456)	Data 0.002 (0.003)	Loss 0.8213 (0.8776)	Acc@1 83.594 (80.056)	Acc@5 99.219 (99.045)
Epoch: [18][256/391]	Time 0.408 (0.457)	Data 0.002 (0.003)	Loss 0.7623 (0.8717)	Acc@1 85.156 (80.362)	Acc@5 100.000 (99.039)
Epoch: [18][320/391]	Time 0.427 (0.454)	Data 0.001 (0.003)	Loss 0.9046 (0.8676)	Acc@1 81.250 (80.605)	Acc@5 96.875 (99.046)
Epoch: [18][384/391]	Time 0.437 (0.455)	Data 0.002 (0.003)	Loss 0.9503 (0.8703)	Acc@1 74.219 (80.550)	Acc@5 99.219 (99.075)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.524 (0.524)	Data 0.208 (0.208)	Loss 0.7528 (0.7528)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [19][64/391]	Time 0.484 (0.445)	Data 0.002 (0.005)	Loss 0.8327 (0.8513)	Acc@1 79.688 (80.649)	Acc@5 99.219 (99.195)
Epoch: [19][128/391]	Time 0.468 (0.448)	Data 0.003 (0.004)	Loss 1.0186 (0.8540)	Acc@1 76.562 (80.481)	Acc@5 97.656 (99.013)
Epoch: [19][192/391]	Time 0.473 (0.449)	Data 0.002 (0.003)	Loss 0.8927 (0.8577)	Acc@1 81.250 (80.396)	Acc@5 99.219 (98.996)
Epoch: [19][256/391]	Time 0.399 (0.449)	Data 0.002 (0.003)	Loss 0.9128 (0.8593)	Acc@1 78.906 (80.335)	Acc@5 99.219 (99.003)
Epoch: [19][320/391]	Time 0.442 (0.449)	Data 0.001 (0.003)	Loss 0.8181 (0.8603)	Acc@1 82.812 (80.313)	Acc@5 100.000 (98.958)
Epoch: [19][384/391]	Time 0.414 (0.451)	Data 0.003 (0.003)	Loss 0.8694 (0.8594)	Acc@1 81.250 (80.379)	Acc@5 100.000 (98.973)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.459 (0.459)	Data 0.236 (0.236)	Loss 0.7499 (0.7499)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [20][64/391]	Time 0.459 (0.458)	Data 0.002 (0.006)	Loss 0.8416 (0.8235)	Acc@1 78.906 (81.743)	Acc@5 100.000 (99.147)
Epoch: [20][128/391]	Time 0.459 (0.454)	Data 0.002 (0.004)	Loss 0.8070 (0.8426)	Acc@1 81.250 (80.990)	Acc@5 100.000 (99.073)
Epoch: [20][192/391]	Time 0.442 (0.454)	Data 0.002 (0.003)	Loss 0.8412 (0.8466)	Acc@1 78.125 (80.910)	Acc@5 100.000 (99.012)
Epoch: [20][256/391]	Time 0.460 (0.453)	Data 0.002 (0.003)	Loss 0.7019 (0.8517)	Acc@1 87.500 (80.761)	Acc@5 100.000 (99.027)
Epoch: [20][320/391]	Time 0.446 (0.453)	Data 0.002 (0.003)	Loss 0.8052 (0.8490)	Acc@1 82.031 (80.809)	Acc@5 98.438 (99.019)
Epoch: [20][384/391]	Time 0.459 (0.453)	Data 0.002 (0.003)	Loss 0.7985 (0.8478)	Acc@1 82.812 (80.812)	Acc@5 98.438 (98.998)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 339170 ; 487386 ; 0.6958960659518328

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.476 (0.476)	Data 0.201 (0.201)	Loss 0.8510 (0.8510)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [21][64/391]	Time 0.394 (0.453)	Data 0.002 (0.005)	Loss 1.0503 (0.8386)	Acc@1 75.781 (80.649)	Acc@5 99.219 (98.978)
Epoch: [21][128/391]	Time 0.463 (0.452)	Data 0.002 (0.004)	Loss 0.8905 (0.8472)	Acc@1 82.031 (80.481)	Acc@5 100.000 (98.983)
Epoch: [21][192/391]	Time 0.463 (0.450)	Data 0.002 (0.003)	Loss 0.6747 (0.8423)	Acc@1 85.938 (80.760)	Acc@5 100.000 (98.964)
Epoch: [21][256/391]	Time 0.464 (0.450)	Data 0.002 (0.003)	Loss 0.7730 (0.8479)	Acc@1 82.031 (80.606)	Acc@5 99.219 (98.973)
Epoch: [21][320/391]	Time 0.468 (0.452)	Data 0.002 (0.003)	Loss 0.9480 (0.8451)	Acc@1 80.469 (80.771)	Acc@5 99.219 (98.988)
Epoch: [21][384/391]	Time 0.463 (0.452)	Data 0.002 (0.002)	Loss 0.8617 (0.8450)	Acc@1 78.906 (80.785)	Acc@5 99.219 (99.000)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.505 (0.505)	Data 0.271 (0.271)	Loss 0.8444 (0.8444)	Acc@1 82.812 (82.812)	Acc@5 98.438 (98.438)
Epoch: [22][64/391]	Time 0.483 (0.450)	Data 0.003 (0.006)	Loss 0.6489 (0.8111)	Acc@1 89.844 (81.803)	Acc@5 100.000 (99.219)
Epoch: [22][128/391]	Time 0.473 (0.452)	Data 0.002 (0.004)	Loss 0.7040 (0.8258)	Acc@1 84.375 (81.450)	Acc@5 99.219 (99.122)
Epoch: [22][192/391]	Time 0.450 (0.451)	Data 0.001 (0.003)	Loss 0.8045 (0.8356)	Acc@1 85.938 (81.177)	Acc@5 99.219 (99.049)
Epoch: [22][256/391]	Time 0.471 (0.452)	Data 0.002 (0.003)	Loss 0.7411 (0.8366)	Acc@1 83.594 (81.131)	Acc@5 100.000 (99.036)
Epoch: [22][320/391]	Time 0.513 (0.452)	Data 0.002 (0.003)	Loss 0.9123 (0.8402)	Acc@1 75.781 (80.948)	Acc@5 100.000 (99.019)
Epoch: [22][384/391]	Time 0.408 (0.452)	Data 0.001 (0.003)	Loss 0.8186 (0.8384)	Acc@1 82.031 (80.998)	Acc@5 98.438 (99.016)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.437 (0.437)	Data 0.201 (0.201)	Loss 0.8554 (0.8554)	Acc@1 80.469 (80.469)	Acc@5 97.656 (97.656)
Epoch: [23][64/391]	Time 0.447 (0.452)	Data 0.002 (0.005)	Loss 0.9723 (0.8329)	Acc@1 72.656 (81.454)	Acc@5 100.000 (99.135)
Epoch: [23][128/391]	Time 0.463 (0.452)	Data 0.002 (0.004)	Loss 0.8801 (0.8301)	Acc@1 81.250 (81.595)	Acc@5 97.656 (99.049)
Epoch: [23][192/391]	Time 0.434 (0.451)	Data 0.002 (0.003)	Loss 0.8814 (0.8233)	Acc@1 78.125 (81.647)	Acc@5 99.219 (99.037)
Epoch: [23][256/391]	Time 0.401 (0.447)	Data 0.002 (0.003)	Loss 0.8507 (0.8239)	Acc@1 79.688 (81.673)	Acc@5 97.656 (99.018)
Epoch: [23][320/391]	Time 0.443 (0.448)	Data 0.002 (0.003)	Loss 0.7906 (0.8299)	Acc@1 81.250 (81.510)	Acc@5 99.219 (99.026)
Epoch: [23][384/391]	Time 0.476 (0.447)	Data 0.002 (0.002)	Loss 0.8790 (0.8344)	Acc@1 80.469 (81.356)	Acc@5 98.438 (99.038)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.488 (0.488)	Data 0.192 (0.192)	Loss 0.9089 (0.9089)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [24][64/391]	Time 0.467 (0.460)	Data 0.002 (0.005)	Loss 0.8123 (0.8279)	Acc@1 81.250 (82.103)	Acc@5 97.656 (98.870)
Epoch: [24][128/391]	Time 0.451 (0.456)	Data 0.002 (0.003)	Loss 0.9019 (0.8267)	Acc@1 81.250 (81.747)	Acc@5 99.219 (98.898)
Epoch: [24][192/391]	Time 0.487 (0.454)	Data 0.002 (0.003)	Loss 0.8179 (0.8240)	Acc@1 82.812 (81.805)	Acc@5 98.438 (98.960)
Epoch: [24][256/391]	Time 0.480 (0.455)	Data 0.002 (0.003)	Loss 0.8308 (0.8253)	Acc@1 80.469 (81.648)	Acc@5 100.000 (98.945)
Epoch: [24][320/391]	Time 0.436 (0.455)	Data 0.002 (0.003)	Loss 0.9307 (0.8220)	Acc@1 75.000 (81.715)	Acc@5 96.094 (98.978)
Epoch: [24][384/391]	Time 0.475 (0.455)	Data 0.002 (0.002)	Loss 0.7449 (0.8255)	Acc@1 86.719 (81.597)	Acc@5 99.219 (98.977)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.456 (0.456)	Data 0.198 (0.198)	Loss 0.7860 (0.7860)	Acc@1 82.812 (82.812)	Acc@5 98.438 (98.438)
Epoch: [25][64/391]	Time 0.386 (0.448)	Data 0.002 (0.005)	Loss 0.9299 (0.8236)	Acc@1 78.906 (81.611)	Acc@5 99.219 (98.954)
Epoch: [25][128/391]	Time 0.448 (0.450)	Data 0.002 (0.004)	Loss 0.8341 (0.8262)	Acc@1 80.469 (81.292)	Acc@5 98.438 (98.970)
Epoch: [25][192/391]	Time 0.452 (0.451)	Data 0.002 (0.003)	Loss 0.8273 (0.8303)	Acc@1 81.250 (81.189)	Acc@5 97.656 (98.996)
Epoch: [25][256/391]	Time 0.454 (0.452)	Data 0.002 (0.003)	Loss 0.8396 (0.8281)	Acc@1 84.375 (81.326)	Acc@5 100.000 (98.997)
Epoch: [25][320/391]	Time 0.429 (0.453)	Data 0.001 (0.003)	Loss 0.9227 (0.8301)	Acc@1 78.906 (81.313)	Acc@5 97.656 (98.983)
Epoch: [25][384/391]	Time 0.457 (0.452)	Data 0.002 (0.003)	Loss 1.0293 (0.8323)	Acc@1 78.125 (81.207)	Acc@5 96.094 (98.969)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.554 (0.554)	Data 0.215 (0.215)	Loss 3.2479 (3.2479)	Acc@1 7.031 (7.031)	Acc@5 58.594 (58.594)
Epoch: [1][64/391]	Time 0.441 (0.455)	Data 0.002 (0.005)	Loss 2.5560 (2.7972)	Acc@1 28.125 (21.755)	Acc@5 78.125 (73.690)
Epoch: [1][128/391]	Time 0.499 (0.453)	Data 0.002 (0.004)	Loss 2.4994 (2.6254)	Acc@1 32.812 (26.096)	Acc@5 85.156 (79.282)
Epoch: [1][192/391]	Time 0.440 (0.456)	Data 0.001 (0.003)	Loss 2.2206 (2.5159)	Acc@1 39.062 (29.177)	Acc@5 89.062 (82.242)
Epoch: [1][256/391]	Time 0.477 (0.458)	Data 0.002 (0.003)	Loss 2.1771 (2.4374)	Acc@1 32.031 (31.490)	Acc@5 91.406 (84.132)
Epoch: [1][320/391]	Time 0.459 (0.458)	Data 0.001 (0.003)	Loss 2.1155 (2.3655)	Acc@1 43.750 (33.832)	Acc@5 86.719 (85.538)
Epoch: [1][384/391]	Time 0.462 (0.459)	Data 0.002 (0.002)	Loss 2.0432 (2.3061)	Acc@1 44.531 (35.710)	Acc@5 94.531 (86.658)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.469 (0.469)	Data 0.257 (0.257)	Loss 2.0361 (2.0361)	Acc@1 43.750 (43.750)	Acc@5 93.750 (93.750)
Epoch: [2][64/391]	Time 0.505 (0.459)	Data 0.002 (0.006)	Loss 1.8377 (1.8872)	Acc@1 50.781 (49.207)	Acc@5 93.750 (93.161)
Epoch: [2][128/391]	Time 0.481 (0.461)	Data 0.002 (0.004)	Loss 1.7094 (1.8296)	Acc@1 53.906 (50.921)	Acc@5 96.094 (93.786)
Epoch: [2][192/391]	Time 0.462 (0.462)	Data 0.002 (0.003)	Loss 1.7235 (1.7793)	Acc@1 53.125 (52.886)	Acc@5 94.531 (94.066)
Epoch: [2][256/391]	Time 0.494 (0.463)	Data 0.002 (0.003)	Loss 1.6457 (1.7374)	Acc@1 60.938 (54.277)	Acc@5 95.312 (94.473)
Epoch: [2][320/391]	Time 0.468 (0.462)	Data 0.002 (0.003)	Loss 1.6471 (1.6994)	Acc@1 54.688 (55.381)	Acc@5 92.969 (94.835)
Epoch: [2][384/391]	Time 0.451 (0.463)	Data 0.002 (0.003)	Loss 1.4361 (1.6602)	Acc@1 63.281 (56.678)	Acc@5 96.094 (95.057)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.486 (0.486)	Data 0.231 (0.231)	Loss 1.3589 (1.3589)	Acc@1 64.844 (64.844)	Acc@5 95.312 (95.312)
Epoch: [3][64/391]	Time 0.467 (0.462)	Data 0.002 (0.005)	Loss 1.4787 (1.3915)	Acc@1 63.281 (64.976)	Acc@5 95.312 (96.671)
Epoch: [3][128/391]	Time 0.486 (0.469)	Data 0.001 (0.004)	Loss 1.1966 (1.3647)	Acc@1 71.094 (65.673)	Acc@5 100.000 (96.814)
Epoch: [3][192/391]	Time 0.475 (0.467)	Data 0.002 (0.003)	Loss 1.4282 (1.3494)	Acc@1 63.281 (66.018)	Acc@5 98.438 (96.988)
Epoch: [3][256/391]	Time 0.418 (0.466)	Data 0.002 (0.003)	Loss 1.3387 (1.3300)	Acc@1 69.531 (66.698)	Acc@5 96.875 (97.067)
Epoch: [3][320/391]	Time 0.439 (0.466)	Data 0.002 (0.003)	Loss 1.1637 (1.3098)	Acc@1 68.750 (67.258)	Acc@5 96.875 (97.155)
Epoch: [3][384/391]	Time 0.483 (0.466)	Data 0.002 (0.003)	Loss 1.1071 (1.2893)	Acc@1 71.094 (67.969)	Acc@5 99.219 (97.250)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.543 (0.543)	Data 0.219 (0.219)	Loss 1.1473 (1.1473)	Acc@1 70.312 (70.312)	Acc@5 99.219 (99.219)
Epoch: [4][64/391]	Time 0.454 (0.473)	Data 0.002 (0.005)	Loss 1.1350 (1.1647)	Acc@1 71.875 (71.623)	Acc@5 98.438 (97.704)
Epoch: [4][128/391]	Time 0.462 (0.470)	Data 0.002 (0.004)	Loss 1.2074 (1.1619)	Acc@1 70.312 (71.421)	Acc@5 100.000 (97.711)
Epoch: [4][192/391]	Time 0.451 (0.468)	Data 0.001 (0.003)	Loss 1.0341 (1.1444)	Acc@1 73.438 (71.984)	Acc@5 98.438 (97.871)
Epoch: [4][256/391]	Time 0.515 (0.467)	Data 0.002 (0.003)	Loss 1.1199 (1.1348)	Acc@1 75.781 (72.291)	Acc@5 97.656 (97.872)
Epoch: [4][320/391]	Time 0.468 (0.465)	Data 0.002 (0.003)	Loss 1.3673 (1.1260)	Acc@1 64.062 (72.605)	Acc@5 97.656 (97.902)
Epoch: [4][384/391]	Time 0.417 (0.464)	Data 0.001 (0.002)	Loss 0.9360 (1.1162)	Acc@1 84.375 (72.869)	Acc@5 98.438 (97.961)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.502 (0.502)	Data 0.287 (0.287)	Loss 1.0351 (1.0351)	Acc@1 77.344 (77.344)	Acc@5 97.656 (97.656)
Epoch: [5][64/391]	Time 0.483 (0.466)	Data 0.002 (0.006)	Loss 1.2829 (1.0360)	Acc@1 68.750 (75.252)	Acc@5 97.656 (98.413)
Epoch: [5][128/391]	Time 0.508 (0.465)	Data 0.002 (0.004)	Loss 1.0603 (1.0377)	Acc@1 75.781 (74.897)	Acc@5 98.438 (98.310)
Epoch: [5][192/391]	Time 0.449 (0.466)	Data 0.002 (0.003)	Loss 0.9361 (1.0269)	Acc@1 78.125 (75.279)	Acc@5 99.219 (98.361)
Epoch: [5][256/391]	Time 0.523 (0.466)	Data 0.002 (0.003)	Loss 0.8656 (1.0244)	Acc@1 82.031 (75.283)	Acc@5 99.219 (98.389)
Epoch: [5][320/391]	Time 0.442 (0.465)	Data 0.002 (0.003)	Loss 1.0989 (1.0224)	Acc@1 72.656 (75.338)	Acc@5 97.656 (98.360)
Epoch: [5][384/391]	Time 0.488 (0.464)	Data 0.002 (0.003)	Loss 1.0056 (1.0180)	Acc@1 75.000 (75.418)	Acc@5 98.438 (98.403)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.522 (0.522)	Data 0.242 (0.242)	Loss 0.9364 (0.9364)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [6][64/391]	Time 0.492 (0.465)	Data 0.002 (0.006)	Loss 1.0191 (0.9737)	Acc@1 78.125 (76.983)	Acc@5 99.219 (98.534)
Epoch: [6][128/391]	Time 0.455 (0.465)	Data 0.002 (0.004)	Loss 0.8814 (0.9817)	Acc@1 79.688 (76.696)	Acc@5 97.656 (98.492)
Epoch: [6][192/391]	Time 0.489 (0.464)	Data 0.002 (0.003)	Loss 0.9025 (0.9890)	Acc@1 81.250 (76.255)	Acc@5 99.219 (98.442)
Epoch: [6][256/391]	Time 0.499 (0.465)	Data 0.002 (0.003)	Loss 0.9969 (0.9855)	Acc@1 74.219 (76.353)	Acc@5 98.438 (98.480)
Epoch: [6][320/391]	Time 0.460 (0.465)	Data 0.002 (0.003)	Loss 0.8939 (0.9811)	Acc@1 79.688 (76.562)	Acc@5 99.219 (98.464)
Epoch: [6][384/391]	Time 0.466 (0.464)	Data 0.002 (0.003)	Loss 0.8907 (0.9789)	Acc@1 81.250 (76.629)	Acc@5 99.219 (98.517)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.505 (0.505)	Data 0.244 (0.244)	Loss 0.8973 (0.8973)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [7][64/391]	Time 0.463 (0.458)	Data 0.002 (0.006)	Loss 0.9331 (0.9478)	Acc@1 77.344 (77.500)	Acc@5 98.438 (98.654)
Epoch: [7][128/391]	Time 0.469 (0.459)	Data 0.002 (0.004)	Loss 0.9389 (0.9489)	Acc@1 77.344 (77.683)	Acc@5 97.656 (98.607)
Epoch: [7][192/391]	Time 0.491 (0.459)	Data 0.002 (0.003)	Loss 1.2118 (0.9472)	Acc@1 67.969 (77.740)	Acc@5 98.438 (98.616)
Epoch: [7][256/391]	Time 0.484 (0.463)	Data 0.002 (0.003)	Loss 0.8035 (0.9464)	Acc@1 81.250 (77.760)	Acc@5 98.438 (98.599)
Epoch: [7][320/391]	Time 0.473 (0.462)	Data 0.002 (0.003)	Loss 0.8923 (0.9474)	Acc@1 82.812 (77.650)	Acc@5 97.656 (98.625)
Epoch: [7][384/391]	Time 0.484 (0.462)	Data 0.002 (0.003)	Loss 0.8925 (0.9465)	Acc@1 80.469 (77.595)	Acc@5 99.219 (98.683)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.505 (0.505)	Data 0.322 (0.322)	Loss 0.8114 (0.8114)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [8][64/391]	Time 0.476 (0.461)	Data 0.002 (0.007)	Loss 0.9866 (0.9250)	Acc@1 76.562 (78.462)	Acc@5 96.875 (98.582)
Epoch: [8][128/391]	Time 0.466 (0.460)	Data 0.002 (0.004)	Loss 1.0036 (0.9248)	Acc@1 73.438 (78.180)	Acc@5 100.000 (98.662)
Epoch: [8][192/391]	Time 0.450 (0.461)	Data 0.002 (0.004)	Loss 0.8444 (0.9279)	Acc@1 79.688 (77.890)	Acc@5 96.875 (98.668)
Epoch: [8][256/391]	Time 0.466 (0.461)	Data 0.003 (0.003)	Loss 0.9451 (0.9245)	Acc@1 75.000 (78.049)	Acc@5 99.219 (98.629)
Epoch: [8][320/391]	Time 0.446 (0.462)	Data 0.002 (0.003)	Loss 0.9234 (0.9216)	Acc@1 75.000 (78.183)	Acc@5 98.438 (98.686)
Epoch: [8][384/391]	Time 0.437 (0.462)	Data 0.002 (0.003)	Loss 0.9364 (0.9192)	Acc@1 79.688 (78.202)	Acc@5 98.438 (98.699)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.531 (0.531)	Data 0.277 (0.277)	Loss 1.0535 (1.0535)	Acc@1 69.531 (69.531)	Acc@5 99.219 (99.219)
Epoch: [9][64/391]	Time 0.434 (0.463)	Data 0.002 (0.006)	Loss 0.8755 (0.9212)	Acc@1 79.688 (78.029)	Acc@5 97.656 (98.906)
Epoch: [9][128/391]	Time 0.486 (0.463)	Data 0.002 (0.004)	Loss 1.0673 (0.9185)	Acc@1 70.312 (78.131)	Acc@5 99.219 (98.771)
Epoch: [9][192/391]	Time 0.451 (0.462)	Data 0.002 (0.003)	Loss 0.8734 (0.9138)	Acc@1 78.906 (78.425)	Acc@5 100.000 (98.810)
Epoch: [9][256/391]	Time 0.453 (0.461)	Data 0.002 (0.003)	Loss 0.9141 (0.9145)	Acc@1 73.438 (78.450)	Acc@5 100.000 (98.741)
Epoch: [9][320/391]	Time 0.443 (0.460)	Data 0.002 (0.003)	Loss 0.8698 (0.9134)	Acc@1 78.906 (78.561)	Acc@5 100.000 (98.727)
Epoch: [9][384/391]	Time 0.472 (0.460)	Data 0.002 (0.003)	Loss 1.0529 (0.9106)	Acc@1 75.000 (78.671)	Acc@5 99.219 (98.738)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.514 (0.514)	Data 0.228 (0.228)	Loss 0.9621 (0.9621)	Acc@1 78.125 (78.125)	Acc@5 98.438 (98.438)
Epoch: [10][64/391]	Time 0.469 (0.466)	Data 0.002 (0.006)	Loss 0.8787 (0.8961)	Acc@1 78.906 (78.918)	Acc@5 98.438 (99.002)
Epoch: [10][128/391]	Time 0.532 (0.463)	Data 0.002 (0.004)	Loss 0.9571 (0.9053)	Acc@1 72.656 (79.130)	Acc@5 100.000 (98.849)
Epoch: [10][192/391]	Time 0.447 (0.463)	Data 0.002 (0.003)	Loss 0.8976 (0.9006)	Acc@1 78.906 (79.279)	Acc@5 98.438 (98.834)
Epoch: [10][256/391]	Time 0.480 (0.464)	Data 0.002 (0.003)	Loss 0.8649 (0.8980)	Acc@1 78.125 (79.204)	Acc@5 99.219 (98.860)
Epoch: [10][320/391]	Time 0.494 (0.464)	Data 0.002 (0.003)	Loss 0.8607 (0.8961)	Acc@1 82.031 (79.286)	Acc@5 99.219 (98.866)
Epoch: [10][384/391]	Time 0.454 (0.464)	Data 0.002 (0.003)	Loss 0.7946 (0.8953)	Acc@1 81.250 (79.229)	Acc@5 98.438 (98.874)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 440072 ; 487386 ; 0.9029229399285166

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.495 (0.495)	Data 0.199 (0.199)	Loss 0.8096 (0.8096)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [11][64/391]	Time 0.461 (0.462)	Data 0.002 (0.005)	Loss 0.8924 (0.8915)	Acc@1 78.125 (79.675)	Acc@5 98.438 (98.714)
Epoch: [11][128/391]	Time 0.489 (0.460)	Data 0.002 (0.004)	Loss 0.7089 (0.8814)	Acc@1 85.938 (79.863)	Acc@5 99.219 (98.783)
Epoch: [11][192/391]	Time 0.459 (0.461)	Data 0.002 (0.003)	Loss 0.9499 (0.8810)	Acc@1 75.000 (79.671)	Acc@5 99.219 (98.834)
Epoch: [11][256/391]	Time 0.479 (0.462)	Data 0.002 (0.003)	Loss 0.8300 (0.8796)	Acc@1 81.250 (79.718)	Acc@5 100.000 (98.860)
Epoch: [11][320/391]	Time 0.455 (0.461)	Data 0.002 (0.003)	Loss 0.7960 (0.8802)	Acc@1 82.031 (79.722)	Acc@5 100.000 (98.856)
Epoch: [11][384/391]	Time 0.445 (0.461)	Data 0.002 (0.002)	Loss 0.8316 (0.8820)	Acc@1 82.031 (79.681)	Acc@5 98.438 (98.835)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.500 (0.500)	Data 0.242 (0.242)	Loss 0.9338 (0.9338)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [12][64/391]	Time 0.463 (0.464)	Data 0.002 (0.006)	Loss 0.9882 (0.8992)	Acc@1 75.000 (79.567)	Acc@5 99.219 (98.750)
Epoch: [12][128/391]	Time 0.553 (0.462)	Data 0.002 (0.004)	Loss 0.8318 (0.8785)	Acc@1 83.594 (80.051)	Acc@5 99.219 (98.964)
Epoch: [12][192/391]	Time 0.438 (0.461)	Data 0.002 (0.003)	Loss 1.0141 (0.8787)	Acc@1 77.344 (80.068)	Acc@5 99.219 (98.903)
Epoch: [12][256/391]	Time 0.459 (0.464)	Data 0.002 (0.003)	Loss 0.9555 (0.8794)	Acc@1 78.125 (79.861)	Acc@5 96.875 (98.869)
Epoch: [12][320/391]	Time 0.461 (0.463)	Data 0.002 (0.003)	Loss 0.8792 (0.8824)	Acc@1 79.688 (79.785)	Acc@5 99.219 (98.861)
Epoch: [12][384/391]	Time 0.464 (0.463)	Data 0.001 (0.003)	Loss 1.0432 (0.8808)	Acc@1 75.781 (79.815)	Acc@5 96.875 (98.825)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.463 (0.463)	Data 0.231 (0.231)	Loss 0.9413 (0.9413)	Acc@1 78.125 (78.125)	Acc@5 98.438 (98.438)
Epoch: [13][64/391]	Time 0.438 (0.458)	Data 0.002 (0.006)	Loss 0.8735 (0.8633)	Acc@1 78.125 (80.397)	Acc@5 99.219 (98.906)
Epoch: [13][128/391]	Time 0.461 (0.458)	Data 0.002 (0.004)	Loss 0.8898 (0.8580)	Acc@1 78.906 (80.372)	Acc@5 97.656 (98.946)
Epoch: [13][192/391]	Time 0.418 (0.460)	Data 0.002 (0.003)	Loss 0.8326 (0.8611)	Acc@1 81.250 (80.198)	Acc@5 99.219 (98.907)
Epoch: [13][256/391]	Time 0.438 (0.462)	Data 0.002 (0.003)	Loss 0.9945 (0.8628)	Acc@1 75.781 (80.223)	Acc@5 98.438 (98.906)
Epoch: [13][320/391]	Time 0.453 (0.461)	Data 0.003 (0.003)	Loss 0.8924 (0.8656)	Acc@1 81.250 (80.077)	Acc@5 100.000 (98.883)
Epoch: [13][384/391]	Time 0.438 (0.461)	Data 0.002 (0.003)	Loss 0.7975 (0.8699)	Acc@1 84.375 (79.988)	Acc@5 99.219 (98.888)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.492 (0.492)	Data 0.234 (0.234)	Loss 0.8581 (0.8581)	Acc@1 85.938 (85.938)	Acc@5 98.438 (98.438)
Epoch: [14][64/391]	Time 0.437 (0.455)	Data 0.002 (0.006)	Loss 0.7390 (0.8310)	Acc@1 84.375 (81.346)	Acc@5 97.656 (98.966)
Epoch: [14][128/391]	Time 0.482 (0.456)	Data 0.002 (0.004)	Loss 0.9212 (0.8468)	Acc@1 75.000 (80.711)	Acc@5 99.219 (98.970)
Epoch: [14][192/391]	Time 0.450 (0.457)	Data 0.002 (0.003)	Loss 0.7852 (0.8458)	Acc@1 79.688 (80.861)	Acc@5 99.219 (98.964)
Epoch: [14][256/391]	Time 0.454 (0.457)	Data 0.002 (0.003)	Loss 0.9296 (0.8498)	Acc@1 76.562 (80.572)	Acc@5 99.219 (98.994)
Epoch: [14][320/391]	Time 0.406 (0.456)	Data 0.002 (0.003)	Loss 0.9130 (0.8562)	Acc@1 80.469 (80.440)	Acc@5 96.094 (98.888)
Epoch: [14][384/391]	Time 0.411 (0.456)	Data 0.002 (0.003)	Loss 0.8199 (0.8593)	Acc@1 83.594 (80.339)	Acc@5 96.875 (98.876)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.477 (0.477)	Data 0.210 (0.210)	Loss 0.8901 (0.8901)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [15][64/391]	Time 0.470 (0.453)	Data 0.002 (0.005)	Loss 0.8348 (0.8397)	Acc@1 82.031 (80.805)	Acc@5 98.438 (99.038)
Epoch: [15][128/391]	Time 0.502 (0.455)	Data 0.002 (0.004)	Loss 0.8258 (0.8409)	Acc@1 81.250 (81.093)	Acc@5 100.000 (98.970)
Epoch: [15][192/391]	Time 0.434 (0.456)	Data 0.002 (0.003)	Loss 0.8557 (0.8486)	Acc@1 82.812 (80.845)	Acc@5 99.219 (98.960)
Epoch: [15][256/391]	Time 0.485 (0.456)	Data 0.002 (0.003)	Loss 0.8562 (0.8516)	Acc@1 80.469 (80.663)	Acc@5 98.438 (98.939)
Epoch: [15][320/391]	Time 0.456 (0.456)	Data 0.002 (0.003)	Loss 0.7793 (0.8484)	Acc@1 82.812 (80.649)	Acc@5 99.219 (98.958)
Epoch: [15][384/391]	Time 0.450 (0.457)	Data 0.002 (0.003)	Loss 0.7720 (0.8534)	Acc@1 84.375 (80.548)	Acc@5 100.000 (98.947)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 367038 ; 487386 ; 0.7530745651290763

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.463 (0.463)	Data 0.217 (0.217)	Loss 0.9968 (0.9968)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [16][64/391]	Time 0.463 (0.456)	Data 0.002 (0.005)	Loss 0.8880 (0.8269)	Acc@1 78.125 (81.142)	Acc@5 96.875 (99.050)
Epoch: [16][128/391]	Time 0.422 (0.456)	Data 0.001 (0.004)	Loss 0.7671 (0.8326)	Acc@1 84.375 (81.183)	Acc@5 98.438 (99.037)
Epoch: [16][192/391]	Time 0.510 (0.456)	Data 0.002 (0.003)	Loss 0.8853 (0.8449)	Acc@1 78.125 (80.865)	Acc@5 99.219 (98.956)
Epoch: [16][256/391]	Time 0.481 (0.457)	Data 0.002 (0.003)	Loss 0.8057 (0.8421)	Acc@1 82.031 (80.992)	Acc@5 97.656 (98.976)
Epoch: [16][320/391]	Time 0.470 (0.457)	Data 0.002 (0.003)	Loss 0.7919 (0.8443)	Acc@1 83.594 (80.909)	Acc@5 100.000 (98.958)
Epoch: [16][384/391]	Time 0.461 (0.456)	Data 0.001 (0.003)	Loss 0.7012 (0.8469)	Acc@1 85.156 (80.814)	Acc@5 99.219 (98.941)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.521 (0.521)	Data 0.291 (0.291)	Loss 0.8018 (0.8018)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [17][64/391]	Time 0.445 (0.458)	Data 0.002 (0.006)	Loss 0.8051 (0.8219)	Acc@1 78.906 (81.334)	Acc@5 100.000 (99.159)
Epoch: [17][128/391]	Time 0.384 (0.458)	Data 0.003 (0.004)	Loss 0.8562 (0.8323)	Acc@1 78.906 (81.062)	Acc@5 98.438 (99.037)
Epoch: [17][192/391]	Time 0.442 (0.456)	Data 0.001 (0.004)	Loss 0.8355 (0.8369)	Acc@1 81.250 (80.926)	Acc@5 98.438 (99.053)
Epoch: [17][256/391]	Time 0.467 (0.458)	Data 0.002 (0.003)	Loss 0.7993 (0.8403)	Acc@1 81.250 (80.858)	Acc@5 97.656 (98.969)
Epoch: [17][320/391]	Time 0.421 (0.456)	Data 0.002 (0.003)	Loss 0.7328 (0.8407)	Acc@1 83.594 (80.839)	Acc@5 100.000 (98.932)
Epoch: [17][384/391]	Time 0.442 (0.457)	Data 0.002 (0.003)	Loss 0.8287 (0.8435)	Acc@1 81.250 (80.728)	Acc@5 99.219 (98.886)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.498 (0.498)	Data 0.239 (0.239)	Loss 0.7859 (0.7859)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [18][64/391]	Time 0.454 (0.461)	Data 0.002 (0.006)	Loss 0.8766 (0.8372)	Acc@1 85.156 (80.938)	Acc@5 98.438 (98.714)
Epoch: [18][128/391]	Time 0.460 (0.457)	Data 0.001 (0.004)	Loss 0.9786 (0.8325)	Acc@1 76.562 (81.298)	Acc@5 97.656 (98.934)
Epoch: [18][192/391]	Time 0.443 (0.457)	Data 0.002 (0.003)	Loss 0.7147 (0.8315)	Acc@1 86.719 (81.266)	Acc@5 99.219 (98.915)
Epoch: [18][256/391]	Time 0.493 (0.455)	Data 0.002 (0.003)	Loss 0.7871 (0.8360)	Acc@1 82.031 (81.034)	Acc@5 98.438 (98.957)
Epoch: [18][320/391]	Time 0.475 (0.453)	Data 0.002 (0.003)	Loss 0.8960 (0.8383)	Acc@1 79.688 (80.900)	Acc@5 100.000 (98.966)
Epoch: [18][384/391]	Time 0.468 (0.454)	Data 0.002 (0.003)	Loss 0.7054 (0.8363)	Acc@1 86.719 (80.899)	Acc@5 96.875 (98.947)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.508 (0.508)	Data 0.222 (0.222)	Loss 0.8449 (0.8449)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [19][64/391]	Time 0.465 (0.464)	Data 0.002 (0.005)	Loss 0.7951 (0.8015)	Acc@1 78.906 (82.007)	Acc@5 99.219 (99.014)
Epoch: [19][128/391]	Time 0.494 (0.459)	Data 0.002 (0.004)	Loss 0.9698 (0.8109)	Acc@1 79.688 (81.716)	Acc@5 97.656 (99.007)
Epoch: [19][192/391]	Time 0.460 (0.458)	Data 0.002 (0.003)	Loss 0.7987 (0.8136)	Acc@1 78.125 (81.545)	Acc@5 100.000 (99.041)
Epoch: [19][256/391]	Time 0.441 (0.456)	Data 0.002 (0.003)	Loss 0.7116 (0.8140)	Acc@1 85.938 (81.472)	Acc@5 98.438 (99.058)
Epoch: [19][320/391]	Time 0.483 (0.457)	Data 0.002 (0.003)	Loss 0.8336 (0.8206)	Acc@1 82.031 (81.218)	Acc@5 98.438 (99.026)
Epoch: [19][384/391]	Time 0.458 (0.456)	Data 0.002 (0.003)	Loss 0.8054 (0.8205)	Acc@1 79.688 (81.169)	Acc@5 100.000 (99.006)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.484 (0.484)	Data 0.227 (0.227)	Loss 0.8250 (0.8250)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [20][64/391]	Time 0.475 (0.456)	Data 0.002 (0.005)	Loss 0.7850 (0.8022)	Acc@1 81.250 (81.791)	Acc@5 98.438 (99.026)
Epoch: [20][128/391]	Time 0.451 (0.459)	Data 0.002 (0.004)	Loss 0.7821 (0.8094)	Acc@1 85.938 (81.353)	Acc@5 100.000 (99.001)
Epoch: [20][192/391]	Time 0.462 (0.457)	Data 0.001 (0.003)	Loss 0.8105 (0.8077)	Acc@1 80.469 (81.444)	Acc@5 98.438 (99.008)
Epoch: [20][256/391]	Time 0.495 (0.461)	Data 0.002 (0.003)	Loss 0.7549 (0.8129)	Acc@1 82.812 (81.210)	Acc@5 99.219 (98.985)
Epoch: [20][320/391]	Time 0.482 (0.460)	Data 0.002 (0.003)	Loss 0.8749 (0.8156)	Acc@1 75.781 (81.204)	Acc@5 100.000 (99.012)
Epoch: [20][384/391]	Time 0.428 (0.460)	Data 0.002 (0.003)	Loss 0.8816 (0.8151)	Acc@1 80.469 (81.291)	Acc@5 98.438 (99.000)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv21.weight

 RM:  module.conv22.weight

 RM:  module.conv30.weight

 RM:  module.conv31.weight

Module List Length:  68
Index1: 60
Index: 31
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 32; k: 32
Bool1: False
Bool2: True
j: 33; k: 32
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(41, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 58: Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 59: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 60: Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 61: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 62 gegen 66: Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 63 gegen 67: BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(41, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  64

Module List Length:  64
Index1: 58
Index: 30
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 31; k: 30
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(41, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 58 gegen 62: Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 59 gegen 63: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(41, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): AdaptiveAvgPool2d(output_size=(1, 1))
    (59): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  60

Module List Length:  60
Index1: 42
Index: 22
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 54 gegen 58: Conv2d(64, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 55 gegen 59: BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): AdaptiveAvgPool2d(output_size=(1, 1))
    (55): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  56

Module List Length:  56
Index1: 40
Index: 21
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 50 gegen 54: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 51 gegen 55: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  52
Count: 163382 ; 487386 ; 0.3352209542334002

Epoch: [21 | 180] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 318, in main
    use_gpu_num)
  File "main.py", line 442, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 459, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 107, in forward
    exponential_average_factor, self.eps)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 0
























 ab hier mit pruneTrain
