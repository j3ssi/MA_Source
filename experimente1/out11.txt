no display found. Using non-interactive Agg backend
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 0
total    : 11018
free     : 11008
used     : 10


Files already downloaded and verified
batch_size: 1017 ; 134.8243
Epoch: [1][0/50]	Time 1.144 (1.144)	Data 0.465 (0.465)	Loss 2.4563 (2.4563)	Acc@1 9.341 (9.341)	Acc@5 47.001 (47.001)
Epoch: [1][10/50]	Time 0.844 (0.880)	Data 0.006 (0.047)	Loss 2.2083 (2.3286)	Acc@1 19.272 (13.963)	Acc@5 66.863 (59.605)
Epoch: [1][20/50]	Time 0.853 (0.861)	Data 0.006 (0.027)	Loss 1.9595 (2.1865)	Acc@1 22.321 (18.270)	Acc@5 80.433 (67.730)
Epoch: [1][30/50]	Time 0.836 (0.856)	Data 0.004 (0.020)	Loss 1.8666 (2.1071)	Acc@1 30.777 (20.992)	Acc@5 82.793 (71.792)
Epoch: [1][40/50]	Time 0.849 (0.856)	Data 0.003 (0.016)	Loss 1.7767 (2.0384)	Acc@1 31.957 (23.086)	Acc@5 86.726 (74.919)
Epoch: [2][0/50]	Time 0.905 (0.905)	Data 0.417 (0.417)	Loss 1.7426 (1.7426)	Acc@1 32.842 (32.842)	Acc@5 86.922 (86.922)
Epoch: [2][10/50]	Time 0.856 (0.867)	Data 0.004 (0.042)	Loss 1.7279 (1.7177)	Acc@1 33.628 (34.576)	Acc@5 87.414 (87.691)
Epoch: [2][20/50]	Time 0.893 (0.868)	Data 0.006 (0.025)	Loss 1.7124 (1.7086)	Acc@1 34.317 (34.930)	Acc@5 88.496 (87.793)
Epoch: [2][30/50]	Time 0.880 (0.869)	Data 0.005 (0.019)	Loss 1.5794 (1.6872)	Acc@1 39.626 (35.725)	Acc@5 90.462 (88.175)
Epoch: [2][40/50]	Time 0.944 (0.872)	Data 0.003 (0.015)	Loss 1.6412 (1.6739)	Acc@1 38.151 (36.166)	Acc@5 88.299 (88.428)
Epoch: [3][0/50]	Time 0.952 (0.952)	Data 0.446 (0.446)	Loss 1.5484 (1.5484)	Acc@1 42.281 (42.281)	Acc@5 90.659 (90.659)
Epoch: [3][10/50]	Time 0.881 (0.895)	Data 0.005 (0.044)	Loss 1.4592 (1.5362)	Acc@1 43.756 (41.369)	Acc@5 92.822 (90.927)
Epoch: [3][20/50]	Time 0.912 (0.898)	Data 0.006 (0.026)	Loss 1.4558 (1.5136)	Acc@1 45.919 (43.176)	Acc@5 91.544 (91.150)
Epoch: [3][30/50]	Time 0.893 (0.897)	Data 0.006 (0.019)	Loss 1.4005 (1.4866)	Acc@1 48.279 (44.384)	Acc@5 93.805 (91.766)
Epoch: [3][40/50]	Time 0.906 (0.899)	Data 0.005 (0.016)	Loss 1.3202 (1.4584)	Acc@1 51.229 (45.586)	Acc@5 93.707 (92.155)
Epoch: [4][0/50]	Time 0.893 (0.893)	Data 0.388 (0.388)	Loss 1.3713 (1.3713)	Acc@1 47.886 (47.886)	Acc@5 94.395 (94.395)
Epoch: [4][10/50]	Time 0.904 (0.914)	Data 0.003 (0.039)	Loss 1.2945 (1.2854)	Acc@1 51.524 (52.972)	Acc@5 94.297 (94.628)
Epoch: [4][20/50]	Time 0.912 (0.912)	Data 0.003 (0.023)	Loss 1.2296 (1.2733)	Acc@1 54.081 (53.561)	Acc@5 95.575 (94.615)
Epoch: [4][30/50]	Time 0.900 (0.916)	Data 0.006 (0.017)	Loss 1.1862 (1.2480)	Acc@1 56.146 (54.433)	Acc@5 96.264 (94.919)
Epoch: [4][40/50]	Time 0.899 (0.913)	Data 0.005 (0.014)	Loss 1.1337 (1.2295)	Acc@1 60.669 (55.191)	Acc@5 96.362 (95.122)
Epoch: [5][0/50]	Time 0.895 (0.895)	Data 0.427 (0.427)	Loss 1.2287 (1.2287)	Acc@1 54.277 (54.277)	Acc@5 95.870 (95.870)
Epoch: [5][10/50]	Time 0.924 (0.906)	Data 0.006 (0.043)	Loss 1.1184 (1.1571)	Acc@1 60.374 (58.184)	Acc@5 95.870 (95.700)
Epoch: [5][20/50]	Time 0.931 (0.910)	Data 0.007 (0.025)	Loss 1.0522 (1.1132)	Acc@1 63.225 (59.737)	Acc@5 95.969 (96.001)
Epoch: [5][30/50]	Time 0.913 (0.908)	Data 0.004 (0.019)	Loss 1.0554 (1.0969)	Acc@1 61.947 (60.418)	Acc@5 96.559 (96.035)
Epoch: [5][40/50]	Time 0.910 (0.910)	Data 0.006 (0.015)	Loss 1.0387 (1.0818)	Acc@1 61.160 (60.913)	Acc@5 96.952 (96.180)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 11 ; 1
I: 20 ; 2

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight', 'module.conv11.weight'], ['module.conv14.weight', 'module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.conv26.weight', 'module.fc29.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight', 'module.conv10.weight'], ['module.conv13.weight', 'module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight', 'module.conv28.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight'], ['module.conv26.weight', 'module.conv27.weight', 'module.conv28.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): AdaptiveAvgPool2d(output_size=(1, 1))
    (57): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Epoch: [6][0/50]	Time 0.895 (0.895)	Data 0.383 (0.383)	Loss 1.0288 (1.0288)	Acc@1 63.520 (63.520)	Acc@5 95.280 (95.280)
Epoch: [6][10/50]	Time 0.888 (0.912)	Data 0.005 (0.039)	Loss 0.9774 (0.9987)	Acc@1 64.405 (64.405)	Acc@5 96.952 (96.916)
Epoch: [6][20/50]	Time 0.894 (0.908)	Data 0.006 (0.023)	Loss 0.9627 (0.9800)	Acc@1 64.503 (64.756)	Acc@5 96.264 (97.102)
Epoch: [6][30/50]	Time 0.910 (0.906)	Data 0.005 (0.017)	Loss 0.9367 (0.9672)	Acc@1 65.782 (65.163)	Acc@5 97.837 (97.193)
Epoch: [6][40/50]	Time 0.904 (0.905)	Data 0.006 (0.014)	Loss 0.9790 (0.9691)	Acc@1 64.307 (65.007)	Acc@5 96.853 (97.180)
Epoch: [7][0/50]	Time 0.898 (0.898)	Data 0.427 (0.427)	Loss 0.9594 (0.9594)	Acc@1 64.602 (64.602)	Acc@5 97.640 (97.640)
Epoch: [7][10/50]	Time 0.903 (0.902)	Data 0.003 (0.043)	Loss 0.9479 (0.9517)	Acc@1 64.307 (65.335)	Acc@5 98.230 (97.435)
Epoch: [7][20/50]	Time 0.912 (0.904)	Data 0.006 (0.025)	Loss 0.9248 (0.9224)	Acc@1 66.077 (66.728)	Acc@5 97.148 (97.561)
Epoch: [7][30/50]	Time 0.899 (0.904)	Data 0.003 (0.019)	Loss 0.9030 (0.9194)	Acc@1 66.077 (66.882)	Acc@5 97.345 (97.529)
Epoch: [7][40/50]	Time 0.875 (0.906)	Data 0.006 (0.015)	Loss 0.8504 (0.9131)	Acc@1 69.125 (67.041)	Acc@5 98.132 (97.539)
Epoch: [8][0/50]	Time 0.909 (0.909)	Data 0.437 (0.437)	Loss 0.8989 (0.8989)	Acc@1 68.535 (68.535)	Acc@5 98.132 (98.132)
Epoch: [8][10/50]	Time 0.922 (0.903)	Data 0.003 (0.044)	Loss 0.8978 (0.8832)	Acc@1 66.765 (68.562)	Acc@5 97.640 (97.595)
Epoch: [8][20/50]	Time 0.903 (0.899)	Data 0.004 (0.026)	Loss 0.8009 (0.8662)	Acc@1 70.895 (69.116)	Acc@5 98.525 (97.659)
Epoch: [8][30/50]	Time 0.916 (0.898)	Data 0.006 (0.019)	Loss 0.7887 (0.8581)	Acc@1 73.156 (69.401)	Acc@5 98.132 (97.716)
Epoch: [8][40/50]	Time 0.913 (0.898)	Data 0.006 (0.016)	Loss 0.7903 (0.8459)	Acc@1 70.501 (69.852)	Acc@5 98.623 (97.777)
Epoch: [9][0/50]	Time 0.991 (0.991)	Data 0.546 (0.546)	Loss 0.8716 (0.8716)	Acc@1 70.206 (70.206)	Acc@5 97.345 (97.345)
Epoch: [9][10/50]	Time 0.902 (0.917)	Data 0.006 (0.055)	Loss 0.8221 (0.8437)	Acc@1 70.010 (69.920)	Acc@5 97.738 (97.765)
Epoch: [9][20/50]	Time 0.897 (0.917)	Data 0.004 (0.031)	Loss 0.8197 (0.8331)	Acc@1 71.190 (70.314)	Acc@5 98.525 (97.888)
Epoch: [9][30/50]	Time 0.934 (0.911)	Data 0.004 (0.023)	Loss 0.7722 (0.8159)	Acc@1 71.878 (70.961)	Acc@5 98.033 (98.046)
Epoch: [9][40/50]	Time 0.902 (0.907)	Data 0.005 (0.018)	Loss 0.8134 (0.8040)	Acc@1 70.600 (71.348)	Acc@5 97.935 (98.062)
Epoch: [10][0/50]	Time 0.951 (0.951)	Data 0.388 (0.388)	Loss 0.9295 (0.9295)	Acc@1 66.175 (66.175)	Acc@5 98.132 (98.132)
Epoch: [10][10/50]	Time 0.878 (0.899)	Data 0.006 (0.040)	Loss 0.7845 (0.7893)	Acc@1 71.681 (72.137)	Acc@5 98.132 (98.176)
Epoch: [10][20/50]	Time 0.891 (0.897)	Data 0.004 (0.023)	Loss 0.7775 (0.7835)	Acc@1 73.451 (72.379)	Acc@5 97.542 (98.272)
Epoch: [10][30/50]	Time 0.881 (0.897)	Data 0.006 (0.017)	Loss 0.7102 (0.7657)	Acc@1 74.533 (73.061)	Acc@5 98.722 (98.354)
Epoch: [10][40/50]	Time 0.892 (0.900)	Data 0.003 (0.014)	Loss 0.7297 (0.7593)	Acc@1 74.336 (73.161)	Acc@5 98.722 (98.417)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 11 ; 1
I: 20 ; 2

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight', 'module.conv11.weight'], ['module.conv14.weight', 'module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.conv26.weight', 'module.fc29.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight', 'module.conv10.weight'], ['module.conv13.weight', 'module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight', 'module.conv28.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight'], ['module.conv26.weight', 'module.conv27.weight', 'module.conv28.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(61, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(52, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): AdaptiveAvgPool2d(output_size=(1, 1))
    (57): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 376506 ; 415546 ; 0.9060513156184875
new batch_size: 1124
Warning: Traceback of forward call that caused the error:
  File "main.py", line 712, in <module>
    main()
  File "main.py", line 483, in main
    use_gpu_num)
  File "main.py", line 580, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 244, in forward
    _x = self.relu(_x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/activation.py", line 94, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/functional.py", line 912, in relu
    result = torch.relu_(input)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:57)
Traceback (most recent call last):
  File "main.py", line 712, in <module>
    main()
  File "main.py", line 483, in main
    use_gpu_num)
  File "main.py", line 628, in train
    loss.backward()
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 282.00 MiB (GPU 0; 10.76 GiB total capacity; 8.77 GiB already allocated; 228.25 MiB free; 9.68 GiB reserved in total by PyTorch)
