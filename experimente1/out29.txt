no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.338 (0.338)	Data 0.136 (0.136)	Loss 3.5653 (3.5653)	Acc@1 10.156 (10.156)	Acc@5 56.250 (56.250)
Epoch: [1][64/391]	Time 0.251 (0.235)	Data 0.002 (0.004)	Loss 2.6151 (2.8518)	Acc@1 28.906 (20.096)	Acc@5 75.000 (73.365)
Epoch: [1][128/391]	Time 0.353 (0.276)	Data 0.002 (0.003)	Loss 2.4557 (2.6724)	Acc@1 27.344 (23.771)	Acc@5 82.812 (78.591)
Epoch: [1][192/391]	Time 0.531 (0.331)	Data 0.002 (0.003)	Loss 2.1827 (2.5576)	Acc@1 32.031 (26.146)	Acc@5 92.188 (81.416)
Epoch: [1][256/391]	Time 0.532 (0.378)	Data 0.001 (0.002)	Loss 2.3267 (2.4775)	Acc@1 28.906 (28.438)	Acc@5 87.500 (83.229)
Epoch: [1][320/391]	Time 0.560 (0.408)	Data 0.002 (0.002)	Loss 2.2892 (2.4074)	Acc@1 32.812 (30.607)	Acc@5 89.062 (84.528)
Epoch: [1][384/391]	Time 0.536 (0.428)	Data 0.002 (0.002)	Loss 1.8440 (2.3432)	Acc@1 50.781 (32.709)	Acc@5 95.312 (85.674)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.588 (0.588)	Data 0.260 (0.260)	Loss 2.0161 (2.0161)	Acc@1 42.188 (42.188)	Acc@5 92.969 (92.969)
Epoch: [2][64/391]	Time 0.499 (0.527)	Data 0.002 (0.006)	Loss 2.1395 (1.9156)	Acc@1 41.406 (46.442)	Acc@5 88.281 (92.608)
Epoch: [2][128/391]	Time 0.510 (0.526)	Data 0.002 (0.004)	Loss 1.7399 (1.8547)	Acc@1 50.000 (49.019)	Acc@5 92.969 (93.011)
Epoch: [2][192/391]	Time 0.536 (0.517)	Data 0.002 (0.003)	Loss 1.6462 (1.8135)	Acc@1 53.906 (50.623)	Acc@5 97.656 (93.560)
Epoch: [2][256/391]	Time 0.494 (0.520)	Data 0.001 (0.003)	Loss 1.7086 (1.7839)	Acc@1 55.469 (51.781)	Acc@5 95.312 (93.923)
Epoch: [2][320/391]	Time 0.516 (0.522)	Data 0.002 (0.003)	Loss 1.4600 (1.7485)	Acc@1 60.938 (53.076)	Acc@5 97.656 (94.222)
Epoch: [2][384/391]	Time 0.544 (0.524)	Data 0.002 (0.003)	Loss 1.5364 (1.7122)	Acc@1 61.719 (54.320)	Acc@5 96.094 (94.550)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.589 (0.589)	Data 0.225 (0.225)	Loss 1.5139 (1.5139)	Acc@1 62.500 (62.500)	Acc@5 96.875 (96.875)
Epoch: [3][64/391]	Time 0.508 (0.528)	Data 0.002 (0.005)	Loss 1.2791 (1.4742)	Acc@1 71.094 (62.476)	Acc@5 96.875 (96.250)
Epoch: [3][128/391]	Time 0.611 (0.526)	Data 0.002 (0.004)	Loss 1.3315 (1.4475)	Acc@1 69.531 (63.354)	Acc@5 96.875 (96.378)
Epoch: [3][192/391]	Time 0.549 (0.519)	Data 0.002 (0.003)	Loss 1.3538 (1.4189)	Acc@1 67.188 (64.115)	Acc@5 97.656 (96.632)
Epoch: [3][256/391]	Time 0.532 (0.523)	Data 0.002 (0.003)	Loss 1.2149 (1.3960)	Acc@1 69.531 (64.670)	Acc@5 98.438 (96.778)
Epoch: [3][320/391]	Time 0.562 (0.524)	Data 0.002 (0.003)	Loss 1.1984 (1.3746)	Acc@1 68.750 (65.282)	Acc@5 96.875 (96.904)
Epoch: [3][384/391]	Time 0.528 (0.525)	Data 0.002 (0.003)	Loss 1.2232 (1.3576)	Acc@1 68.750 (65.731)	Acc@5 98.438 (96.997)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.525 (0.525)	Data 0.241 (0.241)	Loss 1.1543 (1.1543)	Acc@1 71.875 (71.875)	Acc@5 98.438 (98.438)
Epoch: [4][64/391]	Time 0.535 (0.534)	Data 0.002 (0.006)	Loss 1.0235 (1.1935)	Acc@1 71.875 (70.361)	Acc@5 97.656 (97.849)
Epoch: [4][128/391]	Time 0.576 (0.533)	Data 0.002 (0.004)	Loss 1.2148 (1.1894)	Acc@1 67.969 (70.040)	Acc@5 98.438 (97.947)
Epoch: [4][192/391]	Time 0.465 (0.521)	Data 0.001 (0.003)	Loss 1.0994 (1.1855)	Acc@1 71.875 (70.507)	Acc@5 97.656 (97.847)
Epoch: [4][256/391]	Time 0.465 (0.506)	Data 0.002 (0.003)	Loss 1.0641 (1.1761)	Acc@1 72.656 (70.772)	Acc@5 98.438 (97.869)
Epoch: [4][320/391]	Time 0.444 (0.496)	Data 0.002 (0.003)	Loss 1.1276 (1.1624)	Acc@1 71.875 (71.123)	Acc@5 98.438 (97.946)
Epoch: [4][384/391]	Time 0.455 (0.491)	Data 0.002 (0.003)	Loss 1.0154 (1.1511)	Acc@1 78.125 (71.402)	Acc@5 98.438 (97.926)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.470 (0.470)	Data 0.183 (0.183)	Loss 1.1351 (1.1351)	Acc@1 71.875 (71.875)	Acc@5 99.219 (99.219)
Epoch: [5][64/391]	Time 0.455 (0.463)	Data 0.002 (0.005)	Loss 0.9544 (1.0755)	Acc@1 75.000 (74.002)	Acc@5 100.000 (98.245)
Epoch: [5][128/391]	Time 0.545 (0.464)	Data 0.002 (0.003)	Loss 0.9225 (1.0607)	Acc@1 77.344 (74.388)	Acc@5 99.219 (98.244)
Epoch: [5][192/391]	Time 0.424 (0.464)	Data 0.002 (0.003)	Loss 0.9791 (1.0527)	Acc@1 76.562 (74.506)	Acc@5 98.438 (98.304)
Epoch: [5][256/391]	Time 0.435 (0.463)	Data 0.002 (0.003)	Loss 0.9757 (1.0462)	Acc@1 80.469 (74.492)	Acc@5 99.219 (98.383)
Epoch: [5][320/391]	Time 0.470 (0.463)	Data 0.002 (0.003)	Loss 1.2268 (1.0448)	Acc@1 67.188 (74.506)	Acc@5 95.312 (98.362)
Epoch: [5][384/391]	Time 0.481 (0.463)	Data 0.002 (0.002)	Loss 1.1195 (1.0422)	Acc@1 70.312 (74.539)	Acc@5 98.438 (98.385)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.474 (0.474)	Data 0.208 (0.208)	Loss 1.0497 (1.0497)	Acc@1 69.531 (69.531)	Acc@5 99.219 (99.219)
Epoch: [6][64/391]	Time 0.467 (0.465)	Data 0.002 (0.005)	Loss 1.0524 (0.9864)	Acc@1 78.906 (76.406)	Acc@5 98.438 (98.558)
Epoch: [6][128/391]	Time 0.480 (0.466)	Data 0.002 (0.004)	Loss 1.0509 (0.9873)	Acc@1 71.875 (76.266)	Acc@5 99.219 (98.359)
Epoch: [6][192/391]	Time 0.461 (0.465)	Data 0.002 (0.003)	Loss 0.9666 (0.9837)	Acc@1 75.000 (76.348)	Acc@5 97.656 (98.397)
Epoch: [6][256/391]	Time 0.446 (0.464)	Data 0.002 (0.003)	Loss 0.9661 (0.9817)	Acc@1 75.000 (76.374)	Acc@5 98.438 (98.374)
Epoch: [6][320/391]	Time 0.459 (0.464)	Data 0.002 (0.003)	Loss 1.0837 (0.9856)	Acc@1 75.000 (76.317)	Acc@5 96.875 (98.401)
Epoch: [6][384/391]	Time 0.447 (0.464)	Data 0.002 (0.002)	Loss 0.8065 (0.9870)	Acc@1 81.250 (76.305)	Acc@5 100.000 (98.401)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.449 (0.449)	Data 0.225 (0.225)	Loss 0.9251 (0.9251)	Acc@1 77.344 (77.344)	Acc@5 100.000 (100.000)
Epoch: [7][64/391]	Time 0.396 (0.462)	Data 0.002 (0.005)	Loss 0.9015 (0.9665)	Acc@1 82.031 (76.815)	Acc@5 97.656 (98.389)
Epoch: [7][128/391]	Time 0.464 (0.460)	Data 0.001 (0.004)	Loss 0.9128 (0.9615)	Acc@1 78.906 (77.089)	Acc@5 96.875 (98.510)
Epoch: [7][192/391]	Time 0.477 (0.463)	Data 0.002 (0.003)	Loss 0.8918 (0.9735)	Acc@1 77.344 (76.607)	Acc@5 98.438 (98.454)
Epoch: [7][256/391]	Time 0.449 (0.464)	Data 0.001 (0.003)	Loss 1.0388 (0.9723)	Acc@1 75.000 (76.657)	Acc@5 100.000 (98.535)
Epoch: [7][320/391]	Time 0.466 (0.463)	Data 0.002 (0.003)	Loss 1.1313 (0.9706)	Acc@1 73.438 (76.711)	Acc@5 97.656 (98.559)
Epoch: [7][384/391]	Time 0.460 (0.464)	Data 0.002 (0.003)	Loss 0.9451 (0.9674)	Acc@1 78.906 (76.757)	Acc@5 99.219 (98.596)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.487 (0.487)	Data 0.224 (0.224)	Loss 1.0493 (1.0493)	Acc@1 75.000 (75.000)	Acc@5 97.656 (97.656)
Epoch: [8][64/391]	Time 0.462 (0.462)	Data 0.002 (0.006)	Loss 0.9539 (0.9181)	Acc@1 79.688 (79.111)	Acc@5 99.219 (98.894)
Epoch: [8][128/391]	Time 0.463 (0.467)	Data 0.003 (0.004)	Loss 0.8261 (0.9321)	Acc@1 82.031 (78.567)	Acc@5 99.219 (98.728)
Epoch: [8][192/391]	Time 0.463 (0.466)	Data 0.002 (0.003)	Loss 0.9832 (0.9414)	Acc@1 76.562 (78.040)	Acc@5 97.656 (98.693)
Epoch: [8][256/391]	Time 0.461 (0.465)	Data 0.002 (0.003)	Loss 0.9378 (0.9386)	Acc@1 74.219 (78.052)	Acc@5 99.219 (98.714)
Epoch: [8][320/391]	Time 0.490 (0.464)	Data 0.002 (0.003)	Loss 0.9256 (0.9354)	Acc@1 81.250 (78.147)	Acc@5 97.656 (98.688)
Epoch: [8][384/391]	Time 0.459 (0.463)	Data 0.002 (0.003)	Loss 0.9874 (0.9358)	Acc@1 76.562 (78.131)	Acc@5 99.219 (98.705)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.523 (0.523)	Data 0.298 (0.298)	Loss 1.0697 (1.0697)	Acc@1 72.656 (72.656)	Acc@5 99.219 (99.219)
Epoch: [9][64/391]	Time 0.536 (0.467)	Data 0.002 (0.006)	Loss 0.9203 (0.9139)	Acc@1 82.031 (78.570)	Acc@5 98.438 (98.702)
Epoch: [9][128/391]	Time 0.449 (0.466)	Data 0.002 (0.004)	Loss 0.8712 (0.9256)	Acc@1 81.250 (78.046)	Acc@5 100.000 (98.680)
Epoch: [9][192/391]	Time 0.473 (0.468)	Data 0.002 (0.004)	Loss 0.9521 (0.9254)	Acc@1 78.125 (78.056)	Acc@5 96.875 (98.672)
Epoch: [9][256/391]	Time 0.485 (0.466)	Data 0.002 (0.003)	Loss 1.0977 (0.9244)	Acc@1 76.562 (78.155)	Acc@5 96.875 (98.705)
Epoch: [9][320/391]	Time 0.469 (0.465)	Data 0.002 (0.003)	Loss 0.8473 (0.9319)	Acc@1 83.594 (77.925)	Acc@5 99.219 (98.695)
Epoch: [9][384/391]	Time 0.452 (0.464)	Data 0.002 (0.003)	Loss 0.7911 (0.9294)	Acc@1 83.594 (78.105)	Acc@5 99.219 (98.675)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.543 (0.543)	Data 0.188 (0.188)	Loss 0.9054 (0.9054)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.377 (0.464)	Data 0.002 (0.005)	Loss 0.8626 (0.9018)	Acc@1 79.688 (78.894)	Acc@5 98.438 (98.906)
Epoch: [10][128/391]	Time 0.466 (0.464)	Data 0.002 (0.003)	Loss 0.9195 (0.9167)	Acc@1 77.344 (78.658)	Acc@5 100.000 (98.783)
Epoch: [10][192/391]	Time 0.466 (0.464)	Data 0.002 (0.003)	Loss 0.9044 (0.9188)	Acc@1 78.906 (78.538)	Acc@5 99.219 (98.765)
Epoch: [10][256/391]	Time 0.447 (0.463)	Data 0.002 (0.003)	Loss 0.7771 (0.9168)	Acc@1 83.594 (78.420)	Acc@5 99.219 (98.763)
Epoch: [10][320/391]	Time 0.455 (0.462)	Data 0.003 (0.003)	Loss 1.2061 (0.9203)	Acc@1 70.312 (78.373)	Acc@5 99.219 (98.732)
Epoch: [10][384/391]	Time 0.459 (0.462)	Data 0.002 (0.002)	Loss 1.0747 (0.9178)	Acc@1 71.875 (78.513)	Acc@5 99.219 (98.768)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 445250 ; 487386 ; 0.9135469627769366

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.542 (0.542)	Data 0.177 (0.177)	Loss 0.8942 (0.8942)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [11][64/391]	Time 0.484 (0.460)	Data 0.002 (0.005)	Loss 1.0093 (0.8974)	Acc@1 76.562 (78.750)	Acc@5 96.875 (98.690)
Epoch: [11][128/391]	Time 0.507 (0.461)	Data 0.002 (0.003)	Loss 0.8505 (0.8995)	Acc@1 80.469 (78.773)	Acc@5 98.438 (98.746)
Epoch: [11][192/391]	Time 0.464 (0.462)	Data 0.001 (0.003)	Loss 0.7862 (0.9038)	Acc@1 84.375 (78.765)	Acc@5 100.000 (98.717)
Epoch: [11][256/391]	Time 0.438 (0.462)	Data 0.001 (0.003)	Loss 0.9364 (0.9058)	Acc@1 78.125 (78.651)	Acc@5 99.219 (98.748)
Epoch: [11][320/391]	Time 0.461 (0.462)	Data 0.002 (0.002)	Loss 0.8249 (0.9035)	Acc@1 82.812 (78.867)	Acc@5 99.219 (98.778)
Epoch: [11][384/391]	Time 0.443 (0.463)	Data 0.002 (0.002)	Loss 1.0030 (0.9045)	Acc@1 78.125 (78.864)	Acc@5 97.656 (98.762)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.463 (0.463)	Data 0.276 (0.276)	Loss 0.8647 (0.8647)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [12][64/391]	Time 0.446 (0.472)	Data 0.002 (0.006)	Loss 1.0187 (0.8857)	Acc@1 75.781 (79.784)	Acc@5 99.219 (98.642)
Epoch: [12][128/391]	Time 0.453 (0.469)	Data 0.002 (0.004)	Loss 0.9758 (0.8934)	Acc@1 77.344 (79.439)	Acc@5 99.219 (98.752)
Epoch: [12][192/391]	Time 0.459 (0.467)	Data 0.002 (0.003)	Loss 0.8243 (0.8961)	Acc@1 83.594 (79.291)	Acc@5 99.219 (98.737)
Epoch: [12][256/391]	Time 0.465 (0.466)	Data 0.002 (0.003)	Loss 0.9368 (0.9022)	Acc@1 78.906 (79.122)	Acc@5 99.219 (98.766)
Epoch: [12][320/391]	Time 0.465 (0.465)	Data 0.001 (0.003)	Loss 1.0618 (0.8955)	Acc@1 75.000 (79.430)	Acc@5 97.656 (98.805)
Epoch: [12][384/391]	Time 0.458 (0.465)	Data 0.002 (0.003)	Loss 0.8033 (0.8944)	Acc@1 87.500 (79.440)	Acc@5 100.000 (98.791)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.494 (0.494)	Data 0.279 (0.279)	Loss 0.9031 (0.9031)	Acc@1 81.250 (81.250)	Acc@5 96.875 (96.875)
Epoch: [13][64/391]	Time 0.450 (0.464)	Data 0.002 (0.006)	Loss 0.9018 (0.8535)	Acc@1 78.125 (81.070)	Acc@5 98.438 (98.966)
Epoch: [13][128/391]	Time 0.468 (0.463)	Data 0.002 (0.004)	Loss 1.0302 (0.8632)	Acc@1 75.000 (80.463)	Acc@5 97.656 (98.922)
Epoch: [13][192/391]	Time 0.443 (0.466)	Data 0.002 (0.003)	Loss 0.7806 (0.8702)	Acc@1 80.469 (80.088)	Acc@5 100.000 (98.943)
Epoch: [13][256/391]	Time 0.452 (0.464)	Data 0.002 (0.003)	Loss 0.9662 (0.8739)	Acc@1 72.656 (79.900)	Acc@5 96.094 (98.903)
Epoch: [13][320/391]	Time 0.460 (0.464)	Data 0.002 (0.003)	Loss 0.9146 (0.8767)	Acc@1 78.906 (79.829)	Acc@5 99.219 (98.888)
Epoch: [13][384/391]	Time 0.468 (0.465)	Data 0.002 (0.003)	Loss 0.9610 (0.8795)	Acc@1 77.344 (79.807)	Acc@5 96.875 (98.876)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.487 (0.487)	Data 0.221 (0.221)	Loss 0.8672 (0.8672)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [14][64/391]	Time 0.486 (0.464)	Data 0.001 (0.005)	Loss 0.9450 (0.8807)	Acc@1 78.125 (79.916)	Acc@5 98.438 (98.762)
Epoch: [14][128/391]	Time 0.473 (0.467)	Data 0.002 (0.004)	Loss 0.7691 (0.8911)	Acc@1 84.375 (79.348)	Acc@5 100.000 (98.777)
Epoch: [14][192/391]	Time 0.468 (0.465)	Data 0.002 (0.003)	Loss 0.9476 (0.8880)	Acc@1 76.562 (79.335)	Acc@5 99.219 (98.834)
Epoch: [14][256/391]	Time 0.490 (0.465)	Data 0.002 (0.003)	Loss 1.0287 (0.8903)	Acc@1 71.094 (79.341)	Acc@5 100.000 (98.799)
Epoch: [14][320/391]	Time 0.463 (0.464)	Data 0.002 (0.003)	Loss 0.8497 (0.8823)	Acc@1 82.812 (79.697)	Acc@5 98.438 (98.807)
Epoch: [14][384/391]	Time 0.461 (0.464)	Data 0.002 (0.003)	Loss 0.9155 (0.8830)	Acc@1 77.344 (79.690)	Acc@5 100.000 (98.807)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.508 (0.508)	Data 0.213 (0.213)	Loss 0.9326 (0.9326)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [15][64/391]	Time 0.478 (0.467)	Data 0.002 (0.005)	Loss 0.8606 (0.8737)	Acc@1 75.781 (79.880)	Acc@5 99.219 (98.870)
Epoch: [15][128/391]	Time 0.473 (0.467)	Data 0.002 (0.004)	Loss 0.7835 (0.8623)	Acc@1 82.031 (80.481)	Acc@5 100.000 (98.910)
Epoch: [15][192/391]	Time 0.473 (0.467)	Data 0.002 (0.003)	Loss 0.8845 (0.8690)	Acc@1 78.125 (80.173)	Acc@5 100.000 (98.838)
Epoch: [15][256/391]	Time 0.454 (0.465)	Data 0.002 (0.003)	Loss 0.8729 (0.8737)	Acc@1 78.906 (80.071)	Acc@5 97.656 (98.817)
Epoch: [15][320/391]	Time 0.464 (0.464)	Data 0.002 (0.003)	Loss 0.7982 (0.8781)	Acc@1 81.250 (79.980)	Acc@5 99.219 (98.793)
Epoch: [15][384/391]	Time 0.466 (0.464)	Data 0.002 (0.003)	Loss 0.8190 (0.8769)	Acc@1 81.250 (80.051)	Acc@5 99.219 (98.801)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv21.weight

 RM:  module.conv22.weight

Module List Length:  68
Index1: 42
Index: 22
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: False
Bool2: True
j: 32; k: 32
Bool1: False
Bool2: True
j: 33; k: 32
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(17, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 58: Conv2d(64, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 59: BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 60: Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 61: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 62 gegen 66: Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 63 gegen 67: BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(17, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  64

Module List Length:  64
Index1: 40
Index: 21
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(17, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 58 gegen 62: Conv2d(64, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 59 gegen 63: BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(17, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): AdaptiveAvgPool2d(output_size=(1, 1))
    (59): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  60
Count: 306750 ; 487386 ; 0.629377946842954

Epoch: [16 | 180] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 318, in main
    use_gpu_num)
  File "main.py", line 442, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 459, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 107, in forward
    exponential_average_factor, self.eps)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 17 elements not 0
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.552 (0.552)	Data 0.203 (0.203)	Loss 3.4536 (3.4536)	Acc@1 7.031 (7.031)	Acc@5 50.000 (50.000)
Epoch: [1][64/391]	Time 0.529 (0.472)	Data 0.002 (0.005)	Loss 2.7504 (2.8256)	Acc@1 17.969 (19.567)	Acc@5 80.469 (74.111)
Epoch: [1][128/391]	Time 0.450 (0.475)	Data 0.001 (0.003)	Loss 2.4926 (2.6799)	Acc@1 31.250 (23.492)	Acc@5 85.156 (78.591)
Epoch: [1][192/391]	Time 0.486 (0.471)	Data 0.002 (0.003)	Loss 2.2951 (2.5798)	Acc@1 32.812 (26.376)	Acc@5 87.500 (81.469)
Epoch: [1][256/391]	Time 0.472 (0.469)	Data 0.002 (0.003)	Loss 2.2786 (2.5072)	Acc@1 29.688 (28.669)	Acc@5 90.625 (83.086)
Epoch: [1][320/391]	Time 0.455 (0.468)	Data 0.002 (0.002)	Loss 2.1716 (2.4355)	Acc@1 36.719 (31.158)	Acc@5 91.406 (84.638)
Epoch: [1][384/391]	Time 0.461 (0.469)	Data 0.001 (0.002)	Loss 2.1858 (2.3690)	Acc@1 39.844 (33.630)	Acc@5 87.500 (85.733)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.491 (0.491)	Data 0.217 (0.217)	Loss 2.0731 (2.0731)	Acc@1 44.531 (44.531)	Acc@5 86.719 (86.719)
Epoch: [2][64/391]	Time 0.423 (0.469)	Data 0.002 (0.005)	Loss 1.7705 (1.9212)	Acc@1 52.344 (48.786)	Acc@5 94.531 (92.909)
Epoch: [2][128/391]	Time 0.459 (0.473)	Data 0.002 (0.003)	Loss 1.9060 (1.8718)	Acc@1 51.562 (50.254)	Acc@5 89.844 (93.314)
Epoch: [2][192/391]	Time 0.469 (0.471)	Data 0.001 (0.003)	Loss 1.6368 (1.8293)	Acc@1 59.375 (51.538)	Acc@5 94.531 (93.847)
Epoch: [2][256/391]	Time 0.406 (0.470)	Data 0.002 (0.003)	Loss 1.6673 (1.7882)	Acc@1 55.469 (53.025)	Acc@5 96.875 (94.148)
Epoch: [2][320/391]	Time 0.473 (0.470)	Data 0.002 (0.003)	Loss 1.4630 (1.7461)	Acc@1 63.281 (54.490)	Acc@5 94.531 (94.434)
Epoch: [2][384/391]	Time 0.465 (0.470)	Data 0.002 (0.002)	Loss 1.5203 (1.7073)	Acc@1 60.938 (55.668)	Acc@5 97.656 (94.771)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.445 (0.445)	Data 0.252 (0.252)	Loss 1.3604 (1.3604)	Acc@1 68.750 (68.750)	Acc@5 95.312 (95.312)
Epoch: [3][64/391]	Time 0.474 (0.469)	Data 0.002 (0.006)	Loss 1.5943 (1.4899)	Acc@1 62.500 (61.575)	Acc@5 96.094 (96.190)
Epoch: [3][128/391]	Time 0.466 (0.470)	Data 0.001 (0.004)	Loss 1.3111 (1.4471)	Acc@1 72.656 (63.124)	Acc@5 98.438 (96.681)
Epoch: [3][192/391]	Time 0.458 (0.470)	Data 0.001 (0.003)	Loss 1.4067 (1.4138)	Acc@1 60.938 (64.184)	Acc@5 96.094 (96.758)
Epoch: [3][256/391]	Time 0.470 (0.471)	Data 0.002 (0.003)	Loss 1.2605 (1.3889)	Acc@1 71.875 (64.838)	Acc@5 96.875 (96.848)
Epoch: [3][320/391]	Time 0.471 (0.471)	Data 0.002 (0.003)	Loss 1.3724 (1.3611)	Acc@1 66.406 (65.664)	Acc@5 95.312 (97.038)
Epoch: [3][384/391]	Time 0.454 (0.469)	Data 0.002 (0.003)	Loss 1.2384 (1.3428)	Acc@1 67.969 (66.220)	Acc@5 94.531 (97.151)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.515 (0.515)	Data 0.224 (0.224)	Loss 1.1356 (1.1356)	Acc@1 71.875 (71.875)	Acc@5 97.656 (97.656)
Epoch: [4][64/391]	Time 0.465 (0.470)	Data 0.002 (0.005)	Loss 1.3171 (1.2033)	Acc@1 67.188 (69.820)	Acc@5 96.875 (97.668)
Epoch: [4][128/391]	Time 0.484 (0.476)	Data 0.002 (0.004)	Loss 1.1659 (1.2084)	Acc@1 71.875 (69.767)	Acc@5 97.656 (97.729)
Epoch: [4][192/391]	Time 0.457 (0.469)	Data 0.002 (0.003)	Loss 1.1199 (1.1860)	Acc@1 74.219 (70.458)	Acc@5 97.656 (97.802)
Epoch: [4][256/391]	Time 0.456 (0.468)	Data 0.002 (0.003)	Loss 1.0618 (1.1731)	Acc@1 75.781 (70.933)	Acc@5 96.875 (97.857)
Epoch: [4][320/391]	Time 0.451 (0.471)	Data 0.002 (0.003)	Loss 0.9120 (1.1675)	Acc@1 81.250 (71.099)	Acc@5 99.219 (97.817)
Epoch: [4][384/391]	Time 0.470 (0.470)	Data 0.002 (0.003)	Loss 1.0756 (1.1573)	Acc@1 74.219 (71.479)	Acc@5 96.094 (97.869)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.526 (0.526)	Data 0.256 (0.256)	Loss 0.9502 (0.9502)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [5][64/391]	Time 0.442 (0.477)	Data 0.002 (0.006)	Loss 1.1537 (1.0948)	Acc@1 71.094 (73.317)	Acc@5 96.875 (98.197)
Epoch: [5][128/391]	Time 0.461 (0.472)	Data 0.002 (0.004)	Loss 1.1157 (1.0864)	Acc@1 72.656 (73.649)	Acc@5 94.531 (98.086)
Epoch: [5][192/391]	Time 0.460 (0.470)	Data 0.002 (0.003)	Loss 0.9808 (1.0846)	Acc@1 79.688 (73.688)	Acc@5 99.219 (98.122)
Epoch: [5][256/391]	Time 0.453 (0.470)	Data 0.002 (0.003)	Loss 1.2130 (1.0803)	Acc@1 71.094 (73.802)	Acc@5 94.531 (98.118)
Epoch: [5][320/391]	Time 0.438 (0.469)	Data 0.002 (0.003)	Loss 1.1372 (1.0707)	Acc@1 70.312 (74.095)	Acc@5 98.438 (98.189)
Epoch: [5][384/391]	Time 0.462 (0.467)	Data 0.002 (0.003)	Loss 0.9645 (1.0631)	Acc@1 75.000 (74.290)	Acc@5 96.875 (98.218)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.611 (0.611)	Data 0.233 (0.233)	Loss 0.9944 (0.9944)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [6][64/391]	Time 0.541 (0.478)	Data 0.002 (0.006)	Loss 1.0272 (1.0296)	Acc@1 78.125 (75.036)	Acc@5 97.656 (98.438)
Epoch: [6][128/391]	Time 0.447 (0.476)	Data 0.002 (0.004)	Loss 1.0080 (1.0314)	Acc@1 76.562 (74.861)	Acc@5 99.219 (98.347)
Epoch: [6][192/391]	Time 0.458 (0.473)	Data 0.002 (0.003)	Loss 1.0695 (1.0274)	Acc@1 71.094 (74.899)	Acc@5 98.438 (98.369)
Epoch: [6][256/391]	Time 0.481 (0.475)	Data 0.002 (0.003)	Loss 0.9545 (1.0198)	Acc@1 82.031 (75.109)	Acc@5 99.219 (98.383)
Epoch: [6][320/391]	Time 0.503 (0.475)	Data 0.002 (0.003)	Loss 0.9951 (1.0135)	Acc@1 75.781 (75.309)	Acc@5 98.438 (98.425)
Epoch: [6][384/391]	Time 0.444 (0.476)	Data 0.002 (0.003)	Loss 0.9175 (1.0081)	Acc@1 76.562 (75.412)	Acc@5 100.000 (98.464)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.551 (0.551)	Data 0.245 (0.245)	Loss 0.9608 (0.9608)	Acc@1 73.438 (73.438)	Acc@5 98.438 (98.438)
Epoch: [7][64/391]	Time 0.488 (0.482)	Data 0.002 (0.006)	Loss 0.9241 (0.9536)	Acc@1 76.562 (77.200)	Acc@5 99.219 (98.678)
Epoch: [7][128/391]	Time 0.453 (0.473)	Data 0.002 (0.004)	Loss 0.8868 (0.9608)	Acc@1 80.469 (76.835)	Acc@5 99.219 (98.589)
Epoch: [7][192/391]	Time 0.472 (0.474)	Data 0.002 (0.003)	Loss 0.9137 (0.9596)	Acc@1 77.344 (76.826)	Acc@5 98.438 (98.644)
Epoch: [7][256/391]	Time 0.440 (0.474)	Data 0.002 (0.003)	Loss 0.8851 (0.9567)	Acc@1 82.031 (77.012)	Acc@5 99.219 (98.653)
Epoch: [7][320/391]	Time 0.452 (0.474)	Data 0.002 (0.003)	Loss 0.9227 (0.9585)	Acc@1 81.250 (76.881)	Acc@5 99.219 (98.625)
Epoch: [7][384/391]	Time 0.558 (0.475)	Data 0.002 (0.003)	Loss 1.1798 (0.9580)	Acc@1 73.438 (77.011)	Acc@5 96.875 (98.655)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.321 (0.321)	Data 0.259 (0.259)	Loss 0.9514 (0.9514)	Acc@1 79.688 (79.688)	Acc@5 96.875 (96.875)
Epoch: [8][64/391]	Time 0.430 (0.482)	Data 0.002 (0.006)	Loss 0.9773 (0.9567)	Acc@1 76.562 (77.392)	Acc@5 98.438 (98.786)
Epoch: [8][128/391]	Time 0.524 (0.485)	Data 0.002 (0.004)	Loss 0.8123 (0.9559)	Acc@1 82.812 (77.592)	Acc@5 100.000 (98.698)
Epoch: [8][192/391]	Time 0.514 (0.484)	Data 0.002 (0.003)	Loss 0.9494 (0.9549)	Acc@1 81.250 (77.623)	Acc@5 99.219 (98.644)
Epoch: [8][256/391]	Time 0.464 (0.482)	Data 0.002 (0.003)	Loss 0.9295 (0.9574)	Acc@1 79.688 (77.429)	Acc@5 97.656 (98.605)
Epoch: [8][320/391]	Time 0.513 (0.481)	Data 0.002 (0.003)	Loss 0.8386 (0.9508)	Acc@1 81.250 (77.548)	Acc@5 99.219 (98.654)
Epoch: [8][384/391]	Time 0.413 (0.481)	Data 0.002 (0.003)	Loss 0.9093 (0.9497)	Acc@1 77.344 (77.543)	Acc@5 99.219 (98.661)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.499 (0.499)	Data 0.266 (0.266)	Loss 0.8569 (0.8569)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [9][64/391]	Time 0.536 (0.482)	Data 0.003 (0.006)	Loss 0.9059 (0.9239)	Acc@1 78.125 (78.137)	Acc@5 97.656 (98.870)
Epoch: [9][128/391]	Time 0.462 (0.479)	Data 0.002 (0.004)	Loss 0.8884 (0.9297)	Acc@1 83.594 (78.295)	Acc@5 98.438 (98.819)
Epoch: [9][192/391]	Time 0.470 (0.478)	Data 0.002 (0.003)	Loss 0.9458 (0.9312)	Acc@1 73.438 (78.210)	Acc@5 99.219 (98.846)
Epoch: [9][256/391]	Time 0.505 (0.476)	Data 0.002 (0.003)	Loss 0.8774 (0.9312)	Acc@1 79.688 (78.313)	Acc@5 99.219 (98.827)
Epoch: [9][320/391]	Time 0.468 (0.477)	Data 0.002 (0.003)	Loss 0.9364 (0.9335)	Acc@1 76.562 (78.205)	Acc@5 97.656 (98.751)
Epoch: [9][384/391]	Time 0.435 (0.476)	Data 0.002 (0.003)	Loss 0.9873 (0.9321)	Acc@1 75.000 (78.318)	Acc@5 98.438 (98.720)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.530 (0.530)	Data 0.202 (0.202)	Loss 0.8429 (0.8429)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.437 (0.479)	Data 0.001 (0.005)	Loss 0.8593 (0.9240)	Acc@1 85.156 (78.582)	Acc@5 100.000 (98.642)
Epoch: [10][128/391]	Time 0.490 (0.476)	Data 0.002 (0.004)	Loss 0.9401 (0.9193)	Acc@1 75.781 (78.821)	Acc@5 97.656 (98.789)
Epoch: [10][192/391]	Time 0.423 (0.477)	Data 0.002 (0.003)	Loss 0.9637 (0.9102)	Acc@1 78.125 (79.068)	Acc@5 99.219 (98.879)
Epoch: [10][256/391]	Time 0.546 (0.472)	Data 0.002 (0.003)	Loss 0.9948 (0.9126)	Acc@1 78.906 (79.073)	Acc@5 98.438 (98.836)
Epoch: [10][320/391]	Time 0.460 (0.475)	Data 0.002 (0.003)	Loss 0.9473 (0.9179)	Acc@1 81.250 (78.928)	Acc@5 100.000 (98.859)
Epoch: [10][384/391]	Time 0.479 (0.474)	Data 0.001 (0.002)	Loss 1.0939 (0.9191)	Acc@1 75.000 (78.937)	Acc@5 96.875 (98.819)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 470362 ; 487386 ; 0.965070806301371

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.501 (0.501)	Data 0.237 (0.237)	Loss 0.8266 (0.8266)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [11][64/391]	Time 0.469 (0.475)	Data 0.002 (0.006)	Loss 1.0100 (0.8610)	Acc@1 75.781 (80.457)	Acc@5 97.656 (98.930)
Epoch: [11][128/391]	Time 0.440 (0.472)	Data 0.002 (0.004)	Loss 0.8059 (0.8782)	Acc@1 82.812 (80.087)	Acc@5 100.000 (98.819)
Epoch: [11][192/391]	Time 0.498 (0.474)	Data 0.001 (0.003)	Loss 0.9775 (0.8878)	Acc@1 75.000 (79.720)	Acc@5 99.219 (98.830)
Epoch: [11][256/391]	Time 0.491 (0.477)	Data 0.002 (0.003)	Loss 0.7912 (0.8988)	Acc@1 82.812 (79.274)	Acc@5 99.219 (98.760)
Epoch: [11][320/391]	Time 0.497 (0.478)	Data 0.002 (0.003)	Loss 1.0465 (0.8954)	Acc@1 78.125 (79.515)	Acc@5 99.219 (98.764)
Epoch: [11][384/391]	Time 0.451 (0.476)	Data 0.002 (0.003)	Loss 1.0565 (0.8988)	Acc@1 73.438 (79.438)	Acc@5 99.219 (98.780)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.534 (0.534)	Data 0.242 (0.242)	Loss 1.0963 (1.0963)	Acc@1 75.000 (75.000)	Acc@5 96.094 (96.094)
Epoch: [12][64/391]	Time 0.434 (0.470)	Data 0.002 (0.006)	Loss 0.8437 (0.8969)	Acc@1 82.812 (79.207)	Acc@5 99.219 (98.846)
Epoch: [12][128/391]	Time 0.475 (0.471)	Data 0.002 (0.004)	Loss 0.7858 (0.8923)	Acc@1 84.375 (79.542)	Acc@5 100.000 (98.958)
Epoch: [12][192/391]	Time 0.515 (0.474)	Data 0.002 (0.003)	Loss 0.9312 (0.8845)	Acc@1 80.469 (79.833)	Acc@5 98.438 (98.919)
Epoch: [12][256/391]	Time 0.480 (0.474)	Data 0.002 (0.003)	Loss 0.8147 (0.8862)	Acc@1 80.469 (79.770)	Acc@5 100.000 (98.875)
Epoch: [12][320/391]	Time 0.469 (0.475)	Data 0.002 (0.003)	Loss 0.7348 (0.8887)	Acc@1 85.938 (79.663)	Acc@5 100.000 (98.905)
Epoch: [12][384/391]	Time 0.471 (0.472)	Data 0.001 (0.003)	Loss 0.8836 (0.8916)	Acc@1 78.125 (79.535)	Acc@5 100.000 (98.866)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.483 (0.483)	Data 0.253 (0.253)	Loss 0.7121 (0.7121)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [13][64/391]	Time 0.457 (0.468)	Data 0.002 (0.006)	Loss 0.8266 (0.8614)	Acc@1 81.250 (80.325)	Acc@5 99.219 (99.075)
Epoch: [13][128/391]	Time 0.459 (0.468)	Data 0.002 (0.004)	Loss 0.7797 (0.8710)	Acc@1 82.812 (80.039)	Acc@5 100.000 (99.007)
Epoch: [13][192/391]	Time 0.484 (0.469)	Data 0.002 (0.003)	Loss 1.0809 (0.8835)	Acc@1 72.656 (79.602)	Acc@5 96.875 (98.968)
Epoch: [13][256/391]	Time 0.467 (0.467)	Data 0.002 (0.003)	Loss 0.7936 (0.8823)	Acc@1 82.031 (79.660)	Acc@5 98.438 (98.933)
Epoch: [13][320/391]	Time 0.486 (0.469)	Data 0.002 (0.003)	Loss 1.0011 (0.8829)	Acc@1 75.000 (79.690)	Acc@5 99.219 (98.936)
Epoch: [13][384/391]	Time 0.444 (0.469)	Data 0.002 (0.003)	Loss 0.8117 (0.8851)	Acc@1 86.719 (79.698)	Acc@5 99.219 (98.884)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.498 (0.498)	Data 0.263 (0.263)	Loss 1.0101 (1.0101)	Acc@1 78.125 (78.125)	Acc@5 98.438 (98.438)
Epoch: [14][64/391]	Time 0.429 (0.470)	Data 0.002 (0.006)	Loss 0.9443 (0.8643)	Acc@1 75.781 (80.685)	Acc@5 96.094 (98.846)
Epoch: [14][128/391]	Time 0.441 (0.471)	Data 0.002 (0.004)	Loss 1.0697 (0.8707)	Acc@1 75.781 (80.136)	Acc@5 99.219 (98.898)
Epoch: [14][192/391]	Time 0.448 (0.473)	Data 0.002 (0.003)	Loss 0.8486 (0.8691)	Acc@1 80.469 (80.246)	Acc@5 100.000 (98.923)
Epoch: [14][256/391]	Time 0.505 (0.470)	Data 0.002 (0.003)	Loss 0.7041 (0.8720)	Acc@1 85.938 (80.116)	Acc@5 100.000 (98.924)
Epoch: [14][320/391]	Time 0.527 (0.472)	Data 0.002 (0.003)	Loss 0.8258 (0.8711)	Acc@1 80.469 (80.182)	Acc@5 98.438 (98.932)
Epoch: [14][384/391]	Time 0.476 (0.472)	Data 0.002 (0.003)	Loss 0.8903 (0.8772)	Acc@1 83.594 (79.961)	Acc@5 100.000 (98.916)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.525 (0.525)	Data 0.327 (0.327)	Loss 0.8724 (0.8724)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [15][64/391]	Time 0.589 (0.469)	Data 0.002 (0.007)	Loss 0.8752 (0.8774)	Acc@1 81.250 (79.892)	Acc@5 97.656 (98.942)
Epoch: [15][128/391]	Time 0.475 (0.473)	Data 0.001 (0.005)	Loss 0.7649 (0.8697)	Acc@1 85.156 (80.208)	Acc@5 100.000 (98.964)
Epoch: [15][192/391]	Time 0.489 (0.472)	Data 0.002 (0.004)	Loss 0.7662 (0.8698)	Acc@1 84.375 (80.181)	Acc@5 100.000 (98.927)
Epoch: [15][256/391]	Time 0.445 (0.472)	Data 0.002 (0.003)	Loss 0.8901 (0.8665)	Acc@1 80.469 (80.271)	Acc@5 97.656 (98.921)
Epoch: [15][320/391]	Time 0.487 (0.473)	Data 0.002 (0.003)	Loss 0.9422 (0.8690)	Acc@1 77.344 (80.150)	Acc@5 97.656 (98.907)
Epoch: [15][384/391]	Time 0.451 (0.473)	Data 0.002 (0.003)	Loss 0.8219 (0.8716)	Acc@1 81.250 (80.028)	Acc@5 99.219 (98.900)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 395328 ; 487386 ; 0.8111189078061332

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.552 (0.552)	Data 0.218 (0.218)	Loss 0.9093 (0.9093)	Acc@1 81.250 (81.250)	Acc@5 96.875 (96.875)
Epoch: [16][64/391]	Time 0.470 (0.469)	Data 0.002 (0.005)	Loss 0.9397 (0.8385)	Acc@1 80.469 (81.575)	Acc@5 97.656 (98.918)
Epoch: [16][128/391]	Time 0.456 (0.469)	Data 0.001 (0.004)	Loss 0.7946 (0.8562)	Acc@1 85.156 (80.778)	Acc@5 100.000 (98.874)
Epoch: [16][192/391]	Time 0.440 (0.472)	Data 0.002 (0.003)	Loss 0.8847 (0.8533)	Acc@1 83.594 (80.837)	Acc@5 96.875 (98.838)
Epoch: [16][256/391]	Time 0.458 (0.470)	Data 0.002 (0.003)	Loss 0.9409 (0.8643)	Acc@1 80.469 (80.402)	Acc@5 100.000 (98.833)
Epoch: [16][320/391]	Time 0.453 (0.473)	Data 0.002 (0.003)	Loss 0.8618 (0.8702)	Acc@1 81.250 (80.162)	Acc@5 99.219 (98.793)
Epoch: [16][384/391]	Time 0.424 (0.471)	Data 0.002 (0.003)	Loss 0.9262 (0.8709)	Acc@1 77.344 (80.134)	Acc@5 99.219 (98.793)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.472 (0.472)	Data 0.249 (0.249)	Loss 0.7950 (0.7950)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)
Epoch: [17][64/391]	Time 0.552 (0.480)	Data 0.002 (0.006)	Loss 0.8343 (0.8453)	Acc@1 82.812 (81.310)	Acc@5 99.219 (99.026)
Epoch: [17][128/391]	Time 0.455 (0.472)	Data 0.002 (0.004)	Loss 0.9131 (0.8518)	Acc@1 79.688 (81.026)	Acc@5 97.656 (98.849)
Epoch: [17][192/391]	Time 0.459 (0.472)	Data 0.002 (0.003)	Loss 0.9039 (0.8550)	Acc@1 78.125 (80.869)	Acc@5 99.219 (98.931)
Epoch: [17][256/391]	Time 0.439 (0.470)	Data 0.002 (0.003)	Loss 0.8390 (0.8556)	Acc@1 79.688 (80.706)	Acc@5 100.000 (98.942)
Epoch: [17][320/391]	Time 0.475 (0.471)	Data 0.002 (0.003)	Loss 0.8860 (0.8607)	Acc@1 80.469 (80.508)	Acc@5 96.875 (98.946)
Epoch: [17][384/391]	Time 0.472 (0.472)	Data 0.002 (0.003)	Loss 0.8931 (0.8601)	Acc@1 82.812 (80.503)	Acc@5 96.875 (98.933)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.474 (0.474)	Data 0.234 (0.234)	Loss 0.7588 (0.7588)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [18][64/391]	Time 0.460 (0.472)	Data 0.002 (0.006)	Loss 0.8154 (0.8379)	Acc@1 82.031 (81.418)	Acc@5 99.219 (99.099)
Epoch: [18][128/391]	Time 0.532 (0.473)	Data 0.002 (0.004)	Loss 0.9003 (0.8521)	Acc@1 82.031 (80.735)	Acc@5 96.875 (98.934)
Epoch: [18][192/391]	Time 0.496 (0.473)	Data 0.002 (0.003)	Loss 0.9129 (0.8515)	Acc@1 80.469 (80.898)	Acc@5 99.219 (98.964)
Epoch: [18][256/391]	Time 0.472 (0.471)	Data 0.002 (0.003)	Loss 0.7808 (0.8529)	Acc@1 82.812 (80.852)	Acc@5 99.219 (98.988)
Epoch: [18][320/391]	Time 0.472 (0.472)	Data 0.002 (0.003)	Loss 0.8041 (0.8570)	Acc@1 82.812 (80.654)	Acc@5 99.219 (98.946)
Epoch: [18][384/391]	Time 0.489 (0.471)	Data 0.003 (0.003)	Loss 0.8208 (0.8546)	Acc@1 85.156 (80.749)	Acc@5 100.000 (98.947)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.499 (0.499)	Data 0.214 (0.214)	Loss 1.0527 (1.0527)	Acc@1 79.688 (79.688)	Acc@5 97.656 (97.656)
Epoch: [19][64/391]	Time 0.475 (0.469)	Data 0.002 (0.005)	Loss 0.8634 (0.8306)	Acc@1 81.250 (81.803)	Acc@5 96.875 (99.099)
Epoch: [19][128/391]	Time 0.468 (0.468)	Data 0.002 (0.004)	Loss 0.8035 (0.8382)	Acc@1 82.812 (81.226)	Acc@5 98.438 (99.019)
Epoch: [19][192/391]	Time 0.468 (0.469)	Data 0.002 (0.003)	Loss 0.8347 (0.8402)	Acc@1 79.688 (81.246)	Acc@5 100.000 (99.016)
Epoch: [19][256/391]	Time 0.447 (0.467)	Data 0.002 (0.003)	Loss 0.7217 (0.8460)	Acc@1 83.594 (80.955)	Acc@5 100.000 (99.036)
Epoch: [19][320/391]	Time 0.450 (0.469)	Data 0.002 (0.003)	Loss 0.9703 (0.8473)	Acc@1 76.562 (80.980)	Acc@5 97.656 (99.029)
Epoch: [19][384/391]	Time 0.459 (0.471)	Data 0.001 (0.003)	Loss 0.8652 (0.8514)	Acc@1 75.781 (80.812)	Acc@5 100.000 (99.028)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.516 (0.516)	Data 0.225 (0.225)	Loss 0.8829 (0.8829)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [20][64/391]	Time 0.466 (0.475)	Data 0.001 (0.005)	Loss 0.7190 (0.8264)	Acc@1 88.281 (81.791)	Acc@5 99.219 (99.123)
Epoch: [20][128/391]	Time 0.399 (0.471)	Data 0.002 (0.004)	Loss 0.8521 (0.8590)	Acc@1 85.156 (80.596)	Acc@5 96.875 (98.922)
Epoch: [20][192/391]	Time 0.485 (0.472)	Data 0.002 (0.003)	Loss 0.7673 (0.8489)	Acc@1 83.594 (80.829)	Acc@5 100.000 (98.964)
Epoch: [20][256/391]	Time 0.507 (0.471)	Data 0.002 (0.003)	Loss 0.8687 (0.8507)	Acc@1 81.250 (80.751)	Acc@5 98.438 (98.988)
Epoch: [20][320/391]	Time 0.458 (0.470)	Data 0.001 (0.003)	Loss 0.8912 (0.8531)	Acc@1 77.344 (80.622)	Acc@5 98.438 (98.995)
Epoch: [20][384/391]	Time 0.449 (0.469)	Data 0.001 (0.003)	Loss 0.9302 (0.8540)	Acc@1 80.469 (80.582)	Acc@5 96.094 (98.985)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 362412 ; 487386 ; 0.7435831148206965

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.523 (0.523)	Data 0.230 (0.230)	Loss 0.8647 (0.8647)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [21][64/391]	Time 0.507 (0.478)	Data 0.002 (0.006)	Loss 0.7213 (0.8069)	Acc@1 84.375 (82.584)	Acc@5 100.000 (99.207)
Epoch: [21][128/391]	Time 0.491 (0.475)	Data 0.002 (0.004)	Loss 0.7706 (0.8309)	Acc@1 82.031 (81.650)	Acc@5 99.219 (99.043)
Epoch: [21][192/391]	Time 0.462 (0.471)	Data 0.002 (0.003)	Loss 0.8612 (0.8366)	Acc@1 77.344 (81.339)	Acc@5 97.656 (99.057)
Epoch: [21][256/391]	Time 0.496 (0.472)	Data 0.001 (0.003)	Loss 0.8752 (0.8372)	Acc@1 81.250 (81.262)	Acc@5 100.000 (99.052)
Epoch: [21][320/391]	Time 0.459 (0.473)	Data 0.002 (0.003)	Loss 0.7572 (0.8421)	Acc@1 88.281 (81.043)	Acc@5 99.219 (98.995)
Epoch: [21][384/391]	Time 0.464 (0.472)	Data 0.002 (0.003)	Loss 0.9357 (0.8455)	Acc@1 80.469 (81.004)	Acc@5 97.656 (99.002)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.495 (0.495)	Data 0.256 (0.256)	Loss 0.9311 (0.9311)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [22][64/391]	Time 0.468 (0.475)	Data 0.002 (0.006)	Loss 0.7904 (0.8276)	Acc@1 82.031 (81.358)	Acc@5 98.438 (99.026)
Epoch: [22][128/391]	Time 0.482 (0.474)	Data 0.002 (0.004)	Loss 0.7489 (0.8324)	Acc@1 85.938 (81.195)	Acc@5 97.656 (98.934)
Epoch: [22][192/391]	Time 0.470 (0.473)	Data 0.002 (0.003)	Loss 0.7989 (0.8307)	Acc@1 83.594 (81.452)	Acc@5 98.438 (98.903)
Epoch: [22][256/391]	Time 0.553 (0.472)	Data 0.003 (0.003)	Loss 0.8791 (0.8401)	Acc@1 78.125 (81.095)	Acc@5 97.656 (98.890)
Epoch: [22][320/391]	Time 0.480 (0.473)	Data 0.002 (0.003)	Loss 0.9071 (0.8387)	Acc@1 78.906 (81.179)	Acc@5 97.656 (98.888)
Epoch: [22][384/391]	Time 0.450 (0.471)	Data 0.002 (0.003)	Loss 0.7679 (0.8378)	Acc@1 85.156 (81.209)	Acc@5 99.219 (98.898)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.505 (0.505)	Data 0.265 (0.265)	Loss 0.7644 (0.7644)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [23][64/391]	Time 0.442 (0.465)	Data 0.002 (0.006)	Loss 0.7223 (0.8145)	Acc@1 86.719 (81.695)	Acc@5 98.438 (99.123)
Epoch: [23][128/391]	Time 0.465 (0.468)	Data 0.002 (0.004)	Loss 0.8467 (0.8403)	Acc@1 81.250 (80.826)	Acc@5 99.219 (98.977)
Epoch: [23][192/391]	Time 0.469 (0.468)	Data 0.002 (0.003)	Loss 0.7493 (0.8345)	Acc@1 83.594 (81.133)	Acc@5 99.219 (99.012)
Epoch: [23][256/391]	Time 0.495 (0.467)	Data 0.002 (0.003)	Loss 0.7493 (0.8356)	Acc@1 85.156 (81.113)	Acc@5 97.656 (99.030)
Epoch: [23][320/391]	Time 0.459 (0.467)	Data 0.002 (0.003)	Loss 0.8100 (0.8346)	Acc@1 84.375 (81.167)	Acc@5 99.219 (98.992)
Epoch: [23][384/391]	Time 0.452 (0.467)	Data 0.002 (0.003)	Loss 0.8946 (0.8407)	Acc@1 80.469 (81.033)	Acc@5 96.875 (98.971)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.529 (0.529)	Data 0.241 (0.241)	Loss 0.8815 (0.8815)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [24][64/391]	Time 0.455 (0.467)	Data 0.002 (0.006)	Loss 0.8423 (0.8108)	Acc@1 78.906 (81.887)	Acc@5 100.000 (99.050)
Epoch: [24][128/391]	Time 0.443 (0.469)	Data 0.002 (0.004)	Loss 0.7153 (0.8142)	Acc@1 87.500 (81.953)	Acc@5 100.000 (99.049)
Epoch: [24][192/391]	Time 0.451 (0.467)	Data 0.002 (0.003)	Loss 0.8026 (0.8258)	Acc@1 80.469 (81.643)	Acc@5 99.219 (99.045)
Epoch: [24][256/391]	Time 0.473 (0.467)	Data 0.002 (0.003)	Loss 0.7723 (0.8325)	Acc@1 86.719 (81.359)	Acc@5 99.219 (99.027)
Epoch: [24][320/391]	Time 0.449 (0.467)	Data 0.002 (0.003)	Loss 0.8900 (0.8327)	Acc@1 78.125 (81.304)	Acc@5 100.000 (99.012)
Epoch: [24][384/391]	Time 0.452 (0.468)	Data 0.002 (0.003)	Loss 0.9807 (0.8338)	Acc@1 75.781 (81.248)	Acc@5 98.438 (99.010)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.514 (0.514)	Data 0.254 (0.254)	Loss 0.9182 (0.9182)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [25][64/391]	Time 0.487 (0.471)	Data 0.002 (0.006)	Loss 1.1625 (0.8427)	Acc@1 74.219 (80.974)	Acc@5 99.219 (99.231)
Epoch: [25][128/391]	Time 0.449 (0.473)	Data 0.002 (0.004)	Loss 0.7282 (0.8366)	Acc@1 84.375 (81.111)	Acc@5 100.000 (99.201)
Epoch: [25][192/391]	Time 0.443 (0.469)	Data 0.002 (0.003)	Loss 0.8931 (0.8337)	Acc@1 81.250 (81.456)	Acc@5 97.656 (99.109)
Epoch: [25][256/391]	Time 0.409 (0.468)	Data 0.002 (0.003)	Loss 0.7924 (0.8373)	Acc@1 85.156 (81.232)	Acc@5 98.438 (99.064)
Epoch: [25][320/391]	Time 0.471 (0.467)	Data 0.002 (0.003)	Loss 0.7745 (0.8369)	Acc@1 84.375 (81.248)	Acc@5 100.000 (99.061)
Epoch: [25][384/391]	Time 0.483 (0.466)	Data 0.002 (0.003)	Loss 0.9291 (0.8379)	Acc@1 75.781 (81.191)	Acc@5 96.875 (99.034)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv30.weight

 RM:  module.conv31.weight

Module List Length:  68
Index1: 60
Index: 31
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 32; k: 32
Bool1: False
Bool2: True
j: 33; k: 32
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(21, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 58: Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 59: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 60: Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 61: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 62 gegen 66: Conv2d(64, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 63 gegen 67: BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(21, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  64

Module List Length:  64
Index1: 58
Index: 30
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 31; k: 30
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(21, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 58 gegen 62: Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 59 gegen 63: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(21, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): AdaptiveAvgPool2d(output_size=(1, 1))
    (59): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  60
Count: 277894 ; 487386 ; 0.5701723069599866

Epoch: [26 | 180] LR: 0.100000
Epoch: [26][0/391]	Time 0.436 (0.436)	Data 0.242 (0.242)	Loss 1.9651 (1.9651)	Acc@1 48.438 (48.438)	Acc@5 93.750 (93.750)
Epoch: [26][64/391]	Time 0.396 (0.432)	Data 0.002 (0.006)	Loss 0.7557 (1.0722)	Acc@1 82.812 (75.012)	Acc@5 100.000 (98.389)
Epoch: [26][128/391]	Time 0.408 (0.435)	Data 0.002 (0.004)	Loss 0.9661 (0.9629)	Acc@1 71.875 (77.095)	Acc@5 100.000 (98.595)
Epoch: [26][192/391]	Time 0.382 (0.434)	Data 0.002 (0.003)	Loss 0.8031 (0.9148)	Acc@1 86.719 (78.372)	Acc@5 98.438 (98.693)
Epoch: [26][256/391]	Time 0.447 (0.433)	Data 0.001 (0.003)	Loss 0.9307 (0.8979)	Acc@1 78.125 (78.593)	Acc@5 97.656 (98.708)
Epoch: [26][320/391]	Time 0.440 (0.434)	Data 0.002 (0.003)	Loss 0.8330 (0.8877)	Acc@1 82.031 (78.785)	Acc@5 98.438 (98.768)
Epoch: [26][384/391]	Time 0.412 (0.433)	Data 0.002 (0.003)	Loss 0.7766 (0.8769)	Acc@1 81.250 (79.000)	Acc@5 100.000 (98.793)

Epoch: [27 | 180] LR: 0.100000
Epoch: [27][0/391]	Time 0.478 (0.478)	Data 0.317 (0.317)	Loss 0.7577 (0.7577)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [27][64/391]	Time 0.442 (0.431)	Data 0.002 (0.007)	Loss 0.7894 (0.8170)	Acc@1 79.688 (80.757)	Acc@5 100.000 (99.050)
Epoch: [27][128/391]	Time 0.445 (0.434)	Data 0.002 (0.004)	Loss 0.7739 (0.8253)	Acc@1 80.469 (80.426)	Acc@5 99.219 (98.989)
Epoch: [27][192/391]	Time 0.441 (0.431)	Data 0.002 (0.004)	Loss 0.9317 (0.8265)	Acc@1 77.344 (80.513)	Acc@5 97.656 (98.992)
Epoch: [27][256/391]	Time 0.408 (0.431)	Data 0.002 (0.003)	Loss 0.8163 (0.8232)	Acc@1 79.688 (80.718)	Acc@5 98.438 (98.945)
Epoch: [27][320/391]	Time 0.435 (0.432)	Data 0.002 (0.003)	Loss 0.9294 (0.8271)	Acc@1 79.688 (80.600)	Acc@5 98.438 (98.949)
Epoch: [27][384/391]	Time 0.425 (0.433)	Data 0.001 (0.003)	Loss 0.7963 (0.8269)	Acc@1 80.469 (80.550)	Acc@5 97.656 (98.961)

Epoch: [28 | 180] LR: 0.100000
Epoch: [28][0/391]	Time 0.447 (0.447)	Data 0.225 (0.225)	Loss 0.7244 (0.7244)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [28][64/391]	Time 0.423 (0.432)	Data 0.002 (0.005)	Loss 0.8756 (0.8114)	Acc@1 80.469 (81.034)	Acc@5 98.438 (99.062)
Epoch: [28][128/391]	Time 0.391 (0.429)	Data 0.002 (0.004)	Loss 0.7460 (0.8040)	Acc@1 79.688 (81.214)	Acc@5 98.438 (99.025)
Epoch: [28][192/391]	Time 0.436 (0.431)	Data 0.002 (0.003)	Loss 0.6181 (0.8136)	Acc@1 88.281 (81.052)	Acc@5 100.000 (99.028)
Epoch: [28][256/391]	Time 0.368 (0.429)	Data 0.002 (0.003)	Loss 0.8688 (0.8127)	Acc@1 79.688 (81.144)	Acc@5 97.656 (99.036)
Epoch: [28][320/391]	Time 0.411 (0.429)	Data 0.002 (0.003)	Loss 0.7507 (0.8130)	Acc@1 81.250 (81.158)	Acc@5 100.000 (99.000)
Epoch: [28][384/391]	Time 0.417 (0.428)	Data 0.001 (0.003)	Loss 0.7439 (0.8126)	Acc@1 81.250 (81.185)	Acc@5 99.219 (98.996)

Epoch: [29 | 180] LR: 0.100000
Epoch: [29][0/391]	Time 0.474 (0.474)	Data 0.213 (0.213)	Loss 0.8744 (0.8744)	Acc@1 76.562 (76.562)	Acc@5 99.219 (99.219)
Epoch: [29][64/391]	Time 0.442 (0.428)	Data 0.002 (0.005)	Loss 0.8418 (0.8147)	Acc@1 79.688 (80.685)	Acc@5 100.000 (99.014)
Epoch: [29][128/391]	Time 0.427 (0.430)	Data 0.002 (0.004)	Loss 0.9912 (0.8083)	Acc@1 81.250 (81.171)	Acc@5 98.438 (98.940)
Epoch: [29][192/391]	Time 0.408 (0.432)	Data 0.002 (0.003)	Loss 0.8266 (0.8116)	Acc@1 80.469 (81.048)	Acc@5 97.656 (98.968)
Epoch: [29][256/391]	Time 0.474 (0.433)	Data 0.002 (0.003)	Loss 0.7812 (0.8157)	Acc@1 78.906 (80.952)	Acc@5 99.219 (99.000)
Epoch: [29][320/391]	Time 0.435 (0.433)	Data 0.002 (0.003)	Loss 0.9385 (0.8167)	Acc@1 80.469 (80.902)	Acc@5 98.438 (99.014)
Epoch: [29][384/391]	Time 0.416 (0.432)	Data 0.002 (0.003)	Loss 0.7249 (0.8113)	Acc@1 83.594 (81.053)	Acc@5 99.219 (99.040)

Epoch: [30 | 180] LR: 0.100000
Epoch: [30][0/391]	Time 0.407 (0.407)	Data 0.236 (0.236)	Loss 1.0360 (1.0360)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [30][64/391]	Time 0.472 (0.434)	Data 0.002 (0.006)	Loss 0.9212 (0.7903)	Acc@1 77.344 (81.923)	Acc@5 99.219 (99.135)
Epoch: [30][128/391]	Time 0.454 (0.429)	Data 0.002 (0.004)	Loss 0.8971 (0.7979)	Acc@1 75.781 (81.529)	Acc@5 98.438 (99.122)
Epoch: [30][192/391]	Time 0.383 (0.433)	Data 0.002 (0.003)	Loss 0.7497 (0.8078)	Acc@1 84.375 (81.400)	Acc@5 99.219 (99.008)
Epoch: [30][256/391]	Time 0.391 (0.429)	Data 0.001 (0.003)	Loss 0.7741 (0.8090)	Acc@1 87.500 (81.381)	Acc@5 99.219 (99.058)
Epoch: [30][320/391]	Time 0.413 (0.429)	Data 0.002 (0.003)	Loss 0.7555 (0.8060)	Acc@1 81.250 (81.381)	Acc@5 99.219 (99.065)
Epoch: [30][384/391]	Time 0.443 (0.429)	Data 0.002 (0.003)	Loss 0.7358 (0.8072)	Acc@1 82.031 (81.303)	Acc@5 97.656 (99.048)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 264894 ; 487386 ; 0.5434994029373023

Epoch: [31 | 180] LR: 0.100000
Epoch: [31][0/391]	Time 0.448 (0.448)	Data 0.242 (0.242)	Loss 0.7213 (0.7213)	Acc@1 82.812 (82.812)	Acc@5 98.438 (98.438)
Epoch: [31][64/391]	Time 0.457 (0.428)	Data 0.002 (0.006)	Loss 0.7312 (0.7684)	Acc@1 84.375 (82.356)	Acc@5 99.219 (99.135)
Epoch: [31][128/391]	Time 0.445 (0.431)	Data 0.001 (0.004)	Loss 0.7668 (0.7746)	Acc@1 82.812 (82.249)	Acc@5 100.000 (99.049)
Epoch: [31][192/391]	Time 0.382 (0.428)	Data 0.002 (0.003)	Loss 0.9593 (0.7897)	Acc@1 73.438 (81.849)	Acc@5 97.656 (98.976)
Epoch: [31][256/391]	Time 0.435 (0.429)	Data 0.002 (0.003)	Loss 0.8967 (0.7908)	Acc@1 78.906 (81.800)	Acc@5 98.438 (98.991)
Epoch: [31][320/391]	Time 0.439 (0.429)	Data 0.002 (0.003)	Loss 0.7642 (0.7936)	Acc@1 81.250 (81.698)	Acc@5 99.219 (98.990)
Epoch: [31][384/391]	Time 0.425 (0.431)	Data 0.002 (0.003)	Loss 0.7638 (0.7936)	Acc@1 83.594 (81.674)	Acc@5 99.219 (98.989)

Epoch: [32 | 180] LR: 0.100000
Epoch: [32][0/391]	Time 0.451 (0.451)	Data 0.248 (0.248)	Loss 0.7101 (0.7101)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [32][64/391]	Time 0.449 (0.432)	Data 0.002 (0.006)	Loss 0.7439 (0.7732)	Acc@1 82.031 (82.957)	Acc@5 98.438 (99.147)
Epoch: [32][128/391]	Time 0.455 (0.431)	Data 0.002 (0.004)	Loss 0.7617 (0.7805)	Acc@1 85.156 (82.158)	Acc@5 99.219 (99.146)
Epoch: [32][192/391]	Time 0.416 (0.432)	Data 0.002 (0.003)	Loss 0.8275 (0.7742)	Acc@1 78.906 (82.379)	Acc@5 99.219 (99.215)
Epoch: [32][256/391]	Time 0.420 (0.433)	Data 0.002 (0.003)	Loss 0.8372 (0.7845)	Acc@1 79.688 (82.034)	Acc@5 99.219 (99.091)
Epoch: [32][320/391]	Time 0.416 (0.432)	Data 0.003 (0.003)	Loss 0.7379 (0.7888)	Acc@1 82.031 (81.895)	Acc@5 99.219 (99.068)
Epoch: [32][384/391]	Time 0.413 (0.433)	Data 0.002 (0.003)	Loss 0.8434 (0.7917)	Acc@1 76.562 (81.741)	Acc@5 100.000 (99.046)

Epoch: [33 | 180] LR: 0.100000
Epoch: [33][0/391]	Time 0.358 (0.358)	Data 0.268 (0.268)	Loss 0.7120 (0.7120)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [33][64/391]	Time 0.367 (0.393)	Data 0.002 (0.006)	Loss 0.8554 (0.7691)	Acc@1 81.250 (82.548)	Acc@5 97.656 (98.990)
Epoch: [33][128/391]	Time 0.350 (0.385)	Data 0.002 (0.004)	Loss 0.7030 (0.7827)	Acc@1 85.938 (82.128)	Acc@5 100.000 (98.977)
Epoch: [33][192/391]	Time 0.403 (0.384)	Data 0.002 (0.003)	Loss 0.6873 (0.7899)	Acc@1 86.719 (81.833)	Acc@5 100.000 (98.992)
Epoch: [33][256/391]	Time 0.359 (0.383)	Data 0.002 (0.003)	Loss 0.7557 (0.7866)	Acc@1 85.156 (81.882)	Acc@5 99.219 (99.049)
Epoch: [33][320/391]	Time 0.387 (0.383)	Data 0.002 (0.003)	Loss 0.8520 (0.7881)	Acc@1 78.906 (81.837)	Acc@5 99.219 (99.034)
Epoch: [33][384/391]	Time 0.381 (0.383)	Data 0.001 (0.003)	Loss 0.8877 (0.7916)	Acc@1 78.906 (81.711)	Acc@5 99.219 (99.024)

Epoch: [34 | 180] LR: 0.100000
Epoch: [34][0/391]	Time 0.350 (0.350)	Data 0.303 (0.303)	Loss 0.7815 (0.7815)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [34][64/391]	Time 0.356 (0.378)	Data 0.003 (0.007)	Loss 0.6816 (0.7677)	Acc@1 83.594 (82.788)	Acc@5 100.000 (99.111)
Epoch: [34][128/391]	Time 0.394 (0.378)	Data 0.002 (0.004)	Loss 0.6900 (0.7731)	Acc@1 84.375 (82.376)	Acc@5 100.000 (99.116)
Epoch: [34][192/391]	Time 0.361 (0.379)	Data 0.002 (0.004)	Loss 0.8593 (0.7832)	Acc@1 80.469 (81.983)	Acc@5 98.438 (99.089)
Epoch: [34][256/391]	Time 0.386 (0.378)	Data 0.003 (0.003)	Loss 0.7230 (0.7879)	Acc@1 84.375 (81.818)	Acc@5 98.438 (99.085)
Epoch: [34][320/391]	Time 0.376 (0.379)	Data 0.002 (0.003)	Loss 0.6939 (0.7868)	Acc@1 82.031 (81.815)	Acc@5 100.000 (99.109)
Epoch: [34][384/391]	Time 0.349 (0.379)	Data 0.002 (0.003)	Loss 0.7870 (0.7893)	Acc@1 82.812 (81.751)	Acc@5 100.000 (99.097)

Epoch: [35 | 180] LR: 0.100000
Epoch: [35][0/391]	Time 0.360 (0.360)	Data 0.208 (0.208)	Loss 0.6334 (0.6334)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [35][64/391]	Time 0.386 (0.378)	Data 0.002 (0.005)	Loss 0.7073 (0.7943)	Acc@1 82.812 (81.562)	Acc@5 99.219 (99.075)
Epoch: [35][128/391]	Time 0.397 (0.379)	Data 0.002 (0.004)	Loss 0.6743 (0.7809)	Acc@1 84.375 (82.122)	Acc@5 100.000 (99.049)
Epoch: [35][192/391]	Time 0.354 (0.380)	Data 0.002 (0.003)	Loss 0.8039 (0.7809)	Acc@1 82.031 (82.043)	Acc@5 97.656 (99.093)
Epoch: [35][256/391]	Time 0.357 (0.379)	Data 0.002 (0.003)	Loss 0.8616 (0.7844)	Acc@1 82.031 (81.967)	Acc@5 100.000 (99.109)
Epoch: [35][320/391]	Time 0.415 (0.379)	Data 0.002 (0.003)	Loss 0.6922 (0.7870)	Acc@1 85.938 (81.805)	Acc@5 100.000 (99.109)
Epoch: [35][384/391]	Time 0.384 (0.380)	Data 0.002 (0.002)	Loss 0.7087 (0.7870)	Acc@1 84.375 (81.788)	Acc@5 99.219 (99.073)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 259402 ; 487386 ; 0.5322311268686422

Epoch: [36 | 180] LR: 0.100000
Epoch: [36][0/391]	Time 0.432 (0.432)	Data 0.230 (0.230)	Loss 0.7606 (0.7606)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [36][64/391]	Time 0.379 (0.379)	Data 0.002 (0.006)	Loss 0.7770 (0.7507)	Acc@1 78.906 (82.716)	Acc@5 98.438 (99.219)
Epoch: [36][128/391]	Time 0.390 (0.379)	Data 0.002 (0.004)	Loss 0.8334 (0.7664)	Acc@1 75.781 (82.043)	Acc@5 100.000 (99.297)
Epoch: [36][192/391]	Time 0.395 (0.378)	Data 0.002 (0.003)	Loss 0.8089 (0.7713)	Acc@1 78.906 (82.056)	Acc@5 99.219 (99.263)
Epoch: [36][256/391]	Time 0.335 (0.377)	Data 0.002 (0.003)	Loss 0.6964 (0.7808)	Acc@1 83.594 (81.758)	Acc@5 99.219 (99.197)
Epoch: [36][320/391]	Time 0.448 (0.378)	Data 0.002 (0.003)	Loss 0.6910 (0.7763)	Acc@1 83.594 (81.924)	Acc@5 100.000 (99.185)
Epoch: [36][384/391]	Time 0.341 (0.378)	Data 0.002 (0.003)	Loss 0.8200 (0.7802)	Acc@1 78.906 (81.786)	Acc@5 98.438 (99.166)

Epoch: [37 | 180] LR: 0.100000
Epoch: [37][0/391]	Time 0.369 (0.369)	Data 0.230 (0.230)	Loss 0.7205 (0.7205)	Acc@1 83.594 (83.594)	Acc@5 98.438 (98.438)
Epoch: [37][64/391]	Time 0.334 (0.384)	Data 0.002 (0.006)	Loss 0.8400 (0.7913)	Acc@1 82.812 (81.635)	Acc@5 99.219 (99.050)
Epoch: [37][128/391]	Time 0.386 (0.380)	Data 0.002 (0.004)	Loss 0.6667 (0.7854)	Acc@1 85.938 (81.946)	Acc@5 100.000 (99.116)
Epoch: [37][192/391]	Time 0.403 (0.379)	Data 0.002 (0.003)	Loss 0.7217 (0.7825)	Acc@1 83.594 (82.177)	Acc@5 99.219 (99.105)
Epoch: [37][256/391]	Time 0.380 (0.378)	Data 0.002 (0.003)	Loss 0.7181 (0.7796)	Acc@1 85.938 (82.174)	Acc@5 99.219 (99.152)
Epoch: [37][320/391]	Time 0.356 (0.378)	Data 0.001 (0.003)	Loss 0.7560 (0.7818)	Acc@1 83.594 (82.097)	Acc@5 100.000 (99.136)
Epoch: [37][384/391]	Time 0.395 (0.380)	Data 0.002 (0.003)	Loss 0.6372 (0.7838)	Acc@1 86.719 (81.909)	Acc@5 99.219 (99.121)

Epoch: [38 | 180] LR: 0.100000
Epoch: [38][0/391]	Time 0.409 (0.409)	Data 0.234 (0.234)	Loss 0.5722 (0.5722)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [38][64/391]	Time 0.411 (0.377)	Data 0.002 (0.006)	Loss 0.6289 (0.7633)	Acc@1 89.844 (83.017)	Acc@5 98.438 (99.267)
Epoch: [38][128/391]	Time 0.362 (0.375)	Data 0.002 (0.004)	Loss 0.8116 (0.7784)	Acc@1 80.469 (82.304)	Acc@5 98.438 (99.170)
Epoch: [38][192/391]	Time 0.394 (0.377)	Data 0.002 (0.003)	Loss 0.7392 (0.7730)	Acc@1 85.156 (82.472)	Acc@5 99.219 (99.207)
Epoch: [38][256/391]	Time 0.339 (0.376)	Data 0.002 (0.003)	Loss 0.6913 (0.7748)	Acc@1 81.250 (82.311)	Acc@5 100.000 (99.170)
Epoch: [38][320/391]	Time 0.356 (0.377)	Data 0.002 (0.003)	Loss 0.8525 (0.7743)	Acc@1 76.562 (82.270)	Acc@5 96.875 (99.180)
Epoch: [38][384/391]	Time 0.394 (0.377)	Data 0.002 (0.003)	Loss 0.8200 (0.7750)	Acc@1 82.031 (82.242)	Acc@5 99.219 (99.166)

Epoch: [39 | 180] LR: 0.100000
Epoch: [39][0/391]	Time 0.456 (0.456)	Data 0.226 (0.226)	Loss 0.7570 (0.7570)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [39][64/391]	Time 0.374 (0.380)	Data 0.002 (0.005)	Loss 0.7640 (0.7648)	Acc@1 81.250 (82.524)	Acc@5 100.000 (99.219)
Epoch: [39][128/391]	Time 0.368 (0.379)	Data 0.002 (0.004)	Loss 0.8584 (0.7619)	Acc@1 78.125 (82.570)	Acc@5 99.219 (99.255)
Epoch: [39][192/391]	Time 0.432 (0.379)	Data 0.002 (0.003)	Loss 0.8533 (0.7713)	Acc@1 76.562 (82.274)	Acc@5 98.438 (99.162)
Epoch: [39][256/391]	Time 0.415 (0.377)	Data 0.002 (0.003)	Loss 0.6600 (0.7779)	Acc@1 86.719 (82.062)	Acc@5 100.000 (99.106)
Epoch: [39][320/391]	Time 0.360 (0.376)	Data 0.002 (0.003)	Loss 0.7791 (0.7732)	Acc@1 82.812 (82.282)	Acc@5 98.438 (99.109)
Epoch: [39][384/391]	Time 0.399 (0.376)	Data 0.002 (0.003)	Loss 0.8036 (0.7796)	Acc@1 82.031 (82.104)	Acc@5 100.000 (99.077)

Epoch: [40 | 180] LR: 0.100000
Epoch: [40][0/391]	Time 0.492 (0.492)	Data 0.301 (0.301)	Loss 0.7372 (0.7372)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [40][64/391]	Time 0.364 (0.375)	Data 0.002 (0.006)	Loss 0.8295 (0.7621)	Acc@1 82.812 (82.548)	Acc@5 99.219 (99.327)
Epoch: [40][128/391]	Time 0.402 (0.378)	Data 0.002 (0.004)	Loss 0.7000 (0.7643)	Acc@1 82.031 (82.558)	Acc@5 98.438 (99.176)
Epoch: [40][192/391]	Time 0.394 (0.378)	Data 0.002 (0.004)	Loss 0.8107 (0.7726)	Acc@1 82.031 (82.234)	Acc@5 100.000 (99.194)
Epoch: [40][256/391]	Time 0.356 (0.376)	Data 0.003 (0.003)	Loss 0.7028 (0.7766)	Acc@1 84.375 (82.144)	Acc@5 97.656 (99.173)
Epoch: [40][320/391]	Time 0.394 (0.376)	Data 0.003 (0.003)	Loss 0.5770 (0.7782)	Acc@1 90.625 (82.180)	Acc@5 100.000 (99.136)
Epoch: [40][384/391]	Time 0.380 (0.375)	Data 0.002 (0.003)	Loss 0.7903 (0.7779)	Acc@1 82.031 (82.119)	Acc@5 99.219 (99.164)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv21.weight

 RM:  module.conv22.weight

Module List Length:  60
Index1: 42
Index: 22
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(58, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 54 gegen 58: Conv2d(64, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 55 gegen 59: BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(58, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): AdaptiveAvgPool2d(output_size=(1, 1))
    (55): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  56

Module List Length:  56
Index1: 40
Index: 21
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 50 gegen 54: Conv2d(64, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 51 gegen 55: BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  52
Count: 137820 ; 487386 ; 0.28277381787741135

Epoch: [41 | 180] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 318, in main
    use_gpu_num)
  File "main.py", line 442, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 459, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 107, in forward
    exponential_average_factor, self.eps)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 1 elements not 0
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.537 (0.537)	Data 0.185 (0.185)	Loss 3.2711 (3.2711)	Acc@1 9.375 (9.375)	Acc@5 44.531 (44.531)
Epoch: [1][64/391]	Time 0.402 (0.405)	Data 0.002 (0.005)	Loss 2.5478 (2.7758)	Acc@1 27.344 (19.952)	Acc@5 78.125 (73.810)
Epoch: [1][128/391]	Time 0.419 (0.405)	Data 0.002 (0.003)	Loss 2.3698 (2.6180)	Acc@1 32.812 (24.746)	Acc@5 85.938 (79.209)
Epoch: [1][192/391]	Time 0.415 (0.402)	Data 0.001 (0.003)	Loss 2.2338 (2.5182)	Acc@1 36.719 (27.639)	Acc@5 92.188 (82.047)
Epoch: [1][256/391]	Time 0.411 (0.401)	Data 0.002 (0.003)	Loss 2.2287 (2.4465)	Acc@1 42.188 (30.004)	Acc@5 85.938 (83.776)
Epoch: [1][320/391]	Time 0.428 (0.401)	Data 0.002 (0.002)	Loss 1.9734 (2.3838)	Acc@1 43.750 (32.258)	Acc@5 92.969 (85.112)
Epoch: [1][384/391]	Time 0.420 (0.402)	Data 0.002 (0.002)	Loss 1.7407 (2.3206)	Acc@1 64.062 (34.450)	Acc@5 92.969 (86.191)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.398 (0.398)	Data 0.247 (0.247)	Loss 1.8709 (1.8709)	Acc@1 46.094 (46.094)	Acc@5 96.094 (96.094)
Epoch: [2][64/391]	Time 0.375 (0.411)	Data 0.001 (0.006)	Loss 1.8037 (1.8882)	Acc@1 54.688 (48.041)	Acc@5 94.531 (93.173)
Epoch: [2][128/391]	Time 0.388 (0.407)	Data 0.001 (0.004)	Loss 1.7122 (1.8391)	Acc@1 52.344 (50.164)	Acc@5 93.750 (93.611)
Epoch: [2][192/391]	Time 0.393 (0.406)	Data 0.002 (0.003)	Loss 1.6392 (1.8049)	Acc@1 60.156 (51.453)	Acc@5 94.531 (93.839)
Epoch: [2][256/391]	Time 0.424 (0.405)	Data 0.002 (0.003)	Loss 1.6381 (1.7666)	Acc@1 57.031 (52.742)	Acc@5 96.875 (94.173)
Epoch: [2][320/391]	Time 0.389 (0.405)	Data 0.002 (0.003)	Loss 1.4394 (1.7260)	Acc@1 67.188 (54.021)	Acc@5 92.969 (94.397)
Epoch: [2][384/391]	Time 0.421 (0.405)	Data 0.002 (0.002)	Loss 1.5990 (1.6896)	Acc@1 60.938 (55.099)	Acc@5 92.188 (94.653)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.467 (0.467)	Data 0.248 (0.248)	Loss 1.4708 (1.4708)	Acc@1 61.719 (61.719)	Acc@5 97.656 (97.656)
Epoch: [3][64/391]	Time 0.414 (0.413)	Data 0.002 (0.006)	Loss 1.5587 (1.4487)	Acc@1 60.938 (62.849)	Acc@5 95.312 (96.370)
Epoch: [3][128/391]	Time 0.440 (0.411)	Data 0.002 (0.004)	Loss 1.3592 (1.4213)	Acc@1 61.719 (63.723)	Acc@5 100.000 (96.584)
Epoch: [3][192/391]	Time 0.387 (0.407)	Data 0.002 (0.003)	Loss 1.3578 (1.3966)	Acc@1 65.625 (64.605)	Acc@5 96.094 (96.741)
Epoch: [3][256/391]	Time 0.433 (0.408)	Data 0.002 (0.003)	Loss 1.2730 (1.3769)	Acc@1 64.844 (65.139)	Acc@5 96.094 (96.893)
Epoch: [3][320/391]	Time 0.385 (0.407)	Data 0.001 (0.003)	Loss 1.1500 (1.3562)	Acc@1 78.906 (65.722)	Acc@5 99.219 (97.023)
Epoch: [3][384/391]	Time 0.363 (0.407)	Data 0.002 (0.003)	Loss 1.2914 (1.3375)	Acc@1 69.531 (66.222)	Acc@5 95.312 (97.141)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.414 (0.414)	Data 0.257 (0.257)	Loss 1.0166 (1.0166)	Acc@1 78.125 (78.125)	Acc@5 99.219 (99.219)
Epoch: [4][64/391]	Time 0.384 (0.411)	Data 0.002 (0.006)	Loss 1.0374 (1.1739)	Acc@1 75.781 (71.298)	Acc@5 100.000 (97.680)
Epoch: [4][128/391]	Time 0.397 (0.408)	Data 0.002 (0.004)	Loss 1.0708 (1.1630)	Acc@1 75.000 (71.463)	Acc@5 100.000 (97.983)
Epoch: [4][192/391]	Time 0.404 (0.408)	Data 0.002 (0.003)	Loss 1.0200 (1.1567)	Acc@1 76.562 (71.571)	Acc@5 99.219 (97.887)
Epoch: [4][256/391]	Time 0.405 (0.405)	Data 0.002 (0.003)	Loss 1.1526 (1.1454)	Acc@1 72.656 (71.957)	Acc@5 100.000 (97.936)
Epoch: [4][320/391]	Time 0.403 (0.405)	Data 0.002 (0.003)	Loss 1.1544 (1.1386)	Acc@1 75.000 (72.194)	Acc@5 96.094 (97.963)
Epoch: [4][384/391]	Time 0.394 (0.406)	Data 0.002 (0.003)	Loss 0.9491 (1.1328)	Acc@1 75.000 (72.299)	Acc@5 100.000 (97.967)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.472 (0.472)	Data 0.232 (0.232)	Loss 1.2178 (1.2178)	Acc@1 68.750 (68.750)	Acc@5 96.875 (96.875)
Epoch: [5][64/391]	Time 0.422 (0.409)	Data 0.002 (0.005)	Loss 1.0566 (1.0803)	Acc@1 74.219 (73.858)	Acc@5 100.000 (98.089)
Epoch: [5][128/391]	Time 0.399 (0.407)	Data 0.002 (0.004)	Loss 1.1110 (1.0551)	Acc@1 73.438 (74.376)	Acc@5 99.219 (98.280)
Epoch: [5][192/391]	Time 0.403 (0.406)	Data 0.002 (0.003)	Loss 1.1255 (1.0520)	Acc@1 75.000 (74.462)	Acc@5 93.750 (98.243)
Epoch: [5][256/391]	Time 0.399 (0.405)	Data 0.002 (0.003)	Loss 1.0803 (1.0472)	Acc@1 73.438 (74.617)	Acc@5 99.219 (98.258)
Epoch: [5][320/391]	Time 0.495 (0.406)	Data 0.002 (0.003)	Loss 1.0790 (1.0438)	Acc@1 70.312 (74.800)	Acc@5 100.000 (98.299)
Epoch: [5][384/391]	Time 0.363 (0.407)	Data 0.002 (0.003)	Loss 1.0940 (1.0422)	Acc@1 75.000 (74.878)	Acc@5 99.219 (98.322)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.454 (0.454)	Data 0.254 (0.254)	Loss 1.0567 (1.0567)	Acc@1 75.000 (75.000)	Acc@5 99.219 (99.219)
Epoch: [6][64/391]	Time 0.408 (0.408)	Data 0.002 (0.006)	Loss 1.0820 (0.9555)	Acc@1 71.875 (77.752)	Acc@5 97.656 (98.654)
Epoch: [6][128/391]	Time 0.361 (0.407)	Data 0.001 (0.004)	Loss 1.1060 (0.9777)	Acc@1 70.312 (76.926)	Acc@5 99.219 (98.601)
Epoch: [6][192/391]	Time 0.384 (0.404)	Data 0.002 (0.003)	Loss 0.8831 (0.9813)	Acc@1 77.344 (76.842)	Acc@5 98.438 (98.498)
Epoch: [6][256/391]	Time 0.390 (0.406)	Data 0.002 (0.003)	Loss 0.8885 (0.9772)	Acc@1 79.688 (76.988)	Acc@5 98.438 (98.526)
Epoch: [6][320/391]	Time 0.446 (0.407)	Data 0.002 (0.003)	Loss 1.0479 (0.9768)	Acc@1 80.469 (77.005)	Acc@5 96.875 (98.496)
Epoch: [6][384/391]	Time 0.325 (0.406)	Data 0.002 (0.003)	Loss 0.8383 (0.9773)	Acc@1 79.688 (76.954)	Acc@5 100.000 (98.519)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.446 (0.446)	Data 0.230 (0.230)	Loss 0.8435 (0.8435)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [7][64/391]	Time 0.408 (0.409)	Data 0.002 (0.005)	Loss 0.9581 (0.9418)	Acc@1 76.562 (78.173)	Acc@5 100.000 (98.774)
Epoch: [7][128/391]	Time 0.362 (0.409)	Data 0.002 (0.004)	Loss 0.9323 (0.9498)	Acc@1 79.688 (77.750)	Acc@5 100.000 (98.716)
Epoch: [7][192/391]	Time 0.404 (0.405)	Data 0.002 (0.003)	Loss 0.8559 (0.9558)	Acc@1 80.469 (77.498)	Acc@5 98.438 (98.656)
Epoch: [7][256/391]	Time 0.379 (0.405)	Data 0.002 (0.003)	Loss 0.9839 (0.9623)	Acc@1 77.344 (77.222)	Acc@5 99.219 (98.602)
Epoch: [7][320/391]	Time 0.381 (0.404)	Data 0.002 (0.003)	Loss 0.9605 (0.9563)	Acc@1 78.125 (77.480)	Acc@5 98.438 (98.605)
Epoch: [7][384/391]	Time 0.388 (0.404)	Data 0.002 (0.003)	Loss 0.9545 (0.9541)	Acc@1 74.219 (77.530)	Acc@5 98.438 (98.620)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.455 (0.455)	Data 0.202 (0.202)	Loss 0.9401 (0.9401)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [8][64/391]	Time 0.430 (0.420)	Data 0.002 (0.005)	Loss 1.0333 (0.9502)	Acc@1 75.000 (77.716)	Acc@5 97.656 (98.486)
Epoch: [8][128/391]	Time 0.414 (0.415)	Data 0.002 (0.003)	Loss 1.0104 (0.9482)	Acc@1 76.562 (77.683)	Acc@5 96.875 (98.607)
Epoch: [8][192/391]	Time 0.465 (0.411)	Data 0.002 (0.003)	Loss 1.0695 (0.9396)	Acc@1 75.000 (77.963)	Acc@5 99.219 (98.688)
Epoch: [8][256/391]	Time 0.438 (0.410)	Data 0.002 (0.003)	Loss 1.0411 (0.9365)	Acc@1 71.094 (77.991)	Acc@5 98.438 (98.729)
Epoch: [8][320/391]	Time 0.394 (0.410)	Data 0.002 (0.003)	Loss 1.0310 (0.9384)	Acc@1 76.562 (78.023)	Acc@5 96.094 (98.737)
Epoch: [8][384/391]	Time 0.417 (0.409)	Data 0.002 (0.002)	Loss 0.9972 (0.9379)	Acc@1 77.344 (78.151)	Acc@5 96.875 (98.705)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.594 (0.594)	Data 0.338 (0.338)	Loss 0.9631 (0.9631)	Acc@1 75.781 (75.781)	Acc@5 97.656 (97.656)
Epoch: [9][64/391]	Time 0.415 (0.406)	Data 0.002 (0.007)	Loss 0.8926 (0.9481)	Acc@1 80.469 (78.257)	Acc@5 97.656 (98.510)
Epoch: [9][128/391]	Time 0.396 (0.406)	Data 0.002 (0.005)	Loss 0.9196 (0.9443)	Acc@1 74.219 (78.137)	Acc@5 99.219 (98.571)
Epoch: [9][192/391]	Time 0.418 (0.405)	Data 0.003 (0.004)	Loss 0.7815 (0.9430)	Acc@1 82.812 (77.951)	Acc@5 98.438 (98.547)
Epoch: [9][256/391]	Time 0.409 (0.405)	Data 0.002 (0.003)	Loss 1.0212 (0.9362)	Acc@1 76.562 (78.198)	Acc@5 98.438 (98.620)
Epoch: [9][320/391]	Time 0.420 (0.405)	Data 0.002 (0.003)	Loss 0.8893 (0.9349)	Acc@1 82.812 (78.286)	Acc@5 97.656 (98.610)
Epoch: [9][384/391]	Time 0.384 (0.405)	Data 0.001 (0.003)	Loss 0.9881 (0.9383)	Acc@1 78.906 (78.188)	Acc@5 99.219 (98.608)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.553 (0.553)	Data 0.291 (0.291)	Loss 0.8705 (0.8705)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.401 (0.406)	Data 0.001 (0.006)	Loss 0.9152 (0.8930)	Acc@1 82.031 (80.132)	Acc@5 97.656 (98.822)
Epoch: [10][128/391]	Time 0.443 (0.410)	Data 0.002 (0.004)	Loss 0.8587 (0.9015)	Acc@1 80.469 (79.645)	Acc@5 100.000 (98.843)
Epoch: [10][192/391]	Time 0.398 (0.408)	Data 0.002 (0.003)	Loss 0.9724 (0.9068)	Acc@1 77.344 (79.331)	Acc@5 99.219 (98.806)
Epoch: [10][256/391]	Time 0.396 (0.406)	Data 0.002 (0.003)	Loss 0.9130 (0.9100)	Acc@1 79.688 (79.216)	Acc@5 99.219 (98.802)
Epoch: [10][320/391]	Time 0.408 (0.407)	Data 0.002 (0.003)	Loss 0.7778 (0.9111)	Acc@1 83.594 (79.210)	Acc@5 100.000 (98.764)
Epoch: [10][384/391]	Time 0.364 (0.406)	Data 0.002 (0.003)	Loss 0.8166 (0.9113)	Acc@1 85.156 (79.215)	Acc@5 100.000 (98.748)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.425 (0.425)	Data 0.254 (0.254)	Loss 0.9253 (0.9253)	Acc@1 76.562 (76.562)	Acc@5 100.000 (100.000)
Epoch: [11][64/391]	Time 0.404 (0.407)	Data 0.002 (0.006)	Loss 1.0046 (0.8887)	Acc@1 75.781 (80.397)	Acc@5 98.438 (99.050)
Epoch: [11][128/391]	Time 0.423 (0.408)	Data 0.002 (0.004)	Loss 0.9623 (0.9068)	Acc@1 75.781 (79.899)	Acc@5 99.219 (98.946)
Epoch: [11][192/391]	Time 0.390 (0.406)	Data 0.002 (0.003)	Loss 0.9530 (0.9154)	Acc@1 76.562 (79.558)	Acc@5 99.219 (98.879)
Epoch: [11][256/391]	Time 0.427 (0.404)	Data 0.002 (0.003)	Loss 0.8982 (0.9191)	Acc@1 80.469 (79.508)	Acc@5 98.438 (98.872)
Epoch: [11][320/391]	Time 0.444 (0.405)	Data 0.002 (0.003)	Loss 1.1030 (0.9250)	Acc@1 76.562 (79.257)	Acc@5 98.438 (98.854)
Epoch: [11][384/391]	Time 0.405 (0.406)	Data 0.002 (0.003)	Loss 0.7665 (0.9232)	Acc@1 85.156 (79.345)	Acc@5 100.000 (98.884)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.429 (0.429)	Data 0.218 (0.218)	Loss 0.9228 (0.9228)	Acc@1 77.344 (77.344)	Acc@5 97.656 (97.656)
Epoch: [12][64/391]	Time 0.405 (0.398)	Data 0.002 (0.005)	Loss 0.8122 (0.8857)	Acc@1 84.375 (80.240)	Acc@5 100.000 (99.062)
Epoch: [12][128/391]	Time 0.414 (0.402)	Data 0.002 (0.004)	Loss 0.7869 (0.8895)	Acc@1 85.156 (80.142)	Acc@5 98.438 (99.007)
Epoch: [12][192/391]	Time 0.423 (0.402)	Data 0.002 (0.003)	Loss 0.9316 (0.9020)	Acc@1 75.000 (79.570)	Acc@5 99.219 (98.948)
Epoch: [12][256/391]	Time 0.389 (0.404)	Data 0.002 (0.003)	Loss 0.8200 (0.9083)	Acc@1 83.594 (79.575)	Acc@5 99.219 (98.869)
Epoch: [12][320/391]	Time 0.371 (0.404)	Data 0.002 (0.003)	Loss 0.8914 (0.9094)	Acc@1 82.031 (79.483)	Acc@5 99.219 (98.866)
Epoch: [12][384/391]	Time 0.411 (0.404)	Data 0.002 (0.003)	Loss 0.6752 (0.9067)	Acc@1 88.281 (79.641)	Acc@5 100.000 (98.866)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.458 (0.458)	Data 0.258 (0.258)	Loss 0.9979 (0.9979)	Acc@1 78.125 (78.125)	Acc@5 99.219 (99.219)
Epoch: [13][64/391]	Time 0.416 (0.405)	Data 0.002 (0.006)	Loss 0.8947 (0.8856)	Acc@1 77.344 (80.132)	Acc@5 97.656 (98.882)
Epoch: [13][128/391]	Time 0.395 (0.404)	Data 0.002 (0.004)	Loss 0.8757 (0.8979)	Acc@1 81.250 (79.760)	Acc@5 98.438 (98.867)
Epoch: [13][192/391]	Time 0.393 (0.403)	Data 0.002 (0.003)	Loss 0.8535 (0.8891)	Acc@1 78.125 (80.044)	Acc@5 99.219 (98.871)
Epoch: [13][256/391]	Time 0.395 (0.403)	Data 0.002 (0.003)	Loss 0.9006 (0.8927)	Acc@1 78.125 (79.909)	Acc@5 98.438 (98.857)
Epoch: [13][320/391]	Time 0.394 (0.403)	Data 0.002 (0.003)	Loss 0.8259 (0.8952)	Acc@1 82.031 (79.899)	Acc@5 97.656 (98.856)
Epoch: [13][384/391]	Time 0.451 (0.403)	Data 0.002 (0.003)	Loss 0.9469 (0.8963)	Acc@1 73.438 (79.939)	Acc@5 99.219 (98.827)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.475 (0.475)	Data 0.229 (0.229)	Loss 0.9486 (0.9486)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [14][64/391]	Time 0.393 (0.408)	Data 0.002 (0.006)	Loss 0.9009 (0.8730)	Acc@1 78.125 (80.481)	Acc@5 99.219 (99.062)
Epoch: [14][128/391]	Time 0.413 (0.407)	Data 0.002 (0.004)	Loss 0.8713 (0.8784)	Acc@1 81.250 (80.354)	Acc@5 99.219 (98.995)
Epoch: [14][192/391]	Time 0.422 (0.405)	Data 0.002 (0.003)	Loss 1.0617 (0.8737)	Acc@1 72.656 (80.590)	Acc@5 98.438 (99.000)
Epoch: [14][256/391]	Time 0.377 (0.405)	Data 0.001 (0.003)	Loss 0.8256 (0.8737)	Acc@1 81.250 (80.666)	Acc@5 99.219 (98.988)
Epoch: [14][320/391]	Time 0.352 (0.405)	Data 0.002 (0.003)	Loss 0.6985 (0.8826)	Acc@1 85.156 (80.371)	Acc@5 100.000 (98.961)
Epoch: [14][384/391]	Time 0.435 (0.406)	Data 0.002 (0.003)	Loss 0.8816 (0.8824)	Acc@1 79.688 (80.315)	Acc@5 97.656 (98.975)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.558 (0.558)	Data 0.292 (0.292)	Loss 0.9882 (0.9882)	Acc@1 78.906 (78.906)	Acc@5 97.656 (97.656)
Epoch: [15][64/391]	Time 0.362 (0.419)	Data 0.002 (0.007)	Loss 0.8248 (0.9041)	Acc@1 80.469 (79.279)	Acc@5 98.438 (98.750)
Epoch: [15][128/391]	Time 0.386 (0.413)	Data 0.003 (0.004)	Loss 0.9114 (0.8829)	Acc@1 81.250 (80.408)	Acc@5 100.000 (98.789)
Epoch: [15][192/391]	Time 0.449 (0.410)	Data 0.001 (0.004)	Loss 0.7354 (0.8871)	Acc@1 86.719 (80.141)	Acc@5 98.438 (98.842)
Epoch: [15][256/391]	Time 0.365 (0.409)	Data 0.004 (0.003)	Loss 0.9330 (0.8839)	Acc@1 81.250 (80.317)	Acc@5 97.656 (98.869)
Epoch: [15][320/391]	Time 0.416 (0.410)	Data 0.002 (0.003)	Loss 0.7791 (0.8843)	Acc@1 83.594 (80.306)	Acc@5 99.219 (98.907)
Epoch: [15][384/391]	Time 0.376 (0.410)	Data 0.002 (0.003)	Loss 0.8536 (0.8871)	Acc@1 81.250 (80.244)	Acc@5 99.219 (98.866)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 424456 ; 487386 ; 0.8708826269117291

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.429 (0.429)	Data 0.248 (0.248)	Loss 0.7398 (0.7398)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [16][64/391]	Time 0.407 (0.408)	Data 0.002 (0.006)	Loss 0.8267 (0.8619)	Acc@1 80.469 (80.998)	Acc@5 99.219 (98.978)
Epoch: [16][128/391]	Time 0.430 (0.401)	Data 0.002 (0.004)	Loss 0.8693 (0.8563)	Acc@1 78.906 (80.723)	Acc@5 99.219 (98.964)
Epoch: [16][192/391]	Time 0.460 (0.402)	Data 0.002 (0.003)	Loss 0.6872 (0.8537)	Acc@1 88.281 (80.793)	Acc@5 98.438 (98.931)
Epoch: [16][256/391]	Time 0.380 (0.405)	Data 0.002 (0.003)	Loss 0.7449 (0.8583)	Acc@1 84.375 (80.645)	Acc@5 100.000 (98.951)
Epoch: [16][320/391]	Time 0.401 (0.408)	Data 0.002 (0.003)	Loss 0.9363 (0.8540)	Acc@1 77.344 (80.768)	Acc@5 99.219 (98.949)
Epoch: [16][384/391]	Time 0.418 (0.408)	Data 0.002 (0.003)	Loss 0.7933 (0.8539)	Acc@1 80.469 (80.787)	Acc@5 100.000 (98.957)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.467 (0.467)	Data 0.240 (0.240)	Loss 0.8137 (0.8137)	Acc@1 82.812 (82.812)	Acc@5 96.875 (96.875)
Epoch: [17][64/391]	Time 0.390 (0.405)	Data 0.002 (0.006)	Loss 0.8594 (0.8257)	Acc@1 80.469 (81.911)	Acc@5 99.219 (99.075)
Epoch: [17][128/391]	Time 0.375 (0.402)	Data 0.002 (0.004)	Loss 0.8826 (0.8330)	Acc@1 80.469 (81.468)	Acc@5 99.219 (99.140)
Epoch: [17][192/391]	Time 0.420 (0.403)	Data 0.002 (0.003)	Loss 0.8212 (0.8416)	Acc@1 82.812 (81.193)	Acc@5 99.219 (99.114)
Epoch: [17][256/391]	Time 0.400 (0.404)	Data 0.002 (0.003)	Loss 0.9780 (0.8426)	Acc@1 78.906 (81.198)	Acc@5 98.438 (99.064)
Epoch: [17][320/391]	Time 0.389 (0.404)	Data 0.002 (0.003)	Loss 0.8568 (0.8464)	Acc@1 75.000 (81.031)	Acc@5 100.000 (98.988)
Epoch: [17][384/391]	Time 0.355 (0.405)	Data 0.001 (0.003)	Loss 0.8827 (0.8469)	Acc@1 74.219 (80.968)	Acc@5 100.000 (98.973)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.461 (0.461)	Data 0.280 (0.280)	Loss 0.8643 (0.8643)	Acc@1 81.250 (81.250)	Acc@5 96.875 (96.875)
Epoch: [18][64/391]	Time 0.385 (0.400)	Data 0.002 (0.006)	Loss 0.8620 (0.8414)	Acc@1 81.250 (81.178)	Acc@5 98.438 (98.846)
Epoch: [18][128/391]	Time 0.451 (0.403)	Data 0.002 (0.004)	Loss 0.8254 (0.8456)	Acc@1 82.812 (80.790)	Acc@5 98.438 (98.874)
Epoch: [18][192/391]	Time 0.408 (0.403)	Data 0.002 (0.003)	Loss 0.8420 (0.8433)	Acc@1 80.469 (80.829)	Acc@5 100.000 (98.976)
Epoch: [18][256/391]	Time 0.369 (0.403)	Data 0.002 (0.003)	Loss 1.0636 (0.8465)	Acc@1 74.219 (80.748)	Acc@5 99.219 (98.948)
Epoch: [18][320/391]	Time 0.468 (0.402)	Data 0.002 (0.003)	Loss 0.7436 (0.8461)	Acc@1 87.500 (80.831)	Acc@5 100.000 (98.953)
Epoch: [18][384/391]	Time 0.419 (0.404)	Data 0.002 (0.003)	Loss 0.7526 (0.8489)	Acc@1 85.938 (80.789)	Acc@5 98.438 (98.929)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.412 (0.412)	Data 0.232 (0.232)	Loss 0.8892 (0.8892)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [19][64/391]	Time 0.461 (0.404)	Data 0.002 (0.006)	Loss 0.7248 (0.8225)	Acc@1 83.594 (81.430)	Acc@5 99.219 (99.099)
Epoch: [19][128/391]	Time 0.435 (0.406)	Data 0.002 (0.004)	Loss 0.8783 (0.8368)	Acc@1 79.688 (81.117)	Acc@5 96.875 (99.007)
Epoch: [19][192/391]	Time 0.372 (0.404)	Data 0.002 (0.003)	Loss 0.7257 (0.8374)	Acc@1 89.062 (81.133)	Acc@5 100.000 (99.020)
Epoch: [19][256/391]	Time 0.447 (0.404)	Data 0.002 (0.003)	Loss 0.9179 (0.8414)	Acc@1 77.344 (80.992)	Acc@5 98.438 (99.055)
Epoch: [19][320/391]	Time 0.412 (0.405)	Data 0.002 (0.003)	Loss 0.8052 (0.8416)	Acc@1 82.031 (81.016)	Acc@5 100.000 (99.039)
Epoch: [19][384/391]	Time 0.440 (0.406)	Data 0.001 (0.003)	Loss 0.7893 (0.8461)	Acc@1 83.594 (80.802)	Acc@5 98.438 (98.996)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.430 (0.430)	Data 0.268 (0.268)	Loss 0.8047 (0.8047)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [20][64/391]	Time 0.423 (0.409)	Data 0.002 (0.006)	Loss 0.9961 (0.8423)	Acc@1 71.875 (80.769)	Acc@5 100.000 (99.038)
Epoch: [20][128/391]	Time 0.392 (0.409)	Data 0.002 (0.004)	Loss 0.6818 (0.8359)	Acc@1 86.719 (80.875)	Acc@5 99.219 (99.019)
Epoch: [20][192/391]	Time 0.437 (0.406)	Data 0.002 (0.003)	Loss 0.7973 (0.8365)	Acc@1 84.375 (81.035)	Acc@5 99.219 (99.061)
Epoch: [20][256/391]	Time 0.372 (0.406)	Data 0.003 (0.003)	Loss 0.8680 (0.8386)	Acc@1 82.812 (81.092)	Acc@5 98.438 (99.036)
Epoch: [20][320/391]	Time 0.395 (0.406)	Data 0.002 (0.003)	Loss 0.9839 (0.8382)	Acc@1 77.344 (81.087)	Acc@5 98.438 (99.014)
Epoch: [20][384/391]	Time 0.421 (0.407)	Data 0.002 (0.003)	Loss 0.9212 (0.8406)	Acc@1 78.125 (81.031)	Acc@5 100.000 (98.983)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 371484 ; 487386 ; 0.7621966983048344

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.412 (0.412)	Data 0.208 (0.208)	Loss 0.8608 (0.8608)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [21][64/391]	Time 0.416 (0.408)	Data 0.002 (0.005)	Loss 0.8252 (0.7807)	Acc@1 79.688 (83.462)	Acc@5 98.438 (99.315)
Epoch: [21][128/391]	Time 0.405 (0.404)	Data 0.003 (0.004)	Loss 0.7983 (0.8102)	Acc@1 84.375 (82.625)	Acc@5 98.438 (99.134)
Epoch: [21][192/391]	Time 0.410 (0.405)	Data 0.002 (0.003)	Loss 0.9042 (0.8255)	Acc@1 76.562 (82.011)	Acc@5 99.219 (99.101)
Epoch: [21][256/391]	Time 0.411 (0.406)	Data 0.002 (0.003)	Loss 0.8877 (0.8318)	Acc@1 82.812 (81.882)	Acc@5 97.656 (99.106)
Epoch: [21][320/391]	Time 0.366 (0.406)	Data 0.001 (0.003)	Loss 0.8528 (0.8373)	Acc@1 80.469 (81.688)	Acc@5 99.219 (99.117)
Epoch: [21][384/391]	Time 0.390 (0.407)	Data 0.002 (0.002)	Loss 0.7662 (0.8415)	Acc@1 83.594 (81.471)	Acc@5 99.219 (99.103)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.450 (0.450)	Data 0.217 (0.217)	Loss 1.0165 (1.0165)	Acc@1 76.562 (76.562)	Acc@5 96.875 (96.875)
Epoch: [22][64/391]	Time 0.437 (0.409)	Data 0.002 (0.005)	Loss 0.7257 (0.8381)	Acc@1 85.938 (81.310)	Acc@5 99.219 (99.135)
Epoch: [22][128/391]	Time 0.355 (0.407)	Data 0.002 (0.004)	Loss 0.9115 (0.8394)	Acc@1 81.250 (81.420)	Acc@5 98.438 (99.049)
Epoch: [22][192/391]	Time 0.432 (0.406)	Data 0.002 (0.003)	Loss 0.8287 (0.8449)	Acc@1 79.688 (81.234)	Acc@5 99.219 (99.008)
Epoch: [22][256/391]	Time 0.390 (0.406)	Data 0.002 (0.003)	Loss 0.7669 (0.8439)	Acc@1 83.594 (81.314)	Acc@5 99.219 (99.039)
Epoch: [22][320/391]	Time 0.367 (0.404)	Data 0.002 (0.003)	Loss 0.8143 (0.8453)	Acc@1 81.250 (81.296)	Acc@5 100.000 (99.012)
Epoch: [22][384/391]	Time 0.410 (0.406)	Data 0.002 (0.002)	Loss 0.8078 (0.8464)	Acc@1 82.812 (81.278)	Acc@5 98.438 (99.008)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.452 (0.452)	Data 0.213 (0.213)	Loss 0.7933 (0.7933)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)
Epoch: [23][64/391]	Time 0.406 (0.407)	Data 0.002 (0.005)	Loss 0.8692 (0.8230)	Acc@1 81.250 (82.139)	Acc@5 97.656 (99.159)
Epoch: [23][128/391]	Time 0.428 (0.405)	Data 0.002 (0.004)	Loss 0.8462 (0.8416)	Acc@1 80.469 (81.305)	Acc@5 100.000 (99.031)
Epoch: [23][192/391]	Time 0.376 (0.406)	Data 0.002 (0.003)	Loss 0.8384 (0.8365)	Acc@1 82.812 (81.554)	Acc@5 98.438 (99.020)
Epoch: [23][256/391]	Time 0.424 (0.406)	Data 0.002 (0.003)	Loss 0.9385 (0.8352)	Acc@1 78.125 (81.670)	Acc@5 97.656 (98.994)
Epoch: [23][320/391]	Time 0.344 (0.406)	Data 0.002 (0.003)	Loss 0.7378 (0.8348)	Acc@1 85.156 (81.676)	Acc@5 100.000 (99.036)
Epoch: [23][384/391]	Time 0.398 (0.408)	Data 0.002 (0.003)	Loss 0.9228 (0.8375)	Acc@1 78.906 (81.577)	Acc@5 99.219 (99.018)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.393 (0.393)	Data 0.257 (0.257)	Loss 0.8541 (0.8541)	Acc@1 82.031 (82.031)	Acc@5 98.438 (98.438)
Epoch: [24][64/391]	Time 0.375 (0.409)	Data 0.002 (0.006)	Loss 0.7625 (0.8204)	Acc@1 84.375 (81.911)	Acc@5 98.438 (99.171)
Epoch: [24][128/391]	Time 0.401 (0.407)	Data 0.002 (0.004)	Loss 0.8992 (0.8444)	Acc@1 81.250 (80.941)	Acc@5 98.438 (99.110)
Epoch: [24][192/391]	Time 0.409 (0.406)	Data 0.002 (0.003)	Loss 0.9430 (0.8451)	Acc@1 79.688 (80.950)	Acc@5 98.438 (99.065)
Epoch: [24][256/391]	Time 0.419 (0.405)	Data 0.002 (0.003)	Loss 0.8961 (0.8419)	Acc@1 79.688 (81.159)	Acc@5 98.438 (99.061)
Epoch: [24][320/391]	Time 0.379 (0.405)	Data 0.002 (0.003)	Loss 0.9240 (0.8462)	Acc@1 78.906 (80.968)	Acc@5 97.656 (99.039)
Epoch: [24][384/391]	Time 0.393 (0.407)	Data 0.002 (0.003)	Loss 0.8243 (0.8451)	Acc@1 82.812 (81.023)	Acc@5 99.219 (99.026)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.473 (0.473)	Data 0.281 (0.281)	Loss 0.8457 (0.8457)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [25][64/391]	Time 0.374 (0.408)	Data 0.002 (0.006)	Loss 0.7473 (0.8404)	Acc@1 88.281 (81.514)	Acc@5 100.000 (99.002)
Epoch: [25][128/391]	Time 0.362 (0.405)	Data 0.002 (0.004)	Loss 0.8241 (0.8408)	Acc@1 81.250 (81.395)	Acc@5 99.219 (98.977)
Epoch: [25][192/391]	Time 0.468 (0.404)	Data 0.002 (0.003)	Loss 0.7940 (0.8390)	Acc@1 82.031 (81.517)	Acc@5 100.000 (99.008)
Epoch: [25][256/391]	Time 0.402 (0.402)	Data 0.001 (0.003)	Loss 0.8622 (0.8371)	Acc@1 78.906 (81.563)	Acc@5 97.656 (99.039)
Epoch: [25][320/391]	Time 0.449 (0.403)	Data 0.002 (0.003)	Loss 0.8683 (0.8355)	Acc@1 78.906 (81.513)	Acc@5 99.219 (99.048)
Epoch: [25][384/391]	Time 0.441 (0.405)	Data 0.002 (0.003)	Loss 0.8578 (0.8347)	Acc@1 81.250 (81.504)	Acc@5 98.438 (99.038)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.524 (0.524)	Data 0.182 (0.182)	Loss 3.1407 (3.1407)	Acc@1 9.375 (9.375)	Acc@5 54.688 (54.688)
Epoch: [1][64/391]	Time 0.394 (0.407)	Data 0.002 (0.005)	Loss 2.4365 (2.7394)	Acc@1 28.906 (21.623)	Acc@5 88.281 (74.832)
Epoch: [1][128/391]	Time 0.355 (0.401)	Data 0.002 (0.003)	Loss 2.3599 (2.6011)	Acc@1 32.812 (25.133)	Acc@5 85.938 (79.336)
Epoch: [1][192/391]	Time 0.377 (0.401)	Data 0.002 (0.003)	Loss 2.3821 (2.5019)	Acc@1 32.031 (28.214)	Acc@5 88.281 (82.270)
Epoch: [1][256/391]	Time 0.402 (0.401)	Data 0.003 (0.003)	Loss 2.0543 (2.4233)	Acc@1 46.094 (30.733)	Acc@5 92.188 (83.992)
Epoch: [1][320/391]	Time 0.407 (0.402)	Data 0.002 (0.002)	Loss 2.0281 (2.3561)	Acc@1 46.875 (32.917)	Acc@5 89.844 (85.344)
Epoch: [1][384/391]	Time 0.392 (0.402)	Data 0.002 (0.002)	Loss 2.0562 (2.2926)	Acc@1 41.406 (35.004)	Acc@5 93.750 (86.485)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.432 (0.432)	Data 0.221 (0.221)	Loss 1.7384 (1.7384)	Acc@1 60.156 (60.156)	Acc@5 95.312 (95.312)
Epoch: [2][64/391]	Time 0.370 (0.402)	Data 0.002 (0.005)	Loss 1.6730 (1.8736)	Acc@1 53.906 (48.894)	Acc@5 96.875 (93.137)
Epoch: [2][128/391]	Time 0.413 (0.401)	Data 0.002 (0.004)	Loss 1.9699 (1.8430)	Acc@1 48.438 (49.782)	Acc@5 92.969 (93.490)
Epoch: [2][192/391]	Time 0.327 (0.403)	Data 0.002 (0.003)	Loss 1.9973 (1.8079)	Acc@1 41.406 (50.943)	Acc@5 92.188 (93.722)
Epoch: [2][256/391]	Time 0.375 (0.403)	Data 0.002 (0.003)	Loss 1.7927 (1.7629)	Acc@1 53.125 (52.405)	Acc@5 91.406 (94.127)
Epoch: [2][320/391]	Time 0.393 (0.403)	Data 0.001 (0.003)	Loss 1.4738 (1.7283)	Acc@1 60.938 (53.643)	Acc@5 99.219 (94.434)
Epoch: [2][384/391]	Time 0.450 (0.404)	Data 0.002 (0.002)	Loss 1.5791 (1.6920)	Acc@1 55.469 (54.838)	Acc@5 94.531 (94.732)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.388 (0.388)	Data 0.243 (0.243)	Loss 1.6384 (1.6384)	Acc@1 54.688 (54.688)	Acc@5 92.969 (92.969)
Epoch: [3][64/391]	Time 0.451 (0.401)	Data 0.002 (0.006)	Loss 1.3146 (1.4372)	Acc@1 67.188 (62.139)	Acc@5 95.312 (96.082)
Epoch: [3][128/391]	Time 0.380 (0.401)	Data 0.002 (0.004)	Loss 1.3919 (1.4153)	Acc@1 64.844 (63.087)	Acc@5 96.875 (96.330)
Epoch: [3][192/391]	Time 0.375 (0.402)	Data 0.002 (0.003)	Loss 1.2936 (1.3965)	Acc@1 67.188 (63.686)	Acc@5 97.656 (96.409)
Epoch: [3][256/391]	Time 0.421 (0.404)	Data 0.001 (0.003)	Loss 1.2741 (1.3755)	Acc@1 60.938 (64.512)	Acc@5 97.656 (96.571)
Epoch: [3][320/391]	Time 0.395 (0.404)	Data 0.002 (0.003)	Loss 1.2384 (1.3552)	Acc@1 71.875 (65.219)	Acc@5 99.219 (96.783)
Epoch: [3][384/391]	Time 0.399 (0.405)	Data 0.001 (0.003)	Loss 1.1227 (1.3326)	Acc@1 76.562 (65.970)	Acc@5 96.875 (96.867)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.472 (0.472)	Data 0.203 (0.203)	Loss 1.2952 (1.2952)	Acc@1 68.750 (68.750)	Acc@5 94.531 (94.531)
Epoch: [4][64/391]	Time 0.409 (0.409)	Data 0.002 (0.005)	Loss 1.2214 (1.2253)	Acc@1 71.094 (69.339)	Acc@5 98.438 (97.548)
Epoch: [4][128/391]	Time 0.370 (0.407)	Data 0.001 (0.003)	Loss 0.9776 (1.2003)	Acc@1 78.125 (70.137)	Acc@5 98.438 (97.796)
Epoch: [4][192/391]	Time 0.407 (0.404)	Data 0.002 (0.003)	Loss 1.3231 (1.1890)	Acc@1 64.062 (70.462)	Acc@5 96.094 (97.725)
Epoch: [4][256/391]	Time 0.364 (0.403)	Data 0.002 (0.003)	Loss 1.2601 (1.1714)	Acc@1 65.625 (70.960)	Acc@5 99.219 (97.823)
Epoch: [4][320/391]	Time 0.402 (0.403)	Data 0.002 (0.003)	Loss 1.0377 (1.1576)	Acc@1 75.781 (71.430)	Acc@5 99.219 (97.849)
Epoch: [4][384/391]	Time 0.419 (0.405)	Data 0.002 (0.002)	Loss 0.9829 (1.1412)	Acc@1 82.031 (71.863)	Acc@5 100.000 (97.936)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.432 (0.432)	Data 0.225 (0.225)	Loss 1.1173 (1.1173)	Acc@1 74.219 (74.219)	Acc@5 98.438 (98.438)
Epoch: [5][64/391]	Time 0.396 (0.407)	Data 0.002 (0.005)	Loss 1.1694 (1.0586)	Acc@1 69.531 (74.231)	Acc@5 98.438 (98.293)
Epoch: [5][128/391]	Time 0.377 (0.405)	Data 0.002 (0.004)	Loss 0.9794 (1.0376)	Acc@1 74.219 (75.206)	Acc@5 99.219 (98.298)
Epoch: [5][192/391]	Time 0.445 (0.404)	Data 0.002 (0.003)	Loss 1.0792 (1.0297)	Acc@1 75.000 (75.409)	Acc@5 97.656 (98.296)
Epoch: [5][256/391]	Time 0.403 (0.403)	Data 0.002 (0.003)	Loss 1.0502 (1.0274)	Acc@1 76.562 (75.362)	Acc@5 98.438 (98.270)
Epoch: [5][320/391]	Time 0.360 (0.401)	Data 0.002 (0.003)	Loss 0.9413 (1.0298)	Acc@1 80.469 (75.112)	Acc@5 98.438 (98.235)
Epoch: [5][384/391]	Time 0.329 (0.392)	Data 0.002 (0.003)	Loss 0.9593 (1.0262)	Acc@1 79.688 (75.160)	Acc@5 99.219 (98.277)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.400 (0.400)	Data 0.193 (0.193)	Loss 0.9375 (0.9375)	Acc@1 77.344 (77.344)	Acc@5 100.000 (100.000)
Epoch: [6][64/391]	Time 0.348 (0.350)	Data 0.001 (0.005)	Loss 0.9957 (0.9569)	Acc@1 76.562 (77.043)	Acc@5 97.656 (98.882)
Epoch: [6][128/391]	Time 0.337 (0.346)	Data 0.002 (0.003)	Loss 1.0229 (0.9634)	Acc@1 74.219 (76.871)	Acc@5 99.219 (98.710)
Epoch: [6][192/391]	Time 0.319 (0.344)	Data 0.002 (0.003)	Loss 1.1126 (0.9635)	Acc@1 75.781 (77.008)	Acc@5 96.875 (98.636)
Epoch: [6][256/391]	Time 0.341 (0.343)	Data 0.002 (0.003)	Loss 0.9746 (0.9693)	Acc@1 82.031 (76.906)	Acc@5 96.875 (98.632)
Epoch: [6][320/391]	Time 0.328 (0.344)	Data 0.002 (0.003)	Loss 0.9071 (0.9693)	Acc@1 79.688 (76.850)	Acc@5 98.438 (98.644)
Epoch: [6][384/391]	Time 0.295 (0.341)	Data 0.002 (0.002)	Loss 1.0234 (0.9659)	Acc@1 73.438 (76.985)	Acc@5 99.219 (98.612)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.294 (0.294)	Data 0.193 (0.193)	Loss 0.8500 (0.8500)	Acc@1 79.688 (79.688)	Acc@5 97.656 (97.656)
Epoch: [7][64/391]	Time 0.327 (0.296)	Data 0.002 (0.005)	Loss 0.9006 (0.9369)	Acc@1 81.250 (78.017)	Acc@5 98.438 (98.690)
Epoch: [7][128/391]	Time 0.313 (0.295)	Data 0.002 (0.004)	Loss 0.9470 (0.9508)	Acc@1 76.562 (77.435)	Acc@5 97.656 (98.656)
Epoch: [7][192/391]	Time 0.321 (0.294)	Data 0.003 (0.003)	Loss 0.9488 (0.9471)	Acc@1 77.344 (77.688)	Acc@5 98.438 (98.636)
Epoch: [7][256/391]	Time 0.254 (0.294)	Data 0.002 (0.003)	Loss 1.0068 (0.9429)	Acc@1 72.656 (77.785)	Acc@5 98.438 (98.669)
Epoch: [7][320/391]	Time 0.292 (0.295)	Data 0.002 (0.003)	Loss 1.1570 (0.9427)	Acc@1 71.875 (77.835)	Acc@5 97.656 (98.632)
Epoch: [7][384/391]	Time 0.273 (0.295)	Data 0.002 (0.003)	Loss 0.9442 (0.9484)	Acc@1 76.562 (77.579)	Acc@5 100.000 (98.606)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.308 (0.308)	Data 0.195 (0.195)	Loss 0.8968 (0.8968)	Acc@1 78.906 (78.906)	Acc@5 100.000 (100.000)
Epoch: [8][64/391]	Time 0.278 (0.291)	Data 0.002 (0.005)	Loss 0.9365 (0.9124)	Acc@1 77.344 (78.666)	Acc@5 100.000 (98.834)
Epoch: [8][128/391]	Time 0.295 (0.289)	Data 0.002 (0.004)	Loss 0.8197 (0.9217)	Acc@1 85.156 (78.282)	Acc@5 98.438 (98.886)
Epoch: [8][192/391]	Time 0.288 (0.292)	Data 0.002 (0.003)	Loss 1.0015 (0.9294)	Acc@1 76.562 (78.024)	Acc@5 97.656 (98.830)
Epoch: [8][256/391]	Time 0.230 (0.293)	Data 0.003 (0.003)	Loss 0.9542 (0.9233)	Acc@1 75.000 (78.359)	Acc@5 99.219 (98.872)
Epoch: [8][320/391]	Time 0.232 (0.293)	Data 0.003 (0.003)	Loss 0.8834 (0.9256)	Acc@1 80.469 (78.351)	Acc@5 98.438 (98.832)
Epoch: [8][384/391]	Time 0.234 (0.293)	Data 0.002 (0.003)	Loss 1.0164 (0.9206)	Acc@1 76.562 (78.537)	Acc@5 96.875 (98.841)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.311 (0.311)	Data 0.213 (0.213)	Loss 0.7952 (0.7952)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [9][64/391]	Time 0.268 (0.290)	Data 0.002 (0.005)	Loss 0.8231 (0.9044)	Acc@1 82.031 (79.339)	Acc@5 100.000 (98.714)
Epoch: [9][128/391]	Time 0.272 (0.289)	Data 0.002 (0.004)	Loss 0.7651 (0.9014)	Acc@1 82.812 (79.494)	Acc@5 100.000 (98.692)
Epoch: [9][192/391]	Time 0.289 (0.292)	Data 0.002 (0.003)	Loss 0.9175 (0.9015)	Acc@1 78.125 (79.311)	Acc@5 99.219 (98.737)
Epoch: [9][256/391]	Time 0.286 (0.292)	Data 0.002 (0.003)	Loss 1.0238 (0.9032)	Acc@1 73.438 (79.152)	Acc@5 99.219 (98.763)
Epoch: [9][320/391]	Time 0.330 (0.293)	Data 0.002 (0.003)	Loss 0.9303 (0.9053)	Acc@1 78.906 (79.013)	Acc@5 100.000 (98.776)
Epoch: [9][384/391]	Time 0.281 (0.292)	Data 0.002 (0.003)	Loss 0.8450 (0.9045)	Acc@1 83.594 (79.099)	Acc@5 99.219 (98.799)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.353 (0.353)	Data 0.209 (0.209)	Loss 0.8462 (0.8462)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.312 (0.299)	Data 0.002 (0.005)	Loss 0.7654 (0.8734)	Acc@1 86.719 (80.156)	Acc@5 98.438 (98.930)
Epoch: [10][128/391]	Time 0.269 (0.291)	Data 0.002 (0.004)	Loss 0.8971 (0.8815)	Acc@1 77.344 (79.930)	Acc@5 97.656 (98.910)
Epoch: [10][192/391]	Time 0.227 (0.293)	Data 0.002 (0.003)	Loss 0.9610 (0.8824)	Acc@1 78.906 (79.874)	Acc@5 97.656 (98.834)
Epoch: [10][256/391]	Time 0.314 (0.292)	Data 0.002 (0.003)	Loss 1.1348 (0.8860)	Acc@1 71.875 (79.800)	Acc@5 97.656 (98.799)
Epoch: [10][320/391]	Time 0.271 (0.291)	Data 0.003 (0.003)	Loss 0.7231 (0.8878)	Acc@1 86.719 (79.770)	Acc@5 100.000 (98.820)
Epoch: [10][384/391]	Time 0.302 (0.291)	Data 0.003 (0.003)	Loss 0.9033 (0.8909)	Acc@1 78.906 (79.641)	Acc@5 100.000 (98.807)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 486518 ; 487386 ; 0.9982190707160239

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.363 (0.363)	Data 0.187 (0.187)	Loss 0.8046 (0.8046)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)
Epoch: [11][64/391]	Time 0.277 (0.272)	Data 0.003 (0.005)	Loss 0.7056 (0.8535)	Acc@1 82.031 (81.022)	Acc@5 100.000 (98.882)
Epoch: [11][128/391]	Time 0.276 (0.268)	Data 0.002 (0.004)	Loss 0.7346 (0.8532)	Acc@1 86.719 (81.020)	Acc@5 99.219 (98.964)
Epoch: [11][192/391]	Time 0.220 (0.266)	Data 0.002 (0.003)	Loss 1.0056 (0.8734)	Acc@1 68.750 (80.214)	Acc@5 99.219 (98.980)
Epoch: [11][256/391]	Time 0.263 (0.266)	Data 0.002 (0.003)	Loss 0.8253 (0.8793)	Acc@1 82.031 (80.019)	Acc@5 98.438 (98.924)
Epoch: [11][320/391]	Time 0.269 (0.268)	Data 0.002 (0.003)	Loss 0.8544 (0.8857)	Acc@1 77.344 (79.683)	Acc@5 97.656 (98.895)
Epoch: [11][384/391]	Time 0.295 (0.267)	Data 0.002 (0.003)	Loss 0.8484 (0.8869)	Acc@1 81.250 (79.570)	Acc@5 98.438 (98.872)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.257 (0.257)	Data 0.197 (0.197)	Loss 0.7951 (0.7951)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [12][64/391]	Time 0.258 (0.267)	Data 0.002 (0.005)	Loss 1.1134 (0.8672)	Acc@1 75.000 (80.469)	Acc@5 95.312 (99.050)
Epoch: [12][128/391]	Time 0.238 (0.261)	Data 0.002 (0.004)	Loss 0.8833 (0.8645)	Acc@1 82.812 (80.554)	Acc@5 98.438 (98.983)
Epoch: [12][192/391]	Time 0.242 (0.258)	Data 0.002 (0.003)	Loss 0.8680 (0.8667)	Acc@1 81.250 (80.420)	Acc@5 97.656 (99.008)
Epoch: [12][256/391]	Time 0.211 (0.256)	Data 0.002 (0.003)	Loss 0.8790 (0.8715)	Acc@1 82.031 (80.280)	Acc@5 96.875 (98.979)
Epoch: [12][320/391]	Time 1.445 (0.303)	Data 0.002 (0.003)	Loss 0.9990 (0.8732)	Acc@1 76.562 (80.172)	Acc@5 98.438 (98.968)
Epoch: [12][384/391]	Time 0.365 (0.422)	Data 0.003 (0.003)	Loss 0.8760 (0.8733)	Acc@1 82.031 (80.193)	Acc@5 99.219 (98.957)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.281 (0.281)	Data 0.203 (0.203)	Loss 0.9140 (0.9140)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [13][64/391]	Time 0.568 (0.604)	Data 0.003 (0.006)	Loss 0.9254 (0.8573)	Acc@1 76.562 (80.793)	Acc@5 98.438 (99.135)
Epoch: [13][128/391]	Time 0.818 (0.589)	Data 0.003 (0.005)	Loss 0.9757 (0.8576)	Acc@1 75.781 (80.656)	Acc@5 98.438 (99.067)
Epoch: [13][192/391]	Time 0.553 (0.560)	Data 0.002 (0.004)	Loss 1.0380 (0.8705)	Acc@1 74.219 (80.137)	Acc@5 96.875 (99.004)
Epoch: [13][256/391]	Time 0.482 (0.530)	Data 0.002 (0.004)	Loss 0.8225 (0.8671)	Acc@1 82.031 (80.308)	Acc@5 100.000 (98.994)
Epoch: [13][320/391]	Time 0.579 (0.581)	Data 0.002 (0.004)	Loss 0.8139 (0.8713)	Acc@1 79.688 (80.152)	Acc@5 100.000 (98.968)
Epoch: [13][384/391]	Time 0.585 (0.563)	Data 0.002 (0.003)	Loss 0.9277 (0.8688)	Acc@1 81.250 (80.262)	Acc@5 97.656 (98.973)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.488 (0.488)	Data 0.414 (0.414)	Loss 0.8119 (0.8119)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [14][64/391]	Time 0.569 (0.455)	Data 0.003 (0.009)	Loss 0.8383 (0.8560)	Acc@1 78.906 (80.613)	Acc@5 97.656 (98.882)
Epoch: [14][128/391]	Time 0.470 (0.482)	Data 0.002 (0.006)	Loss 0.9406 (0.8575)	Acc@1 78.125 (80.759)	Acc@5 98.438 (98.934)
Epoch: [14][192/391]	Time 0.462 (0.485)	Data 0.002 (0.005)	Loss 0.9029 (0.8543)	Acc@1 75.781 (80.841)	Acc@5 99.219 (98.943)
Epoch: [14][256/391]	Time 0.290 (0.509)	Data 0.002 (0.004)	Loss 0.8274 (0.8569)	Acc@1 82.031 (80.739)	Acc@5 98.438 (98.960)
Epoch: [14][320/391]	Time 0.603 (0.479)	Data 0.002 (0.004)	Loss 0.8459 (0.8568)	Acc@1 80.469 (80.749)	Acc@5 100.000 (98.995)
Epoch: [14][384/391]	Time 0.465 (0.514)	Data 0.002 (0.004)	Loss 0.9460 (0.8599)	Acc@1 79.688 (80.694)	Acc@5 98.438 (98.983)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.359 (0.359)	Data 0.488 (0.488)	Loss 0.7228 (0.7228)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [15][64/391]	Time 0.504 (0.532)	Data 0.002 (0.010)	Loss 0.8408 (0.8377)	Acc@1 78.906 (81.707)	Acc@5 100.000 (98.894)
Epoch: [15][128/391]	Time 0.565 (0.535)	Data 0.003 (0.006)	Loss 0.9321 (0.8388)	Acc@1 75.781 (81.504)	Acc@5 99.219 (98.989)
Epoch: [15][192/391]	Time 0.320 (0.547)	Data 0.002 (0.005)	Loss 0.7260 (0.8430)	Acc@1 83.594 (81.355)	Acc@5 100.000 (98.964)
Epoch: [15][256/391]	Time 0.711 (0.558)	Data 0.004 (0.005)	Loss 0.8818 (0.8498)	Acc@1 77.344 (80.964)	Acc@5 100.000 (98.963)
Epoch: [15][320/391]	Time 0.571 (0.590)	Data 0.002 (0.004)	Loss 0.9565 (0.8504)	Acc@1 75.781 (80.890)	Acc@5 99.219 (98.971)
Epoch: [15][384/391]	Time 0.841 (0.618)	Data 0.003 (0.004)	Loss 0.9224 (0.8540)	Acc@1 82.812 (80.828)	Acc@5 96.875 (98.931)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 433380 ; 487386 ; 0.8891925496423779

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.809 (0.809)	Data 0.989 (0.989)	Loss 0.9303 (0.9303)	Acc@1 79.688 (79.688)	Acc@5 97.656 (97.656)
Epoch: [16][64/391]	Time 0.891 (1.056)	Data 0.002 (0.019)	Loss 0.8824 (0.8148)	Acc@1 82.812 (81.863)	Acc@5 98.438 (99.075)
Epoch: [16][128/391]	Time 0.519 (0.806)	Data 0.002 (0.011)	Loss 0.7183 (0.8331)	Acc@1 88.281 (81.414)	Acc@5 98.438 (99.007)
Epoch: [16][192/391]	Time 0.308 (0.707)	Data 0.002 (0.009)	Loss 0.7885 (0.8398)	Acc@1 81.250 (81.189)	Acc@5 99.219 (98.939)
Epoch: [16][256/391]	Time 0.712 (0.671)	Data 0.004 (0.007)	Loss 0.8261 (0.8394)	Acc@1 78.125 (81.271)	Acc@5 100.000 (99.033)
Epoch: [16][320/391]	Time 0.641 (0.664)	Data 0.002 (0.006)	Loss 0.9191 (0.8467)	Acc@1 81.250 (81.114)	Acc@5 98.438 (98.980)
Epoch: [16][384/391]	Time 0.671 (0.668)	Data 0.002 (0.006)	Loss 1.0035 (0.8478)	Acc@1 71.094 (81.021)	Acc@5 98.438 (98.996)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.657 (0.657)	Data 0.850 (0.850)	Loss 0.6899 (0.6899)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [17][64/391]	Time 0.606 (0.787)	Data 0.002 (0.016)	Loss 0.7256 (0.8287)	Acc@1 84.375 (81.647)	Acc@5 100.000 (99.014)
Epoch: [17][128/391]	Time 0.500 (0.719)	Data 0.002 (0.010)	Loss 0.9607 (0.8254)	Acc@1 73.438 (81.577)	Acc@5 96.875 (98.989)
Epoch: [17][192/391]	Time 0.380 (0.747)	Data 0.002 (0.007)	Loss 0.9221 (0.8293)	Acc@1 76.562 (81.428)	Acc@5 97.656 (99.061)
Epoch: [17][256/391]	Time 0.567 (0.691)	Data 0.002 (0.006)	Loss 0.9413 (0.8358)	Acc@1 75.781 (81.062)	Acc@5 99.219 (99.021)
Epoch: [17][320/391]	Time 0.658 (0.667)	Data 0.002 (0.006)	Loss 0.8739 (0.8419)	Acc@1 81.250 (80.817)	Acc@5 96.875 (99.012)
Epoch: [17][384/391]	Time 0.729 (0.659)	Data 0.002 (0.005)	Loss 0.7675 (0.8424)	Acc@1 87.500 (80.875)	Acc@5 98.438 (99.018)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.854 (0.854)	Data 0.874 (0.874)	Loss 0.8043 (0.8043)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [18][64/391]	Time 0.652 (0.582)	Data 0.003 (0.016)	Loss 0.8409 (0.8246)	Acc@1 80.469 (81.635)	Acc@5 100.000 (99.050)
Epoch: [18][128/391]	Time 0.474 (0.553)	Data 0.003 (0.010)	Loss 0.8008 (0.8202)	Acc@1 82.812 (81.698)	Acc@5 99.219 (99.079)
Epoch: [18][192/391]	Time 0.426 (0.536)	Data 0.002 (0.008)	Loss 0.9056 (0.8253)	Acc@1 80.469 (81.570)	Acc@5 98.438 (99.065)
Epoch: [18][256/391]	Time 0.490 (0.520)	Data 0.003 (0.006)	Loss 0.7589 (0.8330)	Acc@1 81.250 (81.350)	Acc@5 99.219 (99.018)
Epoch: [18][320/391]	Time 0.609 (0.524)	Data 0.002 (0.006)	Loss 0.7589 (0.8362)	Acc@1 82.812 (81.243)	Acc@5 100.000 (99.000)
Epoch: [18][384/391]	Time 0.567 (0.545)	Data 0.002 (0.005)	Loss 0.9585 (0.8346)	Acc@1 78.125 (81.305)	Acc@5 97.656 (98.981)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.442 (0.442)	Data 0.494 (0.494)	Loss 1.0170 (1.0170)	Acc@1 77.344 (77.344)	Acc@5 97.656 (97.656)
Epoch: [19][64/391]	Time 0.512 (0.493)	Data 0.002 (0.011)	Loss 0.7989 (0.8220)	Acc@1 82.812 (81.526)	Acc@5 97.656 (99.183)
Epoch: [19][128/391]	Time 0.356 (0.487)	Data 0.002 (0.007)	Loss 0.7557 (0.8277)	Acc@1 83.594 (81.371)	Acc@5 100.000 (99.079)
Epoch: [19][192/391]	Time 0.613 (0.506)	Data 0.013 (0.005)	Loss 0.8115 (0.8371)	Acc@1 82.031 (81.100)	Acc@5 99.219 (98.996)
Epoch: [19][256/391]	Time 0.503 (0.513)	Data 0.002 (0.005)	Loss 0.7427 (0.8371)	Acc@1 82.031 (81.104)	Acc@5 99.219 (98.988)
Epoch: [19][320/391]	Time 0.388 (0.495)	Data 0.017 (0.005)	Loss 0.7870 (0.8357)	Acc@1 82.812 (81.182)	Acc@5 100.000 (99.036)
Epoch: [19][384/391]	Time 0.530 (0.487)	Data 0.002 (0.004)	Loss 0.8181 (0.8365)	Acc@1 80.469 (81.132)	Acc@5 99.219 (99.012)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.534 (0.534)	Data 0.585 (0.585)	Loss 0.8182 (0.8182)	Acc@1 82.031 (82.031)	Acc@5 100.000 (100.000)
Epoch: [20][64/391]	Time 0.625 (0.464)	Data 0.002 (0.012)	Loss 0.9078 (0.8223)	Acc@1 78.906 (82.548)	Acc@5 98.438 (99.014)
Epoch: [20][128/391]	Time 0.534 (0.454)	Data 0.002 (0.007)	Loss 0.7840 (0.8086)	Acc@1 82.031 (82.582)	Acc@5 100.000 (99.055)
Epoch: [20][192/391]	Time 0.551 (0.470)	Data 0.002 (0.006)	Loss 0.8802 (0.8265)	Acc@1 82.812 (81.914)	Acc@5 98.438 (98.988)
Epoch: [20][256/391]	Time 0.660 (0.507)	Data 0.002 (0.005)	Loss 0.8316 (0.8253)	Acc@1 84.375 (81.788)	Acc@5 99.219 (99.039)
Epoch: [20][320/391]	Time 0.609 (0.556)	Data 0.002 (0.005)	Loss 0.8331 (0.8241)	Acc@1 80.469 (81.778)	Acc@5 100.000 (99.051)
Epoch: [20][384/391]	Time 0.264 (0.543)	Data 0.003 (0.005)	Loss 0.8765 (0.8287)	Acc@1 78.906 (81.593)	Acc@5 99.219 (99.000)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 382722 ; 487386 ; 0.785254397951521

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.440 (0.440)	Data 0.730 (0.730)	Loss 0.7513 (0.7513)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [21][64/391]	Time 0.683 (0.610)	Data 0.002 (0.014)	Loss 0.8692 (0.7968)	Acc@1 80.469 (82.728)	Acc@5 99.219 (99.207)
Epoch: [21][128/391]	Time 0.591 (0.563)	Data 0.002 (0.009)	Loss 0.7752 (0.8062)	Acc@1 81.250 (82.225)	Acc@5 100.000 (99.188)
Epoch: [21][192/391]	Time 0.544 (0.580)	Data 0.003 (0.007)	Loss 0.9039 (0.8201)	Acc@1 77.344 (81.748)	Acc@5 100.000 (99.138)
Epoch: [21][256/391]	Time 0.292 (0.559)	Data 0.002 (0.006)	Loss 0.8599 (0.8249)	Acc@1 82.031 (81.660)	Acc@5 99.219 (99.109)
Epoch: [21][320/391]	Time 0.334 (0.555)	Data 0.005 (0.005)	Loss 0.8072 (0.8262)	Acc@1 80.469 (81.637)	Acc@5 99.219 (99.082)
Epoch: [21][384/391]	Time 0.489 (0.545)	Data 0.003 (0.005)	Loss 0.8400 (0.8269)	Acc@1 81.250 (81.516)	Acc@5 100.000 (99.030)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.484 (0.484)	Data 0.684 (0.684)	Loss 0.8656 (0.8656)	Acc@1 82.812 (82.812)	Acc@5 97.656 (97.656)
Epoch: [22][64/391]	Time 0.737 (0.733)	Data 0.002 (0.013)	Loss 0.9237 (0.8392)	Acc@1 78.906 (81.370)	Acc@5 99.219 (99.147)
Epoch: [22][128/391]	Time 0.516 (0.604)	Data 0.010 (0.008)	Loss 0.8711 (0.8124)	Acc@1 78.125 (82.243)	Acc@5 99.219 (99.037)
Epoch: [22][192/391]	Time 0.319 (0.556)	Data 0.003 (0.006)	Loss 0.8106 (0.8171)	Acc@1 78.906 (81.886)	Acc@5 100.000 (99.028)
Epoch: [22][256/391]	Time 0.394 (0.540)	Data 0.003 (0.006)	Loss 1.0000 (0.8201)	Acc@1 77.344 (81.676)	Acc@5 97.656 (99.033)
Epoch: [22][320/391]	Time 0.653 (0.550)	Data 0.003 (0.005)	Loss 0.8339 (0.8306)	Acc@1 78.906 (81.262)	Acc@5 99.219 (99.007)
Epoch: [22][384/391]	Time 0.604 (0.555)	Data 0.006 (0.005)	Loss 0.8902 (0.8312)	Acc@1 77.344 (81.364)	Acc@5 98.438 (98.998)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.538 (0.538)	Data 0.687 (0.687)	Loss 0.7913 (0.7913)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [23][64/391]	Time 0.701 (0.618)	Data 0.002 (0.013)	Loss 0.8938 (0.8114)	Acc@1 75.781 (81.923)	Acc@5 99.219 (99.002)
Epoch: [23][128/391]	Time 0.656 (0.668)	Data 0.003 (0.009)	Loss 0.6734 (0.8078)	Acc@1 86.719 (81.953)	Acc@5 99.219 (99.092)
Epoch: [23][192/391]	Time 0.296 (0.622)	Data 0.003 (0.007)	Loss 0.7846 (0.8117)	Acc@1 80.469 (81.732)	Acc@5 99.219 (99.118)
Epoch: [23][256/391]	Time 0.712 (0.604)	Data 0.003 (0.006)	Loss 0.7262 (0.8126)	Acc@1 89.062 (81.846)	Acc@5 97.656 (99.109)
Epoch: [23][320/391]	Time 0.446 (0.587)	Data 0.002 (0.005)	Loss 0.9056 (0.8123)	Acc@1 74.219 (81.910)	Acc@5 100.000 (99.131)
Epoch: [23][384/391]	Time 0.543 (0.564)	Data 0.002 (0.005)	Loss 0.7146 (0.8138)	Acc@1 86.719 (81.859)	Acc@5 100.000 (99.123)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.633 (0.633)	Data 0.724 (0.724)	Loss 0.8115 (0.8115)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [24][64/391]	Time 0.331 (0.430)	Data 0.005 (0.014)	Loss 0.8547 (0.8192)	Acc@1 75.781 (81.839)	Acc@5 100.000 (99.123)
Epoch: [24][128/391]	Time 0.196 (0.422)	Data 0.002 (0.008)	Loss 0.7305 (0.8238)	Acc@1 85.156 (81.801)	Acc@5 100.000 (98.970)
Epoch: [24][192/391]	Time 0.302 (0.422)	Data 0.002 (0.006)	Loss 0.8437 (0.8188)	Acc@1 76.562 (81.886)	Acc@5 100.000 (99.004)
Epoch: [24][256/391]	Time 0.594 (0.428)	Data 0.004 (0.006)	Loss 0.8790 (0.8140)	Acc@1 82.812 (82.071)	Acc@5 100.000 (99.006)
Epoch: [24][320/391]	Time 0.471 (0.430)	Data 0.002 (0.005)	Loss 0.8554 (0.8133)	Acc@1 79.688 (82.034)	Acc@5 99.219 (99.041)
Epoch: [24][384/391]	Time 0.310 (0.429)	Data 0.002 (0.005)	Loss 0.7670 (0.8153)	Acc@1 82.031 (81.964)	Acc@5 99.219 (99.065)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.426 (0.426)	Data 0.411 (0.411)	Loss 0.8111 (0.8111)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [25][64/391]	Time 0.467 (0.478)	Data 0.002 (0.009)	Loss 0.7346 (0.8175)	Acc@1 85.938 (81.538)	Acc@5 100.000 (99.099)
Epoch: [25][128/391]	Time 0.555 (0.431)	Data 0.003 (0.006)	Loss 0.8168 (0.8094)	Acc@1 82.031 (81.771)	Acc@5 98.438 (99.146)
Epoch: [25][192/391]	Time 0.583 (0.437)	Data 0.002 (0.005)	Loss 0.7958 (0.8067)	Acc@1 79.688 (81.796)	Acc@5 99.219 (99.178)
Epoch: [25][256/391]	Time 0.492 (0.453)	Data 0.002 (0.004)	Loss 0.7401 (0.8087)	Acc@1 85.156 (81.800)	Acc@5 98.438 (99.140)
Epoch: [25][320/391]	Time 0.239 (0.451)	Data 0.002 (0.004)	Loss 0.8340 (0.8101)	Acc@1 80.469 (81.802)	Acc@5 100.000 (99.087)
Epoch: [25][384/391]	Time 0.297 (0.456)	Data 0.003 (0.004)	Loss 0.9651 (0.8128)	Acc@1 73.438 (81.660)	Acc@5 100.000 (99.085)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
























 ab hier mit pruneTrain
