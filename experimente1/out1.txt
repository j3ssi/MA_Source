no display found. Using non-interactive Agg backend
This Gpu is free
GPU Id: 0
total    : 11018
free     : 11018
used     : 0



Use Gpu with the ID:  cuda:0
Files already downloaded and verified
Available before Model Creation: 11008
Use before Model Creation: smi 10 torch 0
Max memory before modell creation: 0
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Available after Model Creation: 10143
Use after modell creation: smi 875 torch 4384768
Max memory after modell creation 4384768
count0: 1082906
Batch Size: 667

Epoch: [1 | 80] LR: 0.100000
Epoch: [1][0/75]	Time 1.226 (1.226)	Data 0.339 (0.339)	Loss 3.4589 (3.4589)	Acc@1 8.696 (8.696)	Acc@5 49.625 (49.625)
Epoch: [1][10/75]	Time 0.982 (0.993)	Data 0.005 (0.035)	Loss 3.2263 (3.3218)	Acc@1 16.942 (12.662)	Acc@5 67.466 (58.866)
Epoch: [1][20/75]	Time 0.970 (0.983)	Data 0.005 (0.021)	Loss 2.9626 (3.1687)	Acc@1 26.387 (18.691)	Acc@5 81.409 (68.344)
Epoch: [1][30/75]	Time 0.978 (0.981)	Data 0.004 (0.016)	Loss 2.8520 (3.0732)	Acc@1 33.283 (22.087)	Acc@5 85.457 (73.531)
Epoch: [1][40/75]	Time 0.984 (0.980)	Data 0.005 (0.013)	Loss 2.7937 (3.0025)	Acc@1 28.936 (24.284)	Acc@5 85.307 (76.776)
Epoch: [1][50/75]	Time 0.984 (0.979)	Data 0.005 (0.011)	Loss 2.6759 (2.9537)	Acc@1 36.132 (25.902)	Acc@5 86.657 (78.752)
Epoch: [1][60/75]	Time 0.992 (0.981)	Data 0.005 (0.010)	Loss 2.6999 (2.9059)	Acc@1 35.982 (27.414)	Acc@5 88.156 (80.379)
Epoch: [1][70/75]	Time 1.016 (0.983)	Data 0.005 (0.010)	Loss 2.5834 (2.8621)	Acc@1 39.730 (28.993)	Acc@5 89.805 (81.737)

Epoche:  1  ; NumbOfParameters:  1082906

Test Acc:  33.67

Epoch: [2 | 80] LR: 0.100000
Epoch: [2][0/75]	Time 0.977 (0.977)	Data 0.344 (0.344)	Loss 2.5880 (2.5880)	Acc@1 40.330 (40.330)	Acc@5 90.555 (90.555)
Epoch: [2][10/75]	Time 1.003 (0.999)	Data 0.005 (0.036)	Loss 2.4734 (2.5090)	Acc@1 41.379 (42.170)	Acc@5 91.604 (91.127)
Epoch: [2][20/75]	Time 1.005 (0.997)	Data 0.005 (0.021)	Loss 2.4456 (2.4838)	Acc@1 45.427 (42.807)	Acc@5 92.654 (91.740)
Epoch: [2][30/75]	Time 0.995 (0.998)	Data 0.005 (0.016)	Loss 2.3855 (2.4574)	Acc@1 46.027 (44.044)	Acc@5 92.654 (92.073)
Epoch: [2][40/75]	Time 1.005 (0.999)	Data 0.005 (0.013)	Loss 2.3348 (2.4302)	Acc@1 50.375 (45.292)	Acc@5 93.403 (92.405)
Epoch: [2][50/75]	Time 1.011 (1.000)	Data 0.005 (0.011)	Loss 2.3090 (2.4047)	Acc@1 49.025 (46.256)	Acc@5 91.904 (92.601)
Epoch: [2][60/75]	Time 1.006 (1.002)	Data 0.005 (0.010)	Loss 2.2960 (2.3802)	Acc@1 47.676 (47.106)	Acc@5 95.352 (92.840)
Epoch: [2][70/75]	Time 1.012 (1.003)	Data 0.004 (0.010)	Loss 2.2121 (2.3515)	Acc@1 52.024 (48.079)	Acc@5 94.003 (93.163)

Epoche:  2  ; NumbOfParameters:  1082906

Test Acc:  41.74

Epoch: [3 | 80] LR: 0.100000
Epoch: [3][0/75]	Time 0.994 (0.994)	Data 0.343 (0.343)	Loss 2.1591 (2.1591)	Acc@1 53.223 (53.223)	Acc@5 94.753 (94.753)
Epoch: [3][10/75]	Time 1.017 (1.012)	Data 0.005 (0.036)	Loss 2.1006 (2.0980)	Acc@1 58.621 (57.462)	Acc@5 94.153 (95.393)
Epoch: [3][20/75]	Time 1.014 (1.013)	Data 0.005 (0.021)	Loss 1.9909 (2.0914)	Acc@1 62.819 (57.578)	Acc@5 95.502 (95.509)
Epoch: [3][30/75]	Time 1.007 (1.013)	Data 0.005 (0.016)	Loss 1.9689 (2.0701)	Acc@1 60.420 (58.253)	Acc@5 97.151 (95.734)
Epoch: [3][40/75]	Time 1.011 (1.013)	Data 0.005 (0.013)	Loss 2.1024 (2.0549)	Acc@1 57.871 (58.705)	Acc@5 94.303 (95.809)
Epoch: [3][50/75]	Time 1.017 (1.014)	Data 0.007 (0.012)	Loss 2.0572 (2.0406)	Acc@1 57.271 (59.088)	Acc@5 95.952 (95.914)
Epoch: [3][60/75]	Time 1.019 (1.014)	Data 0.005 (0.011)	Loss 2.0607 (2.0335)	Acc@1 54.573 (59.171)	Acc@5 96.702 (95.986)
Epoch: [3][70/75]	Time 1.025 (1.015)	Data 0.002 (0.010)	Loss 1.9297 (2.0237)	Acc@1 63.118 (59.362)	Acc@5 96.552 (96.032)

Epoche:  3  ; NumbOfParameters:  1082906

Test Acc:  46.84

Epoch: [4 | 80] LR: 0.100000
Epoch: [4][0/75]	Time 1.003 (1.003)	Data 0.351 (0.351)	Loss 1.8989 (1.8989)	Acc@1 65.067 (65.067)	Acc@5 97.002 (97.002)
Epoch: [4][10/75]	Time 1.020 (1.015)	Data 0.005 (0.036)	Loss 1.8836 (1.8979)	Acc@1 64.318 (63.255)	Acc@5 96.852 (96.865)
Epoch: [4][20/75]	Time 1.014 (1.014)	Data 0.005 (0.021)	Loss 1.8598 (1.8757)	Acc@1 64.618 (63.790)	Acc@5 97.451 (96.923)
Epoch: [4][30/75]	Time 1.010 (1.014)	Data 0.005 (0.016)	Loss 1.7989 (1.8651)	Acc@1 65.667 (63.965)	Acc@5 97.002 (97.064)
Epoch: [4][40/75]	Time 1.027 (1.014)	Data 0.005 (0.013)	Loss 1.8387 (1.8537)	Acc@1 65.367 (64.274)	Acc@5 97.451 (97.162)
Epoch: [4][50/75]	Time 1.007 (1.015)	Data 0.005 (0.012)	Loss 1.8144 (1.8446)	Acc@1 66.417 (64.497)	Acc@5 97.451 (97.181)
Epoch: [4][60/75]	Time 1.023 (1.016)	Data 0.005 (0.011)	Loss 1.8101 (1.8410)	Acc@1 64.018 (64.433)	Acc@5 97.901 (97.210)
Epoch: [4][70/75]	Time 1.029 (1.016)	Data 0.005 (0.010)	Loss 1.7410 (1.8351)	Acc@1 66.717 (64.556)	Acc@5 97.751 (97.177)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoche:  4  ; NumbOfParameters:  1082906

Test Acc:  36.7

Epoch: [5 | 80] LR: 0.100000
Epoch: [5][0/75]	Time 1.007 (1.007)	Data 0.335 (0.335)	Loss 1.7492 (1.7492)	Acc@1 68.666 (68.666)	Acc@5 96.852 (96.852)
Epoch: [5][10/75]	Time 1.003 (1.008)	Data 0.005 (0.035)	Loss 1.7405 (1.7054)	Acc@1 67.616 (68.243)	Acc@5 97.601 (97.560)
Epoch: [5][20/75]	Time 1.001 (1.007)	Data 0.005 (0.021)	Loss 1.7258 (1.7047)	Acc@1 66.567 (68.330)	Acc@5 97.451 (97.665)
Epoch: [5][30/75]	Time 1.013 (1.009)	Data 0.005 (0.016)	Loss 1.6777 (1.7070)	Acc@1 67.616 (68.269)	Acc@5 98.351 (97.630)
Epoch: [5][40/75]	Time 1.016 (1.009)	Data 0.005 (0.013)	Loss 1.7421 (1.7066)	Acc@1 66.117 (68.260)	Acc@5 97.151 (97.630)
Epoch: [5][50/75]	Time 1.015 (1.010)	Data 0.005 (0.012)	Loss 1.6500 (1.7010)	Acc@1 70.615 (68.442)	Acc@5 97.901 (97.707)
Epoch: [5][60/75]	Time 1.004 (1.010)	Data 0.005 (0.010)	Loss 1.6786 (1.6965)	Acc@1 65.517 (68.442)	Acc@5 98.351 (97.724)
Epoch: [5][70/75]	Time 1.025 (1.011)	Data 0.005 (0.010)	Loss 1.6795 (1.6914)	Acc@1 67.616 (68.594)	Acc@5 96.852 (97.722)

Epoche:  5  ; NumbOfParameters:  1082906

Test Acc:  53.95

Epoch: [6 | 80] LR: 0.100000
Epoch: [6][0/75]	Time 1.005 (1.005)	Data 0.351 (0.351)	Loss 1.6058 (1.6058)	Acc@1 68.516 (68.516)	Acc@5 98.201 (98.201)
Epoch: [6][10/75]	Time 1.012 (1.012)	Data 0.005 (0.036)	Loss 1.5623 (1.6114)	Acc@1 74.063 (70.642)	Acc@5 98.501 (98.146)
Epoch: [6][20/75]	Time 1.017 (1.012)	Data 0.005 (0.021)	Loss 1.5992 (1.6064)	Acc@1 69.715 (70.772)	Acc@5 98.651 (97.972)
Epoch: [6][30/75]	Time 1.023 (1.012)	Data 0.005 (0.016)	Loss 1.6448 (1.6077)	Acc@1 68.816 (70.711)	Acc@5 96.702 (97.949)
Epoch: [6][40/75]	Time 1.009 (1.013)	Data 0.004 (0.013)	Loss 1.6197 (1.6034)	Acc@1 70.015 (70.743)	Acc@5 97.751 (97.916)
Epoch: [6][50/75]	Time 1.013 (1.012)	Data 0.005 (0.012)	Loss 1.5215 (1.5979)	Acc@1 73.613 (70.856)	Acc@5 98.051 (97.972)
Epoch: [6][60/75]	Time 1.005 (1.012)	Data 0.005 (0.011)	Loss 1.5313 (1.5903)	Acc@1 73.463 (71.052)	Acc@5 98.801 (98.029)
Epoch: [6][70/75]	Time 1.026 (1.012)	Data 0.005 (0.010)	Loss 1.4575 (1.5833)	Acc@1 74.813 (71.206)	Acc@5 98.651 (98.040)

Epoche:  6  ; NumbOfParameters:  1082906

Test Acc:  61.28

Epoch: [7 | 80] LR: 0.100000
Epoch: [7][0/75]	Time 0.999 (0.999)	Data 0.345 (0.345)	Loss 1.5341 (1.5341)	Acc@1 71.664 (71.664)	Acc@5 98.201 (98.201)
Epoch: [7][10/75]	Time 1.005 (1.012)	Data 0.005 (0.036)	Loss 1.5044 (1.5050)	Acc@1 73.463 (73.313)	Acc@5 99.250 (98.487)
Epoch: [7][20/75]	Time 1.011 (1.012)	Data 0.005 (0.021)	Loss 1.4848 (1.4966)	Acc@1 74.213 (73.470)	Acc@5 97.901 (98.422)
Epoch: [7][30/75]	Time 1.016 (1.011)	Data 0.005 (0.016)	Loss 1.4886 (1.4953)	Acc@1 70.915 (73.454)	Acc@5 98.801 (98.404)
Epoch: [7][40/75]	Time 1.016 (1.011)	Data 0.005 (0.013)	Loss 1.5672 (1.4904)	Acc@1 70.315 (73.445)	Acc@5 98.501 (98.420)
Epoch: [7][50/75]	Time 1.018 (1.012)	Data 0.004 (0.012)	Loss 1.4490 (1.4861)	Acc@1 73.463 (73.466)	Acc@5 98.351 (98.377)
Epoch: [7][60/75]	Time 1.006 (1.012)	Data 0.004 (0.011)	Loss 1.3948 (1.4827)	Acc@1 76.462 (73.527)	Acc@5 97.601 (98.304)
Epoch: [7][70/75]	Time 1.031 (1.013)	Data 0.005 (0.010)	Loss 1.5002 (1.4800)	Acc@1 74.063 (73.563)	Acc@5 98.351 (98.328)

Epoche:  7  ; NumbOfParameters:  1082906

Test Acc:  64.53

Epoch: [8 | 80] LR: 0.100000
Epoch: [8][0/75]	Time 0.985 (0.985)	Data 0.348 (0.348)	Loss 1.4388 (1.4388)	Acc@1 75.562 (75.562)	Acc@5 98.951 (98.951)
Epoch: [8][10/75]	Time 1.007 (1.013)	Data 0.005 (0.036)	Loss 1.3812 (1.4187)	Acc@1 76.762 (75.276)	Acc@5 99.400 (98.705)
Epoch: [8][20/75]	Time 1.020 (1.013)	Data 0.005 (0.021)	Loss 1.3198 (1.4174)	Acc@1 77.361 (75.155)	Acc@5 99.250 (98.529)
Epoch: [8][30/75]	Time 1.002 (1.013)	Data 0.005 (0.016)	Loss 1.3628 (1.4099)	Acc@1 74.963 (75.267)	Acc@5 99.700 (98.525)
Epoch: [8][40/75]	Time 1.009 (1.013)	Data 0.005 (0.013)	Loss 1.3753 (1.4012)	Acc@1 76.012 (75.379)	Acc@5 98.501 (98.526)
Epoch: [8][50/75]	Time 1.015 (1.014)	Data 0.005 (0.012)	Loss 1.3970 (1.3942)	Acc@1 75.712 (75.627)	Acc@5 98.651 (98.527)
Epoch: [8][60/75]	Time 1.011 (1.014)	Data 0.005 (0.011)	Loss 1.3537 (1.3866)	Acc@1 76.312 (75.791)	Acc@5 97.901 (98.550)
Epoch: [8][70/75]	Time 1.027 (1.014)	Data 0.004 (0.010)	Loss 1.3395 (1.3808)	Acc@1 76.462 (75.858)	Acc@5 99.400 (98.613)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoche:  8  ; NumbOfParameters:  1082906

Test Acc:  70.85

Epoch: [9 | 80] LR: 0.100000
Epoch: [9][0/75]	Time 1.009 (1.009)	Data 0.344 (0.344)	Loss 1.3799 (1.3799)	Acc@1 75.862 (75.862)	Acc@5 98.351 (98.351)
Epoch: [9][10/75]	Time 1.000 (1.010)	Data 0.006 (0.036)	Loss 1.2648 (1.2826)	Acc@1 78.411 (78.493)	Acc@5 99.400 (98.978)
Epoch: [9][20/75]	Time 1.006 (1.008)	Data 0.004 (0.021)	Loss 1.2973 (1.2862)	Acc@1 77.211 (78.140)	Acc@5 99.100 (98.908)
Epoch: [9][30/75]	Time 1.012 (1.009)	Data 0.005 (0.016)	Loss 1.2987 (1.2986)	Acc@1 78.861 (77.676)	Acc@5 98.351 (98.801)
Epoch: [9][40/75]	Time 1.015 (1.010)	Data 0.005 (0.013)	Loss 1.2901 (1.2978)	Acc@1 78.261 (77.661)	Acc@5 98.801 (98.793)
Epoch: [9][50/75]	Time 1.011 (1.010)	Data 0.005 (0.012)	Loss 1.2904 (1.2965)	Acc@1 76.012 (77.670)	Acc@5 98.801 (98.798)
Epoch: [9][60/75]	Time 1.011 (1.011)	Data 0.003 (0.011)	Loss 1.3047 (1.2980)	Acc@1 76.312 (77.526)	Acc@5 98.501 (98.791)
Epoch: [9][70/75]	Time 1.018 (1.012)	Data 0.005 (0.010)	Loss 1.2786 (1.2958)	Acc@1 77.211 (77.501)	Acc@5 98.801 (98.790)

Epoche:  9  ; NumbOfParameters:  1082906

Test Acc:  62.81

Epoch: [10 | 80] LR: 0.100000
Epoch: [10][0/75]	Time 0.992 (0.992)	Data 0.351 (0.351)	Loss 1.2640 (1.2640)	Acc@1 78.261 (78.261)	Acc@5 99.100 (99.100)
Epoch: [10][10/75]	Time 1.021 (1.013)	Data 0.005 (0.037)	Loss 1.2401 (1.2543)	Acc@1 78.711 (78.370)	Acc@5 99.100 (98.937)
Epoch: [10][20/75]	Time 1.026 (1.015)	Data 0.006 (0.021)	Loss 1.2205 (1.2512)	Acc@1 78.111 (78.561)	Acc@5 99.400 (98.886)
Epoch: [10][30/75]	Time 1.010 (1.015)	Data 0.005 (0.016)	Loss 1.2431 (1.2448)	Acc@1 77.061 (78.575)	Acc@5 98.801 (98.907)
Epoch: [10][40/75]	Time 1.014 (1.014)	Data 0.005 (0.013)	Loss 1.2356 (1.2484)	Acc@1 80.060 (78.455)	Acc@5 97.601 (98.844)
Epoch: [10][50/75]	Time 1.019 (1.014)	Data 0.005 (0.012)	Loss 1.1646 (1.2448)	Acc@1 80.210 (78.534)	Acc@5 99.100 (98.824)
Epoch: [10][60/75]	Time 1.015 (1.014)	Data 0.005 (0.011)	Loss 1.2602 (1.2418)	Acc@1 77.211 (78.571)	Acc@5 98.801 (98.798)
Epoch: [10][70/75]	Time 1.021 (1.015)	Data 0.005 (0.010)	Loss 1.1868 (1.2408)	Acc@1 78.711 (78.521)	Acc@5 98.951 (98.792)

Epoche:  10  ; NumbOfParameters:  1082906

Test Acc:  46.44

Epoch: [11 | 80] LR: 0.100000
Epoch: [11][0/75]	Time 1.004 (1.004)	Data 0.349 (0.349)	Loss 1.1976 (1.1976)	Acc@1 79.460 (79.460)	Acc@5 99.250 (99.250)
Epoch: [11][10/75]	Time 1.016 (1.011)	Data 0.005 (0.036)	Loss 1.1703 (1.1832)	Acc@1 81.259 (79.815)	Acc@5 99.400 (99.141)
Epoch: [11][20/75]	Time 1.008 (1.013)	Data 0.005 (0.021)	Loss 1.2542 (1.1810)	Acc@1 77.811 (79.703)	Acc@5 98.801 (99.072)
Epoch: [11][30/75]	Time 1.023 (1.015)	Data 0.004 (0.016)	Loss 1.2280 (1.1775)	Acc@1 76.612 (79.654)	Acc@5 98.501 (99.071)
Epoch: [11][40/75]	Time 1.018 (1.015)	Data 0.005 (0.013)	Loss 1.1859 (1.1791)	Acc@1 78.861 (79.515)	Acc@5 99.100 (99.031)
Epoch: [11][50/75]	Time 1.005 (1.015)	Data 0.004 (0.012)	Loss 1.1621 (1.1758)	Acc@1 81.559 (79.551)	Acc@5 99.250 (98.989)
Epoch: [11][60/75]	Time 1.017 (1.015)	Data 0.005 (0.011)	Loss 1.2613 (1.1751)	Acc@1 76.462 (79.485)	Acc@5 99.100 (98.992)
Epoch: [11][70/75]	Time 1.020 (1.015)	Data 0.005 (0.010)	Loss 1.1520 (1.1757)	Acc@1 79.910 (79.452)	Acc@5 98.951 (98.965)

Epoche:  11  ; NumbOfParameters:  1082906

Test Acc:  58.67

Epoch: [12 | 80] LR: 0.100000
Epoch: [12][0/75]	Time 0.996 (0.996)	Data 0.359 (0.359)	Loss 1.1595 (1.1595)	Acc@1 79.010 (79.010)	Acc@5 98.501 (98.501)
Epoch: [12][10/75]	Time 1.016 (1.018)	Data 0.005 (0.036)	Loss 1.1133 (1.1591)	Acc@1 81.259 (79.474)	Acc@5 99.100 (98.951)
Epoch: [12][20/75]	Time 1.009 (1.014)	Data 0.005 (0.022)	Loss 1.0962 (1.1390)	Acc@1 81.559 (80.124)	Acc@5 98.951 (99.029)
Epoch: [12][30/75]	Time 1.016 (1.014)	Data 0.005 (0.016)	Loss 1.1233 (1.1291)	Acc@1 79.460 (80.307)	Acc@5 98.951 (99.057)
Epoch: [12][40/75]	Time 1.009 (1.013)	Data 0.005 (0.014)	Loss 1.1304 (1.1301)	Acc@1 79.760 (80.082)	Acc@5 99.400 (99.064)
Epoch: [12][50/75]	Time 1.005 (1.013)	Data 0.005 (0.012)	Loss 1.1100 (1.1263)	Acc@1 79.910 (80.136)	Acc@5 99.250 (99.065)
Epoch: [12][60/75]	Time 1.012 (1.013)	Data 0.004 (0.011)	Loss 1.1129 (1.1221)	Acc@1 80.660 (80.266)	Acc@5 98.351 (99.076)
Epoch: [12][70/75]	Time 1.004 (1.013)	Data 0.004 (0.010)	Loss 1.0877 (1.1162)	Acc@1 82.309 (80.527)	Acc@5 98.951 (99.103)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoche:  12  ; NumbOfParameters:  1082906

Test Acc:  69.6

Epoch: [13 | 80] LR: 0.100000
Epoch: [13][0/75]	Time 1.011 (1.011)	Data 0.340 (0.340)	Loss 1.0697 (1.0697)	Acc@1 80.810 (80.810)	Acc@5 99.400 (99.400)
Epoch: [13][10/75]	Time 1.012 (1.010)	Data 0.005 (0.035)	Loss 1.1034 (1.0469)	Acc@1 79.760 (82.118)	Acc@5 99.100 (99.387)
Epoch: [13][20/75]	Time 1.009 (1.009)	Data 0.005 (0.021)	Loss 1.0678 (1.0548)	Acc@1 80.060 (81.988)	Acc@5 99.400 (99.236)
Epoch: [13][30/75]	Time 1.005 (1.010)	Data 0.005 (0.016)	Loss 1.1096 (1.0589)	Acc@1 78.561 (81.728)	Acc@5 99.550 (99.250)
Epoch: [13][40/75]	Time 1.018 (1.010)	Data 0.005 (0.013)	Loss 1.0883 (1.0662)	Acc@1 80.210 (81.486)	Acc@5 99.250 (99.206)
Epoch: [13][50/75]	Time 1.003 (1.010)	Data 0.005 (0.012)	Loss 1.0973 (1.0717)	Acc@1 79.010 (81.271)	Acc@5 99.100 (99.156)
Epoch: [13][60/75]	Time 1.000 (1.010)	Data 0.005 (0.011)	Loss 1.0686 (1.0740)	Acc@1 80.510 (81.068)	Acc@5 99.400 (99.159)
Epoch: [13][70/75]	Time 1.015 (1.011)	Data 0.005 (0.010)	Loss 1.0657 (1.0787)	Acc@1 82.009 (80.890)	Acc@5 98.501 (99.122)

Epoche:  13  ; NumbOfParameters:  1082906

Test Acc:  70.53

Epoch: [14 | 80] LR: 0.100000
Epoch: [14][0/75]	Time 1.003 (1.003)	Data 0.346 (0.346)	Loss 0.9956 (0.9956)	Acc@1 84.708 (84.708)	Acc@5 99.400 (99.400)
Epoch: [14][10/75]	Time 1.026 (1.019)	Data 0.005 (0.036)	Loss 1.0301 (1.0473)	Acc@1 80.060 (81.723)	Acc@5 99.250 (98.951)
Epoch: [14][20/75]	Time 1.017 (1.017)	Data 0.005 (0.021)	Loss 1.0612 (1.0427)	Acc@1 82.309 (81.852)	Acc@5 98.651 (99.058)
Epoch: [14][30/75]	Time 1.028 (1.017)	Data 0.003 (0.016)	Loss 1.0260 (1.0384)	Acc@1 81.409 (81.903)	Acc@5 98.801 (99.086)
Epoch: [14][40/75]	Time 1.011 (1.018)	Data 0.005 (0.013)	Loss 1.0640 (1.0346)	Acc@1 80.960 (81.994)	Acc@5 99.100 (99.093)
Epoch: [14][50/75]	Time 1.023 (1.018)	Data 0.005 (0.012)	Loss 1.0136 (1.0364)	Acc@1 82.009 (81.821)	Acc@5 99.550 (99.100)
Epoch: [14][60/75]	Time 1.012 (1.018)	Data 0.005 (0.011)	Loss 1.0328 (1.0347)	Acc@1 80.810 (81.699)	Acc@5 99.400 (99.130)
Epoch: [14][70/75]	Time 1.036 (1.019)	Data 0.004 (0.010)	Loss 0.9829 (1.0341)	Acc@1 83.508 (81.597)	Acc@5 99.400 (99.149)

Epoche:  14  ; NumbOfParameters:  1082906

Test Acc:  68.2

Epoch: [15 | 80] LR: 0.100000
Epoch: [15][0/75]	Time 0.999 (0.999)	Data 0.359 (0.359)	Loss 1.0065 (1.0065)	Acc@1 81.409 (81.409)	Acc@5 99.250 (99.250)
Epoch: [15][10/75]	Time 1.013 (1.013)	Data 0.005 (0.036)	Loss 1.0180 (0.9949)	Acc@1 81.709 (82.568)	Acc@5 99.550 (99.264)
Epoch: [15][20/75]	Time 1.019 (1.014)	Data 0.004 (0.021)	Loss 0.9803 (0.9983)	Acc@1 82.459 (82.487)	Acc@5 99.550 (99.279)
Epoch: [15][30/75]	Time 1.014 (1.014)	Data 0.005 (0.016)	Loss 0.9824 (0.9999)	Acc@1 83.508 (82.207)	Acc@5 99.100 (99.250)
Epoch: [15][40/75]	Time 1.010 (1.014)	Data 0.005 (0.014)	Loss 1.0211 (0.9994)	Acc@1 81.559 (82.104)	Acc@5 99.550 (99.250)
Epoch: [15][50/75]	Time 1.011 (1.014)	Data 0.005 (0.012)	Loss 0.9619 (0.9994)	Acc@1 81.409 (81.983)	Acc@5 98.951 (99.224)
Epoch: [15][60/75]	Time 1.012 (1.014)	Data 0.005 (0.011)	Loss 0.9891 (0.9986)	Acc@1 81.859 (81.975)	Acc@5 98.951 (99.204)
Epoch: [15][70/75]	Time 1.042 (1.015)	Data 0.005 (0.010)	Loss 0.9405 (1.0021)	Acc@1 84.258 (81.842)	Acc@5 99.700 (99.176)

Epoche:  15  ; NumbOfParameters:  1082906

Test Acc:  61.09

Epoch: [16 | 80] LR: 0.100000
Epoch: [16][0/75]	Time 1.002 (1.002)	Data 0.361 (0.361)	Loss 0.9952 (0.9952)	Acc@1 80.210 (80.210)	Acc@5 98.951 (98.951)
Epoch: [16][10/75]	Time 1.015 (1.014)	Data 0.005 (0.036)	Loss 0.9044 (0.9505)	Acc@1 84.858 (83.168)	Acc@5 99.400 (99.359)
Epoch: [16][20/75]	Time 1.013 (1.013)	Data 0.005 (0.021)	Loss 1.0284 (0.9585)	Acc@1 79.160 (82.880)	Acc@5 99.100 (99.243)
Epoch: [16][30/75]	Time 1.022 (1.012)	Data 0.005 (0.016)	Loss 0.9079 (0.9508)	Acc@1 83.358 (83.112)	Acc@5 99.400 (99.246)
Epoch: [16][40/75]	Time 1.017 (1.012)	Data 0.006 (0.013)	Loss 0.9407 (0.9536)	Acc@1 82.009 (82.755)	Acc@5 99.400 (99.276)
Epoch: [16][50/75]	Time 1.019 (1.013)	Data 0.005 (0.012)	Loss 0.9781 (0.9514)	Acc@1 80.660 (82.706)	Acc@5 98.951 (99.283)
Epoch: [16][60/75]	Time 1.008 (1.013)	Data 0.005 (0.011)	Loss 0.9308 (0.9529)	Acc@1 82.909 (82.599)	Acc@5 99.550 (99.285)
Epoch: [16][70/75]	Time 1.036 (1.014)	Data 0.005 (0.010)	Loss 0.9366 (0.9521)	Acc@1 82.759 (82.600)	Acc@5 99.550 (99.295)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoche:  16  ; NumbOfParameters:  1082906

Test Acc:  73.27

Epoch: [17 | 80] LR: 0.100000
Epoch: [17][0/75]	Time 1.009 (1.009)	Data 0.343 (0.343)	Loss 0.8652 (0.8652)	Acc@1 85.757 (85.757)	Acc@5 99.850 (99.850)
Epoch: [17][10/75]	Time 1.001 (1.010)	Data 0.005 (0.036)	Loss 0.8819 (0.8784)	Acc@1 85.757 (85.007)	Acc@5 99.700 (99.564)
Epoch: [17][20/75]	Time 1.004 (1.010)	Data 0.005 (0.021)	Loss 0.8696 (0.8827)	Acc@1 84.858 (84.950)	Acc@5 99.850 (99.507)
Epoch: [17][30/75]	Time 1.014 (1.011)	Data 0.005 (0.016)	Loss 0.9183 (0.8985)	Acc@1 82.459 (84.301)	Acc@5 99.400 (99.405)
Epoch: [17][40/75]	Time 1.021 (1.012)	Data 0.005 (0.013)	Loss 0.9257 (0.9088)	Acc@1 82.159 (83.889)	Acc@5 99.550 (99.364)
Epoch: [17][50/75]	Time 1.020 (1.013)	Data 0.004 (0.012)	Loss 0.9964 (0.9154)	Acc@1 80.810 (83.544)	Acc@5 99.400 (99.344)
Epoch: [17][60/75]	Time 1.006 (1.013)	Data 0.004 (0.011)	Loss 0.9229 (0.9197)	Acc@1 82.159 (83.425)	Acc@5 98.951 (99.297)
Epoch: [17][70/75]	Time 1.031 (1.014)	Data 0.005 (0.010)	Loss 0.9320 (0.9240)	Acc@1 82.609 (83.177)	Acc@5 99.400 (99.261)

Epoche:  17  ; NumbOfParameters:  1082906

Test Acc:  71.15

Epoch: [18 | 80] LR: 0.100000
Epoch: [18][0/75]	Time 1.003 (1.003)	Data 0.351 (0.351)	Loss 0.8611 (0.8611)	Acc@1 85.157 (85.157)	Acc@5 99.700 (99.700)
Epoch: [18][10/75]	Time 1.011 (1.016)	Data 0.005 (0.036)	Loss 0.9209 (0.8934)	Acc@1 85.607 (84.326)	Acc@5 98.801 (99.264)
Epoch: [18][20/75]	Time 1.012 (1.015)	Data 0.005 (0.021)	Loss 0.9444 (0.8951)	Acc@1 80.360 (83.722)	Acc@5 99.550 (99.350)
Epoch: [18][30/75]	Time 1.021 (1.015)	Data 0.005 (0.016)	Loss 0.8883 (0.9010)	Acc@1 83.508 (83.387)	Acc@5 99.700 (99.313)
Epoch: [18][40/75]	Time 1.014 (1.015)	Data 0.005 (0.013)	Loss 0.9125 (0.9049)	Acc@1 83.658 (83.186)	Acc@5 99.400 (99.283)
Epoch: [18][50/75]	Time 1.019 (1.014)	Data 0.005 (0.012)	Loss 0.8833 (0.9040)	Acc@1 83.058 (83.223)	Acc@5 99.100 (99.277)
Epoch: [18][60/75]	Time 1.010 (1.014)	Data 0.005 (0.011)	Loss 0.8426 (0.9030)	Acc@1 85.457 (83.331)	Acc@5 98.951 (99.263)
Epoch: [18][70/75]	Time 1.022 (1.015)	Data 0.005 (0.010)	Loss 0.8290 (0.9027)	Acc@1 86.057 (83.276)	Acc@5 99.550 (99.257)

Epoche:  18  ; NumbOfParameters:  1082906

Test Acc:  65.45

Epoch: [19 | 80] LR: 0.100000
Epoch: [19][0/75]	Time 1.003 (1.003)	Data 0.349 (0.349)	Loss 0.8803 (0.8803)	Acc@1 85.607 (85.607)	Acc@5 99.250 (99.250)
Epoch: [19][10/75]	Time 1.028 (1.016)	Data 0.005 (0.036)	Loss 0.8761 (0.8785)	Acc@1 84.108 (83.849)	Acc@5 99.250 (99.414)
Epoch: [19][20/75]	Time 1.023 (1.018)	Data 0.005 (0.021)	Loss 0.8719 (0.8807)	Acc@1 84.858 (83.787)	Acc@5 98.951 (99.293)
Epoch: [19][30/75]	Time 1.006 (1.017)	Data 0.005 (0.016)	Loss 0.8877 (0.8756)	Acc@1 83.058 (83.885)	Acc@5 99.400 (99.333)
Epoch: [19][40/75]	Time 1.010 (1.016)	Data 0.005 (0.013)	Loss 0.8504 (0.8753)	Acc@1 85.757 (83.848)	Acc@5 99.550 (99.345)
Epoch: [19][50/75]	Time 1.017 (1.016)	Data 0.005 (0.012)	Loss 0.8541 (0.8713)	Acc@1 83.808 (83.896)	Acc@5 99.100 (99.347)
Epoch: [19][60/75]	Time 1.013 (1.015)	Data 0.005 (0.011)	Loss 0.8491 (0.8722)	Acc@1 84.258 (83.914)	Acc@5 99.850 (99.339)
Epoch: [19][70/75]	Time 1.007 (1.015)	Data 0.005 (0.010)	Loss 0.9024 (0.8759)	Acc@1 82.309 (83.694)	Acc@5 98.801 (99.309)

Epoche:  19  ; NumbOfParameters:  1082906

Test Acc:  70.16

Epoch: [20 | 80] LR: 0.100000
Epoch: [20][0/75]	Time 1.007 (1.007)	Data 0.348 (0.348)	Loss 0.8239 (0.8239)	Acc@1 85.307 (85.307)	Acc@5 99.550 (99.550)
Epoch: [20][10/75]	Time 0.990 (1.013)	Data 0.005 (0.036)	Loss 0.8614 (0.8470)	Acc@1 82.909 (84.503)	Acc@5 99.250 (99.455)
Epoch: [20][20/75]	Time 1.016 (1.013)	Data 0.005 (0.021)	Loss 0.8123 (0.8496)	Acc@1 86.207 (84.308)	Acc@5 99.700 (99.372)
Epoch: [20][30/75]	Time 1.013 (1.013)	Data 0.005 (0.016)	Loss 0.8404 (0.8564)	Acc@1 85.007 (83.948)	Acc@5 99.250 (99.381)
Epoch: [20][40/75]	Time 1.019 (1.013)	Data 0.004 (0.013)	Loss 0.8887 (0.8606)	Acc@1 81.109 (83.644)	Acc@5 99.400 (99.345)
Epoch: [20][50/75]	Time 1.019 (1.013)	Data 0.005 (0.012)	Loss 0.8330 (0.8621)	Acc@1 82.459 (83.544)	Acc@5 99.700 (99.356)
Epoch: [20][60/75]	Time 1.015 (1.014)	Data 0.005 (0.011)	Loss 0.8769 (0.8637)	Acc@1 84.108 (83.503)	Acc@5 99.100 (99.329)
Epoch: [20][70/75]	Time 1.023 (1.015)	Data 0.003 (0.010)	Loss 0.8546 (0.8621)	Acc@1 82.759 (83.574)	Acc@5 99.250 (99.343)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoche:  20  ; NumbOfParameters:  1082906

Test Acc:  65.92

Epoch: [21 | 80] LR: 0.100000
Epoch: [21][0/75]	Time 1.005 (1.005)	Data 0.355 (0.355)	Loss 0.8655 (0.8655)	Acc@1 83.958 (83.958)	Acc@5 99.550 (99.550)
Epoch: [21][10/75]	Time 1.010 (1.009)	Data 0.005 (0.037)	Loss 0.7965 (0.7848)	Acc@1 86.057 (86.234)	Acc@5 98.951 (99.564)
Epoch: [21][20/75]	Time 1.007 (1.009)	Data 0.005 (0.022)	Loss 0.8075 (0.7927)	Acc@1 85.607 (86.014)	Acc@5 99.700 (99.493)
Epoch: [21][30/75]	Time 1.008 (1.010)	Data 0.005 (0.016)	Loss 0.8596 (0.8071)	Acc@1 83.208 (85.288)	Acc@5 98.351 (99.444)
Epoch: [21][40/75]	Time 1.019 (1.011)	Data 0.005 (0.014)	Loss 0.8822 (0.8174)	Acc@1 82.159 (84.792)	Acc@5 98.951 (99.404)
Epoch: [21][50/75]	Time 1.007 (1.012)	Data 0.005 (0.012)	Loss 0.8523 (0.8253)	Acc@1 81.859 (84.575)	Acc@5 99.250 (99.380)
Epoch: [21][60/75]	Time 1.018 (1.012)	Data 0.005 (0.011)	Loss 0.8501 (0.8273)	Acc@1 82.609 (84.486)	Acc@5 99.400 (99.351)
Epoch: [21][70/75]	Time 1.025 (1.012)	Data 0.005 (0.010)	Loss 0.8911 (0.8273)	Acc@1 80.960 (84.448)	Acc@5 99.100 (99.352)

Epoche:  21  ; NumbOfParameters:  1082906

Test Acc:  63.03

Epoch: [22 | 80] LR: 0.100000
Epoch: [22][0/75]	Time 0.998 (0.998)	Data 0.358 (0.358)	Loss 0.8357 (0.8357)	Acc@1 84.258 (84.258)	Acc@5 99.100 (99.100)
Epoch: [22][10/75]	Time 1.021 (1.016)	Data 0.005 (0.037)	Loss 0.7967 (0.8061)	Acc@1 85.607 (84.994)	Acc@5 99.400 (99.414)
Epoch: [22][20/75]	Time 1.014 (1.014)	Data 0.005 (0.022)	Loss 0.8225 (0.8166)	Acc@1 83.958 (84.465)	Acc@5 98.801 (99.379)
Epoch: [22][30/75]	Time 1.033 (1.015)	Data 0.005 (0.016)	Loss 0.8220 (0.8186)	Acc@1 83.208 (84.422)	Acc@5 99.250 (99.429)
Epoch: [22][40/75]	Time 1.025 (1.015)	Data 0.005 (0.014)	Loss 0.8331 (0.8272)	Acc@1 83.958 (84.126)	Acc@5 99.400 (99.400)
Epoch: [22][50/75]	Time 1.020 (1.015)	Data 0.005 (0.012)	Loss 0.8232 (0.8296)	Acc@1 83.808 (83.979)	Acc@5 99.550 (99.383)
Epoch: [22][60/75]	Time 1.009 (1.015)	Data 0.005 (0.011)	Loss 0.8483 (0.8305)	Acc@1 82.909 (83.919)	Acc@5 99.400 (99.388)
Epoch: [22][70/75]	Time 1.036 (1.015)	Data 0.005 (0.010)	Loss 0.8313 (0.8320)	Acc@1 82.459 (83.859)	Acc@5 99.400 (99.379)

Epoche:  22  ; NumbOfParameters:  1082906

Test Acc:  63.02

Epoch: [23 | 80] LR: 0.100000
Epoch: [23][0/75]	Time 1.009 (1.009)	Data 0.385 (0.385)	Loss 0.7939 (0.7939)	Acc@1 86.657 (86.657)	Acc@5 99.100 (99.100)
Epoch: [23][10/75]	Time 1.022 (1.014)	Data 0.005 (0.039)	Loss 0.8200 (0.8074)	Acc@1 84.708 (84.912)	Acc@5 99.250 (99.346)
Epoch: [23][20/75]	Time 1.011 (1.014)	Data 0.005 (0.023)	Loss 0.8230 (0.7990)	Acc@1 84.708 (85.036)	Acc@5 99.250 (99.422)
Epoch: [23][30/75]	Time 1.008 (1.014)	Data 0.005 (0.017)	Loss 0.8114 (0.7952)	Acc@1 83.808 (84.988)	Acc@5 98.801 (99.386)
Epoch: [23][40/75]	Time 0.996 (1.013)	Data 0.005 (0.014)	Loss 0.8269 (0.7970)	Acc@1 83.958 (84.905)	Acc@5 98.801 (99.386)
Epoch: [23][50/75]	Time 1.017 (1.013)	Data 0.005 (0.012)	Loss 0.7843 (0.8007)	Acc@1 85.007 (84.781)	Acc@5 99.550 (99.391)
Epoch: [23][60/75]	Time 1.022 (1.013)	Data 0.005 (0.011)	Loss 0.7743 (0.8017)	Acc@1 85.007 (84.673)	Acc@5 100.000 (99.378)
Epoch: [23][70/75]	Time 1.033 (1.013)	Data 0.005 (0.010)	Loss 0.8088 (0.8018)	Acc@1 83.808 (84.665)	Acc@5 99.550 (99.386)

Epoche:  23  ; NumbOfParameters:  1082906

Test Acc:  75.56

Epoch: [24 | 80] LR: 0.100000
Epoch: [24][0/75]	Time 0.997 (0.997)	Data 0.357 (0.357)	Loss 0.7569 (0.7569)	Acc@1 85.307 (85.307)	Acc@5 99.550 (99.550)
Epoch: [24][10/75]	Time 0.999 (1.014)	Data 0.005 (0.036)	Loss 0.7966 (0.7641)	Acc@1 83.208 (85.525)	Acc@5 99.400 (99.577)
Epoch: [24][20/75]	Time 1.006 (1.012)	Data 0.005 (0.022)	Loss 0.7865 (0.7762)	Acc@1 85.307 (85.314)	Acc@5 98.951 (99.422)
Epoch: [24][30/75]	Time 1.006 (1.012)	Data 0.005 (0.016)	Loss 0.8109 (0.7838)	Acc@1 82.909 (85.032)	Acc@5 99.700 (99.371)
Epoch: [24][40/75]	Time 1.011 (1.012)	Data 0.006 (0.013)	Loss 0.8281 (0.7910)	Acc@1 83.808 (84.821)	Acc@5 99.550 (99.382)
Epoch: [24][50/75]	Time 1.014 (1.012)	Data 0.005 (0.012)	Loss 0.7714 (0.7951)	Acc@1 85.907 (84.690)	Acc@5 99.250 (99.386)
Epoch: [24][60/75]	Time 1.008 (1.012)	Data 0.005 (0.011)	Loss 0.8327 (0.7965)	Acc@1 82.909 (84.536)	Acc@5 99.700 (99.386)
Epoch: [24][70/75]	Time 1.020 (1.012)	Data 0.005 (0.010)	Loss 0.8177 (0.7952)	Acc@1 83.358 (84.539)	Acc@5 99.250 (99.386)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(127, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(126, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(126, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 1076006 ; 1082906 ; 0.9936282558227584
new batch_size: 667

Epoche:  24  ; NumbOfParameters:  1076006

Test Acc:  66.54

Epoch: [25 | 80] LR: 0.100000
Epoch: [25][0/75]	Time 1.141 (1.141)	Data 0.348 (0.348)	Loss 0.7927 (0.7927)	Acc@1 84.858 (84.858)	Acc@5 99.550 (99.550)
Epoch: [25][10/75]	Time 0.997 (1.017)	Data 0.005 (0.036)	Loss 0.7498 (0.7448)	Acc@1 87.106 (86.261)	Acc@5 99.700 (99.537)
Epoch: [25][20/75]	Time 1.011 (1.012)	Data 0.005 (0.021)	Loss 0.7523 (0.7420)	Acc@1 86.207 (86.300)	Acc@5 99.400 (99.536)
Epoch: [25][30/75]	Time 1.014 (1.010)	Data 0.005 (0.016)	Loss 0.8237 (0.7567)	Acc@1 83.808 (85.738)	Acc@5 99.550 (99.487)
Epoch: [25][40/75]	Time 1.009 (1.011)	Data 0.005 (0.013)	Loss 0.8061 (0.7636)	Acc@1 85.457 (85.366)	Acc@5 99.250 (99.466)
Epoch: [25][50/75]	Time 1.008 (1.011)	Data 0.006 (0.012)	Loss 0.7812 (0.7726)	Acc@1 85.607 (85.122)	Acc@5 99.400 (99.430)
Epoch: [25][60/75]	Time 1.011 (1.011)	Data 0.005 (0.011)	Loss 0.7942 (0.7714)	Acc@1 83.358 (85.076)	Acc@5 98.951 (99.442)
Epoch: [25][70/75]	Time 1.030 (1.012)	Data 0.005 (0.010)	Loss 0.7905 (0.7751)	Acc@1 85.307 (84.963)	Acc@5 99.250 (99.419)

Epoche:  25  ; NumbOfParameters:  1076006

Test Acc:  55.54

Epoch: [26 | 80] LR: 0.100000
Epoch: [26][0/75]	Time 0.991 (0.991)	Data 0.347 (0.347)	Loss 0.7549 (0.7549)	Acc@1 86.957 (86.957)	Acc@5 99.400 (99.400)
Epoch: [26][10/75]	Time 0.991 (1.007)	Data 0.005 (0.036)	Loss 0.7999 (0.7829)	Acc@1 84.408 (84.612)	Acc@5 99.400 (99.291)
Epoch: [26][20/75]	Time 1.017 (1.009)	Data 0.005 (0.021)	Loss 0.7797 (0.7828)	Acc@1 86.057 (84.908)	Acc@5 99.550 (99.415)
Epoch: [26][30/75]	Time 1.010 (1.009)	Data 0.005 (0.016)	Loss 0.7522 (0.7833)	Acc@1 85.307 (84.877)	Acc@5 99.850 (99.434)
Epoch: [26][40/75]	Time 1.015 (1.009)	Data 0.005 (0.013)	Loss 0.8175 (0.7825)	Acc@1 83.658 (84.887)	Acc@5 99.250 (99.415)
Epoch: [26][50/75]	Time 0.994 (1.009)	Data 0.005 (0.012)	Loss 0.7740 (0.7846)	Acc@1 86.657 (84.908)	Acc@5 99.400 (99.391)
Epoch: [26][60/75]	Time 1.018 (1.009)	Data 0.005 (0.010)	Loss 0.7597 (0.7869)	Acc@1 84.258 (84.818)	Acc@5 99.850 (99.388)
Epoch: [26][70/75]	Time 1.031 (1.010)	Data 0.005 (0.010)	Loss 0.7934 (0.7866)	Acc@1 86.057 (84.828)	Acc@5 99.250 (99.364)

Epoche:  26  ; NumbOfParameters:  1076006

Test Acc:  72.53

Epoch: [27 | 80] LR: 0.100000
Epoch: [27][0/75]	Time 0.993 (0.993)	Data 0.347 (0.347)	Loss 0.7319 (0.7319)	Acc@1 86.657 (86.657)	Acc@5 99.550 (99.550)
Epoch: [27][10/75]	Time 1.019 (1.009)	Data 0.004 (0.036)	Loss 0.7311 (0.7574)	Acc@1 85.157 (85.716)	Acc@5 99.550 (99.455)
Epoch: [27][20/75]	Time 1.009 (1.009)	Data 0.005 (0.021)	Loss 0.7498 (0.7671)	Acc@1 87.106 (85.486)	Acc@5 98.951 (99.407)
Epoch: [27][30/75]	Time 1.010 (1.008)	Data 0.005 (0.016)	Loss 0.7200 (0.7579)	Acc@1 87.406 (85.738)	Acc@5 99.550 (99.453)
Epoch: [27][40/75]	Time 0.999 (1.009)	Data 0.005 (0.013)	Loss 0.7972 (0.7611)	Acc@1 83.958 (85.461)	Acc@5 99.550 (99.462)
Epoch: [27][50/75]	Time 1.004 (1.009)	Data 0.005 (0.011)	Loss 0.7785 (0.7633)	Acc@1 83.958 (85.346)	Acc@5 99.550 (99.480)
Epoch: [27][60/75]	Time 1.011 (1.009)	Data 0.005 (0.010)	Loss 0.7751 (0.7661)	Acc@1 84.858 (85.261)	Acc@5 99.400 (99.449)
Epoch: [27][70/75]	Time 1.024 (1.010)	Data 0.005 (0.010)	Loss 0.7410 (0.7660)	Acc@1 86.507 (85.225)	Acc@5 99.250 (99.445)

Epoche:  27  ; NumbOfParameters:  1076006

Test Acc:  67.11

Epoch: [28 | 80] LR: 0.100000
Epoch: [28][0/75]	Time 0.995 (0.995)	Data 0.345 (0.345)	Loss 0.7680 (0.7680)	Acc@1 85.757 (85.757)	Acc@5 99.100 (99.100)
Epoch: [28][10/75]	Time 1.007 (1.008)	Data 0.005 (0.036)	Loss 0.7167 (0.7511)	Acc@1 86.957 (85.716)	Acc@5 99.250 (99.482)
Epoch: [28][20/75]	Time 1.003 (1.011)	Data 0.005 (0.021)	Loss 0.7596 (0.7548)	Acc@1 84.558 (85.479)	Acc@5 99.100 (99.450)
Epoch: [28][30/75]	Time 1.009 (1.011)	Data 0.003 (0.016)	Loss 0.7580 (0.7541)	Acc@1 84.858 (85.530)	Acc@5 99.700 (99.468)
Epoch: [28][40/75]	Time 1.014 (1.012)	Data 0.005 (0.013)	Loss 0.8148 (0.7563)	Acc@1 82.909 (85.432)	Acc@5 98.951 (99.455)
Epoch: [28][50/75]	Time 1.006 (1.011)	Data 0.004 (0.012)	Loss 0.8561 (0.7582)	Acc@1 81.109 (85.304)	Acc@5 99.700 (99.462)
Epoch: [28][60/75]	Time 1.015 (1.010)	Data 0.005 (0.011)	Loss 0.7254 (0.7612)	Acc@1 87.256 (85.295)	Acc@5 99.850 (99.464)
Epoch: [28][70/75]	Time 1.025 (1.011)	Data 0.005 (0.010)	Loss 0.7791 (0.7644)	Acc@1 85.907 (85.246)	Acc@5 99.550 (99.466)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 111, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(111, 110, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(110, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 1004950 ; 1082906 ; 0.9280122189737613
new batch_size: 667

Epoche:  28  ; NumbOfParameters:  1004950

Test Acc:  67.23

Epoch: [29 | 80] LR: 0.100000
Epoch: [29][0/75]	Time 0.955 (0.955)	Data 0.338 (0.338)	Loss 0.7102 (0.7102)	Acc@1 87.256 (87.256)	Acc@5 99.550 (99.550)
Epoch: [29][10/75]	Time 0.973 (0.972)	Data 0.005 (0.035)	Loss 0.7013 (0.7032)	Acc@1 86.057 (87.365)	Acc@5 99.400 (99.441)
Epoch: [29][20/75]	Time 0.972 (0.975)	Data 0.005 (0.021)	Loss 0.6971 (0.7092)	Acc@1 86.057 (87.006)	Acc@5 99.850 (99.500)
Epoch: [29][30/75]	Time 0.982 (0.977)	Data 0.005 (0.016)	Loss 0.7627 (0.7224)	Acc@1 85.007 (86.574)	Acc@5 100.000 (99.516)
Epoch: [29][40/75]	Time 0.988 (0.978)	Data 0.006 (0.013)	Loss 0.7586 (0.7366)	Acc@1 84.408 (86.046)	Acc@5 98.951 (99.466)
Epoch: [29][50/75]	Time 0.977 (0.978)	Data 0.005 (0.012)	Loss 0.7677 (0.7458)	Acc@1 84.108 (85.698)	Acc@5 99.400 (99.439)
Epoch: [29][60/75]	Time 0.989 (0.979)	Data 0.005 (0.010)	Loss 0.7480 (0.7490)	Acc@1 85.607 (85.546)	Acc@5 100.000 (99.449)
Epoch: [29][70/75]	Time 0.992 (0.979)	Data 0.005 (0.010)	Loss 0.7654 (0.7494)	Acc@1 84.408 (85.481)	Acc@5 99.250 (99.443)

Epoche:  29  ; NumbOfParameters:  1004950

Test Acc:  70.21

Epoch: [30 | 80] LR: 0.100000
Epoch: [30][0/75]	Time 0.967 (0.967)	Data 0.356 (0.356)	Loss 0.7135 (0.7135)	Acc@1 85.907 (85.907)	Acc@5 99.850 (99.850)
Epoch: [30][10/75]	Time 0.994 (0.983)	Data 0.005 (0.036)	Loss 0.7896 (0.7466)	Acc@1 83.808 (85.321)	Acc@5 99.250 (99.605)
Epoch: [30][20/75]	Time 0.985 (0.982)	Data 0.005 (0.021)	Loss 0.7082 (0.7384)	Acc@1 86.807 (85.693)	Acc@5 99.850 (99.593)
Epoch: [30][30/75]	Time 0.984 (0.981)	Data 0.005 (0.016)	Loss 0.7290 (0.7437)	Acc@1 85.757 (85.738)	Acc@5 99.850 (99.565)
Epoch: [30][40/75]	Time 0.983 (0.981)	Data 0.005 (0.013)	Loss 0.7332 (0.7448)	Acc@1 85.607 (85.764)	Acc@5 99.250 (99.492)
Epoch: [30][50/75]	Time 0.980 (0.981)	Data 0.006 (0.012)	Loss 0.7541 (0.7471)	Acc@1 84.408 (85.684)	Acc@5 99.700 (99.483)
Epoch: [30][60/75]	Time 0.993 (0.981)	Data 0.005 (0.011)	Loss 0.7924 (0.7498)	Acc@1 83.358 (85.602)	Acc@5 99.400 (99.449)
Epoch: [30][70/75]	Time 0.983 (0.982)	Data 0.004 (0.010)	Loss 0.8291 (0.7504)	Acc@1 84.708 (85.567)	Acc@5 99.250 (99.455)

Epoche:  30  ; NumbOfParameters:  1004950

Test Acc:  73.77

Epoch: [31 | 80] LR: 0.100000
Epoch: [31][0/75]	Time 0.953 (0.953)	Data 0.348 (0.348)	Loss 0.7233 (0.7233)	Acc@1 86.057 (86.057)	Acc@5 99.250 (99.250)
Epoch: [31][10/75]	Time 0.972 (0.976)	Data 0.005 (0.036)	Loss 0.7364 (0.7417)	Acc@1 86.207 (86.043)	Acc@5 99.550 (99.496)
Epoch: [31][20/75]	Time 0.995 (0.976)	Data 0.005 (0.021)	Loss 0.7110 (0.7368)	Acc@1 87.106 (86.157)	Acc@5 99.700 (99.500)
Epoch: [31][30/75]	Time 0.978 (0.978)	Data 0.003 (0.016)	Loss 0.7466 (0.7386)	Acc@1 84.858 (85.975)	Acc@5 99.550 (99.492)
Epoch: [31][40/75]	Time 0.983 (0.979)	Data 0.005 (0.013)	Loss 0.7368 (0.7394)	Acc@1 87.256 (85.933)	Acc@5 99.850 (99.499)
Epoch: [31][50/75]	Time 0.986 (0.981)	Data 0.005 (0.012)	Loss 0.7325 (0.7413)	Acc@1 86.357 (85.839)	Acc@5 99.550 (99.468)
Epoch: [31][60/75]	Time 0.983 (0.981)	Data 0.005 (0.011)	Loss 0.6935 (0.7439)	Acc@1 86.807 (85.725)	Acc@5 99.850 (99.452)
Epoch: [31][70/75]	Time 0.994 (0.982)	Data 0.005 (0.010)	Loss 0.7708 (0.7461)	Acc@1 85.907 (85.635)	Acc@5 99.550 (99.447)

Epoche:  31  ; NumbOfParameters:  1004950

Test Acc:  65.83

Epoch: [32 | 80] LR: 0.100000
Epoch: [32][0/75]	Time 0.952 (0.952)	Data 0.353 (0.353)	Loss 0.7440 (0.7440)	Acc@1 85.907 (85.907)	Acc@5 99.400 (99.400)
Epoch: [32][10/75]	Time 0.990 (0.979)	Data 0.006 (0.036)	Loss 0.7330 (0.7016)	Acc@1 86.057 (86.957)	Acc@5 99.100 (99.564)
Epoch: [32][20/75]	Time 0.994 (0.980)	Data 0.005 (0.022)	Loss 0.7399 (0.7084)	Acc@1 84.558 (86.735)	Acc@5 99.400 (99.543)
Epoch: [32][30/75]	Time 0.979 (0.980)	Data 0.005 (0.016)	Loss 0.7392 (0.7207)	Acc@1 84.858 (86.444)	Acc@5 99.400 (99.507)
Epoch: [32][40/75]	Time 0.975 (0.981)	Data 0.005 (0.014)	Loss 0.7862 (0.7262)	Acc@1 83.058 (86.276)	Acc@5 98.801 (99.488)
Epoch: [32][50/75]	Time 0.966 (0.980)	Data 0.004 (0.012)	Loss 0.7549 (0.7294)	Acc@1 84.858 (86.163)	Acc@5 99.550 (99.483)
Epoch: [32][60/75]	Time 0.986 (0.979)	Data 0.004 (0.011)	Loss 0.7980 (0.7305)	Acc@1 84.558 (86.172)	Acc@5 99.100 (99.464)
Epoch: [32][70/75]	Time 0.980 (0.980)	Data 0.004 (0.010)	Loss 0.7894 (0.7343)	Acc@1 84.558 (86.027)	Acc@5 99.400 (99.440)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 98, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(98, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(98, 101, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(101, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(101, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 958754 ; 1082906 ; 0.8853529299865363
new batch_size: 667

Epoche:  32  ; NumbOfParameters:  958754

Test Acc:  69.36

Epoch: [33 | 80] LR: 0.100000
Epoch: [33][0/75]	Time 0.956 (0.956)	Data 0.341 (0.341)	Loss 0.7369 (0.7369)	Acc@1 86.357 (86.357)	Acc@5 99.550 (99.550)
Epoch: [33][10/75]	Time 0.964 (0.959)	Data 0.005 (0.035)	Loss 0.6772 (0.6818)	Acc@1 87.856 (87.856)	Acc@5 99.250 (99.632)
Epoch: [33][20/75]	Time 0.960 (0.959)	Data 0.005 (0.021)	Loss 0.6626 (0.6781)	Acc@1 88.456 (87.977)	Acc@5 99.550 (99.507)
Epoch: [33][30/75]	Time 0.961 (0.960)	Data 0.005 (0.016)	Loss 0.6941 (0.6813)	Acc@1 86.957 (87.735)	Acc@5 99.700 (99.550)
Epoch: [33][40/75]	Time 0.976 (0.962)	Data 0.005 (0.013)	Loss 0.7722 (0.6969)	Acc@1 83.208 (87.180)	Acc@5 99.850 (99.525)
Epoch: [33][50/75]	Time 0.963 (0.962)	Data 0.005 (0.012)	Loss 0.7388 (0.7043)	Acc@1 85.907 (87.009)	Acc@5 99.550 (99.518)
Epoch: [33][60/75]	Time 0.963 (0.962)	Data 0.005 (0.011)	Loss 0.7101 (0.7087)	Acc@1 86.207 (86.792)	Acc@5 99.550 (99.523)
Epoch: [33][70/75]	Time 0.983 (0.964)	Data 0.005 (0.010)	Loss 0.6836 (0.7133)	Acc@1 87.556 (86.608)	Acc@5 99.700 (99.495)

Epoche:  33  ; NumbOfParameters:  958754

Test Acc:  73.31

Epoch: [34 | 80] LR: 0.100000
Epoch: [34][0/75]	Time 0.938 (0.938)	Data 0.350 (0.350)	Loss 0.7103 (0.7103)	Acc@1 87.256 (87.256)	Acc@5 99.700 (99.700)
Epoch: [34][10/75]	Time 0.950 (0.962)	Data 0.003 (0.036)	Loss 0.7376 (0.7425)	Acc@1 85.457 (85.594)	Acc@5 99.850 (99.591)
Epoch: [34][20/75]	Time 0.949 (0.964)	Data 0.005 (0.021)	Loss 0.7885 (0.7380)	Acc@1 83.958 (85.614)	Acc@5 98.951 (99.465)
Epoch: [34][30/75]	Time 0.957 (0.963)	Data 0.005 (0.016)	Loss 0.7304 (0.7353)	Acc@1 86.657 (85.762)	Acc@5 99.550 (99.463)
Epoch: [34][40/75]	Time 0.962 (0.963)	Data 0.005 (0.013)	Loss 0.6951 (0.7363)	Acc@1 87.406 (85.838)	Acc@5 99.850 (99.488)
Epoch: [34][50/75]	Time 0.966 (0.963)	Data 0.005 (0.012)	Loss 0.7429 (0.7367)	Acc@1 87.106 (85.910)	Acc@5 99.400 (99.468)
Epoch: [34][60/75]	Time 0.961 (0.963)	Data 0.005 (0.011)	Loss 0.7636 (0.7358)	Acc@1 84.108 (85.968)	Acc@5 99.400 (99.479)
Epoch: [34][70/75]	Time 0.968 (0.964)	Data 0.004 (0.010)	Loss 0.6969 (0.7378)	Acc@1 87.256 (85.899)	Acc@5 99.400 (99.470)

Epoche:  34  ; NumbOfParameters:  958754

Test Acc:  67.99

Epoch: [35 | 80] LR: 0.100000
Epoch: [35][0/75]	Time 0.945 (0.945)	Data 0.346 (0.346)	Loss 0.7407 (0.7407)	Acc@1 85.157 (85.157)	Acc@5 99.400 (99.400)
Epoch: [35][10/75]	Time 0.964 (0.966)	Data 0.005 (0.035)	Loss 0.7308 (0.7169)	Acc@1 86.807 (86.834)	Acc@5 99.250 (99.359)
Epoch: [35][20/75]	Time 0.968 (0.963)	Data 0.005 (0.021)	Loss 0.7518 (0.7293)	Acc@1 83.658 (86.364)	Acc@5 99.250 (99.372)
Epoch: [35][30/75]	Time 0.974 (0.963)	Data 0.005 (0.016)	Loss 0.7442 (0.7273)	Acc@1 86.507 (86.362)	Acc@5 99.250 (99.434)
Epoch: [35][40/75]	Time 0.966 (0.963)	Data 0.005 (0.013)	Loss 0.7527 (0.7229)	Acc@1 85.907 (86.463)	Acc@5 99.250 (99.481)
Epoch: [35][50/75]	Time 0.970 (0.964)	Data 0.005 (0.012)	Loss 0.7464 (0.7223)	Acc@1 85.607 (86.324)	Acc@5 99.850 (99.524)
Epoch: [35][60/75]	Time 0.964 (0.964)	Data 0.004 (0.011)	Loss 0.6835 (0.7257)	Acc@1 87.406 (86.212)	Acc@5 99.550 (99.504)
Epoch: [35][70/75]	Time 0.977 (0.964)	Data 0.005 (0.010)	Loss 0.7085 (0.7247)	Acc@1 86.207 (86.289)	Acc@5 99.850 (99.506)

Epoche:  35  ; NumbOfParameters:  958754

Test Acc:  62.17

Epoch: [36 | 80] LR: 0.100000
Epoch: [36][0/75]	Time 0.933 (0.933)	Data 0.353 (0.353)	Loss 0.6934 (0.6934)	Acc@1 88.006 (88.006)	Acc@5 99.700 (99.700)
Epoch: [36][10/75]	Time 0.955 (0.961)	Data 0.005 (0.036)	Loss 0.6868 (0.6956)	Acc@1 87.556 (87.420)	Acc@5 99.850 (99.618)
Epoch: [36][20/75]	Time 0.953 (0.963)	Data 0.005 (0.021)	Loss 0.6816 (0.6940)	Acc@1 87.106 (87.335)	Acc@5 99.550 (99.679)
Epoch: [36][30/75]	Time 0.964 (0.963)	Data 0.005 (0.016)	Loss 0.6624 (0.7004)	Acc@1 88.606 (87.179)	Acc@5 99.400 (99.613)
Epoch: [36][40/75]	Time 0.958 (0.963)	Data 0.005 (0.014)	Loss 0.7646 (0.7122)	Acc@1 85.757 (86.807)	Acc@5 99.400 (99.590)
Epoch: [36][50/75]	Time 0.971 (0.963)	Data 0.005 (0.012)	Loss 0.7162 (0.7147)	Acc@1 86.657 (86.613)	Acc@5 99.700 (99.586)
Epoch: [36][60/75]	Time 0.961 (0.963)	Data 0.005 (0.011)	Loss 0.7257 (0.7170)	Acc@1 86.057 (86.529)	Acc@5 99.400 (99.567)
Epoch: [36][70/75]	Time 0.975 (0.963)	Data 0.004 (0.010)	Loss 0.7356 (0.7210)	Acc@1 85.907 (86.424)	Acc@5 99.550 (99.521)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(92, 93, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(93, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(93, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 930520 ; 1082906 ; 0.8592804915662117
new batch_size: 667

Epoche:  36  ; NumbOfParameters:  930520

Test Acc:  59.82

Epoch: [37 | 80] LR: 0.100000
Epoch: [37][0/75]	Time 0.926 (0.926)	Data 0.350 (0.350)	Loss 0.7484 (0.7484)	Acc@1 87.106 (87.106)	Acc@5 98.801 (98.801)
Epoch: [37][10/75]	Time 0.955 (0.957)	Data 0.003 (0.035)	Loss 0.6446 (0.6801)	Acc@1 88.756 (88.306)	Acc@5 100.000 (99.550)
Epoch: [37][20/75]	Time 0.954 (0.960)	Data 0.005 (0.021)	Loss 0.6829 (0.6817)	Acc@1 87.256 (87.956)	Acc@5 99.400 (99.515)
Epoch: [37][30/75]	Time 0.963 (0.959)	Data 0.004 (0.016)	Loss 0.6627 (0.6869)	Acc@1 87.706 (87.861)	Acc@5 99.850 (99.516)
Epoch: [37][40/75]	Time 0.999 (0.961)	Data 0.005 (0.013)	Loss 0.7421 (0.6966)	Acc@1 85.457 (87.355)	Acc@5 99.700 (99.528)
Epoch: [37][50/75]	Time 0.949 (0.961)	Data 0.005 (0.012)	Loss 0.7362 (0.6969)	Acc@1 84.558 (87.300)	Acc@5 99.700 (99.521)
Epoch: [37][60/75]	Time 0.944 (0.960)	Data 0.005 (0.011)	Loss 0.7273 (0.7002)	Acc@1 85.307 (87.143)	Acc@5 99.400 (99.528)
Epoch: [37][70/75]	Time 0.983 (0.962)	Data 0.005 (0.010)	Loss 0.7115 (0.7042)	Acc@1 87.106 (86.950)	Acc@5 99.250 (99.521)

Epoche:  37  ; NumbOfParameters:  930520

Test Acc:  67.5

Epoch: [38 | 80] LR: 0.100000
Epoch: [38][0/75]	Time 0.949 (0.949)	Data 0.350 (0.350)	Loss 0.7126 (0.7126)	Acc@1 85.757 (85.757)	Acc@5 99.550 (99.550)
Epoch: [38][10/75]	Time 0.976 (0.964)	Data 0.005 (0.036)	Loss 0.7662 (0.7127)	Acc@1 82.909 (86.384)	Acc@5 98.951 (99.400)
Epoch: [38][20/75]	Time 0.967 (0.962)	Data 0.005 (0.021)	Loss 0.7102 (0.7069)	Acc@1 87.706 (86.857)	Acc@5 99.250 (99.472)
Epoch: [38][30/75]	Time 0.960 (0.963)	Data 0.005 (0.016)	Loss 0.7526 (0.7094)	Acc@1 86.207 (86.753)	Acc@5 99.400 (99.487)
Epoch: [38][40/75]	Time 0.960 (0.962)	Data 0.005 (0.013)	Loss 0.6964 (0.7124)	Acc@1 87.256 (86.660)	Acc@5 99.700 (99.492)
Epoch: [38][50/75]	Time 0.958 (0.962)	Data 0.005 (0.012)	Loss 0.7313 (0.7150)	Acc@1 85.907 (86.657)	Acc@5 99.400 (99.509)
Epoch: [38][60/75]	Time 0.961 (0.961)	Data 0.005 (0.011)	Loss 0.7099 (0.7157)	Acc@1 86.357 (86.681)	Acc@5 99.550 (99.499)
Epoch: [38][70/75]	Time 0.968 (0.961)	Data 0.005 (0.010)	Loss 0.7044 (0.7176)	Acc@1 87.556 (86.623)	Acc@5 99.400 (99.497)

Epoche:  38  ; NumbOfParameters:  930520

Test Acc:  64.91

Epoch: [39 | 80] LR: 0.100000
Epoch: [39][0/75]	Time 0.958 (0.958)	Data 0.362 (0.362)	Loss 0.6750 (0.6750)	Acc@1 88.606 (88.606)	Acc@5 99.700 (99.700)
Epoch: [39][10/75]	Time 0.953 (0.964)	Data 0.005 (0.036)	Loss 0.7009 (0.7042)	Acc@1 87.106 (86.861)	Acc@5 99.100 (99.646)
Epoch: [39][20/75]	Time 0.967 (0.963)	Data 0.005 (0.021)	Loss 0.6682 (0.7020)	Acc@1 88.156 (86.942)	Acc@5 99.700 (99.672)
Epoch: [39][30/75]	Time 0.951 (0.962)	Data 0.005 (0.016)	Loss 0.7224 (0.7019)	Acc@1 86.807 (86.845)	Acc@5 99.250 (99.613)
Epoch: [39][40/75]	Time 0.954 (0.962)	Data 0.005 (0.013)	Loss 0.7096 (0.7052)	Acc@1 87.256 (86.755)	Acc@5 100.000 (99.594)
Epoch: [39][50/75]	Time 0.962 (0.961)	Data 0.005 (0.012)	Loss 0.7211 (0.7091)	Acc@1 87.556 (86.610)	Acc@5 99.400 (99.553)
Epoch: [39][60/75]	Time 0.963 (0.962)	Data 0.004 (0.011)	Loss 0.7785 (0.7141)	Acc@1 83.958 (86.406)	Acc@5 98.951 (99.540)
Epoch: [39][70/75]	Time 0.955 (0.962)	Data 0.005 (0.010)	Loss 0.6966 (0.7147)	Acc@1 87.856 (86.384)	Acc@5 99.700 (99.535)

Epoche:  39  ; NumbOfParameters:  930520

Test Acc:  69.97

Epoch: [40 | 80] LR: 0.010000
Epoch: [40][0/75]	Time 0.937 (0.937)	Data 0.348 (0.348)	Loss 0.6691 (0.6691)	Acc@1 88.756 (88.756)	Acc@5 99.700 (99.700)
Epoch: [40][10/75]	Time 0.971 (0.962)	Data 0.005 (0.036)	Loss 0.6414 (0.6581)	Acc@1 88.756 (88.837)	Acc@5 99.700 (99.577)
Epoch: [40][20/75]	Time 0.962 (0.963)	Data 0.005 (0.021)	Loss 0.6195 (0.6390)	Acc@1 90.555 (89.505)	Acc@5 99.850 (99.679)
Epoch: [40][30/75]	Time 0.963 (0.961)	Data 0.005 (0.016)	Loss 0.6183 (0.6257)	Acc@1 89.805 (89.839)	Acc@5 99.850 (99.719)
Epoch: [40][40/75]	Time 0.963 (0.961)	Data 0.005 (0.013)	Loss 0.5985 (0.6134)	Acc@1 90.405 (90.295)	Acc@5 99.850 (99.751)
Epoch: [40][50/75]	Time 0.956 (0.960)	Data 0.005 (0.012)	Loss 0.5570 (0.6046)	Acc@1 92.504 (90.622)	Acc@5 99.550 (99.753)
Epoch: [40][60/75]	Time 0.956 (0.959)	Data 0.005 (0.010)	Loss 0.5547 (0.5980)	Acc@1 93.103 (90.926)	Acc@5 99.550 (99.752)
Epoch: [40][70/75]	Time 0.976 (0.960)	Data 0.005 (0.010)	Loss 0.5454 (0.5919)	Acc@1 92.954 (91.125)	Acc@5 99.850 (99.768)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(86, 85, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(85, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(85, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 903150 ; 1082906 ; 0.83400590632982
new batch_size: 667

Epoche:  40  ; NumbOfParameters:  903150

Test Acc:  89.12

Epoch: [41 | 80] LR: 0.010000
Epoch: [41][0/75]	Time 0.959 (0.959)	Data 0.348 (0.348)	Loss 0.5616 (0.5616)	Acc@1 92.354 (92.354)	Acc@5 99.700 (99.700)
Epoch: [41][10/75]	Time 0.973 (0.960)	Data 0.006 (0.036)	Loss 0.5354 (0.5477)	Acc@1 94.153 (92.449)	Acc@5 99.700 (99.850)
Epoch: [41][20/75]	Time 0.967 (0.962)	Data 0.005 (0.021)	Loss 0.5382 (0.5425)	Acc@1 93.103 (92.675)	Acc@5 99.700 (99.807)
Epoch: [41][30/75]	Time 0.977 (0.964)	Data 0.005 (0.016)	Loss 0.5292 (0.5387)	Acc@1 92.954 (92.779)	Acc@5 100.000 (99.831)
Epoch: [41][40/75]	Time 0.966 (0.965)	Data 0.003 (0.013)	Loss 0.5379 (0.5380)	Acc@1 92.804 (92.749)	Acc@5 99.850 (99.846)
Epoch: [41][50/75]	Time 0.957 (0.965)	Data 0.004 (0.012)	Loss 0.5221 (0.5370)	Acc@1 93.103 (92.768)	Acc@5 99.400 (99.835)
Epoch: [41][60/75]	Time 0.967 (0.965)	Data 0.005 (0.011)	Loss 0.5190 (0.5359)	Acc@1 93.553 (92.816)	Acc@5 99.700 (99.833)
Epoch: [41][70/75]	Time 0.973 (0.966)	Data 0.005 (0.010)	Loss 0.5312 (0.5345)	Acc@1 92.654 (92.897)	Acc@5 100.000 (99.835)

Epoche:  41  ; NumbOfParameters:  903150

Test Acc:  89.24

Epoch: [42 | 80] LR: 0.010000
Epoch: [42][0/75]	Time 0.948 (0.948)	Data 0.356 (0.356)	Loss 0.4955 (0.4955)	Acc@1 93.553 (93.553)	Acc@5 99.850 (99.850)
Epoch: [42][10/75]	Time 0.969 (0.965)	Data 0.005 (0.037)	Loss 0.5284 (0.5148)	Acc@1 92.954 (93.417)	Acc@5 99.850 (99.796)
Epoch: [42][20/75]	Time 0.978 (0.967)	Data 0.005 (0.022)	Loss 0.5184 (0.5111)	Acc@1 92.954 (93.489)	Acc@5 100.000 (99.871)
Epoch: [42][30/75]	Time 0.971 (0.968)	Data 0.005 (0.016)	Loss 0.5075 (0.5137)	Acc@1 94.003 (93.389)	Acc@5 99.850 (99.869)
Epoch: [42][40/75]	Time 0.967 (0.968)	Data 0.005 (0.014)	Loss 0.5123 (0.5145)	Acc@1 94.303 (93.418)	Acc@5 100.000 (99.872)
Epoch: [42][50/75]	Time 0.960 (0.967)	Data 0.005 (0.012)	Loss 0.5110 (0.5133)	Acc@1 92.954 (93.389)	Acc@5 100.000 (99.885)
Epoch: [42][60/75]	Time 0.961 (0.967)	Data 0.003 (0.011)	Loss 0.5099 (0.5114)	Acc@1 93.403 (93.467)	Acc@5 99.850 (99.877)
Epoch: [42][70/75]	Time 0.989 (0.969)	Data 0.004 (0.010)	Loss 0.5488 (0.5122)	Acc@1 90.105 (93.365)	Acc@5 100.000 (99.865)

Epoche:  42  ; NumbOfParameters:  903150

Test Acc:  89.61

Epoch: [43 | 80] LR: 0.010000
Epoch: [43][0/75]	Time 0.941 (0.941)	Data 0.357 (0.357)	Loss 0.5065 (0.5065)	Acc@1 92.954 (92.954)	Acc@5 100.000 (100.000)
Epoch: [43][10/75]	Time 0.965 (0.971)	Data 0.005 (0.036)	Loss 0.4923 (0.4948)	Acc@1 94.753 (94.180)	Acc@5 99.850 (99.877)
Epoch: [43][20/75]	Time 0.981 (0.971)	Data 0.005 (0.021)	Loss 0.5281 (0.4962)	Acc@1 93.853 (94.032)	Acc@5 99.700 (99.879)
Epoch: [43][30/75]	Time 0.969 (0.971)	Data 0.005 (0.016)	Loss 0.4983 (0.4965)	Acc@1 95.052 (93.988)	Acc@5 100.000 (99.874)
Epoch: [43][40/75]	Time 0.978 (0.970)	Data 0.005 (0.014)	Loss 0.4909 (0.4941)	Acc@1 93.403 (94.025)	Acc@5 100.000 (99.898)
Epoch: [43][50/75]	Time 0.962 (0.969)	Data 0.005 (0.012)	Loss 0.5078 (0.4928)	Acc@1 94.003 (94.038)	Acc@5 99.850 (99.897)
Epoch: [43][60/75]	Time 0.967 (0.969)	Data 0.005 (0.011)	Loss 0.4843 (0.4939)	Acc@1 94.153 (94.003)	Acc@5 100.000 (99.892)
Epoch: [43][70/75]	Time 0.977 (0.969)	Data 0.005 (0.010)	Loss 0.5138 (0.4930)	Acc@1 92.204 (94.011)	Acc@5 99.850 (99.875)

Epoche:  43  ; NumbOfParameters:  903150

Test Acc:  89.12

Epoch: [44 | 80] LR: 0.010000
Epoch: [44][0/75]	Time 0.952 (0.952)	Data 0.348 (0.348)	Loss 0.4503 (0.4503)	Acc@1 95.802 (95.802)	Acc@5 100.000 (100.000)
Epoch: [44][10/75]	Time 0.975 (0.970)	Data 0.005 (0.036)	Loss 0.4723 (0.4790)	Acc@1 94.753 (94.535)	Acc@5 99.850 (99.905)
Epoch: [44][20/75]	Time 0.975 (0.968)	Data 0.005 (0.021)	Loss 0.4502 (0.4792)	Acc@1 95.952 (94.624)	Acc@5 99.850 (99.929)
Epoch: [44][30/75]	Time 0.965 (0.966)	Data 0.005 (0.016)	Loss 0.4899 (0.4782)	Acc@1 93.403 (94.554)	Acc@5 100.000 (99.932)
Epoch: [44][40/75]	Time 0.973 (0.967)	Data 0.004 (0.013)	Loss 0.4416 (0.4791)	Acc@1 96.252 (94.573)	Acc@5 99.850 (99.916)
Epoch: [44][50/75]	Time 0.971 (0.967)	Data 0.005 (0.012)	Loss 0.5182 (0.4794)	Acc@1 92.804 (94.538)	Acc@5 99.550 (99.897)
Epoch: [44][60/75]	Time 0.966 (0.967)	Data 0.005 (0.011)	Loss 0.4537 (0.4807)	Acc@1 94.603 (94.445)	Acc@5 99.850 (99.894)
Epoch: [44][70/75]	Time 0.988 (0.969)	Data 0.005 (0.010)	Loss 0.5041 (0.4821)	Acc@1 93.253 (94.398)	Acc@5 99.850 (99.888)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 81, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(81, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(80, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 884140 ; 1082906 ; 0.8164512894009268
new batch_size: 667

Epoche:  44  ; NumbOfParameters:  884140

Test Acc:  89.33

Epoch: [45 | 80] LR: 0.010000
Epoch: [45][0/75]	Time 0.916 (0.916)	Data 0.341 (0.341)	Loss 0.4725 (0.4725)	Acc@1 94.303 (94.303)	Acc@5 99.700 (99.700)
Epoch: [45][10/75]	Time 0.940 (0.941)	Data 0.005 (0.035)	Loss 0.4777 (0.4685)	Acc@1 94.603 (94.957)	Acc@5 100.000 (99.823)
Epoch: [45][20/75]	Time 0.924 (0.942)	Data 0.005 (0.021)	Loss 0.4662 (0.4702)	Acc@1 95.052 (94.810)	Acc@5 100.000 (99.886)
Epoch: [45][30/75]	Time 0.961 (0.945)	Data 0.005 (0.016)	Loss 0.4991 (0.4724)	Acc@1 93.703 (94.680)	Acc@5 99.850 (99.889)
Epoch: [45][40/75]	Time 0.956 (0.946)	Data 0.005 (0.013)	Loss 0.4734 (0.4725)	Acc@1 93.703 (94.573)	Acc@5 100.000 (99.898)
Epoch: [45][50/75]	Time 0.945 (0.946)	Data 0.005 (0.012)	Loss 0.4478 (0.4703)	Acc@1 95.352 (94.644)	Acc@5 100.000 (99.894)
Epoch: [45][60/75]	Time 0.950 (0.947)	Data 0.005 (0.011)	Loss 0.4921 (0.4712)	Acc@1 94.003 (94.544)	Acc@5 99.700 (99.894)
Epoch: [45][70/75]	Time 0.964 (0.948)	Data 0.005 (0.010)	Loss 0.4734 (0.4705)	Acc@1 94.453 (94.527)	Acc@5 99.700 (99.897)

Epoche:  45  ; NumbOfParameters:  884140

Test Acc:  89.66

Epoch: [46 | 80] LR: 0.010000
Epoch: [46][0/75]	Time 0.930 (0.930)	Data 0.353 (0.353)	Loss 0.4687 (0.4687)	Acc@1 94.153 (94.153)	Acc@5 99.850 (99.850)
Epoch: [46][10/75]	Time 0.951 (0.952)	Data 0.005 (0.036)	Loss 0.4307 (0.4433)	Acc@1 96.402 (95.380)	Acc@5 100.000 (99.932)
Epoch: [46][20/75]	Time 0.944 (0.950)	Data 0.005 (0.021)	Loss 0.4754 (0.4520)	Acc@1 94.453 (95.110)	Acc@5 99.850 (99.907)
Epoch: [46][30/75]	Time 0.946 (0.951)	Data 0.005 (0.016)	Loss 0.4694 (0.4550)	Acc@1 94.303 (95.009)	Acc@5 100.000 (99.908)
Epoch: [46][40/75]	Time 0.955 (0.951)	Data 0.005 (0.013)	Loss 0.5070 (0.4574)	Acc@1 93.553 (94.946)	Acc@5 99.550 (99.898)
Epoch: [46][50/75]	Time 0.966 (0.951)	Data 0.005 (0.012)	Loss 0.4582 (0.4574)	Acc@1 94.303 (94.997)	Acc@5 99.850 (99.891)
Epoch: [46][60/75]	Time 0.944 (0.951)	Data 0.005 (0.011)	Loss 0.4610 (0.4581)	Acc@1 95.802 (94.976)	Acc@5 100.000 (99.902)
Epoch: [46][70/75]	Time 0.971 (0.952)	Data 0.005 (0.010)	Loss 0.4478 (0.4595)	Acc@1 94.753 (94.909)	Acc@5 100.000 (99.903)

Epoche:  46  ; NumbOfParameters:  884140

Test Acc:  88.95

Epoch: [47 | 80] LR: 0.010000
Epoch: [47][0/75]	Time 0.924 (0.924)	Data 0.359 (0.359)	Loss 0.4240 (0.4240)	Acc@1 95.502 (95.502)	Acc@5 100.000 (100.000)
Epoch: [47][10/75]	Time 0.957 (0.949)	Data 0.005 (0.037)	Loss 0.4264 (0.4511)	Acc@1 95.802 (95.121)	Acc@5 100.000 (99.959)
Epoch: [47][20/75]	Time 0.945 (0.950)	Data 0.004 (0.022)	Loss 0.4231 (0.4477)	Acc@1 95.652 (95.138)	Acc@5 99.850 (99.950)
Epoch: [47][30/75]	Time 0.946 (0.950)	Data 0.005 (0.016)	Loss 0.4205 (0.4493)	Acc@1 95.652 (95.062)	Acc@5 99.850 (99.913)
Epoch: [47][40/75]	Time 0.955 (0.950)	Data 0.005 (0.014)	Loss 0.4511 (0.4493)	Acc@1 95.502 (95.122)	Acc@5 100.000 (99.901)
Epoch: [47][50/75]	Time 0.954 (0.950)	Data 0.005 (0.012)	Loss 0.4493 (0.4492)	Acc@1 94.303 (95.129)	Acc@5 100.000 (99.894)
Epoch: [47][60/75]	Time 0.925 (0.949)	Data 0.005 (0.011)	Loss 0.4450 (0.4485)	Acc@1 95.052 (95.119)	Acc@5 99.850 (99.892)
Epoch: [47][70/75]	Time 0.957 (0.950)	Data 0.005 (0.010)	Loss 0.4676 (0.4491)	Acc@1 94.603 (95.131)	Acc@5 99.700 (99.894)

Epoche:  47  ; NumbOfParameters:  884140

Test Acc:  89.58

Epoch: [48 | 80] LR: 0.010000
Epoch: [48][0/75]	Time 0.919 (0.919)	Data 0.345 (0.345)	Loss 0.4362 (0.4362)	Acc@1 96.702 (96.702)	Acc@5 99.850 (99.850)
Epoch: [48][10/75]	Time 0.944 (0.952)	Data 0.005 (0.036)	Loss 0.4130 (0.4419)	Acc@1 97.002 (95.652)	Acc@5 99.850 (99.891)
Epoch: [48][20/75]	Time 0.961 (0.952)	Data 0.005 (0.021)	Loss 0.4478 (0.4347)	Acc@1 94.753 (95.845)	Acc@5 100.000 (99.936)
Epoch: [48][30/75]	Time 0.952 (0.951)	Data 0.005 (0.016)	Loss 0.4089 (0.4357)	Acc@1 96.402 (95.730)	Acc@5 100.000 (99.932)
Epoch: [48][40/75]	Time 0.945 (0.952)	Data 0.005 (0.013)	Loss 0.4374 (0.4365)	Acc@1 95.352 (95.656)	Acc@5 99.850 (99.916)
Epoch: [48][50/75]	Time 0.952 (0.952)	Data 0.005 (0.012)	Loss 0.4324 (0.4350)	Acc@1 95.802 (95.682)	Acc@5 99.850 (99.924)
Epoch: [48][60/75]	Time 0.957 (0.951)	Data 0.005 (0.011)	Loss 0.4402 (0.4367)	Acc@1 95.502 (95.539)	Acc@5 100.000 (99.931)
Epoch: [48][70/75]	Time 0.964 (0.952)	Data 0.005 (0.010)	Loss 0.4395 (0.4373)	Acc@1 95.502 (95.433)	Acc@5 99.850 (99.928)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(127, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(74, 68, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(68, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 846876 ; 1082906 ; 0.7820401770790817
new batch_size: 667

Epoche:  48  ; NumbOfParameters:  846876

Test Acc:  88.99

Epoch: [49 | 80] LR: 0.010000
Epoch: [49][0/75]	Time 0.941 (0.941)	Data 0.347 (0.347)	Loss 0.4275 (0.4275)	Acc@1 95.952 (95.952)	Acc@5 99.850 (99.850)
Epoch: [49][10/75]	Time 0.947 (0.941)	Data 0.005 (0.036)	Loss 0.4224 (0.4222)	Acc@1 95.652 (95.993)	Acc@5 100.000 (99.905)
Epoch: [49][20/75]	Time 0.931 (0.940)	Data 0.005 (0.021)	Loss 0.4518 (0.4218)	Acc@1 93.853 (95.895)	Acc@5 100.000 (99.921)
Epoch: [49][30/75]	Time 0.929 (0.939)	Data 0.005 (0.016)	Loss 0.4290 (0.4216)	Acc@1 95.352 (95.831)	Acc@5 100.000 (99.923)
Epoch: [49][40/75]	Time 0.945 (0.939)	Data 0.005 (0.014)	Loss 0.4296 (0.4222)	Acc@1 95.202 (95.809)	Acc@5 100.000 (99.934)
Epoch: [49][50/75]	Time 0.955 (0.940)	Data 0.005 (0.012)	Loss 0.4400 (0.4240)	Acc@1 94.903 (95.740)	Acc@5 99.850 (99.929)
Epoch: [49][60/75]	Time 0.958 (0.941)	Data 0.004 (0.011)	Loss 0.4202 (0.4261)	Acc@1 95.802 (95.650)	Acc@5 100.000 (99.934)
Epoch: [49][70/75]	Time 0.974 (0.943)	Data 0.005 (0.010)	Loss 0.4270 (0.4272)	Acc@1 95.502 (95.623)	Acc@5 99.850 (99.930)

Epoche:  49  ; NumbOfParameters:  846876

Test Acc:  89.73

Epoch: [50 | 80] LR: 0.010000
Epoch: [50][0/75]	Time 0.907 (0.907)	Data 0.345 (0.345)	Loss 0.4380 (0.4380)	Acc@1 95.052 (95.052)	Acc@5 100.000 (100.000)
Epoch: [50][10/75]	Time 0.947 (0.942)	Data 0.005 (0.035)	Loss 0.4107 (0.4147)	Acc@1 96.402 (96.088)	Acc@5 100.000 (99.959)
Epoch: [50][20/75]	Time 0.936 (0.942)	Data 0.006 (0.021)	Loss 0.3946 (0.4139)	Acc@1 96.852 (96.059)	Acc@5 99.850 (99.936)
Epoch: [50][30/75]	Time 0.943 (0.942)	Data 0.005 (0.016)	Loss 0.4046 (0.4139)	Acc@1 97.002 (96.029)	Acc@5 100.000 (99.947)
Epoch: [50][40/75]	Time 0.939 (0.942)	Data 0.005 (0.013)	Loss 0.4263 (0.4146)	Acc@1 95.052 (96.000)	Acc@5 100.000 (99.938)
Epoch: [50][50/75]	Time 0.946 (0.941)	Data 0.005 (0.012)	Loss 0.4139 (0.4149)	Acc@1 95.352 (95.961)	Acc@5 100.000 (99.941)
Epoch: [50][60/75]	Time 0.942 (0.942)	Data 0.005 (0.011)	Loss 0.4349 (0.4166)	Acc@1 96.252 (95.856)	Acc@5 100.000 (99.934)
Epoch: [50][70/75]	Time 0.961 (0.943)	Data 0.005 (0.010)	Loss 0.4285 (0.4174)	Acc@1 95.352 (95.783)	Acc@5 99.850 (99.935)

Epoche:  50  ; NumbOfParameters:  846876

Test Acc:  89.46

Epoch: [51 | 80] LR: 0.010000
Epoch: [51][0/75]	Time 0.926 (0.926)	Data 0.352 (0.352)	Loss 0.3764 (0.3764)	Acc@1 98.051 (98.051)	Acc@5 100.000 (100.000)
Epoch: [51][10/75]	Time 0.939 (0.943)	Data 0.005 (0.036)	Loss 0.4278 (0.4102)	Acc@1 94.753 (95.966)	Acc@5 99.850 (99.959)
Epoch: [51][20/75]	Time 0.949 (0.942)	Data 0.005 (0.021)	Loss 0.4301 (0.4072)	Acc@1 96.102 (96.138)	Acc@5 100.000 (99.914)
Epoch: [51][30/75]	Time 0.944 (0.942)	Data 0.005 (0.016)	Loss 0.3926 (0.4062)	Acc@1 96.852 (96.204)	Acc@5 100.000 (99.923)
Epoch: [51][40/75]	Time 0.930 (0.942)	Data 0.005 (0.013)	Loss 0.4239 (0.4051)	Acc@1 95.352 (96.252)	Acc@5 99.850 (99.927)
Epoch: [51][50/75]	Time 0.945 (0.943)	Data 0.005 (0.012)	Loss 0.4174 (0.4077)	Acc@1 95.052 (96.125)	Acc@5 100.000 (99.935)
Epoch: [51][60/75]	Time 0.939 (0.942)	Data 0.004 (0.011)	Loss 0.4163 (0.4108)	Acc@1 95.952 (96.031)	Acc@5 100.000 (99.934)
Epoch: [51][70/75]	Time 0.960 (0.943)	Data 0.005 (0.010)	Loss 0.4073 (0.4116)	Acc@1 96.102 (96.020)	Acc@5 100.000 (99.932)

Epoche:  51  ; NumbOfParameters:  846876

Test Acc:  89.17

Epoch: [52 | 80] LR: 0.010000
Epoch: [52][0/75]	Time 0.924 (0.924)	Data 0.355 (0.355)	Loss 0.3689 (0.3689)	Acc@1 97.301 (97.301)	Acc@5 100.000 (100.000)
Epoch: [52][10/75]	Time 0.941 (0.942)	Data 0.005 (0.036)	Loss 0.3847 (0.3990)	Acc@1 96.702 (96.538)	Acc@5 100.000 (99.945)
Epoch: [52][20/75]	Time 0.946 (0.944)	Data 0.005 (0.021)	Loss 0.3966 (0.3997)	Acc@1 97.151 (96.430)	Acc@5 100.000 (99.943)
Epoch: [52][30/75]	Time 0.956 (0.944)	Data 0.005 (0.016)	Loss 0.3986 (0.4045)	Acc@1 95.652 (96.184)	Acc@5 99.850 (99.942)
Epoch: [52][40/75]	Time 0.946 (0.943)	Data 0.004 (0.013)	Loss 0.3916 (0.4066)	Acc@1 96.252 (96.062)	Acc@5 100.000 (99.941)
Epoch: [52][50/75]	Time 0.934 (0.943)	Data 0.005 (0.012)	Loss 0.4184 (0.4067)	Acc@1 94.753 (96.023)	Acc@5 100.000 (99.941)
Epoch: [52][60/75]	Time 0.945 (0.942)	Data 0.005 (0.011)	Loss 0.3897 (0.4062)	Acc@1 95.952 (96.004)	Acc@5 100.000 (99.946)
Epoch: [52][70/75]	Time 0.959 (0.943)	Data 0.005 (0.010)	Loss 0.4112 (0.4064)	Acc@1 96.402 (96.005)	Acc@5 99.850 (99.941)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(127, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 69, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(69, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(59, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 822071 ; 1082906 ; 0.7591342184824906
new batch_size: 667

Epoche:  52  ; NumbOfParameters:  822071

Test Acc:  89.07

Epoch: [53 | 80] LR: 0.010000
Epoch: [53][0/75]	Time 0.908 (0.908)	Data 0.340 (0.340)	Loss 0.3733 (0.3733)	Acc@1 96.852 (96.852)	Acc@5 100.000 (100.000)
Epoch: [53][10/75]	Time 0.916 (0.927)	Data 0.005 (0.035)	Loss 0.3852 (0.3862)	Acc@1 96.552 (96.702)	Acc@5 100.000 (99.959)
Epoch: [53][20/75]	Time 0.922 (0.929)	Data 0.005 (0.021)	Loss 0.3832 (0.3902)	Acc@1 97.751 (96.652)	Acc@5 99.850 (99.957)
Epoch: [53][30/75]	Time 0.927 (0.929)	Data 0.005 (0.016)	Loss 0.3881 (0.3896)	Acc@1 96.702 (96.610)	Acc@5 99.850 (99.947)
Epoch: [53][40/75]	Time 0.927 (0.930)	Data 0.005 (0.013)	Loss 0.4011 (0.3902)	Acc@1 96.852 (96.643)	Acc@5 100.000 (99.949)
Epoch: [53][50/75]	Time 0.940 (0.930)	Data 0.006 (0.012)	Loss 0.3638 (0.3911)	Acc@1 97.451 (96.543)	Acc@5 99.850 (99.950)
Epoch: [53][60/75]	Time 0.947 (0.930)	Data 0.005 (0.011)	Loss 0.4089 (0.3916)	Acc@1 95.352 (96.483)	Acc@5 100.000 (99.948)
Epoch: [53][70/75]	Time 0.949 (0.932)	Data 0.005 (0.010)	Loss 0.3957 (0.3925)	Acc@1 95.952 (96.419)	Acc@5 100.000 (99.954)

Epoche:  53  ; NumbOfParameters:  822071

Test Acc:  88.65

Epoch: [54 | 80] LR: 0.010000
Epoch: [54][0/75]	Time 0.900 (0.900)	Data 0.349 (0.349)	Loss 0.3774 (0.3774)	Acc@1 97.451 (97.451)	Acc@5 100.000 (100.000)
Epoch: [54][10/75]	Time 0.925 (0.929)	Data 0.005 (0.036)	Loss 0.3561 (0.3847)	Acc@1 97.601 (96.538)	Acc@5 100.000 (99.973)
Epoch: [54][20/75]	Time 0.938 (0.933)	Data 0.005 (0.022)	Loss 0.3836 (0.3911)	Acc@1 96.702 (96.359)	Acc@5 99.850 (99.936)
Epoch: [54][30/75]	Time 0.939 (0.932)	Data 0.005 (0.016)	Loss 0.3973 (0.3899)	Acc@1 95.502 (96.334)	Acc@5 99.850 (99.952)
Epoch: [54][40/75]	Time 0.927 (0.932)	Data 0.005 (0.014)	Loss 0.3784 (0.3903)	Acc@1 96.402 (96.310)	Acc@5 100.000 (99.956)
Epoch: [54][50/75]	Time 0.923 (0.932)	Data 0.005 (0.012)	Loss 0.3919 (0.3902)	Acc@1 96.552 (96.325)	Acc@5 100.000 (99.947)
Epoch: [54][60/75]	Time 0.938 (0.932)	Data 0.005 (0.011)	Loss 0.3784 (0.3897)	Acc@1 97.151 (96.340)	Acc@5 99.850 (99.951)
Epoch: [54][70/75]	Time 0.952 (0.933)	Data 0.005 (0.010)	Loss 0.3662 (0.3889)	Acc@1 97.751 (96.376)	Acc@5 100.000 (99.958)

Epoche:  54  ; NumbOfParameters:  822071

Test Acc:  89.25

Epoch: [55 | 80] LR: 0.010000
Epoch: [55][0/75]	Time 0.916 (0.916)	Data 0.354 (0.354)	Loss 0.3688 (0.3688)	Acc@1 97.002 (97.002)	Acc@5 99.850 (99.850)
Epoch: [55][10/75]	Time 0.947 (0.935)	Data 0.005 (0.037)	Loss 0.3733 (0.3735)	Acc@1 96.402 (96.879)	Acc@5 100.000 (99.973)
Epoch: [55][20/75]	Time 0.943 (0.937)	Data 0.005 (0.022)	Loss 0.3789 (0.3766)	Acc@1 96.552 (96.780)	Acc@5 99.850 (99.957)
Epoch: [55][30/75]	Time 0.943 (0.937)	Data 0.005 (0.016)	Loss 0.4059 (0.3795)	Acc@1 96.252 (96.726)	Acc@5 100.000 (99.961)
Epoch: [55][40/75]	Time 0.934 (0.936)	Data 0.005 (0.014)	Loss 0.3749 (0.3799)	Acc@1 96.852 (96.643)	Acc@5 100.000 (99.956)
Epoch: [55][50/75]	Time 0.931 (0.936)	Data 0.005 (0.012)	Loss 0.4125 (0.3830)	Acc@1 95.502 (96.496)	Acc@5 100.000 (99.953)
Epoch: [55][60/75]	Time 0.943 (0.936)	Data 0.005 (0.011)	Loss 0.3896 (0.3828)	Acc@1 95.802 (96.466)	Acc@5 100.000 (99.951)
Epoch: [55][70/75]	Time 0.964 (0.936)	Data 0.005 (0.010)	Loss 0.3776 (0.3822)	Acc@1 97.002 (96.474)	Acc@5 99.700 (99.951)

Epoche:  55  ; NumbOfParameters:  822071

Test Acc:  88.98

Epoch: [56 | 80] LR: 0.010000
Epoch: [56][0/75]	Time 0.920 (0.920)	Data 0.351 (0.351)	Loss 0.3495 (0.3495)	Acc@1 97.751 (97.751)	Acc@5 100.000 (100.000)
Epoch: [56][10/75]	Time 0.940 (0.935)	Data 0.005 (0.036)	Loss 0.3505 (0.3583)	Acc@1 97.601 (97.410)	Acc@5 100.000 (99.973)
Epoch: [56][20/75]	Time 0.928 (0.935)	Data 0.002 (0.021)	Loss 0.3655 (0.3630)	Acc@1 97.601 (97.337)	Acc@5 99.850 (99.964)
Epoch: [56][30/75]	Time 0.923 (0.934)	Data 0.005 (0.016)	Loss 0.3918 (0.3658)	Acc@1 96.102 (97.176)	Acc@5 100.000 (99.947)
Epoch: [56][40/75]	Time 0.947 (0.933)	Data 0.005 (0.013)	Loss 0.3790 (0.3705)	Acc@1 96.852 (96.932)	Acc@5 100.000 (99.945)
Epoch: [56][50/75]	Time 0.934 (0.933)	Data 0.005 (0.012)	Loss 0.3641 (0.3717)	Acc@1 97.151 (96.925)	Acc@5 100.000 (99.950)
Epoch: [56][60/75]	Time 0.940 (0.934)	Data 0.004 (0.011)	Loss 0.3944 (0.3734)	Acc@1 96.402 (96.896)	Acc@5 99.850 (99.948)
Epoch: [56][70/75]	Time 0.946 (0.934)	Data 0.005 (0.010)	Loss 0.3762 (0.3739)	Acc@1 96.402 (96.837)	Acc@5 100.000 (99.949)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 127, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(127, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(63, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(54, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 803356 ; 1082906 ; 0.7418520167032041
new batch_size: 667

Epoche:  56  ; NumbOfParameters:  803356

Test Acc:  89.34

Epoch: [57 | 80] LR: 0.010000
Epoch: [57][0/75]	Time 0.895 (0.895)	Data 0.339 (0.339)	Loss 0.3589 (0.3589)	Acc@1 97.751 (97.751)	Acc@5 100.000 (100.000)
Epoch: [57][10/75]	Time 0.917 (0.916)	Data 0.005 (0.036)	Loss 0.3636 (0.3571)	Acc@1 97.002 (97.506)	Acc@5 100.000 (99.959)
Epoch: [57][20/75]	Time 0.914 (0.916)	Data 0.006 (0.021)	Loss 0.3861 (0.3594)	Acc@1 95.502 (97.337)	Acc@5 100.000 (99.964)
Epoch: [57][30/75]	Time 0.908 (0.916)	Data 0.005 (0.016)	Loss 0.3598 (0.3593)	Acc@1 96.852 (97.316)	Acc@5 100.000 (99.961)
Epoch: [57][40/75]	Time 0.920 (0.916)	Data 0.005 (0.013)	Loss 0.3624 (0.3621)	Acc@1 97.151 (97.184)	Acc@5 100.000 (99.971)
Epoch: [57][50/75]	Time 0.918 (0.917)	Data 0.005 (0.012)	Loss 0.3798 (0.3633)	Acc@1 96.702 (97.154)	Acc@5 99.700 (99.959)
Epoch: [57][60/75]	Time 0.912 (0.916)	Data 0.005 (0.011)	Loss 0.3695 (0.3641)	Acc@1 96.552 (97.083)	Acc@5 100.000 (99.961)
Epoch: [57][70/75]	Time 0.937 (0.917)	Data 0.004 (0.010)	Loss 0.3773 (0.3668)	Acc@1 95.952 (96.978)	Acc@5 100.000 (99.960)

Epoche:  57  ; NumbOfParameters:  803356

Test Acc:  88.89

Epoch: [58 | 80] LR: 0.010000
Epoch: [58][0/75]	Time 0.897 (0.897)	Data 0.364 (0.364)	Loss 0.3465 (0.3465)	Acc@1 97.301 (97.301)	Acc@5 100.000 (100.000)
Epoch: [58][10/75]	Time 0.921 (0.921)	Data 0.005 (0.037)	Loss 0.3497 (0.3602)	Acc@1 97.002 (97.015)	Acc@5 100.000 (100.000)
Epoch: [58][20/75]	Time 0.925 (0.920)	Data 0.005 (0.022)	Loss 0.3693 (0.3619)	Acc@1 96.252 (96.973)	Acc@5 100.000 (100.000)
Epoch: [58][30/75]	Time 0.916 (0.920)	Data 0.005 (0.017)	Loss 0.3351 (0.3601)	Acc@1 98.501 (97.103)	Acc@5 100.000 (99.995)
Epoch: [58][40/75]	Time 0.925 (0.921)	Data 0.005 (0.014)	Loss 0.3613 (0.3604)	Acc@1 96.702 (97.067)	Acc@5 100.000 (99.985)
Epoch: [58][50/75]	Time 0.915 (0.920)	Data 0.005 (0.012)	Loss 0.3641 (0.3605)	Acc@1 96.852 (97.069)	Acc@5 100.000 (99.985)
Epoch: [58][60/75]	Time 0.918 (0.920)	Data 0.005 (0.011)	Loss 0.3516 (0.3608)	Acc@1 97.751 (97.048)	Acc@5 100.000 (99.985)
Epoch: [58][70/75]	Time 0.939 (0.921)	Data 0.004 (0.010)	Loss 0.3633 (0.3611)	Acc@1 97.002 (97.006)	Acc@5 100.000 (99.985)

Epoche:  58  ; NumbOfParameters:  803356

Test Acc:  88.99

Epoch: [59 | 80] LR: 0.010000
Epoch: [59][0/75]	Time 0.894 (0.894)	Data 0.348 (0.348)	Loss 0.3451 (0.3451)	Acc@1 97.751 (97.751)	Acc@5 99.850 (99.850)
Epoch: [59][10/75]	Time 0.918 (0.922)	Data 0.004 (0.036)	Loss 0.3544 (0.3484)	Acc@1 97.451 (97.547)	Acc@5 99.850 (99.932)
Epoch: [59][20/75]	Time 0.922 (0.919)	Data 0.005 (0.021)	Loss 0.3589 (0.3517)	Acc@1 97.151 (97.251)	Acc@5 100.000 (99.957)
Epoch: [59][30/75]	Time 0.921 (0.920)	Data 0.005 (0.016)	Loss 0.3370 (0.3543)	Acc@1 98.051 (97.118)	Acc@5 100.000 (99.971)
Epoch: [59][40/75]	Time 0.923 (0.919)	Data 0.005 (0.013)	Loss 0.3343 (0.3536)	Acc@1 98.201 (97.170)	Acc@5 100.000 (99.974)
Epoch: [59][50/75]	Time 0.919 (0.919)	Data 0.005 (0.012)	Loss 0.3826 (0.3550)	Acc@1 95.802 (97.116)	Acc@5 99.700 (99.965)
Epoch: [59][60/75]	Time 0.924 (0.919)	Data 0.005 (0.011)	Loss 0.3517 (0.3547)	Acc@1 97.751 (97.142)	Acc@5 100.000 (99.966)
Epoch: [59][70/75]	Time 0.930 (0.921)	Data 0.005 (0.010)	Loss 0.3703 (0.3563)	Acc@1 95.952 (97.048)	Acc@5 100.000 (99.966)

Epoche:  59  ; NumbOfParameters:  803356

Test Acc:  87.19

Epoch: [60 | 80] LR: 0.010000
Epoch: [60][0/75]	Time 0.900 (0.900)	Data 0.360 (0.360)	Loss 0.3335 (0.3335)	Acc@1 98.501 (98.501)	Acc@5 100.000 (100.000)
Epoch: [60][10/75]	Time 0.912 (0.921)	Data 0.004 (0.037)	Loss 0.3468 (0.3560)	Acc@1 97.601 (96.892)	Acc@5 100.000 (99.959)
Epoch: [60][20/75]	Time 0.925 (0.920)	Data 0.005 (0.022)	Loss 0.3345 (0.3527)	Acc@1 97.601 (97.009)	Acc@5 100.000 (99.964)
Epoch: [60][30/75]	Time 0.919 (0.921)	Data 0.005 (0.016)	Loss 0.3532 (0.3508)	Acc@1 96.852 (97.156)	Acc@5 99.850 (99.956)
Epoch: [60][40/75]	Time 0.916 (0.921)	Data 0.005 (0.014)	Loss 0.3674 (0.3511)	Acc@1 96.702 (97.133)	Acc@5 99.850 (99.963)
Epoch: [60][50/75]	Time 0.926 (0.921)	Data 0.005 (0.012)	Loss 0.3532 (0.3499)	Acc@1 97.601 (97.184)	Acc@5 99.850 (99.965)
Epoch: [60][60/75]	Time 0.928 (0.922)	Data 0.005 (0.011)	Loss 0.3403 (0.3508)	Acc@1 97.451 (97.132)	Acc@5 100.000 (99.963)
Epoch: [60][70/75]	Time 0.916 (0.922)	Data 0.005 (0.010)	Loss 0.3631 (0.3511)	Acc@1 96.702 (97.122)	Acc@5 99.850 (99.958)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(126, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(126, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(59, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(52, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 791120 ; 1082906 ; 0.7305527903622291
new batch_size: 667

Epoche:  60  ; NumbOfParameters:  791120

Test Acc:  89.72

Epoch: [61 | 80] LR: 0.010000
Epoch: [61][0/75]	Time 0.914 (0.914)	Data 0.344 (0.344)	Loss 0.3321 (0.3321)	Acc@1 98.501 (98.501)	Acc@5 100.000 (100.000)
Epoch: [61][10/75]	Time 0.915 (0.918)	Data 0.005 (0.035)	Loss 0.3404 (0.3386)	Acc@1 97.451 (97.710)	Acc@5 100.000 (99.986)
Epoch: [61][20/75]	Time 0.918 (0.919)	Data 0.005 (0.021)	Loss 0.3351 (0.3369)	Acc@1 97.451 (97.651)	Acc@5 100.000 (99.979)
Epoch: [61][30/75]	Time 0.916 (0.918)	Data 0.005 (0.016)	Loss 0.3383 (0.3390)	Acc@1 97.901 (97.553)	Acc@5 100.000 (99.985)
Epoch: [61][40/75]	Time 0.915 (0.918)	Data 0.005 (0.013)	Loss 0.3400 (0.3388)	Acc@1 97.601 (97.554)	Acc@5 100.000 (99.982)
Epoch: [61][50/75]	Time 0.919 (0.918)	Data 0.005 (0.012)	Loss 0.3380 (0.3396)	Acc@1 97.901 (97.495)	Acc@5 100.000 (99.974)
Epoch: [61][60/75]	Time 0.929 (0.919)	Data 0.005 (0.011)	Loss 0.3391 (0.3404)	Acc@1 97.751 (97.476)	Acc@5 100.000 (99.968)
Epoch: [61][70/75]	Time 0.931 (0.919)	Data 0.005 (0.010)	Loss 0.3435 (0.3413)	Acc@1 97.601 (97.417)	Acc@5 100.000 (99.968)

Epoche:  61  ; NumbOfParameters:  791120

Test Acc:  87.3

Epoch: [62 | 80] LR: 0.010000
Epoch: [62][0/75]	Time 0.894 (0.894)	Data 0.350 (0.350)	Loss 0.3644 (0.3644)	Acc@1 96.552 (96.552)	Acc@5 99.700 (99.700)
Epoch: [62][10/75]	Time 0.916 (0.918)	Data 0.004 (0.036)	Loss 0.3421 (0.3374)	Acc@1 97.901 (97.765)	Acc@5 99.850 (99.959)
Epoch: [62][20/75]	Time 0.920 (0.917)	Data 0.005 (0.021)	Loss 0.3374 (0.3360)	Acc@1 98.201 (97.737)	Acc@5 99.850 (99.964)
Epoch: [62][30/75]	Time 0.923 (0.918)	Data 0.005 (0.016)	Loss 0.3401 (0.3376)	Acc@1 96.852 (97.601)	Acc@5 100.000 (99.971)
Epoch: [62][40/75]	Time 0.913 (0.918)	Data 0.005 (0.013)	Loss 0.3635 (0.3407)	Acc@1 95.802 (97.374)	Acc@5 100.000 (99.974)
Epoch: [62][50/75]	Time 0.916 (0.918)	Data 0.005 (0.012)	Loss 0.3474 (0.3420)	Acc@1 97.002 (97.360)	Acc@5 100.000 (99.974)
Epoch: [62][60/75]	Time 0.919 (0.918)	Data 0.005 (0.011)	Loss 0.3537 (0.3430)	Acc@1 97.151 (97.299)	Acc@5 100.000 (99.973)
Epoch: [62][70/75]	Time 0.935 (0.919)	Data 0.005 (0.010)	Loss 0.3378 (0.3429)	Acc@1 97.301 (97.306)	Acc@5 100.000 (99.966)

Epoche:  62  ; NumbOfParameters:  791120

Test Acc:  89.28

Epoch: [63 | 80] LR: 0.010000
Epoch: [63][0/75]	Time 0.895 (0.895)	Data 0.350 (0.350)	Loss 0.3322 (0.3322)	Acc@1 97.751 (97.751)	Acc@5 100.000 (100.000)
Epoch: [63][10/75]	Time 0.920 (0.919)	Data 0.005 (0.036)	Loss 0.3412 (0.3311)	Acc@1 96.852 (97.560)	Acc@5 100.000 (100.000)
Epoch: [63][20/75]	Time 0.916 (0.919)	Data 0.005 (0.021)	Loss 0.3293 (0.3346)	Acc@1 97.451 (97.451)	Acc@5 100.000 (99.986)
Epoch: [63][30/75]	Time 0.904 (0.919)	Data 0.004 (0.016)	Loss 0.3627 (0.3330)	Acc@1 96.852 (97.596)	Acc@5 99.850 (99.985)
Epoch: [63][40/75]	Time 0.928 (0.919)	Data 0.005 (0.013)	Loss 0.3492 (0.3331)	Acc@1 96.552 (97.587)	Acc@5 100.000 (99.978)
Epoch: [63][50/75]	Time 0.922 (0.920)	Data 0.006 (0.012)	Loss 0.3382 (0.3331)	Acc@1 97.751 (97.575)	Acc@5 100.000 (99.976)
Epoch: [63][60/75]	Time 0.905 (0.919)	Data 0.005 (0.011)	Loss 0.3409 (0.3346)	Acc@1 97.601 (97.510)	Acc@5 100.000 (99.978)
Epoch: [63][70/75]	Time 0.943 (0.920)	Data 0.005 (0.010)	Loss 0.3579 (0.3360)	Acc@1 96.852 (97.449)	Acc@5 100.000 (99.977)

Epoche:  63  ; NumbOfParameters:  791120

Test Acc:  87.93

Epoch: [64 | 80] LR: 0.010000
Epoch: [64][0/75]	Time 0.894 (0.894)	Data 0.355 (0.355)	Loss 0.3283 (0.3283)	Acc@1 97.901 (97.901)	Acc@5 100.000 (100.000)
Epoch: [64][10/75]	Time 0.905 (0.918)	Data 0.004 (0.037)	Loss 0.3294 (0.3261)	Acc@1 97.901 (97.778)	Acc@5 100.000 (100.000)
Epoch: [64][20/75]	Time 0.914 (0.918)	Data 0.005 (0.022)	Loss 0.3503 (0.3274)	Acc@1 96.852 (97.730)	Acc@5 100.000 (99.979)
Epoch: [64][30/75]	Time 0.914 (0.918)	Data 0.005 (0.016)	Loss 0.3344 (0.3292)	Acc@1 97.301 (97.640)	Acc@5 100.000 (99.981)
Epoch: [64][40/75]	Time 0.908 (0.918)	Data 0.005 (0.014)	Loss 0.3314 (0.3298)	Acc@1 97.451 (97.587)	Acc@5 99.850 (99.974)
Epoch: [64][50/75]	Time 0.921 (0.917)	Data 0.005 (0.012)	Loss 0.3274 (0.3300)	Acc@1 98.051 (97.592)	Acc@5 99.850 (99.965)
Epoch: [64][60/75]	Time 0.920 (0.918)	Data 0.005 (0.011)	Loss 0.3542 (0.3322)	Acc@1 96.702 (97.505)	Acc@5 100.000 (99.963)
Epoch: [64][70/75]	Time 0.930 (0.919)	Data 0.005 (0.010)	Loss 0.3346 (0.3329)	Acc@1 97.601 (97.477)	Acc@5 100.000 (99.964)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(125, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(57, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(50, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 782236 ; 1082906 ; 0.7223489388737342
new batch_size: 667

Epoche:  64  ; NumbOfParameters:  782236

Test Acc:  88.98

Epoch: [65 | 80] LR: 0.010000
Epoch: [65][0/75]	Time 0.903 (0.903)	Data 0.346 (0.346)	Loss 0.3059 (0.3059)	Acc@1 98.201 (98.201)	Acc@5 100.000 (100.000)
Epoch: [65][10/75]	Time 0.915 (0.903)	Data 0.004 (0.036)	Loss 0.3246 (0.3203)	Acc@1 96.852 (97.792)	Acc@5 100.000 (100.000)
Epoch: [65][20/75]	Time 0.897 (0.902)	Data 0.005 (0.021)	Loss 0.3144 (0.3174)	Acc@1 97.901 (97.908)	Acc@5 100.000 (100.000)
Epoch: [65][30/75]	Time 0.891 (0.901)	Data 0.005 (0.016)	Loss 0.3187 (0.3160)	Acc@1 98.351 (97.959)	Acc@5 100.000 (100.000)
Epoch: [65][40/75]	Time 0.898 (0.901)	Data 0.005 (0.013)	Loss 0.3315 (0.3196)	Acc@1 97.751 (97.857)	Acc@5 100.000 (99.982)
Epoch: [65][50/75]	Time 0.912 (0.902)	Data 0.005 (0.012)	Loss 0.3207 (0.3213)	Acc@1 97.901 (97.769)	Acc@5 100.000 (99.982)
Epoch: [65][60/75]	Time 0.897 (0.901)	Data 0.005 (0.011)	Loss 0.3575 (0.3222)	Acc@1 96.102 (97.756)	Acc@5 100.000 (99.978)
Epoch: [65][70/75]	Time 0.923 (0.902)	Data 0.005 (0.010)	Loss 0.3280 (0.3237)	Acc@1 97.901 (97.707)	Acc@5 99.850 (99.970)

Epoche:  65  ; NumbOfParameters:  782236

Test Acc:  88.4

Epoch: [66 | 80] LR: 0.010000
Epoch: [66][0/75]	Time 0.879 (0.879)	Data 0.355 (0.355)	Loss 0.3032 (0.3032)	Acc@1 98.801 (98.801)	Acc@5 100.000 (100.000)
Epoch: [66][10/75]	Time 0.916 (0.907)	Data 0.005 (0.036)	Loss 0.3349 (0.3208)	Acc@1 96.852 (97.847)	Acc@5 100.000 (99.986)
Epoch: [66][20/75]	Time 0.896 (0.904)	Data 0.004 (0.021)	Loss 0.3240 (0.3212)	Acc@1 97.601 (97.758)	Acc@5 100.000 (99.993)
Epoch: [66][30/75]	Time 0.891 (0.904)	Data 0.005 (0.016)	Loss 0.3115 (0.3238)	Acc@1 97.901 (97.654)	Acc@5 100.000 (99.976)
Epoch: [66][40/75]	Time 0.892 (0.904)	Data 0.005 (0.013)	Loss 0.3332 (0.3230)	Acc@1 97.301 (97.693)	Acc@5 100.000 (99.982)
Epoch: [66][50/75]	Time 0.895 (0.903)	Data 0.005 (0.012)	Loss 0.3354 (0.3242)	Acc@1 97.002 (97.578)	Acc@5 100.000 (99.982)
Epoch: [66][60/75]	Time 0.895 (0.903)	Data 0.005 (0.011)	Loss 0.3386 (0.3247)	Acc@1 97.451 (97.579)	Acc@5 99.850 (99.980)
Epoch: [66][70/75]	Time 0.917 (0.904)	Data 0.005 (0.010)	Loss 0.3259 (0.3251)	Acc@1 97.151 (97.572)	Acc@5 100.000 (99.979)

Epoche:  66  ; NumbOfParameters:  782236

Test Acc:  87.96

Epoch: [67 | 80] LR: 0.010000
Epoch: [67][0/75]	Time 0.867 (0.867)	Data 0.353 (0.353)	Loss 0.3241 (0.3241)	Acc@1 97.601 (97.601)	Acc@5 100.000 (100.000)
Epoch: [67][10/75]	Time 0.906 (0.901)	Data 0.005 (0.036)	Loss 0.3257 (0.3159)	Acc@1 96.552 (97.833)	Acc@5 100.000 (99.986)
Epoch: [67][20/75]	Time 0.908 (0.904)	Data 0.005 (0.021)	Loss 0.3382 (0.3171)	Acc@1 96.552 (97.794)	Acc@5 100.000 (99.986)
Epoch: [67][30/75]	Time 0.895 (0.902)	Data 0.005 (0.016)	Loss 0.3136 (0.3164)	Acc@1 97.451 (97.853)	Acc@5 100.000 (99.985)
Epoch: [67][40/75]	Time 0.900 (0.903)	Data 0.005 (0.014)	Loss 0.3214 (0.3182)	Acc@1 97.901 (97.813)	Acc@5 99.700 (99.978)
Epoch: [67][50/75]	Time 0.906 (0.903)	Data 0.005 (0.012)	Loss 0.3147 (0.3202)	Acc@1 98.201 (97.722)	Acc@5 99.850 (99.974)
Epoch: [67][60/75]	Time 0.908 (0.903)	Data 0.005 (0.011)	Loss 0.3059 (0.3214)	Acc@1 98.201 (97.631)	Acc@5 100.000 (99.973)
Epoch: [67][70/75]	Time 0.920 (0.904)	Data 0.005 (0.010)	Loss 0.3499 (0.3222)	Acc@1 96.252 (97.563)	Acc@5 99.850 (99.975)

Epoche:  67  ; NumbOfParameters:  782236

Test Acc:  87.92

Epoch: [68 | 80] LR: 0.010000
Epoch: [68][0/75]	Time 0.873 (0.873)	Data 0.343 (0.343)	Loss 0.3162 (0.3162)	Acc@1 98.201 (98.201)	Acc@5 100.000 (100.000)
Epoch: [68][10/75]	Time 0.899 (0.907)	Data 0.005 (0.035)	Loss 0.3069 (0.3107)	Acc@1 98.201 (98.119)	Acc@5 100.000 (99.986)
Epoch: [68][20/75]	Time 0.896 (0.906)	Data 0.005 (0.021)	Loss 0.3039 (0.3123)	Acc@1 98.651 (97.987)	Acc@5 100.000 (99.979)
Epoch: [68][30/75]	Time 0.902 (0.906)	Data 0.005 (0.016)	Loss 0.3270 (0.3139)	Acc@1 97.751 (97.858)	Acc@5 100.000 (99.985)
Epoch: [68][40/75]	Time 0.898 (0.906)	Data 0.005 (0.013)	Loss 0.3269 (0.3155)	Acc@1 97.451 (97.824)	Acc@5 100.000 (99.985)
Epoch: [68][50/75]	Time 0.906 (0.906)	Data 0.005 (0.012)	Loss 0.3100 (0.3161)	Acc@1 98.201 (97.789)	Acc@5 100.000 (99.985)
Epoch: [68][60/75]	Time 0.907 (0.905)	Data 0.005 (0.011)	Loss 0.3144 (0.3168)	Acc@1 97.901 (97.793)	Acc@5 100.000 (99.980)
Epoch: [68][70/75]	Time 0.922 (0.905)	Data 0.005 (0.010)	Loss 0.3421 (0.3178)	Acc@1 97.151 (97.743)	Acc@5 100.000 (99.983)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(125, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(54, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(49, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 775784 ; 1082906 ; 0.7163908963474207
new batch_size: 667

Epoche:  68  ; NumbOfParameters:  775784

Test Acc:  87.73

Epoch: [69 | 80] LR: 0.010000
Epoch: [69][0/75]	Time 0.875 (0.875)	Data 0.342 (0.342)	Loss 0.3113 (0.3113)	Acc@1 97.901 (97.901)	Acc@5 100.000 (100.000)
Epoch: [69][10/75]	Time 0.888 (0.887)	Data 0.004 (0.035)	Loss 0.3168 (0.3056)	Acc@1 97.751 (98.255)	Acc@5 100.000 (100.000)
Epoch: [69][20/75]	Time 0.888 (0.891)	Data 0.005 (0.021)	Loss 0.3059 (0.3062)	Acc@1 98.051 (98.158)	Acc@5 100.000 (100.000)
Epoch: [69][30/75]	Time 0.888 (0.893)	Data 0.005 (0.016)	Loss 0.3084 (0.3083)	Acc@1 98.201 (97.988)	Acc@5 100.000 (99.995)
Epoch: [69][40/75]	Time 0.888 (0.893)	Data 0.005 (0.013)	Loss 0.3174 (0.3094)	Acc@1 97.751 (97.989)	Acc@5 100.000 (99.996)
Epoch: [69][50/75]	Time 0.900 (0.894)	Data 0.005 (0.012)	Loss 0.3112 (0.3114)	Acc@1 98.201 (97.910)	Acc@5 100.000 (99.994)
Epoch: [69][60/75]	Time 0.895 (0.895)	Data 0.005 (0.011)	Loss 0.3193 (0.3124)	Acc@1 97.601 (97.899)	Acc@5 100.000 (99.993)
Epoch: [69][70/75]	Time 0.912 (0.896)	Data 0.005 (0.010)	Loss 0.3100 (0.3133)	Acc@1 97.751 (97.857)	Acc@5 99.850 (99.987)

Epoche:  69  ; NumbOfParameters:  775784

Test Acc:  86.04

Epoch: [70 | 80] LR: 0.000100
Epoch: [70][0/75]	Time 0.871 (0.871)	Data 0.344 (0.344)	Loss 0.3017 (0.3017)	Acc@1 98.201 (98.201)	Acc@5 100.000 (100.000)
Epoch: [70][10/75]	Time 0.904 (0.900)	Data 0.005 (0.036)	Loss 0.3047 (0.3083)	Acc@1 97.901 (97.996)	Acc@5 100.000 (100.000)
Epoch: [70][20/75]	Time 0.905 (0.900)	Data 0.004 (0.021)	Loss 0.3195 (0.3062)	Acc@1 96.852 (97.980)	Acc@5 100.000 (99.986)
Epoch: [70][30/75]	Time 0.887 (0.900)	Data 0.005 (0.016)	Loss 0.3071 (0.3075)	Acc@1 98.051 (97.998)	Acc@5 100.000 (99.985)
Epoch: [70][40/75]	Time 0.892 (0.899)	Data 0.005 (0.013)	Loss 0.3017 (0.3072)	Acc@1 98.201 (97.996)	Acc@5 100.000 (99.989)
Epoch: [70][50/75]	Time 0.901 (0.900)	Data 0.005 (0.012)	Loss 0.2907 (0.3056)	Acc@1 98.501 (98.042)	Acc@5 100.000 (99.991)
Epoch: [70][60/75]	Time 0.889 (0.900)	Data 0.005 (0.011)	Loss 0.2985 (0.3051)	Acc@1 98.351 (98.056)	Acc@5 100.000 (99.990)
Epoch: [70][70/75]	Time 0.911 (0.901)	Data 0.004 (0.010)	Loss 0.3085 (0.3043)	Acc@1 97.451 (98.089)	Acc@5 100.000 (99.992)

Epoche:  70  ; NumbOfParameters:  775784

Test Acc:  90.01

Epoch: [71 | 80] LR: 0.000100
Epoch: [71][0/75]	Time 0.882 (0.882)	Data 0.342 (0.342)	Loss 0.3122 (0.3122)	Acc@1 97.601 (97.601)	Acc@5 100.000 (100.000)
Epoch: [71][10/75]	Time 0.908 (0.898)	Data 0.005 (0.035)	Loss 0.2769 (0.2898)	Acc@1 99.700 (98.787)	Acc@5 100.000 (100.000)
Epoch: [71][20/75]	Time 0.902 (0.900)	Data 0.004 (0.021)	Loss 0.3069 (0.2918)	Acc@1 97.751 (98.601)	Acc@5 100.000 (100.000)
Epoch: [71][30/75]	Time 0.910 (0.899)	Data 0.005 (0.016)	Loss 0.2879 (0.2933)	Acc@1 98.801 (98.568)	Acc@5 100.000 (100.000)
Epoch: [71][40/75]	Time 0.890 (0.898)	Data 0.005 (0.013)	Loss 0.3009 (0.2928)	Acc@1 98.201 (98.614)	Acc@5 100.000 (99.996)
Epoch: [71][50/75]	Time 0.901 (0.898)	Data 0.005 (0.012)	Loss 0.2941 (0.2935)	Acc@1 98.651 (98.574)	Acc@5 100.000 (99.994)
Epoch: [71][60/75]	Time 0.897 (0.899)	Data 0.005 (0.011)	Loss 0.2968 (0.2936)	Acc@1 98.051 (98.572)	Acc@5 100.000 (99.988)
Epoch: [71][70/75]	Time 0.921 (0.899)	Data 0.004 (0.010)	Loss 0.3033 (0.2933)	Acc@1 98.351 (98.577)	Acc@5 100.000 (99.989)

Epoche:  71  ; NumbOfParameters:  775784

Test Acc:  90.21

Epoch: [72 | 80] LR: 0.000100
Epoch: [72][0/75]	Time 0.890 (0.890)	Data 0.347 (0.347)	Loss 0.2978 (0.2978)	Acc@1 98.051 (98.051)	Acc@5 99.850 (99.850)
Epoch: [72][10/75]	Time 0.898 (0.898)	Data 0.005 (0.035)	Loss 0.2804 (0.2873)	Acc@1 99.100 (98.896)	Acc@5 100.000 (99.986)
Epoch: [72][20/75]	Time 0.893 (0.900)	Data 0.005 (0.021)	Loss 0.2885 (0.2895)	Acc@1 98.651 (98.729)	Acc@5 100.000 (99.986)
Epoch: [72][30/75]	Time 0.903 (0.900)	Data 0.005 (0.016)	Loss 0.2879 (0.2909)	Acc@1 98.951 (98.704)	Acc@5 100.000 (99.985)
Epoch: [72][40/75]	Time 0.887 (0.900)	Data 0.005 (0.013)	Loss 0.2992 (0.2909)	Acc@1 97.901 (98.662)	Acc@5 100.000 (99.985)
Epoch: [72][50/75]	Time 0.903 (0.899)	Data 0.005 (0.012)	Loss 0.2958 (0.2908)	Acc@1 98.651 (98.674)	Acc@5 100.000 (99.985)
Epoch: [72][60/75]	Time 0.901 (0.899)	Data 0.005 (0.011)	Loss 0.2784 (0.2899)	Acc@1 99.100 (98.712)	Acc@5 100.000 (99.988)
Epoch: [72][70/75]	Time 0.913 (0.900)	Data 0.005 (0.010)	Loss 0.2970 (0.2897)	Acc@1 98.201 (98.733)	Acc@5 100.000 (99.989)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(125, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(53, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(49, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Count: 774189 ; 1082906 ; 0.7149180076571743
new batch_size: 667

Epoche:  72  ; NumbOfParameters:  774189

Test Acc:  90.24

Epoch: [73 | 80] LR: 0.000100
Epoch: [73][0/75]	Time 0.879 (0.879)	Data 0.345 (0.345)	Loss 0.2897 (0.2897)	Acc@1 98.801 (98.801)	Acc@5 100.000 (100.000)
Epoch: [73][10/75]	Time 0.900 (0.902)	Data 0.006 (0.036)	Loss 0.2946 (0.2895)	Acc@1 98.501 (98.760)	Acc@5 100.000 (100.000)
Epoch: [73][20/75]	Time 0.897 (0.900)	Data 0.005 (0.021)	Loss 0.2781 (0.2895)	Acc@1 99.250 (98.758)	Acc@5 100.000 (99.993)
Epoch: [73][30/75]	Time 0.892 (0.898)	Data 0.005 (0.016)	Loss 0.2784 (0.2889)	Acc@1 98.951 (98.743)	Acc@5 100.000 (99.990)
Epoch: [73][40/75]	Time 0.895 (0.897)	Data 0.005 (0.013)	Loss 0.2973 (0.2890)	Acc@1 98.351 (98.731)	Acc@5 100.000 (99.985)
Epoch: [73][50/75]	Time 0.909 (0.899)	Data 0.005 (0.012)	Loss 0.2813 (0.2885)	Acc@1 98.951 (98.754)	Acc@5 100.000 (99.985)
Epoch: [73][60/75]	Time 0.899 (0.899)	Data 0.005 (0.011)	Loss 0.2908 (0.2885)	Acc@1 98.351 (98.742)	Acc@5 100.000 (99.983)
Epoch: [73][70/75]	Time 0.916 (0.900)	Data 0.005 (0.010)	Loss 0.2851 (0.2887)	Acc@1 99.100 (98.727)	Acc@5 100.000 (99.981)

Epoche:  73  ; NumbOfParameters:  774189

Test Acc:  90.02

Epoch: [74 | 80] LR: 0.000100
Epoch: [74][0/75]	Time 0.887 (0.887)	Data 0.345 (0.345)	Loss 0.2885 (0.2885)	Acc@1 99.250 (99.250)	Acc@5 100.000 (100.000)
Epoch: [74][10/75]	Time 0.901 (0.897)	Data 0.005 (0.036)	Loss 0.2886 (0.2919)	Acc@1 98.801 (98.678)	Acc@5 100.000 (99.986)
Epoch: [74][20/75]	Time 0.898 (0.896)	Data 0.005 (0.021)	Loss 0.2830 (0.2893)	Acc@1 99.100 (98.779)	Acc@5 100.000 (99.993)
Epoch: [74][30/75]	Time 0.905 (0.898)	Data 0.005 (0.016)	Loss 0.2850 (0.2890)	Acc@1 98.651 (98.801)	Acc@5 100.000 (99.990)
Epoch: [74][40/75]	Time 0.893 (0.897)	Data 0.004 (0.013)	Loss 0.2835 (0.2887)	Acc@1 98.951 (98.797)	Acc@5 100.000 (99.989)
Epoch: [74][50/75]	Time 0.888 (0.897)	Data 0.005 (0.012)	Loss 0.2731 (0.2873)	Acc@1 99.400 (98.856)	Acc@5 100.000 (99.991)
Epoch: [74][60/75]	Time 0.903 (0.897)	Data 0.005 (0.011)	Loss 0.2858 (0.2867)	Acc@1 99.250 (98.882)	Acc@5 100.000 (99.993)
Epoch: [74][70/75]	Time 0.897 (0.898)	Data 0.005 (0.010)	Loss 0.3037 (0.2872)	Acc@1 98.501 (98.858)	Acc@5 99.850 (99.989)

Epoche:  74  ; NumbOfParameters:  774189

Test Acc:  90.39

Epoch: [75 | 80] LR: 0.000100
Epoch: [75][0/75]	Time 0.883 (0.883)	Data 0.340 (0.340)	Loss 0.2815 (0.2815)	Acc@1 98.951 (98.951)	Acc@5 100.000 (100.000)
Epoch: [75][10/75]	Time 0.904 (0.898)	Data 0.005 (0.035)	Loss 0.2746 (0.2847)	Acc@1 99.700 (98.896)	Acc@5 100.000 (100.000)
Epoch: [75][20/75]	Time 0.907 (0.899)	Data 0.005 (0.021)	Loss 0.2747 (0.2844)	Acc@1 99.700 (98.936)	Acc@5 100.000 (99.986)
Epoch: [75][30/75]	Time 0.894 (0.899)	Data 0.005 (0.016)	Loss 0.2879 (0.2844)	Acc@1 98.501 (98.936)	Acc@5 100.000 (99.990)
Epoch: [75][40/75]	Time 0.874 (0.898)	Data 0.005 (0.013)	Loss 0.2995 (0.2860)	Acc@1 98.201 (98.881)	Acc@5 100.000 (99.993)
Epoch: [75][50/75]	Time 0.888 (0.899)	Data 0.005 (0.012)	Loss 0.2869 (0.2865)	Acc@1 98.201 (98.836)	Acc@5 100.000 (99.994)
Epoch: [75][60/75]	Time 0.919 (0.899)	Data 0.005 (0.011)	Loss 0.3037 (0.2870)	Acc@1 98.651 (98.808)	Acc@5 100.000 (99.995)
Epoch: [75][70/75]	Time 0.919 (0.900)	Data 0.005 (0.010)	Loss 0.2751 (0.2867)	Acc@1 99.400 (98.826)	Acc@5 100.000 (99.996)

Epoche:  75  ; NumbOfParameters:  774189

Test Acc:  90.35

Epoch: [76 | 80] LR: 0.000100
Epoch: [76][0/75]	Time 0.877 (0.877)	Data 0.352 (0.352)	Loss 0.2799 (0.2799)	Acc@1 99.250 (99.250)	Acc@5 100.000 (100.000)
Epoch: [76][10/75]	Time 0.909 (0.902)	Data 0.005 (0.036)	Loss 0.2740 (0.2814)	Acc@1 99.400 (99.073)	Acc@5 100.000 (99.986)
Epoch: [76][20/75]	Time 0.897 (0.903)	Data 0.006 (0.021)	Loss 0.2822 (0.2825)	Acc@1 99.100 (99.029)	Acc@5 100.000 (99.986)
Epoch: [76][30/75]	Time 0.908 (0.903)	Data 0.005 (0.016)	Loss 0.2878 (0.2820)	Acc@1 98.951 (99.081)	Acc@5 100.000 (99.990)
Epoch: [76][40/75]	Time 0.893 (0.903)	Data 0.004 (0.013)	Loss 0.2759 (0.2827)	Acc@1 99.550 (99.031)	Acc@5 100.000 (99.993)
Epoch: [76][50/75]	Time 0.892 (0.902)	Data 0.004 (0.012)	Loss 0.2831 (0.2835)	Acc@1 99.100 (98.974)	Acc@5 100.000 (99.994)
Epoch: [76][60/75]	Time 0.896 (0.902)	Data 0.004 (0.011)	Loss 0.2765 (0.2838)	Acc@1 99.550 (98.985)	Acc@5 100.000 (99.993)
Epoch: [76][70/75]	Time 0.922 (0.902)	Data 0.005 (0.010)	Loss 0.2885 (0.2837)	Acc@1 98.801 (98.986)	Acc@5 100.000 (99.994)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(125, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(53, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(49, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoche:  76  ; NumbOfParameters:  774189

Test Acc:  90.46

Epoch: [77 | 80] LR: 0.000100
Epoch: [77][0/75]	Time 0.869 (0.869)	Data 0.337 (0.337)	Loss 0.2851 (0.2851)	Acc@1 99.100 (99.100)	Acc@5 100.000 (100.000)
Epoch: [77][10/75]	Time 0.901 (0.892)	Data 0.005 (0.035)	Loss 0.2871 (0.2818)	Acc@1 98.951 (99.087)	Acc@5 100.000 (99.973)
Epoch: [77][20/75]	Time 0.897 (0.895)	Data 0.005 (0.021)	Loss 0.2821 (0.2830)	Acc@1 98.801 (99.043)	Acc@5 100.000 (99.979)
Epoch: [77][30/75]	Time 0.892 (0.895)	Data 0.005 (0.016)	Loss 0.2738 (0.2824)	Acc@1 99.550 (99.028)	Acc@5 100.000 (99.985)
Epoch: [77][40/75]	Time 0.885 (0.895)	Data 0.004 (0.013)	Loss 0.2786 (0.2823)	Acc@1 99.100 (99.046)	Acc@5 100.000 (99.985)
Epoch: [77][50/75]	Time 0.895 (0.895)	Data 0.005 (0.012)	Loss 0.2894 (0.2824)	Acc@1 98.501 (99.039)	Acc@5 100.000 (99.988)
Epoch: [77][60/75]	Time 0.904 (0.895)	Data 0.006 (0.011)	Loss 0.2754 (0.2827)	Acc@1 99.100 (98.995)	Acc@5 100.000 (99.988)
Epoch: [77][70/75]	Time 0.921 (0.896)	Data 0.005 (0.010)	Loss 0.2714 (0.2828)	Acc@1 99.250 (98.978)	Acc@5 100.000 (99.989)

Epoche:  77  ; NumbOfParameters:  774189

Test Acc:  90.15

Epoch: [78 | 80] LR: 0.000100
Epoch: [78][0/75]	Time 0.872 (0.872)	Data 0.339 (0.339)	Loss 0.2890 (0.2890)	Acc@1 98.951 (98.951)	Acc@5 100.000 (100.000)
Epoch: [78][10/75]	Time 0.892 (0.895)	Data 0.005 (0.035)	Loss 0.2903 (0.2829)	Acc@1 98.651 (99.114)	Acc@5 100.000 (99.986)
Epoch: [78][20/75]	Time 0.901 (0.897)	Data 0.005 (0.021)	Loss 0.2800 (0.2810)	Acc@1 98.801 (99.129)	Acc@5 100.000 (99.993)
Epoch: [78][30/75]	Time 0.905 (0.897)	Data 0.004 (0.016)	Loss 0.2847 (0.2800)	Acc@1 98.801 (99.197)	Acc@5 99.850 (99.990)
Epoch: [78][40/75]	Time 0.888 (0.897)	Data 0.005 (0.013)	Loss 0.2837 (0.2797)	Acc@1 98.801 (99.210)	Acc@5 100.000 (99.993)
Epoch: [78][50/75]	Time 0.896 (0.897)	Data 0.005 (0.012)	Loss 0.2914 (0.2807)	Acc@1 98.801 (99.165)	Acc@5 100.000 (99.994)
Epoch: [78][60/75]	Time 0.895 (0.896)	Data 0.005 (0.011)	Loss 0.2831 (0.2810)	Acc@1 98.501 (99.155)	Acc@5 100.000 (99.995)
Epoch: [78][70/75]	Time 0.919 (0.898)	Data 0.005 (0.010)	Loss 0.2712 (0.2813)	Acc@1 99.550 (99.130)	Acc@5 99.850 (99.994)

Epoche:  78  ; NumbOfParameters:  774189

Test Acc:  90.49

Epoch: [79 | 80] LR: 0.000100
Epoch: [79][0/75]	Time 0.851 (0.851)	Data 0.361 (0.361)	Loss 0.2811 (0.2811)	Acc@1 98.801 (98.801)	Acc@5 100.000 (100.000)
Epoch: [79][10/75]	Time 0.899 (0.897)	Data 0.005 (0.037)	Loss 0.2879 (0.2798)	Acc@1 98.951 (99.073)	Acc@5 100.000 (100.000)
Epoch: [79][20/75]	Time 0.891 (0.898)	Data 0.005 (0.022)	Loss 0.2856 (0.2808)	Acc@1 99.100 (99.043)	Acc@5 100.000 (99.986)
Epoch: [79][30/75]	Time 0.907 (0.899)	Data 0.005 (0.016)	Loss 0.2892 (0.2821)	Acc@1 98.801 (99.009)	Acc@5 100.000 (99.990)
Epoch: [79][40/75]	Time 0.896 (0.898)	Data 0.005 (0.014)	Loss 0.2688 (0.2807)	Acc@1 99.400 (99.075)	Acc@5 100.000 (99.993)
Epoch: [79][50/75]	Time 0.903 (0.898)	Data 0.005 (0.012)	Loss 0.2763 (0.2805)	Acc@1 98.651 (99.074)	Acc@5 100.000 (99.994)
Epoch: [79][60/75]	Time 0.911 (0.898)	Data 0.005 (0.011)	Loss 0.2904 (0.2812)	Acc@1 98.801 (99.046)	Acc@5 100.000 (99.993)
Epoch: [79][70/75]	Time 0.901 (0.899)	Data 0.004 (0.010)	Loss 0.2862 (0.2812)	Acc@1 99.100 (99.048)	Acc@5 100.000 (99.994)

Epoche:  79  ; NumbOfParameters:  774189

Test Acc:  90.46

Epoch: [80 | 80] LR: 0.000100
Epoch: [80][0/75]	Time 0.861 (0.861)	Data 0.340 (0.340)	Loss 0.2967 (0.2967)	Acc@1 98.201 (98.201)	Acc@5 100.000 (100.000)
Epoch: [80][10/75]	Time 0.901 (0.899)	Data 0.005 (0.035)	Loss 0.2759 (0.2807)	Acc@1 99.400 (99.060)	Acc@5 100.000 (99.986)
Epoch: [80][20/75]	Time 0.891 (0.898)	Data 0.004 (0.021)	Loss 0.2837 (0.2794)	Acc@1 98.801 (99.136)	Acc@5 100.000 (99.986)
Epoch: [80][30/75]	Time 0.901 (0.898)	Data 0.005 (0.016)	Loss 0.2813 (0.2796)	Acc@1 98.801 (99.081)	Acc@5 100.000 (99.990)
Epoch: [80][40/75]	Time 0.895 (0.898)	Data 0.005 (0.013)	Loss 0.2856 (0.2797)	Acc@1 98.801 (99.115)	Acc@5 100.000 (99.989)
Epoch: [80][50/75]	Time 0.894 (0.899)	Data 0.004 (0.012)	Loss 0.2702 (0.2795)	Acc@1 99.550 (99.150)	Acc@5 100.000 (99.991)
Epoch: [80][60/75]	Time 0.905 (0.899)	Data 0.005 (0.011)	Loss 0.2817 (0.2796)	Acc@1 99.100 (99.123)	Acc@5 100.000 (99.990)
Epoch: [80][70/75]	Time 0.906 (0.900)	Data 0.005 (0.010)	Loss 0.2812 (0.2797)	Acc@1 99.550 (99.096)	Acc@5 100.000 (99.992)
[INFO] Force the sparse filters to zero...
I: 2 ; 0
I: 8 ; 1
I: 14 ; 2
I: 20 ; 3

StagesI:  [['module.conv2.weight', 'module.conv5.weight', 'module.conv8.weight'], ['module.conv11.weight', 'module.conv14.weight'], ['module.conv17.weight', 'module.conv20.weight'], ['module.conv23.weight', 'module.fc26.weight']]

StagesO:  [['module.conv1.weight', 'module.conv4.weight', 'module.conv7.weight'], ['module.conv10.weight', 'module.conv13.weight'], ['module.conv16.weight', 'module.conv19.weight'], ['module.conv22.weight', 'module.conv25.weight']]

Same Node:  [['module.conv2.weight', 'module.conv3.weight', 'module.conv4.weight'], ['module.conv5.weight', 'module.conv6.weight', 'module.conv7.weight'], ['module.conv8.weight', 'module.conv9.weight', 'module.conv10.weight'], ['module.conv11.weight', 'module.conv12.weight', 'module.conv13.weight'], ['module.conv14.weight', 'module.conv15.weight', 'module.conv16.weight'], ['module.conv17.weight', 'module.conv18.weight', 'module.conv19.weight'], ['module.conv20.weight', 'module.conv21.weight', 'module.conv22.weight'], ['module.conv23.weight', 'module.conv24.weight', 'module.conv25.weight']]
[INFO] Squeezing the sparse model to dense one...

new Model:  N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (27): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(128, 125, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(125, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(128, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (45): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(53, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(49, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): AdaptiveAvgPool2d(output_size=(1, 1))
    (51): Linear(in_features=128, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Epoche:  80  ; NumbOfParameters:  774189

Test Acc:  90.45

 Verhltnis Modell Gre:  0.7149180076571743

  4  ;  2  ;  3  ;  80
6137.323s  

