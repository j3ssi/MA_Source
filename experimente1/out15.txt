no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  100
  59.88
 176.867s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0

Epoch: [50 | 50] LR: 0.100000

  200
  78.33
 98.245s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  300
  77.96
 70.578s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  400
  78.8
 57.555s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  500
  74.97
 50.617s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  600
  79.27
 46.088s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  700
  73.02
 41.117s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  800
  76.83
 38.989s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  900
  63.79
 37.384s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  1000
  75.13
 35.375s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  1100
  69.72
 34.412s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  1200
  71.81
 33.245s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  1300
  74.9
 32.371s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  1400
  75.18
 31.651s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  1500
  78.96
 30.941s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  1600
  79.75
 30.329s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  1700
  73.52
 30.438s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  1800
  70.35
 29.832s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  1900
  80.01
 29.076s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000

Epoch: [2 | 50] LR: 0.100000

Epoch: [3 | 50] LR: 0.100000

Epoch: [4 | 50] LR: 0.100000

Epoch: [5 | 50] LR: 0.100000

Epoch: [6 | 50] LR: 0.100000

Epoch: [7 | 50] LR: 0.100000

Epoch: [8 | 50] LR: 0.100000

Epoch: [9 | 50] LR: 0.100000

Epoch: [10 | 50] LR: 0.100000

Epoch: [11 | 50] LR: 0.100000

Epoch: [12 | 50] LR: 0.100000

Epoch: [13 | 50] LR: 0.100000

Epoch: [14 | 50] LR: 0.100000

Epoch: [15 | 50] LR: 0.100000

Epoch: [16 | 50] LR: 0.100000

Epoch: [17 | 50] LR: 0.100000

Epoch: [18 | 50] LR: 0.100000

Epoch: [19 | 50] LR: 0.100000

Epoch: [20 | 50] LR: 0.100000

Epoch: [21 | 50] LR: 0.100000

Epoch: [22 | 50] LR: 0.100000

Epoch: [23 | 50] LR: 0.100000

Epoch: [24 | 50] LR: 0.100000

Epoch: [25 | 50] LR: 0.100000

Epoch: [26 | 50] LR: 0.100000

Epoch: [27 | 50] LR: 0.100000

Epoch: [28 | 50] LR: 0.100000

Epoch: [29 | 50] LR: 0.100000

Epoch: [30 | 50] LR: 0.100000

Epoch: [31 | 50] LR: 0.100000

Epoch: [32 | 50] LR: 0.100000

Epoch: [33 | 50] LR: 0.100000

Epoch: [34 | 50] LR: 0.100000

Epoch: [35 | 50] LR: 0.100000

Epoch: [36 | 50] LR: 0.100000

Epoch: [37 | 50] LR: 0.100000

Epoch: [38 | 50] LR: 0.100000

Epoch: [39 | 50] LR: 0.100000

Epoch: [40 | 50] LR: 0.100000

Epoch: [41 | 50] LR: 0.100000

Epoch: [42 | 50] LR: 0.100000

Epoch: [43 | 50] LR: 0.100000

Epoch: [44 | 50] LR: 0.100000

Epoch: [45 | 50] LR: 0.100000

Epoch: [46 | 50] LR: 0.100000

Epoch: [47 | 50] LR: 0.100000

Epoch: [48 | 50] LR: 0.100000

Epoch: [49 | 50] LR: 0.100000

Epoch: [50 | 50] LR: 0.100000

  2000
  75.96
 28.851s  no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000
Warning: Traceback of forward call that caused the error:
  File "main.py", line 743, in <module>
    main()
  File "main.py", line 503, in main
    use_gpu_num)
  File "main.py", line 611, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 230, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:57)
Traceback (most recent call last):
  File "main.py", line 743, in <module>
    main()
  File "main.py", line 503, in main
    use_gpu_num)
  File "main.py", line 657, in train
    scaled_loss.backward()
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 788.00 MiB (GPU 2; 10.76 GiB total capacity; 8.51 GiB already allocated; 513.56 MiB free; 9.40 GiB reserved in total by PyTorch)
no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000
Warning: Traceback of forward call that caused the error:
  File "main.py", line 743, in <module>
    main()
  File "main.py", line 503, in main
    use_gpu_num)
  File "main.py", line 611, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 230, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:57)
Traceback (most recent call last):
  File "main.py", line 743, in <module>
    main()
  File "main.py", line 503, in main
    use_gpu_num)
  File "main.py", line 657, in train
    scaled_loss.backward()
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 826.00 MiB (GPU 2; 10.76 GiB total capacity; 8.92 GiB already allocated; 55.56 MiB free; 9.85 GiB reserved in total by PyTorch)
no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 743, in <module>
    main()
  File "main.py", line 503, in main
    use_gpu_num)
  File "main.py", line 611, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 230, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 10.76 GiB total capacity; 8.76 GiB already allocated; 199.56 MiB free; 9.71 GiB reserved in total by PyTorch)
no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 743, in <module>
    main()
  File "main.py", line 503, in main
    use_gpu_num)
  File "main.py", line 611, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 266, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 602.00 MiB (GPU 2; 10.76 GiB total capacity; 8.54 GiB already allocated; 329.56 MiB free; 9.59 GiB reserved in total by PyTorch)
no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 743, in <module>
    main()
  File "main.py", line 503, in main
    use_gpu_num)
  File "main.py", line 611, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 213, in forward
    x = self.module_list[j](_x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 626.00 MiB (GPU 2; 10.76 GiB total capacity; 8.60 GiB already allocated; 551.56 MiB free; 9.37 GiB reserved in total by PyTorch)
no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 743, in <module>
    main()
  File "main.py", line 503, in main
    use_gpu_num)
  File "main.py", line 611, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 213, in forward
    x = self.module_list[j](_x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 652.00 MiB (GPU 2; 10.76 GiB total capacity; 8.95 GiB already allocated; 179.56 MiB free; 9.73 GiB reserved in total by PyTorch)
no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 743, in <module>
    main()
  File "main.py", line 503, in main
    use_gpu_num)
  File "main.py", line 611, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 213, in forward
    x = self.module_list[j](_x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 338.00 MiB (GPU 2; 10.76 GiB total capacity; 8.96 GiB already allocated; 145.56 MiB free; 9.77 GiB reserved in total by PyTorch)
no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 743, in <module>
    main()
  File "main.py", line 503, in main
    use_gpu_num)
  File "main.py", line 611, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 230, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 702.00 MiB (GPU 2; 10.76 GiB total capacity; 8.61 GiB already allocated; 467.56 MiB free; 9.45 GiB reserved in total by PyTorch)
no display found. Using non-interactive Agg backend
gpu id:0
Device Name: GeForce RTX 2080 Ti
GPU id:2
Device Name: GeForce RTX 2080 Ti
This Gpu is free
GPU Id: 2
total    : 11019
free     : 11009
used     : 10


Files already downloaded and verified
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic

Epoch: [1 | 50] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 743, in <module>
    main()
  File "main.py", line 503, in main
    use_gpu_num)
  File "main.py", line 611, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 230, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 345, in forward
    return self.conv2d_forward(input, self.weight)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 342, in conv2d_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 726.00 MiB (GPU 2; 10.76 GiB total capacity; 8.91 GiB already allocated; 133.56 MiB free; 9.78 GiB reserved in total by PyTorch)
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 100 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 200 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 300 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 400 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 500 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 600 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 700 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 800 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 900 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 1000 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 1100 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 1200 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 1300 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 1400 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 1500 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 1600 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 1700 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 1800 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 1900 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 2000 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 2100 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 2200 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 2300 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 2400 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 2500 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 2600 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 2700 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 2800 --test_batch 100 -s 3 -n 3 -l 3
python3 main.py --workers 4 --epochs 50 --O2 --test --learning-rate 0.1 --batchTrue --batch_size 2900 --test_batch 100 -s 3 -n 3 -l 3
