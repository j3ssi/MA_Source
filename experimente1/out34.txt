no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.354 (0.354)	Data 0.153 (0.153)	Loss 3.0661 (3.0661)	Acc@1 6.250 (6.250)	Acc@5 55.469 (55.469)
Epoch: [1][64/391]	Time 0.319 (0.309)	Data 0.002 (0.004)	Loss 2.5018 (2.7593)	Acc@1 25.781 (21.983)	Acc@5 88.281 (75.312)
Epoch: [1][128/391]	Time 0.516 (0.366)	Data 0.002 (0.003)	Loss 2.2246 (2.5582)	Acc@1 36.719 (27.150)	Acc@5 88.281 (80.784)
Epoch: [1][192/391]	Time 0.517 (0.417)	Data 0.002 (0.003)	Loss 2.2432 (2.4440)	Acc@1 41.406 (30.946)	Acc@5 88.281 (83.630)
Epoch: [1][256/391]	Time 0.497 (0.443)	Data 0.002 (0.002)	Loss 2.0105 (2.3483)	Acc@1 48.438 (34.156)	Acc@5 89.844 (85.600)
Epoch: [1][320/391]	Time 0.556 (0.460)	Data 0.002 (0.002)	Loss 1.8443 (2.2674)	Acc@1 51.562 (36.994)	Acc@5 92.969 (87.064)
Epoch: [1][384/391]	Time 0.537 (0.471)	Data 0.002 (0.002)	Loss 1.6453 (2.1997)	Acc@1 57.812 (39.243)	Acc@5 95.312 (88.170)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.557 (0.557)	Data 0.215 (0.215)	Loss 1.7718 (1.7718)	Acc@1 55.469 (55.469)	Acc@5 91.406 (91.406)
Epoch: [2][64/391]	Time 0.532 (0.529)	Data 0.003 (0.005)	Loss 1.8287 (1.7489)	Acc@1 50.000 (54.243)	Acc@5 92.188 (94.700)
Epoch: [2][128/391]	Time 0.497 (0.514)	Data 0.002 (0.004)	Loss 1.7013 (1.7126)	Acc@1 56.250 (55.426)	Acc@5 93.750 (94.876)
Epoch: [2][192/391]	Time 0.543 (0.518)	Data 0.002 (0.003)	Loss 1.4725 (1.6813)	Acc@1 65.625 (56.861)	Acc@5 94.531 (95.240)
Epoch: [2][256/391]	Time 0.539 (0.521)	Data 0.002 (0.003)	Loss 1.5746 (1.6521)	Acc@1 60.938 (57.864)	Acc@5 97.656 (95.522)
Epoch: [2][320/391]	Time 0.490 (0.523)	Data 0.001 (0.003)	Loss 1.4011 (1.6293)	Acc@1 64.844 (58.528)	Acc@5 98.438 (95.665)
Epoch: [2][384/391]	Time 0.530 (0.524)	Data 0.002 (0.003)	Loss 1.3492 (1.6026)	Acc@1 65.625 (59.418)	Acc@5 96.875 (95.834)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.511 (0.511)	Data 0.270 (0.270)	Loss 1.5376 (1.5376)	Acc@1 57.812 (57.812)	Acc@5 99.219 (99.219)
Epoch: [3][64/391]	Time 0.523 (0.526)	Data 0.002 (0.006)	Loss 1.2540 (1.4028)	Acc@1 70.312 (65.204)	Acc@5 99.219 (96.875)
Epoch: [3][128/391]	Time 0.545 (0.517)	Data 0.002 (0.004)	Loss 1.4087 (1.3975)	Acc@1 60.938 (65.304)	Acc@5 97.656 (97.008)
Epoch: [3][192/391]	Time 0.547 (0.522)	Data 0.002 (0.003)	Loss 1.2750 (1.3786)	Acc@1 67.969 (65.904)	Acc@5 96.875 (96.964)
Epoch: [3][256/391]	Time 0.540 (0.526)	Data 0.002 (0.003)	Loss 1.2226 (1.3560)	Acc@1 73.438 (66.519)	Acc@5 97.656 (97.091)
Epoch: [3][320/391]	Time 0.535 (0.528)	Data 0.002 (0.003)	Loss 1.2936 (1.3411)	Acc@1 67.188 (66.859)	Acc@5 99.219 (97.189)
Epoch: [3][384/391]	Time 0.563 (0.530)	Data 0.002 (0.003)	Loss 1.1165 (1.3275)	Acc@1 74.219 (67.175)	Acc@5 98.438 (97.299)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.562 (0.562)	Data 0.231 (0.231)	Loss 1.2250 (1.2250)	Acc@1 67.969 (67.969)	Acc@5 96.875 (96.875)
Epoch: [4][64/391]	Time 0.524 (0.532)	Data 0.002 (0.006)	Loss 1.1337 (1.2072)	Acc@1 78.125 (69.964)	Acc@5 99.219 (97.584)
Epoch: [4][128/391]	Time 0.467 (0.517)	Data 0.002 (0.004)	Loss 1.1358 (1.1986)	Acc@1 70.312 (70.422)	Acc@5 97.656 (97.711)
Epoch: [4][192/391]	Time 0.476 (0.498)	Data 0.002 (0.003)	Loss 1.3372 (1.1834)	Acc@1 68.750 (71.069)	Acc@5 96.875 (97.689)
Epoch: [4][256/391]	Time 0.452 (0.489)	Data 0.002 (0.003)	Loss 0.9248 (1.1749)	Acc@1 76.562 (71.218)	Acc@5 100.000 (97.775)
Epoch: [4][320/391]	Time 0.449 (0.484)	Data 0.002 (0.003)	Loss 1.1315 (1.1638)	Acc@1 75.781 (71.454)	Acc@5 96.875 (97.834)
Epoch: [4][384/391]	Time 0.449 (0.481)	Data 0.002 (0.003)	Loss 1.1511 (1.1617)	Acc@1 71.094 (71.443)	Acc@5 97.656 (97.837)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.520 (0.520)	Data 0.227 (0.227)	Loss 1.0205 (1.0205)	Acc@1 74.219 (74.219)	Acc@5 97.656 (97.656)
Epoch: [5][64/391]	Time 0.453 (0.467)	Data 0.002 (0.005)	Loss 0.9028 (1.0966)	Acc@1 78.906 (73.065)	Acc@5 99.219 (98.161)
Epoch: [5][128/391]	Time 0.455 (0.466)	Data 0.002 (0.004)	Loss 1.0158 (1.0852)	Acc@1 76.562 (73.056)	Acc@5 97.656 (98.244)
Epoch: [5][192/391]	Time 0.472 (0.464)	Data 0.002 (0.003)	Loss 1.1717 (1.0736)	Acc@1 70.312 (73.393)	Acc@5 93.750 (98.276)
Epoch: [5][256/391]	Time 0.454 (0.464)	Data 0.002 (0.003)	Loss 1.0915 (1.0717)	Acc@1 76.562 (73.538)	Acc@5 96.094 (98.264)
Epoch: [5][320/391]	Time 0.465 (0.465)	Data 0.002 (0.003)	Loss 1.1713 (1.0650)	Acc@1 69.531 (73.895)	Acc@5 96.875 (98.235)
Epoch: [5][384/391]	Time 0.465 (0.465)	Data 0.001 (0.003)	Loss 0.9841 (1.0583)	Acc@1 74.219 (74.067)	Acc@5 99.219 (98.257)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.442 (0.442)	Data 0.243 (0.243)	Loss 0.9834 (0.9834)	Acc@1 76.562 (76.562)	Acc@5 97.656 (97.656)
Epoch: [6][64/391]	Time 0.465 (0.466)	Data 0.002 (0.006)	Loss 1.0962 (0.9843)	Acc@1 67.969 (75.793)	Acc@5 99.219 (98.594)
Epoch: [6][128/391]	Time 0.475 (0.464)	Data 0.002 (0.004)	Loss 1.1351 (1.0049)	Acc@1 71.875 (75.242)	Acc@5 96.875 (98.498)
Epoch: [6][192/391]	Time 0.472 (0.463)	Data 0.002 (0.003)	Loss 0.9380 (1.0046)	Acc@1 78.125 (75.401)	Acc@5 98.438 (98.506)
Epoch: [6][256/391]	Time 0.455 (0.462)	Data 0.002 (0.003)	Loss 1.1021 (1.0100)	Acc@1 70.312 (75.112)	Acc@5 99.219 (98.495)
Epoch: [6][320/391]	Time 0.447 (0.464)	Data 0.002 (0.003)	Loss 0.9477 (1.0089)	Acc@1 78.906 (75.139)	Acc@5 100.000 (98.472)
Epoch: [6][384/391]	Time 0.477 (0.465)	Data 0.002 (0.003)	Loss 1.1906 (1.0102)	Acc@1 69.531 (75.207)	Acc@5 96.875 (98.438)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.481 (0.481)	Data 0.213 (0.213)	Loss 1.0297 (1.0297)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [7][64/391]	Time 0.438 (0.469)	Data 0.002 (0.005)	Loss 0.8611 (0.9960)	Acc@1 80.469 (76.082)	Acc@5 98.438 (98.558)
Epoch: [7][128/391]	Time 0.465 (0.470)	Data 0.002 (0.004)	Loss 1.0292 (0.9904)	Acc@1 76.562 (76.193)	Acc@5 99.219 (98.583)
Epoch: [7][192/391]	Time 0.453 (0.468)	Data 0.002 (0.003)	Loss 1.0182 (0.9837)	Acc@1 71.875 (76.392)	Acc@5 99.219 (98.640)
Epoch: [7][256/391]	Time 0.505 (0.467)	Data 0.002 (0.003)	Loss 0.8505 (0.9833)	Acc@1 82.812 (76.353)	Acc@5 97.656 (98.620)
Epoch: [7][320/391]	Time 0.466 (0.466)	Data 0.002 (0.003)	Loss 1.1634 (0.9796)	Acc@1 68.750 (76.533)	Acc@5 96.875 (98.603)
Epoch: [7][384/391]	Time 0.475 (0.467)	Data 0.002 (0.003)	Loss 0.9556 (0.9820)	Acc@1 76.562 (76.408)	Acc@5 98.438 (98.573)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.542 (0.542)	Data 0.203 (0.203)	Loss 0.9018 (0.9018)	Acc@1 80.469 (80.469)	Acc@5 97.656 (97.656)
Epoch: [8][64/391]	Time 0.446 (0.475)	Data 0.001 (0.005)	Loss 1.0105 (0.9485)	Acc@1 71.094 (77.861)	Acc@5 98.438 (98.714)
Epoch: [8][128/391]	Time 0.462 (0.471)	Data 0.002 (0.003)	Loss 0.9029 (0.9519)	Acc@1 80.469 (77.713)	Acc@5 98.438 (98.692)
Epoch: [8][192/391]	Time 0.469 (0.470)	Data 0.002 (0.003)	Loss 1.0147 (0.9626)	Acc@1 78.906 (77.437)	Acc@5 97.656 (98.587)
Epoch: [8][256/391]	Time 0.484 (0.468)	Data 0.002 (0.003)	Loss 0.9857 (0.9634)	Acc@1 78.125 (77.371)	Acc@5 96.875 (98.596)
Epoch: [8][320/391]	Time 0.461 (0.467)	Data 0.001 (0.003)	Loss 0.9838 (0.9594)	Acc@1 75.000 (77.563)	Acc@5 98.438 (98.581)
Epoch: [8][384/391]	Time 0.476 (0.466)	Data 0.002 (0.002)	Loss 0.9679 (0.9579)	Acc@1 75.000 (77.547)	Acc@5 98.438 (98.582)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.468 (0.468)	Data 0.242 (0.242)	Loss 0.9610 (0.9610)	Acc@1 80.469 (80.469)	Acc@5 96.875 (96.875)
Epoch: [9][64/391]	Time 0.456 (0.466)	Data 0.001 (0.006)	Loss 0.9691 (0.9770)	Acc@1 77.344 (76.887)	Acc@5 97.656 (98.534)
Epoch: [9][128/391]	Time 0.480 (0.468)	Data 0.002 (0.004)	Loss 0.8937 (0.9520)	Acc@1 78.125 (77.822)	Acc@5 99.219 (98.740)
Epoch: [9][192/391]	Time 0.470 (0.465)	Data 0.002 (0.003)	Loss 0.9330 (0.9506)	Acc@1 78.906 (77.720)	Acc@5 100.000 (98.701)
Epoch: [9][256/391]	Time 0.472 (0.465)	Data 0.002 (0.003)	Loss 0.9108 (0.9454)	Acc@1 81.250 (77.906)	Acc@5 99.219 (98.735)
Epoch: [9][320/391]	Time 0.461 (0.465)	Data 0.002 (0.003)	Loss 1.0263 (0.9426)	Acc@1 73.438 (77.984)	Acc@5 98.438 (98.747)
Epoch: [9][384/391]	Time 0.464 (0.466)	Data 0.002 (0.003)	Loss 0.8198 (0.9433)	Acc@1 83.594 (77.997)	Acc@5 100.000 (98.724)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.452 (0.452)	Data 0.248 (0.248)	Loss 0.9514 (0.9514)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [10][64/391]	Time 0.470 (0.456)	Data 0.002 (0.006)	Loss 1.1253 (0.9413)	Acc@1 74.219 (77.752)	Acc@5 96.875 (98.774)
Epoch: [10][128/391]	Time 0.441 (0.464)	Data 0.001 (0.004)	Loss 1.0111 (0.9393)	Acc@1 78.906 (78.070)	Acc@5 97.656 (98.704)
Epoch: [10][192/391]	Time 0.466 (0.463)	Data 0.002 (0.003)	Loss 0.9729 (0.9495)	Acc@1 77.344 (77.736)	Acc@5 97.656 (98.595)
Epoch: [10][256/391]	Time 0.477 (0.461)	Data 0.002 (0.003)	Loss 0.9741 (0.9436)	Acc@1 75.000 (78.067)	Acc@5 99.219 (98.656)
Epoch: [10][320/391]	Time 0.409 (0.462)	Data 0.002 (0.003)	Loss 0.9223 (0.9409)	Acc@1 81.250 (78.205)	Acc@5 96.094 (98.610)
Epoch: [10][384/391]	Time 0.462 (0.463)	Data 0.002 (0.003)	Loss 1.0159 (0.9369)	Acc@1 78.125 (78.273)	Acc@5 99.219 (98.628)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 468922 ; 487386 ; 0.9621162692403967

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.470 (0.470)	Data 0.200 (0.200)	Loss 1.0553 (1.0553)	Acc@1 78.125 (78.125)	Acc@5 100.000 (100.000)
Epoch: [11][64/391]	Time 0.474 (0.461)	Data 0.002 (0.005)	Loss 0.9289 (0.9096)	Acc@1 76.562 (79.014)	Acc@5 100.000 (98.798)
Epoch: [11][128/391]	Time 0.486 (0.463)	Data 0.002 (0.004)	Loss 0.8584 (0.9123)	Acc@1 81.250 (79.033)	Acc@5 98.438 (98.680)
Epoch: [11][192/391]	Time 0.454 (0.464)	Data 0.002 (0.003)	Loss 0.9398 (0.9227)	Acc@1 81.250 (78.716)	Acc@5 96.875 (98.668)
Epoch: [11][256/391]	Time 0.465 (0.463)	Data 0.002 (0.003)	Loss 0.7747 (0.9283)	Acc@1 89.062 (78.660)	Acc@5 99.219 (98.650)
Epoch: [11][320/391]	Time 0.516 (0.465)	Data 0.002 (0.003)	Loss 0.9364 (0.9282)	Acc@1 77.344 (78.677)	Acc@5 97.656 (98.666)
Epoch: [11][384/391]	Time 0.480 (0.466)	Data 0.002 (0.003)	Loss 0.8556 (0.9263)	Acc@1 82.031 (78.736)	Acc@5 99.219 (98.677)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.480 (0.480)	Data 0.277 (0.277)	Loss 0.9177 (0.9177)	Acc@1 78.125 (78.125)	Acc@5 100.000 (100.000)
Epoch: [12][64/391]	Time 0.449 (0.461)	Data 0.002 (0.006)	Loss 0.9880 (0.9014)	Acc@1 71.094 (79.026)	Acc@5 99.219 (98.894)
Epoch: [12][128/391]	Time 0.468 (0.461)	Data 0.002 (0.004)	Loss 0.8900 (0.9104)	Acc@1 80.469 (78.888)	Acc@5 96.875 (98.837)
Epoch: [12][192/391]	Time 0.455 (0.462)	Data 0.002 (0.003)	Loss 0.9696 (0.9137)	Acc@1 75.000 (78.809)	Acc@5 99.219 (98.761)
Epoch: [12][256/391]	Time 0.480 (0.461)	Data 0.002 (0.003)	Loss 0.9101 (0.9173)	Acc@1 80.469 (78.763)	Acc@5 100.000 (98.781)
Epoch: [12][320/391]	Time 0.510 (0.461)	Data 0.002 (0.003)	Loss 0.9992 (0.9134)	Acc@1 77.344 (78.928)	Acc@5 99.219 (98.793)
Epoch: [12][384/391]	Time 0.455 (0.462)	Data 0.005 (0.003)	Loss 1.1790 (0.9133)	Acc@1 71.875 (79.028)	Acc@5 96.094 (98.768)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.461 (0.461)	Data 0.256 (0.256)	Loss 0.7629 (0.7629)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [13][64/391]	Time 0.469 (0.469)	Data 0.002 (0.006)	Loss 0.8105 (0.9000)	Acc@1 83.594 (80.337)	Acc@5 100.000 (98.726)
Epoch: [13][128/391]	Time 0.453 (0.466)	Data 0.002 (0.004)	Loss 0.9039 (0.9020)	Acc@1 78.125 (79.978)	Acc@5 99.219 (98.722)
Epoch: [13][192/391]	Time 0.471 (0.463)	Data 0.002 (0.003)	Loss 0.8508 (0.9035)	Acc@1 78.906 (79.534)	Acc@5 100.000 (98.741)
Epoch: [13][256/391]	Time 0.454 (0.464)	Data 0.002 (0.003)	Loss 0.9304 (0.9104)	Acc@1 82.031 (79.277)	Acc@5 98.438 (98.760)
Epoch: [13][320/391]	Time 0.525 (0.466)	Data 0.002 (0.003)	Loss 0.9340 (0.9126)	Acc@1 79.688 (79.206)	Acc@5 98.438 (98.761)
Epoch: [13][384/391]	Time 0.477 (0.466)	Data 0.002 (0.003)	Loss 0.8631 (0.9167)	Acc@1 78.906 (79.097)	Acc@5 99.219 (98.744)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.467 (0.467)	Data 0.223 (0.223)	Loss 1.0083 (1.0083)	Acc@1 78.125 (78.125)	Acc@5 96.875 (96.875)
Epoch: [14][64/391]	Time 0.456 (0.465)	Data 0.002 (0.005)	Loss 0.8664 (0.9011)	Acc@1 82.031 (79.543)	Acc@5 98.438 (99.014)
Epoch: [14][128/391]	Time 0.469 (0.466)	Data 0.002 (0.004)	Loss 1.0934 (0.9014)	Acc@1 71.094 (79.415)	Acc@5 96.875 (98.910)
Epoch: [14][192/391]	Time 0.468 (0.466)	Data 0.002 (0.003)	Loss 0.9707 (0.9107)	Acc@1 74.219 (79.121)	Acc@5 99.219 (98.871)
Epoch: [14][256/391]	Time 0.437 (0.464)	Data 0.002 (0.003)	Loss 0.7739 (0.9078)	Acc@1 83.594 (79.271)	Acc@5 100.000 (98.909)
Epoch: [14][320/391]	Time 0.519 (0.465)	Data 0.002 (0.003)	Loss 0.8586 (0.9082)	Acc@1 83.594 (79.179)	Acc@5 98.438 (98.917)
Epoch: [14][384/391]	Time 0.474 (0.465)	Data 0.002 (0.003)	Loss 0.6835 (0.9019)	Acc@1 87.500 (79.316)	Acc@5 99.219 (98.947)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.505 (0.505)	Data 0.196 (0.196)	Loss 0.9587 (0.9587)	Acc@1 78.125 (78.125)	Acc@5 100.000 (100.000)
Epoch: [15][64/391]	Time 0.468 (0.469)	Data 0.002 (0.005)	Loss 0.8989 (0.9008)	Acc@1 81.250 (79.808)	Acc@5 98.438 (98.714)
Epoch: [15][128/391]	Time 0.454 (0.470)	Data 0.002 (0.004)	Loss 0.9964 (0.8896)	Acc@1 79.688 (80.281)	Acc@5 98.438 (98.807)
Epoch: [15][192/391]	Time 0.444 (0.466)	Data 0.002 (0.003)	Loss 1.0053 (0.8954)	Acc@1 75.781 (79.975)	Acc@5 97.656 (98.773)
Epoch: [15][256/391]	Time 0.498 (0.464)	Data 0.002 (0.003)	Loss 0.9162 (0.8922)	Acc@1 78.906 (80.071)	Acc@5 97.656 (98.757)
Epoch: [15][320/391]	Time 0.533 (0.465)	Data 0.002 (0.003)	Loss 0.9659 (0.8926)	Acc@1 76.562 (79.982)	Acc@5 98.438 (98.815)
Epoch: [15][384/391]	Time 0.452 (0.462)	Data 0.002 (0.002)	Loss 0.9302 (0.8942)	Acc@1 78.906 (79.897)	Acc@5 96.875 (98.811)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 412932 ; 487386 ; 0.8472381233765435

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.464 (0.464)	Data 0.182 (0.182)	Loss 0.8324 (0.8324)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)
Epoch: [16][64/391]	Time 0.489 (0.467)	Data 0.002 (0.005)	Loss 0.7775 (0.8650)	Acc@1 84.375 (80.673)	Acc@5 100.000 (98.954)
Epoch: [16][128/391]	Time 0.485 (0.464)	Data 0.002 (0.003)	Loss 0.9940 (0.8806)	Acc@1 75.781 (80.336)	Acc@5 99.219 (98.946)
Epoch: [16][192/391]	Time 0.453 (0.463)	Data 0.003 (0.003)	Loss 0.8368 (0.8851)	Acc@1 82.031 (80.278)	Acc@5 99.219 (98.931)
Epoch: [16][256/391]	Time 0.460 (0.461)	Data 0.002 (0.003)	Loss 1.0732 (0.8944)	Acc@1 74.219 (79.858)	Acc@5 100.000 (98.942)
Epoch: [16][320/391]	Time 0.468 (0.461)	Data 0.002 (0.003)	Loss 0.8247 (0.8939)	Acc@1 84.375 (79.894)	Acc@5 96.094 (98.944)
Epoch: [16][384/391]	Time 0.474 (0.461)	Data 0.002 (0.002)	Loss 0.9398 (0.8957)	Acc@1 78.125 (79.907)	Acc@5 99.219 (98.890)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.463 (0.463)	Data 0.275 (0.275)	Loss 0.9195 (0.9195)	Acc@1 78.125 (78.125)	Acc@5 98.438 (98.438)
Epoch: [17][64/391]	Time 0.368 (0.465)	Data 0.002 (0.006)	Loss 0.7655 (0.8575)	Acc@1 82.812 (81.466)	Acc@5 99.219 (98.846)
Epoch: [17][128/391]	Time 0.464 (0.464)	Data 0.002 (0.004)	Loss 0.9249 (0.8734)	Acc@1 77.344 (80.790)	Acc@5 98.438 (98.843)
Epoch: [17][192/391]	Time 0.475 (0.461)	Data 0.001 (0.003)	Loss 0.9382 (0.8816)	Acc@1 81.250 (80.704)	Acc@5 99.219 (98.891)
Epoch: [17][256/391]	Time 0.461 (0.461)	Data 0.002 (0.003)	Loss 0.8145 (0.8817)	Acc@1 82.031 (80.633)	Acc@5 100.000 (98.930)
Epoch: [17][320/391]	Time 0.452 (0.462)	Data 0.002 (0.003)	Loss 0.9095 (0.8855)	Acc@1 78.125 (80.444)	Acc@5 98.438 (98.885)
Epoch: [17][384/391]	Time 0.450 (0.461)	Data 0.001 (0.003)	Loss 0.8842 (0.8857)	Acc@1 76.562 (80.345)	Acc@5 99.219 (98.892)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.517 (0.517)	Data 0.191 (0.191)	Loss 0.7547 (0.7547)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [18][64/391]	Time 0.398 (0.458)	Data 0.002 (0.005)	Loss 0.7909 (0.8895)	Acc@1 85.156 (80.036)	Acc@5 99.219 (98.894)
Epoch: [18][128/391]	Time 0.451 (0.456)	Data 0.002 (0.003)	Loss 0.7669 (0.8958)	Acc@1 85.156 (79.663)	Acc@5 99.219 (98.958)
Epoch: [18][192/391]	Time 0.447 (0.457)	Data 0.003 (0.003)	Loss 0.8806 (0.8926)	Acc@1 82.031 (79.781)	Acc@5 98.438 (99.008)
Epoch: [18][256/391]	Time 0.467 (0.458)	Data 0.002 (0.003)	Loss 0.7757 (0.8909)	Acc@1 82.812 (79.855)	Acc@5 100.000 (99.015)
Epoch: [18][320/391]	Time 0.435 (0.458)	Data 0.002 (0.003)	Loss 0.9477 (0.8867)	Acc@1 79.688 (79.975)	Acc@5 100.000 (99.002)
Epoch: [18][384/391]	Time 0.466 (0.458)	Data 0.002 (0.002)	Loss 0.8964 (0.8877)	Acc@1 81.250 (79.945)	Acc@5 98.438 (98.959)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.487 (0.487)	Data 0.264 (0.264)	Loss 0.8891 (0.8891)	Acc@1 78.125 (78.125)	Acc@5 98.438 (98.438)
Epoch: [19][64/391]	Time 0.378 (0.466)	Data 0.001 (0.006)	Loss 0.6574 (0.8583)	Acc@1 88.281 (80.613)	Acc@5 99.219 (99.050)
Epoch: [19][128/391]	Time 0.474 (0.464)	Data 0.002 (0.004)	Loss 0.7521 (0.8645)	Acc@1 85.938 (80.675)	Acc@5 98.438 (98.977)
Epoch: [19][192/391]	Time 0.462 (0.463)	Data 0.002 (0.003)	Loss 0.9370 (0.8740)	Acc@1 78.906 (80.335)	Acc@5 99.219 (98.915)
Epoch: [19][256/391]	Time 0.489 (0.461)	Data 0.002 (0.003)	Loss 0.7414 (0.8797)	Acc@1 85.938 (80.177)	Acc@5 99.219 (98.939)
Epoch: [19][320/391]	Time 0.430 (0.460)	Data 0.003 (0.003)	Loss 0.7703 (0.8770)	Acc@1 86.719 (80.259)	Acc@5 98.438 (98.939)
Epoch: [19][384/391]	Time 0.477 (0.460)	Data 0.002 (0.003)	Loss 0.8621 (0.8781)	Acc@1 82.812 (80.177)	Acc@5 98.438 (98.957)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.455 (0.455)	Data 0.226 (0.226)	Loss 0.9607 (0.9607)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [20][64/391]	Time 0.394 (0.460)	Data 0.002 (0.005)	Loss 0.7444 (0.8542)	Acc@1 85.938 (81.502)	Acc@5 99.219 (98.966)
Epoch: [20][128/391]	Time 0.483 (0.459)	Data 0.002 (0.004)	Loss 0.9468 (0.8669)	Acc@1 75.000 (80.784)	Acc@5 99.219 (98.934)
Epoch: [20][192/391]	Time 0.482 (0.459)	Data 0.002 (0.003)	Loss 0.9780 (0.8732)	Acc@1 79.688 (80.679)	Acc@5 99.219 (98.935)
Epoch: [20][256/391]	Time 0.460 (0.459)	Data 0.002 (0.003)	Loss 0.8691 (0.8756)	Acc@1 78.906 (80.457)	Acc@5 100.000 (98.948)
Epoch: [20][320/391]	Time 0.343 (0.460)	Data 0.002 (0.003)	Loss 0.8042 (0.8742)	Acc@1 84.375 (80.481)	Acc@5 98.438 (98.971)
Epoch: [20][384/391]	Time 0.461 (0.458)	Data 0.002 (0.003)	Loss 0.7832 (0.8764)	Acc@1 85.938 (80.432)	Acc@5 99.219 (98.953)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 356206 ; 487386 ; 0.7308498807926367

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.503 (0.503)	Data 0.193 (0.193)	Loss 0.7640 (0.7640)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [21][64/391]	Time 0.468 (0.463)	Data 0.002 (0.005)	Loss 1.0819 (0.8507)	Acc@1 74.219 (81.322)	Acc@5 97.656 (99.147)
Epoch: [21][128/391]	Time 0.426 (0.453)	Data 0.002 (0.003)	Loss 0.7663 (0.8591)	Acc@1 83.594 (81.044)	Acc@5 98.438 (99.025)
Epoch: [21][192/391]	Time 0.442 (0.455)	Data 0.002 (0.003)	Loss 0.8428 (0.8617)	Acc@1 82.031 (80.821)	Acc@5 97.656 (98.996)
Epoch: [21][256/391]	Time 0.474 (0.456)	Data 0.002 (0.003)	Loss 0.9273 (0.8589)	Acc@1 75.781 (80.858)	Acc@5 100.000 (99.012)
Epoch: [21][320/391]	Time 0.453 (0.457)	Data 0.002 (0.003)	Loss 0.9421 (0.8574)	Acc@1 76.562 (80.858)	Acc@5 97.656 (99.012)
Epoch: [21][384/391]	Time 0.450 (0.458)	Data 0.002 (0.003)	Loss 0.8113 (0.8589)	Acc@1 84.375 (80.755)	Acc@5 99.219 (98.947)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.491 (0.491)	Data 0.188 (0.188)	Loss 0.7981 (0.7981)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [22][64/391]	Time 0.465 (0.460)	Data 0.002 (0.005)	Loss 0.8164 (0.8353)	Acc@1 82.812 (80.998)	Acc@5 99.219 (98.918)
Epoch: [22][128/391]	Time 0.440 (0.459)	Data 0.002 (0.003)	Loss 0.7276 (0.8550)	Acc@1 86.719 (80.566)	Acc@5 100.000 (98.849)
Epoch: [22][192/391]	Time 0.436 (0.458)	Data 0.002 (0.003)	Loss 0.9316 (0.8538)	Acc@1 75.000 (80.554)	Acc@5 98.438 (98.887)
Epoch: [22][256/391]	Time 0.487 (0.458)	Data 0.002 (0.003)	Loss 0.8353 (0.8533)	Acc@1 81.250 (80.630)	Acc@5 100.000 (98.906)
Epoch: [22][320/391]	Time 0.438 (0.458)	Data 0.002 (0.003)	Loss 0.9532 (0.8556)	Acc@1 76.562 (80.583)	Acc@5 99.219 (98.924)
Epoch: [22][384/391]	Time 0.494 (0.458)	Data 0.002 (0.002)	Loss 0.9367 (0.8521)	Acc@1 78.125 (80.668)	Acc@5 99.219 (98.925)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.493 (0.493)	Data 0.208 (0.208)	Loss 0.7638 (0.7638)	Acc@1 78.906 (78.906)	Acc@5 100.000 (100.000)
Epoch: [23][64/391]	Time 0.478 (0.461)	Data 0.002 (0.005)	Loss 0.9514 (0.8656)	Acc@1 75.000 (80.325)	Acc@5 96.094 (98.678)
Epoch: [23][128/391]	Time 0.432 (0.457)	Data 0.001 (0.004)	Loss 1.0513 (0.8607)	Acc@1 76.562 (80.420)	Acc@5 97.656 (98.849)
Epoch: [23][192/391]	Time 0.456 (0.457)	Data 0.002 (0.003)	Loss 0.8364 (0.8556)	Acc@1 82.031 (80.724)	Acc@5 97.656 (98.907)
Epoch: [23][256/391]	Time 0.443 (0.456)	Data 0.002 (0.003)	Loss 0.8744 (0.8515)	Acc@1 75.781 (80.815)	Acc@5 100.000 (98.945)
Epoch: [23][320/391]	Time 0.476 (0.456)	Data 0.002 (0.003)	Loss 0.8299 (0.8470)	Acc@1 79.688 (80.880)	Acc@5 96.875 (98.953)
Epoch: [23][384/391]	Time 0.483 (0.456)	Data 0.002 (0.003)	Loss 0.8785 (0.8501)	Acc@1 81.250 (80.690)	Acc@5 100.000 (98.985)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.480 (0.480)	Data 0.199 (0.199)	Loss 0.7743 (0.7743)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [24][64/391]	Time 0.492 (0.457)	Data 0.002 (0.005)	Loss 0.6851 (0.8237)	Acc@1 90.625 (81.947)	Acc@5 100.000 (99.123)
Epoch: [24][128/391]	Time 0.435 (0.457)	Data 0.002 (0.004)	Loss 0.8006 (0.8328)	Acc@1 83.594 (81.632)	Acc@5 100.000 (99.043)
Epoch: [24][192/391]	Time 0.399 (0.455)	Data 0.001 (0.003)	Loss 0.9414 (0.8333)	Acc@1 76.562 (81.444)	Acc@5 96.875 (98.939)
Epoch: [24][256/391]	Time 0.434 (0.455)	Data 0.002 (0.003)	Loss 0.7862 (0.8402)	Acc@1 85.156 (81.329)	Acc@5 100.000 (98.909)
Epoch: [24][320/391]	Time 0.483 (0.454)	Data 0.002 (0.003)	Loss 0.9348 (0.8439)	Acc@1 80.469 (81.145)	Acc@5 99.219 (98.902)
Epoch: [24][384/391]	Time 0.476 (0.455)	Data 0.001 (0.003)	Loss 0.7994 (0.8428)	Acc@1 79.688 (81.147)	Acc@5 99.219 (98.922)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.450 (0.450)	Data 0.217 (0.217)	Loss 0.8272 (0.8272)	Acc@1 78.906 (78.906)	Acc@5 100.000 (100.000)
Epoch: [25][64/391]	Time 0.452 (0.456)	Data 0.002 (0.005)	Loss 0.8440 (0.8394)	Acc@1 76.562 (81.058)	Acc@5 99.219 (99.267)
Epoch: [25][128/391]	Time 0.440 (0.455)	Data 0.003 (0.004)	Loss 0.7401 (0.8351)	Acc@1 79.688 (81.250)	Acc@5 100.000 (99.176)
Epoch: [25][192/391]	Time 0.460 (0.455)	Data 0.002 (0.003)	Loss 0.7329 (0.8395)	Acc@1 84.375 (81.108)	Acc@5 99.219 (99.085)
Epoch: [25][256/391]	Time 0.360 (0.454)	Data 0.002 (0.003)	Loss 0.8836 (0.8374)	Acc@1 81.250 (81.214)	Acc@5 99.219 (99.036)
Epoch: [25][320/391]	Time 0.374 (0.450)	Data 0.002 (0.003)	Loss 0.9552 (0.8406)	Acc@1 81.250 (81.155)	Acc@5 97.656 (99.051)
Epoch: [25][384/391]	Time 0.478 (0.452)	Data 0.002 (0.003)	Loss 0.9339 (0.8415)	Acc@1 76.562 (81.118)	Acc@5 99.219 (99.032)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.561 (0.561)	Data 0.199 (0.199)	Loss 3.2870 (3.2870)	Acc@1 8.594 (8.594)	Acc@5 56.250 (56.250)
Epoch: [1][64/391]	Time 0.403 (0.460)	Data 0.002 (0.005)	Loss 2.3958 (2.7299)	Acc@1 33.594 (22.464)	Acc@5 84.375 (76.406)
Epoch: [1][128/391]	Time 0.454 (0.460)	Data 0.002 (0.003)	Loss 2.2305 (2.5924)	Acc@1 37.500 (25.739)	Acc@5 91.406 (80.675)
Epoch: [1][192/391]	Time 0.470 (0.458)	Data 0.003 (0.003)	Loss 2.4732 (2.5087)	Acc@1 31.250 (28.372)	Acc@5 78.125 (82.748)
Epoch: [1][256/391]	Time 0.456 (0.459)	Data 0.001 (0.003)	Loss 2.2267 (2.4324)	Acc@1 33.594 (30.806)	Acc@5 92.969 (84.381)
Epoch: [1][320/391]	Time 0.428 (0.459)	Data 0.002 (0.002)	Loss 1.8775 (2.3660)	Acc@1 47.656 (32.868)	Acc@5 96.875 (85.857)
Epoch: [1][384/391]	Time 0.437 (0.458)	Data 0.002 (0.002)	Loss 1.8682 (2.2992)	Acc@1 52.344 (35.185)	Acc@5 92.188 (86.987)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.514 (0.514)	Data 0.202 (0.202)	Loss 2.0788 (2.0788)	Acc@1 46.094 (46.094)	Acc@5 93.750 (93.750)
Epoch: [2][64/391]	Time 0.450 (0.463)	Data 0.002 (0.005)	Loss 1.7529 (1.8603)	Acc@1 52.344 (50.577)	Acc@5 92.188 (93.702)
Epoch: [2][128/391]	Time 0.467 (0.462)	Data 0.001 (0.003)	Loss 1.6589 (1.8154)	Acc@1 57.031 (51.599)	Acc@5 94.531 (94.162)
Epoch: [2][192/391]	Time 0.473 (0.462)	Data 0.002 (0.003)	Loss 1.6219 (1.7661)	Acc@1 58.594 (53.510)	Acc@5 96.094 (94.588)
Epoch: [2][256/391]	Time 0.467 (0.462)	Data 0.002 (0.003)	Loss 1.6021 (1.7219)	Acc@1 52.344 (55.128)	Acc@5 95.312 (94.914)
Epoch: [2][320/391]	Time 0.447 (0.461)	Data 0.001 (0.002)	Loss 1.4469 (1.6860)	Acc@1 64.062 (56.119)	Acc@5 99.219 (95.186)
Epoch: [2][384/391]	Time 0.472 (0.461)	Data 0.002 (0.002)	Loss 1.3502 (1.6470)	Acc@1 66.406 (57.401)	Acc@5 96.094 (95.412)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.535 (0.535)	Data 0.238 (0.238)	Loss 1.3062 (1.3062)	Acc@1 66.406 (66.406)	Acc@5 98.438 (98.438)
Epoch: [3][64/391]	Time 0.493 (0.454)	Data 0.002 (0.005)	Loss 1.3398 (1.3937)	Acc@1 66.406 (64.651)	Acc@5 98.438 (96.911)
Epoch: [3][128/391]	Time 0.457 (0.460)	Data 0.002 (0.004)	Loss 1.3503 (1.3642)	Acc@1 64.062 (65.437)	Acc@5 96.875 (97.184)
Epoch: [3][192/391]	Time 0.465 (0.462)	Data 0.002 (0.003)	Loss 1.2069 (1.3464)	Acc@1 74.219 (66.091)	Acc@5 96.875 (97.260)
Epoch: [3][256/391]	Time 0.422 (0.462)	Data 0.001 (0.003)	Loss 1.1930 (1.3175)	Acc@1 70.312 (67.008)	Acc@5 96.094 (97.331)
Epoch: [3][320/391]	Time 0.449 (0.463)	Data 0.001 (0.003)	Loss 1.1404 (1.3014)	Acc@1 72.656 (67.528)	Acc@5 100.000 (97.384)
Epoch: [3][384/391]	Time 0.460 (0.462)	Data 0.002 (0.003)	Loss 1.1282 (1.2824)	Acc@1 66.406 (68.105)	Acc@5 98.438 (97.488)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.494 (0.494)	Data 0.240 (0.240)	Loss 1.1555 (1.1555)	Acc@1 75.000 (75.000)	Acc@5 96.875 (96.875)
Epoch: [4][64/391]	Time 0.464 (0.465)	Data 0.002 (0.006)	Loss 1.2420 (1.1523)	Acc@1 70.312 (72.103)	Acc@5 97.656 (98.077)
Epoch: [4][128/391]	Time 0.465 (0.466)	Data 0.002 (0.004)	Loss 1.2378 (1.1465)	Acc@1 65.625 (72.063)	Acc@5 96.875 (98.026)
Epoch: [4][192/391]	Time 0.453 (0.465)	Data 0.002 (0.003)	Loss 1.1160 (1.1375)	Acc@1 73.438 (72.389)	Acc@5 98.438 (98.077)
Epoch: [4][256/391]	Time 0.471 (0.464)	Data 0.003 (0.003)	Loss 1.0576 (1.1219)	Acc@1 72.656 (72.869)	Acc@5 99.219 (98.173)
Epoch: [4][320/391]	Time 0.467 (0.463)	Data 0.002 (0.003)	Loss 1.0865 (1.1131)	Acc@1 71.875 (73.121)	Acc@5 97.656 (98.165)
Epoch: [4][384/391]	Time 0.486 (0.463)	Data 0.002 (0.003)	Loss 1.0278 (1.1047)	Acc@1 76.562 (73.338)	Acc@5 99.219 (98.200)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.461 (0.461)	Data 0.222 (0.222)	Loss 1.1274 (1.1274)	Acc@1 68.750 (68.750)	Acc@5 98.438 (98.438)
Epoch: [5][64/391]	Time 0.441 (0.463)	Data 0.002 (0.005)	Loss 1.0567 (1.0394)	Acc@1 78.125 (74.892)	Acc@5 98.438 (98.438)
Epoch: [5][128/391]	Time 0.442 (0.468)	Data 0.002 (0.004)	Loss 0.8956 (1.0361)	Acc@1 80.469 (75.030)	Acc@5 98.438 (98.419)
Epoch: [5][192/391]	Time 0.485 (0.467)	Data 0.002 (0.003)	Loss 1.0693 (1.0353)	Acc@1 75.000 (75.036)	Acc@5 100.000 (98.405)
Epoch: [5][256/391]	Time 0.451 (0.468)	Data 0.002 (0.003)	Loss 1.0600 (1.0349)	Acc@1 71.094 (75.070)	Acc@5 99.219 (98.371)
Epoch: [5][320/391]	Time 0.459 (0.468)	Data 0.002 (0.003)	Loss 1.0552 (1.0308)	Acc@1 71.094 (75.241)	Acc@5 99.219 (98.328)
Epoch: [5][384/391]	Time 0.436 (0.467)	Data 0.001 (0.003)	Loss 1.0688 (1.0231)	Acc@1 70.312 (75.388)	Acc@5 96.875 (98.387)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.474 (0.474)	Data 0.252 (0.252)	Loss 0.9360 (0.9360)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [6][64/391]	Time 0.468 (0.461)	Data 0.002 (0.006)	Loss 0.8458 (0.9390)	Acc@1 83.594 (78.438)	Acc@5 100.000 (98.630)
Epoch: [6][128/391]	Time 0.471 (0.468)	Data 0.002 (0.004)	Loss 1.0191 (0.9585)	Acc@1 76.562 (77.277)	Acc@5 96.875 (98.571)
Epoch: [6][192/391]	Time 0.446 (0.466)	Data 0.002 (0.003)	Loss 0.8869 (0.9753)	Acc@1 81.250 (76.793)	Acc@5 100.000 (98.470)
Epoch: [6][256/391]	Time 0.480 (0.466)	Data 0.002 (0.003)	Loss 0.9302 (0.9801)	Acc@1 78.906 (76.626)	Acc@5 99.219 (98.459)
Epoch: [6][320/391]	Time 0.476 (0.466)	Data 0.002 (0.003)	Loss 0.8941 (0.9791)	Acc@1 80.469 (76.699)	Acc@5 98.438 (98.430)
Epoch: [6][384/391]	Time 0.468 (0.464)	Data 0.001 (0.003)	Loss 0.9387 (0.9792)	Acc@1 80.469 (76.644)	Acc@5 99.219 (98.504)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.499 (0.499)	Data 0.213 (0.213)	Loss 1.0024 (1.0024)	Acc@1 75.781 (75.781)	Acc@5 97.656 (97.656)
Epoch: [7][64/391]	Time 0.478 (0.468)	Data 0.003 (0.005)	Loss 0.9802 (0.9342)	Acc@1 78.906 (78.245)	Acc@5 99.219 (98.558)
Epoch: [7][128/391]	Time 0.487 (0.464)	Data 0.002 (0.004)	Loss 0.9534 (0.9493)	Acc@1 78.125 (77.435)	Acc@5 100.000 (98.637)
Epoch: [7][192/391]	Time 0.450 (0.464)	Data 0.002 (0.003)	Loss 1.0175 (0.9505)	Acc@1 72.656 (77.583)	Acc@5 98.438 (98.583)
Epoch: [7][256/391]	Time 0.469 (0.464)	Data 0.002 (0.003)	Loss 0.9804 (0.9500)	Acc@1 74.219 (77.645)	Acc@5 99.219 (98.565)
Epoch: [7][320/391]	Time 0.505 (0.464)	Data 0.002 (0.003)	Loss 0.9700 (0.9499)	Acc@1 78.125 (77.548)	Acc@5 98.438 (98.569)
Epoch: [7][384/391]	Time 0.488 (0.464)	Data 0.002 (0.002)	Loss 1.0280 (0.9486)	Acc@1 75.781 (77.599)	Acc@5 98.438 (98.580)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.511 (0.511)	Data 0.273 (0.273)	Loss 0.8276 (0.8276)	Acc@1 80.469 (80.469)	Acc@5 100.000 (100.000)
Epoch: [8][64/391]	Time 0.488 (0.465)	Data 0.002 (0.006)	Loss 0.8917 (0.9288)	Acc@1 81.250 (78.269)	Acc@5 99.219 (98.762)
Epoch: [8][128/391]	Time 0.497 (0.465)	Data 0.002 (0.004)	Loss 0.8627 (0.9223)	Acc@1 80.469 (78.434)	Acc@5 100.000 (98.758)
Epoch: [8][192/391]	Time 0.454 (0.464)	Data 0.002 (0.003)	Loss 0.9208 (0.9225)	Acc@1 78.906 (78.400)	Acc@5 97.656 (98.741)
Epoch: [8][256/391]	Time 0.471 (0.464)	Data 0.002 (0.003)	Loss 0.9394 (0.9220)	Acc@1 77.344 (78.417)	Acc@5 98.438 (98.745)
Epoch: [8][320/391]	Time 0.460 (0.462)	Data 0.002 (0.003)	Loss 1.0135 (0.9258)	Acc@1 73.438 (78.381)	Acc@5 99.219 (98.727)
Epoch: [8][384/391]	Time 0.448 (0.463)	Data 0.001 (0.003)	Loss 0.8887 (0.9268)	Acc@1 82.031 (78.421)	Acc@5 98.438 (98.726)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.478 (0.478)	Data 0.285 (0.285)	Loss 1.0123 (1.0123)	Acc@1 75.781 (75.781)	Acc@5 100.000 (100.000)
Epoch: [9][64/391]	Time 0.423 (0.467)	Data 0.002 (0.006)	Loss 1.1426 (0.9204)	Acc@1 67.969 (78.077)	Acc@5 96.875 (98.618)
Epoch: [9][128/391]	Time 0.391 (0.464)	Data 0.002 (0.004)	Loss 0.9974 (0.9120)	Acc@1 75.000 (78.779)	Acc@5 99.219 (98.649)
Epoch: [9][192/391]	Time 0.448 (0.462)	Data 0.002 (0.003)	Loss 0.9994 (0.9115)	Acc@1 75.000 (78.825)	Acc@5 98.438 (98.688)
Epoch: [9][256/391]	Time 0.487 (0.462)	Data 0.002 (0.003)	Loss 1.0287 (0.9140)	Acc@1 74.219 (78.678)	Acc@5 97.656 (98.717)
Epoch: [9][320/391]	Time 0.452 (0.463)	Data 0.002 (0.003)	Loss 1.2168 (0.9142)	Acc@1 71.875 (78.736)	Acc@5 93.750 (98.705)
Epoch: [9][384/391]	Time 0.478 (0.463)	Data 0.002 (0.003)	Loss 0.9833 (0.9135)	Acc@1 74.219 (78.772)	Acc@5 97.656 (98.711)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.507 (0.507)	Data 0.225 (0.225)	Loss 1.0365 (1.0365)	Acc@1 73.438 (73.438)	Acc@5 97.656 (97.656)
Epoch: [10][64/391]	Time 0.459 (0.466)	Data 0.003 (0.005)	Loss 0.9016 (0.8968)	Acc@1 78.906 (79.471)	Acc@5 96.875 (98.978)
Epoch: [10][128/391]	Time 0.445 (0.463)	Data 0.002 (0.004)	Loss 0.8865 (0.8943)	Acc@1 75.000 (79.542)	Acc@5 100.000 (98.898)
Epoch: [10][192/391]	Time 0.405 (0.463)	Data 0.002 (0.003)	Loss 0.9405 (0.8963)	Acc@1 76.562 (79.437)	Acc@5 97.656 (98.871)
Epoch: [10][256/391]	Time 0.461 (0.463)	Data 0.002 (0.003)	Loss 1.0406 (0.9002)	Acc@1 75.000 (79.265)	Acc@5 99.219 (98.869)
Epoch: [10][320/391]	Time 0.459 (0.464)	Data 0.003 (0.003)	Loss 1.0004 (0.9000)	Acc@1 73.438 (79.247)	Acc@5 99.219 (98.871)
Epoch: [10][384/391]	Time 0.431 (0.463)	Data 0.002 (0.003)	Loss 0.8727 (0.9003)	Acc@1 81.250 (79.263)	Acc@5 98.438 (98.851)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 459400 ; 487386 ; 0.9425793929247045

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.499 (0.499)	Data 0.202 (0.202)	Loss 0.8815 (0.8815)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [11][64/391]	Time 0.436 (0.460)	Data 0.002 (0.005)	Loss 0.8913 (0.8522)	Acc@1 82.031 (80.974)	Acc@5 99.219 (98.906)
Epoch: [11][128/391]	Time 0.510 (0.462)	Data 0.002 (0.003)	Loss 0.9031 (0.8820)	Acc@1 79.688 (79.899)	Acc@5 100.000 (98.934)
Epoch: [11][192/391]	Time 0.467 (0.462)	Data 0.002 (0.003)	Loss 0.8841 (0.8903)	Acc@1 75.781 (79.602)	Acc@5 100.000 (98.867)
Epoch: [11][256/391]	Time 0.485 (0.461)	Data 0.002 (0.003)	Loss 0.9433 (0.8897)	Acc@1 80.469 (79.532)	Acc@5 99.219 (98.845)
Epoch: [11][320/391]	Time 0.478 (0.461)	Data 0.001 (0.003)	Loss 0.8485 (0.8883)	Acc@1 82.031 (79.541)	Acc@5 98.438 (98.851)
Epoch: [11][384/391]	Time 0.473 (0.461)	Data 0.002 (0.002)	Loss 0.8970 (0.8840)	Acc@1 79.688 (79.746)	Acc@5 99.219 (98.849)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.475 (0.475)	Data 0.234 (0.234)	Loss 0.7728 (0.7728)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [12][64/391]	Time 0.438 (0.471)	Data 0.002 (0.006)	Loss 0.9901 (0.8662)	Acc@1 77.344 (79.796)	Acc@5 96.875 (98.894)
Epoch: [12][128/391]	Time 0.473 (0.463)	Data 0.002 (0.004)	Loss 1.0129 (0.8761)	Acc@1 73.438 (79.669)	Acc@5 97.656 (98.910)
Epoch: [12][192/391]	Time 0.435 (0.464)	Data 0.002 (0.003)	Loss 0.8323 (0.8750)	Acc@1 85.156 (79.866)	Acc@5 100.000 (98.895)
Epoch: [12][256/391]	Time 0.450 (0.464)	Data 0.002 (0.003)	Loss 1.0334 (0.8796)	Acc@1 78.125 (79.794)	Acc@5 96.875 (98.851)
Epoch: [12][320/391]	Time 0.439 (0.463)	Data 0.002 (0.003)	Loss 1.0383 (0.8861)	Acc@1 75.781 (79.505)	Acc@5 99.219 (98.837)
Epoch: [12][384/391]	Time 0.495 (0.463)	Data 0.002 (0.003)	Loss 0.8093 (0.8851)	Acc@1 83.594 (79.533)	Acc@5 99.219 (98.874)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.463 (0.463)	Data 0.198 (0.198)	Loss 0.8581 (0.8581)	Acc@1 78.125 (78.125)	Acc@5 99.219 (99.219)
Epoch: [13][64/391]	Time 0.506 (0.463)	Data 0.002 (0.005)	Loss 0.9431 (0.8580)	Acc@1 75.000 (80.553)	Acc@5 99.219 (99.002)
Epoch: [13][128/391]	Time 0.463 (0.459)	Data 0.002 (0.003)	Loss 0.7878 (0.8623)	Acc@1 81.250 (80.378)	Acc@5 99.219 (98.970)
Epoch: [13][192/391]	Time 0.448 (0.460)	Data 0.002 (0.003)	Loss 0.9922 (0.8709)	Acc@1 80.469 (80.084)	Acc@5 96.875 (99.004)
Epoch: [13][256/391]	Time 0.456 (0.461)	Data 0.002 (0.003)	Loss 0.7005 (0.8681)	Acc@1 85.156 (80.128)	Acc@5 100.000 (99.000)
Epoch: [13][320/391]	Time 0.486 (0.460)	Data 0.002 (0.003)	Loss 0.8133 (0.8659)	Acc@1 81.250 (80.332)	Acc@5 98.438 (98.985)
Epoch: [13][384/391]	Time 0.477 (0.460)	Data 0.002 (0.002)	Loss 0.8542 (0.8687)	Acc@1 78.906 (80.221)	Acc@5 99.219 (98.973)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.491 (0.491)	Data 0.235 (0.235)	Loss 0.8383 (0.8383)	Acc@1 82.812 (82.812)	Acc@5 98.438 (98.438)
Epoch: [14][64/391]	Time 0.472 (0.464)	Data 0.002 (0.006)	Loss 0.8215 (0.8748)	Acc@1 83.594 (80.240)	Acc@5 97.656 (98.714)
Epoch: [14][128/391]	Time 0.480 (0.464)	Data 0.002 (0.004)	Loss 0.8368 (0.8694)	Acc@1 82.031 (80.287)	Acc@5 98.438 (98.819)
Epoch: [14][192/391]	Time 0.465 (0.462)	Data 0.002 (0.003)	Loss 0.8657 (0.8715)	Acc@1 82.031 (80.202)	Acc@5 99.219 (98.826)
Epoch: [14][256/391]	Time 0.443 (0.461)	Data 0.002 (0.003)	Loss 1.0298 (0.8656)	Acc@1 74.219 (80.441)	Acc@5 99.219 (98.881)
Epoch: [14][320/391]	Time 0.468 (0.461)	Data 0.002 (0.003)	Loss 0.8288 (0.8706)	Acc@1 81.250 (80.298)	Acc@5 99.219 (98.863)
Epoch: [14][384/391]	Time 0.455 (0.460)	Data 0.002 (0.003)	Loss 0.8215 (0.8691)	Acc@1 79.688 (80.280)	Acc@5 99.219 (98.872)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.481 (0.481)	Data 0.244 (0.244)	Loss 0.7876 (0.7876)	Acc@1 82.031 (82.031)	Acc@5 100.000 (100.000)
Epoch: [15][64/391]	Time 0.457 (0.469)	Data 0.002 (0.006)	Loss 0.8309 (0.8579)	Acc@1 77.344 (80.733)	Acc@5 99.219 (99.062)
Epoch: [15][128/391]	Time 0.453 (0.463)	Data 0.002 (0.004)	Loss 0.7023 (0.8568)	Acc@1 89.844 (80.608)	Acc@5 99.219 (99.037)
Epoch: [15][192/391]	Time 0.467 (0.460)	Data 0.002 (0.003)	Loss 0.9470 (0.8536)	Acc@1 72.656 (80.752)	Acc@5 100.000 (98.988)
Epoch: [15][256/391]	Time 0.457 (0.460)	Data 0.002 (0.003)	Loss 0.8819 (0.8585)	Acc@1 81.250 (80.624)	Acc@5 97.656 (98.951)
Epoch: [15][320/391]	Time 0.476 (0.461)	Data 0.002 (0.003)	Loss 0.9886 (0.8606)	Acc@1 75.781 (80.544)	Acc@5 97.656 (98.919)
Epoch: [15][384/391]	Time 0.428 (0.460)	Data 0.002 (0.003)	Loss 0.8592 (0.8617)	Acc@1 75.781 (80.526)	Acc@5 99.219 (98.935)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 380604 ; 487386 ; 0.7809087663576714

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.435 (0.435)	Data 0.219 (0.219)	Loss 0.8580 (0.8580)	Acc@1 78.125 (78.125)	Acc@5 100.000 (100.000)
Epoch: [16][64/391]	Time 0.487 (0.454)	Data 0.002 (0.005)	Loss 0.7835 (0.8251)	Acc@1 82.812 (81.611)	Acc@5 100.000 (99.038)
Epoch: [16][128/391]	Time 0.486 (0.456)	Data 0.002 (0.004)	Loss 0.8962 (0.8446)	Acc@1 80.469 (80.953)	Acc@5 100.000 (98.946)
Epoch: [16][192/391]	Time 0.511 (0.457)	Data 0.002 (0.003)	Loss 0.8724 (0.8446)	Acc@1 83.594 (80.914)	Acc@5 98.438 (98.992)
Epoch: [16][256/391]	Time 0.477 (0.456)	Data 0.002 (0.003)	Loss 0.9098 (0.8462)	Acc@1 75.781 (80.770)	Acc@5 97.656 (98.973)
Epoch: [16][320/391]	Time 0.472 (0.455)	Data 0.002 (0.003)	Loss 0.9196 (0.8513)	Acc@1 81.250 (80.690)	Acc@5 98.438 (98.946)
Epoch: [16][384/391]	Time 0.459 (0.455)	Data 0.002 (0.003)	Loss 0.8511 (0.8513)	Acc@1 78.125 (80.657)	Acc@5 99.219 (98.929)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.477 (0.477)	Data 0.254 (0.254)	Loss 0.8690 (0.8690)	Acc@1 78.906 (78.906)	Acc@5 100.000 (100.000)
Epoch: [17][64/391]	Time 0.500 (0.459)	Data 0.002 (0.006)	Loss 0.9043 (0.8440)	Acc@1 79.688 (80.709)	Acc@5 96.875 (98.942)
Epoch: [17][128/391]	Time 0.450 (0.454)	Data 0.003 (0.004)	Loss 0.8275 (0.8479)	Acc@1 81.250 (80.850)	Acc@5 100.000 (98.880)
Epoch: [17][192/391]	Time 0.459 (0.453)	Data 0.002 (0.003)	Loss 0.8870 (0.8442)	Acc@1 79.688 (80.878)	Acc@5 98.438 (98.931)
Epoch: [17][256/391]	Time 0.431 (0.455)	Data 0.002 (0.003)	Loss 0.8779 (0.8473)	Acc@1 80.469 (80.736)	Acc@5 99.219 (98.951)
Epoch: [17][320/391]	Time 0.506 (0.455)	Data 0.002 (0.003)	Loss 0.9190 (0.8460)	Acc@1 77.344 (80.754)	Acc@5 99.219 (98.971)
Epoch: [17][384/391]	Time 0.448 (0.455)	Data 0.002 (0.003)	Loss 0.9596 (0.8472)	Acc@1 75.000 (80.674)	Acc@5 99.219 (98.996)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.443 (0.443)	Data 0.210 (0.210)	Loss 0.8261 (0.8261)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [18][64/391]	Time 0.448 (0.458)	Data 0.002 (0.005)	Loss 0.7939 (0.8335)	Acc@1 81.250 (81.562)	Acc@5 98.438 (99.038)
Epoch: [18][128/391]	Time 0.427 (0.455)	Data 0.002 (0.004)	Loss 0.7178 (0.8380)	Acc@1 88.281 (81.244)	Acc@5 100.000 (99.007)
Epoch: [18][192/391]	Time 0.441 (0.456)	Data 0.002 (0.003)	Loss 0.8988 (0.8482)	Acc@1 79.688 (80.687)	Acc@5 97.656 (98.919)
Epoch: [18][256/391]	Time 0.489 (0.456)	Data 0.002 (0.003)	Loss 0.8818 (0.8443)	Acc@1 76.562 (80.840)	Acc@5 99.219 (98.976)
Epoch: [18][320/391]	Time 0.453 (0.455)	Data 0.002 (0.003)	Loss 0.8167 (0.8461)	Acc@1 80.469 (80.897)	Acc@5 100.000 (98.939)
Epoch: [18][384/391]	Time 0.400 (0.455)	Data 0.002 (0.003)	Loss 0.7575 (0.8438)	Acc@1 82.031 (80.986)	Acc@5 98.438 (98.955)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.492 (0.492)	Data 0.203 (0.203)	Loss 0.8606 (0.8606)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [19][64/391]	Time 0.455 (0.457)	Data 0.002 (0.005)	Loss 0.8516 (0.8484)	Acc@1 85.156 (80.998)	Acc@5 98.438 (98.942)
Epoch: [19][128/391]	Time 0.492 (0.455)	Data 0.002 (0.004)	Loss 0.7508 (0.8497)	Acc@1 84.375 (80.929)	Acc@5 100.000 (98.958)
Epoch: [19][192/391]	Time 0.479 (0.454)	Data 0.002 (0.003)	Loss 0.7315 (0.8442)	Acc@1 83.594 (81.003)	Acc@5 100.000 (99.000)
Epoch: [19][256/391]	Time 0.486 (0.454)	Data 0.001 (0.003)	Loss 1.0645 (0.8406)	Acc@1 71.875 (81.022)	Acc@5 99.219 (99.061)
Epoch: [19][320/391]	Time 0.444 (0.455)	Data 0.002 (0.003)	Loss 0.8683 (0.8398)	Acc@1 82.031 (81.050)	Acc@5 98.438 (99.063)
Epoch: [19][384/391]	Time 0.449 (0.454)	Data 0.001 (0.003)	Loss 0.7428 (0.8355)	Acc@1 84.375 (81.234)	Acc@5 99.219 (99.052)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.486 (0.486)	Data 0.241 (0.241)	Loss 0.9487 (0.9487)	Acc@1 75.000 (75.000)	Acc@5 99.219 (99.219)
Epoch: [20][64/391]	Time 0.441 (0.455)	Data 0.002 (0.006)	Loss 0.8863 (0.8261)	Acc@1 78.906 (80.733)	Acc@5 99.219 (99.219)
Epoch: [20][128/391]	Time 0.445 (0.455)	Data 0.002 (0.004)	Loss 0.8754 (0.8345)	Acc@1 77.344 (80.656)	Acc@5 99.219 (99.079)
Epoch: [20][192/391]	Time 0.507 (0.456)	Data 0.002 (0.003)	Loss 0.8103 (0.8395)	Acc@1 81.250 (80.655)	Acc@5 97.656 (99.024)
Epoch: [20][256/391]	Time 0.444 (0.454)	Data 0.002 (0.003)	Loss 0.7848 (0.8409)	Acc@1 81.250 (80.627)	Acc@5 100.000 (99.036)
Epoch: [20][320/391]	Time 0.478 (0.454)	Data 0.002 (0.003)	Loss 0.8864 (0.8381)	Acc@1 80.469 (80.802)	Acc@5 98.438 (99.007)
Epoch: [20][384/391]	Time 0.474 (0.453)	Data 0.002 (0.003)	Loss 0.8117 (0.8370)	Acc@1 81.250 (80.787)	Acc@5 99.219 (99.000)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 343062 ; 487386 ; 0.703881523063855

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.462 (0.462)	Data 0.196 (0.196)	Loss 1.0092 (1.0092)	Acc@1 72.656 (72.656)	Acc@5 96.875 (96.875)
Epoch: [21][64/391]	Time 0.472 (0.463)	Data 0.002 (0.005)	Loss 0.9005 (0.7983)	Acc@1 82.031 (81.959)	Acc@5 96.875 (99.111)
Epoch: [21][128/391]	Time 0.502 (0.458)	Data 0.002 (0.003)	Loss 0.8473 (0.8143)	Acc@1 80.469 (81.274)	Acc@5 99.219 (99.098)
Epoch: [21][192/391]	Time 0.477 (0.458)	Data 0.002 (0.003)	Loss 0.8416 (0.8179)	Acc@1 83.594 (81.299)	Acc@5 97.656 (99.053)
Epoch: [21][256/391]	Time 0.408 (0.455)	Data 0.002 (0.003)	Loss 0.8296 (0.8167)	Acc@1 82.031 (81.201)	Acc@5 99.219 (99.088)
Epoch: [21][320/391]	Time 0.469 (0.454)	Data 0.001 (0.003)	Loss 0.6579 (0.8154)	Acc@1 88.281 (81.223)	Acc@5 100.000 (99.087)
Epoch: [21][384/391]	Time 0.430 (0.454)	Data 0.002 (0.002)	Loss 0.8185 (0.8152)	Acc@1 78.906 (81.234)	Acc@5 99.219 (99.077)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.466 (0.466)	Data 0.262 (0.262)	Loss 0.6725 (0.6725)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [22][64/391]	Time 0.496 (0.462)	Data 0.002 (0.006)	Loss 0.7296 (0.8101)	Acc@1 83.594 (81.599)	Acc@5 100.000 (99.026)
Epoch: [22][128/391]	Time 0.477 (0.461)	Data 0.002 (0.004)	Loss 0.7381 (0.8125)	Acc@1 83.594 (81.571)	Acc@5 98.438 (99.122)
Epoch: [22][192/391]	Time 0.491 (0.459)	Data 0.002 (0.003)	Loss 0.8271 (0.8081)	Acc@1 82.031 (81.655)	Acc@5 98.438 (99.118)
Epoch: [22][256/391]	Time 0.430 (0.458)	Data 0.002 (0.003)	Loss 0.9745 (0.8073)	Acc@1 78.906 (81.645)	Acc@5 96.875 (99.070)
Epoch: [22][320/391]	Time 0.461 (0.459)	Data 0.001 (0.003)	Loss 0.9247 (0.8128)	Acc@1 78.125 (81.508)	Acc@5 99.219 (99.041)
Epoch: [22][384/391]	Time 0.416 (0.458)	Data 0.002 (0.003)	Loss 0.7210 (0.8132)	Acc@1 84.375 (81.520)	Acc@5 100.000 (99.040)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.476 (0.476)	Data 0.215 (0.215)	Loss 0.6972 (0.6972)	Acc@1 85.156 (85.156)	Acc@5 98.438 (98.438)
Epoch: [23][64/391]	Time 0.416 (0.444)	Data 0.002 (0.005)	Loss 0.8224 (0.8111)	Acc@1 80.469 (81.334)	Acc@5 99.219 (99.195)
Epoch: [23][128/391]	Time 0.414 (0.418)	Data 0.002 (0.004)	Loss 0.6728 (0.8025)	Acc@1 89.062 (81.771)	Acc@5 100.000 (99.134)
Epoch: [23][192/391]	Time 0.379 (0.410)	Data 0.002 (0.003)	Loss 0.7638 (0.7968)	Acc@1 83.594 (82.035)	Acc@5 98.438 (99.134)
Epoch: [23][256/391]	Time 0.369 (0.405)	Data 0.002 (0.003)	Loss 0.8767 (0.8005)	Acc@1 78.906 (81.897)	Acc@5 96.875 (99.088)
Epoch: [23][320/391]	Time 0.427 (0.403)	Data 0.002 (0.003)	Loss 0.8599 (0.8045)	Acc@1 78.906 (81.751)	Acc@5 98.438 (99.048)
Epoch: [23][384/391]	Time 0.390 (0.402)	Data 0.002 (0.003)	Loss 0.7614 (0.8083)	Acc@1 82.031 (81.597)	Acc@5 99.219 (99.052)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.394 (0.394)	Data 0.246 (0.246)	Loss 0.7052 (0.7052)	Acc@1 82.031 (82.031)	Acc@5 100.000 (100.000)
Epoch: [24][64/391]	Time 0.420 (0.396)	Data 0.002 (0.006)	Loss 0.8061 (0.7998)	Acc@1 82.812 (81.887)	Acc@5 99.219 (99.147)
Epoch: [24][128/391]	Time 0.369 (0.395)	Data 0.002 (0.004)	Loss 0.8047 (0.8053)	Acc@1 79.688 (81.420)	Acc@5 100.000 (99.152)
Epoch: [24][192/391]	Time 0.432 (0.394)	Data 0.002 (0.003)	Loss 0.7407 (0.7969)	Acc@1 84.375 (81.890)	Acc@5 100.000 (99.162)
Epoch: [24][256/391]	Time 0.401 (0.394)	Data 0.002 (0.003)	Loss 0.7479 (0.7991)	Acc@1 84.375 (81.821)	Acc@5 99.219 (99.128)
Epoch: [24][320/391]	Time 0.382 (0.396)	Data 0.002 (0.003)	Loss 0.7904 (0.8060)	Acc@1 81.250 (81.644)	Acc@5 100.000 (99.112)
Epoch: [24][384/391]	Time 0.393 (0.395)	Data 0.002 (0.003)	Loss 0.6373 (0.8067)	Acc@1 86.719 (81.558)	Acc@5 100.000 (99.091)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.404 (0.404)	Data 0.220 (0.220)	Loss 0.7428 (0.7428)	Acc@1 82.031 (82.031)	Acc@5 100.000 (100.000)
Epoch: [25][64/391]	Time 0.387 (0.394)	Data 0.002 (0.005)	Loss 0.8378 (0.7887)	Acc@1 82.812 (81.719)	Acc@5 100.000 (99.087)
Epoch: [25][128/391]	Time 0.404 (0.394)	Data 0.002 (0.004)	Loss 0.8413 (0.7932)	Acc@1 78.906 (81.868)	Acc@5 99.219 (99.092)
Epoch: [25][192/391]	Time 0.401 (0.395)	Data 0.002 (0.003)	Loss 0.8656 (0.8009)	Acc@1 81.250 (81.639)	Acc@5 98.438 (99.057)
Epoch: [25][256/391]	Time 0.360 (0.394)	Data 0.002 (0.003)	Loss 0.8111 (0.8052)	Acc@1 78.125 (81.487)	Acc@5 98.438 (99.045)
Epoch: [25][320/391]	Time 0.416 (0.394)	Data 0.002 (0.003)	Loss 0.7706 (0.8062)	Acc@1 82.812 (81.389)	Acc@5 98.438 (99.000)
Epoch: [25][384/391]	Time 0.401 (0.395)	Data 0.003 (0.002)	Loss 0.7742 (0.8076)	Acc@1 82.812 (81.420)	Acc@5 98.438 (99.024)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv30.weight

 RM:  module.conv31.weight

Module List Length:  68
Index1: 60
Index: 31
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 32; k: 32
Bool1: False
Bool2: True
j: 33; k: 32
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 58: Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 59: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 60: Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 61: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 62 gegen 66: Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 63 gegen 67: BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  64

Module List Length:  64
Index1: 58
Index: 30
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 31; k: 30
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 58 gegen 62: Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 59 gegen 63: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(9, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(36, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): AdaptiveAvgPool2d(output_size=(1, 1))
    (59): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  60
Count: 252484 ; 487386 ; 0.5180370384048782

Epoch: [26 | 180] LR: 0.100000
Epoch: [26][0/391]	Time 0.393 (0.393)	Data 0.240 (0.240)	Loss 1.9856 (1.9856)	Acc@1 45.312 (45.312)	Acc@5 90.625 (90.625)
Epoch: [26][64/391]	Time 0.327 (0.369)	Data 0.002 (0.006)	Loss 0.8274 (1.1282)	Acc@1 78.906 (71.370)	Acc@5 98.438 (97.716)
Epoch: [26][128/391]	Time 0.390 (0.372)	Data 0.002 (0.004)	Loss 0.7020 (0.9794)	Acc@1 85.156 (75.648)	Acc@5 98.438 (98.268)
Epoch: [26][192/391]	Time 0.359 (0.369)	Data 0.002 (0.003)	Loss 0.9687 (0.9291)	Acc@1 75.000 (77.145)	Acc@5 99.219 (98.506)
Epoch: [26][256/391]	Time 0.353 (0.369)	Data 0.002 (0.003)	Loss 0.8103 (0.9026)	Acc@1 77.344 (77.812)	Acc@5 99.219 (98.647)
Epoch: [26][320/391]	Time 0.418 (0.370)	Data 0.002 (0.003)	Loss 0.8431 (0.8858)	Acc@1 79.688 (78.368)	Acc@5 96.094 (98.652)
Epoch: [26][384/391]	Time 0.428 (0.369)	Data 0.002 (0.003)	Loss 0.7921 (0.8758)	Acc@1 79.688 (78.669)	Acc@5 97.656 (98.677)

Epoch: [27 | 180] LR: 0.100000
Epoch: [27][0/391]	Time 0.376 (0.376)	Data 0.253 (0.253)	Loss 0.7452 (0.7452)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [27][64/391]	Time 0.376 (0.368)	Data 0.003 (0.006)	Loss 0.8097 (0.7875)	Acc@1 81.250 (81.863)	Acc@5 99.219 (99.159)
Epoch: [27][128/391]	Time 0.397 (0.367)	Data 0.003 (0.004)	Loss 0.7164 (0.7966)	Acc@1 82.812 (81.147)	Acc@5 99.219 (99.049)
Epoch: [27][192/391]	Time 0.380 (0.366)	Data 0.002 (0.003)	Loss 0.9321 (0.8063)	Acc@1 75.781 (80.740)	Acc@5 99.219 (98.972)
Epoch: [27][256/391]	Time 0.395 (0.366)	Data 0.002 (0.003)	Loss 0.8515 (0.8053)	Acc@1 80.469 (80.867)	Acc@5 99.219 (98.973)
Epoch: [27][320/391]	Time 0.363 (0.366)	Data 0.003 (0.003)	Loss 0.8058 (0.8047)	Acc@1 82.031 (80.868)	Acc@5 98.438 (98.946)
Epoch: [27][384/391]	Time 0.349 (0.367)	Data 0.002 (0.003)	Loss 0.7723 (0.8011)	Acc@1 81.250 (81.027)	Acc@5 96.875 (98.961)

Epoch: [28 | 180] LR: 0.100000
Epoch: [28][0/391]	Time 0.427 (0.427)	Data 0.225 (0.225)	Loss 0.8553 (0.8553)	Acc@1 78.125 (78.125)	Acc@5 99.219 (99.219)
Epoch: [28][64/391]	Time 0.391 (0.369)	Data 0.002 (0.005)	Loss 0.7206 (0.7917)	Acc@1 86.719 (81.550)	Acc@5 100.000 (99.087)
Epoch: [28][128/391]	Time 0.338 (0.366)	Data 0.002 (0.004)	Loss 0.8900 (0.8035)	Acc@1 74.219 (81.074)	Acc@5 100.000 (99.031)
Epoch: [28][192/391]	Time 0.368 (0.367)	Data 0.002 (0.003)	Loss 0.9673 (0.8095)	Acc@1 72.656 (80.768)	Acc@5 99.219 (98.984)
Epoch: [28][256/391]	Time 0.345 (0.366)	Data 0.002 (0.003)	Loss 0.8986 (0.8135)	Acc@1 75.781 (80.727)	Acc@5 99.219 (98.933)
Epoch: [28][320/391]	Time 0.345 (0.365)	Data 0.002 (0.003)	Loss 0.8544 (0.8082)	Acc@1 75.781 (80.790)	Acc@5 99.219 (98.961)
Epoch: [28][384/391]	Time 0.377 (0.366)	Data 0.002 (0.003)	Loss 0.7607 (0.8020)	Acc@1 83.594 (81.009)	Acc@5 99.219 (98.979)

Epoch: [29 | 180] LR: 0.100000
Epoch: [29][0/391]	Time 0.356 (0.356)	Data 0.233 (0.233)	Loss 0.7734 (0.7734)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [29][64/391]	Time 0.354 (0.366)	Data 0.001 (0.006)	Loss 0.9455 (0.7662)	Acc@1 74.219 (82.344)	Acc@5 97.656 (99.159)
Epoch: [29][128/391]	Time 0.394 (0.367)	Data 0.002 (0.004)	Loss 0.8004 (0.7855)	Acc@1 82.031 (81.650)	Acc@5 96.875 (99.067)
Epoch: [29][192/391]	Time 0.367 (0.368)	Data 0.002 (0.003)	Loss 0.8317 (0.7880)	Acc@1 83.594 (81.501)	Acc@5 99.219 (99.037)
Epoch: [29][256/391]	Time 0.372 (0.366)	Data 0.001 (0.003)	Loss 0.8102 (0.7882)	Acc@1 78.906 (81.487)	Acc@5 99.219 (99.039)
Epoch: [29][320/391]	Time 0.324 (0.365)	Data 0.002 (0.003)	Loss 0.8836 (0.7887)	Acc@1 79.688 (81.537)	Acc@5 99.219 (99.036)
Epoch: [29][384/391]	Time 0.313 (0.365)	Data 0.002 (0.003)	Loss 0.7521 (0.7911)	Acc@1 82.812 (81.406)	Acc@5 99.219 (99.036)

Epoch: [30 | 180] LR: 0.100000
Epoch: [30][0/391]	Time 0.397 (0.397)	Data 0.205 (0.205)	Loss 0.6662 (0.6662)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [30][64/391]	Time 0.390 (0.376)	Data 0.002 (0.005)	Loss 0.6958 (0.7659)	Acc@1 83.594 (82.127)	Acc@5 100.000 (98.954)
Epoch: [30][128/391]	Time 0.396 (0.375)	Data 0.002 (0.004)	Loss 0.7567 (0.7875)	Acc@1 80.469 (81.535)	Acc@5 99.219 (98.922)
Epoch: [30][192/391]	Time 0.391 (0.373)	Data 0.002 (0.003)	Loss 0.7669 (0.7821)	Acc@1 83.594 (81.756)	Acc@5 100.000 (99.012)
Epoch: [30][256/391]	Time 0.370 (0.371)	Data 0.002 (0.003)	Loss 0.6834 (0.7881)	Acc@1 87.500 (81.627)	Acc@5 100.000 (98.991)
Epoch: [30][320/391]	Time 0.376 (0.369)	Data 0.002 (0.003)	Loss 0.8841 (0.7947)	Acc@1 75.781 (81.342)	Acc@5 100.000 (98.995)
Epoch: [30][384/391]	Time 0.358 (0.369)	Data 0.001 (0.002)	Loss 0.7400 (0.7960)	Acc@1 82.812 (81.335)	Acc@5 100.000 (98.973)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.508 (0.508)	Data 0.191 (0.191)	Loss 3.1972 (3.1972)	Acc@1 10.156 (10.156)	Acc@5 46.875 (46.875)
Epoch: [1][64/391]	Time 0.389 (0.393)	Data 0.001 (0.005)	Loss 2.6040 (2.7863)	Acc@1 29.688 (22.404)	Acc@5 80.469 (74.363)
Epoch: [1][128/391]	Time 0.434 (0.393)	Data 0.001 (0.003)	Loss 2.4887 (2.6041)	Acc@1 32.812 (27.204)	Acc@5 81.250 (80.239)
Epoch: [1][192/391]	Time 0.369 (0.395)	Data 0.002 (0.003)	Loss 2.1281 (2.4932)	Acc@1 42.188 (30.400)	Acc@5 92.969 (83.173)
Epoch: [1][256/391]	Time 0.411 (0.395)	Data 0.002 (0.003)	Loss 2.0713 (2.4045)	Acc@1 45.312 (33.192)	Acc@5 91.406 (85.026)
Epoch: [1][320/391]	Time 0.407 (0.394)	Data 0.002 (0.002)	Loss 2.0295 (2.3211)	Acc@1 50.781 (35.891)	Acc@5 89.062 (86.561)
Epoch: [1][384/391]	Time 0.405 (0.395)	Data 0.001 (0.002)	Loss 1.8672 (2.2533)	Acc@1 53.125 (38.147)	Acc@5 91.406 (87.630)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.425 (0.425)	Data 0.231 (0.231)	Loss 1.8135 (1.8135)	Acc@1 50.781 (50.781)	Acc@5 96.094 (96.094)
Epoch: [2][64/391]	Time 0.413 (0.407)	Data 0.002 (0.005)	Loss 1.6091 (1.7811)	Acc@1 58.594 (52.837)	Acc@5 95.312 (94.531)
Epoch: [2][128/391]	Time 0.374 (0.402)	Data 0.002 (0.004)	Loss 1.7343 (1.7377)	Acc@1 57.031 (54.542)	Acc@5 93.750 (94.731)
Epoch: [2][192/391]	Time 0.399 (0.400)	Data 0.001 (0.003)	Loss 1.6115 (1.6952)	Acc@1 60.156 (55.922)	Acc@5 96.094 (95.029)
Epoch: [2][256/391]	Time 0.419 (0.399)	Data 0.002 (0.003)	Loss 1.5084 (1.6540)	Acc@1 65.625 (57.484)	Acc@5 94.531 (95.279)
Epoch: [2][320/391]	Time 0.402 (0.399)	Data 0.002 (0.003)	Loss 1.4911 (1.6153)	Acc@1 62.500 (58.771)	Acc@5 94.531 (95.466)
Epoch: [2][384/391]	Time 0.406 (0.398)	Data 0.002 (0.002)	Loss 1.5232 (1.5816)	Acc@1 61.719 (59.830)	Acc@5 96.094 (95.704)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.444 (0.444)	Data 0.209 (0.209)	Loss 1.2666 (1.2666)	Acc@1 73.438 (73.438)	Acc@5 99.219 (99.219)
Epoch: [3][64/391]	Time 0.367 (0.401)	Data 0.002 (0.005)	Loss 1.4745 (1.3750)	Acc@1 61.719 (65.853)	Acc@5 96.875 (97.067)
Epoch: [3][128/391]	Time 0.438 (0.403)	Data 0.001 (0.004)	Loss 1.1607 (1.3392)	Acc@1 73.438 (67.406)	Acc@5 96.875 (97.111)
Epoch: [3][192/391]	Time 0.382 (0.403)	Data 0.002 (0.003)	Loss 1.1719 (1.3276)	Acc@1 71.875 (67.698)	Acc@5 100.000 (97.203)
Epoch: [3][256/391]	Time 0.424 (0.402)	Data 0.002 (0.003)	Loss 1.2133 (1.3082)	Acc@1 70.312 (68.282)	Acc@5 99.219 (97.307)
Epoch: [3][320/391]	Time 0.374 (0.401)	Data 0.002 (0.003)	Loss 1.1958 (1.2874)	Acc@1 67.188 (68.864)	Acc@5 97.656 (97.408)
Epoch: [3][384/391]	Time 0.385 (0.400)	Data 0.001 (0.002)	Loss 1.1761 (1.2714)	Acc@1 72.656 (69.172)	Acc@5 98.438 (97.463)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.437 (0.437)	Data 0.190 (0.190)	Loss 1.0499 (1.0499)	Acc@1 74.219 (74.219)	Acc@5 100.000 (100.000)
Epoch: [4][64/391]	Time 0.414 (0.407)	Data 0.002 (0.005)	Loss 1.3022 (1.1578)	Acc@1 66.406 (71.983)	Acc@5 96.875 (97.909)
Epoch: [4][128/391]	Time 0.437 (0.405)	Data 0.002 (0.003)	Loss 1.1750 (1.1358)	Acc@1 77.344 (72.959)	Acc@5 96.875 (97.965)
Epoch: [4][192/391]	Time 0.427 (0.403)	Data 0.002 (0.003)	Loss 1.1413 (1.1351)	Acc@1 73.438 (72.830)	Acc@5 99.219 (97.871)
Epoch: [4][256/391]	Time 0.396 (0.401)	Data 0.001 (0.003)	Loss 1.0731 (1.1302)	Acc@1 72.656 (72.936)	Acc@5 99.219 (97.915)
Epoch: [4][320/391]	Time 0.397 (0.400)	Data 0.002 (0.002)	Loss 1.1268 (1.1225)	Acc@1 73.438 (73.048)	Acc@5 96.094 (97.943)
Epoch: [4][384/391]	Time 0.430 (0.400)	Data 0.002 (0.002)	Loss 1.0980 (1.1123)	Acc@1 71.875 (73.346)	Acc@5 99.219 (97.995)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.471 (0.471)	Data 0.212 (0.212)	Loss 1.0556 (1.0556)	Acc@1 69.531 (69.531)	Acc@5 97.656 (97.656)
Epoch: [5][64/391]	Time 0.420 (0.403)	Data 0.002 (0.005)	Loss 0.9801 (1.0365)	Acc@1 76.562 (75.264)	Acc@5 99.219 (98.389)
Epoch: [5][128/391]	Time 0.384 (0.405)	Data 0.002 (0.004)	Loss 1.1404 (1.0413)	Acc@1 68.750 (74.982)	Acc@5 100.000 (98.316)
Epoch: [5][192/391]	Time 0.404 (0.405)	Data 0.002 (0.003)	Loss 0.9959 (1.0431)	Acc@1 74.219 (74.705)	Acc@5 100.000 (98.381)
Epoch: [5][256/391]	Time 0.417 (0.403)	Data 0.002 (0.003)	Loss 0.9763 (1.0426)	Acc@1 80.469 (74.626)	Acc@5 97.656 (98.407)
Epoch: [5][320/391]	Time 0.387 (0.402)	Data 0.003 (0.003)	Loss 1.1236 (1.0431)	Acc@1 70.312 (74.674)	Acc@5 97.656 (98.350)
Epoch: [5][384/391]	Time 0.403 (0.402)	Data 0.002 (0.002)	Loss 1.0224 (1.0378)	Acc@1 76.562 (74.763)	Acc@5 98.438 (98.369)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.416 (0.416)	Data 0.240 (0.240)	Loss 1.0302 (1.0302)	Acc@1 73.438 (73.438)	Acc@5 97.656 (97.656)
Epoch: [6][64/391]	Time 0.456 (0.404)	Data 0.002 (0.006)	Loss 0.9546 (0.9743)	Acc@1 74.219 (76.959)	Acc@5 100.000 (98.486)
Epoch: [6][128/391]	Time 0.369 (0.404)	Data 0.002 (0.004)	Loss 0.9956 (0.9955)	Acc@1 75.781 (76.193)	Acc@5 96.875 (98.395)
Epoch: [6][192/391]	Time 0.380 (0.404)	Data 0.002 (0.003)	Loss 1.0567 (0.9962)	Acc@1 74.219 (76.101)	Acc@5 99.219 (98.328)
Epoch: [6][256/391]	Time 0.411 (0.401)	Data 0.002 (0.003)	Loss 1.0056 (0.9915)	Acc@1 75.781 (76.310)	Acc@5 99.219 (98.371)
Epoch: [6][320/391]	Time 0.377 (0.402)	Data 0.002 (0.003)	Loss 1.0801 (0.9922)	Acc@1 73.438 (76.232)	Acc@5 99.219 (98.411)
Epoch: [6][384/391]	Time 0.387 (0.401)	Data 0.001 (0.003)	Loss 0.8007 (0.9878)	Acc@1 78.125 (76.345)	Acc@5 99.219 (98.458)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.412 (0.412)	Data 0.191 (0.191)	Loss 0.9656 (0.9656)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [7][64/391]	Time 0.422 (0.397)	Data 0.002 (0.005)	Loss 0.9928 (0.9683)	Acc@1 72.656 (77.055)	Acc@5 98.438 (98.341)
Epoch: [7][128/391]	Time 0.395 (0.404)	Data 0.002 (0.003)	Loss 0.8027 (0.9614)	Acc@1 82.031 (77.059)	Acc@5 99.219 (98.571)
Epoch: [7][192/391]	Time 0.373 (0.403)	Data 0.002 (0.003)	Loss 0.8942 (0.9590)	Acc@1 81.250 (77.259)	Acc@5 99.219 (98.527)
Epoch: [7][256/391]	Time 0.421 (0.401)	Data 0.002 (0.003)	Loss 0.9188 (0.9572)	Acc@1 76.562 (77.402)	Acc@5 99.219 (98.532)
Epoch: [7][320/391]	Time 0.376 (0.400)	Data 0.002 (0.003)	Loss 0.8264 (0.9606)	Acc@1 81.250 (77.242)	Acc@5 100.000 (98.537)
Epoch: [7][384/391]	Time 0.386 (0.399)	Data 0.002 (0.002)	Loss 0.7844 (0.9576)	Acc@1 83.594 (77.281)	Acc@5 100.000 (98.555)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.451 (0.451)	Data 0.250 (0.250)	Loss 0.8094 (0.8094)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [8][64/391]	Time 0.465 (0.399)	Data 0.002 (0.006)	Loss 0.8416 (0.9494)	Acc@1 78.125 (77.392)	Acc@5 98.438 (98.642)
Epoch: [8][128/391]	Time 0.429 (0.403)	Data 0.002 (0.004)	Loss 0.9169 (0.9410)	Acc@1 82.031 (77.828)	Acc@5 100.000 (98.692)
Epoch: [8][192/391]	Time 0.401 (0.403)	Data 0.002 (0.003)	Loss 0.9411 (0.9419)	Acc@1 76.562 (77.890)	Acc@5 100.000 (98.640)
Epoch: [8][256/391]	Time 0.378 (0.401)	Data 0.002 (0.003)	Loss 1.0291 (0.9439)	Acc@1 75.781 (77.851)	Acc@5 96.875 (98.626)
Epoch: [8][320/391]	Time 0.383 (0.399)	Data 0.002 (0.003)	Loss 0.8686 (0.9475)	Acc@1 78.125 (77.736)	Acc@5 100.000 (98.598)
Epoch: [8][384/391]	Time 0.413 (0.398)	Data 0.002 (0.003)	Loss 0.9836 (0.9487)	Acc@1 75.781 (77.780)	Acc@5 97.656 (98.608)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.409 (0.409)	Data 0.221 (0.221)	Loss 0.9857 (0.9857)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [9][64/391]	Time 0.344 (0.394)	Data 0.002 (0.005)	Loss 0.8435 (0.9368)	Acc@1 82.812 (77.548)	Acc@5 99.219 (98.702)
Epoch: [9][128/391]	Time 0.412 (0.399)	Data 0.002 (0.004)	Loss 0.8897 (0.9370)	Acc@1 82.031 (77.780)	Acc@5 100.000 (98.783)
Epoch: [9][192/391]	Time 0.403 (0.399)	Data 0.002 (0.003)	Loss 0.9729 (0.9340)	Acc@1 74.219 (77.927)	Acc@5 99.219 (98.782)
Epoch: [9][256/391]	Time 0.408 (0.399)	Data 0.002 (0.003)	Loss 0.8941 (0.9336)	Acc@1 79.688 (77.958)	Acc@5 99.219 (98.760)
Epoch: [9][320/391]	Time 0.388 (0.397)	Data 0.002 (0.003)	Loss 0.9658 (0.9303)	Acc@1 79.688 (78.052)	Acc@5 99.219 (98.788)
Epoch: [9][384/391]	Time 0.409 (0.397)	Data 0.001 (0.002)	Loss 0.9680 (0.9267)	Acc@1 77.344 (78.135)	Acc@5 99.219 (98.801)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.438 (0.438)	Data 0.233 (0.233)	Loss 0.8784 (0.8784)	Acc@1 79.688 (79.688)	Acc@5 97.656 (97.656)
Epoch: [10][64/391]	Time 0.402 (0.398)	Data 0.002 (0.005)	Loss 0.9502 (0.9205)	Acc@1 76.562 (79.099)	Acc@5 99.219 (98.714)
Epoch: [10][128/391]	Time 0.400 (0.401)	Data 0.002 (0.004)	Loss 0.7819 (0.9186)	Acc@1 82.031 (78.894)	Acc@5 99.219 (98.752)
Epoch: [10][192/391]	Time 0.437 (0.401)	Data 0.002 (0.003)	Loss 0.9530 (0.9166)	Acc@1 79.688 (78.898)	Acc@5 97.656 (98.757)
Epoch: [10][256/391]	Time 0.386 (0.401)	Data 0.002 (0.003)	Loss 0.8855 (0.9186)	Acc@1 78.125 (78.809)	Acc@5 99.219 (98.729)
Epoch: [10][320/391]	Time 0.376 (0.400)	Data 0.001 (0.003)	Loss 0.9541 (0.9230)	Acc@1 80.469 (78.658)	Acc@5 100.000 (98.720)
Epoch: [10][384/391]	Time 0.338 (0.399)	Data 0.002 (0.003)	Loss 0.9353 (0.9225)	Acc@1 78.906 (78.691)	Acc@5 100.000 (98.736)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 482192 ; 487386 ; 0.9893431489620137

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.425 (0.425)	Data 0.202 (0.202)	Loss 0.8513 (0.8513)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [11][64/391]	Time 0.455 (0.403)	Data 0.002 (0.005)	Loss 0.8704 (0.8573)	Acc@1 82.031 (80.373)	Acc@5 100.000 (99.075)
Epoch: [11][128/391]	Time 0.382 (0.399)	Data 0.002 (0.003)	Loss 1.1041 (0.8898)	Acc@1 73.438 (79.548)	Acc@5 100.000 (99.025)
Epoch: [11][192/391]	Time 0.401 (0.400)	Data 0.002 (0.003)	Loss 0.7484 (0.9055)	Acc@1 87.500 (79.408)	Acc@5 100.000 (98.988)
Epoch: [11][256/391]	Time 0.402 (0.399)	Data 0.002 (0.003)	Loss 0.9951 (0.9115)	Acc@1 74.219 (79.177)	Acc@5 98.438 (98.930)
Epoch: [11][320/391]	Time 0.383 (0.399)	Data 0.002 (0.003)	Loss 0.7589 (0.9100)	Acc@1 86.719 (79.332)	Acc@5 99.219 (98.893)
Epoch: [11][384/391]	Time 0.401 (0.398)	Data 0.002 (0.002)	Loss 0.9958 (0.9158)	Acc@1 77.344 (79.249)	Acc@5 97.656 (98.870)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.406 (0.406)	Data 0.225 (0.225)	Loss 0.8815 (0.8815)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [12][64/391]	Time 0.388 (0.400)	Data 0.002 (0.005)	Loss 0.8916 (0.8841)	Acc@1 84.375 (80.721)	Acc@5 99.219 (99.075)
Epoch: [12][128/391]	Time 0.387 (0.403)	Data 0.002 (0.004)	Loss 0.8434 (0.8926)	Acc@1 83.594 (80.202)	Acc@5 99.219 (99.007)
Epoch: [12][192/391]	Time 0.432 (0.402)	Data 0.002 (0.003)	Loss 1.0368 (0.9045)	Acc@1 77.344 (79.659)	Acc@5 98.438 (98.883)
Epoch: [12][256/391]	Time 0.408 (0.402)	Data 0.002 (0.003)	Loss 1.0256 (0.9111)	Acc@1 78.906 (79.414)	Acc@5 100.000 (98.796)
Epoch: [12][320/391]	Time 0.454 (0.402)	Data 0.002 (0.003)	Loss 0.9210 (0.9132)	Acc@1 75.781 (79.337)	Acc@5 100.000 (98.832)
Epoch: [12][384/391]	Time 0.406 (0.402)	Data 0.002 (0.003)	Loss 0.9561 (0.9076)	Acc@1 81.250 (79.476)	Acc@5 97.656 (98.858)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.446 (0.446)	Data 0.215 (0.215)	Loss 0.8674 (0.8674)	Acc@1 78.906 (78.906)	Acc@5 100.000 (100.000)
Epoch: [13][64/391]	Time 0.401 (0.397)	Data 0.002 (0.005)	Loss 0.9242 (0.8958)	Acc@1 78.906 (79.844)	Acc@5 96.875 (98.498)
Epoch: [13][128/391]	Time 0.424 (0.404)	Data 0.002 (0.004)	Loss 0.8658 (0.8993)	Acc@1 76.562 (79.578)	Acc@5 100.000 (98.692)
Epoch: [13][192/391]	Time 0.395 (0.402)	Data 0.002 (0.003)	Loss 1.0343 (0.8988)	Acc@1 75.000 (79.574)	Acc@5 99.219 (98.745)
Epoch: [13][256/391]	Time 0.447 (0.401)	Data 0.002 (0.003)	Loss 1.0008 (0.8965)	Acc@1 78.125 (79.636)	Acc@5 97.656 (98.772)
Epoch: [13][320/391]	Time 0.397 (0.401)	Data 0.001 (0.003)	Loss 0.8603 (0.8971)	Acc@1 79.688 (79.673)	Acc@5 100.000 (98.795)
Epoch: [13][384/391]	Time 0.404 (0.400)	Data 0.002 (0.003)	Loss 0.9410 (0.8966)	Acc@1 81.250 (79.716)	Acc@5 97.656 (98.819)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.426 (0.426)	Data 0.253 (0.253)	Loss 0.7839 (0.7839)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [14][64/391]	Time 0.405 (0.397)	Data 0.002 (0.006)	Loss 0.8631 (0.8777)	Acc@1 79.688 (80.421)	Acc@5 97.656 (98.966)
Epoch: [14][128/391]	Time 0.446 (0.403)	Data 0.002 (0.004)	Loss 0.8134 (0.8852)	Acc@1 82.031 (79.887)	Acc@5 100.000 (98.922)
Epoch: [14][192/391]	Time 0.378 (0.401)	Data 0.002 (0.003)	Loss 0.9878 (0.8819)	Acc@1 75.781 (79.971)	Acc@5 99.219 (98.871)
Epoch: [14][256/391]	Time 0.346 (0.400)	Data 0.002 (0.003)	Loss 0.9673 (0.8874)	Acc@1 76.562 (79.882)	Acc@5 100.000 (98.842)
Epoch: [14][320/391]	Time 0.445 (0.399)	Data 0.002 (0.003)	Loss 0.8622 (0.8899)	Acc@1 84.375 (79.809)	Acc@5 99.219 (98.859)
Epoch: [14][384/391]	Time 0.374 (0.399)	Data 0.002 (0.003)	Loss 0.8537 (0.8871)	Acc@1 80.469 (79.888)	Acc@5 98.438 (98.878)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.406 (0.406)	Data 0.228 (0.228)	Loss 0.9123 (0.9123)	Acc@1 82.812 (82.812)	Acc@5 97.656 (97.656)
Epoch: [15][64/391]	Time 0.430 (0.400)	Data 0.002 (0.005)	Loss 0.7378 (0.8781)	Acc@1 82.812 (80.036)	Acc@5 100.000 (98.894)
Epoch: [15][128/391]	Time 0.350 (0.402)	Data 0.002 (0.004)	Loss 0.8723 (0.8774)	Acc@1 83.594 (80.245)	Acc@5 98.438 (98.825)
Epoch: [15][192/391]	Time 0.408 (0.402)	Data 0.002 (0.003)	Loss 0.9149 (0.8873)	Acc@1 81.250 (79.732)	Acc@5 100.000 (98.875)
Epoch: [15][256/391]	Time 0.373 (0.402)	Data 0.002 (0.003)	Loss 0.9572 (0.8897)	Acc@1 78.906 (79.694)	Acc@5 96.094 (98.817)
Epoch: [15][320/391]	Time 0.372 (0.401)	Data 0.002 (0.003)	Loss 1.0340 (0.8835)	Acc@1 77.344 (79.868)	Acc@5 96.094 (98.856)
Epoch: [15][384/391]	Time 0.407 (0.400)	Data 0.002 (0.003)	Loss 0.9329 (0.8828)	Acc@1 75.781 (79.848)	Acc@5 99.219 (98.872)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 399072 ; 487386 ; 0.8188007041646662

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.441 (0.441)	Data 0.239 (0.239)	Loss 0.9570 (0.9570)	Acc@1 77.344 (77.344)	Acc@5 98.438 (98.438)
Epoch: [16][64/391]	Time 0.383 (0.401)	Data 0.002 (0.006)	Loss 0.9395 (0.8502)	Acc@1 78.906 (80.565)	Acc@5 98.438 (98.930)
Epoch: [16][128/391]	Time 0.398 (0.405)	Data 0.002 (0.004)	Loss 0.7879 (0.8535)	Acc@1 80.469 (80.766)	Acc@5 99.219 (98.940)
Epoch: [16][192/391]	Time 0.457 (0.403)	Data 0.002 (0.003)	Loss 0.9350 (0.8643)	Acc@1 78.906 (80.481)	Acc@5 99.219 (98.899)
Epoch: [16][256/391]	Time 0.373 (0.399)	Data 0.002 (0.003)	Loss 0.8291 (0.8688)	Acc@1 78.906 (80.368)	Acc@5 100.000 (98.897)
Epoch: [16][320/391]	Time 0.416 (0.398)	Data 0.003 (0.003)	Loss 0.8167 (0.8688)	Acc@1 82.812 (80.189)	Acc@5 98.438 (98.880)
Epoch: [16][384/391]	Time 0.423 (0.399)	Data 0.002 (0.003)	Loss 0.7413 (0.8656)	Acc@1 82.812 (80.231)	Acc@5 99.219 (98.874)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.397 (0.397)	Data 0.278 (0.278)	Loss 0.8325 (0.8325)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [17][64/391]	Time 0.375 (0.403)	Data 0.002 (0.006)	Loss 0.7450 (0.8291)	Acc@1 87.500 (81.562)	Acc@5 100.000 (98.978)
Epoch: [17][128/391]	Time 0.368 (0.401)	Data 0.002 (0.004)	Loss 0.7856 (0.8449)	Acc@1 84.375 (80.971)	Acc@5 99.219 (98.940)
Epoch: [17][192/391]	Time 0.415 (0.400)	Data 0.002 (0.003)	Loss 0.7770 (0.8529)	Acc@1 84.375 (80.639)	Acc@5 96.875 (98.854)
Epoch: [17][256/391]	Time 0.413 (0.401)	Data 0.002 (0.003)	Loss 0.8162 (0.8502)	Acc@1 82.812 (80.606)	Acc@5 99.219 (98.890)
Epoch: [17][320/391]	Time 0.393 (0.399)	Data 0.002 (0.003)	Loss 0.8906 (0.8503)	Acc@1 78.125 (80.605)	Acc@5 98.438 (98.888)
Epoch: [17][384/391]	Time 0.403 (0.399)	Data 0.001 (0.003)	Loss 0.8674 (0.8506)	Acc@1 80.469 (80.629)	Acc@5 98.438 (98.888)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.443 (0.443)	Data 0.240 (0.240)	Loss 0.7817 (0.7817)	Acc@1 78.906 (78.906)	Acc@5 100.000 (100.000)
Epoch: [18][64/391]	Time 0.370 (0.398)	Data 0.002 (0.006)	Loss 0.9488 (0.8221)	Acc@1 77.344 (81.659)	Acc@5 99.219 (99.159)
Epoch: [18][128/391]	Time 0.439 (0.401)	Data 0.002 (0.004)	Loss 0.7998 (0.8265)	Acc@1 78.125 (81.202)	Acc@5 100.000 (99.061)
Epoch: [18][192/391]	Time 0.417 (0.401)	Data 0.002 (0.003)	Loss 0.7285 (0.8285)	Acc@1 82.812 (81.157)	Acc@5 99.219 (99.004)
Epoch: [18][256/391]	Time 0.447 (0.401)	Data 0.001 (0.003)	Loss 0.9628 (0.8283)	Acc@1 78.125 (81.086)	Acc@5 97.656 (99.027)
Epoch: [18][320/391]	Time 0.404 (0.400)	Data 0.002 (0.003)	Loss 0.8508 (0.8276)	Acc@1 82.031 (81.189)	Acc@5 99.219 (99.036)
Epoch: [18][384/391]	Time 0.411 (0.400)	Data 0.002 (0.003)	Loss 0.8028 (0.8358)	Acc@1 81.250 (80.881)	Acc@5 100.000 (99.032)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.413 (0.413)	Data 0.213 (0.213)	Loss 0.9476 (0.9476)	Acc@1 75.000 (75.000)	Acc@5 96.875 (96.875)
Epoch: [19][64/391]	Time 0.422 (0.396)	Data 0.002 (0.005)	Loss 0.7892 (0.8448)	Acc@1 84.375 (80.493)	Acc@5 98.438 (99.062)
Epoch: [19][128/391]	Time 0.400 (0.398)	Data 0.002 (0.004)	Loss 0.9821 (0.8423)	Acc@1 74.219 (80.578)	Acc@5 97.656 (98.989)
Epoch: [19][192/391]	Time 0.442 (0.400)	Data 0.003 (0.003)	Loss 0.9219 (0.8357)	Acc@1 82.812 (80.861)	Acc@5 97.656 (98.992)
Epoch: [19][256/391]	Time 0.376 (0.399)	Data 0.002 (0.003)	Loss 0.8682 (0.8374)	Acc@1 80.469 (80.767)	Acc@5 96.875 (98.973)
Epoch: [19][320/391]	Time 0.377 (0.399)	Data 0.002 (0.003)	Loss 0.9180 (0.8366)	Acc@1 75.781 (80.831)	Acc@5 99.219 (98.934)
Epoch: [19][384/391]	Time 0.372 (0.398)	Data 0.002 (0.003)	Loss 0.9095 (0.8398)	Acc@1 76.562 (80.755)	Acc@5 99.219 (98.933)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.412 (0.412)	Data 0.213 (0.213)	Loss 0.8327 (0.8327)	Acc@1 78.125 (78.125)	Acc@5 100.000 (100.000)
Epoch: [20][64/391]	Time 0.447 (0.399)	Data 0.002 (0.005)	Loss 0.7384 (0.8223)	Acc@1 82.031 (81.358)	Acc@5 99.219 (99.014)
Epoch: [20][128/391]	Time 0.381 (0.397)	Data 0.002 (0.004)	Loss 1.0097 (0.8258)	Acc@1 74.219 (81.414)	Acc@5 96.875 (98.940)
Epoch: [20][192/391]	Time 0.376 (0.399)	Data 0.003 (0.003)	Loss 0.8171 (0.8343)	Acc@1 82.812 (81.023)	Acc@5 99.219 (98.939)
Epoch: [20][256/391]	Time 0.435 (0.400)	Data 0.002 (0.003)	Loss 0.7818 (0.8351)	Acc@1 82.812 (80.876)	Acc@5 98.438 (98.969)
Epoch: [20][320/391]	Time 0.411 (0.400)	Data 0.002 (0.003)	Loss 0.8292 (0.8305)	Acc@1 80.469 (81.007)	Acc@5 100.000 (99.009)
Epoch: [20][384/391]	Time 0.385 (0.399)	Data 0.002 (0.003)	Loss 0.7703 (0.8339)	Acc@1 80.469 (80.913)	Acc@5 100.000 (98.996)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 323306 ; 487386 ; 0.6633469159967664

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.420 (0.420)	Data 0.203 (0.203)	Loss 0.7343 (0.7343)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [21][64/391]	Time 0.397 (0.397)	Data 0.002 (0.005)	Loss 0.7883 (0.8108)	Acc@1 83.594 (81.418)	Acc@5 99.219 (99.002)
Epoch: [21][128/391]	Time 0.439 (0.397)	Data 0.002 (0.004)	Loss 0.9120 (0.8155)	Acc@1 79.688 (81.329)	Acc@5 100.000 (99.073)
Epoch: [21][192/391]	Time 0.382 (0.397)	Data 0.002 (0.003)	Loss 0.9067 (0.8237)	Acc@1 80.469 (81.262)	Acc@5 100.000 (99.016)
Epoch: [21][256/391]	Time 0.428 (0.398)	Data 0.002 (0.003)	Loss 0.8332 (0.8249)	Acc@1 80.469 (81.341)	Acc@5 100.000 (98.994)
Epoch: [21][320/391]	Time 0.434 (0.400)	Data 0.002 (0.003)	Loss 0.8498 (0.8256)	Acc@1 83.594 (81.423)	Acc@5 99.219 (99.034)
Epoch: [21][384/391]	Time 0.380 (0.399)	Data 0.002 (0.003)	Loss 0.8183 (0.8292)	Acc@1 85.156 (81.254)	Acc@5 99.219 (99.048)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.512 (0.512)	Data 0.209 (0.209)	Loss 0.7786 (0.7786)	Acc@1 82.812 (82.812)	Acc@5 98.438 (98.438)
Epoch: [22][64/391]	Time 0.381 (0.403)	Data 0.002 (0.005)	Loss 0.8496 (0.8229)	Acc@1 83.594 (81.647)	Acc@5 98.438 (99.087)
Epoch: [22][128/391]	Time 0.361 (0.401)	Data 0.002 (0.004)	Loss 0.8853 (0.8214)	Acc@1 82.812 (81.741)	Acc@5 98.438 (99.098)
Epoch: [22][192/391]	Time 0.409 (0.401)	Data 0.001 (0.003)	Loss 0.7963 (0.8300)	Acc@1 83.594 (81.286)	Acc@5 99.219 (99.004)
Epoch: [22][256/391]	Time 0.382 (0.400)	Data 0.002 (0.003)	Loss 0.9289 (0.8306)	Acc@1 77.344 (81.323)	Acc@5 97.656 (98.963)
Epoch: [22][320/391]	Time 0.433 (0.400)	Data 0.002 (0.003)	Loss 0.9886 (0.8328)	Acc@1 77.344 (81.248)	Acc@5 97.656 (98.980)
Epoch: [22][384/391]	Time 0.410 (0.400)	Data 0.002 (0.003)	Loss 0.8672 (0.8380)	Acc@1 83.594 (81.177)	Acc@5 99.219 (98.963)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.444 (0.444)	Data 0.222 (0.222)	Loss 0.7863 (0.7863)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [23][64/391]	Time 0.403 (0.397)	Data 0.002 (0.005)	Loss 0.8390 (0.8261)	Acc@1 82.031 (81.671)	Acc@5 99.219 (99.014)
Epoch: [23][128/391]	Time 0.386 (0.401)	Data 0.002 (0.004)	Loss 0.6817 (0.8279)	Acc@1 87.500 (81.577)	Acc@5 99.219 (99.019)
Epoch: [23][192/391]	Time 0.414 (0.402)	Data 0.002 (0.003)	Loss 0.7010 (0.8351)	Acc@1 84.375 (81.375)	Acc@5 100.000 (99.024)
Epoch: [23][256/391]	Time 0.412 (0.401)	Data 0.002 (0.003)	Loss 0.9503 (0.8345)	Acc@1 78.906 (81.378)	Acc@5 96.875 (99.018)
Epoch: [23][320/391]	Time 0.397 (0.400)	Data 0.002 (0.003)	Loss 0.8194 (0.8356)	Acc@1 81.250 (81.269)	Acc@5 97.656 (99.012)
Epoch: [23][384/391]	Time 0.397 (0.400)	Data 0.002 (0.003)	Loss 0.7103 (0.8365)	Acc@1 85.156 (81.191)	Acc@5 98.438 (98.989)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.442 (0.442)	Data 0.216 (0.216)	Loss 0.7034 (0.7034)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [24][64/391]	Time 0.403 (0.403)	Data 0.002 (0.005)	Loss 0.7845 (0.8085)	Acc@1 83.594 (82.224)	Acc@5 99.219 (99.207)
Epoch: [24][128/391]	Time 0.394 (0.401)	Data 0.002 (0.004)	Loss 0.6488 (0.8061)	Acc@1 86.719 (82.249)	Acc@5 100.000 (99.043)
Epoch: [24][192/391]	Time 0.377 (0.403)	Data 0.003 (0.003)	Loss 0.7028 (0.8223)	Acc@1 87.500 (81.699)	Acc@5 99.219 (98.964)
Epoch: [24][256/391]	Time 0.370 (0.402)	Data 0.001 (0.003)	Loss 0.8024 (0.8178)	Acc@1 78.906 (81.849)	Acc@5 99.219 (99.033)
Epoch: [24][320/391]	Time 0.453 (0.402)	Data 0.002 (0.003)	Loss 0.9496 (0.8269)	Acc@1 82.031 (81.535)	Acc@5 97.656 (98.968)
Epoch: [24][384/391]	Time 0.426 (0.401)	Data 0.002 (0.003)	Loss 0.7502 (0.8282)	Acc@1 85.938 (81.441)	Acc@5 99.219 (98.965)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.468 (0.468)	Data 0.206 (0.206)	Loss 0.7402 (0.7402)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [25][64/391]	Time 0.413 (0.406)	Data 0.002 (0.005)	Loss 0.8015 (0.8338)	Acc@1 82.031 (81.214)	Acc@5 99.219 (99.026)
Epoch: [25][128/391]	Time 0.403 (0.404)	Data 0.002 (0.004)	Loss 0.7006 (0.8169)	Acc@1 88.281 (81.880)	Acc@5 97.656 (99.073)
Epoch: [25][192/391]	Time 0.306 (0.403)	Data 0.001 (0.003)	Loss 0.7139 (0.8142)	Acc@1 85.156 (81.983)	Acc@5 100.000 (99.093)
Epoch: [25][256/391]	Time 0.363 (0.403)	Data 0.002 (0.003)	Loss 0.7640 (0.8156)	Acc@1 80.469 (81.846)	Acc@5 99.219 (99.094)
Epoch: [25][320/391]	Time 0.419 (0.403)	Data 0.002 (0.003)	Loss 0.7596 (0.8236)	Acc@1 82.812 (81.591)	Acc@5 100.000 (99.078)
Epoch: [25][384/391]	Time 0.432 (0.403)	Data 0.002 (0.003)	Loss 0.7810 (0.8283)	Acc@1 85.938 (81.465)	Acc@5 99.219 (99.060)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.513 (0.513)	Data 0.201 (0.201)	Loss 3.2448 (3.2448)	Acc@1 8.594 (8.594)	Acc@5 48.438 (48.438)
Epoch: [1][64/391]	Time 0.375 (0.396)	Data 0.001 (0.005)	Loss 2.6528 (2.7651)	Acc@1 17.969 (19.195)	Acc@5 82.031 (74.195)
Epoch: [1][128/391]	Time 0.441 (0.400)	Data 0.002 (0.003)	Loss 2.5043 (2.6230)	Acc@1 28.125 (22.941)	Acc@5 82.812 (79.245)
Epoch: [1][192/391]	Time 0.320 (0.400)	Data 0.002 (0.003)	Loss 2.3041 (2.5354)	Acc@1 35.156 (25.988)	Acc@5 85.938 (81.748)
Epoch: [1][256/391]	Time 0.414 (0.399)	Data 0.002 (0.003)	Loss 2.0573 (2.4527)	Acc@1 52.344 (28.727)	Acc@5 92.188 (83.676)
Epoch: [1][320/391]	Time 0.402 (0.398)	Data 0.002 (0.003)	Loss 1.8895 (2.3810)	Acc@1 45.312 (31.087)	Acc@5 95.312 (85.095)
Epoch: [1][384/391]	Time 0.412 (0.399)	Data 0.002 (0.002)	Loss 1.9965 (2.3195)	Acc@1 39.062 (33.257)	Acc@5 88.281 (86.185)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.443 (0.443)	Data 0.206 (0.206)	Loss 2.0710 (2.0710)	Acc@1 36.719 (36.719)	Acc@5 92.969 (92.969)
Epoch: [2][64/391]	Time 0.313 (0.394)	Data 0.001 (0.005)	Loss 1.7562 (1.8928)	Acc@1 53.125 (48.678)	Acc@5 93.750 (92.572)
Epoch: [2][128/391]	Time 0.401 (0.397)	Data 0.002 (0.003)	Loss 1.6647 (1.8349)	Acc@1 53.125 (50.303)	Acc@5 96.875 (93.441)
Epoch: [2][192/391]	Time 0.401 (0.399)	Data 0.002 (0.003)	Loss 1.6199 (1.7803)	Acc@1 56.250 (52.117)	Acc@5 96.094 (93.940)
Epoch: [2][256/391]	Time 0.409 (0.401)	Data 0.002 (0.003)	Loss 1.6817 (1.7391)	Acc@1 55.469 (53.414)	Acc@5 91.406 (94.230)
Epoch: [2][320/391]	Time 0.392 (0.400)	Data 0.002 (0.003)	Loss 1.4963 (1.7006)	Acc@1 60.938 (54.678)	Acc@5 96.094 (94.514)
Epoch: [2][384/391]	Time 0.440 (0.400)	Data 0.002 (0.002)	Loss 1.3964 (1.6644)	Acc@1 67.188 (55.927)	Acc@5 94.531 (94.769)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.419 (0.419)	Data 0.200 (0.200)	Loss 1.3623 (1.3623)	Acc@1 67.969 (67.969)	Acc@5 97.656 (97.656)
Epoch: [3][64/391]	Time 0.401 (0.401)	Data 0.002 (0.005)	Loss 1.3351 (1.4136)	Acc@1 62.500 (64.663)	Acc@5 96.875 (96.502)
Epoch: [3][128/391]	Time 0.381 (0.407)	Data 0.002 (0.003)	Loss 1.1971 (1.3910)	Acc@1 70.312 (64.947)	Acc@5 97.656 (96.590)
Epoch: [3][192/391]	Time 0.399 (0.406)	Data 0.001 (0.003)	Loss 1.2274 (1.3642)	Acc@1 70.312 (65.609)	Acc@5 98.438 (96.741)
Epoch: [3][256/391]	Time 0.340 (0.406)	Data 0.002 (0.003)	Loss 1.0924 (1.3333)	Acc@1 71.094 (66.583)	Acc@5 97.656 (96.936)
Epoch: [3][320/391]	Time 0.395 (0.405)	Data 0.002 (0.003)	Loss 1.2554 (1.3156)	Acc@1 67.188 (66.944)	Acc@5 96.094 (97.028)
Epoch: [3][384/391]	Time 0.377 (0.404)	Data 0.001 (0.002)	Loss 1.0420 (1.2950)	Acc@1 69.531 (67.498)	Acc@5 99.219 (97.119)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.431 (0.431)	Data 0.219 (0.219)	Loss 1.1857 (1.1857)	Acc@1 71.875 (71.875)	Acc@5 97.656 (97.656)
Epoch: [4][64/391]	Time 0.404 (0.402)	Data 0.002 (0.005)	Loss 1.1965 (1.1393)	Acc@1 73.438 (71.935)	Acc@5 98.438 (98.221)
Epoch: [4][128/391]	Time 0.397 (0.404)	Data 0.001 (0.004)	Loss 1.1445 (1.1379)	Acc@1 68.750 (71.984)	Acc@5 96.875 (98.056)
Epoch: [4][192/391]	Time 0.352 (0.403)	Data 0.002 (0.003)	Loss 1.0956 (1.1315)	Acc@1 73.438 (72.227)	Acc@5 99.219 (98.081)
Epoch: [4][256/391]	Time 0.423 (0.403)	Data 0.002 (0.003)	Loss 1.2357 (1.1236)	Acc@1 68.750 (72.498)	Acc@5 99.219 (98.091)
Epoch: [4][320/391]	Time 0.389 (0.402)	Data 0.002 (0.003)	Loss 1.1464 (1.1162)	Acc@1 69.531 (72.605)	Acc@5 97.656 (98.111)
Epoch: [4][384/391]	Time 0.382 (0.401)	Data 0.002 (0.002)	Loss 0.9789 (1.1093)	Acc@1 81.250 (72.819)	Acc@5 98.438 (98.111)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.478 (0.478)	Data 0.255 (0.255)	Loss 0.9831 (0.9831)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [5][64/391]	Time 0.386 (0.397)	Data 0.002 (0.006)	Loss 1.0119 (1.0429)	Acc@1 75.781 (74.615)	Acc@5 96.875 (98.450)
Epoch: [5][128/391]	Time 0.371 (0.397)	Data 0.002 (0.004)	Loss 0.9674 (1.0446)	Acc@1 78.125 (74.509)	Acc@5 97.656 (98.244)
Epoch: [5][192/391]	Time 0.386 (0.400)	Data 0.002 (0.003)	Loss 0.9473 (1.0373)	Acc@1 76.562 (74.692)	Acc@5 99.219 (98.296)
Epoch: [5][256/391]	Time 0.385 (0.400)	Data 0.002 (0.003)	Loss 0.9267 (1.0307)	Acc@1 82.812 (74.833)	Acc@5 98.438 (98.352)
Epoch: [5][320/391]	Time 0.409 (0.401)	Data 0.002 (0.003)	Loss 1.0145 (1.0272)	Acc@1 75.000 (74.988)	Acc@5 97.656 (98.355)
Epoch: [5][384/391]	Time 0.378 (0.400)	Data 0.001 (0.003)	Loss 0.9904 (1.0260)	Acc@1 78.125 (74.976)	Acc@5 100.000 (98.379)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.432 (0.432)	Data 0.214 (0.214)	Loss 0.9426 (0.9426)	Acc@1 77.344 (77.344)	Acc@5 99.219 (99.219)
Epoch: [6][64/391]	Time 0.393 (0.397)	Data 0.002 (0.005)	Loss 0.8009 (0.9457)	Acc@1 79.688 (77.248)	Acc@5 100.000 (98.822)
Epoch: [6][128/391]	Time 0.333 (0.393)	Data 0.002 (0.004)	Loss 0.9531 (0.9651)	Acc@1 78.125 (76.647)	Acc@5 99.219 (98.649)
Epoch: [6][192/391]	Time 0.334 (0.378)	Data 0.003 (0.003)	Loss 0.9410 (0.9679)	Acc@1 78.125 (76.830)	Acc@5 97.656 (98.591)
Epoch: [6][256/391]	Time 0.329 (0.369)	Data 0.001 (0.003)	Loss 1.0530 (0.9714)	Acc@1 74.219 (76.645)	Acc@5 100.000 (98.526)
Epoch: [6][320/391]	Time 0.326 (0.364)	Data 0.002 (0.003)	Loss 0.9555 (0.9712)	Acc@1 80.469 (76.660)	Acc@5 98.438 (98.518)
Epoch: [6][384/391]	Time 0.335 (0.360)	Data 0.002 (0.002)	Loss 0.9429 (0.9713)	Acc@1 76.562 (76.646)	Acc@5 100.000 (98.521)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.403 (0.403)	Data 0.239 (0.239)	Loss 0.7809 (0.7809)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [7][64/391]	Time 0.335 (0.346)	Data 0.002 (0.006)	Loss 0.8834 (0.9279)	Acc@1 80.469 (77.825)	Acc@5 99.219 (98.762)
Epoch: [7][128/391]	Time 0.332 (0.344)	Data 0.002 (0.004)	Loss 0.8182 (0.9322)	Acc@1 82.812 (77.677)	Acc@5 99.219 (98.758)
Epoch: [7][192/391]	Time 0.276 (0.341)	Data 0.002 (0.003)	Loss 1.1607 (0.9410)	Acc@1 72.656 (77.554)	Acc@5 96.094 (98.656)
Epoch: [7][256/391]	Time 0.260 (0.330)	Data 0.002 (0.003)	Loss 0.9848 (0.9489)	Acc@1 71.094 (77.265)	Acc@5 98.438 (98.614)
Epoch: [7][320/391]	Time 0.300 (0.321)	Data 0.002 (0.003)	Loss 0.8578 (0.9475)	Acc@1 81.250 (77.268)	Acc@5 98.438 (98.613)
Epoch: [7][384/391]	Time 0.287 (0.317)	Data 0.002 (0.003)	Loss 1.0797 (0.9444)	Acc@1 72.656 (77.394)	Acc@5 97.656 (98.604)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.315 (0.315)	Data 0.205 (0.205)	Loss 0.9760 (0.9760)	Acc@1 74.219 (74.219)	Acc@5 97.656 (97.656)
Epoch: [8][64/391]	Time 0.381 (0.291)	Data 0.002 (0.005)	Loss 0.8853 (0.9248)	Acc@1 79.688 (77.969)	Acc@5 99.219 (98.846)
Epoch: [8][128/391]	Time 0.288 (0.295)	Data 0.003 (0.004)	Loss 0.8862 (0.9245)	Acc@1 81.250 (78.064)	Acc@5 100.000 (98.637)
Epoch: [8][192/391]	Time 0.253 (0.295)	Data 0.002 (0.003)	Loss 0.9507 (0.9217)	Acc@1 81.250 (78.210)	Acc@5 100.000 (98.737)
Epoch: [8][256/391]	Time 0.292 (0.296)	Data 0.002 (0.003)	Loss 1.0374 (0.9211)	Acc@1 72.656 (78.110)	Acc@5 99.219 (98.720)
Epoch: [8][320/391]	Time 0.256 (0.294)	Data 0.002 (0.003)	Loss 0.8169 (0.9175)	Acc@1 82.031 (78.210)	Acc@5 98.438 (98.744)
Epoch: [8][384/391]	Time 0.319 (0.293)	Data 0.002 (0.003)	Loss 0.9272 (0.9193)	Acc@1 80.469 (78.157)	Acc@5 98.438 (98.685)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.341 (0.341)	Data 0.186 (0.186)	Loss 0.9224 (0.9224)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [9][64/391]	Time 0.328 (0.295)	Data 0.002 (0.005)	Loss 0.9623 (0.8955)	Acc@1 76.562 (79.327)	Acc@5 100.000 (98.846)
Epoch: [9][128/391]	Time 0.281 (0.293)	Data 0.002 (0.003)	Loss 1.0030 (0.8987)	Acc@1 76.562 (79.130)	Acc@5 99.219 (98.843)
Epoch: [9][192/391]	Time 0.267 (0.292)	Data 0.002 (0.003)	Loss 0.9077 (0.9029)	Acc@1 78.906 (79.040)	Acc@5 99.219 (98.794)
Epoch: [9][256/391]	Time 0.248 (0.292)	Data 0.002 (0.003)	Loss 0.9483 (0.9015)	Acc@1 77.344 (79.007)	Acc@5 98.438 (98.738)
Epoch: [9][320/391]	Time 0.287 (0.291)	Data 0.002 (0.003)	Loss 0.6978 (0.9022)	Acc@1 87.500 (78.928)	Acc@5 100.000 (98.744)
Epoch: [9][384/391]	Time 0.325 (0.292)	Data 0.002 (0.003)	Loss 0.9426 (0.9047)	Acc@1 75.781 (78.766)	Acc@5 100.000 (98.758)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.319 (0.319)	Data 0.228 (0.228)	Loss 1.0701 (1.0701)	Acc@1 72.656 (72.656)	Acc@5 98.438 (98.438)
Epoch: [10][64/391]	Time 0.273 (0.290)	Data 0.002 (0.006)	Loss 0.9225 (0.9030)	Acc@1 80.469 (78.858)	Acc@5 100.000 (98.738)
Epoch: [10][128/391]	Time 0.329 (0.292)	Data 0.002 (0.004)	Loss 0.9732 (0.8930)	Acc@1 78.906 (79.100)	Acc@5 97.656 (98.819)
Epoch: [10][192/391]	Time 0.324 (0.292)	Data 0.002 (0.003)	Loss 0.8989 (0.8957)	Acc@1 78.125 (79.016)	Acc@5 99.219 (98.822)
Epoch: [10][256/391]	Time 0.276 (0.293)	Data 0.002 (0.003)	Loss 0.9390 (0.8963)	Acc@1 79.688 (78.943)	Acc@5 96.875 (98.802)
Epoch: [10][320/391]	Time 0.277 (0.292)	Data 0.002 (0.003)	Loss 0.8584 (0.8959)	Acc@1 76.562 (78.965)	Acc@5 98.438 (98.810)
Epoch: [10][384/391]	Time 0.290 (0.291)	Data 0.002 (0.003)	Loss 0.9327 (0.8963)	Acc@1 78.125 (78.925)	Acc@5 97.656 (98.774)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 468922 ; 487386 ; 0.9621162692403967

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.301 (0.301)	Data 0.203 (0.203)	Loss 0.7491 (0.7491)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [11][64/391]	Time 0.328 (0.290)	Data 0.002 (0.005)	Loss 0.8747 (0.8650)	Acc@1 78.906 (79.748)	Acc@5 98.438 (98.702)
Epoch: [11][128/391]	Time 0.296 (0.290)	Data 0.002 (0.004)	Loss 0.9410 (0.8716)	Acc@1 80.469 (79.809)	Acc@5 98.438 (98.765)
Epoch: [11][192/391]	Time 0.319 (0.291)	Data 0.002 (0.003)	Loss 0.9811 (0.8764)	Acc@1 78.906 (79.619)	Acc@5 99.219 (98.769)
Epoch: [11][256/391]	Time 0.229 (0.291)	Data 0.003 (0.003)	Loss 0.9197 (0.8752)	Acc@1 74.219 (79.599)	Acc@5 100.000 (98.811)
Epoch: [11][320/391]	Time 0.277 (0.285)	Data 0.002 (0.003)	Loss 0.9412 (0.8777)	Acc@1 75.000 (79.451)	Acc@5 99.219 (98.788)
Epoch: [11][384/391]	Time 0.255 (0.281)	Data 0.002 (0.003)	Loss 0.8760 (0.8800)	Acc@1 79.688 (79.361)	Acc@5 100.000 (98.772)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.320 (0.320)	Data 0.195 (0.195)	Loss 0.8319 (0.8319)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [12][64/391]	Time 0.274 (0.265)	Data 0.002 (0.005)	Loss 0.8093 (0.8897)	Acc@1 81.250 (79.099)	Acc@5 97.656 (98.822)
Epoch: [12][128/391]	Time 0.235 (0.265)	Data 0.002 (0.004)	Loss 0.7349 (0.8732)	Acc@1 85.156 (79.760)	Acc@5 100.000 (98.874)
Epoch: [12][192/391]	Time 0.285 (0.261)	Data 0.002 (0.003)	Loss 0.6982 (0.8640)	Acc@1 84.375 (80.028)	Acc@5 100.000 (98.883)
Epoch: [12][256/391]	Time 0.306 (0.264)	Data 0.002 (0.003)	Loss 1.0839 (0.8646)	Acc@1 73.438 (79.976)	Acc@5 96.094 (98.827)
Epoch: [12][320/391]	Time 0.287 (0.263)	Data 0.003 (0.003)	Loss 0.8308 (0.8639)	Acc@1 82.812 (79.970)	Acc@5 98.438 (98.856)
Epoch: [12][384/391]	Time 0.228 (0.262)	Data 0.002 (0.003)	Loss 0.7697 (0.8644)	Acc@1 83.594 (79.945)	Acc@5 99.219 (98.874)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.246 (0.246)	Data 0.194 (0.194)	Loss 0.7811 (0.7811)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [13][64/391]	Time 0.253 (0.248)	Data 0.002 (0.005)	Loss 0.9279 (0.8495)	Acc@1 78.906 (80.276)	Acc@5 99.219 (98.954)
Epoch: [13][128/391]	Time 0.281 (0.246)	Data 0.003 (0.003)	Loss 1.0488 (0.8457)	Acc@1 73.438 (80.445)	Acc@5 99.219 (98.874)
Epoch: [13][192/391]	Time 0.690 (0.540)	Data 0.002 (0.003)	Loss 0.7923 (0.8470)	Acc@1 82.031 (80.327)	Acc@5 100.000 (98.927)
Epoch: [13][256/391]	Time 0.974 (0.532)	Data 0.003 (0.003)	Loss 0.9615 (0.8511)	Acc@1 79.688 (80.295)	Acc@5 97.656 (98.906)
Epoch: [13][320/391]	Time 0.442 (0.547)	Data 0.006 (0.003)	Loss 0.8950 (0.8535)	Acc@1 77.344 (80.247)	Acc@5 100.000 (98.897)
Epoch: [13][384/391]	Time 0.592 (0.554)	Data 0.002 (0.003)	Loss 0.8114 (0.8558)	Acc@1 83.594 (80.191)	Acc@5 98.438 (98.898)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.457 (0.457)	Data 0.351 (0.351)	Loss 0.9103 (0.9103)	Acc@1 80.469 (80.469)	Acc@5 96.094 (96.094)
Epoch: [14][64/391]	Time 0.278 (0.421)	Data 0.002 (0.009)	Loss 0.8804 (0.8526)	Acc@1 76.562 (80.192)	Acc@5 98.438 (98.750)
Epoch: [14][128/391]	Time 0.835 (0.516)	Data 0.002 (0.006)	Loss 1.0816 (0.8617)	Acc@1 73.438 (80.015)	Acc@5 99.219 (98.740)
Epoch: [14][192/391]	Time 0.473 (0.547)	Data 0.002 (0.005)	Loss 0.8554 (0.8669)	Acc@1 81.250 (79.724)	Acc@5 99.219 (98.790)
Epoch: [14][256/391]	Time 0.413 (0.523)	Data 0.002 (0.004)	Loss 0.8664 (0.8633)	Acc@1 81.250 (79.855)	Acc@5 97.656 (98.793)
Epoch: [14][320/391]	Time 0.559 (0.510)	Data 0.002 (0.004)	Loss 0.8722 (0.8626)	Acc@1 79.688 (79.846)	Acc@5 99.219 (98.846)
Epoch: [14][384/391]	Time 0.390 (0.503)	Data 0.003 (0.004)	Loss 0.8933 (0.8609)	Acc@1 76.562 (79.901)	Acc@5 97.656 (98.870)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.337 (0.337)	Data 0.393 (0.393)	Loss 0.9961 (0.9961)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [15][64/391]	Time 0.810 (0.511)	Data 0.002 (0.009)	Loss 0.8383 (0.8560)	Acc@1 83.594 (80.409)	Acc@5 100.000 (98.942)
Epoch: [15][128/391]	Time 0.271 (0.496)	Data 0.002 (0.006)	Loss 0.8039 (0.8457)	Acc@1 81.250 (80.451)	Acc@5 96.875 (98.952)
Epoch: [15][192/391]	Time 0.806 (0.495)	Data 0.002 (0.005)	Loss 0.7917 (0.8502)	Acc@1 82.812 (80.226)	Acc@5 99.219 (98.863)
Epoch: [15][256/391]	Time 0.396 (0.520)	Data 0.003 (0.004)	Loss 0.8673 (0.8473)	Acc@1 76.562 (80.347)	Acc@5 98.438 (98.878)
Epoch: [15][320/391]	Time 0.721 (0.521)	Data 0.002 (0.004)	Loss 0.8355 (0.8510)	Acc@1 78.906 (80.189)	Acc@5 97.656 (98.871)
Epoch: [15][384/391]	Time 0.343 (0.521)	Data 0.002 (0.004)	Loss 0.9199 (0.8522)	Acc@1 79.688 (80.067)	Acc@5 97.656 (98.884)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 358110 ; 487386 ; 0.734756435351036

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.703 (0.703)	Data 0.700 (0.700)	Loss 0.8135 (0.8135)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [16][64/391]	Time 0.418 (0.509)	Data 0.002 (0.014)	Loss 0.7205 (0.8161)	Acc@1 88.281 (81.454)	Acc@5 98.438 (99.111)
Epoch: [16][128/391]	Time 0.535 (0.556)	Data 0.004 (0.009)	Loss 0.8974 (0.8271)	Acc@1 78.906 (80.820)	Acc@5 98.438 (99.079)
Epoch: [16][192/391]	Time 0.728 (0.602)	Data 0.003 (0.007)	Loss 0.7177 (0.8372)	Acc@1 82.031 (80.428)	Acc@5 99.219 (99.093)
Epoch: [16][256/391]	Time 0.982 (0.642)	Data 0.002 (0.006)	Loss 0.7975 (0.8408)	Acc@1 82.031 (80.475)	Acc@5 98.438 (99.033)
Epoch: [16][320/391]	Time 1.148 (0.697)	Data 0.002 (0.005)	Loss 0.8205 (0.8388)	Acc@1 84.375 (80.593)	Acc@5 98.438 (99.019)
Epoch: [16][384/391]	Time 0.445 (0.707)	Data 0.002 (0.005)	Loss 0.9554 (0.8396)	Acc@1 75.781 (80.607)	Acc@5 99.219 (98.989)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.510 (0.510)	Data 0.551 (0.551)	Loss 0.8460 (0.8460)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [17][64/391]	Time 0.308 (0.497)	Data 0.002 (0.011)	Loss 0.7524 (0.8265)	Acc@1 85.156 (80.781)	Acc@5 99.219 (99.123)
Epoch: [17][128/391]	Time 0.448 (0.507)	Data 0.002 (0.007)	Loss 0.8393 (0.8318)	Acc@1 78.906 (80.959)	Acc@5 99.219 (98.958)
Epoch: [17][192/391]	Time 0.525 (0.531)	Data 0.002 (0.006)	Loss 0.9647 (0.8340)	Acc@1 75.781 (80.894)	Acc@5 100.000 (98.907)
Epoch: [17][256/391]	Time 0.770 (0.567)	Data 0.002 (0.005)	Loss 0.6804 (0.8359)	Acc@1 85.156 (80.897)	Acc@5 100.000 (98.893)
Epoch: [17][320/391]	Time 0.729 (0.593)	Data 0.002 (0.005)	Loss 0.9582 (0.8352)	Acc@1 74.219 (80.802)	Acc@5 98.438 (98.900)
Epoch: [17][384/391]	Time 0.772 (0.626)	Data 0.002 (0.005)	Loss 0.7387 (0.8331)	Acc@1 82.031 (80.871)	Acc@5 100.000 (98.920)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.461 (0.461)	Data 0.447 (0.447)	Loss 0.9165 (0.9165)	Acc@1 78.906 (78.906)	Acc@5 97.656 (97.656)
Epoch: [18][64/391]	Time 0.962 (0.673)	Data 0.002 (0.011)	Loss 0.8681 (0.8322)	Acc@1 80.469 (80.577)	Acc@5 100.000 (99.099)
Epoch: [18][128/391]	Time 0.696 (0.667)	Data 0.002 (0.007)	Loss 0.9754 (0.8319)	Acc@1 77.344 (80.990)	Acc@5 95.312 (99.043)
Epoch: [18][192/391]	Time 0.375 (0.621)	Data 0.003 (0.005)	Loss 0.8775 (0.8260)	Acc@1 78.906 (81.149)	Acc@5 99.219 (99.028)
Epoch: [18][256/391]	Time 0.479 (0.608)	Data 0.003 (0.005)	Loss 0.7865 (0.8288)	Acc@1 82.031 (80.970)	Acc@5 98.438 (99.027)
Epoch: [18][320/391]	Time 0.653 (0.607)	Data 0.015 (0.005)	Loss 1.0903 (0.8337)	Acc@1 72.656 (80.846)	Acc@5 96.875 (99.019)
Epoch: [18][384/391]	Time 0.520 (0.594)	Data 0.002 (0.004)	Loss 0.8276 (0.8352)	Acc@1 80.469 (80.816)	Acc@5 97.656 (98.994)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.355 (0.355)	Data 0.700 (0.700)	Loss 0.7921 (0.7921)	Acc@1 80.469 (80.469)	Acc@5 100.000 (100.000)
Epoch: [19][64/391]	Time 0.343 (0.490)	Data 0.003 (0.014)	Loss 0.7475 (0.8243)	Acc@1 84.375 (81.442)	Acc@5 98.438 (99.183)
Epoch: [19][128/391]	Time 0.356 (0.467)	Data 0.002 (0.008)	Loss 0.7700 (0.8169)	Acc@1 82.812 (81.523)	Acc@5 100.000 (99.170)
Epoch: [19][192/391]	Time 0.566 (0.472)	Data 0.002 (0.007)	Loss 0.7640 (0.8216)	Acc@1 78.906 (81.258)	Acc@5 100.000 (99.130)
Epoch: [19][256/391]	Time 0.582 (0.486)	Data 0.002 (0.006)	Loss 0.7750 (0.8236)	Acc@1 84.375 (81.293)	Acc@5 98.438 (99.131)
Epoch: [19][320/391]	Time 0.406 (0.515)	Data 0.002 (0.005)	Loss 0.7761 (0.8268)	Acc@1 81.250 (81.179)	Acc@5 99.219 (99.073)
Epoch: [19][384/391]	Time 0.422 (0.509)	Data 0.002 (0.005)	Loss 0.8490 (0.8263)	Acc@1 82.812 (81.130)	Acc@5 97.656 (99.056)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.478 (0.478)	Data 0.383 (0.383)	Loss 0.8969 (0.8969)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [20][64/391]	Time 0.514 (0.486)	Data 0.003 (0.010)	Loss 0.8673 (0.8535)	Acc@1 82.031 (79.832)	Acc@5 99.219 (99.075)
Epoch: [20][128/391]	Time 0.605 (0.521)	Data 0.002 (0.007)	Loss 0.7664 (0.8247)	Acc@1 84.375 (81.026)	Acc@5 98.438 (99.019)
Epoch: [20][192/391]	Time 0.353 (0.499)	Data 0.002 (0.006)	Loss 0.9351 (0.8268)	Acc@1 81.250 (80.934)	Acc@5 96.875 (99.004)
Epoch: [20][256/391]	Time 0.455 (0.481)	Data 0.002 (0.005)	Loss 0.7254 (0.8270)	Acc@1 82.812 (80.928)	Acc@5 100.000 (98.948)
Epoch: [20][320/391]	Time 0.364 (0.470)	Data 0.002 (0.005)	Loss 0.7894 (0.8255)	Acc@1 82.031 (80.865)	Acc@5 99.219 (98.971)
Epoch: [20][384/391]	Time 0.416 (0.461)	Data 0.002 (0.004)	Loss 0.9647 (0.8307)	Acc@1 72.656 (80.743)	Acc@5 97.656 (98.943)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 317116 ; 487386 ; 0.6506465101582729

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.536 (0.536)	Data 0.569 (0.569)	Loss 0.7892 (0.7892)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [21][64/391]	Time 0.562 (0.434)	Data 0.004 (0.011)	Loss 0.6418 (0.7696)	Acc@1 85.938 (82.716)	Acc@5 99.219 (99.099)
Epoch: [21][128/391]	Time 0.738 (0.463)	Data 0.003 (0.007)	Loss 0.9369 (0.7993)	Acc@1 74.219 (81.710)	Acc@5 98.438 (99.007)
Epoch: [21][192/391]	Time 0.926 (0.515)	Data 0.002 (0.006)	Loss 0.8853 (0.8210)	Acc@1 78.125 (81.003)	Acc@5 99.219 (98.948)
Epoch: [21][256/391]	Time 0.621 (0.554)	Data 0.002 (0.005)	Loss 0.6670 (0.8191)	Acc@1 87.500 (81.101)	Acc@5 100.000 (98.921)
Epoch: [21][320/391]	Time 0.398 (0.537)	Data 0.004 (0.005)	Loss 0.9060 (0.8200)	Acc@1 76.562 (81.048)	Acc@5 97.656 (98.934)
Epoch: [21][384/391]	Time 0.703 (0.539)	Data 0.003 (0.004)	Loss 0.7993 (0.8231)	Acc@1 81.250 (80.940)	Acc@5 98.438 (98.927)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.696 (0.696)	Data 0.984 (0.984)	Loss 0.8002 (0.8002)	Acc@1 83.594 (83.594)	Acc@5 98.438 (98.438)
Epoch: [22][64/391]	Time 0.318 (0.561)	Data 0.002 (0.018)	Loss 0.7791 (0.8143)	Acc@1 84.375 (81.046)	Acc@5 99.219 (99.147)
Epoch: [22][128/391]	Time 0.435 (0.559)	Data 0.003 (0.012)	Loss 0.8039 (0.8138)	Acc@1 83.594 (81.129)	Acc@5 100.000 (99.140)
Epoch: [22][192/391]	Time 0.450 (0.554)	Data 0.002 (0.009)	Loss 0.9552 (0.8104)	Acc@1 75.000 (81.092)	Acc@5 98.438 (99.118)
Epoch: [22][256/391]	Time 0.516 (0.543)	Data 0.002 (0.008)	Loss 0.8038 (0.8111)	Acc@1 79.688 (81.195)	Acc@5 98.438 (99.021)
Epoch: [22][320/391]	Time 0.395 (0.538)	Data 0.003 (0.007)	Loss 0.7123 (0.8164)	Acc@1 84.375 (81.131)	Acc@5 100.000 (98.980)
Epoch: [22][384/391]	Time 0.535 (0.534)	Data 0.002 (0.006)	Loss 0.7670 (0.8171)	Acc@1 79.688 (81.144)	Acc@5 98.438 (98.949)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 1.053 (1.053)	Data 1.330 (1.330)	Loss 0.9242 (0.9242)	Acc@1 78.906 (78.906)	Acc@5 97.656 (97.656)
Epoch: [23][64/391]	Time 0.465 (0.539)	Data 0.002 (0.023)	Loss 0.8710 (0.7980)	Acc@1 81.250 (81.743)	Acc@5 99.219 (98.930)
Epoch: [23][128/391]	Time 0.474 (0.494)	Data 0.016 (0.013)	Loss 0.5760 (0.8061)	Acc@1 89.062 (81.407)	Acc@5 99.219 (98.922)
Epoch: [23][192/391]	Time 0.539 (0.484)	Data 0.003 (0.010)	Loss 0.8655 (0.8105)	Acc@1 80.469 (81.230)	Acc@5 100.000 (98.948)
Epoch: [23][256/391]	Time 0.377 (0.505)	Data 0.003 (0.008)	Loss 0.6981 (0.8112)	Acc@1 89.062 (81.280)	Acc@5 100.000 (98.976)
Epoch: [23][320/391]	Time 0.561 (0.516)	Data 0.003 (0.007)	Loss 0.8160 (0.8079)	Acc@1 81.250 (81.430)	Acc@5 99.219 (98.983)
Epoch: [23][384/391]	Time 0.698 (0.518)	Data 0.002 (0.007)	Loss 1.0260 (0.8082)	Acc@1 75.781 (81.445)	Acc@5 98.438 (98.989)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.569 (0.569)	Data 1.021 (1.021)	Loss 0.7666 (0.7666)	Acc@1 82.031 (82.031)	Acc@5 98.438 (98.438)
Epoch: [24][64/391]	Time 0.537 (0.730)	Data 0.002 (0.020)	Loss 0.8049 (0.7924)	Acc@1 80.469 (81.971)	Acc@5 99.219 (99.231)
Epoch: [24][128/391]	Time 0.381 (0.640)	Data 0.002 (0.011)	Loss 0.8968 (0.8117)	Acc@1 78.125 (81.256)	Acc@5 98.438 (99.086)
Epoch: [24][192/391]	Time 0.287 (0.580)	Data 0.002 (0.009)	Loss 0.8239 (0.8139)	Acc@1 78.125 (81.137)	Acc@5 99.219 (99.081)
Epoch: [24][256/391]	Time 0.381 (0.587)	Data 0.002 (0.007)	Loss 0.8058 (0.8113)	Acc@1 82.031 (81.274)	Acc@5 98.438 (99.042)
Epoch: [24][320/391]	Time 0.413 (0.558)	Data 0.002 (0.006)	Loss 0.7319 (0.8163)	Acc@1 85.938 (81.087)	Acc@5 100.000 (99.026)
Epoch: [24][384/391]	Time 0.371 (0.536)	Data 0.002 (0.006)	Loss 0.8873 (0.8155)	Acc@1 78.125 (81.108)	Acc@5 96.094 (98.996)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.319 (0.319)	Data 0.451 (0.451)	Loss 0.7883 (0.7883)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [25][64/391]	Time 0.373 (0.393)	Data 0.002 (0.009)	Loss 0.7599 (0.8263)	Acc@1 85.156 (80.397)	Acc@5 98.438 (99.123)
Epoch: [25][128/391]	Time 0.398 (0.404)	Data 0.002 (0.006)	Loss 0.6105 (0.8256)	Acc@1 88.281 (80.342)	Acc@5 99.219 (98.970)
Epoch: [25][192/391]	Time 0.481 (0.402)	Data 0.002 (0.005)	Loss 0.9221 (0.8244)	Acc@1 78.906 (80.538)	Acc@5 99.219 (98.952)
Epoch: [25][256/391]	Time 0.377 (0.414)	Data 0.002 (0.005)	Loss 0.8312 (0.8258)	Acc@1 80.469 (80.554)	Acc@5 99.219 (98.969)
Epoch: [25][320/391]	Time 0.404 (0.415)	Data 0.013 (0.004)	Loss 0.7596 (0.8241)	Acc@1 79.688 (80.719)	Acc@5 99.219 (98.997)
Epoch: [25][384/391]	Time 0.328 (0.415)	Data 0.002 (0.004)	Loss 0.8566 (0.8203)	Acc@1 80.469 (80.848)	Acc@5 99.219 (99.044)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
























 ab hier mit pruneTrain
