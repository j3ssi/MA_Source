no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.414 (0.414)	Data 0.166 (0.166)	Loss 3.0395 (3.0395)	Acc@1 15.625 (15.625)	Acc@5 56.250 (56.250)
Epoch: [1][64/391]	Time 0.506 (0.452)	Data 0.001 (0.004)	Loss 2.6255 (2.7529)	Acc@1 20.312 (20.108)	Acc@5 76.562 (74.062)
Epoch: [1][128/391]	Time 0.505 (0.486)	Data 0.002 (0.003)	Loss 2.4280 (2.5960)	Acc@1 32.031 (23.880)	Acc@5 82.031 (79.482)
Epoch: [1][192/391]	Time 0.590 (0.499)	Data 0.002 (0.003)	Loss 2.2873 (2.5018)	Acc@1 34.375 (26.603)	Acc@5 88.281 (82.157)
Epoch: [1][256/391]	Time 0.512 (0.507)	Data 0.001 (0.002)	Loss 2.1734 (2.4237)	Acc@1 39.062 (29.320)	Acc@5 89.844 (83.873)
Epoch: [1][320/391]	Time 0.546 (0.513)	Data 0.002 (0.002)	Loss 1.8225 (2.3520)	Acc@1 52.344 (32.104)	Acc@5 96.094 (85.278)
Epoch: [1][384/391]	Time 0.529 (0.516)	Data 0.002 (0.002)	Loss 1.9010 (2.2862)	Acc@1 50.000 (34.456)	Acc@5 89.062 (86.494)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.561 (0.561)	Data 0.217 (0.217)	Loss 1.9374 (1.9374)	Acc@1 48.438 (48.438)	Acc@5 94.531 (94.531)
Epoch: [2][64/391]	Time 0.534 (0.498)	Data 0.001 (0.005)	Loss 1.8107 (1.8582)	Acc@1 54.688 (50.841)	Acc@5 90.625 (93.702)
Epoch: [2][128/391]	Time 0.533 (0.510)	Data 0.002 (0.004)	Loss 1.7440 (1.8281)	Acc@1 57.031 (51.908)	Acc@5 92.188 (94.053)
Epoch: [2][192/391]	Time 0.521 (0.515)	Data 0.001 (0.003)	Loss 1.6580 (1.7924)	Acc@1 57.812 (53.004)	Acc@5 92.969 (94.207)
Epoch: [2][256/391]	Time 0.515 (0.519)	Data 0.002 (0.003)	Loss 1.7695 (1.7596)	Acc@1 50.000 (53.885)	Acc@5 96.094 (94.434)
Epoch: [2][320/391]	Time 0.543 (0.521)	Data 0.002 (0.003)	Loss 1.5994 (1.7212)	Acc@1 57.812 (55.072)	Acc@5 95.312 (94.665)
Epoch: [2][384/391]	Time 0.514 (0.522)	Data 0.002 (0.002)	Loss 1.3646 (1.6878)	Acc@1 65.625 (56.045)	Acc@5 97.656 (94.901)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.565 (0.565)	Data 0.224 (0.224)	Loss 1.4793 (1.4793)	Acc@1 63.281 (63.281)	Acc@5 96.094 (96.094)
Epoch: [3][64/391]	Time 0.561 (0.503)	Data 0.002 (0.005)	Loss 1.4715 (1.4607)	Acc@1 59.375 (62.344)	Acc@5 96.875 (96.382)
Epoch: [3][128/391]	Time 0.521 (0.513)	Data 0.001 (0.004)	Loss 1.2688 (1.4480)	Acc@1 68.750 (62.651)	Acc@5 98.438 (96.439)
Epoch: [3][192/391]	Time 0.545 (0.517)	Data 0.002 (0.003)	Loss 1.3626 (1.4277)	Acc@1 64.844 (63.273)	Acc@5 97.656 (96.580)
Epoch: [3][256/391]	Time 0.517 (0.521)	Data 0.002 (0.003)	Loss 1.2452 (1.4038)	Acc@1 75.000 (63.923)	Acc@5 96.875 (96.693)
Epoch: [3][320/391]	Time 0.609 (0.523)	Data 0.002 (0.003)	Loss 1.2400 (1.3815)	Acc@1 67.969 (64.724)	Acc@5 96.094 (96.780)
Epoch: [3][384/391]	Time 0.510 (0.524)	Data 0.001 (0.003)	Loss 1.2585 (1.3635)	Acc@1 72.656 (65.225)	Acc@5 95.312 (96.932)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.560 (0.560)	Data 0.235 (0.235)	Loss 1.2227 (1.2227)	Acc@1 69.531 (69.531)	Acc@5 96.875 (96.875)
Epoch: [4][64/391]	Time 0.451 (0.490)	Data 0.002 (0.006)	Loss 1.2002 (1.2239)	Acc@1 68.750 (69.195)	Acc@5 98.438 (97.548)
Epoch: [4][128/391]	Time 0.453 (0.475)	Data 0.002 (0.004)	Loss 1.2667 (1.2120)	Acc@1 64.844 (69.519)	Acc@5 98.438 (97.529)
Epoch: [4][192/391]	Time 0.459 (0.472)	Data 0.001 (0.003)	Loss 1.1736 (1.1985)	Acc@1 69.531 (69.968)	Acc@5 98.438 (97.571)
Epoch: [4][256/391]	Time 0.459 (0.471)	Data 0.002 (0.003)	Loss 1.1967 (1.1852)	Acc@1 72.656 (70.422)	Acc@5 96.875 (97.614)
Epoch: [4][320/391]	Time 0.505 (0.470)	Data 0.001 (0.003)	Loss 1.0131 (1.1763)	Acc@1 74.219 (70.568)	Acc@5 97.656 (97.668)
Epoch: [4][384/391]	Time 0.429 (0.469)	Data 0.002 (0.003)	Loss 1.0361 (1.1658)	Acc@1 77.344 (70.887)	Acc@5 99.219 (97.752)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.448 (0.448)	Data 0.214 (0.214)	Loss 1.1073 (1.1073)	Acc@1 71.875 (71.875)	Acc@5 99.219 (99.219)
Epoch: [5][64/391]	Time 0.461 (0.468)	Data 0.002 (0.005)	Loss 1.0041 (1.0841)	Acc@1 75.000 (72.788)	Acc@5 100.000 (98.029)
Epoch: [5][128/391]	Time 0.470 (0.466)	Data 0.001 (0.004)	Loss 1.1591 (1.0963)	Acc@1 71.875 (72.608)	Acc@5 97.656 (98.117)
Epoch: [5][192/391]	Time 0.471 (0.465)	Data 0.002 (0.003)	Loss 0.9340 (1.0846)	Acc@1 77.344 (72.907)	Acc@5 100.000 (98.223)
Epoch: [5][256/391]	Time 0.461 (0.465)	Data 0.002 (0.003)	Loss 1.2861 (1.0743)	Acc@1 71.094 (73.173)	Acc@5 98.438 (98.243)
Epoch: [5][320/391]	Time 0.444 (0.464)	Data 0.002 (0.003)	Loss 0.9902 (1.0721)	Acc@1 73.438 (73.321)	Acc@5 100.000 (98.235)
Epoch: [5][384/391]	Time 0.455 (0.464)	Data 0.002 (0.003)	Loss 0.9850 (1.0653)	Acc@1 74.219 (73.486)	Acc@5 97.656 (98.265)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.459 (0.459)	Data 0.219 (0.219)	Loss 1.0913 (1.0913)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [6][64/391]	Time 0.493 (0.467)	Data 0.002 (0.005)	Loss 1.0133 (0.9821)	Acc@1 76.562 (76.322)	Acc@5 99.219 (98.726)
Epoch: [6][128/391]	Time 0.452 (0.462)	Data 0.002 (0.004)	Loss 1.0395 (0.9917)	Acc@1 73.438 (75.939)	Acc@5 98.438 (98.607)
Epoch: [6][192/391]	Time 0.474 (0.464)	Data 0.001 (0.003)	Loss 1.0074 (0.9986)	Acc@1 72.656 (75.631)	Acc@5 100.000 (98.547)
Epoch: [6][256/391]	Time 0.455 (0.464)	Data 0.002 (0.003)	Loss 1.1100 (1.0051)	Acc@1 75.000 (75.319)	Acc@5 100.000 (98.462)
Epoch: [6][320/391]	Time 0.426 (0.465)	Data 0.002 (0.003)	Loss 0.9264 (1.0011)	Acc@1 78.906 (75.399)	Acc@5 99.219 (98.476)
Epoch: [6][384/391]	Time 0.483 (0.466)	Data 0.002 (0.002)	Loss 1.0603 (0.9998)	Acc@1 73.438 (75.483)	Acc@5 100.000 (98.480)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.523 (0.523)	Data 0.195 (0.195)	Loss 0.9548 (0.9548)	Acc@1 76.562 (76.562)	Acc@5 99.219 (99.219)
Epoch: [7][64/391]	Time 0.461 (0.473)	Data 0.002 (0.005)	Loss 1.0228 (0.9750)	Acc@1 75.000 (76.130)	Acc@5 96.094 (98.425)
Epoch: [7][128/391]	Time 0.482 (0.468)	Data 0.001 (0.004)	Loss 1.0224 (0.9740)	Acc@1 75.781 (76.447)	Acc@5 96.094 (98.462)
Epoch: [7][192/391]	Time 0.462 (0.466)	Data 0.003 (0.003)	Loss 0.9874 (0.9804)	Acc@1 75.000 (76.150)	Acc@5 97.656 (98.478)
Epoch: [7][256/391]	Time 0.516 (0.467)	Data 0.003 (0.003)	Loss 0.9436 (0.9748)	Acc@1 76.562 (76.350)	Acc@5 99.219 (98.456)
Epoch: [7][320/391]	Time 0.382 (0.467)	Data 0.002 (0.003)	Loss 0.9343 (0.9738)	Acc@1 79.688 (76.399)	Acc@5 97.656 (98.442)
Epoch: [7][384/391]	Time 0.440 (0.468)	Data 0.002 (0.003)	Loss 0.8711 (0.9729)	Acc@1 78.125 (76.461)	Acc@5 100.000 (98.470)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.481 (0.481)	Data 0.234 (0.234)	Loss 0.7734 (0.7734)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [8][64/391]	Time 0.488 (0.466)	Data 0.002 (0.005)	Loss 1.0667 (0.9639)	Acc@1 74.219 (76.983)	Acc@5 96.875 (98.462)
Epoch: [8][128/391]	Time 0.464 (0.465)	Data 0.002 (0.004)	Loss 0.8725 (0.9582)	Acc@1 82.812 (76.914)	Acc@5 97.656 (98.534)
Epoch: [8][192/391]	Time 0.470 (0.466)	Data 0.002 (0.003)	Loss 0.9459 (0.9590)	Acc@1 75.781 (77.020)	Acc@5 97.656 (98.579)
Epoch: [8][256/391]	Time 0.441 (0.466)	Data 0.002 (0.003)	Loss 1.0138 (0.9591)	Acc@1 74.219 (76.961)	Acc@5 97.656 (98.568)
Epoch: [8][320/391]	Time 0.504 (0.466)	Data 0.002 (0.003)	Loss 0.9989 (0.9554)	Acc@1 71.094 (76.947)	Acc@5 99.219 (98.601)
Epoch: [8][384/391]	Time 0.469 (0.466)	Data 0.002 (0.003)	Loss 1.0315 (0.9534)	Acc@1 72.656 (76.985)	Acc@5 96.875 (98.614)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.474 (0.474)	Data 0.218 (0.218)	Loss 0.8484 (0.8484)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [9][64/391]	Time 0.468 (0.469)	Data 0.002 (0.005)	Loss 1.0218 (0.9208)	Acc@1 72.656 (77.825)	Acc@5 96.875 (98.954)
Epoch: [9][128/391]	Time 0.468 (0.465)	Data 0.002 (0.004)	Loss 0.9858 (0.9416)	Acc@1 75.781 (76.932)	Acc@5 100.000 (98.698)
Epoch: [9][192/391]	Time 0.451 (0.465)	Data 0.002 (0.003)	Loss 0.9749 (0.9458)	Acc@1 74.219 (77.028)	Acc@5 99.219 (98.713)
Epoch: [9][256/391]	Time 0.507 (0.465)	Data 0.002 (0.003)	Loss 0.8825 (0.9471)	Acc@1 79.688 (77.122)	Acc@5 98.438 (98.675)
Epoch: [9][320/391]	Time 0.435 (0.464)	Data 0.002 (0.003)	Loss 0.9520 (0.9440)	Acc@1 75.000 (77.229)	Acc@5 99.219 (98.642)
Epoch: [9][384/391]	Time 0.448 (0.465)	Data 0.002 (0.003)	Loss 1.0929 (0.9407)	Acc@1 74.219 (77.392)	Acc@5 99.219 (98.644)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.519 (0.519)	Data 0.246 (0.246)	Loss 0.8735 (0.8735)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.487 (0.467)	Data 0.002 (0.006)	Loss 0.9794 (0.9372)	Acc@1 72.656 (77.536)	Acc@5 98.438 (98.582)
Epoch: [10][128/391]	Time 0.434 (0.465)	Data 0.002 (0.004)	Loss 1.0254 (0.9299)	Acc@1 73.438 (77.992)	Acc@5 96.094 (98.643)
Epoch: [10][192/391]	Time 0.446 (0.463)	Data 0.002 (0.003)	Loss 0.9221 (0.9325)	Acc@1 76.562 (77.773)	Acc@5 98.438 (98.668)
Epoch: [10][256/391]	Time 0.473 (0.464)	Data 0.002 (0.003)	Loss 0.9121 (0.9241)	Acc@1 81.250 (78.119)	Acc@5 98.438 (98.656)
Epoch: [10][320/391]	Time 0.535 (0.465)	Data 0.002 (0.003)	Loss 0.8703 (0.9256)	Acc@1 78.125 (78.006)	Acc@5 100.000 (98.688)
Epoch: [10][384/391]	Time 0.460 (0.465)	Data 0.002 (0.003)	Loss 0.8528 (0.9265)	Acc@1 78.125 (78.030)	Acc@5 100.000 (98.709)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 448148 ; 487386 ; 0.9194929686121472

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.551 (0.551)	Data 0.218 (0.218)	Loss 0.8907 (0.8907)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [11][64/391]	Time 0.455 (0.465)	Data 0.002 (0.005)	Loss 0.8780 (0.8812)	Acc@1 79.688 (79.651)	Acc@5 100.000 (98.966)
Epoch: [11][128/391]	Time 0.456 (0.463)	Data 0.002 (0.004)	Loss 0.7633 (0.8862)	Acc@1 79.688 (79.191)	Acc@5 100.000 (98.849)
Epoch: [11][192/391]	Time 0.498 (0.463)	Data 0.002 (0.003)	Loss 0.9265 (0.8990)	Acc@1 78.906 (78.748)	Acc@5 99.219 (98.818)
Epoch: [11][256/391]	Time 0.503 (0.465)	Data 0.003 (0.003)	Loss 0.8187 (0.9027)	Acc@1 79.688 (78.736)	Acc@5 99.219 (98.745)
Epoch: [11][320/391]	Time 0.553 (0.467)	Data 0.002 (0.003)	Loss 0.7866 (0.9048)	Acc@1 77.344 (78.639)	Acc@5 100.000 (98.773)
Epoch: [11][384/391]	Time 0.462 (0.466)	Data 0.002 (0.003)	Loss 1.0247 (0.9068)	Acc@1 76.562 (78.632)	Acc@5 98.438 (98.762)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.507 (0.507)	Data 0.290 (0.290)	Loss 0.8752 (0.8752)	Acc@1 77.344 (77.344)	Acc@5 99.219 (99.219)
Epoch: [12][64/391]	Time 0.467 (0.462)	Data 0.002 (0.006)	Loss 0.9454 (0.8673)	Acc@1 76.562 (79.808)	Acc@5 98.438 (99.002)
Epoch: [12][128/391]	Time 0.461 (0.462)	Data 0.002 (0.004)	Loss 0.9796 (0.8952)	Acc@1 74.219 (79.106)	Acc@5 99.219 (98.977)
Epoch: [12][192/391]	Time 0.455 (0.462)	Data 0.002 (0.003)	Loss 0.8422 (0.8933)	Acc@1 82.812 (79.177)	Acc@5 99.219 (98.879)
Epoch: [12][256/391]	Time 0.471 (0.463)	Data 0.002 (0.003)	Loss 0.9173 (0.8954)	Acc@1 78.125 (79.244)	Acc@5 99.219 (98.814)
Epoch: [12][320/391]	Time 0.525 (0.463)	Data 0.002 (0.003)	Loss 0.8677 (0.8960)	Acc@1 78.906 (79.084)	Acc@5 99.219 (98.815)
Epoch: [12][384/391]	Time 0.462 (0.463)	Data 0.002 (0.003)	Loss 0.9858 (0.8951)	Acc@1 75.781 (79.077)	Acc@5 98.438 (98.809)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.446 (0.446)	Data 0.251 (0.251)	Loss 0.9715 (0.9715)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [13][64/391]	Time 0.435 (0.458)	Data 0.002 (0.006)	Loss 0.8443 (0.8721)	Acc@1 81.250 (79.808)	Acc@5 100.000 (99.026)
Epoch: [13][128/391]	Time 0.471 (0.459)	Data 0.002 (0.004)	Loss 0.8953 (0.8781)	Acc@1 77.344 (79.875)	Acc@5 97.656 (99.086)
Epoch: [13][192/391]	Time 0.468 (0.460)	Data 0.002 (0.003)	Loss 0.8617 (0.8791)	Acc@1 78.906 (79.777)	Acc@5 100.000 (99.016)
Epoch: [13][256/391]	Time 0.490 (0.463)	Data 0.002 (0.003)	Loss 1.0504 (0.8782)	Acc@1 70.312 (79.636)	Acc@5 99.219 (98.951)
Epoch: [13][320/391]	Time 0.542 (0.464)	Data 0.002 (0.003)	Loss 0.9021 (0.8784)	Acc@1 75.781 (79.627)	Acc@5 96.875 (98.949)
Epoch: [13][384/391]	Time 0.491 (0.465)	Data 0.002 (0.003)	Loss 0.6275 (0.8831)	Acc@1 89.062 (79.426)	Acc@5 100.000 (98.906)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.496 (0.496)	Data 0.263 (0.263)	Loss 0.7421 (0.7421)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [14][64/391]	Time 0.484 (0.465)	Data 0.002 (0.006)	Loss 0.7216 (0.8630)	Acc@1 82.031 (80.060)	Acc@5 100.000 (98.798)
Epoch: [14][128/391]	Time 0.463 (0.465)	Data 0.002 (0.004)	Loss 0.8328 (0.8811)	Acc@1 78.125 (79.288)	Acc@5 98.438 (98.831)
Epoch: [14][192/391]	Time 0.458 (0.463)	Data 0.003 (0.003)	Loss 0.9049 (0.8749)	Acc@1 78.906 (79.526)	Acc@5 100.000 (98.875)
Epoch: [14][256/391]	Time 0.495 (0.465)	Data 0.002 (0.003)	Loss 0.8114 (0.8849)	Acc@1 83.594 (79.189)	Acc@5 100.000 (98.842)
Epoch: [14][320/391]	Time 0.517 (0.464)	Data 0.002 (0.003)	Loss 0.7430 (0.8878)	Acc@1 82.812 (79.096)	Acc@5 100.000 (98.815)
Epoch: [14][384/391]	Time 0.471 (0.465)	Data 0.002 (0.003)	Loss 1.0351 (0.8864)	Acc@1 76.562 (79.144)	Acc@5 97.656 (98.831)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.575 (0.575)	Data 0.219 (0.219)	Loss 0.9414 (0.9414)	Acc@1 78.125 (78.125)	Acc@5 97.656 (97.656)
Epoch: [15][64/391]	Time 0.443 (0.462)	Data 0.002 (0.005)	Loss 0.9266 (0.8599)	Acc@1 78.906 (80.337)	Acc@5 100.000 (98.930)
Epoch: [15][128/391]	Time 0.461 (0.459)	Data 0.003 (0.004)	Loss 0.9935 (0.8566)	Acc@1 75.000 (80.208)	Acc@5 100.000 (98.946)
Epoch: [15][192/391]	Time 0.479 (0.460)	Data 0.002 (0.003)	Loss 0.8184 (0.8658)	Acc@1 82.031 (79.943)	Acc@5 99.219 (98.907)
Epoch: [15][256/391]	Time 0.448 (0.462)	Data 0.002 (0.003)	Loss 0.8641 (0.8681)	Acc@1 79.688 (79.867)	Acc@5 96.875 (98.909)
Epoch: [15][320/391]	Time 0.513 (0.459)	Data 0.002 (0.003)	Loss 0.9530 (0.8701)	Acc@1 75.781 (79.758)	Acc@5 99.219 (98.936)
Epoch: [15][384/391]	Time 0.470 (0.460)	Data 0.002 (0.003)	Loss 0.8284 (0.8688)	Acc@1 82.031 (79.801)	Acc@5 98.438 (98.941)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 328966 ; 487386 ; 0.6749598880558736

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.449 (0.449)	Data 0.233 (0.233)	Loss 1.0317 (1.0317)	Acc@1 77.344 (77.344)	Acc@5 96.094 (96.094)
Epoch: [16][64/391]	Time 0.453 (0.456)	Data 0.002 (0.006)	Loss 0.9176 (0.8525)	Acc@1 77.344 (80.264)	Acc@5 100.000 (98.786)
Epoch: [16][128/391]	Time 0.437 (0.457)	Data 0.002 (0.004)	Loss 0.7936 (0.8689)	Acc@1 81.250 (79.942)	Acc@5 100.000 (98.801)
Epoch: [16][192/391]	Time 0.474 (0.456)	Data 0.002 (0.003)	Loss 0.9247 (0.8787)	Acc@1 75.000 (79.627)	Acc@5 98.438 (98.778)
Epoch: [16][256/391]	Time 0.492 (0.457)	Data 0.002 (0.003)	Loss 0.9001 (0.8796)	Acc@1 80.469 (79.678)	Acc@5 97.656 (98.778)
Epoch: [16][320/391]	Time 0.421 (0.458)	Data 0.001 (0.003)	Loss 0.8483 (0.8762)	Acc@1 78.906 (79.751)	Acc@5 99.219 (98.817)
Epoch: [16][384/391]	Time 0.508 (0.459)	Data 0.002 (0.003)	Loss 0.8468 (0.8766)	Acc@1 78.906 (79.761)	Acc@5 98.438 (98.845)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.489 (0.489)	Data 0.264 (0.264)	Loss 0.9893 (0.9893)	Acc@1 75.781 (75.781)	Acc@5 99.219 (99.219)
Epoch: [17][64/391]	Time 0.493 (0.461)	Data 0.002 (0.006)	Loss 0.8216 (0.8706)	Acc@1 80.469 (80.180)	Acc@5 100.000 (98.954)
Epoch: [17][128/391]	Time 0.476 (0.459)	Data 0.002 (0.004)	Loss 1.0763 (0.8669)	Acc@1 73.438 (79.972)	Acc@5 98.438 (99.001)
Epoch: [17][192/391]	Time 0.457 (0.459)	Data 0.002 (0.003)	Loss 0.8514 (0.8643)	Acc@1 80.469 (80.230)	Acc@5 98.438 (98.858)
Epoch: [17][256/391]	Time 0.461 (0.459)	Data 0.002 (0.003)	Loss 0.8709 (0.8642)	Acc@1 83.594 (80.408)	Acc@5 96.094 (98.854)
Epoch: [17][320/391]	Time 0.436 (0.459)	Data 0.002 (0.003)	Loss 0.8235 (0.8669)	Acc@1 85.156 (80.354)	Acc@5 98.438 (98.861)
Epoch: [17][384/391]	Time 0.407 (0.460)	Data 0.002 (0.003)	Loss 0.8516 (0.8687)	Acc@1 82.812 (80.260)	Acc@5 99.219 (98.878)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.493 (0.493)	Data 0.207 (0.207)	Loss 0.8390 (0.8390)	Acc@1 82.031 (82.031)	Acc@5 100.000 (100.000)
Epoch: [18][64/391]	Time 0.464 (0.459)	Data 0.002 (0.005)	Loss 0.8451 (0.8414)	Acc@1 82.031 (81.046)	Acc@5 100.000 (98.930)
Epoch: [18][128/391]	Time 0.492 (0.460)	Data 0.002 (0.004)	Loss 1.1219 (0.8736)	Acc@1 76.562 (79.778)	Acc@5 99.219 (98.801)
Epoch: [18][192/391]	Time 0.478 (0.462)	Data 0.002 (0.003)	Loss 0.7886 (0.8691)	Acc@1 82.812 (79.862)	Acc@5 98.438 (98.838)
Epoch: [18][256/391]	Time 0.470 (0.461)	Data 0.002 (0.003)	Loss 0.8556 (0.8708)	Acc@1 83.594 (79.830)	Acc@5 99.219 (98.848)
Epoch: [18][320/391]	Time 0.507 (0.461)	Data 0.002 (0.003)	Loss 0.9269 (0.8691)	Acc@1 73.438 (79.875)	Acc@5 100.000 (98.837)
Epoch: [18][384/391]	Time 0.474 (0.461)	Data 0.001 (0.002)	Loss 0.7013 (0.8689)	Acc@1 85.156 (79.878)	Acc@5 99.219 (98.868)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.508 (0.508)	Data 0.226 (0.226)	Loss 0.8992 (0.8992)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [19][64/391]	Time 0.443 (0.460)	Data 0.002 (0.006)	Loss 0.8079 (0.8526)	Acc@1 81.250 (80.373)	Acc@5 98.438 (98.918)
Epoch: [19][128/391]	Time 0.472 (0.460)	Data 0.002 (0.004)	Loss 0.8763 (0.8530)	Acc@1 78.125 (80.293)	Acc@5 99.219 (98.970)
Epoch: [19][192/391]	Time 0.428 (0.458)	Data 0.002 (0.003)	Loss 0.8672 (0.8502)	Acc@1 80.469 (80.440)	Acc@5 98.438 (98.996)
Epoch: [19][256/391]	Time 0.482 (0.458)	Data 0.002 (0.003)	Loss 0.9959 (0.8562)	Acc@1 78.125 (80.265)	Acc@5 97.656 (98.921)
Epoch: [19][320/391]	Time 0.401 (0.458)	Data 0.002 (0.003)	Loss 0.7259 (0.8559)	Acc@1 82.031 (80.328)	Acc@5 100.000 (98.951)
Epoch: [19][384/391]	Time 0.382 (0.459)	Data 0.002 (0.003)	Loss 0.7739 (0.8554)	Acc@1 82.031 (80.467)	Acc@5 99.219 (98.916)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.478 (0.478)	Data 0.230 (0.230)	Loss 0.7673 (0.7673)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [20][64/391]	Time 0.462 (0.463)	Data 0.002 (0.006)	Loss 0.9087 (0.8247)	Acc@1 74.219 (81.166)	Acc@5 98.438 (98.750)
Epoch: [20][128/391]	Time 0.421 (0.464)	Data 0.002 (0.004)	Loss 0.9891 (0.8479)	Acc@1 77.344 (80.596)	Acc@5 98.438 (98.880)
Epoch: [20][192/391]	Time 0.445 (0.461)	Data 0.002 (0.003)	Loss 1.0461 (0.8536)	Acc@1 74.219 (80.461)	Acc@5 97.656 (98.867)
Epoch: [20][256/391]	Time 0.424 (0.461)	Data 0.002 (0.003)	Loss 0.8326 (0.8535)	Acc@1 80.469 (80.472)	Acc@5 97.656 (98.854)
Epoch: [20][320/391]	Time 0.497 (0.460)	Data 0.002 (0.003)	Loss 0.8823 (0.8533)	Acc@1 78.906 (80.603)	Acc@5 100.000 (98.873)
Epoch: [20][384/391]	Time 0.504 (0.460)	Data 0.002 (0.003)	Loss 0.8550 (0.8562)	Acc@1 80.469 (80.544)	Acc@5 99.219 (98.870)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv28.weight

 RM:  module.conv29.weight

Module List Length:  68
Index1: 56
Index: 29
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: True
Bool2: True
numDelete: 2
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: False
Bool2: True
j: 32; k: 32
Bool1: False
Bool2: True
j: 33; k: 32
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 58: Conv2d(64, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 59: BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 60: Conv2d(55, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 61: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 62 gegen 66: Conv2d(64, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 63 gegen 67: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(55, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  64

Module List Length:  64
Index1: 54
Index: 28
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: True
Bool2: True
numDelete: 2
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 58 gegen 62: Conv2d(64, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 59 gegen 63: BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): AdaptiveAvgPool2d(output_size=(1, 1))
    (59): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  60
Count: 236362 ; 487386 ; 0.484958533893054

Epoch: [21 | 180] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 318, in main
    use_gpu_num)
  File "main.py", line 442, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 459, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 107, in forward
    exponential_average_factor, self.eps)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 2 elements not 0
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.519 (0.519)	Data 0.195 (0.195)	Loss 3.2813 (3.2813)	Acc@1 13.281 (13.281)	Acc@5 51.562 (51.562)
Epoch: [1][64/391]	Time 0.456 (0.462)	Data 0.002 (0.005)	Loss 2.5120 (2.7373)	Acc@1 20.312 (20.925)	Acc@5 82.031 (75.637)
Epoch: [1][128/391]	Time 0.463 (0.460)	Data 0.002 (0.003)	Loss 2.3808 (2.5956)	Acc@1 34.375 (25.157)	Acc@5 86.719 (80.190)
Epoch: [1][192/391]	Time 0.488 (0.462)	Data 0.002 (0.003)	Loss 2.3719 (2.5017)	Acc@1 33.594 (28.291)	Acc@5 87.500 (82.703)
Epoch: [1][256/391]	Time 0.429 (0.463)	Data 0.003 (0.003)	Loss 2.2065 (2.4285)	Acc@1 36.719 (30.782)	Acc@5 92.188 (84.521)
Epoch: [1][320/391]	Time 0.514 (0.464)	Data 0.003 (0.002)	Loss 2.0723 (2.3631)	Acc@1 40.625 (32.893)	Acc@5 90.625 (85.689)
Epoch: [1][384/391]	Time 0.451 (0.465)	Data 0.001 (0.002)	Loss 1.9440 (2.3063)	Acc@1 44.531 (34.844)	Acc@5 93.750 (86.662)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.413 (0.413)	Data 0.247 (0.247)	Loss 1.7285 (1.7285)	Acc@1 53.125 (53.125)	Acc@5 94.531 (94.531)
Epoch: [2][64/391]	Time 0.472 (0.465)	Data 0.002 (0.006)	Loss 1.8025 (1.8680)	Acc@1 50.000 (48.834)	Acc@5 92.188 (93.185)
Epoch: [2][128/391]	Time 0.493 (0.465)	Data 0.002 (0.004)	Loss 1.7228 (1.8314)	Acc@1 53.125 (50.151)	Acc@5 94.531 (93.823)
Epoch: [2][192/391]	Time 0.433 (0.467)	Data 0.002 (0.003)	Loss 1.5836 (1.7894)	Acc@1 57.812 (52.089)	Acc@5 96.875 (94.102)
Epoch: [2][256/391]	Time 0.445 (0.468)	Data 0.002 (0.003)	Loss 1.4967 (1.7461)	Acc@1 64.844 (53.776)	Acc@5 97.656 (94.394)
Epoch: [2][320/391]	Time 0.505 (0.471)	Data 0.002 (0.003)	Loss 1.5849 (1.7105)	Acc@1 57.812 (54.945)	Acc@5 97.656 (94.626)
Epoch: [2][384/391]	Time 0.464 (0.470)	Data 0.002 (0.002)	Loss 1.4898 (1.6706)	Acc@1 59.375 (56.228)	Acc@5 96.875 (94.911)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.484 (0.484)	Data 0.273 (0.273)	Loss 1.3450 (1.3450)	Acc@1 62.500 (62.500)	Acc@5 98.438 (98.438)
Epoch: [3][64/391]	Time 0.489 (0.467)	Data 0.002 (0.006)	Loss 1.4363 (1.4389)	Acc@1 60.938 (63.221)	Acc@5 96.094 (96.466)
Epoch: [3][128/391]	Time 0.504 (0.468)	Data 0.002 (0.004)	Loss 1.3244 (1.4054)	Acc@1 64.062 (64.268)	Acc@5 99.219 (96.748)
Epoch: [3][192/391]	Time 0.430 (0.470)	Data 0.002 (0.003)	Loss 1.2406 (1.3866)	Acc@1 70.312 (64.929)	Acc@5 99.219 (96.879)
Epoch: [3][256/391]	Time 0.436 (0.472)	Data 0.002 (0.003)	Loss 1.1997 (1.3646)	Acc@1 71.875 (65.494)	Acc@5 99.219 (96.987)
Epoch: [3][320/391]	Time 0.465 (0.473)	Data 0.002 (0.003)	Loss 1.1491 (1.3421)	Acc@1 76.562 (66.151)	Acc@5 97.656 (97.077)
Epoch: [3][384/391]	Time 0.452 (0.472)	Data 0.002 (0.003)	Loss 1.4040 (1.3263)	Acc@1 64.844 (66.506)	Acc@5 98.438 (97.173)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.528 (0.528)	Data 0.211 (0.211)	Loss 1.0559 (1.0559)	Acc@1 74.219 (74.219)	Acc@5 97.656 (97.656)
Epoch: [4][64/391]	Time 0.472 (0.471)	Data 0.002 (0.005)	Loss 1.3548 (1.1838)	Acc@1 64.062 (70.625)	Acc@5 96.875 (97.933)
Epoch: [4][128/391]	Time 0.460 (0.472)	Data 0.002 (0.004)	Loss 1.1770 (1.1754)	Acc@1 68.750 (71.130)	Acc@5 98.438 (97.880)
Epoch: [4][192/391]	Time 0.458 (0.473)	Data 0.002 (0.003)	Loss 1.2619 (1.1679)	Acc@1 67.188 (71.207)	Acc@5 96.875 (97.842)
Epoch: [4][256/391]	Time 0.497 (0.475)	Data 0.002 (0.003)	Loss 1.2456 (1.1553)	Acc@1 64.844 (71.474)	Acc@5 99.219 (97.982)
Epoch: [4][320/391]	Time 0.462 (0.474)	Data 0.002 (0.003)	Loss 0.9808 (1.1413)	Acc@1 82.031 (71.829)	Acc@5 97.656 (97.999)
Epoch: [4][384/391]	Time 0.531 (0.474)	Data 0.002 (0.002)	Loss 1.0054 (1.1356)	Acc@1 77.344 (72.037)	Acc@5 98.438 (97.995)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.487 (0.487)	Data 0.250 (0.250)	Loss 1.2769 (1.2769)	Acc@1 66.406 (66.406)	Acc@5 99.219 (99.219)
Epoch: [5][64/391]	Time 0.471 (0.467)	Data 0.002 (0.006)	Loss 0.8421 (1.0432)	Acc@1 84.375 (75.168)	Acc@5 98.438 (98.570)
Epoch: [5][128/391]	Time 0.451 (0.463)	Data 0.003 (0.004)	Loss 0.9397 (1.0396)	Acc@1 78.125 (75.121)	Acc@5 100.000 (98.547)
Epoch: [5][192/391]	Time 0.462 (0.465)	Data 0.001 (0.003)	Loss 1.0021 (1.0467)	Acc@1 76.562 (74.822)	Acc@5 97.656 (98.417)
Epoch: [5][256/391]	Time 0.462 (0.466)	Data 0.002 (0.003)	Loss 0.9268 (1.0458)	Acc@1 78.125 (74.845)	Acc@5 100.000 (98.374)
Epoch: [5][320/391]	Time 0.527 (0.466)	Data 0.002 (0.003)	Loss 1.0043 (1.0456)	Acc@1 78.125 (74.740)	Acc@5 99.219 (98.396)
Epoch: [5][384/391]	Time 0.451 (0.467)	Data 0.002 (0.003)	Loss 1.2642 (1.0396)	Acc@1 69.531 (74.890)	Acc@5 96.094 (98.385)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.522 (0.522)	Data 0.275 (0.275)	Loss 1.0531 (1.0531)	Acc@1 72.656 (72.656)	Acc@5 97.656 (97.656)
Epoch: [6][64/391]	Time 0.519 (0.474)	Data 0.003 (0.006)	Loss 1.0224 (0.9895)	Acc@1 74.219 (76.755)	Acc@5 97.656 (98.546)
Epoch: [6][128/391]	Time 0.456 (0.476)	Data 0.002 (0.004)	Loss 0.9416 (0.9911)	Acc@1 75.781 (76.696)	Acc@5 98.438 (98.516)
Epoch: [6][192/391]	Time 0.452 (0.476)	Data 0.002 (0.003)	Loss 0.9209 (0.9992)	Acc@1 78.125 (76.255)	Acc@5 99.219 (98.454)
Epoch: [6][256/391]	Time 0.440 (0.476)	Data 0.002 (0.003)	Loss 0.9880 (0.9944)	Acc@1 73.438 (76.407)	Acc@5 99.219 (98.501)
Epoch: [6][320/391]	Time 0.399 (0.473)	Data 0.001 (0.003)	Loss 1.0448 (0.9903)	Acc@1 75.000 (76.502)	Acc@5 98.438 (98.486)
Epoch: [6][384/391]	Time 0.475 (0.473)	Data 0.002 (0.003)	Loss 1.0240 (0.9877)	Acc@1 77.344 (76.562)	Acc@5 97.656 (98.519)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.544 (0.544)	Data 0.220 (0.220)	Loss 0.9228 (0.9228)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [7][64/391]	Time 0.479 (0.472)	Data 0.002 (0.005)	Loss 1.0321 (0.9754)	Acc@1 75.000 (77.091)	Acc@5 98.438 (98.582)
Epoch: [7][128/391]	Time 0.453 (0.473)	Data 0.002 (0.004)	Loss 0.9828 (0.9779)	Acc@1 77.344 (76.708)	Acc@5 96.875 (98.498)
Epoch: [7][192/391]	Time 0.491 (0.474)	Data 0.002 (0.003)	Loss 1.1915 (0.9718)	Acc@1 72.656 (76.955)	Acc@5 96.094 (98.563)
Epoch: [7][256/391]	Time 0.417 (0.474)	Data 0.002 (0.003)	Loss 0.9184 (0.9739)	Acc@1 75.000 (76.885)	Acc@5 100.000 (98.614)
Epoch: [7][320/391]	Time 0.510 (0.472)	Data 0.002 (0.003)	Loss 1.0044 (0.9629)	Acc@1 71.875 (77.327)	Acc@5 98.438 (98.659)
Epoch: [7][384/391]	Time 0.486 (0.471)	Data 0.002 (0.003)	Loss 0.8006 (0.9626)	Acc@1 85.938 (77.390)	Acc@5 99.219 (98.596)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.479 (0.479)	Data 0.258 (0.258)	Loss 0.8263 (0.8263)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [8][64/391]	Time 0.446 (0.463)	Data 0.002 (0.006)	Loss 0.9799 (0.9260)	Acc@1 78.125 (78.486)	Acc@5 97.656 (98.786)
Epoch: [8][128/391]	Time 0.487 (0.467)	Data 0.002 (0.004)	Loss 0.8721 (0.9399)	Acc@1 78.906 (77.889)	Acc@5 99.219 (98.692)
Epoch: [8][192/391]	Time 0.459 (0.467)	Data 0.002 (0.003)	Loss 0.9072 (0.9439)	Acc@1 78.125 (77.830)	Acc@5 100.000 (98.632)
Epoch: [8][256/391]	Time 0.529 (0.470)	Data 0.002 (0.003)	Loss 0.8413 (0.9498)	Acc@1 81.250 (77.623)	Acc@5 98.438 (98.626)
Epoch: [8][320/391]	Time 0.436 (0.469)	Data 0.002 (0.003)	Loss 0.9780 (0.9516)	Acc@1 75.000 (77.706)	Acc@5 98.438 (98.591)
Epoch: [8][384/391]	Time 0.436 (0.469)	Data 0.002 (0.003)	Loss 0.9688 (0.9495)	Acc@1 77.344 (77.837)	Acc@5 100.000 (98.563)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.516 (0.516)	Data 0.259 (0.259)	Loss 1.0783 (1.0783)	Acc@1 75.781 (75.781)	Acc@5 96.094 (96.094)
Epoch: [9][64/391]	Time 0.453 (0.466)	Data 0.002 (0.006)	Loss 0.8019 (0.9310)	Acc@1 80.469 (78.137)	Acc@5 100.000 (98.786)
Epoch: [9][128/391]	Time 0.463 (0.470)	Data 0.002 (0.004)	Loss 0.9254 (0.9330)	Acc@1 80.469 (78.379)	Acc@5 97.656 (98.540)
Epoch: [9][192/391]	Time 0.449 (0.468)	Data 0.002 (0.003)	Loss 0.8482 (0.9340)	Acc@1 78.906 (78.295)	Acc@5 97.656 (98.608)
Epoch: [9][256/391]	Time 0.481 (0.468)	Data 0.001 (0.003)	Loss 0.9359 (0.9295)	Acc@1 79.688 (78.456)	Acc@5 96.875 (98.656)
Epoch: [9][320/391]	Time 0.489 (0.469)	Data 0.002 (0.003)	Loss 0.8490 (0.9261)	Acc@1 84.375 (78.583)	Acc@5 97.656 (98.683)
Epoch: [9][384/391]	Time 0.458 (0.470)	Data 0.002 (0.003)	Loss 0.8187 (0.9244)	Acc@1 79.688 (78.563)	Acc@5 99.219 (98.701)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.525 (0.525)	Data 0.226 (0.226)	Loss 0.8774 (0.8774)	Acc@1 81.250 (81.250)	Acc@5 97.656 (97.656)
Epoch: [10][64/391]	Time 0.437 (0.469)	Data 0.001 (0.005)	Loss 0.8503 (0.9039)	Acc@1 82.812 (79.531)	Acc@5 100.000 (98.846)
Epoch: [10][128/391]	Time 0.483 (0.470)	Data 0.001 (0.004)	Loss 0.9509 (0.9097)	Acc@1 75.000 (79.294)	Acc@5 98.438 (98.867)
Epoch: [10][192/391]	Time 0.455 (0.469)	Data 0.001 (0.003)	Loss 0.8143 (0.9102)	Acc@1 80.469 (79.258)	Acc@5 100.000 (98.830)
Epoch: [10][256/391]	Time 0.501 (0.470)	Data 0.002 (0.003)	Loss 0.7593 (0.9197)	Acc@1 85.156 (78.912)	Acc@5 99.219 (98.802)
Epoch: [10][320/391]	Time 0.452 (0.469)	Data 0.002 (0.003)	Loss 0.7191 (0.9160)	Acc@1 86.719 (79.128)	Acc@5 99.219 (98.805)
Epoch: [10][384/391]	Time 0.497 (0.469)	Data 0.002 (0.003)	Loss 0.9506 (0.9179)	Acc@1 75.000 (79.099)	Acc@5 97.656 (98.768)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 478730 ; 487386 ; 0.982239949444588

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.514 (0.514)	Data 0.209 (0.209)	Loss 1.0238 (1.0238)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [11][64/391]	Time 0.520 (0.470)	Data 0.001 (0.005)	Loss 0.9326 (0.9036)	Acc@1 80.469 (79.087)	Acc@5 95.312 (98.798)
Epoch: [11][128/391]	Time 0.476 (0.470)	Data 0.002 (0.004)	Loss 0.8806 (0.9000)	Acc@1 79.688 (79.348)	Acc@5 98.438 (98.740)
Epoch: [11][192/391]	Time 0.453 (0.470)	Data 0.002 (0.003)	Loss 0.9691 (0.9080)	Acc@1 76.562 (79.218)	Acc@5 98.438 (98.729)
Epoch: [11][256/391]	Time 0.453 (0.472)	Data 0.002 (0.003)	Loss 0.8634 (0.9119)	Acc@1 79.688 (79.016)	Acc@5 99.219 (98.790)
Epoch: [11][320/391]	Time 0.412 (0.472)	Data 0.002 (0.003)	Loss 0.7716 (0.9122)	Acc@1 84.375 (78.989)	Acc@5 100.000 (98.810)
Epoch: [11][384/391]	Time 0.469 (0.472)	Data 0.002 (0.002)	Loss 0.8773 (0.9130)	Acc@1 79.688 (79.008)	Acc@5 98.438 (98.797)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.523 (0.523)	Data 0.215 (0.215)	Loss 0.9499 (0.9499)	Acc@1 75.000 (75.000)	Acc@5 99.219 (99.219)
Epoch: [12][64/391]	Time 0.475 (0.467)	Data 0.002 (0.005)	Loss 0.9977 (0.9008)	Acc@1 75.000 (79.243)	Acc@5 100.000 (98.942)
Epoch: [12][128/391]	Time 0.464 (0.466)	Data 0.003 (0.004)	Loss 0.8803 (0.8955)	Acc@1 79.688 (79.572)	Acc@5 99.219 (98.861)
Epoch: [12][192/391]	Time 0.449 (0.465)	Data 0.002 (0.003)	Loss 0.9144 (0.8981)	Acc@1 76.562 (79.574)	Acc@5 96.875 (98.802)
Epoch: [12][256/391]	Time 0.489 (0.466)	Data 0.002 (0.003)	Loss 0.9627 (0.9008)	Acc@1 78.125 (79.529)	Acc@5 97.656 (98.796)
Epoch: [12][320/391]	Time 0.468 (0.466)	Data 0.002 (0.003)	Loss 0.8098 (0.8972)	Acc@1 82.812 (79.780)	Acc@5 100.000 (98.812)
Epoch: [12][384/391]	Time 0.415 (0.467)	Data 0.002 (0.002)	Loss 0.7206 (0.8984)	Acc@1 85.938 (79.604)	Acc@5 98.438 (98.815)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.510 (0.510)	Data 0.215 (0.215)	Loss 0.7208 (0.7208)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [13][64/391]	Time 0.441 (0.466)	Data 0.002 (0.005)	Loss 0.8521 (0.8711)	Acc@1 82.031 (80.529)	Acc@5 99.219 (99.038)
Epoch: [13][128/391]	Time 0.441 (0.465)	Data 0.002 (0.004)	Loss 0.7745 (0.8820)	Acc@1 83.594 (80.269)	Acc@5 99.219 (98.934)
Epoch: [13][192/391]	Time 0.462 (0.468)	Data 0.002 (0.003)	Loss 0.6848 (0.8830)	Acc@1 87.500 (80.028)	Acc@5 99.219 (98.907)
Epoch: [13][256/391]	Time 0.441 (0.470)	Data 0.002 (0.003)	Loss 0.9019 (0.8817)	Acc@1 79.688 (80.025)	Acc@5 98.438 (98.951)
Epoch: [13][320/391]	Time 0.454 (0.469)	Data 0.002 (0.003)	Loss 0.9545 (0.8864)	Acc@1 81.250 (79.892)	Acc@5 97.656 (98.888)
Epoch: [13][384/391]	Time 0.445 (0.468)	Data 0.002 (0.003)	Loss 0.9607 (0.8900)	Acc@1 76.562 (79.708)	Acc@5 98.438 (98.894)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.509 (0.509)	Data 0.237 (0.237)	Loss 0.8152 (0.8152)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [14][64/391]	Time 0.559 (0.470)	Data 0.002 (0.006)	Loss 0.9375 (0.8586)	Acc@1 78.906 (81.238)	Acc@5 100.000 (99.038)
Epoch: [14][128/391]	Time 0.473 (0.470)	Data 0.002 (0.004)	Loss 0.8977 (0.8794)	Acc@1 78.906 (80.178)	Acc@5 98.438 (98.934)
Epoch: [14][192/391]	Time 0.463 (0.467)	Data 0.002 (0.003)	Loss 0.8613 (0.8852)	Acc@1 81.250 (80.048)	Acc@5 100.000 (98.875)
Epoch: [14][256/391]	Time 0.465 (0.468)	Data 0.002 (0.003)	Loss 1.0827 (0.8864)	Acc@1 75.781 (80.013)	Acc@5 97.656 (98.851)
Epoch: [14][320/391]	Time 0.467 (0.469)	Data 0.002 (0.003)	Loss 0.8111 (0.8818)	Acc@1 82.031 (80.276)	Acc@5 99.219 (98.866)
Epoch: [14][384/391]	Time 0.456 (0.469)	Data 0.002 (0.003)	Loss 0.8671 (0.8819)	Acc@1 76.562 (80.211)	Acc@5 98.438 (98.878)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.472 (0.472)	Data 0.281 (0.281)	Loss 0.8960 (0.8960)	Acc@1 78.906 (78.906)	Acc@5 100.000 (100.000)
Epoch: [15][64/391]	Time 0.482 (0.470)	Data 0.002 (0.006)	Loss 0.8507 (0.8868)	Acc@1 82.031 (79.639)	Acc@5 97.656 (98.894)
Epoch: [15][128/391]	Time 0.438 (0.471)	Data 0.003 (0.004)	Loss 0.9622 (0.8719)	Acc@1 73.438 (80.438)	Acc@5 98.438 (98.837)
Epoch: [15][192/391]	Time 0.459 (0.468)	Data 0.002 (0.003)	Loss 0.8165 (0.8708)	Acc@1 78.906 (80.501)	Acc@5 98.438 (98.903)
Epoch: [15][256/391]	Time 0.452 (0.470)	Data 0.002 (0.003)	Loss 1.0146 (0.8768)	Acc@1 74.219 (80.195)	Acc@5 99.219 (98.927)
Epoch: [15][320/391]	Time 0.468 (0.469)	Data 0.002 (0.003)	Loss 1.0014 (0.8772)	Acc@1 76.562 (80.196)	Acc@5 96.875 (98.922)
Epoch: [15][384/391]	Time 0.477 (0.469)	Data 0.002 (0.003)	Loss 0.8327 (0.8773)	Acc@1 82.812 (80.154)	Acc@5 100.000 (98.900)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 416368 ; 487386 ; 0.8542879770859236

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.532 (0.532)	Data 0.252 (0.252)	Loss 0.7388 (0.7388)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [16][64/391]	Time 0.462 (0.470)	Data 0.002 (0.006)	Loss 0.7243 (0.8167)	Acc@1 89.062 (82.308)	Acc@5 99.219 (99.219)
Epoch: [16][128/391]	Time 0.454 (0.469)	Data 0.002 (0.004)	Loss 1.0155 (0.8400)	Acc@1 78.125 (81.323)	Acc@5 96.875 (99.152)
Epoch: [16][192/391]	Time 0.433 (0.472)	Data 0.002 (0.003)	Loss 0.9666 (0.8590)	Acc@1 76.562 (80.704)	Acc@5 98.438 (99.004)
Epoch: [16][256/391]	Time 0.471 (0.471)	Data 0.002 (0.003)	Loss 0.8300 (0.8610)	Acc@1 80.469 (80.612)	Acc@5 100.000 (98.966)
Epoch: [16][320/391]	Time 0.445 (0.469)	Data 0.003 (0.003)	Loss 0.7044 (0.8632)	Acc@1 86.719 (80.564)	Acc@5 100.000 (98.941)
Epoch: [16][384/391]	Time 0.459 (0.468)	Data 0.002 (0.003)	Loss 0.7229 (0.8651)	Acc@1 89.062 (80.538)	Acc@5 99.219 (98.935)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.487 (0.487)	Data 0.286 (0.286)	Loss 0.8678 (0.8678)	Acc@1 78.906 (78.906)	Acc@5 100.000 (100.000)
Epoch: [17][64/391]	Time 0.475 (0.468)	Data 0.003 (0.006)	Loss 0.8072 (0.8347)	Acc@1 81.250 (81.418)	Acc@5 99.219 (99.183)
Epoch: [17][128/391]	Time 0.468 (0.466)	Data 0.002 (0.004)	Loss 0.8514 (0.8428)	Acc@1 79.688 (81.426)	Acc@5 98.438 (98.977)
Epoch: [17][192/391]	Time 0.464 (0.465)	Data 0.001 (0.003)	Loss 0.8045 (0.8438)	Acc@1 80.469 (81.181)	Acc@5 100.000 (99.004)
Epoch: [17][256/391]	Time 0.497 (0.467)	Data 0.002 (0.003)	Loss 0.7180 (0.8376)	Acc@1 84.375 (81.369)	Acc@5 100.000 (99.006)
Epoch: [17][320/391]	Time 0.474 (0.466)	Data 0.002 (0.003)	Loss 0.8695 (0.8430)	Acc@1 81.250 (81.177)	Acc@5 98.438 (98.995)
Epoch: [17][384/391]	Time 0.476 (0.466)	Data 0.002 (0.003)	Loss 0.7143 (0.8480)	Acc@1 86.719 (81.035)	Acc@5 98.438 (98.985)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.493 (0.493)	Data 0.224 (0.224)	Loss 0.8260 (0.8260)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [18][64/391]	Time 0.494 (0.475)	Data 0.002 (0.005)	Loss 0.9018 (0.8316)	Acc@1 78.125 (81.394)	Acc@5 97.656 (99.195)
Epoch: [18][128/391]	Time 0.504 (0.470)	Data 0.002 (0.004)	Loss 0.7857 (0.8449)	Acc@1 85.938 (80.947)	Acc@5 100.000 (99.079)
Epoch: [18][192/391]	Time 0.464 (0.468)	Data 0.002 (0.003)	Loss 0.7092 (0.8459)	Acc@1 85.938 (80.817)	Acc@5 99.219 (98.980)
Epoch: [18][256/391]	Time 0.471 (0.468)	Data 0.002 (0.003)	Loss 1.0371 (0.8552)	Acc@1 71.094 (80.621)	Acc@5 98.438 (98.954)
Epoch: [18][320/391]	Time 0.475 (0.468)	Data 0.002 (0.003)	Loss 0.8323 (0.8544)	Acc@1 79.688 (80.702)	Acc@5 100.000 (98.949)
Epoch: [18][384/391]	Time 0.444 (0.467)	Data 0.001 (0.003)	Loss 0.9214 (0.8509)	Acc@1 78.125 (80.848)	Acc@5 99.219 (98.977)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.513 (0.513)	Data 0.241 (0.241)	Loss 0.9995 (0.9995)	Acc@1 75.781 (75.781)	Acc@5 97.656 (97.656)
Epoch: [19][64/391]	Time 0.462 (0.469)	Data 0.002 (0.006)	Loss 0.8582 (0.8405)	Acc@1 81.250 (81.382)	Acc@5 100.000 (99.171)
Epoch: [19][128/391]	Time 0.476 (0.464)	Data 0.002 (0.004)	Loss 0.8748 (0.8363)	Acc@1 80.469 (81.547)	Acc@5 99.219 (99.134)
Epoch: [19][192/391]	Time 0.416 (0.464)	Data 0.002 (0.003)	Loss 0.9016 (0.8463)	Acc@1 76.562 (81.031)	Acc@5 97.656 (99.061)
Epoch: [19][256/391]	Time 0.471 (0.463)	Data 0.002 (0.003)	Loss 0.9031 (0.8495)	Acc@1 80.469 (80.925)	Acc@5 100.000 (99.033)
Epoch: [19][320/391]	Time 0.479 (0.464)	Data 0.002 (0.003)	Loss 0.8065 (0.8469)	Acc@1 85.156 (81.048)	Acc@5 99.219 (99.007)
Epoch: [19][384/391]	Time 0.461 (0.464)	Data 0.002 (0.003)	Loss 0.8805 (0.8480)	Acc@1 78.125 (80.938)	Acc@5 98.438 (98.994)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.447 (0.447)	Data 0.315 (0.315)	Loss 0.6512 (0.6512)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [20][64/391]	Time 0.440 (0.470)	Data 0.002 (0.007)	Loss 0.8476 (0.8234)	Acc@1 82.031 (81.418)	Acc@5 100.000 (99.014)
Epoch: [20][128/391]	Time 0.461 (0.465)	Data 0.002 (0.004)	Loss 0.7379 (0.8317)	Acc@1 85.938 (81.111)	Acc@5 99.219 (99.001)
Epoch: [20][192/391]	Time 0.466 (0.463)	Data 0.002 (0.004)	Loss 0.7434 (0.8380)	Acc@1 83.594 (81.011)	Acc@5 100.000 (98.948)
Epoch: [20][256/391]	Time 0.460 (0.463)	Data 0.002 (0.003)	Loss 0.7401 (0.8377)	Acc@1 85.156 (81.086)	Acc@5 100.000 (98.945)
Epoch: [20][320/391]	Time 0.433 (0.463)	Data 0.002 (0.003)	Loss 0.6697 (0.8395)	Acc@1 85.938 (81.109)	Acc@5 100.000 (98.951)
Epoch: [20][384/391]	Time 0.472 (0.463)	Data 0.001 (0.003)	Loss 0.8848 (0.8397)	Acc@1 82.812 (81.078)	Acc@5 98.438 (98.951)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 360358 ; 487386 ; 0.7393687959851124

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.467 (0.467)	Data 0.230 (0.230)	Loss 0.6700 (0.6700)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [21][64/391]	Time 0.462 (0.465)	Data 0.002 (0.006)	Loss 0.7005 (0.8181)	Acc@1 83.594 (82.200)	Acc@5 100.000 (99.099)
Epoch: [21][128/391]	Time 0.451 (0.461)	Data 0.002 (0.004)	Loss 0.8554 (0.8321)	Acc@1 80.469 (81.535)	Acc@5 100.000 (98.958)
Epoch: [21][192/391]	Time 0.516 (0.463)	Data 0.002 (0.003)	Loss 0.8696 (0.8262)	Acc@1 77.344 (81.784)	Acc@5 99.219 (99.008)
Epoch: [21][256/391]	Time 0.452 (0.462)	Data 0.002 (0.003)	Loss 0.7876 (0.8278)	Acc@1 82.031 (81.648)	Acc@5 100.000 (99.055)
Epoch: [21][320/391]	Time 0.471 (0.461)	Data 0.002 (0.003)	Loss 0.6746 (0.8314)	Acc@1 85.938 (81.562)	Acc@5 100.000 (99.031)
Epoch: [21][384/391]	Time 0.465 (0.461)	Data 0.002 (0.003)	Loss 1.0194 (0.8308)	Acc@1 75.781 (81.597)	Acc@5 96.875 (99.006)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.517 (0.517)	Data 0.281 (0.281)	Loss 0.8733 (0.8733)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [22][64/391]	Time 0.473 (0.464)	Data 0.002 (0.006)	Loss 0.8325 (0.8227)	Acc@1 82.812 (81.875)	Acc@5 99.219 (99.026)
Epoch: [22][128/391]	Time 0.433 (0.461)	Data 0.002 (0.004)	Loss 0.8124 (0.8312)	Acc@1 82.031 (81.583)	Acc@5 99.219 (98.916)
Epoch: [22][192/391]	Time 0.457 (0.463)	Data 0.002 (0.003)	Loss 0.9698 (0.8246)	Acc@1 76.562 (81.554)	Acc@5 100.000 (99.033)
Epoch: [22][256/391]	Time 0.466 (0.462)	Data 0.002 (0.003)	Loss 0.9972 (0.8210)	Acc@1 78.906 (81.560)	Acc@5 96.875 (99.042)
Epoch: [22][320/391]	Time 0.471 (0.461)	Data 0.004 (0.003)	Loss 0.7066 (0.8231)	Acc@1 85.156 (81.564)	Acc@5 100.000 (99.029)
Epoch: [22][384/391]	Time 0.472 (0.461)	Data 0.001 (0.003)	Loss 0.8826 (0.8294)	Acc@1 78.906 (81.301)	Acc@5 98.438 (99.002)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.505 (0.505)	Data 0.285 (0.285)	Loss 0.8333 (0.8333)	Acc@1 78.125 (78.125)	Acc@5 100.000 (100.000)
Epoch: [23][64/391]	Time 0.443 (0.461)	Data 0.002 (0.006)	Loss 0.7911 (0.8130)	Acc@1 82.031 (81.971)	Acc@5 99.219 (99.062)
Epoch: [23][128/391]	Time 0.432 (0.462)	Data 0.002 (0.004)	Loss 0.8471 (0.8157)	Acc@1 81.250 (82.049)	Acc@5 99.219 (98.989)
Epoch: [23][192/391]	Time 0.345 (0.463)	Data 0.003 (0.003)	Loss 0.7243 (0.8214)	Acc@1 86.719 (81.954)	Acc@5 99.219 (98.960)
Epoch: [23][256/391]	Time 0.426 (0.462)	Data 0.002 (0.003)	Loss 0.7540 (0.8202)	Acc@1 82.031 (81.885)	Acc@5 98.438 (98.979)
Epoch: [23][320/391]	Time 0.474 (0.463)	Data 0.002 (0.003)	Loss 0.9184 (0.8239)	Acc@1 82.812 (81.722)	Acc@5 99.219 (98.983)
Epoch: [23][384/391]	Time 0.465 (0.463)	Data 0.002 (0.003)	Loss 0.7638 (0.8277)	Acc@1 82.812 (81.652)	Acc@5 100.000 (98.967)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.508 (0.508)	Data 0.291 (0.291)	Loss 0.7327 (0.7327)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [24][64/391]	Time 0.442 (0.465)	Data 0.003 (0.007)	Loss 0.8101 (0.8259)	Acc@1 83.594 (81.827)	Acc@5 98.438 (99.075)
Epoch: [24][128/391]	Time 0.473 (0.463)	Data 0.002 (0.004)	Loss 0.6831 (0.8197)	Acc@1 89.844 (82.080)	Acc@5 99.219 (99.086)
Epoch: [24][192/391]	Time 0.476 (0.464)	Data 0.002 (0.004)	Loss 0.7670 (0.8216)	Acc@1 82.031 (81.865)	Acc@5 98.438 (99.085)
Epoch: [24][256/391]	Time 0.455 (0.463)	Data 0.002 (0.003)	Loss 0.7606 (0.8167)	Acc@1 82.031 (82.040)	Acc@5 100.000 (99.125)
Epoch: [24][320/391]	Time 0.422 (0.462)	Data 0.002 (0.003)	Loss 0.8217 (0.8197)	Acc@1 78.906 (81.902)	Acc@5 98.438 (99.131)
Epoch: [24][384/391]	Time 0.454 (0.461)	Data 0.002 (0.003)	Loss 1.0530 (0.8200)	Acc@1 75.000 (81.857)	Acc@5 97.656 (99.111)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.528 (0.528)	Data 0.328 (0.328)	Loss 0.7002 (0.7002)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [25][64/391]	Time 0.410 (0.455)	Data 0.002 (0.007)	Loss 0.6628 (0.8074)	Acc@1 90.625 (81.887)	Acc@5 99.219 (98.966)
Epoch: [25][128/391]	Time 0.467 (0.454)	Data 0.002 (0.004)	Loss 0.7515 (0.8085)	Acc@1 83.594 (81.910)	Acc@5 100.000 (98.952)
Epoch: [25][192/391]	Time 0.475 (0.460)	Data 0.002 (0.004)	Loss 0.7451 (0.8113)	Acc@1 81.250 (81.695)	Acc@5 100.000 (98.964)
Epoch: [25][256/391]	Time 0.477 (0.461)	Data 0.002 (0.003)	Loss 0.8366 (0.8119)	Acc@1 78.906 (81.542)	Acc@5 100.000 (99.012)
Epoch: [25][320/391]	Time 0.479 (0.460)	Data 0.002 (0.003)	Loss 0.8645 (0.8143)	Acc@1 79.688 (81.437)	Acc@5 99.219 (98.992)
Epoch: [25][384/391]	Time 0.471 (0.460)	Data 0.002 (0.003)	Loss 0.9526 (0.8100)	Acc@1 77.344 (81.565)	Acc@5 98.438 (99.004)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.551 (0.551)	Data 0.189 (0.189)	Loss 3.1294 (3.1294)	Acc@1 5.469 (5.469)	Acc@5 51.562 (51.562)
Epoch: [1][64/391]	Time 0.440 (0.466)	Data 0.002 (0.005)	Loss 2.6538 (2.8803)	Acc@1 18.750 (18.329)	Acc@5 85.156 (71.118)
Epoch: [1][128/391]	Time 0.534 (0.462)	Data 0.002 (0.003)	Loss 2.4618 (2.7079)	Acc@1 23.438 (21.439)	Acc@5 84.375 (76.593)
Epoch: [1][192/391]	Time 0.481 (0.464)	Data 0.001 (0.003)	Loss 2.3441 (2.6046)	Acc@1 32.812 (24.093)	Acc@5 86.719 (79.635)
Epoch: [1][256/391]	Time 0.451 (0.462)	Data 0.001 (0.003)	Loss 2.0691 (2.5222)	Acc@1 37.500 (26.572)	Acc@5 96.094 (81.609)
Epoch: [1][320/391]	Time 0.450 (0.462)	Data 0.002 (0.002)	Loss 2.1074 (2.4555)	Acc@1 39.062 (28.760)	Acc@5 89.062 (82.983)
Epoch: [1][384/391]	Time 0.429 (0.461)	Data 0.002 (0.002)	Loss 2.0637 (2.3914)	Acc@1 41.406 (30.901)	Acc@5 93.750 (84.227)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.487 (0.487)	Data 0.231 (0.231)	Loss 1.9777 (1.9777)	Acc@1 42.969 (42.969)	Acc@5 94.531 (94.531)
Epoch: [2][64/391]	Time 0.437 (0.464)	Data 0.002 (0.005)	Loss 1.9126 (1.9532)	Acc@1 48.438 (45.300)	Acc@5 92.188 (92.248)
Epoch: [2][128/391]	Time 0.421 (0.461)	Data 0.002 (0.004)	Loss 1.7549 (1.8966)	Acc@1 52.344 (48.117)	Acc@5 95.312 (92.593)
Epoch: [2][192/391]	Time 0.398 (0.456)	Data 0.002 (0.003)	Loss 1.7496 (1.8465)	Acc@1 51.562 (49.648)	Acc@5 95.312 (93.123)
Epoch: [2][256/391]	Time 0.406 (0.441)	Data 0.002 (0.003)	Loss 1.5315 (1.8002)	Acc@1 58.594 (51.192)	Acc@5 95.312 (93.555)
Epoch: [2][320/391]	Time 0.395 (0.432)	Data 0.002 (0.003)	Loss 1.4095 (1.7570)	Acc@1 64.062 (52.429)	Acc@5 100.000 (93.989)
Epoch: [2][384/391]	Time 0.406 (0.427)	Data 0.002 (0.002)	Loss 1.4349 (1.7163)	Acc@1 60.156 (53.649)	Acc@5 96.094 (94.326)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.491 (0.491)	Data 0.237 (0.237)	Loss 1.2676 (1.2676)	Acc@1 72.656 (72.656)	Acc@5 97.656 (97.656)
Epoch: [3][64/391]	Time 0.413 (0.402)	Data 0.002 (0.005)	Loss 1.3450 (1.4643)	Acc@1 66.406 (61.262)	Acc@5 97.656 (96.334)
Epoch: [3][128/391]	Time 0.384 (0.402)	Data 0.002 (0.004)	Loss 1.3639 (1.4490)	Acc@1 65.625 (61.622)	Acc@5 96.094 (96.403)
Epoch: [3][192/391]	Time 0.356 (0.401)	Data 0.002 (0.003)	Loss 1.3013 (1.4172)	Acc@1 67.969 (62.775)	Acc@5 95.312 (96.596)
Epoch: [3][256/391]	Time 0.437 (0.401)	Data 0.001 (0.003)	Loss 1.3963 (1.3952)	Acc@1 64.062 (63.366)	Acc@5 98.438 (96.650)
Epoch: [3][320/391]	Time 0.434 (0.401)	Data 0.002 (0.003)	Loss 1.1299 (1.3796)	Acc@1 70.312 (63.890)	Acc@5 98.438 (96.741)
Epoch: [3][384/391]	Time 0.402 (0.400)	Data 0.002 (0.002)	Loss 1.3178 (1.3524)	Acc@1 66.406 (64.742)	Acc@5 98.438 (96.875)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.435 (0.435)	Data 0.245 (0.245)	Loss 1.2099 (1.2099)	Acc@1 67.969 (67.969)	Acc@5 96.094 (96.094)
Epoch: [4][64/391]	Time 0.345 (0.400)	Data 0.002 (0.006)	Loss 1.2001 (1.1999)	Acc@1 68.750 (68.498)	Acc@5 95.312 (97.524)
Epoch: [4][128/391]	Time 0.445 (0.402)	Data 0.002 (0.004)	Loss 1.1752 (1.1999)	Acc@1 73.438 (69.077)	Acc@5 98.438 (97.493)
Epoch: [4][192/391]	Time 0.415 (0.400)	Data 0.002 (0.003)	Loss 1.0196 (1.1785)	Acc@1 75.781 (69.815)	Acc@5 98.438 (97.571)
Epoch: [4][256/391]	Time 0.424 (0.400)	Data 0.002 (0.003)	Loss 1.0679 (1.1629)	Acc@1 73.438 (70.273)	Acc@5 98.438 (97.705)
Epoch: [4][320/391]	Time 0.425 (0.400)	Data 0.002 (0.003)	Loss 1.0077 (1.1509)	Acc@1 77.344 (70.656)	Acc@5 98.438 (97.756)
Epoch: [4][384/391]	Time 0.431 (0.401)	Data 0.001 (0.003)	Loss 0.9338 (1.1423)	Acc@1 78.125 (70.899)	Acc@5 96.875 (97.810)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.426 (0.426)	Data 0.250 (0.250)	Loss 1.1883 (1.1883)	Acc@1 69.531 (69.531)	Acc@5 98.438 (98.438)
Epoch: [5][64/391]	Time 0.407 (0.398)	Data 0.001 (0.006)	Loss 0.9760 (1.0635)	Acc@1 77.344 (73.486)	Acc@5 98.438 (98.041)
Epoch: [5][128/391]	Time 0.377 (0.404)	Data 0.002 (0.004)	Loss 1.0633 (1.0612)	Acc@1 71.094 (73.583)	Acc@5 100.000 (98.098)
Epoch: [5][192/391]	Time 0.409 (0.402)	Data 0.002 (0.003)	Loss 0.9943 (1.0529)	Acc@1 78.125 (73.753)	Acc@5 98.438 (98.195)
Epoch: [5][256/391]	Time 0.389 (0.402)	Data 0.002 (0.003)	Loss 1.1431 (1.0486)	Acc@1 70.312 (73.821)	Acc@5 96.094 (98.158)
Epoch: [5][320/391]	Time 0.400 (0.401)	Data 0.001 (0.003)	Loss 0.7819 (1.0406)	Acc@1 83.594 (74.065)	Acc@5 100.000 (98.221)
Epoch: [5][384/391]	Time 0.398 (0.401)	Data 0.002 (0.003)	Loss 1.0713 (1.0322)	Acc@1 75.000 (74.416)	Acc@5 98.438 (98.243)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.500 (0.500)	Data 0.283 (0.283)	Loss 0.8768 (0.8768)	Acc@1 77.344 (77.344)	Acc@5 100.000 (100.000)
Epoch: [6][64/391]	Time 0.378 (0.406)	Data 0.002 (0.006)	Loss 0.8583 (0.9630)	Acc@1 79.688 (76.430)	Acc@5 99.219 (98.690)
Epoch: [6][128/391]	Time 0.392 (0.409)	Data 0.002 (0.004)	Loss 1.1492 (0.9789)	Acc@1 71.875 (75.600)	Acc@5 97.656 (98.504)
Epoch: [6][192/391]	Time 0.369 (0.405)	Data 0.002 (0.003)	Loss 1.1294 (0.9770)	Acc@1 71.875 (75.834)	Acc@5 96.875 (98.502)
Epoch: [6][256/391]	Time 0.408 (0.404)	Data 0.002 (0.003)	Loss 1.0127 (0.9762)	Acc@1 78.125 (75.930)	Acc@5 96.875 (98.489)
Epoch: [6][320/391]	Time 0.376 (0.403)	Data 0.002 (0.003)	Loss 0.8840 (0.9763)	Acc@1 80.469 (75.976)	Acc@5 99.219 (98.498)
Epoch: [6][384/391]	Time 0.392 (0.403)	Data 0.002 (0.003)	Loss 1.1895 (0.9746)	Acc@1 71.094 (76.086)	Acc@5 96.875 (98.482)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.504 (0.504)	Data 0.237 (0.237)	Loss 0.8928 (0.8928)	Acc@1 76.562 (76.562)	Acc@5 99.219 (99.219)
Epoch: [7][64/391]	Time 0.433 (0.403)	Data 0.002 (0.006)	Loss 0.9278 (0.9156)	Acc@1 77.344 (78.065)	Acc@5 99.219 (98.438)
Epoch: [7][128/391]	Time 0.420 (0.403)	Data 0.002 (0.004)	Loss 0.9301 (0.9430)	Acc@1 75.000 (77.095)	Acc@5 98.438 (98.438)
Epoch: [7][192/391]	Time 0.418 (0.403)	Data 0.002 (0.003)	Loss 1.1215 (0.9465)	Acc@1 70.312 (76.927)	Acc@5 97.656 (98.490)
Epoch: [7][256/391]	Time 0.398 (0.402)	Data 0.002 (0.003)	Loss 1.0094 (0.9475)	Acc@1 75.000 (76.930)	Acc@5 97.656 (98.489)
Epoch: [7][320/391]	Time 0.416 (0.401)	Data 0.002 (0.003)	Loss 0.9211 (0.9440)	Acc@1 79.688 (77.022)	Acc@5 97.656 (98.506)
Epoch: [7][384/391]	Time 0.413 (0.401)	Data 0.002 (0.003)	Loss 0.8439 (0.9434)	Acc@1 84.375 (77.119)	Acc@5 97.656 (98.537)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.414 (0.414)	Data 0.288 (0.288)	Loss 0.7882 (0.7882)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [8][64/391]	Time 0.399 (0.400)	Data 0.002 (0.006)	Loss 0.8393 (0.9125)	Acc@1 83.594 (78.053)	Acc@5 98.438 (98.930)
Epoch: [8][128/391]	Time 0.400 (0.402)	Data 0.002 (0.004)	Loss 0.8438 (0.9169)	Acc@1 80.469 (77.998)	Acc@5 99.219 (98.752)
Epoch: [8][192/391]	Time 0.388 (0.403)	Data 0.002 (0.003)	Loss 1.0789 (0.9200)	Acc@1 70.312 (77.971)	Acc@5 98.438 (98.741)
Epoch: [8][256/391]	Time 0.386 (0.403)	Data 0.001 (0.003)	Loss 1.0318 (0.9191)	Acc@1 75.000 (78.022)	Acc@5 98.438 (98.720)
Epoch: [8][320/391]	Time 0.415 (0.402)	Data 0.003 (0.003)	Loss 0.9795 (0.9230)	Acc@1 74.219 (77.869)	Acc@5 98.438 (98.666)
Epoch: [8][384/391]	Time 0.346 (0.402)	Data 0.002 (0.003)	Loss 0.8228 (0.9250)	Acc@1 82.812 (77.873)	Acc@5 98.438 (98.647)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.503 (0.503)	Data 0.271 (0.271)	Loss 0.7286 (0.7286)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [9][64/391]	Time 0.412 (0.407)	Data 0.002 (0.006)	Loss 0.7627 (0.9185)	Acc@1 84.375 (78.498)	Acc@5 99.219 (98.750)
Epoch: [9][128/391]	Time 0.417 (0.406)	Data 0.001 (0.004)	Loss 0.6971 (0.9005)	Acc@1 88.281 (78.937)	Acc@5 99.219 (98.831)
Epoch: [9][192/391]	Time 0.457 (0.404)	Data 0.002 (0.003)	Loss 0.9194 (0.9043)	Acc@1 78.906 (78.821)	Acc@5 99.219 (98.749)
Epoch: [9][256/391]	Time 0.396 (0.403)	Data 0.002 (0.003)	Loss 0.8525 (0.9028)	Acc@1 81.250 (78.754)	Acc@5 99.219 (98.729)
Epoch: [9][320/391]	Time 0.420 (0.403)	Data 0.002 (0.003)	Loss 0.7599 (0.9027)	Acc@1 83.594 (78.753)	Acc@5 98.438 (98.703)
Epoch: [9][384/391]	Time 0.447 (0.401)	Data 0.002 (0.003)	Loss 0.9115 (0.9044)	Acc@1 78.125 (78.738)	Acc@5 98.438 (98.716)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.468 (0.468)	Data 0.278 (0.278)	Loss 0.8921 (0.8921)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.371 (0.408)	Data 0.002 (0.006)	Loss 0.8065 (0.8953)	Acc@1 82.812 (79.423)	Acc@5 99.219 (98.834)
Epoch: [10][128/391]	Time 0.408 (0.410)	Data 0.002 (0.004)	Loss 1.0206 (0.9022)	Acc@1 73.438 (79.064)	Acc@5 96.875 (98.892)
Epoch: [10][192/391]	Time 0.419 (0.407)	Data 0.002 (0.003)	Loss 0.8059 (0.9117)	Acc@1 83.594 (78.963)	Acc@5 100.000 (98.741)
Epoch: [10][256/391]	Time 0.402 (0.405)	Data 0.002 (0.003)	Loss 0.9080 (0.9095)	Acc@1 73.438 (79.028)	Acc@5 100.000 (98.799)
Epoch: [10][320/391]	Time 0.396 (0.406)	Data 0.002 (0.003)	Loss 0.8459 (0.9091)	Acc@1 82.031 (79.057)	Acc@5 99.219 (98.810)
Epoch: [10][384/391]	Time 0.387 (0.406)	Data 0.001 (0.003)	Loss 0.9204 (0.9091)	Acc@1 81.250 (79.042)	Acc@5 97.656 (98.795)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 483924 ; 487386 ; 0.9928968004825743

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.488 (0.488)	Data 0.235 (0.235)	Loss 1.0136 (1.0136)	Acc@1 75.781 (75.781)	Acc@5 96.875 (96.875)
Epoch: [11][64/391]	Time 0.427 (0.409)	Data 0.002 (0.006)	Loss 0.7624 (0.8656)	Acc@1 84.375 (80.649)	Acc@5 99.219 (98.918)
Epoch: [11][128/391]	Time 0.404 (0.407)	Data 0.002 (0.004)	Loss 0.9039 (0.8893)	Acc@1 77.344 (79.578)	Acc@5 98.438 (98.795)
Epoch: [11][192/391]	Time 0.376 (0.405)	Data 0.002 (0.003)	Loss 1.1903 (0.8977)	Acc@1 71.875 (79.234)	Acc@5 96.875 (98.741)
Epoch: [11][256/391]	Time 0.428 (0.404)	Data 0.002 (0.003)	Loss 0.7816 (0.8968)	Acc@1 82.031 (79.292)	Acc@5 99.219 (98.729)
Epoch: [11][320/391]	Time 0.496 (0.404)	Data 0.002 (0.003)	Loss 0.9445 (0.8985)	Acc@1 79.688 (79.296)	Acc@5 97.656 (98.700)
Epoch: [11][384/391]	Time 0.432 (0.405)	Data 0.002 (0.003)	Loss 0.8734 (0.8978)	Acc@1 79.688 (79.322)	Acc@5 99.219 (98.728)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.429 (0.429)	Data 0.255 (0.255)	Loss 0.8159 (0.8159)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [12][64/391]	Time 0.406 (0.405)	Data 0.002 (0.006)	Loss 0.8276 (0.8603)	Acc@1 80.469 (80.589)	Acc@5 99.219 (98.774)
Epoch: [12][128/391]	Time 0.382 (0.406)	Data 0.003 (0.004)	Loss 0.8012 (0.8732)	Acc@1 81.250 (80.033)	Acc@5 100.000 (98.855)
Epoch: [12][192/391]	Time 0.421 (0.404)	Data 0.002 (0.003)	Loss 0.7862 (0.8828)	Acc@1 84.375 (79.878)	Acc@5 99.219 (98.875)
Epoch: [12][256/391]	Time 0.422 (0.404)	Data 0.002 (0.003)	Loss 0.9145 (0.8853)	Acc@1 76.562 (79.627)	Acc@5 99.219 (98.851)
Epoch: [12][320/391]	Time 0.434 (0.404)	Data 0.001 (0.003)	Loss 0.8560 (0.8864)	Acc@1 81.250 (79.692)	Acc@5 99.219 (98.827)
Epoch: [12][384/391]	Time 0.413 (0.404)	Data 0.002 (0.003)	Loss 0.8866 (0.8881)	Acc@1 78.125 (79.596)	Acc@5 99.219 (98.821)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.481 (0.481)	Data 0.220 (0.220)	Loss 0.7742 (0.7742)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [13][64/391]	Time 0.395 (0.407)	Data 0.002 (0.005)	Loss 0.9206 (0.8663)	Acc@1 78.125 (80.445)	Acc@5 98.438 (98.726)
Epoch: [13][128/391]	Time 0.431 (0.406)	Data 0.002 (0.004)	Loss 0.9588 (0.8805)	Acc@1 78.906 (79.863)	Acc@5 99.219 (98.837)
Epoch: [13][192/391]	Time 0.443 (0.405)	Data 0.002 (0.003)	Loss 0.9172 (0.8837)	Acc@1 81.250 (79.728)	Acc@5 100.000 (98.778)
Epoch: [13][256/391]	Time 0.395 (0.404)	Data 0.001 (0.003)	Loss 0.8192 (0.8810)	Acc@1 83.594 (79.788)	Acc@5 100.000 (98.821)
Epoch: [13][320/391]	Time 0.437 (0.404)	Data 0.002 (0.003)	Loss 1.0244 (0.8793)	Acc@1 71.094 (79.851)	Acc@5 96.875 (98.815)
Epoch: [13][384/391]	Time 0.428 (0.405)	Data 0.001 (0.003)	Loss 1.1226 (0.8795)	Acc@1 71.875 (79.854)	Acc@5 99.219 (98.817)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.411 (0.411)	Data 0.262 (0.262)	Loss 0.7599 (0.7599)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)
Epoch: [14][64/391]	Time 0.429 (0.403)	Data 0.002 (0.006)	Loss 0.8394 (0.8514)	Acc@1 83.594 (81.178)	Acc@5 98.438 (98.882)
Epoch: [14][128/391]	Time 0.450 (0.404)	Data 0.002 (0.004)	Loss 0.8391 (0.8641)	Acc@1 79.688 (80.451)	Acc@5 98.438 (98.910)
Epoch: [14][192/391]	Time 0.426 (0.404)	Data 0.001 (0.003)	Loss 0.8433 (0.8727)	Acc@1 83.594 (80.214)	Acc@5 98.438 (98.875)
Epoch: [14][256/391]	Time 0.390 (0.402)	Data 0.002 (0.003)	Loss 0.9355 (0.8675)	Acc@1 78.906 (80.302)	Acc@5 98.438 (98.897)
Epoch: [14][320/391]	Time 0.397 (0.403)	Data 0.002 (0.003)	Loss 0.9316 (0.8688)	Acc@1 76.562 (80.235)	Acc@5 99.219 (98.883)
Epoch: [14][384/391]	Time 0.422 (0.403)	Data 0.002 (0.003)	Loss 0.9498 (0.8718)	Acc@1 78.125 (80.093)	Acc@5 96.875 (98.876)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.368 (0.368)	Data 0.229 (0.229)	Loss 0.7734 (0.7734)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [15][64/391]	Time 0.404 (0.408)	Data 0.003 (0.005)	Loss 0.8312 (0.8489)	Acc@1 78.906 (81.034)	Acc@5 100.000 (99.062)
Epoch: [15][128/391]	Time 0.376 (0.405)	Data 0.003 (0.004)	Loss 0.8977 (0.8528)	Acc@1 82.812 (80.941)	Acc@5 99.219 (98.995)
Epoch: [15][192/391]	Time 0.409 (0.402)	Data 0.003 (0.003)	Loss 0.8831 (0.8531)	Acc@1 78.906 (80.720)	Acc@5 100.000 (99.000)
Epoch: [15][256/391]	Time 0.383 (0.402)	Data 0.002 (0.003)	Loss 0.8684 (0.8550)	Acc@1 78.906 (80.675)	Acc@5 98.438 (98.936)
Epoch: [15][320/391]	Time 0.460 (0.403)	Data 0.002 (0.003)	Loss 0.8393 (0.8627)	Acc@1 82.812 (80.388)	Acc@5 100.000 (98.846)
Epoch: [15][384/391]	Time 0.449 (0.403)	Data 0.002 (0.003)	Loss 0.8803 (0.8624)	Acc@1 78.125 (80.329)	Acc@5 99.219 (98.864)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 394454 ; 487386 ; 0.8093256679510696

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.461 (0.461)	Data 0.209 (0.209)	Loss 0.8152 (0.8152)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [16][64/391]	Time 0.442 (0.403)	Data 0.002 (0.005)	Loss 0.9085 (0.8006)	Acc@1 78.906 (82.428)	Acc@5 97.656 (99.062)
Epoch: [16][128/391]	Time 0.370 (0.401)	Data 0.002 (0.004)	Loss 0.8013 (0.8309)	Acc@1 78.125 (81.504)	Acc@5 97.656 (98.995)
Epoch: [16][192/391]	Time 0.400 (0.400)	Data 0.002 (0.003)	Loss 0.8129 (0.8455)	Acc@1 82.812 (80.890)	Acc@5 99.219 (98.931)
Epoch: [16][256/391]	Time 0.426 (0.400)	Data 0.002 (0.003)	Loss 0.8694 (0.8498)	Acc@1 75.781 (80.627)	Acc@5 99.219 (98.927)
Epoch: [16][320/391]	Time 0.427 (0.402)	Data 0.002 (0.003)	Loss 0.8939 (0.8524)	Acc@1 74.219 (80.532)	Acc@5 100.000 (98.939)
Epoch: [16][384/391]	Time 0.458 (0.402)	Data 0.002 (0.003)	Loss 0.9437 (0.8532)	Acc@1 76.562 (80.544)	Acc@5 100.000 (98.912)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.399 (0.399)	Data 0.277 (0.277)	Loss 0.8693 (0.8693)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [17][64/391]	Time 0.393 (0.409)	Data 0.002 (0.006)	Loss 0.8462 (0.8548)	Acc@1 81.250 (80.781)	Acc@5 97.656 (98.906)
Epoch: [17][128/391]	Time 0.428 (0.407)	Data 0.001 (0.004)	Loss 0.9311 (0.8450)	Acc@1 76.562 (80.929)	Acc@5 99.219 (98.970)
Epoch: [17][192/391]	Time 0.352 (0.405)	Data 0.002 (0.003)	Loss 0.9481 (0.8460)	Acc@1 75.000 (80.979)	Acc@5 99.219 (98.956)
Epoch: [17][256/391]	Time 0.417 (0.403)	Data 0.002 (0.003)	Loss 0.7696 (0.8476)	Acc@1 80.469 (80.873)	Acc@5 100.000 (98.948)
Epoch: [17][320/391]	Time 0.399 (0.403)	Data 0.002 (0.003)	Loss 0.9579 (0.8546)	Acc@1 76.562 (80.607)	Acc@5 98.438 (98.936)
Epoch: [17][384/391]	Time 0.377 (0.404)	Data 0.001 (0.003)	Loss 0.8343 (0.8572)	Acc@1 82.812 (80.566)	Acc@5 98.438 (98.920)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.406 (0.406)	Data 0.225 (0.225)	Loss 0.9956 (0.9956)	Acc@1 73.438 (73.438)	Acc@5 96.875 (96.875)
Epoch: [18][64/391]	Time 0.381 (0.404)	Data 0.002 (0.005)	Loss 0.7980 (0.8318)	Acc@1 82.812 (81.394)	Acc@5 99.219 (99.171)
Epoch: [18][128/391]	Time 0.418 (0.405)	Data 0.002 (0.004)	Loss 0.8816 (0.8230)	Acc@1 76.562 (81.589)	Acc@5 100.000 (99.176)
Epoch: [18][192/391]	Time 0.400 (0.404)	Data 0.002 (0.003)	Loss 0.8176 (0.8359)	Acc@1 79.688 (80.946)	Acc@5 100.000 (99.101)
Epoch: [18][256/391]	Time 0.370 (0.402)	Data 0.002 (0.003)	Loss 0.7633 (0.8411)	Acc@1 79.688 (80.852)	Acc@5 99.219 (99.052)
Epoch: [18][320/391]	Time 0.401 (0.402)	Data 0.002 (0.003)	Loss 0.7379 (0.8456)	Acc@1 82.031 (80.744)	Acc@5 99.219 (99.034)
Epoch: [18][384/391]	Time 0.398 (0.402)	Data 0.002 (0.003)	Loss 0.9335 (0.8426)	Acc@1 82.812 (80.854)	Acc@5 99.219 (99.034)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.466 (0.466)	Data 0.269 (0.269)	Loss 0.7904 (0.7904)	Acc@1 82.031 (82.031)	Acc@5 98.438 (98.438)
Epoch: [19][64/391]	Time 0.386 (0.405)	Data 0.002 (0.006)	Loss 0.7340 (0.8266)	Acc@1 81.250 (81.298)	Acc@5 99.219 (99.050)
Epoch: [19][128/391]	Time 0.383 (0.405)	Data 0.002 (0.004)	Loss 0.7879 (0.8395)	Acc@1 80.469 (80.808)	Acc@5 99.219 (98.886)
Epoch: [19][192/391]	Time 0.399 (0.405)	Data 0.002 (0.003)	Loss 0.8323 (0.8370)	Acc@1 81.250 (81.108)	Acc@5 99.219 (98.879)
Epoch: [19][256/391]	Time 0.401 (0.404)	Data 0.002 (0.003)	Loss 1.0901 (0.8416)	Acc@1 75.781 (80.931)	Acc@5 99.219 (98.881)
Epoch: [19][320/391]	Time 0.366 (0.406)	Data 0.002 (0.003)	Loss 0.7995 (0.8432)	Acc@1 85.156 (80.856)	Acc@5 98.438 (98.893)
Epoch: [19][384/391]	Time 0.345 (0.404)	Data 0.002 (0.003)	Loss 0.9394 (0.8458)	Acc@1 78.906 (80.769)	Acc@5 98.438 (98.886)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.462 (0.462)	Data 0.347 (0.347)	Loss 0.7726 (0.7726)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [20][64/391]	Time 0.403 (0.406)	Data 0.002 (0.007)	Loss 0.9233 (0.8586)	Acc@1 78.906 (80.745)	Acc@5 98.438 (99.002)
Epoch: [20][128/391]	Time 0.410 (0.405)	Data 0.002 (0.005)	Loss 0.6560 (0.8493)	Acc@1 89.062 (81.305)	Acc@5 100.000 (98.958)
Epoch: [20][192/391]	Time 0.398 (0.403)	Data 0.002 (0.004)	Loss 0.8019 (0.8557)	Acc@1 83.594 (81.064)	Acc@5 99.219 (99.004)
Epoch: [20][256/391]	Time 0.365 (0.401)	Data 0.003 (0.003)	Loss 0.8632 (0.8579)	Acc@1 75.781 (81.062)	Acc@5 100.000 (98.960)
Epoch: [20][320/391]	Time 0.375 (0.403)	Data 0.002 (0.003)	Loss 0.9899 (0.8580)	Acc@1 80.469 (80.953)	Acc@5 97.656 (98.985)
Epoch: [20][384/391]	Time 0.469 (0.403)	Data 0.002 (0.003)	Loss 0.9409 (0.8564)	Acc@1 78.125 (81.041)	Acc@5 98.438 (98.973)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv21.weight

 RM:  module.conv22.weight

Module List Length:  68
Index1: 42
Index: 22
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: False
Bool2: True
j: 32; k: 32
Bool1: False
Bool2: True
j: 33; k: 32
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(22, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(50, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 58: Conv2d(64, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 59: BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 60: Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 61: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 62 gegen 66: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 63 gegen 67: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(22, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(50, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  64

Module List Length:  64
Index1: 40
Index: 21
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(22, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(50, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 58 gegen 62: Conv2d(64, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 59 gegen 63: BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(22, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(50, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): AdaptiveAvgPool2d(output_size=(1, 1))
    (59): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  60
Count: 270928 ; 487386 ; 0.5558797339275235

Epoch: [21 | 180] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 318, in main
    use_gpu_num)
  File "main.py", line 442, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 459, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 107, in forward
    exponential_average_factor, self.eps)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 16 elements not 0
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.544 (0.544)	Data 0.176 (0.176)	Loss 3.3895 (3.3895)	Acc@1 7.812 (7.812)	Acc@5 45.312 (45.312)
Epoch: [1][64/391]	Time 0.414 (0.407)	Data 0.001 (0.004)	Loss 2.4915 (2.8086)	Acc@1 17.188 (21.238)	Acc@5 91.406 (75.637)
Epoch: [1][128/391]	Time 0.367 (0.403)	Data 0.002 (0.003)	Loss 2.2943 (2.6265)	Acc@1 35.938 (25.963)	Acc@5 89.844 (80.656)
Epoch: [1][192/391]	Time 0.408 (0.402)	Data 0.002 (0.003)	Loss 2.1463 (2.5054)	Acc@1 45.312 (29.696)	Acc@5 89.844 (83.339)
Epoch: [1][256/391]	Time 0.407 (0.401)	Data 0.001 (0.003)	Loss 1.8549 (2.4059)	Acc@1 50.781 (33.293)	Acc@5 96.875 (85.396)
Epoch: [1][320/391]	Time 0.416 (0.401)	Data 0.001 (0.002)	Loss 1.9067 (2.3182)	Acc@1 48.438 (36.312)	Acc@5 96.094 (86.930)
Epoch: [1][384/391]	Time 0.313 (0.403)	Data 0.001 (0.002)	Loss 1.7645 (2.2454)	Acc@1 56.250 (38.837)	Acc@5 97.656 (88.019)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.431 (0.431)	Data 0.195 (0.195)	Loss 1.8463 (1.8463)	Acc@1 53.125 (53.125)	Acc@5 95.312 (95.312)
Epoch: [2][64/391]	Time 0.414 (0.396)	Data 0.001 (0.005)	Loss 1.8239 (1.7486)	Acc@1 51.562 (55.421)	Acc@5 93.750 (95.204)
Epoch: [2][128/391]	Time 0.417 (0.399)	Data 0.001 (0.003)	Loss 1.6181 (1.7122)	Acc@1 59.375 (56.401)	Acc@5 96.094 (95.325)
Epoch: [2][192/391]	Time 0.408 (0.399)	Data 0.002 (0.003)	Loss 1.3749 (1.6729)	Acc@1 66.406 (57.428)	Acc@5 99.219 (95.515)
Epoch: [2][256/391]	Time 0.442 (0.400)	Data 0.002 (0.003)	Loss 1.4332 (1.6472)	Acc@1 60.156 (58.123)	Acc@5 97.656 (95.683)
Epoch: [2][320/391]	Time 0.430 (0.400)	Data 0.002 (0.002)	Loss 1.4520 (1.6204)	Acc@1 68.750 (58.881)	Acc@5 96.875 (95.843)
Epoch: [2][384/391]	Time 0.502 (0.402)	Data 0.002 (0.002)	Loss 1.3337 (1.5922)	Acc@1 70.312 (59.720)	Acc@5 96.875 (95.956)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.416 (0.416)	Data 0.265 (0.265)	Loss 1.4544 (1.4544)	Acc@1 59.375 (59.375)	Acc@5 94.531 (94.531)
Epoch: [3][64/391]	Time 0.447 (0.396)	Data 0.001 (0.006)	Loss 1.3235 (1.3870)	Acc@1 69.531 (65.733)	Acc@5 96.094 (96.971)
Epoch: [3][128/391]	Time 0.421 (0.398)	Data 0.002 (0.004)	Loss 1.3825 (1.3538)	Acc@1 60.938 (66.709)	Acc@5 97.656 (97.129)
Epoch: [3][192/391]	Time 0.433 (0.399)	Data 0.002 (0.003)	Loss 1.1712 (1.3405)	Acc@1 75.781 (67.200)	Acc@5 99.219 (97.122)
Epoch: [3][256/391]	Time 0.386 (0.400)	Data 0.002 (0.003)	Loss 1.2114 (1.3214)	Acc@1 70.312 (67.643)	Acc@5 97.656 (97.279)
Epoch: [3][320/391]	Time 0.451 (0.400)	Data 0.002 (0.003)	Loss 1.1068 (1.3087)	Acc@1 75.781 (67.930)	Acc@5 97.656 (97.318)
Epoch: [3][384/391]	Time 0.433 (0.401)	Data 0.002 (0.003)	Loss 1.2783 (1.2948)	Acc@1 64.062 (68.279)	Acc@5 96.875 (97.411)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.396 (0.396)	Data 0.241 (0.241)	Loss 1.1892 (1.1892)	Acc@1 71.875 (71.875)	Acc@5 96.875 (96.875)
Epoch: [4][64/391]	Time 0.386 (0.404)	Data 0.002 (0.006)	Loss 1.1511 (1.1779)	Acc@1 75.781 (71.418)	Acc@5 98.438 (97.752)
Epoch: [4][128/391]	Time 0.383 (0.405)	Data 0.002 (0.004)	Loss 1.1807 (1.1592)	Acc@1 69.531 (71.899)	Acc@5 99.219 (97.856)
Epoch: [4][192/391]	Time 0.409 (0.404)	Data 0.001 (0.003)	Loss 1.3595 (1.1563)	Acc@1 67.188 (72.000)	Acc@5 96.094 (97.911)
Epoch: [4][256/391]	Time 0.444 (0.404)	Data 0.002 (0.003)	Loss 1.1766 (1.1533)	Acc@1 70.312 (72.112)	Acc@5 97.656 (97.930)
Epoch: [4][320/391]	Time 0.381 (0.403)	Data 0.002 (0.003)	Loss 1.0404 (1.1425)	Acc@1 73.438 (72.298)	Acc@5 99.219 (98.002)
Epoch: [4][384/391]	Time 0.376 (0.404)	Data 0.002 (0.002)	Loss 1.2003 (1.1358)	Acc@1 69.531 (72.384)	Acc@5 96.094 (98.017)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.457 (0.457)	Data 0.209 (0.209)	Loss 1.0518 (1.0518)	Acc@1 71.875 (71.875)	Acc@5 99.219 (99.219)
Epoch: [5][64/391]	Time 0.435 (0.399)	Data 0.002 (0.005)	Loss 1.0959 (1.0688)	Acc@1 76.562 (74.543)	Acc@5 96.875 (98.041)
Epoch: [5][128/391]	Time 0.409 (0.393)	Data 0.002 (0.004)	Loss 0.9263 (1.0534)	Acc@1 78.906 (74.982)	Acc@5 99.219 (98.147)
Epoch: [5][192/391]	Time 0.440 (0.396)	Data 0.002 (0.003)	Loss 1.0986 (1.0421)	Acc@1 72.656 (74.980)	Acc@5 99.219 (98.320)
Epoch: [5][256/391]	Time 0.401 (0.398)	Data 0.002 (0.003)	Loss 1.0476 (1.0400)	Acc@1 71.094 (74.799)	Acc@5 99.219 (98.349)
Epoch: [5][320/391]	Time 0.386 (0.399)	Data 0.002 (0.003)	Loss 1.0675 (1.0346)	Acc@1 73.438 (74.951)	Acc@5 100.000 (98.345)
Epoch: [5][384/391]	Time 0.398 (0.400)	Data 0.001 (0.002)	Loss 1.1327 (1.0295)	Acc@1 72.656 (75.037)	Acc@5 97.656 (98.320)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.484 (0.484)	Data 0.239 (0.239)	Loss 1.0702 (1.0702)	Acc@1 71.875 (71.875)	Acc@5 97.656 (97.656)
Epoch: [6][64/391]	Time 0.392 (0.408)	Data 0.002 (0.006)	Loss 0.9614 (0.9656)	Acc@1 78.125 (76.767)	Acc@5 97.656 (98.377)
Epoch: [6][128/391]	Time 0.379 (0.404)	Data 0.002 (0.004)	Loss 1.0539 (0.9762)	Acc@1 75.000 (76.502)	Acc@5 97.656 (98.407)
Epoch: [6][192/391]	Time 0.397 (0.403)	Data 0.002 (0.003)	Loss 1.1191 (0.9798)	Acc@1 69.531 (76.194)	Acc@5 96.094 (98.401)
Epoch: [6][256/391]	Time 0.405 (0.403)	Data 0.002 (0.003)	Loss 1.0660 (0.9768)	Acc@1 75.000 (76.234)	Acc@5 97.656 (98.483)
Epoch: [6][320/391]	Time 0.401 (0.403)	Data 0.002 (0.003)	Loss 0.8517 (0.9729)	Acc@1 82.031 (76.266)	Acc@5 100.000 (98.469)
Epoch: [6][384/391]	Time 0.419 (0.404)	Data 0.002 (0.003)	Loss 1.0948 (0.9690)	Acc@1 68.750 (76.443)	Acc@5 99.219 (98.496)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.471 (0.471)	Data 0.204 (0.204)	Loss 0.9199 (0.9199)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [7][64/391]	Time 0.390 (0.401)	Data 0.002 (0.005)	Loss 0.8121 (0.9408)	Acc@1 85.156 (77.608)	Acc@5 99.219 (98.762)
Epoch: [7][128/391]	Time 0.419 (0.401)	Data 0.002 (0.004)	Loss 1.0135 (0.9535)	Acc@1 71.094 (77.217)	Acc@5 99.219 (98.674)
Epoch: [7][192/391]	Time 0.401 (0.402)	Data 0.002 (0.003)	Loss 1.0325 (0.9499)	Acc@1 75.000 (77.348)	Acc@5 98.438 (98.579)
Epoch: [7][256/391]	Time 0.405 (0.401)	Data 0.002 (0.003)	Loss 0.9101 (0.9509)	Acc@1 79.688 (77.313)	Acc@5 99.219 (98.593)
Epoch: [7][320/391]	Time 0.419 (0.400)	Data 0.002 (0.003)	Loss 0.9949 (0.9495)	Acc@1 71.094 (77.322)	Acc@5 100.000 (98.581)
Epoch: [7][384/391]	Time 0.410 (0.402)	Data 0.002 (0.002)	Loss 1.0006 (0.9502)	Acc@1 76.562 (77.319)	Acc@5 97.656 (98.586)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.429 (0.429)	Data 0.236 (0.236)	Loss 0.9375 (0.9375)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [8][64/391]	Time 0.395 (0.405)	Data 0.001 (0.006)	Loss 0.9676 (0.9298)	Acc@1 79.688 (78.017)	Acc@5 96.875 (98.834)
Epoch: [8][128/391]	Time 0.372 (0.404)	Data 0.002 (0.004)	Loss 0.9467 (0.9343)	Acc@1 76.562 (78.222)	Acc@5 98.438 (98.771)
Epoch: [8][192/391]	Time 0.402 (0.406)	Data 0.003 (0.003)	Loss 1.0474 (0.9297)	Acc@1 76.562 (78.275)	Acc@5 96.875 (98.741)
Epoch: [8][256/391]	Time 0.430 (0.406)	Data 0.002 (0.003)	Loss 0.9739 (0.9319)	Acc@1 72.656 (78.082)	Acc@5 98.438 (98.751)
Epoch: [8][320/391]	Time 0.452 (0.405)	Data 0.002 (0.003)	Loss 0.7826 (0.9326)	Acc@1 85.156 (77.947)	Acc@5 99.219 (98.761)
Epoch: [8][384/391]	Time 0.408 (0.405)	Data 0.003 (0.003)	Loss 0.9475 (0.9321)	Acc@1 75.781 (77.955)	Acc@5 98.438 (98.780)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.459 (0.459)	Data 0.252 (0.252)	Loss 0.8850 (0.8850)	Acc@1 75.781 (75.781)	Acc@5 100.000 (100.000)
Epoch: [9][64/391]	Time 0.400 (0.406)	Data 0.002 (0.006)	Loss 0.8895 (0.9056)	Acc@1 82.812 (78.930)	Acc@5 98.438 (98.678)
Epoch: [9][128/391]	Time 0.362 (0.405)	Data 0.002 (0.004)	Loss 0.9164 (0.9081)	Acc@1 78.125 (78.755)	Acc@5 100.000 (98.783)
Epoch: [9][192/391]	Time 0.445 (0.404)	Data 0.002 (0.003)	Loss 0.8756 (0.9146)	Acc@1 77.344 (78.570)	Acc@5 100.000 (98.717)
Epoch: [9][256/391]	Time 0.386 (0.405)	Data 0.002 (0.003)	Loss 0.9443 (0.9164)	Acc@1 78.906 (78.414)	Acc@5 98.438 (98.708)
Epoch: [9][320/391]	Time 0.389 (0.405)	Data 0.002 (0.003)	Loss 1.1062 (0.9179)	Acc@1 69.531 (78.407)	Acc@5 99.219 (98.737)
Epoch: [9][384/391]	Time 0.399 (0.404)	Data 0.002 (0.003)	Loss 0.7952 (0.9170)	Acc@1 82.812 (78.557)	Acc@5 99.219 (98.738)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.420 (0.420)	Data 0.230 (0.230)	Loss 0.9397 (0.9397)	Acc@1 80.469 (80.469)	Acc@5 97.656 (97.656)
Epoch: [10][64/391]	Time 0.403 (0.401)	Data 0.002 (0.005)	Loss 0.9911 (0.9064)	Acc@1 78.125 (79.195)	Acc@5 97.656 (98.846)
Epoch: [10][128/391]	Time 0.392 (0.405)	Data 0.002 (0.004)	Loss 0.8757 (0.8990)	Acc@1 82.031 (79.494)	Acc@5 99.219 (98.795)
Epoch: [10][192/391]	Time 0.397 (0.405)	Data 0.002 (0.003)	Loss 0.9127 (0.9010)	Acc@1 77.344 (79.428)	Acc@5 100.000 (98.830)
Epoch: [10][256/391]	Time 0.397 (0.404)	Data 0.002 (0.003)	Loss 0.8458 (0.9076)	Acc@1 79.688 (79.292)	Acc@5 99.219 (98.830)
Epoch: [10][320/391]	Time 0.381 (0.404)	Data 0.002 (0.003)	Loss 1.0891 (0.9086)	Acc@1 69.531 (79.206)	Acc@5 98.438 (98.815)
Epoch: [10][384/391]	Time 0.338 (0.405)	Data 0.001 (0.003)	Loss 1.0664 (0.9125)	Acc@1 73.438 (79.111)	Acc@5 96.875 (98.784)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 464876 ; 487386 ; 0.9538148408037982

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.428 (0.428)	Data 0.190 (0.190)	Loss 0.8905 (0.8905)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [11][64/391]	Time 0.415 (0.406)	Data 0.002 (0.005)	Loss 0.8399 (0.8670)	Acc@1 81.250 (80.409)	Acc@5 100.000 (99.014)
Epoch: [11][128/391]	Time 0.357 (0.405)	Data 0.001 (0.003)	Loss 0.9206 (0.8931)	Acc@1 80.469 (79.536)	Acc@5 100.000 (98.795)
Epoch: [11][192/391]	Time 0.397 (0.403)	Data 0.002 (0.003)	Loss 0.8244 (0.8975)	Acc@1 78.906 (79.372)	Acc@5 97.656 (98.757)
Epoch: [11][256/391]	Time 0.406 (0.403)	Data 0.002 (0.003)	Loss 0.8183 (0.8964)	Acc@1 83.594 (79.341)	Acc@5 96.094 (98.790)
Epoch: [11][320/391]	Time 0.402 (0.403)	Data 0.002 (0.003)	Loss 0.8798 (0.9020)	Acc@1 81.250 (79.157)	Acc@5 97.656 (98.761)
Epoch: [11][384/391]	Time 0.448 (0.403)	Data 0.001 (0.002)	Loss 0.9698 (0.9006)	Acc@1 78.125 (79.227)	Acc@5 97.656 (98.764)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.434 (0.434)	Data 0.192 (0.192)	Loss 0.9383 (0.9383)	Acc@1 77.344 (77.344)	Acc@5 98.438 (98.438)
Epoch: [12][64/391]	Time 0.401 (0.404)	Data 0.002 (0.005)	Loss 0.8073 (0.9072)	Acc@1 81.250 (78.558)	Acc@5 99.219 (98.846)
Epoch: [12][128/391]	Time 0.389 (0.406)	Data 0.002 (0.003)	Loss 0.9691 (0.8918)	Acc@1 75.000 (79.233)	Acc@5 98.438 (98.892)
Epoch: [12][192/391]	Time 0.427 (0.406)	Data 0.001 (0.003)	Loss 0.9281 (0.9010)	Acc@1 81.250 (79.101)	Acc@5 100.000 (98.867)
Epoch: [12][256/391]	Time 0.404 (0.406)	Data 0.002 (0.003)	Loss 0.9371 (0.8983)	Acc@1 78.125 (79.222)	Acc@5 99.219 (98.903)
Epoch: [12][320/391]	Time 0.444 (0.406)	Data 0.002 (0.003)	Loss 0.8777 (0.8982)	Acc@1 79.688 (79.154)	Acc@5 99.219 (98.878)
Epoch: [12][384/391]	Time 0.494 (0.406)	Data 0.002 (0.002)	Loss 0.9243 (0.8991)	Acc@1 80.469 (79.190)	Acc@5 100.000 (98.862)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.458 (0.458)	Data 0.227 (0.227)	Loss 0.8999 (0.8999)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)
Epoch: [13][64/391]	Time 0.462 (0.407)	Data 0.002 (0.006)	Loss 0.8115 (0.8759)	Acc@1 85.156 (80.529)	Acc@5 99.219 (98.846)
Epoch: [13][128/391]	Time 0.371 (0.405)	Data 0.002 (0.004)	Loss 1.0172 (0.8754)	Acc@1 75.000 (80.233)	Acc@5 99.219 (98.880)
Epoch: [13][192/391]	Time 0.406 (0.406)	Data 0.002 (0.003)	Loss 0.9105 (0.8832)	Acc@1 80.469 (79.943)	Acc@5 100.000 (98.883)
Epoch: [13][256/391]	Time 0.391 (0.405)	Data 0.001 (0.003)	Loss 1.0654 (0.8886)	Acc@1 71.094 (79.827)	Acc@5 97.656 (98.817)
Epoch: [13][320/391]	Time 0.406 (0.406)	Data 0.002 (0.003)	Loss 0.8779 (0.8927)	Acc@1 82.812 (79.680)	Acc@5 98.438 (98.812)
Epoch: [13][384/391]	Time 0.388 (0.406)	Data 0.002 (0.003)	Loss 0.7856 (0.8935)	Acc@1 82.031 (79.706)	Acc@5 98.438 (98.805)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.461 (0.461)	Data 0.216 (0.216)	Loss 0.8837 (0.8837)	Acc@1 79.688 (79.688)	Acc@5 97.656 (97.656)
Epoch: [14][64/391]	Time 0.411 (0.402)	Data 0.002 (0.005)	Loss 0.8097 (0.8963)	Acc@1 83.594 (79.303)	Acc@5 96.875 (98.918)
Epoch: [14][128/391]	Time 0.392 (0.402)	Data 0.001 (0.004)	Loss 0.7552 (0.8919)	Acc@1 84.375 (79.609)	Acc@5 100.000 (98.880)
Epoch: [14][192/391]	Time 0.419 (0.403)	Data 0.002 (0.003)	Loss 0.8613 (0.8899)	Acc@1 82.812 (79.756)	Acc@5 99.219 (98.931)
Epoch: [14][256/391]	Time 0.453 (0.401)	Data 0.002 (0.003)	Loss 0.8705 (0.8867)	Acc@1 79.688 (79.754)	Acc@5 98.438 (98.884)
Epoch: [14][320/391]	Time 0.403 (0.401)	Data 0.002 (0.003)	Loss 0.7458 (0.8836)	Acc@1 86.719 (79.785)	Acc@5 97.656 (98.897)
Epoch: [14][384/391]	Time 0.483 (0.404)	Data 0.001 (0.003)	Loss 0.9444 (0.8806)	Acc@1 75.000 (79.848)	Acc@5 97.656 (98.868)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.447 (0.447)	Data 0.212 (0.212)	Loss 0.7673 (0.7673)	Acc@1 85.938 (85.938)	Acc@5 98.438 (98.438)
Epoch: [15][64/391]	Time 0.464 (0.405)	Data 0.002 (0.005)	Loss 0.8268 (0.8587)	Acc@1 82.812 (80.204)	Acc@5 100.000 (98.870)
Epoch: [15][128/391]	Time 0.384 (0.404)	Data 0.002 (0.004)	Loss 0.8849 (0.8706)	Acc@1 79.688 (79.700)	Acc@5 100.000 (98.892)
Epoch: [15][192/391]	Time 0.367 (0.403)	Data 0.002 (0.003)	Loss 0.8684 (0.8623)	Acc@1 81.250 (80.096)	Acc@5 100.000 (98.919)
Epoch: [15][256/391]	Time 0.314 (0.404)	Data 0.002 (0.003)	Loss 0.8825 (0.8599)	Acc@1 78.906 (80.101)	Acc@5 97.656 (98.915)
Epoch: [15][320/391]	Time 0.406 (0.404)	Data 0.002 (0.003)	Loss 0.8322 (0.8589)	Acc@1 82.031 (80.028)	Acc@5 100.000 (98.917)
Epoch: [15][384/391]	Time 0.471 (0.405)	Data 0.002 (0.003)	Loss 0.7258 (0.8560)	Acc@1 82.812 (80.110)	Acc@5 100.000 (98.916)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 363436 ; 487386 ; 0.7456841189529448

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.391 (0.391)	Data 0.223 (0.223)	Loss 0.8176 (0.8176)	Acc@1 80.469 (80.469)	Acc@5 100.000 (100.000)
Epoch: [16][64/391]	Time 0.433 (0.406)	Data 0.002 (0.005)	Loss 0.9856 (0.8207)	Acc@1 75.000 (81.442)	Acc@5 98.438 (99.026)
Epoch: [16][128/391]	Time 0.386 (0.402)	Data 0.002 (0.004)	Loss 0.8290 (0.8321)	Acc@1 81.250 (80.650)	Acc@5 100.000 (98.940)
Epoch: [16][192/391]	Time 0.445 (0.403)	Data 0.002 (0.003)	Loss 0.7343 (0.8328)	Acc@1 84.375 (80.724)	Acc@5 98.438 (99.008)
Epoch: [16][256/391]	Time 0.436 (0.404)	Data 0.002 (0.003)	Loss 0.8874 (0.8364)	Acc@1 77.344 (80.739)	Acc@5 100.000 (99.000)
Epoch: [16][320/391]	Time 0.423 (0.405)	Data 0.002 (0.003)	Loss 0.8497 (0.8364)	Acc@1 79.688 (80.707)	Acc@5 99.219 (98.971)
Epoch: [16][384/391]	Time 0.398 (0.406)	Data 0.002 (0.003)	Loss 0.9172 (0.8380)	Acc@1 78.906 (80.643)	Acc@5 99.219 (98.971)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.446 (0.446)	Data 0.242 (0.242)	Loss 0.9437 (0.9437)	Acc@1 77.344 (77.344)	Acc@5 98.438 (98.438)
Epoch: [17][64/391]	Time 0.402 (0.411)	Data 0.002 (0.006)	Loss 0.8500 (0.8454)	Acc@1 80.469 (80.577)	Acc@5 100.000 (98.906)
Epoch: [17][128/391]	Time 0.377 (0.408)	Data 0.002 (0.004)	Loss 0.6837 (0.8422)	Acc@1 82.812 (80.590)	Acc@5 99.219 (99.043)
Epoch: [17][192/391]	Time 0.366 (0.406)	Data 0.002 (0.003)	Loss 0.8918 (0.8403)	Acc@1 79.688 (80.708)	Acc@5 98.438 (99.020)
Epoch: [17][256/391]	Time 0.400 (0.405)	Data 0.002 (0.003)	Loss 0.7699 (0.8348)	Acc@1 84.375 (80.867)	Acc@5 99.219 (98.991)
Epoch: [17][320/391]	Time 0.412 (0.404)	Data 0.002 (0.003)	Loss 0.8202 (0.8389)	Acc@1 83.594 (80.654)	Acc@5 98.438 (98.934)
Epoch: [17][384/391]	Time 0.391 (0.404)	Data 0.001 (0.003)	Loss 0.7874 (0.8354)	Acc@1 82.031 (80.757)	Acc@5 100.000 (98.955)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.397 (0.397)	Data 0.228 (0.228)	Loss 0.8398 (0.8398)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [18][64/391]	Time 0.388 (0.403)	Data 0.001 (0.005)	Loss 0.8184 (0.7968)	Acc@1 82.812 (82.103)	Acc@5 100.000 (99.050)
Epoch: [18][128/391]	Time 0.387 (0.403)	Data 0.002 (0.004)	Loss 0.7754 (0.8192)	Acc@1 82.031 (81.335)	Acc@5 99.219 (99.098)
Epoch: [18][192/391]	Time 0.390 (0.402)	Data 0.002 (0.003)	Loss 0.9011 (0.8253)	Acc@1 78.906 (81.197)	Acc@5 100.000 (99.000)
Epoch: [18][256/391]	Time 0.384 (0.402)	Data 0.002 (0.003)	Loss 0.9539 (0.8322)	Acc@1 73.438 (80.824)	Acc@5 96.875 (98.948)
Epoch: [18][320/391]	Time 0.368 (0.401)	Data 0.002 (0.003)	Loss 0.9063 (0.8337)	Acc@1 80.469 (80.909)	Acc@5 96.094 (98.915)
Epoch: [18][384/391]	Time 0.426 (0.402)	Data 0.002 (0.003)	Loss 0.7408 (0.8349)	Acc@1 83.594 (80.844)	Acc@5 98.438 (98.925)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.470 (0.470)	Data 0.211 (0.211)	Loss 0.8221 (0.8221)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [19][64/391]	Time 0.405 (0.410)	Data 0.002 (0.005)	Loss 0.7316 (0.8341)	Acc@1 85.156 (80.901)	Acc@5 98.438 (98.798)
Epoch: [19][128/391]	Time 0.398 (0.405)	Data 0.002 (0.004)	Loss 0.8998 (0.8279)	Acc@1 78.906 (80.984)	Acc@5 97.656 (99.007)
Epoch: [19][192/391]	Time 0.378 (0.403)	Data 0.002 (0.003)	Loss 0.7858 (0.8329)	Acc@1 83.594 (80.878)	Acc@5 99.219 (98.988)
Epoch: [19][256/391]	Time 0.444 (0.403)	Data 0.002 (0.003)	Loss 0.8562 (0.8306)	Acc@1 83.594 (80.967)	Acc@5 99.219 (99.042)
Epoch: [19][320/391]	Time 0.339 (0.402)	Data 0.002 (0.003)	Loss 0.9369 (0.8293)	Acc@1 79.688 (81.002)	Acc@5 99.219 (99.024)
Epoch: [19][384/391]	Time 0.339 (0.395)	Data 0.002 (0.003)	Loss 0.7476 (0.8304)	Acc@1 85.156 (80.968)	Acc@5 98.438 (99.008)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.402 (0.402)	Data 0.210 (0.210)	Loss 0.7382 (0.7382)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [20][64/391]	Time 0.389 (0.347)	Data 0.002 (0.005)	Loss 0.8707 (0.8197)	Acc@1 80.469 (80.901)	Acc@5 97.656 (99.026)
Epoch: [20][128/391]	Time 0.350 (0.346)	Data 0.003 (0.004)	Loss 0.8723 (0.8240)	Acc@1 78.125 (80.868)	Acc@5 100.000 (99.025)
Epoch: [20][192/391]	Time 0.339 (0.346)	Data 0.002 (0.003)	Loss 0.9648 (0.8335)	Acc@1 77.344 (80.728)	Acc@5 98.438 (98.964)
Epoch: [20][256/391]	Time 0.330 (0.346)	Data 0.002 (0.003)	Loss 0.7083 (0.8283)	Acc@1 85.938 (80.794)	Acc@5 99.219 (98.963)
Epoch: [20][320/391]	Time 0.335 (0.345)	Data 0.002 (0.003)	Loss 0.8480 (0.8253)	Acc@1 80.469 (80.941)	Acc@5 99.219 (98.968)
Epoch: [20][384/391]	Time 0.413 (0.345)	Data 0.002 (0.003)	Loss 0.9462 (0.8281)	Acc@1 76.562 (80.814)	Acc@5 97.656 (98.951)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
























 ab hier mit pruneTrain
