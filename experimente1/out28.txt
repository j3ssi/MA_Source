no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.312 (0.312)	Data 0.156 (0.156)	Loss 3.1506 (3.1506)	Acc@1 10.938 (10.938)	Acc@5 45.312 (45.312)
Epoch: [1][64/391]	Time 0.233 (0.225)	Data 0.002 (0.004)	Loss 2.6722 (2.8159)	Acc@1 21.875 (21.454)	Acc@5 83.594 (74.147)
Epoch: [1][128/391]	Time 0.323 (0.246)	Data 0.002 (0.003)	Loss 2.4837 (2.6403)	Acc@1 21.875 (25.176)	Acc@5 87.500 (79.700)
Epoch: [1][192/391]	Time 0.476 (0.286)	Data 0.002 (0.003)	Loss 2.1906 (2.5290)	Acc@1 32.812 (28.089)	Acc@5 89.062 (82.209)
Epoch: [1][256/391]	Time 0.510 (0.340)	Data 0.002 (0.002)	Loss 2.0802 (2.4342)	Acc@1 42.969 (30.976)	Acc@5 93.750 (84.041)
Epoch: [1][320/391]	Time 0.522 (0.375)	Data 0.002 (0.002)	Loss 1.9181 (2.3503)	Acc@1 47.656 (33.672)	Acc@5 92.969 (85.599)
Epoch: [1][384/391]	Time 0.510 (0.400)	Data 0.002 (0.002)	Loss 1.7207 (2.2767)	Acc@1 53.125 (36.155)	Acc@5 92.969 (86.763)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.567 (0.567)	Data 0.182 (0.182)	Loss 1.9907 (1.9907)	Acc@1 46.094 (46.094)	Acc@5 92.188 (92.188)
Epoch: [2][64/391]	Time 0.543 (0.528)	Data 0.002 (0.005)	Loss 1.6376 (1.8171)	Acc@1 61.719 (51.839)	Acc@5 96.094 (93.906)
Epoch: [2][128/391]	Time 0.542 (0.530)	Data 0.002 (0.003)	Loss 1.6874 (1.7699)	Acc@1 50.781 (53.131)	Acc@5 92.969 (94.204)
Epoch: [2][192/391]	Time 0.465 (0.525)	Data 0.002 (0.003)	Loss 1.5843 (1.7228)	Acc@1 53.906 (54.412)	Acc@5 94.531 (94.503)
Epoch: [2][256/391]	Time 0.544 (0.521)	Data 0.002 (0.003)	Loss 1.5953 (1.6939)	Acc@1 57.812 (55.490)	Acc@5 94.531 (94.726)
Epoch: [2][320/391]	Time 0.514 (0.522)	Data 0.002 (0.002)	Loss 1.4922 (1.6640)	Acc@1 60.938 (56.518)	Acc@5 99.219 (94.952)
Epoch: [2][384/391]	Time 0.528 (0.524)	Data 0.002 (0.002)	Loss 1.3051 (1.6365)	Acc@1 67.188 (57.585)	Acc@5 97.656 (95.144)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.495 (0.495)	Data 0.232 (0.232)	Loss 1.5307 (1.5307)	Acc@1 57.031 (57.031)	Acc@5 97.656 (97.656)
Epoch: [3][64/391]	Time 0.536 (0.527)	Data 0.003 (0.005)	Loss 1.1893 (1.4225)	Acc@1 75.781 (64.736)	Acc@5 97.656 (96.743)
Epoch: [3][128/391]	Time 0.524 (0.527)	Data 0.002 (0.004)	Loss 1.4254 (1.3978)	Acc@1 64.844 (65.492)	Acc@5 94.531 (96.724)
Epoch: [3][192/391]	Time 0.509 (0.526)	Data 0.002 (0.003)	Loss 1.3428 (1.3819)	Acc@1 62.500 (65.665)	Acc@5 96.875 (96.920)
Epoch: [3][256/391]	Time 0.539 (0.521)	Data 0.002 (0.003)	Loss 1.3794 (1.3617)	Acc@1 64.844 (66.279)	Acc@5 97.656 (97.021)
Epoch: [3][320/391]	Time 0.527 (0.522)	Data 0.001 (0.003)	Loss 1.4469 (1.3351)	Acc@1 61.719 (67.073)	Acc@5 96.875 (97.114)
Epoch: [3][384/391]	Time 0.508 (0.524)	Data 0.002 (0.003)	Loss 1.2683 (1.3133)	Acc@1 69.531 (67.711)	Acc@5 96.094 (97.238)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.556 (0.556)	Data 0.296 (0.296)	Loss 1.0521 (1.0521)	Acc@1 74.219 (74.219)	Acc@5 99.219 (99.219)
Epoch: [4][64/391]	Time 0.552 (0.528)	Data 0.002 (0.006)	Loss 1.2220 (1.1850)	Acc@1 67.188 (70.036)	Acc@5 97.656 (97.921)
Epoch: [4][128/391]	Time 0.577 (0.530)	Data 0.002 (0.004)	Loss 1.0501 (1.1868)	Acc@1 72.656 (70.276)	Acc@5 100.000 (97.814)
Epoch: [4][192/391]	Time 0.467 (0.528)	Data 0.002 (0.003)	Loss 1.1278 (1.1664)	Acc@1 71.875 (71.203)	Acc@5 98.438 (97.802)
Epoch: [4][256/391]	Time 0.458 (0.513)	Data 0.002 (0.003)	Loss 0.9195 (1.1564)	Acc@1 78.906 (71.504)	Acc@5 98.438 (97.845)
Epoch: [4][320/391]	Time 0.508 (0.502)	Data 0.002 (0.003)	Loss 1.1563 (1.1459)	Acc@1 69.531 (71.907)	Acc@5 99.219 (97.858)
Epoch: [4][384/391]	Time 0.444 (0.496)	Data 0.002 (0.003)	Loss 1.0596 (1.1347)	Acc@1 75.000 (72.338)	Acc@5 97.656 (97.953)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.494 (0.494)	Data 0.283 (0.283)	Loss 1.0330 (1.0330)	Acc@1 77.344 (77.344)	Acc@5 100.000 (100.000)
Epoch: [5][64/391]	Time 0.468 (0.461)	Data 0.002 (0.006)	Loss 0.9468 (1.0485)	Acc@1 77.344 (74.916)	Acc@5 99.219 (98.413)
Epoch: [5][128/391]	Time 0.474 (0.462)	Data 0.002 (0.004)	Loss 0.9559 (1.0532)	Acc@1 77.344 (74.655)	Acc@5 100.000 (98.401)
Epoch: [5][192/391]	Time 0.455 (0.462)	Data 0.002 (0.003)	Loss 1.0992 (1.0578)	Acc@1 69.531 (74.567)	Acc@5 98.438 (98.328)
Epoch: [5][256/391]	Time 0.474 (0.462)	Data 0.002 (0.003)	Loss 0.7664 (1.0505)	Acc@1 82.031 (74.799)	Acc@5 99.219 (98.371)
Epoch: [5][320/391]	Time 0.476 (0.462)	Data 0.002 (0.003)	Loss 0.8156 (1.0440)	Acc@1 81.250 (74.939)	Acc@5 100.000 (98.367)
Epoch: [5][384/391]	Time 0.475 (0.462)	Data 0.002 (0.003)	Loss 0.8919 (1.0456)	Acc@1 82.812 (74.899)	Acc@5 97.656 (98.377)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.443 (0.443)	Data 0.233 (0.233)	Loss 0.9091 (0.9091)	Acc@1 76.562 (76.562)	Acc@5 100.000 (100.000)
Epoch: [6][64/391]	Time 0.458 (0.464)	Data 0.002 (0.006)	Loss 1.0496 (0.9881)	Acc@1 77.344 (76.382)	Acc@5 98.438 (98.702)
Epoch: [6][128/391]	Time 0.476 (0.464)	Data 0.002 (0.004)	Loss 0.9726 (1.0009)	Acc@1 78.906 (76.290)	Acc@5 99.219 (98.498)
Epoch: [6][192/391]	Time 0.476 (0.464)	Data 0.002 (0.003)	Loss 1.0247 (1.0012)	Acc@1 78.906 (76.312)	Acc@5 99.219 (98.438)
Epoch: [6][256/391]	Time 0.446 (0.463)	Data 0.002 (0.003)	Loss 0.9120 (1.0005)	Acc@1 76.562 (76.298)	Acc@5 99.219 (98.468)
Epoch: [6][320/391]	Time 0.441 (0.462)	Data 0.002 (0.003)	Loss 0.9549 (0.9953)	Acc@1 77.344 (76.548)	Acc@5 98.438 (98.452)
Epoch: [6][384/391]	Time 0.455 (0.463)	Data 0.002 (0.003)	Loss 0.8893 (0.9932)	Acc@1 78.125 (76.605)	Acc@5 98.438 (98.403)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.479 (0.479)	Data 0.206 (0.206)	Loss 1.1835 (1.1835)	Acc@1 64.844 (64.844)	Acc@5 98.438 (98.438)
Epoch: [7][64/391]	Time 0.450 (0.465)	Data 0.002 (0.005)	Loss 1.0551 (0.9780)	Acc@1 77.344 (76.935)	Acc@5 98.438 (98.486)
Epoch: [7][128/391]	Time 0.456 (0.468)	Data 0.002 (0.004)	Loss 1.0388 (0.9743)	Acc@1 69.531 (77.047)	Acc@5 98.438 (98.516)
Epoch: [7][192/391]	Time 0.444 (0.469)	Data 0.002 (0.003)	Loss 0.9734 (0.9803)	Acc@1 77.344 (76.903)	Acc@5 99.219 (98.478)
Epoch: [7][256/391]	Time 0.444 (0.467)	Data 0.002 (0.003)	Loss 0.9657 (0.9775)	Acc@1 80.469 (77.113)	Acc@5 97.656 (98.438)
Epoch: [7][320/391]	Time 0.461 (0.466)	Data 0.002 (0.003)	Loss 0.9618 (0.9726)	Acc@1 75.781 (77.329)	Acc@5 98.438 (98.474)
Epoch: [7][384/391]	Time 0.450 (0.466)	Data 0.002 (0.003)	Loss 0.8960 (0.9692)	Acc@1 78.906 (77.392)	Acc@5 96.875 (98.496)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.511 (0.511)	Data 0.198 (0.198)	Loss 0.9524 (0.9524)	Acc@1 75.781 (75.781)	Acc@5 100.000 (100.000)
Epoch: [8][64/391]	Time 0.460 (0.464)	Data 0.002 (0.005)	Loss 0.8536 (0.9596)	Acc@1 82.031 (77.452)	Acc@5 99.219 (98.654)
Epoch: [8][128/391]	Time 0.506 (0.466)	Data 0.002 (0.004)	Loss 0.9913 (0.9567)	Acc@1 76.562 (77.707)	Acc@5 97.656 (98.619)
Epoch: [8][192/391]	Time 0.449 (0.466)	Data 0.002 (0.003)	Loss 1.0027 (0.9660)	Acc@1 75.781 (77.457)	Acc@5 99.219 (98.620)
Epoch: [8][256/391]	Time 0.496 (0.465)	Data 0.002 (0.003)	Loss 1.2509 (0.9651)	Acc@1 67.969 (77.508)	Acc@5 96.094 (98.623)
Epoch: [8][320/391]	Time 0.450 (0.463)	Data 0.002 (0.003)	Loss 1.0066 (0.9619)	Acc@1 76.562 (77.633)	Acc@5 99.219 (98.596)
Epoch: [8][384/391]	Time 0.394 (0.462)	Data 0.002 (0.003)	Loss 1.0403 (0.9593)	Acc@1 72.656 (77.713)	Acc@5 99.219 (98.604)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.475 (0.475)	Data 0.191 (0.191)	Loss 1.1650 (1.1650)	Acc@1 74.219 (74.219)	Acc@5 98.438 (98.438)
Epoch: [9][64/391]	Time 0.487 (0.465)	Data 0.002 (0.005)	Loss 0.9005 (0.9616)	Acc@1 78.906 (77.656)	Acc@5 98.438 (98.582)
Epoch: [9][128/391]	Time 0.506 (0.465)	Data 0.002 (0.003)	Loss 0.8092 (0.9350)	Acc@1 84.375 (78.591)	Acc@5 100.000 (98.746)
Epoch: [9][192/391]	Time 0.449 (0.465)	Data 0.002 (0.003)	Loss 0.8600 (0.9387)	Acc@1 80.469 (78.542)	Acc@5 99.219 (98.705)
Epoch: [9][256/391]	Time 0.459 (0.464)	Data 0.002 (0.003)	Loss 0.9663 (0.9353)	Acc@1 80.469 (78.526)	Acc@5 100.000 (98.729)
Epoch: [9][320/391]	Time 0.454 (0.463)	Data 0.002 (0.003)	Loss 1.0227 (0.9347)	Acc@1 71.094 (78.483)	Acc@5 97.656 (98.730)
Epoch: [9][384/391]	Time 0.492 (0.463)	Data 0.002 (0.002)	Loss 1.1185 (0.9375)	Acc@1 77.344 (78.377)	Acc@5 98.438 (98.716)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.507 (0.507)	Data 0.197 (0.197)	Loss 0.8404 (0.8404)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [10][64/391]	Time 0.464 (0.464)	Data 0.002 (0.005)	Loss 0.8833 (0.9015)	Acc@1 81.250 (79.375)	Acc@5 98.438 (98.894)
Epoch: [10][128/391]	Time 0.496 (0.464)	Data 0.002 (0.004)	Loss 0.8484 (0.8965)	Acc@1 82.031 (79.778)	Acc@5 99.219 (98.867)
Epoch: [10][192/391]	Time 0.470 (0.466)	Data 0.002 (0.003)	Loss 0.9978 (0.9171)	Acc@1 78.125 (79.262)	Acc@5 99.219 (98.705)
Epoch: [10][256/391]	Time 0.456 (0.465)	Data 0.002 (0.003)	Loss 0.8843 (0.9145)	Acc@1 78.906 (79.308)	Acc@5 98.438 (98.717)
Epoch: [10][320/391]	Time 0.440 (0.464)	Data 0.002 (0.003)	Loss 0.8256 (0.9134)	Acc@1 83.594 (79.349)	Acc@5 98.438 (98.737)
Epoch: [10][384/391]	Time 0.437 (0.463)	Data 0.002 (0.002)	Loss 0.9071 (0.9186)	Acc@1 76.562 (79.125)	Acc@5 97.656 (98.724)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 477860 ; 487386 ; 0.9804549166369161

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.472 (0.472)	Data 0.251 (0.251)	Loss 0.7904 (0.7904)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [11][64/391]	Time 0.484 (0.466)	Data 0.002 (0.006)	Loss 0.8305 (0.8897)	Acc@1 82.031 (80.192)	Acc@5 98.438 (98.882)
Epoch: [11][128/391]	Time 0.475 (0.464)	Data 0.002 (0.004)	Loss 0.9588 (0.9067)	Acc@1 82.031 (79.603)	Acc@5 99.219 (98.789)
Epoch: [11][192/391]	Time 0.465 (0.463)	Data 0.002 (0.003)	Loss 0.8984 (0.9075)	Acc@1 80.469 (79.538)	Acc@5 100.000 (98.838)
Epoch: [11][256/391]	Time 0.479 (0.462)	Data 0.002 (0.003)	Loss 0.9683 (0.9118)	Acc@1 75.000 (79.165)	Acc@5 100.000 (98.857)
Epoch: [11][320/391]	Time 0.439 (0.462)	Data 0.002 (0.003)	Loss 0.8840 (0.9130)	Acc@1 80.469 (79.152)	Acc@5 98.438 (98.793)
Epoch: [11][384/391]	Time 0.467 (0.462)	Data 0.002 (0.003)	Loss 0.8208 (0.9086)	Acc@1 78.125 (79.320)	Acc@5 98.438 (98.805)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.482 (0.482)	Data 0.283 (0.283)	Loss 0.8475 (0.8475)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [12][64/391]	Time 0.457 (0.469)	Data 0.002 (0.006)	Loss 0.8258 (0.8912)	Acc@1 81.250 (80.349)	Acc@5 100.000 (98.858)
Epoch: [12][128/391]	Time 0.509 (0.467)	Data 0.003 (0.004)	Loss 0.8745 (0.8901)	Acc@1 81.250 (80.117)	Acc@5 98.438 (98.837)
Epoch: [12][192/391]	Time 0.472 (0.466)	Data 0.002 (0.003)	Loss 0.7717 (0.8955)	Acc@1 84.375 (79.894)	Acc@5 99.219 (98.846)
Epoch: [12][256/391]	Time 0.456 (0.465)	Data 0.002 (0.003)	Loss 0.8971 (0.8969)	Acc@1 79.688 (79.833)	Acc@5 99.219 (98.890)
Epoch: [12][320/391]	Time 0.455 (0.464)	Data 0.002 (0.003)	Loss 0.9717 (0.9031)	Acc@1 79.688 (79.578)	Acc@5 100.000 (98.844)
Epoch: [12][384/391]	Time 0.459 (0.464)	Data 0.002 (0.003)	Loss 1.0970 (0.9035)	Acc@1 74.219 (79.535)	Acc@5 99.219 (98.839)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.562 (0.562)	Data 0.264 (0.264)	Loss 0.9602 (0.9602)	Acc@1 77.344 (77.344)	Acc@5 98.438 (98.438)
Epoch: [13][64/391]	Time 0.479 (0.461)	Data 0.002 (0.006)	Loss 1.0912 (0.8858)	Acc@1 71.875 (80.409)	Acc@5 97.656 (99.002)
Epoch: [13][128/391]	Time 0.559 (0.463)	Data 0.002 (0.004)	Loss 0.8111 (0.8920)	Acc@1 82.031 (79.930)	Acc@5 98.438 (98.958)
Epoch: [13][192/391]	Time 0.470 (0.467)	Data 0.002 (0.003)	Loss 0.8962 (0.8957)	Acc@1 79.688 (79.768)	Acc@5 99.219 (98.992)
Epoch: [13][256/391]	Time 0.464 (0.465)	Data 0.002 (0.003)	Loss 0.8129 (0.8992)	Acc@1 83.594 (79.605)	Acc@5 99.219 (98.927)
Epoch: [13][320/391]	Time 0.414 (0.464)	Data 0.003 (0.003)	Loss 0.8584 (0.8982)	Acc@1 80.469 (79.661)	Acc@5 97.656 (98.905)
Epoch: [13][384/391]	Time 0.502 (0.463)	Data 0.002 (0.003)	Loss 0.8795 (0.8978)	Acc@1 77.344 (79.694)	Acc@5 100.000 (98.888)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.548 (0.548)	Data 0.227 (0.227)	Loss 0.7941 (0.7941)	Acc@1 85.938 (85.938)	Acc@5 97.656 (97.656)
Epoch: [14][64/391]	Time 0.438 (0.468)	Data 0.002 (0.005)	Loss 0.9222 (0.8816)	Acc@1 79.688 (80.216)	Acc@5 99.219 (98.954)
Epoch: [14][128/391]	Time 0.507 (0.468)	Data 0.001 (0.004)	Loss 0.7886 (0.8869)	Acc@1 83.594 (80.015)	Acc@5 100.000 (98.874)
Epoch: [14][192/391]	Time 0.463 (0.467)	Data 0.003 (0.003)	Loss 0.8361 (0.8901)	Acc@1 78.125 (79.951)	Acc@5 99.219 (98.911)
Epoch: [14][256/391]	Time 0.488 (0.465)	Data 0.001 (0.003)	Loss 0.8176 (0.8857)	Acc@1 84.375 (80.061)	Acc@5 99.219 (98.973)
Epoch: [14][320/391]	Time 0.460 (0.465)	Data 0.002 (0.003)	Loss 1.0535 (0.8886)	Acc@1 75.781 (79.943)	Acc@5 97.656 (98.951)
Epoch: [14][384/391]	Time 0.479 (0.464)	Data 0.002 (0.003)	Loss 0.7799 (0.8899)	Acc@1 86.719 (79.805)	Acc@5 100.000 (98.941)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.496 (0.496)	Data 0.195 (0.195)	Loss 0.7302 (0.7302)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [15][64/391]	Time 0.467 (0.467)	Data 0.002 (0.005)	Loss 0.8428 (0.8530)	Acc@1 80.469 (81.058)	Acc@5 98.438 (98.954)
Epoch: [15][128/391]	Time 0.430 (0.464)	Data 0.002 (0.004)	Loss 0.9647 (0.8675)	Acc@1 75.781 (80.711)	Acc@5 100.000 (99.001)
Epoch: [15][192/391]	Time 0.469 (0.466)	Data 0.002 (0.003)	Loss 0.7270 (0.8698)	Acc@1 83.594 (80.570)	Acc@5 99.219 (99.012)
Epoch: [15][256/391]	Time 0.436 (0.465)	Data 0.002 (0.003)	Loss 0.8719 (0.8759)	Acc@1 82.031 (80.299)	Acc@5 97.656 (98.927)
Epoch: [15][320/391]	Time 0.465 (0.464)	Data 0.002 (0.003)	Loss 1.2159 (0.8776)	Acc@1 68.750 (80.286)	Acc@5 96.094 (98.910)
Epoch: [15][384/391]	Time 0.495 (0.463)	Data 0.002 (0.003)	Loss 0.8786 (0.8779)	Acc@1 75.781 (80.244)	Acc@5 99.219 (98.912)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 410884 ; 487386 ; 0.8430361151120467

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.496 (0.496)	Data 0.196 (0.196)	Loss 0.8518 (0.8518)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [16][64/391]	Time 0.484 (0.446)	Data 0.002 (0.005)	Loss 0.8061 (0.8367)	Acc@1 80.469 (81.442)	Acc@5 99.219 (99.231)
Epoch: [16][128/391]	Time 0.466 (0.453)	Data 0.002 (0.003)	Loss 0.7860 (0.8585)	Acc@1 79.688 (80.796)	Acc@5 100.000 (99.092)
Epoch: [16][192/391]	Time 0.461 (0.456)	Data 0.002 (0.003)	Loss 0.7795 (0.8660)	Acc@1 81.250 (80.772)	Acc@5 100.000 (99.073)
Epoch: [16][256/391]	Time 0.445 (0.456)	Data 0.002 (0.003)	Loss 0.7746 (0.8703)	Acc@1 84.375 (80.730)	Acc@5 98.438 (99.015)
Epoch: [16][320/391]	Time 0.451 (0.457)	Data 0.002 (0.003)	Loss 0.7697 (0.8705)	Acc@1 85.156 (80.788)	Acc@5 100.000 (98.963)
Epoch: [16][384/391]	Time 0.437 (0.457)	Data 0.002 (0.002)	Loss 0.9751 (0.8753)	Acc@1 80.469 (80.611)	Acc@5 98.438 (98.925)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.461 (0.461)	Data 0.210 (0.210)	Loss 0.8994 (0.8994)	Acc@1 78.906 (78.906)	Acc@5 97.656 (97.656)
Epoch: [17][64/391]	Time 0.452 (0.463)	Data 0.002 (0.005)	Loss 0.8177 (0.8635)	Acc@1 82.812 (80.565)	Acc@5 98.438 (99.111)
Epoch: [17][128/391]	Time 0.389 (0.459)	Data 0.002 (0.004)	Loss 0.9918 (0.8531)	Acc@1 77.344 (80.941)	Acc@5 100.000 (99.049)
Epoch: [17][192/391]	Time 0.440 (0.462)	Data 0.002 (0.003)	Loss 0.8207 (0.8632)	Acc@1 81.250 (80.590)	Acc@5 99.219 (98.992)
Epoch: [17][256/391]	Time 0.459 (0.462)	Data 0.002 (0.003)	Loss 1.0620 (0.8674)	Acc@1 73.438 (80.466)	Acc@5 97.656 (98.969)
Epoch: [17][320/391]	Time 0.483 (0.460)	Data 0.002 (0.003)	Loss 0.9277 (0.8703)	Acc@1 81.250 (80.442)	Acc@5 100.000 (98.958)
Epoch: [17][384/391]	Time 0.457 (0.460)	Data 0.001 (0.003)	Loss 0.7958 (0.8754)	Acc@1 84.375 (80.312)	Acc@5 99.219 (98.975)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.498 (0.498)	Data 0.211 (0.211)	Loss 0.9372 (0.9372)	Acc@1 76.562 (76.562)	Acc@5 100.000 (100.000)
Epoch: [18][64/391]	Time 0.431 (0.460)	Data 0.002 (0.005)	Loss 0.8008 (0.8619)	Acc@1 82.031 (80.793)	Acc@5 99.219 (99.099)
Epoch: [18][128/391]	Time 0.435 (0.459)	Data 0.002 (0.004)	Loss 1.0015 (0.8666)	Acc@1 78.125 (80.735)	Acc@5 97.656 (99.055)
Epoch: [18][192/391]	Time 0.470 (0.461)	Data 0.002 (0.003)	Loss 0.9397 (0.8713)	Acc@1 81.250 (80.542)	Acc@5 99.219 (98.984)
Epoch: [18][256/391]	Time 0.455 (0.460)	Data 0.002 (0.003)	Loss 0.7293 (0.8679)	Acc@1 87.500 (80.727)	Acc@5 100.000 (99.015)
Epoch: [18][320/391]	Time 0.477 (0.460)	Data 0.002 (0.003)	Loss 0.8451 (0.8671)	Acc@1 82.031 (80.678)	Acc@5 98.438 (99.007)
Epoch: [18][384/391]	Time 0.450 (0.459)	Data 0.002 (0.003)	Loss 0.8270 (0.8692)	Acc@1 82.812 (80.611)	Acc@5 99.219 (99.016)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.526 (0.526)	Data 0.227 (0.227)	Loss 0.8741 (0.8741)	Acc@1 81.250 (81.250)	Acc@5 97.656 (97.656)
Epoch: [19][64/391]	Time 0.413 (0.458)	Data 0.002 (0.006)	Loss 0.7628 (0.8356)	Acc@1 83.594 (81.875)	Acc@5 100.000 (98.990)
Epoch: [19][128/391]	Time 0.447 (0.461)	Data 0.002 (0.004)	Loss 0.8538 (0.8597)	Acc@1 78.906 (81.056)	Acc@5 98.438 (98.940)
Epoch: [19][192/391]	Time 0.474 (0.462)	Data 0.002 (0.003)	Loss 0.8194 (0.8598)	Acc@1 79.688 (81.027)	Acc@5 96.875 (98.899)
Epoch: [19][256/391]	Time 0.455 (0.461)	Data 0.002 (0.003)	Loss 0.8522 (0.8623)	Acc@1 78.906 (80.913)	Acc@5 100.000 (98.897)
Epoch: [19][320/391]	Time 0.461 (0.460)	Data 0.002 (0.003)	Loss 0.8963 (0.8635)	Acc@1 80.469 (80.909)	Acc@5 99.219 (98.910)
Epoch: [19][384/391]	Time 0.441 (0.460)	Data 0.001 (0.003)	Loss 0.8423 (0.8640)	Acc@1 83.594 (80.844)	Acc@5 98.438 (98.945)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.506 (0.506)	Data 0.190 (0.190)	Loss 0.8202 (0.8202)	Acc@1 78.906 (78.906)	Acc@5 100.000 (100.000)
Epoch: [20][64/391]	Time 0.500 (0.464)	Data 0.002 (0.005)	Loss 0.8612 (0.8254)	Acc@1 78.906 (81.815)	Acc@5 99.219 (99.099)
Epoch: [20][128/391]	Time 0.406 (0.462)	Data 0.002 (0.004)	Loss 0.8884 (0.8367)	Acc@1 79.688 (81.444)	Acc@5 99.219 (99.043)
Epoch: [20][192/391]	Time 0.437 (0.462)	Data 0.002 (0.003)	Loss 0.7244 (0.8467)	Acc@1 88.281 (81.189)	Acc@5 100.000 (99.008)
Epoch: [20][256/391]	Time 0.463 (0.461)	Data 0.002 (0.003)	Loss 0.8822 (0.8538)	Acc@1 77.344 (80.992)	Acc@5 96.875 (98.951)
Epoch: [20][320/391]	Time 0.413 (0.461)	Data 0.002 (0.003)	Loss 0.9569 (0.8562)	Acc@1 80.469 (80.975)	Acc@5 99.219 (98.944)
Epoch: [20][384/391]	Time 0.434 (0.461)	Data 0.001 (0.003)	Loss 0.8723 (0.8543)	Acc@1 82.031 (81.009)	Acc@5 97.656 (98.937)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 351572 ; 487386 ; 0.7213420163894736

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.420 (0.420)	Data 0.188 (0.188)	Loss 0.8542 (0.8542)	Acc@1 83.594 (83.594)	Acc@5 96.094 (96.094)
Epoch: [21][64/391]	Time 0.438 (0.446)	Data 0.002 (0.005)	Loss 0.8172 (0.8310)	Acc@1 84.375 (81.815)	Acc@5 98.438 (99.062)
Epoch: [21][128/391]	Time 0.414 (0.454)	Data 0.002 (0.003)	Loss 0.8747 (0.8435)	Acc@1 81.250 (81.438)	Acc@5 100.000 (99.043)
Epoch: [21][192/391]	Time 0.446 (0.454)	Data 0.002 (0.003)	Loss 0.8462 (0.8485)	Acc@1 78.906 (81.339)	Acc@5 99.219 (99.045)
Epoch: [21][256/391]	Time 0.459 (0.453)	Data 0.002 (0.003)	Loss 0.9108 (0.8513)	Acc@1 79.688 (81.065)	Acc@5 98.438 (99.018)
Epoch: [21][320/391]	Time 0.467 (0.454)	Data 0.002 (0.003)	Loss 0.9113 (0.8502)	Acc@1 81.250 (80.926)	Acc@5 99.219 (98.992)
Epoch: [21][384/391]	Time 0.493 (0.454)	Data 0.002 (0.002)	Loss 0.7814 (0.8489)	Acc@1 81.250 (80.907)	Acc@5 99.219 (99.004)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.469 (0.469)	Data 0.232 (0.232)	Loss 0.8017 (0.8017)	Acc@1 85.156 (85.156)	Acc@5 98.438 (98.438)
Epoch: [22][64/391]	Time 0.453 (0.458)	Data 0.002 (0.006)	Loss 0.8953 (0.8189)	Acc@1 82.812 (81.719)	Acc@5 99.219 (99.026)
Epoch: [22][128/391]	Time 0.456 (0.459)	Data 0.002 (0.004)	Loss 0.9822 (0.8240)	Acc@1 78.906 (81.589)	Acc@5 98.438 (99.025)
Epoch: [22][192/391]	Time 0.470 (0.458)	Data 0.002 (0.003)	Loss 0.7176 (0.8258)	Acc@1 86.719 (81.343)	Acc@5 100.000 (99.150)
Epoch: [22][256/391]	Time 0.466 (0.459)	Data 0.002 (0.003)	Loss 0.6634 (0.8315)	Acc@1 87.500 (81.244)	Acc@5 100.000 (99.064)
Epoch: [22][320/391]	Time 0.454 (0.457)	Data 0.002 (0.003)	Loss 0.7721 (0.8260)	Acc@1 82.812 (81.484)	Acc@5 98.438 (99.065)
Epoch: [22][384/391]	Time 0.440 (0.458)	Data 0.002 (0.003)	Loss 0.8414 (0.8319)	Acc@1 80.469 (81.246)	Acc@5 100.000 (99.044)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.493 (0.493)	Data 0.190 (0.190)	Loss 0.8532 (0.8532)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [23][64/391]	Time 0.526 (0.460)	Data 0.002 (0.005)	Loss 0.7760 (0.7995)	Acc@1 83.594 (82.500)	Acc@5 100.000 (99.111)
Epoch: [23][128/391]	Time 0.459 (0.457)	Data 0.002 (0.003)	Loss 0.6726 (0.8250)	Acc@1 85.156 (81.529)	Acc@5 100.000 (99.134)
Epoch: [23][192/391]	Time 0.466 (0.457)	Data 0.002 (0.003)	Loss 0.7309 (0.8343)	Acc@1 83.594 (81.299)	Acc@5 100.000 (99.073)
Epoch: [23][256/391]	Time 0.428 (0.457)	Data 0.002 (0.003)	Loss 0.7222 (0.8297)	Acc@1 85.156 (81.390)	Acc@5 99.219 (99.058)
Epoch: [23][320/391]	Time 0.430 (0.456)	Data 0.002 (0.003)	Loss 0.8995 (0.8290)	Acc@1 79.688 (81.391)	Acc@5 99.219 (99.073)
Epoch: [23][384/391]	Time 0.441 (0.456)	Data 0.002 (0.003)	Loss 0.8236 (0.8355)	Acc@1 79.688 (81.173)	Acc@5 100.000 (99.056)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.485 (0.485)	Data 0.237 (0.237)	Loss 0.8928 (0.8928)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [24][64/391]	Time 0.479 (0.456)	Data 0.002 (0.006)	Loss 0.9103 (0.8085)	Acc@1 78.125 (81.827)	Acc@5 100.000 (99.099)
Epoch: [24][128/391]	Time 0.487 (0.458)	Data 0.002 (0.004)	Loss 0.7834 (0.8152)	Acc@1 85.938 (81.819)	Acc@5 98.438 (99.092)
Epoch: [24][192/391]	Time 0.461 (0.457)	Data 0.002 (0.003)	Loss 0.9759 (0.8224)	Acc@1 76.562 (81.408)	Acc@5 99.219 (99.057)
Epoch: [24][256/391]	Time 0.467 (0.457)	Data 0.002 (0.003)	Loss 0.9375 (0.8354)	Acc@1 78.906 (81.077)	Acc@5 99.219 (98.985)
Epoch: [24][320/391]	Time 0.461 (0.456)	Data 0.002 (0.003)	Loss 0.7810 (0.8323)	Acc@1 85.156 (81.128)	Acc@5 100.000 (99.000)
Epoch: [24][384/391]	Time 0.498 (0.455)	Data 0.002 (0.003)	Loss 0.8952 (0.8316)	Acc@1 82.031 (81.144)	Acc@5 97.656 (98.987)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.556 (0.556)	Data 0.182 (0.182)	Loss 0.8006 (0.8006)	Acc@1 83.594 (83.594)	Acc@5 97.656 (97.656)
Epoch: [25][64/391]	Time 0.457 (0.454)	Data 0.002 (0.005)	Loss 0.7034 (0.7953)	Acc@1 83.594 (82.692)	Acc@5 100.000 (99.111)
Epoch: [25][128/391]	Time 0.457 (0.456)	Data 0.002 (0.003)	Loss 0.7786 (0.8099)	Acc@1 76.562 (82.146)	Acc@5 100.000 (99.079)
Epoch: [25][192/391]	Time 0.496 (0.457)	Data 0.002 (0.003)	Loss 0.7841 (0.8110)	Acc@1 82.031 (81.914)	Acc@5 100.000 (99.073)
Epoch: [25][256/391]	Time 0.450 (0.457)	Data 0.002 (0.003)	Loss 0.8737 (0.8134)	Acc@1 75.781 (81.864)	Acc@5 99.219 (99.061)
Epoch: [25][320/391]	Time 0.446 (0.457)	Data 0.002 (0.003)	Loss 0.9062 (0.8245)	Acc@1 80.469 (81.506)	Acc@5 100.000 (99.034)
Epoch: [25][384/391]	Time 0.429 (0.455)	Data 0.003 (0.003)	Loss 0.8488 (0.8267)	Acc@1 82.812 (81.425)	Acc@5 99.219 (99.038)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.529 (0.529)	Data 0.201 (0.201)	Loss 3.3652 (3.3652)	Acc@1 10.938 (10.938)	Acc@5 46.875 (46.875)
Epoch: [1][64/391]	Time 0.469 (0.460)	Data 0.001 (0.005)	Loss 2.6876 (2.8117)	Acc@1 29.688 (20.312)	Acc@5 77.344 (73.209)
Epoch: [1][128/391]	Time 0.473 (0.456)	Data 0.002 (0.003)	Loss 2.4695 (2.6547)	Acc@1 32.031 (24.497)	Acc@5 85.938 (78.264)
Epoch: [1][192/391]	Time 0.490 (0.458)	Data 0.002 (0.003)	Loss 2.3450 (2.5568)	Acc@1 31.250 (27.255)	Acc@5 85.938 (81.040)
Epoch: [1][256/391]	Time 0.445 (0.458)	Data 0.002 (0.003)	Loss 2.2939 (2.4892)	Acc@1 37.500 (29.259)	Acc@5 88.281 (82.812)
Epoch: [1][320/391]	Time 0.493 (0.458)	Data 0.002 (0.002)	Loss 2.1664 (2.4285)	Acc@1 39.062 (31.089)	Acc@5 90.625 (84.158)
Epoch: [1][384/391]	Time 0.449 (0.459)	Data 0.002 (0.002)	Loss 2.0430 (2.3753)	Acc@1 41.406 (32.855)	Acc@5 93.750 (85.199)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.507 (0.507)	Data 0.206 (0.206)	Loss 2.1327 (2.1327)	Acc@1 40.625 (40.625)	Acc@5 88.281 (88.281)
Epoch: [2][64/391]	Time 0.410 (0.455)	Data 0.001 (0.005)	Loss 2.1641 (1.9753)	Acc@1 35.156 (45.349)	Acc@5 90.625 (92.224)
Epoch: [2][128/391]	Time 0.532 (0.461)	Data 0.002 (0.004)	Loss 1.8601 (1.9339)	Acc@1 50.781 (47.020)	Acc@5 92.969 (92.630)
Epoch: [2][192/391]	Time 0.516 (0.460)	Data 0.002 (0.003)	Loss 1.9013 (1.8863)	Acc@1 51.562 (48.490)	Acc@5 90.625 (92.949)
Epoch: [2][256/391]	Time 0.439 (0.460)	Data 0.001 (0.003)	Loss 1.6711 (1.8434)	Acc@1 50.000 (49.699)	Acc@5 96.875 (93.510)
Epoch: [2][320/391]	Time 0.429 (0.460)	Data 0.002 (0.003)	Loss 1.5363 (1.8074)	Acc@1 59.375 (50.876)	Acc@5 97.656 (93.760)
Epoch: [2][384/391]	Time 0.423 (0.460)	Data 0.002 (0.002)	Loss 1.4947 (1.7675)	Acc@1 60.938 (52.295)	Acc@5 95.312 (94.073)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.524 (0.524)	Data 0.202 (0.202)	Loss 1.4916 (1.4916)	Acc@1 65.625 (65.625)	Acc@5 96.094 (96.094)
Epoch: [3][64/391]	Time 0.461 (0.466)	Data 0.002 (0.005)	Loss 1.4737 (1.4727)	Acc@1 61.719 (61.442)	Acc@5 94.531 (96.286)
Epoch: [3][128/391]	Time 0.413 (0.465)	Data 0.002 (0.003)	Loss 1.3899 (1.4631)	Acc@1 61.719 (61.543)	Acc@5 98.438 (96.427)
Epoch: [3][192/391]	Time 0.456 (0.461)	Data 0.002 (0.003)	Loss 1.5581 (1.4364)	Acc@1 55.469 (62.318)	Acc@5 93.750 (96.543)
Epoch: [3][256/391]	Time 0.461 (0.461)	Data 0.002 (0.003)	Loss 1.2294 (1.4098)	Acc@1 71.875 (63.132)	Acc@5 99.219 (96.674)
Epoch: [3][320/391]	Time 0.476 (0.460)	Data 0.002 (0.003)	Loss 1.2164 (1.3886)	Acc@1 65.625 (63.826)	Acc@5 99.219 (96.787)
Epoch: [3][384/391]	Time 0.480 (0.460)	Data 0.001 (0.002)	Loss 1.2695 (1.3629)	Acc@1 67.188 (64.759)	Acc@5 96.875 (96.907)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.520 (0.520)	Data 0.237 (0.237)	Loss 1.0660 (1.0660)	Acc@1 70.312 (70.312)	Acc@5 99.219 (99.219)
Epoch: [4][64/391]	Time 0.459 (0.461)	Data 0.001 (0.006)	Loss 1.1435 (1.1811)	Acc@1 76.562 (70.805)	Acc@5 95.312 (97.728)
Epoch: [4][128/391]	Time 0.459 (0.464)	Data 0.002 (0.004)	Loss 1.0505 (1.1843)	Acc@1 77.344 (70.627)	Acc@5 100.000 (97.808)
Epoch: [4][192/391]	Time 0.450 (0.464)	Data 0.002 (0.003)	Loss 1.1972 (1.1683)	Acc@1 67.969 (71.207)	Acc@5 96.875 (97.830)
Epoch: [4][256/391]	Time 0.456 (0.464)	Data 0.001 (0.003)	Loss 1.0669 (1.1653)	Acc@1 72.656 (71.276)	Acc@5 99.219 (97.778)
Epoch: [4][320/391]	Time 0.440 (0.463)	Data 0.002 (0.003)	Loss 1.1244 (1.1515)	Acc@1 72.656 (71.685)	Acc@5 96.875 (97.841)
Epoch: [4][384/391]	Time 0.417 (0.463)	Data 0.001 (0.003)	Loss 0.9997 (1.1440)	Acc@1 74.219 (71.928)	Acc@5 99.219 (97.865)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.463 (0.463)	Data 0.206 (0.206)	Loss 1.1442 (1.1442)	Acc@1 74.219 (74.219)	Acc@5 99.219 (99.219)
Epoch: [5][64/391]	Time 0.486 (0.458)	Data 0.002 (0.005)	Loss 0.9105 (1.0713)	Acc@1 78.125 (74.062)	Acc@5 99.219 (98.137)
Epoch: [5][128/391]	Time 0.451 (0.459)	Data 0.002 (0.004)	Loss 1.2023 (1.0634)	Acc@1 67.969 (74.110)	Acc@5 95.312 (98.117)
Epoch: [5][192/391]	Time 0.469 (0.459)	Data 0.002 (0.003)	Loss 1.1150 (1.0616)	Acc@1 72.656 (74.162)	Acc@5 100.000 (98.178)
Epoch: [5][256/391]	Time 0.459 (0.461)	Data 0.001 (0.003)	Loss 1.0435 (1.0558)	Acc@1 74.219 (74.310)	Acc@5 98.438 (98.228)
Epoch: [5][320/391]	Time 0.439 (0.461)	Data 0.002 (0.003)	Loss 1.0741 (1.0458)	Acc@1 77.344 (74.635)	Acc@5 96.875 (98.274)
Epoch: [5][384/391]	Time 0.480 (0.461)	Data 0.002 (0.002)	Loss 1.1445 (1.0408)	Acc@1 69.531 (74.799)	Acc@5 99.219 (98.279)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.502 (0.502)	Data 0.216 (0.216)	Loss 0.9745 (0.9745)	Acc@1 71.094 (71.094)	Acc@5 99.219 (99.219)
Epoch: [6][64/391]	Time 0.465 (0.461)	Data 0.002 (0.005)	Loss 0.9710 (0.9701)	Acc@1 75.781 (76.887)	Acc@5 100.000 (98.702)
Epoch: [6][128/391]	Time 0.401 (0.463)	Data 0.002 (0.004)	Loss 0.9126 (0.9700)	Acc@1 75.000 (76.914)	Acc@5 97.656 (98.686)
Epoch: [6][192/391]	Time 0.448 (0.464)	Data 0.002 (0.003)	Loss 0.9403 (0.9746)	Acc@1 78.125 (76.724)	Acc@5 100.000 (98.656)
Epoch: [6][256/391]	Time 0.462 (0.464)	Data 0.002 (0.003)	Loss 1.1151 (0.9775)	Acc@1 75.000 (76.751)	Acc@5 96.094 (98.626)
Epoch: [6][320/391]	Time 0.479 (0.464)	Data 0.002 (0.003)	Loss 0.9920 (0.9808)	Acc@1 77.344 (76.696)	Acc@5 98.438 (98.571)
Epoch: [6][384/391]	Time 0.446 (0.464)	Data 0.001 (0.002)	Loss 0.9990 (0.9803)	Acc@1 78.906 (76.745)	Acc@5 96.875 (98.573)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.455 (0.455)	Data 0.262 (0.262)	Loss 0.8924 (0.8924)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [7][64/391]	Time 0.439 (0.457)	Data 0.001 (0.006)	Loss 1.0574 (0.9656)	Acc@1 75.000 (77.392)	Acc@5 99.219 (98.558)
Epoch: [7][128/391]	Time 0.461 (0.460)	Data 0.001 (0.004)	Loss 0.8269 (0.9478)	Acc@1 79.688 (77.865)	Acc@5 99.219 (98.583)
Epoch: [7][192/391]	Time 0.477 (0.462)	Data 0.002 (0.003)	Loss 0.7953 (0.9457)	Acc@1 82.031 (77.927)	Acc@5 100.000 (98.571)
Epoch: [7][256/391]	Time 0.458 (0.462)	Data 0.002 (0.003)	Loss 0.9485 (0.9514)	Acc@1 79.688 (77.724)	Acc@5 99.219 (98.602)
Epoch: [7][320/391]	Time 0.427 (0.462)	Data 0.002 (0.003)	Loss 1.0491 (0.9510)	Acc@1 73.438 (77.706)	Acc@5 98.438 (98.615)
Epoch: [7][384/391]	Time 0.487 (0.462)	Data 0.002 (0.003)	Loss 1.0419 (0.9496)	Acc@1 73.438 (77.677)	Acc@5 99.219 (98.638)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.461 (0.461)	Data 0.223 (0.223)	Loss 0.7726 (0.7726)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [8][64/391]	Time 0.483 (0.459)	Data 0.002 (0.005)	Loss 0.8815 (0.9020)	Acc@1 80.469 (78.966)	Acc@5 98.438 (98.690)
Epoch: [8][128/391]	Time 0.452 (0.462)	Data 0.002 (0.004)	Loss 0.8570 (0.9160)	Acc@1 81.250 (78.543)	Acc@5 99.219 (98.734)
Epoch: [8][192/391]	Time 0.497 (0.463)	Data 0.002 (0.003)	Loss 0.8170 (0.9334)	Acc@1 82.812 (78.198)	Acc@5 99.219 (98.664)
Epoch: [8][256/391]	Time 0.464 (0.463)	Data 0.003 (0.003)	Loss 1.0244 (0.9327)	Acc@1 78.906 (78.180)	Acc@5 98.438 (98.726)
Epoch: [8][320/391]	Time 0.454 (0.462)	Data 0.001 (0.003)	Loss 0.9137 (0.9320)	Acc@1 82.031 (78.183)	Acc@5 98.438 (98.710)
Epoch: [8][384/391]	Time 0.448 (0.462)	Data 0.002 (0.002)	Loss 0.8765 (0.9310)	Acc@1 79.688 (78.247)	Acc@5 100.000 (98.699)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.473 (0.473)	Data 0.274 (0.274)	Loss 1.1238 (1.1238)	Acc@1 69.531 (69.531)	Acc@5 100.000 (100.000)
Epoch: [9][64/391]	Time 0.496 (0.458)	Data 0.002 (0.006)	Loss 1.0378 (0.9228)	Acc@1 72.656 (78.702)	Acc@5 98.438 (98.882)
Epoch: [9][128/391]	Time 0.433 (0.461)	Data 0.002 (0.004)	Loss 0.8271 (0.9296)	Acc@1 83.594 (78.367)	Acc@5 100.000 (98.831)
Epoch: [9][192/391]	Time 0.483 (0.460)	Data 0.002 (0.003)	Loss 0.9792 (0.9243)	Acc@1 78.906 (78.647)	Acc@5 99.219 (98.794)
Epoch: [9][256/391]	Time 0.435 (0.460)	Data 0.002 (0.003)	Loss 0.9940 (0.9221)	Acc@1 75.781 (78.766)	Acc@5 98.438 (98.778)
Epoch: [9][320/391]	Time 0.447 (0.460)	Data 0.003 (0.003)	Loss 1.0210 (0.9190)	Acc@1 75.000 (78.838)	Acc@5 98.438 (98.776)
Epoch: [9][384/391]	Time 0.462 (0.460)	Data 0.002 (0.003)	Loss 0.8502 (0.9173)	Acc@1 82.812 (78.890)	Acc@5 99.219 (98.766)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.524 (0.524)	Data 0.212 (0.212)	Loss 0.7576 (0.7576)	Acc@1 82.031 (82.031)	Acc@5 100.000 (100.000)
Epoch: [10][64/391]	Time 0.450 (0.461)	Data 0.002 (0.005)	Loss 0.7935 (0.8765)	Acc@1 81.250 (80.060)	Acc@5 100.000 (98.882)
Epoch: [10][128/391]	Time 0.467 (0.463)	Data 0.002 (0.004)	Loss 1.0350 (0.8928)	Acc@1 74.219 (79.633)	Acc@5 97.656 (98.837)
Epoch: [10][192/391]	Time 0.444 (0.463)	Data 0.001 (0.003)	Loss 0.8884 (0.9000)	Acc@1 82.031 (79.469)	Acc@5 98.438 (98.838)
Epoch: [10][256/391]	Time 0.450 (0.462)	Data 0.002 (0.003)	Loss 1.0187 (0.9056)	Acc@1 75.000 (79.232)	Acc@5 97.656 (98.836)
Epoch: [10][320/391]	Time 0.465 (0.463)	Data 0.002 (0.003)	Loss 0.8305 (0.9057)	Acc@1 84.375 (79.232)	Acc@5 100.000 (98.846)
Epoch: [10][384/391]	Time 0.461 (0.464)	Data 0.002 (0.002)	Loss 0.9223 (0.9050)	Acc@1 82.031 (79.223)	Acc@5 97.656 (98.829)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 485078 ; 487386 ; 0.9952645336550496

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.494 (0.494)	Data 0.220 (0.220)	Loss 0.7820 (0.7820)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)
Epoch: [11][64/391]	Time 0.428 (0.458)	Data 0.001 (0.005)	Loss 0.8740 (0.8777)	Acc@1 80.469 (79.844)	Acc@5 99.219 (99.026)
Epoch: [11][128/391]	Time 0.453 (0.460)	Data 0.002 (0.004)	Loss 0.8984 (0.9054)	Acc@1 82.031 (78.815)	Acc@5 96.875 (98.831)
Epoch: [11][192/391]	Time 0.442 (0.460)	Data 0.002 (0.003)	Loss 1.1521 (0.9101)	Acc@1 68.750 (78.712)	Acc@5 98.438 (98.806)
Epoch: [11][256/391]	Time 0.470 (0.461)	Data 0.002 (0.003)	Loss 0.8731 (0.9084)	Acc@1 78.906 (78.824)	Acc@5 100.000 (98.836)
Epoch: [11][320/391]	Time 0.467 (0.461)	Data 0.002 (0.003)	Loss 1.0261 (0.9010)	Acc@1 75.000 (79.167)	Acc@5 99.219 (98.854)
Epoch: [11][384/391]	Time 0.479 (0.461)	Data 0.001 (0.003)	Loss 0.9023 (0.9015)	Acc@1 81.250 (79.274)	Acc@5 99.219 (98.825)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.471 (0.471)	Data 0.244 (0.244)	Loss 0.9444 (0.9444)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [12][64/391]	Time 0.476 (0.458)	Data 0.002 (0.006)	Loss 0.9214 (0.8865)	Acc@1 74.219 (80.048)	Acc@5 98.438 (98.750)
Epoch: [12][128/391]	Time 0.512 (0.459)	Data 0.002 (0.004)	Loss 1.0046 (0.8883)	Acc@1 75.781 (79.990)	Acc@5 98.438 (98.783)
Epoch: [12][192/391]	Time 0.435 (0.460)	Data 0.002 (0.003)	Loss 0.8241 (0.8915)	Acc@1 78.125 (79.825)	Acc@5 99.219 (98.810)
Epoch: [12][256/391]	Time 0.456 (0.458)	Data 0.002 (0.003)	Loss 0.8075 (0.8925)	Acc@1 81.250 (79.639)	Acc@5 99.219 (98.848)
Epoch: [12][320/391]	Time 0.439 (0.457)	Data 0.002 (0.003)	Loss 0.8356 (0.8912)	Acc@1 81.250 (79.739)	Acc@5 98.438 (98.815)
Epoch: [12][384/391]	Time 0.448 (0.458)	Data 0.002 (0.003)	Loss 0.8219 (0.8922)	Acc@1 81.250 (79.639)	Acc@5 99.219 (98.833)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.465 (0.465)	Data 0.273 (0.273)	Loss 0.8460 (0.8460)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [13][64/391]	Time 0.436 (0.457)	Data 0.002 (0.006)	Loss 0.8252 (0.8381)	Acc@1 81.250 (81.575)	Acc@5 100.000 (99.014)
Epoch: [13][128/391]	Time 0.488 (0.456)	Data 0.002 (0.004)	Loss 0.8515 (0.8661)	Acc@1 83.594 (80.584)	Acc@5 99.219 (98.910)
Epoch: [13][192/391]	Time 0.390 (0.458)	Data 0.002 (0.003)	Loss 0.8490 (0.8726)	Acc@1 81.250 (80.388)	Acc@5 99.219 (98.863)
Epoch: [13][256/391]	Time 0.471 (0.457)	Data 0.002 (0.003)	Loss 0.8633 (0.8751)	Acc@1 79.688 (80.268)	Acc@5 99.219 (98.860)
Epoch: [13][320/391]	Time 0.450 (0.458)	Data 0.001 (0.003)	Loss 0.6440 (0.8719)	Acc@1 86.719 (80.320)	Acc@5 100.000 (98.893)
Epoch: [13][384/391]	Time 0.449 (0.459)	Data 0.001 (0.003)	Loss 1.0971 (0.8744)	Acc@1 74.219 (80.304)	Acc@5 96.875 (98.874)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.535 (0.535)	Data 0.196 (0.196)	Loss 0.9058 (0.9058)	Acc@1 80.469 (80.469)	Acc@5 97.656 (97.656)
Epoch: [14][64/391]	Time 0.467 (0.464)	Data 0.002 (0.005)	Loss 0.7582 (0.8568)	Acc@1 85.938 (80.685)	Acc@5 100.000 (98.882)
Epoch: [14][128/391]	Time 0.395 (0.461)	Data 0.002 (0.004)	Loss 0.8565 (0.8649)	Acc@1 77.344 (80.457)	Acc@5 98.438 (98.898)
Epoch: [14][192/391]	Time 0.480 (0.461)	Data 0.002 (0.003)	Loss 0.9690 (0.8658)	Acc@1 75.781 (80.278)	Acc@5 96.875 (98.943)
Epoch: [14][256/391]	Time 0.470 (0.459)	Data 0.002 (0.003)	Loss 0.8340 (0.8670)	Acc@1 81.250 (80.481)	Acc@5 99.219 (98.936)
Epoch: [14][320/391]	Time 0.452 (0.459)	Data 0.002 (0.003)	Loss 0.9004 (0.8682)	Acc@1 76.562 (80.444)	Acc@5 100.000 (98.917)
Epoch: [14][384/391]	Time 0.496 (0.459)	Data 0.002 (0.003)	Loss 0.8759 (0.8711)	Acc@1 78.906 (80.268)	Acc@5 100.000 (98.925)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.496 (0.496)	Data 0.228 (0.228)	Loss 0.7770 (0.7770)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [15][64/391]	Time 0.468 (0.457)	Data 0.002 (0.005)	Loss 0.8688 (0.8539)	Acc@1 82.812 (80.841)	Acc@5 99.219 (98.858)
Epoch: [15][128/391]	Time 0.531 (0.457)	Data 0.002 (0.004)	Loss 0.7549 (0.8503)	Acc@1 83.594 (80.887)	Acc@5 99.219 (98.898)
Epoch: [15][192/391]	Time 0.478 (0.458)	Data 0.002 (0.003)	Loss 0.9617 (0.8616)	Acc@1 76.562 (80.449)	Acc@5 98.438 (98.976)
Epoch: [15][256/391]	Time 0.477 (0.457)	Data 0.002 (0.003)	Loss 0.8175 (0.8638)	Acc@1 79.688 (80.411)	Acc@5 99.219 (98.930)
Epoch: [15][320/391]	Time 0.472 (0.458)	Data 0.002 (0.003)	Loss 0.8319 (0.8598)	Acc@1 85.156 (80.659)	Acc@5 99.219 (98.915)
Epoch: [15][384/391]	Time 0.431 (0.459)	Data 0.003 (0.003)	Loss 0.9492 (0.8637)	Acc@1 77.344 (80.499)	Acc@5 98.438 (98.880)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 390688 ; 487386 ; 0.8015987328318828

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.470 (0.470)	Data 0.219 (0.219)	Loss 0.9946 (0.9946)	Acc@1 80.469 (80.469)	Acc@5 96.094 (96.094)
Epoch: [16][64/391]	Time 0.453 (0.457)	Data 0.002 (0.005)	Loss 0.8885 (0.8257)	Acc@1 78.906 (81.659)	Acc@5 99.219 (99.062)
Epoch: [16][128/391]	Time 0.466 (0.458)	Data 0.002 (0.004)	Loss 0.9998 (0.8422)	Acc@1 75.000 (81.038)	Acc@5 97.656 (99.055)
Epoch: [16][192/391]	Time 0.461 (0.458)	Data 0.002 (0.003)	Loss 1.0424 (0.8466)	Acc@1 75.000 (80.975)	Acc@5 96.875 (98.984)
Epoch: [16][256/391]	Time 0.425 (0.458)	Data 0.002 (0.003)	Loss 0.8146 (0.8478)	Acc@1 79.688 (80.900)	Acc@5 98.438 (99.000)
Epoch: [16][320/391]	Time 0.478 (0.458)	Data 0.002 (0.003)	Loss 0.9967 (0.8496)	Acc@1 76.562 (80.749)	Acc@5 97.656 (98.985)
Epoch: [16][384/391]	Time 0.471 (0.458)	Data 0.002 (0.003)	Loss 0.9992 (0.8502)	Acc@1 78.125 (80.706)	Acc@5 99.219 (98.961)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.487 (0.487)	Data 0.210 (0.210)	Loss 0.7235 (0.7235)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [17][64/391]	Time 0.460 (0.451)	Data 0.002 (0.005)	Loss 0.9403 (0.8309)	Acc@1 78.125 (81.971)	Acc@5 98.438 (98.918)
Epoch: [17][128/391]	Time 0.400 (0.456)	Data 0.002 (0.004)	Loss 0.9578 (0.8578)	Acc@1 75.781 (80.602)	Acc@5 99.219 (98.952)
Epoch: [17][192/391]	Time 0.425 (0.456)	Data 0.002 (0.003)	Loss 0.9650 (0.8527)	Acc@1 78.906 (80.772)	Acc@5 96.875 (98.952)
Epoch: [17][256/391]	Time 0.460 (0.455)	Data 0.002 (0.003)	Loss 0.8494 (0.8489)	Acc@1 80.469 (80.952)	Acc@5 97.656 (98.954)
Epoch: [17][320/391]	Time 0.448 (0.455)	Data 0.002 (0.003)	Loss 0.9170 (0.8462)	Acc@1 76.562 (80.990)	Acc@5 99.219 (98.951)
Epoch: [17][384/391]	Time 0.384 (0.455)	Data 0.002 (0.003)	Loss 0.8744 (0.8520)	Acc@1 78.125 (80.710)	Acc@5 100.000 (98.933)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.492 (0.492)	Data 0.213 (0.213)	Loss 0.8574 (0.8574)	Acc@1 77.344 (77.344)	Acc@5 99.219 (99.219)
Epoch: [18][64/391]	Time 0.443 (0.457)	Data 0.002 (0.005)	Loss 0.7552 (0.8235)	Acc@1 83.594 (81.406)	Acc@5 100.000 (98.858)
Epoch: [18][128/391]	Time 0.442 (0.459)	Data 0.002 (0.004)	Loss 0.8193 (0.8382)	Acc@1 80.469 (81.050)	Acc@5 99.219 (98.861)
Epoch: [18][192/391]	Time 0.456 (0.456)	Data 0.002 (0.003)	Loss 0.8558 (0.8353)	Acc@1 78.125 (81.189)	Acc@5 99.219 (98.883)
Epoch: [18][256/391]	Time 0.463 (0.455)	Data 0.002 (0.003)	Loss 0.8404 (0.8335)	Acc@1 79.688 (81.259)	Acc@5 99.219 (98.960)
Epoch: [18][320/391]	Time 0.427 (0.455)	Data 0.002 (0.003)	Loss 0.9399 (0.8370)	Acc@1 83.594 (81.223)	Acc@5 96.875 (98.944)
Epoch: [18][384/391]	Time 0.483 (0.455)	Data 0.002 (0.003)	Loss 0.8295 (0.8419)	Acc@1 83.594 (80.992)	Acc@5 98.438 (98.933)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.494 (0.494)	Data 0.246 (0.246)	Loss 0.8051 (0.8051)	Acc@1 82.812 (82.812)	Acc@5 98.438 (98.438)
Epoch: [19][64/391]	Time 0.440 (0.453)	Data 0.002 (0.006)	Loss 0.7855 (0.8503)	Acc@1 85.156 (81.046)	Acc@5 99.219 (98.834)
Epoch: [19][128/391]	Time 0.455 (0.454)	Data 0.002 (0.004)	Loss 0.7978 (0.8445)	Acc@1 82.812 (81.214)	Acc@5 99.219 (98.867)
Epoch: [19][192/391]	Time 0.448 (0.454)	Data 0.001 (0.003)	Loss 0.8440 (0.8484)	Acc@1 84.375 (80.882)	Acc@5 100.000 (98.923)
Epoch: [19][256/391]	Time 0.451 (0.454)	Data 0.002 (0.003)	Loss 0.8528 (0.8474)	Acc@1 77.344 (80.843)	Acc@5 99.219 (98.927)
Epoch: [19][320/391]	Time 0.482 (0.454)	Data 0.001 (0.003)	Loss 0.8697 (0.8504)	Acc@1 82.812 (80.744)	Acc@5 97.656 (98.919)
Epoch: [19][384/391]	Time 0.501 (0.454)	Data 0.001 (0.003)	Loss 0.8037 (0.8477)	Acc@1 83.594 (80.769)	Acc@5 98.438 (98.949)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.492 (0.492)	Data 0.214 (0.214)	Loss 0.7779 (0.7779)	Acc@1 85.156 (85.156)	Acc@5 96.875 (96.875)
Epoch: [20][64/391]	Time 0.483 (0.453)	Data 0.002 (0.005)	Loss 0.7905 (0.8371)	Acc@1 84.375 (81.382)	Acc@5 98.438 (98.966)
Epoch: [20][128/391]	Time 0.524 (0.452)	Data 0.002 (0.004)	Loss 0.8883 (0.8327)	Acc@1 78.906 (81.432)	Acc@5 100.000 (99.037)
Epoch: [20][192/391]	Time 0.454 (0.450)	Data 0.002 (0.003)	Loss 0.8588 (0.8390)	Acc@1 81.250 (80.959)	Acc@5 100.000 (98.988)
Epoch: [20][256/391]	Time 0.480 (0.451)	Data 0.002 (0.003)	Loss 0.7777 (0.8371)	Acc@1 85.156 (81.055)	Acc@5 98.438 (98.985)
Epoch: [20][320/391]	Time 0.433 (0.451)	Data 0.002 (0.003)	Loss 0.9424 (0.8405)	Acc@1 74.219 (81.019)	Acc@5 99.219 (98.963)
Epoch: [20][384/391]	Time 0.446 (0.451)	Data 0.002 (0.003)	Loss 0.9663 (0.8429)	Acc@1 76.562 (80.978)	Acc@5 97.656 (98.965)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv21.weight

 RM:  module.conv22.weight

Module List Length:  68
Index1: 42
Index: 22
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: False
Bool2: True
j: 32; k: 32
Bool1: False
Bool2: True
j: 33; k: 32
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(53, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 58: Conv2d(64, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 59: BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 60: Conv2d(17, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 61: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 62 gegen 66: Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 63 gegen 67: BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(53, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(17, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  64

Module List Length:  64
Index1: 40
Index: 21
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(53, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 58 gegen 62: Conv2d(64, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 59 gegen 63: BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(53, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): AdaptiveAvgPool2d(output_size=(1, 1))
    (59): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  60
Count: 272400 ; 487386 ; 0.5588999273676306

Epoch: [21 | 180] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 318, in main
    use_gpu_num)
  File "main.py", line 442, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 459, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 107, in forward
    exponential_average_factor, self.eps)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 5 elements not 0
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.574 (0.574)	Data 0.228 (0.228)	Loss 3.2396 (3.2396)	Acc@1 11.719 (11.719)	Acc@5 46.875 (46.875)
Epoch: [1][64/391]	Time 0.373 (0.445)	Data 0.002 (0.005)	Loss 2.4937 (2.7525)	Acc@1 28.125 (20.421)	Acc@5 83.594 (75.096)
Epoch: [1][128/391]	Time 0.415 (0.451)	Data 0.001 (0.004)	Loss 2.4420 (2.6018)	Acc@1 25.781 (23.728)	Acc@5 83.594 (79.621)
Epoch: [1][192/391]	Time 0.418 (0.452)	Data 0.002 (0.003)	Loss 2.2172 (2.5116)	Acc@1 30.469 (26.036)	Acc@5 89.062 (82.007)
Epoch: [1][256/391]	Time 0.424 (0.452)	Data 0.002 (0.003)	Loss 2.0424 (2.4390)	Acc@1 42.969 (28.511)	Acc@5 90.625 (83.588)
Epoch: [1][320/391]	Time 0.448 (0.453)	Data 0.002 (0.003)	Loss 1.9280 (2.3735)	Acc@1 42.188 (30.800)	Acc@5 92.188 (84.772)
Epoch: [1][384/391]	Time 0.443 (0.452)	Data 0.002 (0.002)	Loss 2.0325 (2.3070)	Acc@1 47.656 (33.210)	Acc@5 93.750 (86.000)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.473 (0.473)	Data 0.232 (0.232)	Loss 2.0377 (2.0377)	Acc@1 41.406 (41.406)	Acc@5 90.625 (90.625)
Epoch: [2][64/391]	Time 0.519 (0.456)	Data 0.002 (0.005)	Loss 1.8208 (1.8731)	Acc@1 49.219 (49.135)	Acc@5 95.312 (92.740)
Epoch: [2][128/391]	Time 0.459 (0.456)	Data 0.002 (0.004)	Loss 1.6444 (1.8184)	Acc@1 58.594 (50.757)	Acc@5 96.094 (93.320)
Epoch: [2][192/391]	Time 0.429 (0.459)	Data 0.002 (0.003)	Loss 1.5653 (1.7717)	Acc@1 57.812 (52.336)	Acc@5 96.094 (93.831)
Epoch: [2][256/391]	Time 0.477 (0.459)	Data 0.002 (0.003)	Loss 1.6513 (1.7290)	Acc@1 57.031 (53.754)	Acc@5 94.531 (94.179)
Epoch: [2][320/391]	Time 0.454 (0.459)	Data 0.002 (0.003)	Loss 1.3493 (1.6870)	Acc@1 68.750 (55.031)	Acc@5 96.094 (94.519)
Epoch: [2][384/391]	Time 0.497 (0.459)	Data 0.002 (0.002)	Loss 1.4614 (1.6497)	Acc@1 63.281 (56.171)	Acc@5 95.312 (94.834)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.505 (0.505)	Data 0.293 (0.293)	Loss 1.5496 (1.5496)	Acc@1 57.031 (57.031)	Acc@5 96.094 (96.094)
Epoch: [3][64/391]	Time 0.539 (0.463)	Data 0.002 (0.006)	Loss 1.2447 (1.4088)	Acc@1 70.312 (63.678)	Acc@5 99.219 (96.262)
Epoch: [3][128/391]	Time 0.491 (0.460)	Data 0.002 (0.004)	Loss 1.3729 (1.3855)	Acc@1 65.625 (64.662)	Acc@5 98.438 (96.512)
Epoch: [3][192/391]	Time 0.420 (0.451)	Data 0.002 (0.003)	Loss 1.4023 (1.3679)	Acc@1 64.062 (65.115)	Acc@5 92.969 (96.697)
Epoch: [3][256/391]	Time 0.392 (0.436)	Data 0.001 (0.003)	Loss 1.2539 (1.3441)	Acc@1 65.625 (65.750)	Acc@5 95.312 (96.826)
Epoch: [3][320/391]	Time 0.417 (0.429)	Data 0.002 (0.003)	Loss 1.1427 (1.3234)	Acc@1 71.875 (66.338)	Acc@5 98.438 (96.980)
Epoch: [3][384/391]	Time 0.397 (0.423)	Data 0.002 (0.003)	Loss 1.2267 (1.3079)	Acc@1 67.188 (66.772)	Acc@5 96.875 (97.066)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.434 (0.434)	Data 0.231 (0.231)	Loss 1.2253 (1.2253)	Acc@1 70.312 (70.312)	Acc@5 97.656 (97.656)
Epoch: [4][64/391]	Time 0.516 (0.405)	Data 0.002 (0.005)	Loss 1.1720 (1.1942)	Acc@1 67.969 (69.471)	Acc@5 98.438 (97.812)
Epoch: [4][128/391]	Time 0.376 (0.405)	Data 0.002 (0.004)	Loss 1.2483 (1.1645)	Acc@1 70.312 (70.749)	Acc@5 98.438 (97.777)
Epoch: [4][192/391]	Time 0.362 (0.403)	Data 0.002 (0.003)	Loss 1.2246 (1.1509)	Acc@1 71.094 (71.098)	Acc@5 97.656 (97.851)
Epoch: [4][256/391]	Time 0.391 (0.401)	Data 0.002 (0.003)	Loss 1.1140 (1.1388)	Acc@1 71.875 (71.401)	Acc@5 99.219 (97.875)
Epoch: [4][320/391]	Time 0.400 (0.400)	Data 0.002 (0.003)	Loss 1.0524 (1.1241)	Acc@1 72.656 (71.875)	Acc@5 98.438 (97.978)
Epoch: [4][384/391]	Time 0.366 (0.401)	Data 0.002 (0.002)	Loss 0.9426 (1.1139)	Acc@1 76.562 (72.161)	Acc@5 99.219 (98.017)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.413 (0.413)	Data 0.190 (0.190)	Loss 1.1828 (1.1828)	Acc@1 67.969 (67.969)	Acc@5 97.656 (97.656)
Epoch: [5][64/391]	Time 0.382 (0.400)	Data 0.001 (0.005)	Loss 1.0043 (1.0306)	Acc@1 77.344 (74.724)	Acc@5 99.219 (98.438)
Epoch: [5][128/391]	Time 0.394 (0.400)	Data 0.002 (0.003)	Loss 1.0482 (1.0234)	Acc@1 72.656 (74.945)	Acc@5 98.438 (98.468)
Epoch: [5][192/391]	Time 0.371 (0.398)	Data 0.002 (0.003)	Loss 0.9847 (1.0279)	Acc@1 76.562 (74.636)	Acc@5 99.219 (98.365)
Epoch: [5][256/391]	Time 0.393 (0.398)	Data 0.001 (0.003)	Loss 0.8433 (1.0228)	Acc@1 80.469 (74.881)	Acc@5 99.219 (98.358)
Epoch: [5][320/391]	Time 0.381 (0.398)	Data 0.003 (0.003)	Loss 1.1186 (1.0212)	Acc@1 72.656 (74.908)	Acc@5 97.656 (98.338)
Epoch: [5][384/391]	Time 0.238 (0.398)	Data 0.002 (0.002)	Loss 1.0002 (1.0197)	Acc@1 73.438 (74.951)	Acc@5 98.438 (98.326)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.469 (0.469)	Data 0.192 (0.192)	Loss 1.0161 (1.0161)	Acc@1 75.000 (75.000)	Acc@5 99.219 (99.219)
Epoch: [6][64/391]	Time 0.451 (0.404)	Data 0.002 (0.005)	Loss 1.0727 (0.9521)	Acc@1 71.094 (76.839)	Acc@5 97.656 (98.522)
Epoch: [6][128/391]	Time 0.369 (0.402)	Data 0.002 (0.003)	Loss 0.9242 (0.9711)	Acc@1 77.344 (76.072)	Acc@5 100.000 (98.486)
Epoch: [6][192/391]	Time 0.384 (0.399)	Data 0.002 (0.003)	Loss 0.8478 (0.9677)	Acc@1 85.938 (76.332)	Acc@5 99.219 (98.466)
Epoch: [6][256/391]	Time 0.360 (0.401)	Data 0.002 (0.003)	Loss 0.9538 (0.9662)	Acc@1 75.781 (76.222)	Acc@5 98.438 (98.492)
Epoch: [6][320/391]	Time 0.414 (0.400)	Data 0.002 (0.003)	Loss 1.1830 (0.9628)	Acc@1 65.625 (76.341)	Acc@5 97.656 (98.511)
Epoch: [6][384/391]	Time 0.382 (0.401)	Data 0.002 (0.002)	Loss 1.0006 (0.9587)	Acc@1 72.656 (76.477)	Acc@5 97.656 (98.567)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.451 (0.451)	Data 0.201 (0.201)	Loss 1.0425 (1.0425)	Acc@1 74.219 (74.219)	Acc@5 97.656 (97.656)
Epoch: [7][64/391]	Time 0.421 (0.402)	Data 0.002 (0.005)	Loss 0.8757 (0.9421)	Acc@1 75.000 (77.320)	Acc@5 100.000 (98.642)
Epoch: [7][128/391]	Time 0.389 (0.401)	Data 0.002 (0.003)	Loss 0.9993 (0.9352)	Acc@1 75.000 (77.568)	Acc@5 94.531 (98.553)
Epoch: [7][192/391]	Time 0.399 (0.401)	Data 0.002 (0.003)	Loss 0.8745 (0.9392)	Acc@1 76.562 (77.328)	Acc@5 99.219 (98.547)
Epoch: [7][256/391]	Time 0.414 (0.400)	Data 0.002 (0.003)	Loss 0.9714 (0.9367)	Acc@1 74.219 (77.341)	Acc@5 97.656 (98.571)
Epoch: [7][320/391]	Time 0.416 (0.399)	Data 0.002 (0.003)	Loss 1.0973 (0.9346)	Acc@1 73.438 (77.456)	Acc@5 96.875 (98.588)
Epoch: [7][384/391]	Time 0.333 (0.398)	Data 0.001 (0.002)	Loss 0.9531 (0.9340)	Acc@1 79.688 (77.508)	Acc@5 97.656 (98.616)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.422 (0.422)	Data 0.229 (0.229)	Loss 1.1105 (1.1105)	Acc@1 68.750 (68.750)	Acc@5 99.219 (99.219)
Epoch: [8][64/391]	Time 0.429 (0.400)	Data 0.002 (0.005)	Loss 0.8899 (0.9164)	Acc@1 82.812 (78.582)	Acc@5 99.219 (98.654)
Epoch: [8][128/391]	Time 0.400 (0.398)	Data 0.002 (0.004)	Loss 0.8799 (0.9135)	Acc@1 81.250 (78.500)	Acc@5 99.219 (98.637)
Epoch: [8][192/391]	Time 0.355 (0.397)	Data 0.002 (0.003)	Loss 0.7883 (0.9192)	Acc@1 83.594 (77.947)	Acc@5 98.438 (98.620)
Epoch: [8][256/391]	Time 0.400 (0.396)	Data 0.002 (0.003)	Loss 1.0166 (0.9204)	Acc@1 75.781 (77.933)	Acc@5 96.875 (98.653)
Epoch: [8][320/391]	Time 0.460 (0.395)	Data 0.002 (0.003)	Loss 0.8789 (0.9178)	Acc@1 82.812 (78.030)	Acc@5 97.656 (98.691)
Epoch: [8][384/391]	Time 0.410 (0.396)	Data 0.002 (0.003)	Loss 0.8178 (0.9160)	Acc@1 80.469 (78.091)	Acc@5 100.000 (98.713)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.471 (0.471)	Data 0.186 (0.186)	Loss 0.8320 (0.8320)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [9][64/391]	Time 0.370 (0.399)	Data 0.002 (0.005)	Loss 0.7961 (0.8949)	Acc@1 82.031 (78.762)	Acc@5 100.000 (98.834)
Epoch: [9][128/391]	Time 0.389 (0.400)	Data 0.002 (0.003)	Loss 0.9760 (0.9023)	Acc@1 76.562 (78.452)	Acc@5 98.438 (98.728)
Epoch: [9][192/391]	Time 0.392 (0.398)	Data 0.002 (0.003)	Loss 0.8563 (0.9027)	Acc@1 80.469 (78.473)	Acc@5 99.219 (98.672)
Epoch: [9][256/391]	Time 0.398 (0.397)	Data 0.002 (0.003)	Loss 1.1012 (0.9043)	Acc@1 75.000 (78.496)	Acc@5 99.219 (98.662)
Epoch: [9][320/391]	Time 0.401 (0.397)	Data 0.002 (0.003)	Loss 0.9112 (0.9011)	Acc@1 78.906 (78.631)	Acc@5 99.219 (98.734)
Epoch: [9][384/391]	Time 0.357 (0.397)	Data 0.002 (0.002)	Loss 0.8370 (0.9032)	Acc@1 78.906 (78.478)	Acc@5 99.219 (98.730)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.613 (0.613)	Data 0.317 (0.317)	Loss 0.8073 (0.8073)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.372 (0.406)	Data 0.002 (0.007)	Loss 0.8809 (0.9017)	Acc@1 77.344 (78.810)	Acc@5 99.219 (98.594)
Epoch: [10][128/391]	Time 0.384 (0.400)	Data 0.002 (0.004)	Loss 0.9337 (0.8980)	Acc@1 78.906 (79.021)	Acc@5 99.219 (98.649)
Epoch: [10][192/391]	Time 0.420 (0.401)	Data 0.002 (0.004)	Loss 0.9698 (0.8927)	Acc@1 71.875 (79.007)	Acc@5 96.094 (98.668)
Epoch: [10][256/391]	Time 0.390 (0.400)	Data 0.002 (0.003)	Loss 0.9090 (0.8938)	Acc@1 81.250 (78.855)	Acc@5 96.094 (98.662)
Epoch: [10][320/391]	Time 0.416 (0.399)	Data 0.002 (0.003)	Loss 0.7394 (0.8973)	Acc@1 83.594 (78.772)	Acc@5 99.219 (98.652)
Epoch: [10][384/391]	Time 0.424 (0.397)	Data 0.002 (0.003)	Loss 0.8062 (0.8944)	Acc@1 81.250 (78.920)	Acc@5 100.000 (98.661)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 479298 ; 487386 ; 0.9834053501741946

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.522 (0.522)	Data 0.240 (0.240)	Loss 1.0723 (1.0723)	Acc@1 74.219 (74.219)	Acc@5 99.219 (99.219)
Epoch: [11][64/391]	Time 0.352 (0.403)	Data 0.002 (0.006)	Loss 0.9575 (0.8690)	Acc@1 77.344 (79.820)	Acc@5 98.438 (99.062)
Epoch: [11][128/391]	Time 0.450 (0.406)	Data 0.002 (0.004)	Loss 0.9472 (0.8733)	Acc@1 78.906 (79.627)	Acc@5 96.875 (99.019)
Epoch: [11][192/391]	Time 0.450 (0.404)	Data 0.002 (0.003)	Loss 0.8775 (0.8816)	Acc@1 82.812 (79.489)	Acc@5 99.219 (98.923)
Epoch: [11][256/391]	Time 0.385 (0.404)	Data 0.002 (0.003)	Loss 1.0142 (0.8859)	Acc@1 76.562 (79.390)	Acc@5 100.000 (98.906)
Epoch: [11][320/391]	Time 0.409 (0.403)	Data 0.002 (0.003)	Loss 0.9432 (0.8903)	Acc@1 77.344 (79.330)	Acc@5 99.219 (98.880)
Epoch: [11][384/391]	Time 0.410 (0.403)	Data 0.002 (0.003)	Loss 0.8090 (0.8939)	Acc@1 85.156 (79.223)	Acc@5 100.000 (98.845)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.403 (0.403)	Data 0.215 (0.215)	Loss 0.8474 (0.8474)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [12][64/391]	Time 0.420 (0.408)	Data 0.002 (0.005)	Loss 0.7210 (0.8629)	Acc@1 85.938 (80.288)	Acc@5 99.219 (99.026)
Epoch: [12][128/391]	Time 0.365 (0.406)	Data 0.001 (0.004)	Loss 1.0343 (0.8764)	Acc@1 77.344 (79.918)	Acc@5 99.219 (98.934)
Epoch: [12][192/391]	Time 0.362 (0.406)	Data 0.003 (0.003)	Loss 0.7367 (0.8795)	Acc@1 82.031 (79.700)	Acc@5 100.000 (98.903)
Epoch: [12][256/391]	Time 0.400 (0.404)	Data 0.002 (0.003)	Loss 0.9844 (0.8753)	Acc@1 78.906 (79.827)	Acc@5 98.438 (98.921)
Epoch: [12][320/391]	Time 0.424 (0.403)	Data 0.002 (0.003)	Loss 0.9316 (0.8786)	Acc@1 77.344 (79.705)	Acc@5 96.875 (98.890)
Epoch: [12][384/391]	Time 0.390 (0.403)	Data 0.002 (0.003)	Loss 0.7619 (0.8834)	Acc@1 85.156 (79.517)	Acc@5 98.438 (98.894)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.432 (0.432)	Data 0.225 (0.225)	Loss 0.8711 (0.8711)	Acc@1 78.125 (78.125)	Acc@5 99.219 (99.219)
Epoch: [13][64/391]	Time 0.389 (0.410)	Data 0.002 (0.005)	Loss 0.8523 (0.8793)	Acc@1 80.469 (79.627)	Acc@5 100.000 (98.942)
Epoch: [13][128/391]	Time 0.376 (0.405)	Data 0.002 (0.004)	Loss 0.8422 (0.8700)	Acc@1 78.906 (79.930)	Acc@5 99.219 (98.910)
Epoch: [13][192/391]	Time 0.404 (0.402)	Data 0.001 (0.003)	Loss 0.8005 (0.8691)	Acc@1 82.812 (80.096)	Acc@5 99.219 (98.907)
Epoch: [13][256/391]	Time 0.430 (0.403)	Data 0.002 (0.003)	Loss 0.8757 (0.8757)	Acc@1 79.688 (79.867)	Acc@5 99.219 (98.881)
Epoch: [13][320/391]	Time 0.342 (0.402)	Data 0.001 (0.003)	Loss 0.8786 (0.8763)	Acc@1 80.469 (79.819)	Acc@5 97.656 (98.871)
Epoch: [13][384/391]	Time 0.393 (0.402)	Data 0.002 (0.003)	Loss 0.9483 (0.8773)	Acc@1 74.219 (79.783)	Acc@5 100.000 (98.860)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.459 (0.459)	Data 0.217 (0.217)	Loss 0.7688 (0.7688)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [14][64/391]	Time 0.403 (0.412)	Data 0.002 (0.005)	Loss 0.9109 (0.8504)	Acc@1 81.250 (80.733)	Acc@5 98.438 (99.219)
Epoch: [14][128/391]	Time 0.409 (0.406)	Data 0.002 (0.004)	Loss 0.8413 (0.8510)	Acc@1 78.906 (80.584)	Acc@5 97.656 (99.104)
Epoch: [14][192/391]	Time 0.390 (0.404)	Data 0.002 (0.003)	Loss 0.8250 (0.8565)	Acc@1 81.250 (80.602)	Acc@5 100.000 (99.024)
Epoch: [14][256/391]	Time 0.385 (0.402)	Data 0.002 (0.003)	Loss 0.7586 (0.8626)	Acc@1 83.594 (80.353)	Acc@5 100.000 (98.945)
Epoch: [14][320/391]	Time 0.411 (0.401)	Data 0.002 (0.003)	Loss 0.9837 (0.8667)	Acc@1 75.000 (80.223)	Acc@5 96.875 (98.924)
Epoch: [14][384/391]	Time 0.407 (0.402)	Data 0.002 (0.003)	Loss 0.8283 (0.8673)	Acc@1 85.156 (80.233)	Acc@5 100.000 (98.912)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.499 (0.499)	Data 0.230 (0.230)	Loss 0.8279 (0.8279)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [15][64/391]	Time 0.392 (0.404)	Data 0.002 (0.006)	Loss 0.8990 (0.8222)	Acc@1 76.562 (81.707)	Acc@5 99.219 (99.111)
Epoch: [15][128/391]	Time 0.417 (0.403)	Data 0.002 (0.004)	Loss 0.8478 (0.8352)	Acc@1 81.250 (81.226)	Acc@5 98.438 (99.079)
Epoch: [15][192/391]	Time 0.378 (0.403)	Data 0.001 (0.003)	Loss 0.8197 (0.8524)	Acc@1 79.688 (80.586)	Acc@5 99.219 (98.996)
Epoch: [15][256/391]	Time 0.379 (0.403)	Data 0.002 (0.003)	Loss 0.7872 (0.8551)	Acc@1 80.469 (80.426)	Acc@5 99.219 (98.973)
Epoch: [15][320/391]	Time 0.400 (0.402)	Data 0.002 (0.003)	Loss 0.7605 (0.8553)	Acc@1 85.938 (80.413)	Acc@5 100.000 (98.971)
Epoch: [15][384/391]	Time 0.384 (0.403)	Data 0.001 (0.003)	Loss 0.9004 (0.8537)	Acc@1 75.781 (80.394)	Acc@5 99.219 (98.981)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 398752 ; 487386 ; 0.8181441403733386

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.491 (0.491)	Data 0.218 (0.218)	Loss 0.7619 (0.7619)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [16][64/391]	Time 0.406 (0.406)	Data 0.002 (0.005)	Loss 0.8040 (0.8063)	Acc@1 81.250 (81.611)	Acc@5 100.000 (99.267)
Epoch: [16][128/391]	Time 0.388 (0.405)	Data 0.002 (0.004)	Loss 0.9995 (0.8496)	Acc@1 75.000 (80.220)	Acc@5 98.438 (99.031)
Epoch: [16][192/391]	Time 0.377 (0.402)	Data 0.001 (0.003)	Loss 0.8291 (0.8495)	Acc@1 79.688 (80.473)	Acc@5 99.219 (98.956)
Epoch: [16][256/391]	Time 0.420 (0.402)	Data 0.002 (0.003)	Loss 0.7371 (0.8524)	Acc@1 83.594 (80.435)	Acc@5 100.000 (98.942)
Epoch: [16][320/391]	Time 0.434 (0.402)	Data 0.002 (0.003)	Loss 1.0057 (0.8544)	Acc@1 79.688 (80.449)	Acc@5 96.875 (98.939)
Epoch: [16][384/391]	Time 0.415 (0.402)	Data 0.002 (0.002)	Loss 0.8800 (0.8518)	Acc@1 78.906 (80.479)	Acc@5 98.438 (98.959)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.449 (0.449)	Data 0.226 (0.226)	Loss 0.8058 (0.8058)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [17][64/391]	Time 0.360 (0.411)	Data 0.002 (0.005)	Loss 1.0040 (0.8542)	Acc@1 74.219 (80.757)	Acc@5 97.656 (98.942)
Epoch: [17][128/391]	Time 0.406 (0.408)	Data 0.002 (0.004)	Loss 1.0176 (0.8521)	Acc@1 73.438 (80.475)	Acc@5 97.656 (98.916)
Epoch: [17][192/391]	Time 0.384 (0.402)	Data 0.002 (0.003)	Loss 0.7730 (0.8524)	Acc@1 82.031 (80.404)	Acc@5 97.656 (98.939)
Epoch: [17][256/391]	Time 0.387 (0.401)	Data 0.002 (0.003)	Loss 0.6939 (0.8516)	Acc@1 89.062 (80.548)	Acc@5 100.000 (98.969)
Epoch: [17][320/391]	Time 0.440 (0.400)	Data 0.002 (0.003)	Loss 0.8060 (0.8560)	Acc@1 82.812 (80.398)	Acc@5 100.000 (98.988)
Epoch: [17][384/391]	Time 0.393 (0.399)	Data 0.002 (0.003)	Loss 1.0262 (0.8555)	Acc@1 74.219 (80.337)	Acc@5 99.219 (98.977)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.430 (0.430)	Data 0.233 (0.233)	Loss 0.7111 (0.7111)	Acc@1 83.594 (83.594)	Acc@5 98.438 (98.438)
Epoch: [18][64/391]	Time 0.428 (0.407)	Data 0.002 (0.006)	Loss 0.8620 (0.8370)	Acc@1 79.688 (80.901)	Acc@5 98.438 (99.075)
Epoch: [18][128/391]	Time 0.425 (0.404)	Data 0.002 (0.004)	Loss 0.8913 (0.8383)	Acc@1 79.688 (80.917)	Acc@5 98.438 (99.092)
Epoch: [18][192/391]	Time 0.380 (0.403)	Data 0.002 (0.003)	Loss 0.8076 (0.8403)	Acc@1 82.812 (80.817)	Acc@5 98.438 (98.992)
Epoch: [18][256/391]	Time 0.386 (0.403)	Data 0.001 (0.003)	Loss 0.7839 (0.8425)	Acc@1 86.719 (80.712)	Acc@5 98.438 (98.973)
Epoch: [18][320/391]	Time 0.402 (0.402)	Data 0.002 (0.003)	Loss 0.7581 (0.8446)	Acc@1 84.375 (80.680)	Acc@5 99.219 (98.978)
Epoch: [18][384/391]	Time 0.381 (0.402)	Data 0.002 (0.003)	Loss 0.8859 (0.8435)	Acc@1 80.469 (80.688)	Acc@5 100.000 (98.987)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.408 (0.408)	Data 0.215 (0.215)	Loss 0.7142 (0.7142)	Acc@1 85.156 (85.156)	Acc@5 98.438 (98.438)
Epoch: [19][64/391]	Time 0.398 (0.410)	Data 0.002 (0.005)	Loss 0.7828 (0.8291)	Acc@1 82.812 (81.250)	Acc@5 99.219 (99.062)
Epoch: [19][128/391]	Time 0.363 (0.407)	Data 0.002 (0.004)	Loss 0.7165 (0.8274)	Acc@1 87.500 (81.462)	Acc@5 100.000 (99.049)
Epoch: [19][192/391]	Time 0.368 (0.406)	Data 0.002 (0.003)	Loss 0.7333 (0.8371)	Acc@1 80.469 (80.910)	Acc@5 100.000 (99.028)
Epoch: [19][256/391]	Time 0.420 (0.404)	Data 0.002 (0.003)	Loss 0.7825 (0.8379)	Acc@1 80.469 (80.973)	Acc@5 100.000 (98.942)
Epoch: [19][320/391]	Time 0.489 (0.402)	Data 0.002 (0.003)	Loss 0.9158 (0.8380)	Acc@1 81.250 (80.943)	Acc@5 100.000 (98.985)
Epoch: [19][384/391]	Time 0.380 (0.402)	Data 0.002 (0.003)	Loss 0.8589 (0.8390)	Acc@1 80.469 (80.875)	Acc@5 99.219 (98.965)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.452 (0.452)	Data 0.213 (0.213)	Loss 0.7103 (0.7103)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [20][64/391]	Time 0.394 (0.405)	Data 0.003 (0.005)	Loss 0.9118 (0.8409)	Acc@1 80.469 (80.841)	Acc@5 98.438 (99.087)
Epoch: [20][128/391]	Time 0.446 (0.402)	Data 0.002 (0.004)	Loss 0.7825 (0.8330)	Acc@1 82.031 (81.044)	Acc@5 99.219 (99.067)
Epoch: [20][192/391]	Time 0.392 (0.401)	Data 0.002 (0.003)	Loss 0.7658 (0.8252)	Acc@1 85.938 (81.254)	Acc@5 100.000 (99.105)
Epoch: [20][256/391]	Time 0.402 (0.401)	Data 0.002 (0.003)	Loss 0.9640 (0.8364)	Acc@1 75.781 (80.797)	Acc@5 100.000 (99.049)
Epoch: [20][320/391]	Time 0.424 (0.401)	Data 0.002 (0.003)	Loss 0.7712 (0.8375)	Acc@1 85.156 (80.805)	Acc@5 100.000 (99.078)
Epoch: [20][384/391]	Time 0.379 (0.400)	Data 0.002 (0.002)	Loss 0.7146 (0.8368)	Acc@1 86.719 (80.765)	Acc@5 100.000 (99.075)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv21.weight

 RM:  module.conv22.weight

Module List Length:  68
Index1: 42
Index: 22
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: False
Bool2: True
j: 32; k: 32
Bool1: False
Bool2: True
j: 33; k: 32
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 58: Conv2d(64, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 59: BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 60: Conv2d(30, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 61: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 62 gegen 66: Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 63 gegen 67: BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(30, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  64

Module List Length:  64
Index1: 40
Index: 21
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: True
Bool2: True
numDelete: 2
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 58 gegen 62: Conv2d(64, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 59 gegen 63: BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(5, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): AdaptiveAvgPool2d(output_size=(1, 1))
    (59): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  60
Count: 268928 ; 487386 ; 0.551776210231726

Epoch: [21 | 180] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 318, in main
    use_gpu_num)
  File "main.py", line 442, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 459, in forward
    x = self.module_list[j](x)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 107, in forward
    exponential_average_factor, self.eps)
  File "/home/jessica.buehler/env/local/lib/python3.6/site-packages/torch/nn/functional.py", line 1670, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 4 elements not 0
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.465 (0.465)	Data 0.196 (0.196)	Loss 3.5235 (3.5235)	Acc@1 8.594 (8.594)	Acc@5 39.062 (39.062)
Epoch: [1][64/391]	Time 0.408 (0.401)	Data 0.002 (0.005)	Loss 2.5713 (2.8163)	Acc@1 28.906 (20.300)	Acc@5 78.125 (74.351)
Epoch: [1][128/391]	Time 0.403 (0.400)	Data 0.002 (0.003)	Loss 2.4766 (2.6748)	Acc@1 32.031 (23.855)	Acc@5 88.281 (79.015)
Epoch: [1][192/391]	Time 0.347 (0.398)	Data 0.001 (0.003)	Loss 2.3080 (2.5831)	Acc@1 35.156 (26.482)	Acc@5 86.719 (81.671)
Epoch: [1][256/391]	Time 0.371 (0.396)	Data 0.002 (0.003)	Loss 2.2691 (2.5041)	Acc@1 40.625 (29.201)	Acc@5 85.938 (83.451)
Epoch: [1][320/391]	Time 0.343 (0.397)	Data 0.002 (0.002)	Loss 2.0550 (2.4284)	Acc@1 46.875 (31.873)	Acc@5 92.188 (84.996)
Epoch: [1][384/391]	Time 0.392 (0.398)	Data 0.001 (0.002)	Loss 2.0601 (2.3617)	Acc@1 49.219 (34.115)	Acc@5 89.062 (86.181)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.457 (0.457)	Data 0.182 (0.182)	Loss 2.1531 (2.1531)	Acc@1 44.531 (44.531)	Acc@5 88.281 (88.281)
Epoch: [2][64/391]	Time 0.414 (0.396)	Data 0.002 (0.005)	Loss 1.9927 (1.8980)	Acc@1 46.875 (49.832)	Acc@5 89.844 (93.666)
Epoch: [2][128/391]	Time 0.400 (0.399)	Data 0.002 (0.003)	Loss 1.9273 (1.8369)	Acc@1 50.781 (51.611)	Acc@5 91.406 (94.174)
Epoch: [2][192/391]	Time 0.401 (0.400)	Data 0.001 (0.003)	Loss 1.6586 (1.7883)	Acc@1 60.938 (53.174)	Acc@5 92.969 (94.471)
Epoch: [2][256/391]	Time 0.417 (0.400)	Data 0.002 (0.003)	Loss 1.5967 (1.7446)	Acc@1 60.938 (54.651)	Acc@5 92.188 (94.759)
Epoch: [2][320/391]	Time 0.387 (0.400)	Data 0.001 (0.002)	Loss 1.5312 (1.7072)	Acc@1 58.594 (55.727)	Acc@5 97.656 (94.969)
Epoch: [2][384/391]	Time 0.405 (0.400)	Data 0.002 (0.002)	Loss 1.4525 (1.6716)	Acc@1 66.406 (56.822)	Acc@5 96.875 (95.201)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.394 (0.394)	Data 0.209 (0.209)	Loss 1.4678 (1.4678)	Acc@1 60.156 (60.156)	Acc@5 98.438 (98.438)
Epoch: [3][64/391]	Time 0.368 (0.408)	Data 0.001 (0.005)	Loss 1.4023 (1.4382)	Acc@1 64.062 (63.389)	Acc@5 96.875 (96.695)
Epoch: [3][128/391]	Time 0.381 (0.405)	Data 0.002 (0.004)	Loss 1.4392 (1.4186)	Acc@1 67.969 (64.159)	Acc@5 96.875 (96.736)
Epoch: [3][192/391]	Time 0.406 (0.405)	Data 0.002 (0.003)	Loss 1.3971 (1.3872)	Acc@1 67.969 (65.119)	Acc@5 96.094 (96.887)
Epoch: [3][256/391]	Time 0.405 (0.404)	Data 0.003 (0.003)	Loss 1.2359 (1.3586)	Acc@1 72.656 (65.984)	Acc@5 96.875 (97.076)
Epoch: [3][320/391]	Time 0.400 (0.405)	Data 0.002 (0.003)	Loss 1.2035 (1.3382)	Acc@1 74.219 (66.599)	Acc@5 96.875 (97.177)
Epoch: [3][384/391]	Time 0.405 (0.403)	Data 0.002 (0.002)	Loss 1.1313 (1.3218)	Acc@1 77.344 (67.112)	Acc@5 97.656 (97.265)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.397 (0.397)	Data 0.276 (0.276)	Loss 1.3070 (1.3070)	Acc@1 68.750 (68.750)	Acc@5 97.656 (97.656)
Epoch: [4][64/391]	Time 0.383 (0.405)	Data 0.002 (0.006)	Loss 1.1670 (1.1498)	Acc@1 71.875 (72.127)	Acc@5 97.656 (97.800)
Epoch: [4][128/391]	Time 0.374 (0.402)	Data 0.002 (0.004)	Loss 1.2337 (1.1566)	Acc@1 65.625 (71.857)	Acc@5 99.219 (97.826)
Epoch: [4][192/391]	Time 0.384 (0.402)	Data 0.002 (0.003)	Loss 1.0585 (1.1392)	Acc@1 75.000 (72.543)	Acc@5 99.219 (97.927)
Epoch: [4][256/391]	Time 0.413 (0.401)	Data 0.001 (0.003)	Loss 1.2745 (1.1415)	Acc@1 70.312 (72.413)	Acc@5 96.094 (97.978)
Epoch: [4][320/391]	Time 0.395 (0.401)	Data 0.002 (0.003)	Loss 1.1799 (1.1322)	Acc@1 74.219 (72.615)	Acc@5 96.875 (98.051)
Epoch: [4][384/391]	Time 0.392 (0.401)	Data 0.002 (0.003)	Loss 1.0713 (1.1241)	Acc@1 69.531 (72.849)	Acc@5 99.219 (98.101)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.415 (0.415)	Data 0.270 (0.270)	Loss 0.9815 (0.9815)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [5][64/391]	Time 0.410 (0.412)	Data 0.002 (0.006)	Loss 0.9902 (1.0744)	Acc@1 75.000 (74.267)	Acc@5 99.219 (98.365)
Epoch: [5][128/391]	Time 0.416 (0.408)	Data 0.002 (0.004)	Loss 1.1405 (1.0600)	Acc@1 71.094 (75.018)	Acc@5 97.656 (98.232)
Epoch: [5][192/391]	Time 0.414 (0.405)	Data 0.002 (0.003)	Loss 0.9781 (1.0554)	Acc@1 75.000 (74.960)	Acc@5 100.000 (98.255)
Epoch: [5][256/391]	Time 0.378 (0.403)	Data 0.001 (0.003)	Loss 1.0115 (1.0496)	Acc@1 75.781 (74.976)	Acc@5 99.219 (98.331)
Epoch: [5][320/391]	Time 0.378 (0.402)	Data 0.002 (0.003)	Loss 1.2828 (1.0474)	Acc@1 69.531 (75.061)	Acc@5 96.875 (98.306)
Epoch: [5][384/391]	Time 0.434 (0.402)	Data 0.002 (0.003)	Loss 0.9775 (1.0457)	Acc@1 75.781 (75.014)	Acc@5 98.438 (98.287)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.457 (0.457)	Data 0.194 (0.194)	Loss 1.0925 (1.0925)	Acc@1 70.312 (70.312)	Acc@5 99.219 (99.219)
Epoch: [6][64/391]	Time 0.433 (0.411)	Data 0.002 (0.005)	Loss 1.1061 (1.0002)	Acc@1 75.000 (76.707)	Acc@5 97.656 (98.474)
Epoch: [6][128/391]	Time 0.384 (0.404)	Data 0.002 (0.003)	Loss 1.1312 (0.9932)	Acc@1 70.312 (76.453)	Acc@5 96.094 (98.486)
Epoch: [6][192/391]	Time 0.424 (0.403)	Data 0.002 (0.003)	Loss 0.9030 (0.9899)	Acc@1 78.125 (76.457)	Acc@5 98.438 (98.466)
Epoch: [6][256/391]	Time 0.351 (0.403)	Data 0.002 (0.003)	Loss 0.9235 (0.9837)	Acc@1 82.812 (76.532)	Acc@5 100.000 (98.447)
Epoch: [6][320/391]	Time 0.390 (0.403)	Data 0.002 (0.003)	Loss 0.8984 (0.9809)	Acc@1 82.031 (76.531)	Acc@5 98.438 (98.506)
Epoch: [6][384/391]	Time 0.413 (0.403)	Data 0.002 (0.002)	Loss 1.0566 (0.9792)	Acc@1 74.219 (76.575)	Acc@5 100.000 (98.502)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.451 (0.451)	Data 0.220 (0.220)	Loss 0.9951 (0.9951)	Acc@1 72.656 (72.656)	Acc@5 99.219 (99.219)
Epoch: [7][64/391]	Time 0.418 (0.407)	Data 0.002 (0.005)	Loss 0.9792 (0.9466)	Acc@1 75.781 (77.151)	Acc@5 99.219 (98.714)
Epoch: [7][128/391]	Time 0.419 (0.402)	Data 0.002 (0.004)	Loss 0.9427 (0.9471)	Acc@1 76.562 (77.483)	Acc@5 97.656 (98.565)
Epoch: [7][192/391]	Time 0.456 (0.401)	Data 0.002 (0.003)	Loss 0.9512 (0.9510)	Acc@1 76.562 (77.356)	Acc@5 98.438 (98.612)
Epoch: [7][256/391]	Time 0.397 (0.401)	Data 0.002 (0.003)	Loss 1.0095 (0.9498)	Acc@1 75.781 (77.377)	Acc@5 98.438 (98.611)
Epoch: [7][320/391]	Time 0.423 (0.402)	Data 0.002 (0.003)	Loss 0.9462 (0.9470)	Acc@1 75.000 (77.422)	Acc@5 97.656 (98.654)
Epoch: [7][384/391]	Time 0.391 (0.401)	Data 0.002 (0.002)	Loss 0.9421 (0.9468)	Acc@1 75.781 (77.463)	Acc@5 97.656 (98.638)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.444 (0.444)	Data 0.244 (0.244)	Loss 0.8385 (0.8385)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [8][64/391]	Time 0.396 (0.410)	Data 0.002 (0.006)	Loss 0.8123 (0.9235)	Acc@1 85.938 (77.909)	Acc@5 99.219 (98.630)
Epoch: [8][128/391]	Time 0.441 (0.405)	Data 0.002 (0.004)	Loss 0.8815 (0.9268)	Acc@1 78.125 (78.125)	Acc@5 97.656 (98.625)
Epoch: [8][192/391]	Time 0.371 (0.403)	Data 0.002 (0.003)	Loss 0.9892 (0.9325)	Acc@1 75.781 (77.931)	Acc@5 96.875 (98.628)
Epoch: [8][256/391]	Time 0.394 (0.402)	Data 0.003 (0.003)	Loss 0.8568 (0.9337)	Acc@1 81.250 (78.040)	Acc@5 99.219 (98.669)
Epoch: [8][320/391]	Time 0.389 (0.401)	Data 0.002 (0.003)	Loss 0.9841 (0.9370)	Acc@1 78.906 (77.916)	Acc@5 99.219 (98.678)
Epoch: [8][384/391]	Time 0.397 (0.402)	Data 0.002 (0.003)	Loss 0.9896 (0.9371)	Acc@1 75.781 (78.013)	Acc@5 100.000 (98.667)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.411 (0.411)	Data 0.218 (0.218)	Loss 0.8016 (0.8016)	Acc@1 82.812 (82.812)	Acc@5 98.438 (98.438)
Epoch: [9][64/391]	Time 0.401 (0.407)	Data 0.002 (0.005)	Loss 0.9461 (0.9038)	Acc@1 78.125 (79.267)	Acc@5 98.438 (98.666)
Epoch: [9][128/391]	Time 0.370 (0.405)	Data 0.002 (0.004)	Loss 0.9001 (0.9138)	Acc@1 77.344 (78.900)	Acc@5 100.000 (98.680)
Epoch: [9][192/391]	Time 0.367 (0.404)	Data 0.002 (0.003)	Loss 0.9462 (0.9172)	Acc@1 76.562 (78.643)	Acc@5 99.219 (98.688)
Epoch: [9][256/391]	Time 0.364 (0.403)	Data 0.002 (0.003)	Loss 0.8926 (0.9199)	Acc@1 77.344 (78.475)	Acc@5 99.219 (98.690)
Epoch: [9][320/391]	Time 0.361 (0.403)	Data 0.003 (0.003)	Loss 1.0132 (0.9142)	Acc@1 78.125 (78.709)	Acc@5 99.219 (98.717)
Epoch: [9][384/391]	Time 0.450 (0.402)	Data 0.002 (0.003)	Loss 0.8548 (0.9178)	Acc@1 76.562 (78.450)	Acc@5 99.219 (98.744)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.427 (0.427)	Data 0.264 (0.264)	Loss 0.8904 (0.8904)	Acc@1 77.344 (77.344)	Acc@5 98.438 (98.438)
Epoch: [10][64/391]	Time 0.440 (0.404)	Data 0.002 (0.006)	Loss 0.9598 (0.8985)	Acc@1 78.906 (78.894)	Acc@5 100.000 (98.618)
Epoch: [10][128/391]	Time 0.396 (0.404)	Data 0.002 (0.004)	Loss 0.8034 (0.8937)	Acc@1 81.250 (79.046)	Acc@5 97.656 (98.704)
Epoch: [10][192/391]	Time 0.412 (0.404)	Data 0.002 (0.003)	Loss 0.8359 (0.8969)	Acc@1 78.906 (79.133)	Acc@5 99.219 (98.721)
Epoch: [10][256/391]	Time 0.423 (0.403)	Data 0.002 (0.003)	Loss 0.8197 (0.8983)	Acc@1 83.594 (79.131)	Acc@5 97.656 (98.702)
Epoch: [10][320/391]	Time 0.381 (0.403)	Data 0.002 (0.003)	Loss 0.8289 (0.9007)	Acc@1 82.031 (79.091)	Acc@5 100.000 (98.727)
Epoch: [10][384/391]	Time 0.381 (0.402)	Data 0.002 (0.003)	Loss 0.8345 (0.9009)	Acc@1 81.250 (79.048)	Acc@5 100.000 (98.750)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 480748 ; 487386 ; 0.9863804048536479

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.411 (0.411)	Data 0.225 (0.225)	Loss 0.8575 (0.8575)	Acc@1 80.469 (80.469)	Acc@5 100.000 (100.000)
Epoch: [11][64/391]	Time 0.327 (0.400)	Data 0.002 (0.005)	Loss 0.7285 (0.8919)	Acc@1 88.281 (79.880)	Acc@5 99.219 (98.858)
Epoch: [11][128/391]	Time 0.343 (0.399)	Data 0.002 (0.004)	Loss 0.7681 (0.8956)	Acc@1 83.594 (79.797)	Acc@5 100.000 (98.916)
Epoch: [11][192/391]	Time 0.390 (0.400)	Data 0.002 (0.003)	Loss 0.9636 (0.9040)	Acc@1 79.688 (79.598)	Acc@5 100.000 (98.875)
Epoch: [11][256/391]	Time 0.366 (0.400)	Data 0.002 (0.003)	Loss 0.9078 (0.9027)	Acc@1 78.906 (79.569)	Acc@5 99.219 (98.830)
Epoch: [11][320/391]	Time 0.422 (0.400)	Data 0.002 (0.003)	Loss 0.8594 (0.9024)	Acc@1 78.906 (79.522)	Acc@5 99.219 (98.832)
Epoch: [11][384/391]	Time 0.399 (0.401)	Data 0.002 (0.002)	Loss 0.9367 (0.9069)	Acc@1 77.344 (79.371)	Acc@5 98.438 (98.789)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.402 (0.402)	Data 0.239 (0.239)	Loss 0.8455 (0.8455)	Acc@1 78.125 (78.125)	Acc@5 99.219 (99.219)
Epoch: [12][64/391]	Time 0.408 (0.411)	Data 0.003 (0.006)	Loss 1.0996 (0.8985)	Acc@1 73.438 (78.918)	Acc@5 99.219 (99.026)
Epoch: [12][128/391]	Time 0.391 (0.407)	Data 0.002 (0.004)	Loss 0.9435 (0.8997)	Acc@1 77.344 (79.379)	Acc@5 99.219 (98.886)
Epoch: [12][192/391]	Time 0.393 (0.405)	Data 0.001 (0.003)	Loss 0.8320 (0.9010)	Acc@1 78.906 (79.368)	Acc@5 99.219 (98.879)
Epoch: [12][256/391]	Time 0.411 (0.405)	Data 0.002 (0.003)	Loss 0.9593 (0.8923)	Acc@1 75.781 (79.590)	Acc@5 100.000 (98.942)
Epoch: [12][320/391]	Time 0.405 (0.405)	Data 0.002 (0.003)	Loss 1.1271 (0.8970)	Acc@1 74.219 (79.483)	Acc@5 95.312 (98.910)
Epoch: [12][384/391]	Time 0.393 (0.403)	Data 0.002 (0.003)	Loss 1.1268 (0.9006)	Acc@1 73.438 (79.379)	Acc@5 99.219 (98.888)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.431 (0.431)	Data 0.192 (0.192)	Loss 0.8669 (0.8669)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [13][64/391]	Time 0.382 (0.404)	Data 0.002 (0.005)	Loss 0.8448 (0.8906)	Acc@1 78.125 (80.084)	Acc@5 99.219 (98.630)
Epoch: [13][128/391]	Time 0.254 (0.405)	Data 0.002 (0.003)	Loss 0.8221 (0.8921)	Acc@1 80.469 (79.718)	Acc@5 100.000 (98.692)
Epoch: [13][192/391]	Time 0.392 (0.404)	Data 0.002 (0.003)	Loss 0.8172 (0.8899)	Acc@1 83.594 (79.805)	Acc@5 100.000 (98.769)
Epoch: [13][256/391]	Time 0.399 (0.402)	Data 0.002 (0.003)	Loss 0.9060 (0.8878)	Acc@1 79.688 (79.909)	Acc@5 97.656 (98.760)
Epoch: [13][320/391]	Time 0.330 (0.402)	Data 0.002 (0.003)	Loss 0.8285 (0.8869)	Acc@1 83.594 (79.965)	Acc@5 96.875 (98.798)
Epoch: [13][384/391]	Time 0.364 (0.402)	Data 0.002 (0.002)	Loss 0.9506 (0.8875)	Acc@1 78.906 (79.874)	Acc@5 100.000 (98.795)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.454 (0.454)	Data 0.287 (0.287)	Loss 0.8635 (0.8635)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [14][64/391]	Time 0.383 (0.405)	Data 0.002 (0.006)	Loss 0.8998 (0.8395)	Acc@1 82.031 (81.743)	Acc@5 100.000 (98.930)
Epoch: [14][128/391]	Time 0.370 (0.406)	Data 0.002 (0.004)	Loss 1.0050 (0.8579)	Acc@1 78.125 (80.959)	Acc@5 99.219 (98.958)
Epoch: [14][192/391]	Time 0.391 (0.406)	Data 0.002 (0.003)	Loss 0.9167 (0.8601)	Acc@1 76.562 (80.793)	Acc@5 99.219 (99.004)
Epoch: [14][256/391]	Time 0.411 (0.406)	Data 0.002 (0.003)	Loss 0.9342 (0.8707)	Acc@1 78.906 (80.332)	Acc@5 96.875 (98.951)
Epoch: [14][320/391]	Time 0.394 (0.404)	Data 0.002 (0.003)	Loss 0.9471 (0.8696)	Acc@1 77.344 (80.223)	Acc@5 97.656 (98.915)
Epoch: [14][384/391]	Time 0.372 (0.404)	Data 0.001 (0.003)	Loss 1.0467 (0.8725)	Acc@1 75.781 (80.051)	Acc@5 96.875 (98.884)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.456 (0.456)	Data 0.227 (0.227)	Loss 0.9335 (0.9335)	Acc@1 78.125 (78.125)	Acc@5 96.875 (96.875)
Epoch: [15][64/391]	Time 0.381 (0.411)	Data 0.002 (0.005)	Loss 0.8893 (0.8680)	Acc@1 82.031 (80.709)	Acc@5 99.219 (98.942)
Epoch: [15][128/391]	Time 0.378 (0.407)	Data 0.002 (0.004)	Loss 1.0723 (0.8613)	Acc@1 76.562 (80.844)	Acc@5 96.875 (98.940)
Epoch: [15][192/391]	Time 0.411 (0.404)	Data 0.002 (0.003)	Loss 0.7537 (0.8615)	Acc@1 87.500 (80.776)	Acc@5 99.219 (98.915)
Epoch: [15][256/391]	Time 0.433 (0.404)	Data 0.002 (0.003)	Loss 0.9919 (0.8635)	Acc@1 81.250 (80.669)	Acc@5 97.656 (98.875)
Epoch: [15][320/391]	Time 0.386 (0.402)	Data 0.002 (0.003)	Loss 0.7969 (0.8673)	Acc@1 82.031 (80.449)	Acc@5 99.219 (98.885)
Epoch: [15][384/391]	Time 0.381 (0.402)	Data 0.002 (0.003)	Loss 0.8578 (0.8677)	Acc@1 76.562 (80.284)	Acc@5 98.438 (98.882)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 385642 ; 487386 ; 0.7912455425473854

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.426 (0.426)	Data 0.222 (0.222)	Loss 0.7296 (0.7296)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [16][64/391]	Time 0.503 (0.414)	Data 0.001 (0.005)	Loss 0.8767 (0.8313)	Acc@1 82.031 (81.142)	Acc@5 100.000 (99.099)
Epoch: [16][128/391]	Time 0.365 (0.407)	Data 0.002 (0.004)	Loss 0.7748 (0.8412)	Acc@1 83.594 (80.650)	Acc@5 99.219 (98.995)
Epoch: [16][192/391]	Time 0.383 (0.404)	Data 0.002 (0.003)	Loss 0.6457 (0.8482)	Acc@1 85.938 (80.331)	Acc@5 100.000 (98.968)
Epoch: [16][256/391]	Time 0.405 (0.403)	Data 0.002 (0.003)	Loss 0.7940 (0.8450)	Acc@1 79.688 (80.444)	Acc@5 99.219 (98.966)
Epoch: [16][320/391]	Time 0.386 (0.404)	Data 0.002 (0.003)	Loss 0.8514 (0.8473)	Acc@1 78.125 (80.298)	Acc@5 98.438 (98.944)
Epoch: [16][384/391]	Time 0.391 (0.404)	Data 0.002 (0.003)	Loss 0.7299 (0.8491)	Acc@1 83.594 (80.231)	Acc@5 100.000 (98.953)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.446 (0.446)	Data 0.270 (0.270)	Loss 0.8069 (0.8069)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [17][64/391]	Time 0.437 (0.406)	Data 0.002 (0.006)	Loss 0.8181 (0.8120)	Acc@1 78.125 (81.238)	Acc@5 99.219 (98.978)
Epoch: [17][128/391]	Time 0.386 (0.406)	Data 0.001 (0.004)	Loss 0.8146 (0.8231)	Acc@1 81.250 (81.117)	Acc@5 98.438 (98.958)
Epoch: [17][192/391]	Time 0.385 (0.404)	Data 0.002 (0.003)	Loss 0.7658 (0.8432)	Acc@1 85.156 (80.635)	Acc@5 100.000 (98.927)
Epoch: [17][256/391]	Time 0.292 (0.403)	Data 0.002 (0.003)	Loss 0.7166 (0.8371)	Acc@1 86.719 (80.852)	Acc@5 100.000 (98.930)
Epoch: [17][320/391]	Time 0.367 (0.404)	Data 0.002 (0.003)	Loss 0.8679 (0.8382)	Acc@1 82.031 (80.717)	Acc@5 96.875 (98.946)
Epoch: [17][384/391]	Time 0.393 (0.403)	Data 0.002 (0.003)	Loss 0.8026 (0.8371)	Acc@1 78.125 (80.769)	Acc@5 99.219 (98.931)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.424 (0.424)	Data 0.281 (0.281)	Loss 0.8027 (0.8027)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [18][64/391]	Time 0.488 (0.410)	Data 0.002 (0.006)	Loss 0.8674 (0.8245)	Acc@1 82.812 (80.829)	Acc@5 99.219 (99.207)
Epoch: [18][128/391]	Time 0.410 (0.409)	Data 0.002 (0.004)	Loss 0.9120 (0.8381)	Acc@1 80.469 (80.560)	Acc@5 98.438 (99.031)
Epoch: [18][192/391]	Time 0.373 (0.407)	Data 0.002 (0.003)	Loss 0.8325 (0.8372)	Acc@1 83.594 (80.691)	Acc@5 99.219 (99.037)
Epoch: [18][256/391]	Time 0.397 (0.405)	Data 0.002 (0.003)	Loss 0.8821 (0.8423)	Acc@1 78.125 (80.463)	Acc@5 99.219 (99.003)
Epoch: [18][320/391]	Time 0.403 (0.404)	Data 0.002 (0.003)	Loss 0.6666 (0.8394)	Acc@1 87.500 (80.605)	Acc@5 99.219 (98.961)
Epoch: [18][384/391]	Time 0.378 (0.404)	Data 0.001 (0.003)	Loss 0.8799 (0.8376)	Acc@1 83.594 (80.680)	Acc@5 100.000 (98.971)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.430 (0.430)	Data 0.214 (0.214)	Loss 0.7357 (0.7357)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)
Epoch: [19][64/391]	Time 0.434 (0.408)	Data 0.002 (0.005)	Loss 0.8515 (0.8126)	Acc@1 76.562 (81.334)	Acc@5 100.000 (99.038)
Epoch: [19][128/391]	Time 0.426 (0.406)	Data 0.002 (0.004)	Loss 0.8432 (0.8168)	Acc@1 77.344 (81.268)	Acc@5 100.000 (99.098)
Epoch: [19][192/391]	Time 0.420 (0.403)	Data 0.002 (0.003)	Loss 0.8129 (0.8304)	Acc@1 79.688 (80.671)	Acc@5 100.000 (99.033)
Epoch: [19][256/391]	Time 0.422 (0.402)	Data 0.002 (0.003)	Loss 0.7993 (0.8366)	Acc@1 82.812 (80.527)	Acc@5 99.219 (99.003)
Epoch: [19][320/391]	Time 0.379 (0.402)	Data 0.001 (0.003)	Loss 0.7981 (0.8352)	Acc@1 85.156 (80.564)	Acc@5 99.219 (99.007)
Epoch: [19][384/391]	Time 0.419 (0.401)	Data 0.002 (0.003)	Loss 0.7424 (0.8355)	Acc@1 83.594 (80.623)	Acc@5 99.219 (99.012)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.421 (0.421)	Data 0.243 (0.243)	Loss 0.7868 (0.7868)	Acc@1 78.906 (78.906)	Acc@5 97.656 (97.656)
Epoch: [20][64/391]	Time 0.438 (0.404)	Data 0.002 (0.006)	Loss 0.7543 (0.8271)	Acc@1 82.031 (81.490)	Acc@5 100.000 (99.002)
Epoch: [20][128/391]	Time 0.370 (0.402)	Data 0.002 (0.004)	Loss 0.8413 (0.8296)	Acc@1 78.906 (81.232)	Acc@5 97.656 (98.952)
Epoch: [20][192/391]	Time 0.401 (0.401)	Data 0.002 (0.003)	Loss 0.9028 (0.8273)	Acc@1 75.781 (81.205)	Acc@5 99.219 (98.935)
Epoch: [20][256/391]	Time 0.370 (0.400)	Data 0.002 (0.003)	Loss 0.7033 (0.8349)	Acc@1 82.812 (80.834)	Acc@5 100.000 (98.982)
Epoch: [20][320/391]	Time 0.428 (0.401)	Data 0.003 (0.003)	Loss 0.9977 (0.8331)	Acc@1 75.781 (80.904)	Acc@5 98.438 (98.980)
Epoch: [20][384/391]	Time 0.443 (0.400)	Data 0.002 (0.003)	Loss 0.7558 (0.8280)	Acc@1 78.906 (81.073)	Acc@5 100.000 (98.994)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
























 ab hier mit pruneTrain
