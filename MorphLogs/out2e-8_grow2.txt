no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=2e-08, logger='MorphLogs/logMorphNetFlops2e-8_grow2.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 23.52 (6082/25856)
Train | Batch (196/196) | Top-1: 28.38 (14188/50000)
Regular: 5.438053607940674
Epoche: 0; regular: 5.438053607940674: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 36.26
Epoch 1
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 39.42 (10193/25856)
Train | Batch (196/196) | Top-1: 42.08 (21039/50000)
Regular: 5.360780239105225
Epoche: 1; regular: 5.360780239105225: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 47.49
Epoch 2
Train | Batch (1/196) | Top-1: 45.70 (117/256)
Train | Batch (101/196) | Top-1: 48.72 (12597/25856)
Train | Batch (196/196) | Top-1: 49.85 (24924/50000)
Regular: 5.283349514007568
Epoche: 2; regular: 5.283349514007568: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 49.20
Epoch 3
Train | Batch (1/196) | Top-1: 50.00 (128/256)
Train | Batch (101/196) | Top-1: 52.95 (13692/25856)
Train | Batch (196/196) | Top-1: 54.08 (27042/50000)
Regular: 5.205965042114258
Epoche: 3; regular: 5.205965042114258: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 56.01
Epoch 4
Train | Batch (1/196) | Top-1: 54.69 (140/256)
Train | Batch (101/196) | Top-1: 56.99 (14735/25856)
Train | Batch (196/196) | Top-1: 57.80 (28901/50000)
Regular: 5.128605365753174
Epoche: 4; regular: 5.128605365753174: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 58.86
Epoch 5
Train | Batch (1/196) | Top-1: 58.59 (150/256)
Train | Batch (101/196) | Top-1: 60.08 (15535/25856)
Train | Batch (196/196) | Top-1: 60.68 (30341/50000)
Regular: 5.051314830780029
Epoche: 5; regular: 5.051314830780029: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 58.45
Epoch 6
Train | Batch (1/196) | Top-1: 59.77 (153/256)
Train | Batch (101/196) | Top-1: 62.13 (16064/25856)
Train | Batch (196/196) | Top-1: 62.68 (31342/50000)
Regular: 4.9741315841674805
Epoche: 6; regular: 4.9741315841674805: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 61.04
Epoch 7
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 64.30 (16626/25856)
Train | Batch (196/196) | Top-1: 64.67 (32337/50000)
Regular: 4.897049427032471
Epoche: 7; regular: 4.897049427032471: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 62.36
Epoch 8
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 65.93 (17046/25856)
Train | Batch (196/196) | Top-1: 66.24 (33121/50000)
Regular: 4.8200860023498535
Epoche: 8; regular: 4.8200860023498535: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 66.50
Epoch 9
Train | Batch (1/196) | Top-1: 64.84 (166/256)
Train | Batch (101/196) | Top-1: 67.28 (17397/25856)
Train | Batch (196/196) | Top-1: 67.57 (33787/50000)
Regular: 4.743152141571045
Epoche: 9; regular: 4.743152141571045: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 67.15
Epoch 10
Train | Batch (1/196) | Top-1: 70.31 (180/256)
Train | Batch (101/196) | Top-1: 68.47 (17703/25856)
Train | Batch (196/196) | Top-1: 68.89 (34443/50000)
Regular: 4.6663103103637695
Epoche: 10; regular: 4.6663103103637695: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 66.58
Epoch 11
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 69.87 (18066/25856)
Train | Batch (196/196) | Top-1: 69.73 (34866/50000)
Regular: 4.589524269104004
Epoche: 11; regular: 4.589524269104004: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 63.25
Epoch 12
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 70.94 (18343/25856)
Train | Batch (196/196) | Top-1: 71.07 (35537/50000)
Regular: 4.512829780578613
Epoche: 12; regular: 4.512829780578613: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 69.35
Epoch 13
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 71.81 (18566/25856)
Train | Batch (196/196) | Top-1: 71.92 (35959/50000)
Regular: 4.436197280883789
Epoche: 13; regular: 4.436197280883789: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 65.86
Epoch 14
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 72.90 (18849/25856)
Train | Batch (196/196) | Top-1: 72.84 (36420/50000)
Regular: 4.359688758850098
Epoche: 14; regular: 4.359688758850098: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 70.00
Epoch 15
Train | Batch (1/196) | Top-1: 69.14 (177/256)
Train | Batch (101/196) | Top-1: 73.41 (18981/25856)
Train | Batch (196/196) | Top-1: 73.82 (36911/50000)
Regular: 4.283300399780273
Epoche: 15; regular: 4.283300399780273: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 70.40
Epoch 16
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 74.48 (19257/25856)
Train | Batch (196/196) | Top-1: 74.70 (37348/50000)
Regular: 4.206965446472168
Epoche: 16; regular: 4.206965446472168: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 71.89
Epoch 17
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 75.55 (19533/25856)
Train | Batch (196/196) | Top-1: 75.24 (37621/50000)
Regular: 4.1308159828186035
Epoche: 17; regular: 4.1308159828186035: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.71
Epoch 18
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 76.25 (19715/25856)
Train | Batch (196/196) | Top-1: 76.30 (38148/50000)
Regular: 4.054636478424072
Epoche: 18; regular: 4.054636478424072: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 71.40
Epoch 19
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 77.10 (19935/25856)
Train | Batch (196/196) | Top-1: 77.01 (38506/50000)
Regular: 3.978642702102661
Epoche: 19; regular: 3.978642702102661: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.63
Epoch 20
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 77.41 (20014/25856)
Train | Batch (196/196) | Top-1: 77.61 (38805/50000)
Regular: 3.90281343460083
Epoche: 20; regular: 3.90281343460083: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.75
Epoch 21
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 78.56 (20312/25856)
Train | Batch (196/196) | Top-1: 78.51 (39257/50000)
Regular: 3.8270790576934814
Epoche: 21; regular: 3.8270790576934814: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.28
Epoch 22
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 78.81 (20377/25856)
Train | Batch (196/196) | Top-1: 79.09 (39546/50000)
Regular: 3.7515857219696045
Epoche: 22; regular: 3.7515857219696045: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.58
Epoch 23
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 79.73 (20616/25856)
Train | Batch (196/196) | Top-1: 79.54 (39772/50000)
Regular: 3.6761474609375
Epoche: 23; regular: 3.6761474609375: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.21
Epoch 24
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 80.36 (20777/25856)
Train | Batch (196/196) | Top-1: 80.11 (40054/50000)
Regular: 3.6008780002593994
Epoche: 24; regular: 3.6008780002593994: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.38
Epoch 25
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 80.80 (20892/25856)
Train | Batch (196/196) | Top-1: 80.47 (40237/50000)
Regular: 3.525810480117798
Epoche: 25; regular: 3.525810480117798: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.62
Epoch 26
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 81.09 (20966/25856)
Train | Batch (196/196) | Top-1: 81.11 (40557/50000)
Regular: 3.4509308338165283
Epoche: 26; regular: 3.4509308338165283: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.27
Epoch 27
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 81.25 (21008/25856)
Train | Batch (196/196) | Top-1: 81.32 (40660/50000)
Regular: 3.3764126300811768
Epoche: 27; regular: 3.3764126300811768: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.48
Epoch 28
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 81.64 (21110/25856)
Train | Batch (196/196) | Top-1: 81.82 (40910/50000)
Regular: 3.3020780086517334
Epoche: 28; regular: 3.3020780086517334: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.97
Epoch 29
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 82.28 (21274/25856)
Train | Batch (196/196) | Top-1: 82.37 (41185/50000)
Regular: 3.2279021739959717
Epoche: 29; regular: 3.2279021739959717: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.84
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 1
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 82.71 (21385/25856)
Train | Batch (196/196) | Top-1: 82.44 (41221/50000)
Regular: 3.157097101211548
Epoche: 0; regular: 3.157097101211548: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.71
Epoch 1
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 82.66 (21373/25856)
Train | Batch (196/196) | Top-1: 82.70 (41349/50000)
Regular: 3.083951950073242
Epoche: 1; regular: 3.083951950073242: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.69
Epoch 2
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 83.15 (21498/25856)
Train | Batch (196/196) | Top-1: 83.25 (41627/50000)
Regular: 3.010862350463867
Epoche: 2; regular: 3.010862350463867: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.90
Epoch 3
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 83.45 (21576/25856)
Train | Batch (196/196) | Top-1: 83.57 (41786/50000)
Regular: 2.9384453296661377
Epoche: 3; regular: 2.9384453296661377: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.87
Epoch 4
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 84.02 (21724/25856)
Train | Batch (196/196) | Top-1: 84.04 (42020/50000)
Regular: 2.86665678024292
Epoche: 4; regular: 2.86665678024292: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.34
Epoch 5
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.95 (21707/25856)
Train | Batch (196/196) | Top-1: 84.05 (42026/50000)
Regular: 2.7953946590423584
Epoche: 5; regular: 2.7953946590423584: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.29
Epoch 6
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 84.22 (21777/25856)
Train | Batch (196/196) | Top-1: 84.33 (42164/50000)
Regular: 2.725511312484741
Epoche: 6; regular: 2.725511312484741: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.85
Epoch 7
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 84.55 (21860/25856)
Train | Batch (196/196) | Top-1: 84.58 (42290/50000)
Regular: 2.6561672687530518
Epoche: 7; regular: 2.6561672687530518: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.68
Epoch 8
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 85.03 (21985/25856)
Train | Batch (196/196) | Top-1: 84.89 (42447/50000)
Regular: 2.5885443687438965
Epoche: 8; regular: 2.5885443687438965: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.04
Epoch 9
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 84.72 (21904/25856)
Train | Batch (196/196) | Top-1: 84.82 (42412/50000)
Regular: 2.523268461227417
Epoche: 9; regular: 2.523268461227417: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.99
Epoch 10
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 84.91 (21954/25856)
Train | Batch (196/196) | Top-1: 84.91 (42453/50000)
Regular: 2.46140718460083
Epoche: 10; regular: 2.46140718460083: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.60
Epoch 11
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 85.09 (22000/25856)
Train | Batch (196/196) | Top-1: 84.99 (42495/50000)
Regular: 2.4054861068725586
Epoche: 11; regular: 2.4054861068725586: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.62
Epoch 12
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 85.05 (21990/25856)
Train | Batch (196/196) | Top-1: 85.14 (42569/50000)
Regular: 2.3552253246307373
Epoche: 12; regular: 2.3552253246307373: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.02
Epoch 13
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 85.26 (22044/25856)
Train | Batch (196/196) | Top-1: 85.31 (42657/50000)
Regular: 2.3104400634765625
Epoche: 13; regular: 2.3104400634765625: flops 68862592
#Filters: 1135, #FLOPs: 68.72M | Top-1: 77.58
Epoch 14
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 85.54 (22116/25856)
Train | Batch (196/196) | Top-1: 85.44 (42718/50000)
Regular: 2.2668464183807373
Epoche: 14; regular: 2.2668464183807373: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 80.03
Epoch 15
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 85.74 (22168/25856)
Train | Batch (196/196) | Top-1: 85.48 (42742/50000)
Regular: 2.223667860031128
Epoche: 15; regular: 2.223667860031128: flops 68862592
#Filters: 1135, #FLOPs: 68.72M | Top-1: 80.09
Epoch 16
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 85.89 (22209/25856)
Train | Batch (196/196) | Top-1: 85.95 (42976/50000)
Regular: 2.1823410987854004
Epoche: 16; regular: 2.1823410987854004: flops 68862592
#Filters: 1135, #FLOPs: 68.72M | Top-1: 78.49
Epoch 17
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 86.11 (22265/25856)
Train | Batch (196/196) | Top-1: 85.99 (42995/50000)
Regular: 2.141115665435791
Epoche: 17; regular: 2.141115665435791: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 73.92
Epoch 18
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 86.18 (22283/25856)
Train | Batch (196/196) | Top-1: 86.33 (43164/50000)
Regular: 2.10068941116333
Epoche: 18; regular: 2.10068941116333: flops 68862592
#Filters: 1135, #FLOPs: 68.72M | Top-1: 80.25
Epoch 19
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 86.33 (22321/25856)
Train | Batch (196/196) | Top-1: 86.26 (43130/50000)
Regular: 2.0608625411987305
Epoche: 19; regular: 2.0608625411987305: flops 68862592
#Filters: 1135, #FLOPs: 68.72M | Top-1: 80.75
Epoch 20
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 86.83 (22451/25856)
Train | Batch (196/196) | Top-1: 86.62 (43310/50000)
Regular: 2.0217883586883545
Epoche: 20; regular: 2.0217883586883545: flops 68862592
#Filters: 1135, #FLOPs: 68.72M | Top-1: 78.74
Epoch 21
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 86.59 (22388/25856)
Train | Batch (196/196) | Top-1: 86.63 (43314/50000)
Regular: 1.9837335348129272
Epoche: 21; regular: 1.9837335348129272: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 80.46
Epoch 22
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 86.82 (22447/25856)
Train | Batch (196/196) | Top-1: 86.70 (43349/50000)
Regular: 1.9468779563903809
Epoche: 22; regular: 1.9468779563903809: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 79.22
Epoch 23
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 86.68 (22413/25856)
Train | Batch (196/196) | Top-1: 86.77 (43383/50000)
Regular: 1.910560131072998
Epoche: 23; regular: 1.910560131072998: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 67.89
Epoch 24
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 87.05 (22508/25856)
Train | Batch (196/196) | Top-1: 87.12 (43560/50000)
Regular: 1.8751417398452759
Epoche: 24; regular: 1.8751417398452759: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 80.85
Epoch 25
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 87.55 (22636/25856)
Train | Batch (196/196) | Top-1: 87.37 (43686/50000)
Regular: 1.8403513431549072
Epoche: 25; regular: 1.8403513431549072: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 79.97
Epoch 26
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.69 (22672/25856)
Train | Batch (196/196) | Top-1: 87.39 (43693/50000)
Regular: 1.8064051866531372
Epoche: 26; regular: 1.8064051866531372: flops 68862592
#Filters: 1133, #FLOPs: 68.42M | Top-1: 76.45
Epoch 27
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 87.45 (22610/25856)
Train | Batch (196/196) | Top-1: 87.31 (43653/50000)
Regular: 1.7742763757705688
Epoche: 27; regular: 1.7742763757705688: flops 68862592
#Filters: 1133, #FLOPs: 68.42M | Top-1: 82.54
Epoch 28
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 87.60 (22650/25856)
Train | Batch (196/196) | Top-1: 87.69 (43845/50000)
Regular: 1.7404190301895142
Epoche: 28; regular: 1.7404190301895142: flops 68862592
#Filters: 1133, #FLOPs: 68.42M | Top-1: 73.21
Epoch 29
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 87.82 (22706/25856)
Train | Batch (196/196) | Top-1: 87.70 (43848/50000)
Regular: 1.7082270383834839
Epoche: 29; regular: 1.7082270383834839: flops 68862592
#Filters: 1133, #FLOPs: 68.42M | Top-1: 80.05
Drin!!
Layers that will be prunned: [(7, 3)]
Prunning filters..
Layer index: 7; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 67.978M | #Params: 0.461M
1.0065076990073354
After Growth | FLOPs: 67.978M | #Params: 0.461M
I: 2
flops: 67977856
Before Pruning | FLOPs: 67.978M | #Params: 0.461M
Epoch 0
Train | Batch (1/196) | Top-1: 13.67 (35/256)
Train | Batch (101/196) | Top-1: 26.24 (6784/25856)
Train | Batch (196/196) | Top-1: 32.10 (16050/50000)
Regular: 1.6623320579528809
Epoche: 0; regular: 1.6623320579528809: flops 67977856
#Filters: 1101, #FLOPs: 63.26M | Top-1: 23.20
Epoch 1
Train | Batch (1/196) | Top-1: 41.02 (105/256)
Train | Batch (101/196) | Top-1: 44.11 (11404/25856)
Train | Batch (196/196) | Top-1: 46.20 (23098/50000)
Regular: 1.6201810836791992
Epoche: 1; regular: 1.6201810836791992: flops 67977856
#Filters: 1095, #FLOPs: 62.37M | Top-1: 49.05
Epoch 2
Train | Batch (1/196) | Top-1: 55.86 (143/256)
Train | Batch (101/196) | Top-1: 53.26 (13771/25856)
Train | Batch (196/196) | Top-1: 54.41 (27204/50000)
Regular: 1.5880478620529175
Epoche: 2; regular: 1.5880478620529175: flops 67977856
#Filters: 1088, #FLOPs: 61.34M | Top-1: 48.61
Epoch 3
Train | Batch (1/196) | Top-1: 56.25 (144/256)
Train | Batch (101/196) | Top-1: 57.85 (14958/25856)
Train | Batch (196/196) | Top-1: 58.94 (29469/50000)
Regular: 1.5575084686279297
Epoche: 3; regular: 1.5575084686279297: flops 67977856
#Filters: 1085, #FLOPs: 60.90M | Top-1: 49.97
Epoch 4
Train | Batch (1/196) | Top-1: 56.25 (144/256)
Train | Batch (101/196) | Top-1: 61.82 (15985/25856)
Train | Batch (196/196) | Top-1: 62.38 (31188/50000)
Regular: 1.5248104333877563
Epoche: 4; regular: 1.5248104333877563: flops 67977856
#Filters: 1085, #FLOPs: 60.90M | Top-1: 61.45
Epoch 5
Train | Batch (1/196) | Top-1: 61.33 (157/256)
Train | Batch (101/196) | Top-1: 64.40 (16652/25856)
Train | Batch (196/196) | Top-1: 64.85 (32424/50000)
Regular: 1.4938058853149414
Epoche: 5; regular: 1.4938058853149414: flops 67977856
#Filters: 1082, #FLOPs: 60.46M | Top-1: 52.77
Epoch 6
Train | Batch (1/196) | Top-1: 69.92 (179/256)
Train | Batch (101/196) | Top-1: 66.66 (17236/25856)
Train | Batch (196/196) | Top-1: 66.91 (33456/50000)
Regular: 1.4626713991165161
Epoche: 6; regular: 1.4626713991165161: flops 67977856
#Filters: 1080, #FLOPs: 60.16M | Top-1: 62.84
Epoch 7
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 68.41 (17688/25856)
Train | Batch (196/196) | Top-1: 68.98 (34492/50000)
Regular: 1.4308269023895264
Epoche: 7; regular: 1.4308269023895264: flops 67977856
#Filters: 1080, #FLOPs: 60.16M | Top-1: 68.28
Epoch 8
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 70.54 (18238/25856)
Train | Batch (196/196) | Top-1: 70.75 (35374/50000)
Regular: 1.399228572845459
Epoche: 8; regular: 1.399228572845459: flops 67977856
#Filters: 1081, #FLOPs: 60.31M | Top-1: 63.76
Epoch 9
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 71.98 (18612/25856)
Train | Batch (196/196) | Top-1: 72.05 (36027/50000)
Regular: 1.3683462142944336
Epoche: 9; regular: 1.3683462142944336: flops 67977856
#Filters: 1080, #FLOPs: 60.16M | Top-1: 70.52
Epoch 10
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 73.10 (18900/25856)
Train | Batch (196/196) | Top-1: 73.24 (36619/50000)
Regular: 1.33735191822052
Epoche: 10; regular: 1.33735191822052: flops 67977856
#Filters: 1079, #FLOPs: 60.02M | Top-1: 69.47
Epoch 11
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 74.42 (19243/25856)
Train | Batch (196/196) | Top-1: 74.37 (37183/50000)
Regular: 1.3077846765518188
Epoche: 11; regular: 1.3077846765518188: flops 67977856
#Filters: 1081, #FLOPs: 60.31M | Top-1: 67.60
Epoch 12
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 75.08 (19412/25856)
Train | Batch (196/196) | Top-1: 75.47 (37733/50000)
Regular: 1.2781696319580078
Epoche: 12; regular: 1.2781696319580078: flops 67977856
#Filters: 1079, #FLOPs: 60.02M | Top-1: 71.39
Epoch 13
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 75.75 (19586/25856)
Train | Batch (196/196) | Top-1: 76.22 (38108/50000)
Regular: 1.2489988803863525
Epoche: 13; regular: 1.2489988803863525: flops 67977856
#Filters: 1077, #FLOPs: 59.72M | Top-1: 71.26
Epoch 14
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 77.41 (20014/25856)
Train | Batch (196/196) | Top-1: 77.08 (38542/50000)
Regular: 1.2198598384857178
Epoche: 14; regular: 1.2198598384857178: flops 67977856
#Filters: 1078, #FLOPs: 59.87M | Top-1: 67.07
Epoch 15
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 77.58 (20059/25856)
Train | Batch (196/196) | Top-1: 77.47 (38733/50000)
Regular: 1.1919817924499512
Epoche: 15; regular: 1.1919817924499512: flops 67977856
#Filters: 1076, #FLOPs: 59.57M | Top-1: 75.43
Epoch 16
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 78.22 (20225/25856)
Train | Batch (196/196) | Top-1: 78.32 (39158/50000)
Regular: 1.1644545793533325
Epoche: 16; regular: 1.1644545793533325: flops 67977856
#Filters: 1077, #FLOPs: 59.72M | Top-1: 75.03
Epoch 17
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.09 (20450/25856)
Train | Batch (196/196) | Top-1: 78.72 (39358/50000)
Regular: 1.1383401155471802
Epoche: 17; regular: 1.1383401155471802: flops 67977856
#Filters: 1078, #FLOPs: 59.87M | Top-1: 73.05
Epoch 18
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 79.06 (20441/25856)
Train | Batch (196/196) | Top-1: 79.27 (39635/50000)
Regular: 1.1140012741088867
Epoche: 18; regular: 1.1140012741088867: flops 67977856
#Filters: 1079, #FLOPs: 60.02M | Top-1: 76.00
Epoch 19
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 79.75 (20620/25856)
Train | Batch (196/196) | Top-1: 79.74 (39872/50000)
Regular: 1.0906227827072144
Epoche: 19; regular: 1.0906227827072144: flops 67977856
#Filters: 1077, #FLOPs: 59.72M | Top-1: 66.48
Epoch 20
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 79.64 (20593/25856)
Train | Batch (196/196) | Top-1: 79.99 (39995/50000)
Regular: 1.0684758424758911
Epoche: 20; regular: 1.0684758424758911: flops 67977856
#Filters: 1076, #FLOPs: 59.57M | Top-1: 69.43
Epoch 21
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 80.31 (20764/25856)
Train | Batch (196/196) | Top-1: 80.26 (40128/50000)
Regular: 1.0485916137695312
Epoche: 21; regular: 1.0485916137695312: flops 67977856
#Filters: 1078, #FLOPs: 59.87M | Top-1: 75.46
Epoch 22
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 81.15 (20983/25856)
Train | Batch (196/196) | Top-1: 80.71 (40354/50000)
Regular: 1.0300523042678833
Epoche: 22; regular: 1.0300523042678833: flops 67977856
#Filters: 1078, #FLOPs: 59.87M | Top-1: 77.49
Epoch 23
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 80.88 (20912/25856)
Train | Batch (196/196) | Top-1: 81.09 (40543/50000)
Regular: 1.0134730339050293
Epoche: 23; regular: 1.0134730339050293: flops 67977856
#Filters: 1077, #FLOPs: 59.72M | Top-1: 73.63
Epoch 24
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 81.34 (21031/25856)
Train | Batch (196/196) | Top-1: 81.34 (40669/50000)
Regular: 0.9978577494621277
Epoche: 24; regular: 0.9978577494621277: flops 67977856
#Filters: 1076, #FLOPs: 59.57M | Top-1: 71.87
Epoch 25
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 81.85 (21164/25856)
Train | Batch (196/196) | Top-1: 81.60 (40802/50000)
Regular: 0.9827189445495605
Epoche: 25; regular: 0.9827189445495605: flops 67977856
#Filters: 1075, #FLOPs: 59.50M | Top-1: 70.88
Epoch 26
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 82.00 (21202/25856)
Train | Batch (196/196) | Top-1: 82.00 (40998/50000)
Regular: 0.9685244560241699
Epoche: 26; regular: 0.9685244560241699: flops 67977856
#Filters: 1077, #FLOPs: 59.72M | Top-1: 76.19
Epoch 27
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 82.13 (21236/25856)
Train | Batch (196/196) | Top-1: 82.24 (41122/50000)
Regular: 0.9540771245956421
Epoche: 27; regular: 0.9540771245956421: flops 67977856
#Filters: 1074, #FLOPs: 59.43M | Top-1: 70.26
Epoch 28
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 82.85 (21422/25856)
Train | Batch (196/196) | Top-1: 82.43 (41216/50000)
Regular: 0.940529465675354
Epoche: 28; regular: 0.940529465675354: flops 67977856
#Filters: 1074, #FLOPs: 59.43M | Top-1: 66.77
Epoch 29
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 82.63 (21365/25856)
Train | Batch (196/196) | Top-1: 82.74 (41372/50000)
Regular: 0.9266422986984253
Epoche: 29; regular: 0.9266422986984253: flops 67977856
#Filters: 1074, #FLOPs: 59.50M | Top-1: 75.83
Drin!!
Layers that will be prunned: [(1, 11), (3, 10), (5, 15), (7, 10), (9, 10), (17, 3)]
Prunning filters..
Layer index: 1; Pruned filters: 2
Layer index: 1; Pruned filters: 5
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 2
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 2
Layer index: 3; Pruned filters: 2
Layer index: 3; Pruned filters: 6
Layer index: 5; Pruned filters: 3
Layer index: 5; Pruned filters: 12
Layer index: 7; Pruned filters: 1
Layer index: 7; Pruned filters: 9
Layer index: 9; Pruned filters: 2
Layer index: 9; Pruned filters: 3
Layer index: 9; Pruned filters: 4
Layer index: 9; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 51.020M | #Params: 0.443M
1.16247432879258
After Growth | FLOPs: 68.392M | #Params: 0.593M
I: 3
flops: 68391524
Before Pruning | FLOPs: 68.392M | #Params: 0.593M
Epoch 0
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 81.39 (21045/25856)
Train | Batch (196/196) | Top-1: 81.87 (40933/50000)
Regular: 1.782212257385254
Epoche: 0; regular: 1.782212257385254: flops 68391524
#Filters: 1240, #FLOPs: 67.96M | Top-1: 73.07
Epoch 1
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 82.90 (21435/25856)
Train | Batch (196/196) | Top-1: 82.92 (41459/50000)
Regular: 1.7536976337432861
Epoche: 1; regular: 1.7536976337432861: flops 68391524
#Filters: 1240, #FLOPs: 67.96M | Top-1: 78.21
Epoch 2
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 83.42 (21568/25856)
Train | Batch (196/196) | Top-1: 83.32 (41660/50000)
Regular: 1.7270883321762085
Epoche: 2; regular: 1.7270883321762085: flops 68391524
#Filters: 1239, #FLOPs: 67.87M | Top-1: 78.19
Epoch 3
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 83.85 (21681/25856)
Train | Batch (196/196) | Top-1: 83.77 (41884/50000)
Regular: 1.7012741565704346
Epoche: 3; regular: 1.7012741565704346: flops 68391524
#Filters: 1239, #FLOPs: 67.87M | Top-1: 74.13
Epoch 4
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 83.78 (21663/25856)
Train | Batch (196/196) | Top-1: 84.00 (42001/50000)
Regular: 1.675880789756775
Epoche: 4; regular: 1.675880789756775: flops 68391524
#Filters: 1239, #FLOPs: 67.87M | Top-1: 73.87
Epoch 5
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 84.22 (21776/25856)
Train | Batch (196/196) | Top-1: 83.99 (41993/50000)
Regular: 1.6504263877868652
Epoche: 5; regular: 1.6504263877868652: flops 68391524
#Filters: 1238, #FLOPs: 67.79M | Top-1: 73.91
Epoch 6
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.45 (21836/25856)
Train | Batch (196/196) | Top-1: 84.36 (42182/50000)
Regular: 1.6255717277526855
Epoche: 6; regular: 1.6255717277526855: flops 68391524
#Filters: 1238, #FLOPs: 67.79M | Top-1: 79.31
Epoch 7
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 84.48 (21844/25856)
Train | Batch (196/196) | Top-1: 84.45 (42225/50000)
Regular: 1.6004018783569336
Epoche: 7; regular: 1.6004018783569336: flops 68391524
#Filters: 1238, #FLOPs: 67.79M | Top-1: 77.84
Epoch 8
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 84.75 (21913/25856)
Train | Batch (196/196) | Top-1: 84.81 (42407/50000)
Regular: 1.5761337280273438
Epoche: 8; regular: 1.5761337280273438: flops 68391524
#Filters: 1237, #FLOPs: 67.70M | Top-1: 72.58
Epoch 9
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 84.70 (21899/25856)
Train | Batch (196/196) | Top-1: 84.89 (42445/50000)
Regular: 1.552228331565857
Epoche: 9; regular: 1.552228331565857: flops 68391524
#Filters: 1235, #FLOPs: 67.44M | Top-1: 77.21
Epoch 10
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 85.28 (22051/25856)
Train | Batch (196/196) | Top-1: 85.22 (42610/50000)
Regular: 1.528140902519226
Epoche: 10; regular: 1.528140902519226: flops 68391524
#Filters: 1235, #FLOPs: 67.44M | Top-1: 76.06
Epoch 11
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 85.23 (22038/25856)
Train | Batch (196/196) | Top-1: 85.29 (42644/50000)
Regular: 1.5044554471969604
Epoche: 11; regular: 1.5044554471969604: flops 68391524
#Filters: 1234, #FLOPs: 67.35M | Top-1: 77.48
Epoch 12
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 85.68 (22153/25856)
Train | Batch (196/196) | Top-1: 85.45 (42727/50000)
Regular: 1.4809457063674927
Epoche: 12; regular: 1.4809457063674927: flops 68391524
#Filters: 1235, #FLOPs: 67.44M | Top-1: 68.38
Epoch 13
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 85.74 (22170/25856)
Train | Batch (196/196) | Top-1: 85.57 (42784/50000)
Regular: 1.4582709074020386
Epoche: 13; regular: 1.4582709074020386: flops 68391524
#Filters: 1236, #FLOPs: 67.53M | Top-1: 80.43
Epoch 14
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 85.39 (22078/25856)
Train | Batch (196/196) | Top-1: 85.67 (42833/50000)
Regular: 1.4359118938446045
Epoche: 14; regular: 1.4359118938446045: flops 68391524
#Filters: 1235, #FLOPs: 67.44M | Top-1: 74.82
Epoch 15
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 85.79 (22182/25856)
Train | Batch (196/196) | Top-1: 85.88 (42939/50000)
Regular: 1.4137028455734253
Epoche: 15; regular: 1.4137028455734253: flops 68391524
#Filters: 1233, #FLOPs: 67.27M | Top-1: 70.88
Epoch 16
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 86.14 (22272/25856)
Train | Batch (196/196) | Top-1: 86.08 (43041/50000)
Regular: 1.3913811445236206
Epoche: 16; regular: 1.3913811445236206: flops 68391524
#Filters: 1233, #FLOPs: 67.27M | Top-1: 75.94
Epoch 17
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 86.37 (22331/25856)
Train | Batch (196/196) | Top-1: 86.37 (43184/50000)
Regular: 1.3689825534820557
Epoche: 17; regular: 1.3689825534820557: flops 68391524
#Filters: 1234, #FLOPs: 67.44M | Top-1: 79.14
Epoch 18
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 86.42 (22345/25856)
Train | Batch (196/196) | Top-1: 86.36 (43179/50000)
Regular: 1.3474884033203125
Epoche: 18; regular: 1.3474884033203125: flops 68391524
#Filters: 1234, #FLOPs: 67.44M | Top-1: 80.88
Epoch 19
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 86.90 (22469/25856)
Train | Batch (196/196) | Top-1: 86.58 (43289/50000)
Regular: 1.325100302696228
Epoche: 19; regular: 1.325100302696228: flops 68391524
#Filters: 1234, #FLOPs: 67.35M | Top-1: 70.62
Epoch 20
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 86.92 (22474/25856)
Train | Batch (196/196) | Top-1: 86.67 (43337/50000)
Regular: 1.303407907485962
Epoche: 20; regular: 1.303407907485962: flops 68391524
#Filters: 1234, #FLOPs: 67.35M | Top-1: 76.55
Epoch 21
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 86.88 (22463/25856)
Train | Batch (196/196) | Top-1: 86.75 (43374/50000)
Regular: 1.2824468612670898
Epoche: 21; regular: 1.2824468612670898: flops 68391524
#Filters: 1234, #FLOPs: 67.35M | Top-1: 80.04
Epoch 22
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 87.04 (22504/25856)
Train | Batch (196/196) | Top-1: 87.06 (43529/50000)
Regular: 1.2608225345611572
Epoche: 22; regular: 1.2608225345611572: flops 68391524
#Filters: 1233, #FLOPs: 67.27M | Top-1: 80.63
Epoch 23
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 87.15 (22533/25856)
Train | Batch (196/196) | Top-1: 87.15 (43577/50000)
Regular: 1.2395268678665161
Epoche: 23; regular: 1.2395268678665161: flops 68391524
#Filters: 1232, #FLOPs: 67.18M | Top-1: 75.58
Epoch 24
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 87.21 (22548/25856)
Train | Batch (196/196) | Top-1: 87.30 (43652/50000)
Regular: 1.2179173231124878
Epoche: 24; regular: 1.2179173231124878: flops 68391524
#Filters: 1231, #FLOPs: 67.01M | Top-1: 75.11
Epoch 25
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 87.47 (22615/25856)
Train | Batch (196/196) | Top-1: 87.24 (43622/50000)
Regular: 1.1962218284606934
Epoche: 25; regular: 1.1962218284606934: flops 68391524
#Filters: 1231, #FLOPs: 67.01M | Top-1: 73.96
Epoch 26
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 87.48 (22619/25856)
Train | Batch (196/196) | Top-1: 87.49 (43744/50000)
Regular: 1.1755000352859497
Epoche: 26; regular: 1.1755000352859497: flops 68391524
#Filters: 1233, #FLOPs: 67.27M | Top-1: 78.87
Epoch 27
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 87.72 (22682/25856)
Train | Batch (196/196) | Top-1: 87.75 (43876/50000)
Regular: 1.1536684036254883
Epoche: 27; regular: 1.1536684036254883: flops 68391524
#Filters: 1233, #FLOPs: 67.27M | Top-1: 75.17
Epoch 28
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.96 (22743/25856)
Train | Batch (196/196) | Top-1: 87.80 (43899/50000)
Regular: 1.1330878734588623
Epoche: 28; regular: 1.1330878734588623: flops 68391524
#Filters: 1234, #FLOPs: 67.35M | Top-1: 71.47
Epoch 29
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 87.68 (22671/25856)
Train | Batch (196/196) | Top-1: 87.72 (43859/50000)
Regular: 1.111403465270996
Epoche: 29; regular: 1.111403465270996: flops 68391524
#Filters: 1232, #FLOPs: 67.18M | Top-1: 74.87
Drin!!
Layers that will be prunned: [(3, 1), (7, 1), (9, 1), (15, 1), (17, 6), (19, 1)]
Prunning filters..
Layer index: 3; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 17; Pruned filters: 2
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 65.977M | #Params: 0.587M
1.021721272954166
After Growth | FLOPs: 69.167M | #Params: 0.618M
I: 4
flops: 69166840
Before Pruning | FLOPs: 69.167M | #Params: 0.618M
Epoch 0
Train | Batch (1/196) | Top-1: 7.42 (19/256)
Train | Batch (101/196) | Top-1: 32.38 (8373/25856)
Train | Batch (196/196) | Top-1: 37.29 (18645/50000)
Regular: 1.2230637073516846
Epoche: 0; regular: 1.2230637073516846: flops 69166840
#Filters: 1221, #FLOPs: 65.18M | Top-1: 28.34
Epoch 1
Train | Batch (1/196) | Top-1: 44.92 (115/256)
Train | Batch (101/196) | Top-1: 46.07 (11912/25856)
Train | Batch (196/196) | Top-1: 47.42 (23710/50000)
Regular: 1.183079481124878
Epoche: 1; regular: 1.183079481124878: flops 69166840
#Filters: 1183, #FLOPs: 62.21M | Top-1: 35.03
Epoch 2
Train | Batch (1/196) | Top-1: 51.17 (131/256)
Train | Batch (101/196) | Top-1: 51.26 (13255/25856)
Train | Batch (196/196) | Top-1: 52.53 (26266/50000)
Regular: 1.1537545919418335
Epoche: 2; regular: 1.1537545919418335: flops 69166840
#Filters: 1171, #FLOPs: 61.11M | Top-1: 52.73
Epoch 3
Train | Batch (1/196) | Top-1: 58.20 (149/256)
Train | Batch (101/196) | Top-1: 55.77 (14421/25856)
Train | Batch (196/196) | Top-1: 56.65 (28324/50000)
Regular: 1.1300969123840332
Epoche: 3; regular: 1.1300969123840332: flops 69166840
#Filters: 1168, #FLOPs: 60.94M | Top-1: 52.33
Epoch 4
Train | Batch (1/196) | Top-1: 55.08 (141/256)
Train | Batch (101/196) | Top-1: 59.63 (15419/25856)
Train | Batch (196/196) | Top-1: 60.53 (30266/50000)
Regular: 1.1078180074691772
Epoche: 4; regular: 1.1078180074691772: flops 69166840
#Filters: 1165, #FLOPs: 60.54M | Top-1: 53.65
Epoch 5
Train | Batch (1/196) | Top-1: 63.28 (162/256)
Train | Batch (101/196) | Top-1: 63.17 (16334/25856)
Train | Batch (196/196) | Top-1: 63.80 (31902/50000)
Regular: 1.0863139629364014
Epoche: 5; regular: 1.0863139629364014: flops 69166840
#Filters: 1160, #FLOPs: 60.06M | Top-1: 51.53
Epoch 6
Train | Batch (1/196) | Top-1: 64.45 (165/256)
Train | Batch (101/196) | Top-1: 65.57 (16953/25856)
Train | Batch (196/196) | Top-1: 66.32 (33161/50000)
Regular: 1.0639313459396362
Epoche: 6; regular: 1.0639313459396362: flops 69166840
#Filters: 1158, #FLOPs: 59.93M | Top-1: 60.31
Epoch 7
Train | Batch (1/196) | Top-1: 65.23 (167/256)
Train | Batch (101/196) | Top-1: 68.22 (17638/25856)
Train | Batch (196/196) | Top-1: 68.58 (34288/50000)
Regular: 1.0418500900268555
Epoche: 7; regular: 1.0418500900268555: flops 69166840
#Filters: 1158, #FLOPs: 59.89M | Top-1: 59.36
Epoch 8
Train | Batch (1/196) | Top-1: 68.36 (175/256)
Train | Batch (101/196) | Top-1: 69.54 (17981/25856)
Train | Batch (196/196) | Top-1: 70.10 (35052/50000)
Regular: 1.0200353860855103
Epoche: 8; regular: 1.0200353860855103: flops 69166840
#Filters: 1157, #FLOPs: 59.80M | Top-1: 58.21
Epoch 9
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 71.66 (18528/25856)
Train | Batch (196/196) | Top-1: 72.08 (36039/50000)
Regular: 0.9985204935073853
Epoche: 9; regular: 0.9985204935073853: flops 69166840
#Filters: 1157, #FLOPs: 59.71M | Top-1: 60.58
Epoch 10
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 72.97 (18866/25856)
Train | Batch (196/196) | Top-1: 73.51 (36753/50000)
Regular: 0.9768269658088684
Epoche: 10; regular: 0.9768269658088684: flops 69166840
#Filters: 1157, #FLOPs: 59.71M | Top-1: 62.16
Epoch 11
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 74.56 (19278/25856)
Train | Batch (196/196) | Top-1: 74.62 (37311/50000)
Regular: 0.9553270936012268
Epoche: 11; regular: 0.9553270936012268: flops 69166840
#Filters: 1157, #FLOPs: 59.71M | Top-1: 69.05
Epoch 12
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 75.43 (19502/25856)
Train | Batch (196/196) | Top-1: 75.63 (37814/50000)
Regular: 0.9339574575424194
Epoche: 12; regular: 0.9339574575424194: flops 69166840
#Filters: 1158, #FLOPs: 59.76M | Top-1: 73.28
Epoch 13
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 76.48 (19774/25856)
Train | Batch (196/196) | Top-1: 76.72 (38360/50000)
Regular: 0.9130470752716064
Epoche: 13; regular: 0.9130470752716064: flops 69166840
#Filters: 1157, #FLOPs: 59.71M | Top-1: 74.76
Epoch 14
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 77.15 (19947/25856)
Train | Batch (196/196) | Top-1: 77.52 (38758/50000)
Regular: 0.8917236924171448
Epoche: 14; regular: 0.8917236924171448: flops 69166840
#Filters: 1156, #FLOPs: 59.62M | Top-1: 70.67
Epoch 15
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 78.47 (20290/25856)
Train | Batch (196/196) | Top-1: 78.41 (39203/50000)
Regular: 0.8712595701217651
Epoche: 15; regular: 0.8712595701217651: flops 69166840
#Filters: 1156, #FLOPs: 59.62M | Top-1: 72.02
Epoch 16
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 79.34 (20513/25856)
Train | Batch (196/196) | Top-1: 79.05 (39525/50000)
Regular: 0.850785493850708
Epoche: 16; regular: 0.850785493850708: flops 69166840
#Filters: 1155, #FLOPs: 59.54M | Top-1: 72.89
Epoch 17
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.70 (20607/25856)
Train | Batch (196/196) | Top-1: 79.75 (39875/50000)
Regular: 0.8301663994789124
Epoche: 17; regular: 0.8301663994789124: flops 69166840
#Filters: 1157, #FLOPs: 59.67M | Top-1: 73.42
Epoch 18
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 79.64 (20591/25856)
Train | Batch (196/196) | Top-1: 80.00 (40000/50000)
Regular: 0.8099676370620728
Epoche: 18; regular: 0.8099676370620728: flops 69166840
#Filters: 1156, #FLOPs: 59.49M | Top-1: 72.17
Epoch 19
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 80.98 (20938/25856)
Train | Batch (196/196) | Top-1: 80.69 (40345/50000)
Regular: 0.7906950116157532
Epoche: 19; regular: 0.7906950116157532: flops 69166840
#Filters: 1155, #FLOPs: 59.40M | Top-1: 70.90
Epoch 20
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 81.03 (20950/25856)
Train | Batch (196/196) | Top-1: 81.07 (40534/50000)
Regular: 0.7712944149971008
Epoche: 20; regular: 0.7712944149971008: flops 69166840
#Filters: 1156, #FLOPs: 59.49M | Top-1: 77.68
Epoch 21
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 81.55 (21085/25856)
Train | Batch (196/196) | Top-1: 81.33 (40666/50000)
Regular: 0.7532656192779541
Epoche: 21; regular: 0.7532656192779541: flops 69166840
#Filters: 1156, #FLOPs: 59.58M | Top-1: 68.91
Epoch 22
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 81.55 (21085/25856)
Train | Batch (196/196) | Top-1: 81.68 (40840/50000)
Regular: 0.7348151803016663
Epoche: 22; regular: 0.7348151803016663: flops 69166840
#Filters: 1156, #FLOPs: 59.58M | Top-1: 73.79
Epoch 23
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 81.95 (21188/25856)
Train | Batch (196/196) | Top-1: 81.95 (40974/50000)
Regular: 0.7173761129379272
Epoche: 23; regular: 0.7173761129379272: flops 69166840
#Filters: 1155, #FLOPs: 59.40M | Top-1: 79.69
Epoch 24
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 82.28 (21275/25856)
Train | Batch (196/196) | Top-1: 82.17 (41083/50000)
Regular: 0.7002444267272949
Epoche: 24; regular: 0.7002444267272949: flops 69166840
#Filters: 1155, #FLOPs: 59.40M | Top-1: 78.28
Epoch 25
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 82.83 (21416/25856)
Train | Batch (196/196) | Top-1: 82.47 (41233/50000)
Regular: 0.6837549209594727
Epoche: 25; regular: 0.6837549209594727: flops 69166840
#Filters: 1154, #FLOPs: 59.36M | Top-1: 75.86
Epoch 26
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 82.51 (21335/25856)
Train | Batch (196/196) | Top-1: 82.75 (41377/50000)
Regular: 0.6681263446807861
Epoche: 26; regular: 0.6681263446807861: flops 69166840
#Filters: 1153, #FLOPs: 59.32M | Top-1: 78.03
Epoch 27
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 83.27 (21530/25856)
Train | Batch (196/196) | Top-1: 83.05 (41527/50000)
Regular: 0.6532862186431885
Epoche: 27; regular: 0.6532862186431885: flops 69166840
#Filters: 1154, #FLOPs: 59.40M | Top-1: 69.03
Epoch 28
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 83.55 (21602/25856)
Train | Batch (196/196) | Top-1: 83.33 (41664/50000)
Regular: 0.6389954090118408
Epoche: 28; regular: 0.6389954090118408: flops 69166840
#Filters: 1151, #FLOPs: 59.23M | Top-1: 74.84
Epoch 29
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 83.56 (21606/25856)
Train | Batch (196/196) | Top-1: 83.40 (41702/50000)
Regular: 0.625261127948761
Epoche: 29; regular: 0.625261127948761: flops 69166840
#Filters: 1151, #FLOPs: 59.23M | Top-1: 78.93
Drin!!
Layers that will be prunned: [(1, 4), (3, 3), (9, 1), (11, 7), (13, 29), (15, 24), (17, 19), (19, 20), (29, 4)]
Prunning filters..
Layer index: 1; Pruned filters: 3
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 2
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 2
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 2
Layer index: 13; Pruned filters: 26
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 12
Layer index: 15; Pruned filters: 4
Layer index: 15; Pruned filters: 5
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 16
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 19; Pruned filters: 2
Layer index: 19; Pruned filters: 3
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 4
Layer index: 19; Pruned filters: 2
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 4
Layer index: 19; Pruned filters: 2
Layer index: 19; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 48.986M | #Params: 0.544M
1.186649180676259
After Growth | FLOPs: 68.624M | #Params: 0.762M
I: 5
flops: 68624388
Before Pruning | FLOPs: 68.624M | #Params: 0.762M
Epoch 0
Train | Batch (1/196) | Top-1: 55.86 (143/256)
Train | Batch (101/196) | Top-1: 81.66 (21113/25856)
Train | Batch (196/196) | Top-1: 82.37 (41183/50000)
Regular: 1.4997833967208862
Epoche: 0; regular: 1.4997833967208862: flops 68624388
#Filters: 1362, #FLOPs: 68.37M | Top-1: 77.54
Epoch 1
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.50 (21591/25856)
Train | Batch (196/196) | Top-1: 83.30 (41652/50000)
Regular: 1.474104881286621
Epoche: 1; regular: 1.474104881286621: flops 68624388
#Filters: 1360, #FLOPs: 68.26M | Top-1: 73.04
Epoch 2
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 83.76 (21656/25856)
Train | Batch (196/196) | Top-1: 83.60 (41801/50000)
Regular: 1.4505832195281982
Epoche: 2; regular: 1.4505832195281982: flops 68624388
#Filters: 1360, #FLOPs: 68.26M | Top-1: 68.88
Epoch 3
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.83 (21674/25856)
Train | Batch (196/196) | Top-1: 83.92 (41959/50000)
Regular: 1.4272511005401611
Epoche: 3; regular: 1.4272511005401611: flops 68624388
#Filters: 1355, #FLOPs: 68.00M | Top-1: 77.81
Epoch 4
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.99 (21717/25856)
Train | Batch (196/196) | Top-1: 83.99 (41995/50000)
Regular: 1.404279112815857
Epoche: 4; regular: 1.404279112815857: flops 68624388
#Filters: 1354, #FLOPs: 67.95M | Top-1: 78.96
Epoch 5
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 84.57 (21866/25856)
Train | Batch (196/196) | Top-1: 84.67 (42333/50000)
Regular: 1.3821840286254883
Epoche: 5; regular: 1.3821840286254883: flops 68624388
#Filters: 1354, #FLOPs: 67.95M | Top-1: 72.69
Epoch 6
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 84.57 (21866/25856)
Train | Batch (196/196) | Top-1: 84.62 (42311/50000)
Regular: 1.3600718975067139
Epoche: 6; regular: 1.3600718975067139: flops 68624388
#Filters: 1353, #FLOPs: 67.85M | Top-1: 78.22
Epoch 7
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 84.86 (21941/25856)
Train | Batch (196/196) | Top-1: 84.61 (42303/50000)
Regular: 1.338598608970642
Epoche: 7; regular: 1.338598608970642: flops 68624388
#Filters: 1352, #FLOPs: 67.79M | Top-1: 72.23
Epoch 8
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 85.04 (21988/25856)
Train | Batch (196/196) | Top-1: 84.98 (42489/50000)
Regular: 1.3176482915878296
Epoche: 8; regular: 1.3176482915878296: flops 68624388
#Filters: 1331, #FLOPs: 65.72M | Top-1: 69.10
Epoch 9
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 84.75 (21913/25856)
Train | Batch (196/196) | Top-1: 84.78 (42392/50000)
Regular: 1.2966727018356323
Epoche: 9; regular: 1.2966727018356323: flops 68624388
#Filters: 1330, #FLOPs: 65.62M | Top-1: 69.57
Epoch 10
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 85.13 (22012/25856)
Train | Batch (196/196) | Top-1: 85.12 (42560/50000)
Regular: 1.2788094282150269
Epoche: 10; regular: 1.2788094282150269: flops 68624388
#Filters: 1329, #FLOPs: 65.51M | Top-1: 76.63
Epoch 11
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 85.37 (22073/25856)
Train | Batch (196/196) | Top-1: 85.14 (42569/50000)
Regular: 1.2613601684570312
Epoche: 11; regular: 1.2613601684570312: flops 68624388
#Filters: 1330, #FLOPs: 65.67M | Top-1: 77.54
Epoch 12
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 85.40 (22081/25856)
Train | Batch (196/196) | Top-1: 85.55 (42773/50000)
Regular: 1.2434190511703491
Epoche: 12; regular: 1.2434190511703491: flops 68624388
#Filters: 1329, #FLOPs: 65.57M | Top-1: 75.49
Epoch 13
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 85.65 (22145/25856)
Train | Batch (196/196) | Top-1: 85.60 (42800/50000)
Regular: 1.225992202758789
Epoche: 13; regular: 1.225992202758789: flops 68624388
#Filters: 1328, #FLOPs: 65.46M | Top-1: 72.63
Epoch 14
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 85.83 (22192/25856)
Train | Batch (196/196) | Top-1: 85.85 (42926/50000)
Regular: 1.2090040445327759
Epoche: 14; regular: 1.2090040445327759: flops 68624388
#Filters: 1326, #FLOPs: 65.41M | Top-1: 73.15
Epoch 15
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 86.14 (22272/25856)
Train | Batch (196/196) | Top-1: 85.99 (42993/50000)
Regular: 1.1919790506362915
Epoche: 15; regular: 1.1919790506362915: flops 68624388
#Filters: 1324, #FLOPs: 65.31M | Top-1: 79.07
Epoch 16
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 86.05 (22250/25856)
Train | Batch (196/196) | Top-1: 85.87 (42935/50000)
Regular: 1.1748260259628296
Epoche: 16; regular: 1.1748260259628296: flops 68624388
#Filters: 1324, #FLOPs: 65.25M | Top-1: 69.31
Epoch 17
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 85.96 (22226/25856)
Train | Batch (196/196) | Top-1: 85.91 (42953/50000)
Regular: 1.1584157943725586
Epoche: 17; regular: 1.1584157943725586: flops 68624388
#Filters: 1322, #FLOPs: 65.15M | Top-1: 77.36
Epoch 18
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 86.21 (22290/25856)
Train | Batch (196/196) | Top-1: 86.17 (43087/50000)
Regular: 1.14189875125885
Epoche: 18; regular: 1.14189875125885: flops 68624388
#Filters: 1322, #FLOPs: 65.15M | Top-1: 80.74
Epoch 19
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 86.77 (22435/25856)
Train | Batch (196/196) | Top-1: 86.61 (43305/50000)
Regular: 1.1253224611282349
Epoche: 19; regular: 1.1253224611282349: flops 68624388
#Filters: 1322, #FLOPs: 65.15M | Top-1: 67.85
Epoch 20
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 86.67 (22410/25856)
Train | Batch (196/196) | Top-1: 86.42 (43210/50000)
Regular: 1.109379768371582
Epoche: 20; regular: 1.109379768371582: flops 68624388
#Filters: 1321, #FLOPs: 65.10M | Top-1: 74.96
Epoch 21
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 86.33 (22321/25856)
Train | Batch (196/196) | Top-1: 86.47 (43236/50000)
Regular: 1.0935325622558594
Epoche: 21; regular: 1.0935325622558594: flops 68624388
#Filters: 1319, #FLOPs: 64.99M | Top-1: 73.08
Epoch 22
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 86.31 (22317/25856)
Train | Batch (196/196) | Top-1: 86.70 (43351/50000)
Regular: 1.0772331953048706
Epoche: 22; regular: 1.0772331953048706: flops 68624388
#Filters: 1317, #FLOPs: 64.84M | Top-1: 72.98
Epoch 23
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 86.93 (22476/25856)
Train | Batch (196/196) | Top-1: 86.83 (43415/50000)
Regular: 1.0615023374557495
Epoche: 23; regular: 1.0615023374557495: flops 68624388
#Filters: 1317, #FLOPs: 64.89M | Top-1: 81.50
Epoch 24
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 86.94 (22479/25856)
Train | Batch (196/196) | Top-1: 86.80 (43398/50000)
Regular: 1.046030044555664
Epoche: 24; regular: 1.046030044555664: flops 68624388
#Filters: 1316, #FLOPs: 64.79M | Top-1: 69.61
Epoch 25
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 87.09 (22519/25856)
Train | Batch (196/196) | Top-1: 87.10 (43550/50000)
Regular: 1.0304601192474365
Epoche: 25; regular: 1.0304601192474365: flops 68624388
#Filters: 1317, #FLOPs: 64.89M | Top-1: 70.89
Epoch 26
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 86.96 (22484/25856)
Train | Batch (196/196) | Top-1: 86.99 (43497/50000)
Regular: 1.0155014991760254
Epoche: 26; regular: 1.0155014991760254: flops 68624388
#Filters: 1316, #FLOPs: 64.84M | Top-1: 80.07
Epoch 27
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.36 (22587/25856)
Train | Batch (196/196) | Top-1: 87.41 (43706/50000)
Regular: 0.999939501285553
Epoche: 27; regular: 0.999939501285553: flops 68624388
#Filters: 1311, #FLOPs: 64.58M | Top-1: 78.87
Epoch 28
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.30 (22571/25856)
Train | Batch (196/196) | Top-1: 87.48 (43742/50000)
Regular: 0.9846676588058472
Epoche: 28; regular: 0.9846676588058472: flops 68624388
#Filters: 1310, #FLOPs: 64.52M | Top-1: 76.76
Epoch 29
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 87.25 (22559/25856)
Train | Batch (196/196) | Top-1: 87.31 (43657/50000)
Regular: 0.9705342650413513
Epoche: 29; regular: 0.9705342650413513: flops 68624388
#Filters: 1304, #FLOPs: 63.84M | Top-1: 78.93
Drin!!
Layers that will be prunned: [(3, 1), (9, 1), (11, 5), (13, 6), (15, 5), (17, 7), (19, 6), (25, 1), (27, 13), (29, 17)]
Prunning filters..
Layer index: 3; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 5
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 5
Layer index: 15; Pruned filters: 5
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 5
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 5
Layer index: 25; Pruned filters: 1
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 3
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Target (flops): 68.863M
After Pruning | FLOPs: 58.802M | #Params: 0.688M
1.0826137736268209
After Growth | FLOPs: 68.046M | #Params: 0.801M
Train | Batch (1/196) | Top-1: 41.02 (105/256)
Train | Batch (101/196) | Top-1: 26.71 (6905/25856)
Train | Batch (196/196) | Top-1: 32.66 (16332/50000)
Regular: nan
Epoche: 0; regular: nan: flops 68046154
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
#Filters: 1409, #FLOPs: 68.05M | Top-1: 34.07
Epoch 0 | Top-1: 34.07
Train | Batch (1/196) | Top-1: 37.11 (95/256)
Train | Batch (101/196) | Top-1: 45.51 (11767/25856)
Train | Batch (196/196) | Top-1: 48.12 (24059/50000)
Regular: nan
Epoche: 1; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 46.00
Epoch 1 | Top-1: 46.00
Train | Batch (1/196) | Top-1: 52.34 (134/256)
Train | Batch (101/196) | Top-1: 55.71 (14404/25856)
Train | Batch (196/196) | Top-1: 57.34 (28671/50000)
Regular: nan
Epoche: 2; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 52.26
Epoch 2 | Top-1: 52.26
Train | Batch (1/196) | Top-1: 61.72 (158/256)
Train | Batch (101/196) | Top-1: 62.52 (16166/25856)
Train | Batch (196/196) | Top-1: 63.43 (31714/50000)
Regular: nan
Epoche: 3; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 58.06
Epoch 3 | Top-1: 58.06
Train | Batch (1/196) | Top-1: 63.67 (163/256)
Train | Batch (101/196) | Top-1: 67.08 (17343/25856)
Train | Batch (196/196) | Top-1: 68.06 (34031/50000)
Regular: nan
Epoche: 4; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 51.12
Epoch 4 | Top-1: 51.12
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 70.84 (18316/25856)
Train | Batch (196/196) | Top-1: 71.57 (35787/50000)
Regular: nan
Epoche: 5; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 55.70
Epoch 5 | Top-1: 55.70
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 74.39 (19233/25856)
Train | Batch (196/196) | Top-1: 74.78 (37391/50000)
Regular: nan
Epoche: 6; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 71.71
Epoch 6 | Top-1: 71.71
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 76.43 (19762/25856)
Train | Batch (196/196) | Top-1: 76.56 (38278/50000)
Regular: nan
Epoche: 7; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 67.90
Epoch 7 | Top-1: 67.90
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 77.97 (20161/25856)
Train | Batch (196/196) | Top-1: 77.91 (38954/50000)
Regular: nan
Epoche: 8; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 68.88
Epoch 8 | Top-1: 68.88
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 79.12 (20457/25856)
Train | Batch (196/196) | Top-1: 79.10 (39548/50000)
Regular: nan
Epoche: 9; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 69.54
Epoch 9 | Top-1: 69.54
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 80.00 (20684/25856)
Train | Batch (196/196) | Top-1: 79.97 (39986/50000)
Regular: nan
Epoche: 10; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 69.52
Epoch 10 | Top-1: 69.52
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 80.97 (20935/25856)
Train | Batch (196/196) | Top-1: 80.86 (40431/50000)
Regular: nan
Epoche: 11; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 72.22
Epoch 11 | Top-1: 72.22
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 81.39 (21044/25856)
Train | Batch (196/196) | Top-1: 81.22 (40608/50000)
Regular: nan
Epoche: 12; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 66.16
Epoch 12 | Top-1: 66.16
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 81.98 (21196/25856)
Train | Batch (196/196) | Top-1: 81.78 (40890/50000)
Regular: nan
Epoche: 13; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 67.78
Epoch 13 | Top-1: 67.78
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 82.64 (21368/25856)
Train | Batch (196/196) | Top-1: 82.45 (41224/50000)
Regular: nan
Epoche: 14; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 73.58
Epoch 14 | Top-1: 73.58
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 83.31 (21540/25856)
Train | Batch (196/196) | Top-1: 82.85 (41424/50000)
Regular: nan
Epoche: 15; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 74.24
Epoch 15 | Top-1: 74.24
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 83.15 (21499/25856)
Train | Batch (196/196) | Top-1: 82.99 (41496/50000)
Regular: nan
Epoche: 16; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 76.97
Epoch 16 | Top-1: 76.97
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 83.50 (21589/25856)
Train | Batch (196/196) | Top-1: 83.44 (41721/50000)
Regular: nan
Epoche: 17; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 75.54
Epoch 17 | Top-1: 75.54
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 83.62 (21620/25856)
Train | Batch (196/196) | Top-1: 83.64 (41818/50000)
Regular: nan
Epoche: 18; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 74.29
Epoch 18 | Top-1: 74.29
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 83.98 (21715/25856)
Train | Batch (196/196) | Top-1: 83.96 (41981/50000)
Regular: nan
Epoche: 19; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 70.52
Epoch 19 | Top-1: 70.52
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 84.29 (21794/25856)
Train | Batch (196/196) | Top-1: 84.22 (42112/50000)
Regular: nan
Epoche: 20; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 74.36
Epoch 20 | Top-1: 74.36
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 84.70 (21899/25856)
Train | Batch (196/196) | Top-1: 84.54 (42271/50000)
Regular: nan
Epoche: 21; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 71.94
Epoch 21 | Top-1: 71.94
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 84.80 (21925/25856)
Train | Batch (196/196) | Top-1: 84.58 (42292/50000)
Regular: nan
Epoche: 22; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 77.95
Epoch 22 | Top-1: 77.95
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 85.06 (21992/25856)
Train | Batch (196/196) | Top-1: 84.82 (42409/50000)
Regular: nan
Epoche: 23; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 74.09
Epoch 23 | Top-1: 74.09
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 85.19 (22027/25856)
Train | Batch (196/196) | Top-1: 84.94 (42468/50000)
Regular: nan
Epoche: 24; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 76.50
Epoch 24 | Top-1: 76.50
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 85.22 (22035/25856)
Train | Batch (196/196) | Top-1: 85.07 (42534/50000)
Regular: nan
Epoche: 25; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 77.28
Epoch 25 | Top-1: 77.28
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 85.40 (22080/25856)
Train | Batch (196/196) | Top-1: 85.45 (42723/50000)
Regular: nan
Epoche: 26; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 77.48
Epoch 26 | Top-1: 77.48
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 85.27 (22048/25856)
Train | Batch (196/196) | Top-1: 85.37 (42683/50000)
Regular: nan
Epoche: 27; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 76.42
Epoch 27 | Top-1: 76.42
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 86.14 (22272/25856)
Train | Batch (196/196) | Top-1: 85.73 (42865/50000)
Regular: nan
Epoche: 28; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 77.44
Epoch 28 | Top-1: 77.44
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 86.13 (22271/25856)
Train | Batch (196/196) | Top-1: 85.71 (42857/50000)
Regular: nan
Epoche: 29; regular: nan: flops 68046154
#Filters: 1409, #FLOPs: 68.05M | Top-1: 66.59
Epoch 29 | Top-1: 66.59
model: ResNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(5, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 35, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(35, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(49, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(5, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(49, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(11, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(49, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(5, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(49, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(15, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(49, 97, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(97, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(97, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(97, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(97, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(96, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(97, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(83, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(97, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(74, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (4): AvgPool2d(kernel_size=8, stride=8, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=97, out_features=10, bias=True)
  )
)
Test acc: 66.59
