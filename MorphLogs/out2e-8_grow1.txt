no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=2e-08, logger='MorphLogs/logMorphNetFlops2e-8_grow1.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=4.0, pruner='FilterPrunnerResNet')
I: 0
flops: 17326400
Before Pruning | FLOPs: 17.326M | #Params: 0.116M
Epoch 0
Train | Batch (1/196) | Top-1: 14.06 (36/256)
Train | Batch (101/196) | Top-1: 29.46 (7616/25856)
Train | Batch (196/196) | Top-1: 36.04 (18019/50000)
Regular: 0.9099504947662354
Epoche: 0; regular: 0.9099504947662354: flops 17326400
#Filters: 562, #FLOPs: 16.88M | Top-1: 27.27
Epoch 1
Train | Batch (1/196) | Top-1: 49.22 (126/256)
Train | Batch (101/196) | Top-1: 52.44 (13560/25856)
Train | Batch (196/196) | Top-1: 55.14 (27572/50000)
Regular: 0.34390249848365784
Epoche: 1; regular: 0.34390249848365784: flops 17326400
#Filters: 537, #FLOPs: 15.48M | Top-1: 49.89
Epoch 2
Train | Batch (1/196) | Top-1: 62.11 (159/256)
Train | Batch (101/196) | Top-1: 61.67 (15945/25856)
Train | Batch (196/196) | Top-1: 62.92 (31461/50000)
Regular: 0.2081560343503952
Epoche: 2; regular: 0.2081560343503952: flops 17326400
#Filters: 529, #FLOPs: 15.32M | Top-1: 51.77
Epoch 3
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 67.47 (17445/25856)
Train | Batch (196/196) | Top-1: 67.81 (33905/50000)
Regular: 0.18110913038253784
Epoche: 3; regular: 0.18110913038253784: flops 17326400
#Filters: 513, #FLOPs: 14.71M | Top-1: 46.75
Epoch 4
Train | Batch (1/196) | Top-1: 67.19 (172/256)
Train | Batch (101/196) | Top-1: 69.93 (18082/25856)
Train | Batch (196/196) | Top-1: 70.46 (35231/50000)
Regular: 0.17870205640792847
Epoche: 4; regular: 0.17870205640792847: flops 17326400
#Filters: 505, #FLOPs: 14.27M | Top-1: 42.73
Epoch 5
Train | Batch (1/196) | Top-1: 75.78 (194/256)
Train | Batch (101/196) | Top-1: 71.94 (18602/25856)
Train | Batch (196/196) | Top-1: 72.33 (36165/50000)
Regular: 0.17689689993858337
Epoche: 5; regular: 0.17689689993858337: flops 17326400
#Filters: 500, #FLOPs: 13.99M | Top-1: 53.28
Epoch 6
Train | Batch (1/196) | Top-1: 72.66 (186/256)
Train | Batch (101/196) | Top-1: 73.31 (18956/25856)
Train | Batch (196/196) | Top-1: 73.64 (36819/50000)
Regular: 0.17790336906909943
Epoche: 6; regular: 0.17790336906909943: flops 17326400
#Filters: 492, #FLOPs: 13.77M | Top-1: 56.16
Epoch 7
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 74.50 (19264/25856)
Train | Batch (196/196) | Top-1: 74.74 (37371/50000)
Regular: 0.1786089986562729
Epoche: 7; regular: 0.1786089986562729: flops 17326400
#Filters: 495, #FLOPs: 13.95M | Top-1: 58.86
Epoch 8
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 75.59 (19544/25856)
Train | Batch (196/196) | Top-1: 75.23 (37613/50000)
Regular: 0.17904403805732727
Epoche: 8; regular: 0.17904403805732727: flops 17326400
#Filters: 491, #FLOPs: 13.73M | Top-1: 38.63
Epoch 9
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 75.74 (19584/25856)
Train | Batch (196/196) | Top-1: 75.87 (37935/50000)
Regular: 0.17675864696502686
Epoche: 9; regular: 0.17675864696502686: flops 17326400
#Filters: 489, #FLOPs: 13.60M | Top-1: 65.16
Epoch 10
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 75.98 (19645/25856)
Train | Batch (196/196) | Top-1: 76.10 (38048/50000)
Regular: 0.174562007188797
Epoche: 10; regular: 0.174562007188797: flops 17326400
#Filters: 491, #FLOPs: 13.64M | Top-1: 53.32
Epoch 11
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 76.54 (19791/25856)
Train | Batch (196/196) | Top-1: 76.84 (38420/50000)
Regular: 0.1760043352842331
Epoche: 11; regular: 0.1760043352842331: flops 17326400
#Filters: 488, #FLOPs: 13.60M | Top-1: 39.19
Epoch 12
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 77.43 (20020/25856)
Train | Batch (196/196) | Top-1: 77.00 (38500/50000)
Regular: 0.17447136342525482
Epoche: 12; regular: 0.17447136342525482: flops 17326400
#Filters: 488, #FLOPs: 13.58M | Top-1: 58.03
Epoch 13
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 77.15 (19949/25856)
Train | Batch (196/196) | Top-1: 77.40 (38698/50000)
Regular: 0.1736089289188385
Epoche: 13; regular: 0.1736089289188385: flops 17326400
#Filters: 481, #FLOPs: 13.38M | Top-1: 65.49
Epoch 14
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 77.58 (20059/25856)
Train | Batch (196/196) | Top-1: 77.80 (38898/50000)
Regular: 0.174371138215065
Epoche: 14; regular: 0.174371138215065: flops 17326400
#Filters: 479, #FLOPs: 13.35M | Top-1: 48.01
Epoch 15
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 77.66 (20081/25856)
Train | Batch (196/196) | Top-1: 77.73 (38864/50000)
Regular: 0.17491406202316284
Epoche: 15; regular: 0.17491406202316284: flops 17326400
#Filters: 484, #FLOPs: 13.70M | Top-1: 66.27
Epoch 16
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 78.07 (20185/25856)
Train | Batch (196/196) | Top-1: 78.05 (39024/50000)
Regular: 0.17168882489204407
Epoche: 16; regular: 0.17168882489204407: flops 17326400
#Filters: 481, #FLOPs: 13.40M | Top-1: 44.70
Epoch 17
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 78.13 (20201/25856)
Train | Batch (196/196) | Top-1: 78.45 (39223/50000)
Regular: 0.17310890555381775
Epoche: 17; regular: 0.17310890555381775: flops 17326400
#Filters: 478, #FLOPs: 13.25M | Top-1: 57.07
Epoch 18
Train | Batch (1/196) | Top-1: 75.78 (194/256)
Train | Batch (101/196) | Top-1: 78.31 (20248/25856)
Train | Batch (196/196) | Top-1: 78.20 (39099/50000)
Regular: 0.1734829545021057
Epoche: 18; regular: 0.1734829545021057: flops 17326400
#Filters: 476, #FLOPs: 13.27M | Top-1: 56.66
Epoch 19
Train | Batch (1/196) | Top-1: 72.66 (186/256)
Train | Batch (101/196) | Top-1: 78.72 (20354/25856)
Train | Batch (196/196) | Top-1: 78.61 (39303/50000)
Regular: 0.17265203595161438
Epoche: 19; regular: 0.17265203595161438: flops 17326400
#Filters: 474, #FLOPs: 13.07M | Top-1: 55.42
Epoch 20
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 79.02 (20431/25856)
Train | Batch (196/196) | Top-1: 78.89 (39443/50000)
Regular: 0.17117896676063538
Epoche: 20; regular: 0.17117896676063538: flops 17326400
#Filters: 469, #FLOPs: 12.85M | Top-1: 61.81
Epoch 21
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 78.96 (20415/25856)
Train | Batch (196/196) | Top-1: 78.92 (39460/50000)
Regular: 0.17190365493297577
Epoche: 21; regular: 0.17190365493297577: flops 17326400
#Filters: 470, #FLOPs: 13.05M | Top-1: 58.09
Epoch 22
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 79.13 (20459/25856)
Train | Batch (196/196) | Top-1: 79.11 (39556/50000)
Regular: 0.1725611388683319
Epoche: 22; regular: 0.1725611388683319: flops 17326400
#Filters: 467, #FLOPs: 13.11M | Top-1: 37.61
Epoch 23
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 79.44 (20539/25856)
Train | Batch (196/196) | Top-1: 79.17 (39584/50000)
Regular: 0.1733102649450302
Epoche: 23; regular: 0.1733102649450302: flops 17326400
#Filters: 468, #FLOPs: 13.09M | Top-1: 64.80
Epoch 24
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 79.06 (20441/25856)
Train | Batch (196/196) | Top-1: 79.06 (39530/50000)
Regular: 0.17066001892089844
Epoche: 24; regular: 0.17066001892089844: flops 17326400
#Filters: 468, #FLOPs: 13.09M | Top-1: 56.15
Epoch 25
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 79.53 (20562/25856)
Train | Batch (196/196) | Top-1: 79.29 (39644/50000)
Regular: 0.16944438219070435
Epoche: 25; regular: 0.16944438219070435: flops 17326400
#Filters: 465, #FLOPs: 13.01M | Top-1: 71.23
Epoch 26
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 79.36 (20520/25856)
Train | Batch (196/196) | Top-1: 79.39 (39693/50000)
Regular: 0.17044994235038757
Epoche: 26; regular: 0.17044994235038757: flops 17326400
#Filters: 460, #FLOPs: 12.90M | Top-1: 65.15
Epoch 27
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 79.25 (20490/25856)
Train | Batch (196/196) | Top-1: 79.38 (39690/50000)
Regular: 0.17220838367938995
Epoche: 27; regular: 0.17220838367938995: flops 17326400
#Filters: 463, #FLOPs: 12.94M | Top-1: 53.53
Epoch 28
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 79.51 (20558/25856)
Train | Batch (196/196) | Top-1: 79.54 (39769/50000)
Regular: 0.17221719026565552
Epoche: 28; regular: 0.17221719026565552: flops 17326400
#Filters: 465, #FLOPs: 13.03M | Top-1: 52.88
Epoch 29
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 79.61 (20584/25856)
Train | Batch (196/196) | Top-1: 79.65 (39824/50000)
Regular: 0.17005208134651184
Epoche: 29; regular: 0.17005208134651184: flops 17326400
#Filters: 458, #FLOPs: 12.79M | Top-1: 59.56
Drin!!
Layers that will be prunned: [(1, 7), (3, 6), (5, 7), (7, 6), (9, 5), (13, 15), (15, 13), (17, 14), (19, 11), (23, 2), (25, 6), (27, 6), (29, 2)]
Prunning filters..
Layer index: 1; Pruned filters: 7
Layer index: 3; Pruned filters: 4
Layer index: 3; Pruned filters: 2
Layer index: 5; Pruned filters: 1
Layer index: 5; Pruned filters: 6
Layer index: 7; Pruned filters: 6
Layer index: 9; Pruned filters: 3
Layer index: 9; Pruned filters: 2
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 14
Layer index: 15; Pruned filters: 5
Layer index: 15; Pruned filters: 6
Layer index: 15; Pruned filters: 2
Layer index: 17; Pruned filters: 2
Layer index: 17; Pruned filters: 11
Layer index: 17; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 6
Layer index: 19; Pruned filters: 3
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Target (flops): 69.306M
After Pruning | FLOPs: 8.258M | #Params: 0.087M
2.922917860434577
After Growth | FLOPs: 69.963M | #Params: 0.745M
I: 1
flops: 69963052
Before Pruning | FLOPs: 69.963M | #Params: 0.745M
Epoch 0
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 78.48 (20291/25856)
Train | Batch (196/196) | Top-1: 78.06 (39029/50000)
Regular: 2.3277244567871094
Epoche: 0; regular: 2.3277244567871094: flops 69963052
#Filters: 1270, #FLOPs: 60.99M | Top-1: 41.17
Epoch 1
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 77.76 (20105/25856)
Train | Batch (196/196) | Top-1: 77.21 (38605/50000)
Regular: 0.4751831889152527
Epoche: 1; regular: 0.4751831889152527: flops 69963052
#Filters: 982, #FLOPs: 47.50M | Top-1: 44.95
Epoch 2
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 76.97 (19901/25856)
Train | Batch (196/196) | Top-1: 76.87 (38433/50000)
Regular: 0.3098234534263611
Epoche: 2; regular: 0.3098234534263611: flops 69963052
#Filters: 729, #FLOPs: 35.75M | Top-1: 60.07
Epoch 3
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 77.48 (20033/25856)
Train | Batch (196/196) | Top-1: 76.76 (38379/50000)
Regular: 0.2949966490268707
Epoche: 3; regular: 0.2949966490268707: flops 69963052
#Filters: 593, #FLOPs: 29.13M | Top-1: 63.27
Epoch 4
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 77.21 (19964/25856)
Train | Batch (196/196) | Top-1: 76.77 (38387/50000)
Regular: 0.31090301275253296
Epoche: 4; regular: 0.31090301275253296: flops 69963052
#Filters: 586, #FLOPs: 28.80M | Top-1: 51.10
Epoch 5
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 76.68 (19826/25856)
Train | Batch (196/196) | Top-1: 76.76 (38378/50000)
Regular: 0.3274528980255127
Epoche: 5; regular: 0.3274528980255127: flops 69963052
#Filters: 582, #FLOPs: 28.32M | Top-1: 41.05
Epoch 6
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 77.12 (19941/25856)
Train | Batch (196/196) | Top-1: 76.82 (38408/50000)
Regular: 0.29105398058891296
Epoche: 6; regular: 0.29105398058891296: flops 69963052
#Filters: 576, #FLOPs: 28.42M | Top-1: 47.61
Epoch 7
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 77.65 (20076/25856)
Train | Batch (196/196) | Top-1: 77.36 (38678/50000)
Regular: 0.26667284965515137
Epoche: 7; regular: 0.26667284965515137: flops 69963052
#Filters: 574, #FLOPs: 28.37M | Top-1: 35.57
Epoch 8
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 76.40 (19755/25856)
Train | Batch (196/196) | Top-1: 76.68 (38341/50000)
Regular: 0.35514217615127563
Epoche: 8; regular: 0.35514217615127563: flops 69963052
#Filters: 579, #FLOPs: 28.85M | Top-1: 20.68
Epoch 9
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 76.43 (19762/25856)
Train | Batch (196/196) | Top-1: 76.53 (38264/50000)
Regular: 0.3490341305732727
Epoche: 9; regular: 0.3490341305732727: flops 69963052
#Filters: 572, #FLOPs: 28.56M | Top-1: 25.27
Epoch 10
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 77.39 (20010/25856)
Train | Batch (196/196) | Top-1: 77.05 (38527/50000)
Regular: 0.29123830795288086
Epoche: 10; regular: 0.29123830795288086: flops 69963052
#Filters: 571, #FLOPs: 27.89M | Top-1: 34.96
Epoch 11
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 76.85 (19870/25856)
Train | Batch (196/196) | Top-1: 76.71 (38354/50000)
Regular: 0.3451617360115051
Epoche: 11; regular: 0.3451617360115051: flops 69963052
#Filters: 569, #FLOPs: 27.94M | Top-1: 31.49
Epoch 12
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 76.83 (19864/25856)
Train | Batch (196/196) | Top-1: 76.93 (38463/50000)
Regular: 0.38294002413749695
Epoche: 12; regular: 0.38294002413749695: flops 69963052
#Filters: 569, #FLOPs: 27.89M | Top-1: 28.98
Epoch 13
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 77.46 (20029/25856)
Train | Batch (196/196) | Top-1: 77.08 (38538/50000)
Regular: 0.30991309881210327
Epoche: 13; regular: 0.30991309881210327: flops 69963052
#Filters: 570, #FLOPs: 28.21M | Top-1: 57.27
Epoch 14
Train | Batch (1/196) | Top-1: 75.78 (194/256)
Train | Batch (101/196) | Top-1: 77.12 (19941/25856)
Train | Batch (196/196) | Top-1: 77.22 (38608/50000)
Regular: 0.2567853033542633
Epoche: 14; regular: 0.2567853033542633: flops 69963052
#Filters: 568, #FLOPs: 28.12M | Top-1: 45.97
Epoch 15
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 76.66 (19820/25856)
Train | Batch (196/196) | Top-1: 76.60 (38298/50000)
Regular: 0.2559345066547394
Epoche: 15; regular: 0.2559345066547394: flops 69963052
#Filters: 559, #FLOPs: 27.48M | Top-1: 31.53
Epoch 16
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 77.19 (19958/25856)
Train | Batch (196/196) | Top-1: 76.75 (38376/50000)
Regular: 0.264617919921875
Epoche: 16; regular: 0.264617919921875: flops 69963052
#Filters: 558, #FLOPs: 27.80M | Top-1: 30.35
Epoch 17
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 76.98 (19903/25856)
Train | Batch (196/196) | Top-1: 76.86 (38428/50000)
Regular: 0.2529289722442627
Epoche: 17; regular: 0.2529289722442627: flops 69963052
#Filters: 557, #FLOPs: 27.48M | Top-1: 41.56
Epoch 18
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 77.32 (19992/25856)
Train | Batch (196/196) | Top-1: 77.07 (38537/50000)
Regular: 0.2690761089324951
Epoche: 18; regular: 0.2690761089324951: flops 69963052
#Filters: 567, #FLOPs: 27.97M | Top-1: 50.35
Epoch 19
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 77.27 (19978/25856)
Train | Batch (196/196) | Top-1: 77.11 (38557/50000)
Regular: 0.2823545038700104
Epoche: 19; regular: 0.2823545038700104: flops 69963052
#Filters: 566, #FLOPs: 27.86M | Top-1: 53.36
Epoch 20
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 77.55 (20051/25856)
Train | Batch (196/196) | Top-1: 77.43 (38715/50000)
Regular: 0.2546517252922058
Epoche: 20; regular: 0.2546517252922058: flops 69963052
#Filters: 559, #FLOPs: 27.59M | Top-1: 32.19
Epoch 21
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 77.69 (20087/25856)
Train | Batch (196/196) | Top-1: 77.33 (38666/50000)
Regular: 0.25045904517173767
Epoche: 21; regular: 0.25045904517173767: flops 69963052
#Filters: 559, #FLOPs: 27.43M | Top-1: 36.90
Epoch 22
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 77.02 (19915/25856)
Train | Batch (196/196) | Top-1: 76.98 (38490/50000)
Regular: 0.24769452214241028
Epoche: 22; regular: 0.24769452214241028: flops 69963052
#Filters: 549, #FLOPs: 27.16M | Top-1: 17.90
Epoch 23
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 77.16 (19951/25856)
Train | Batch (196/196) | Top-1: 77.18 (38588/50000)
Regular: 0.2615320086479187
Epoche: 23; regular: 0.2615320086479187: flops 69963052
#Filters: 554, #FLOPs: 27.53M | Top-1: 46.07
Epoch 24
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 76.93 (19892/25856)
Train | Batch (196/196) | Top-1: 77.01 (38505/50000)
Regular: 0.28715917468070984
Epoche: 24; regular: 0.28715917468070984: flops 69963052
#Filters: 555, #FLOPs: 27.43M | Top-1: 36.53
Epoch 25
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 77.42 (20017/25856)
Train | Batch (196/196) | Top-1: 77.27 (38634/50000)
Regular: 0.23926793038845062
Epoche: 25; regular: 0.23926793038845062: flops 69963052
#Filters: 554, #FLOPs: 27.43M | Top-1: 55.94
Epoch 26
Train | Batch (1/196) | Top-1: 73.44 (188/256)
Train | Batch (101/196) | Top-1: 77.55 (20052/25856)
Train | Batch (196/196) | Top-1: 77.31 (38655/50000)
Regular: 0.23872727155685425
Epoche: 26; regular: 0.23872727155685425: flops 69963052
#Filters: 553, #FLOPs: 27.37M | Top-1: 33.92
Epoch 27
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 77.38 (20008/25856)
Train | Batch (196/196) | Top-1: 77.37 (38687/50000)
Regular: 0.23835155367851257
Epoche: 27; regular: 0.23835155367851257: flops 69963052
#Filters: 548, #FLOPs: 27.13M | Top-1: 44.57
Epoch 28
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 77.51 (20042/25856)
Train | Batch (196/196) | Top-1: 77.14 (38568/50000)
Regular: 0.2502404451370239
Epoche: 28; regular: 0.2502404451370239: flops 69963052
#Filters: 549, #FLOPs: 27.58M | Top-1: 41.34
Epoch 29
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 77.20 (19961/25856)
Train | Batch (196/196) | Top-1: 77.37 (38686/50000)
Regular: 0.3102020025253296
Epoche: 29; regular: 0.3102020025253296: flops 69963052
#Filters: 549, #FLOPs: 27.42M | Top-1: 35.99
Drin!!
Layers that will be prunned: [(1, 2), (3, 4), (5, 1), (7, 4), (9, 7), (11, 32), (12, 15), (13, 1), (14, 15), (15, 8), (16, 15), (17, 4), (18, 15), (19, 14), (20, 15), (21, 63), (22, 62), (23, 68), (24, 62), (25, 63), (26, 62), (27, 70), (28, 62), (29, 68), (30, 62)]
Prunning filters..
Layer index: 12; Pruned filters: 15
Layer index: 14; Pruned filters: 15
Layer index: 16; Pruned filters: 15
Layer index: 18; Pruned filters: 15
Layer index: 20; Pruned filters: 15
Layer index: 22; Pruned filters: 62
Layer index: 24; Pruned filters: 62
Layer index: 26; Pruned filters: 62
Layer index: 28; Pruned filters: 62
Layer index: 30; Pruned filters: 62
Layer index: 1; Pruned filters: 2
Layer index: 3; Pruned filters: 4
Layer index: 5; Pruned filters: 1
Layer index: 7; Pruned filters: 4
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 6
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 31
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 8
Layer index: 17; Pruned filters: 4
Layer index: 19; Pruned filters: 2
Layer index: 19; Pruned filters: 12
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 62
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 58
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 52
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 8
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 52
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 3
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 58
Target (flops): 69.306M
After Pruning | FLOPs: 10.555M | #Params: 0.067M
2.6114828879374796
After Growth | FLOPs: 68.969M | #Params: 0.457M
I: 2
flops: 68968776
Before Pruning | FLOPs: 68.969M | #Params: 0.457M
Epoch 0
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 76.57 (19798/25856)
Train | Batch (196/196) | Top-1: 76.45 (38224/50000)
Regular: 2.1106884479522705
Epoche: 0; regular: 2.1106884479522705: flops 68968776
#Filters: 1429, #FLOPs: 55.37M | Top-1: 38.05
Epoch 1
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 76.02 (19655/25856)
Train | Batch (196/196) | Top-1: 76.46 (38230/50000)
Regular: 0.6757035255432129
Epoche: 1; regular: 0.6757035255432129: flops 68968776
#Filters: 1281, #FLOPs: 48.21M | Top-1: 38.61
Epoch 2
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 76.57 (19797/25856)
Train | Batch (196/196) | Top-1: 76.67 (38334/50000)
Regular: 0.7157588005065918
Epoche: 2; regular: 0.7157588005065918: flops 68968776
#Filters: 1281, #FLOPs: 47.71M | Top-1: 51.73
Epoch 3
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 77.29 (19983/25856)
Train | Batch (196/196) | Top-1: 76.98 (38489/50000)
Regular: 0.7794830203056335
Epoche: 3; regular: 0.7794830203056335: flops 68968776
#Filters: 1231, #FLOPs: 48.22M | Top-1: 66.75
Epoch 4
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 77.11 (19938/25856)
Train | Batch (196/196) | Top-1: 76.74 (38369/50000)
Regular: 0.41726911067962646
Epoche: 4; regular: 0.41726911067962646: flops 68968776
#Filters: 1229, #FLOPs: 47.62M | Top-1: 59.23
Epoch 5
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 75.96 (19641/25856)
Train | Batch (196/196) | Top-1: 76.28 (38138/50000)
Regular: 0.30410486459732056
Epoche: 5; regular: 0.30410486459732056: flops 68968776
#Filters: 1227, #FLOPs: 48.03M | Top-1: 28.85
Epoch 6
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 75.85 (19613/25856)
Train | Batch (196/196) | Top-1: 76.12 (38058/50000)
Regular: 0.41034021973609924
Epoche: 6; regular: 0.41034021973609924: flops 68968776
#Filters: 594, #FLOPs: 27.92M | Top-1: 42.95
Epoch 7
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 75.84 (19609/25856)
Train | Batch (196/196) | Top-1: 76.41 (38203/50000)
Regular: 0.7328984141349792
Epoche: 7; regular: 0.7328984141349792: flops 68968776
#Filters: 594, #FLOPs: 27.42M | Top-1: 40.18
Epoch 8
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 76.69 (19828/25856)
Train | Batch (196/196) | Top-1: 76.54 (38269/50000)
Regular: 0.42201268672943115
Epoche: 8; regular: 0.42201268672943115: flops 68968776
#Filters: 594, #FLOPs: 27.92M | Top-1: 62.89
Epoch 9
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 76.81 (19861/25856)
Train | Batch (196/196) | Top-1: 76.81 (38405/50000)
Regular: 0.4123608469963074
Epoche: 9; regular: 0.4123608469963074: flops 68968776
#Filters: 594, #FLOPs: 27.92M | Top-1: 29.54
Epoch 10
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 76.06 (19667/25856)
Train | Batch (196/196) | Top-1: 76.34 (38172/50000)
Regular: 0.3858563303947449
Epoche: 10; regular: 0.3858563303947449: flops 68968776
#Filters: 595, #FLOPs: 28.07M | Top-1: 40.52
Epoch 11
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 77.27 (19979/25856)
Train | Batch (196/196) | Top-1: 76.93 (38466/50000)
Regular: 0.27101701498031616
Epoche: 11; regular: 0.27101701498031616: flops 68968776
#Filters: 598, #FLOPs: 28.38M | Top-1: 44.34
Epoch 12
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 76.35 (19742/25856)
Train | Batch (196/196) | Top-1: 76.66 (38329/50000)
Regular: 1.0443542003631592
Epoche: 12; regular: 1.0443542003631592: flops 68968776
#Filters: 595, #FLOPs: 27.73M | Top-1: 32.29
Epoch 13
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 77.51 (20042/25856)
Train | Batch (196/196) | Top-1: 77.36 (38680/50000)
Regular: 0.5451076030731201
Epoche: 13; regular: 0.5451076030731201: flops 68968776
#Filters: 597, #FLOPs: 27.74M | Top-1: 54.86
Epoch 14
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 76.72 (19837/25856)
Train | Batch (196/196) | Top-1: 76.88 (38442/50000)
Regular: 0.9855127334594727
Epoche: 14; regular: 0.9855127334594727: flops 68968776
#Filters: 594, #FLOPs: 27.09M | Top-1: 43.52
Epoch 15
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 76.74 (19841/25856)
Train | Batch (196/196) | Top-1: 77.02 (38512/50000)
Regular: 1.3456670045852661
Epoche: 15; regular: 1.3456670045852661: flops 68968776
#Filters: 594, #FLOPs: 27.00M | Top-1: 34.11
Epoch 16
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 76.64 (19816/25856)
Train | Batch (196/196) | Top-1: 76.73 (38367/50000)
Regular: 0.8192805051803589
Epoche: 16; regular: 0.8192805051803589: flops 68968776
#Filters: 592, #FLOPs: 27.65M | Top-1: 9.91
Epoch 17
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 77.14 (19945/25856)
Train | Batch (196/196) | Top-1: 77.03 (38514/50000)
Regular: 0.7438102960586548
Epoche: 17; regular: 0.7438102960586548: flops 68968776
#Filters: 591, #FLOPs: 27.00M | Top-1: 53.78
Epoch 18
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 77.22 (19967/25856)
Train | Batch (196/196) | Top-1: 77.17 (38585/50000)
Regular: 0.6689242720603943
Epoche: 18; regular: 0.6689242720603943: flops 68968776
#Filters: 594, #FLOPs: 27.59M | Top-1: 33.08
Epoch 19
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 76.90 (19882/25856)
Train | Batch (196/196) | Top-1: 76.69 (38344/50000)
Regular: 1.1150574684143066
Epoche: 19; regular: 1.1150574684143066: flops 68968776
#Filters: 592, #FLOPs: 27.23M | Top-1: 53.54
Epoch 20
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 77.01 (19911/25856)
Train | Batch (196/196) | Top-1: 77.16 (38582/50000)
Regular: 1.039394736289978
Epoche: 20; regular: 1.039394736289978: flops 68968776
#Filters: 593, #FLOPs: 26.95M | Top-1: 20.13
Epoch 21
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 77.08 (19929/25856)
Train | Batch (196/196) | Top-1: 76.89 (38445/50000)
Regular: 1.7963461875915527
Epoche: 21; regular: 1.7963461875915527: flops 68968776
#Filters: 593, #FLOPs: 26.95M | Top-1: 62.64
Epoch 22
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 76.18 (19696/25856)
Train | Batch (196/196) | Top-1: 76.78 (38392/50000)
Regular: 1.9331098794937134
Epoche: 22; regular: 1.9331098794937134: flops 68968776
#Filters: 593, #FLOPs: 26.95M | Top-1: 51.36
Epoch 23
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 77.16 (19950/25856)
Train | Batch (196/196) | Top-1: 76.76 (38379/50000)
Regular: 1.0473737716674805
Epoche: 23; regular: 1.0473737716674805: flops 68968776
#Filters: 592, #FLOPs: 27.65M | Top-1: 49.63
Epoch 24
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 77.44 (20024/25856)
Train | Batch (196/196) | Top-1: 77.11 (38554/50000)
Regular: 0.36685624718666077
Epoche: 24; regular: 0.36685624718666077: flops 68968776
#Filters: 592, #FLOPs: 27.89M | Top-1: 24.33
Epoch 25
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 76.83 (19866/25856)
Train | Batch (196/196) | Top-1: 76.79 (38397/50000)
Regular: 0.8143781423568726
Epoche: 25; regular: 0.8143781423568726: flops 68968776
#Filters: 590, #FLOPs: 27.20M | Top-1: 67.95
Epoch 26
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 77.08 (19931/25856)
Train | Batch (196/196) | Top-1: 76.92 (38458/50000)
Regular: 1.194774866104126
Epoche: 26; regular: 1.194774866104126: flops 68968776
#Filters: 588, #FLOPs: 26.86M | Top-1: 29.87
Epoch 27
Train | Batch (1/196) | Top-1: 67.58 (173/256)
Train | Batch (101/196) | Top-1: 75.99 (19647/25856)
Train | Batch (196/196) | Top-1: 76.06 (38032/50000)
Regular: 1.0915334224700928
Epoche: 27; regular: 1.0915334224700928: flops 68968776
#Filters: 589, #FLOPs: 27.60M | Top-1: 41.62
Epoch 28
