no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=2e-08, logger='MorphLogs/logMorphNetFlops2e-8_grow1.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 23.38 (6044/25856)
Train | Batch (196/196) | Top-1: 28.47 (14233/50000)
Regular: 5.438046932220459
Epoche: 0; regular: 5.438046932220459: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 37.32
Epoch 1
Train | Batch (1/196) | Top-1: 38.67 (99/256)
Train | Batch (101/196) | Top-1: 38.79 (10030/25856)
Train | Batch (196/196) | Top-1: 40.92 (20458/50000)
Regular: 5.3609089851379395
Epoche: 1; regular: 5.3609089851379395: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 42.34
Epoch 2
Train | Batch (1/196) | Top-1: 41.02 (105/256)
Train | Batch (101/196) | Top-1: 46.13 (11928/25856)
Train | Batch (196/196) | Top-1: 47.06 (23531/50000)
Regular: 5.283517837524414
Epoche: 2; regular: 5.283517837524414: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 48.88
Epoch 3
Train | Batch (1/196) | Top-1: 50.00 (128/256)
Train | Batch (101/196) | Top-1: 50.51 (13059/25856)
Train | Batch (196/196) | Top-1: 51.07 (25533/50000)
Regular: 5.2061896324157715
Epoche: 3; regular: 5.2061896324157715: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 52.10
Epoch 4
Train | Batch (1/196) | Top-1: 53.91 (138/256)
Train | Batch (101/196) | Top-1: 53.68 (13879/25856)
Train | Batch (196/196) | Top-1: 54.36 (27180/50000)
Regular: 5.128885269165039
Epoche: 4; regular: 5.128885269165039: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 55.71
Epoch 5
Train | Batch (1/196) | Top-1: 57.03 (146/256)
Train | Batch (101/196) | Top-1: 57.02 (14744/25856)
Train | Batch (196/196) | Top-1: 57.63 (28817/50000)
Regular: 5.051635265350342
Epoche: 5; regular: 5.051635265350342: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 58.59
Epoch 6
Train | Batch (1/196) | Top-1: 60.94 (156/256)
Train | Batch (101/196) | Top-1: 59.50 (15384/25856)
Train | Batch (196/196) | Top-1: 60.15 (30076/50000)
Regular: 4.9744744300842285
Epoche: 6; regular: 4.9744744300842285: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 60.16
Epoch 7
Train | Batch (1/196) | Top-1: 62.89 (161/256)
Train | Batch (101/196) | Top-1: 61.80 (15979/25856)
Train | Batch (196/196) | Top-1: 62.52 (31258/50000)
Regular: 4.89735746383667
Epoche: 7; regular: 4.89735746383667: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 58.14
Epoch 8
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 64.37 (16643/25856)
Train | Batch (196/196) | Top-1: 64.37 (32186/50000)
Regular: 4.820310115814209
Epoche: 8; regular: 4.820310115814209: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 61.49
Epoch 9
Train | Batch (1/196) | Top-1: 64.84 (166/256)
Train | Batch (101/196) | Top-1: 65.94 (17049/25856)
Train | Batch (196/196) | Top-1: 65.98 (32989/50000)
Regular: 4.743346691131592
Epoche: 9; regular: 4.743346691131592: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 64.28
Epoch 10
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 67.18 (17371/25856)
Train | Batch (196/196) | Top-1: 67.41 (33703/50000)
Regular: 4.6664533615112305
Epoche: 10; regular: 4.6664533615112305: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 65.47
Epoch 11
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 67.97 (17575/25856)
Train | Batch (196/196) | Top-1: 68.52 (34261/50000)
Regular: 4.589671611785889
Epoche: 11; regular: 4.589671611785889: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 68.38
Epoch 12
Train | Batch (1/196) | Top-1: 71.09 (182/256)
Train | Batch (101/196) | Top-1: 69.26 (17909/25856)
Train | Batch (196/196) | Top-1: 69.63 (34813/50000)
Regular: 4.512923240661621
Epoche: 12; regular: 4.512923240661621: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 67.10
Epoch 13
Train | Batch (1/196) | Top-1: 69.14 (177/256)
Train | Batch (101/196) | Top-1: 70.68 (18274/25856)
Train | Batch (196/196) | Top-1: 70.66 (35330/50000)
Regular: 4.436303615570068
Epoche: 13; regular: 4.436303615570068: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 67.24
Epoch 14
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 71.60 (18512/25856)
Train | Batch (196/196) | Top-1: 71.81 (35903/50000)
Regular: 4.359735012054443
Epoche: 14; regular: 4.359735012054443: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 67.69
Epoch 15
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 72.78 (18818/25856)
Train | Batch (196/196) | Top-1: 72.89 (36446/50000)
Regular: 4.283243179321289
Epoche: 15; regular: 4.283243179321289: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 71.15
Epoch 16
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 73.66 (19046/25856)
Train | Batch (196/196) | Top-1: 73.86 (36931/50000)
Regular: 4.206795692443848
Epoche: 16; regular: 4.206795692443848: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.19
Epoch 17
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 74.72 (19320/25856)
Train | Batch (196/196) | Top-1: 74.61 (37304/50000)
Regular: 4.130455017089844
Epoche: 17; regular: 4.130455017089844: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 71.43
Epoch 18
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 75.50 (19520/25856)
Train | Batch (196/196) | Top-1: 75.52 (37758/50000)
Regular: 4.054241180419922
Epoche: 18; regular: 4.054241180419922: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 71.28
Epoch 19
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 76.15 (19689/25856)
Train | Batch (196/196) | Top-1: 76.17 (38086/50000)
Regular: 3.978252410888672
Epoche: 19; regular: 3.978252410888672: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.83
Epoch 20
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 77.20 (19960/25856)
Train | Batch (196/196) | Top-1: 76.97 (38484/50000)
Regular: 3.902329683303833
Epoche: 20; regular: 3.902329683303833: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.83
Epoch 21
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 77.55 (20051/25856)
Train | Batch (196/196) | Top-1: 77.40 (38698/50000)
Regular: 3.826591730117798
Epoche: 21; regular: 3.826591730117798: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.95
Epoch 22
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 78.39 (20268/25856)
Train | Batch (196/196) | Top-1: 78.07 (39033/50000)
Regular: 3.7508811950683594
Epoche: 22; regular: 3.7508811950683594: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.21
Epoch 23
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 78.54 (20307/25856)
Train | Batch (196/196) | Top-1: 78.37 (39183/50000)
Regular: 3.6755127906799316
Epoche: 23; regular: 3.6755127906799316: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.30
Epoch 24
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.42 (20534/25856)
Train | Batch (196/196) | Top-1: 79.17 (39585/50000)
Regular: 3.6000826358795166
Epoche: 24; regular: 3.6000826358795166: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.41
Epoch 25
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 79.45 (20543/25856)
Train | Batch (196/196) | Top-1: 79.44 (39722/50000)
Regular: 3.5250134468078613
Epoche: 25; regular: 3.5250134468078613: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.14
Epoch 26
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 79.83 (20640/25856)
Train | Batch (196/196) | Top-1: 79.84 (39918/50000)
Regular: 3.450119972229004
Epoche: 26; regular: 3.450119972229004: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.33
Epoch 27
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 80.98 (20937/25856)
Train | Batch (196/196) | Top-1: 80.60 (40301/50000)
Regular: 3.3754851818084717
Epoche: 27; regular: 3.3754851818084717: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.15
Epoch 28
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 80.91 (20919/25856)
Train | Batch (196/196) | Top-1: 80.76 (40381/50000)
Regular: 3.3011138439178467
Epoche: 28; regular: 3.3011138439178467: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.26
Epoch 29
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 81.36 (21036/25856)
Train | Batch (196/196) | Top-1: 81.32 (40658/50000)
Regular: 3.2272748947143555
Epoche: 29; regular: 3.2272748947143555: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.72
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 1
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 81.66 (21113/25856)
Train | Batch (196/196) | Top-1: 81.64 (40821/50000)
Regular: 3.156522274017334
Epoche: 0; regular: 3.156522274017334: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.97
Epoch 1
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 81.92 (21181/25856)
Train | Batch (196/196) | Top-1: 81.93 (40967/50000)
Regular: 3.0833559036254883
Epoche: 1; regular: 3.0833559036254883: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.10
Epoch 2
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 82.41 (21309/25856)
Train | Batch (196/196) | Top-1: 82.13 (41066/50000)
Regular: 3.0106749534606934
Epoche: 2; regular: 3.0106749534606934: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.86
Epoch 3
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 82.79 (21407/25856)
Train | Batch (196/196) | Top-1: 82.70 (41351/50000)
Regular: 2.9385721683502197
Epoche: 3; regular: 2.9385721683502197: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.14
Epoch 4
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 82.94 (21444/25856)
Train | Batch (196/196) | Top-1: 82.95 (41476/50000)
Regular: 2.8670341968536377
Epoche: 4; regular: 2.8670341968536377: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.33
Epoch 5
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 82.98 (21456/25856)
Train | Batch (196/196) | Top-1: 83.06 (41532/50000)
Regular: 2.796034812927246
Epoche: 5; regular: 2.796034812927246: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.77
Epoch 6
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 83.17 (21504/25856)
Train | Batch (196/196) | Top-1: 83.29 (41647/50000)
Regular: 2.726041793823242
Epoche: 6; regular: 2.726041793823242: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.56
Epoch 7
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 84.03 (21726/25856)
Train | Batch (196/196) | Top-1: 83.75 (41873/50000)
Regular: 2.65706205368042
Epoche: 7; regular: 2.65706205368042: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.87
Epoch 8
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.95 (21706/25856)
Train | Batch (196/196) | Top-1: 83.88 (41939/50000)
Regular: 2.5895895957946777
Epoche: 8; regular: 2.5895895957946777: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.69
Epoch 9
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.26 (21785/25856)
Train | Batch (196/196) | Top-1: 84.04 (42020/50000)
Regular: 2.524488687515259
Epoche: 9; regular: 2.524488687515259: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.17
Epoch 10
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.39 (21819/25856)
Train | Batch (196/196) | Top-1: 84.18 (42091/50000)
Regular: 2.4627368450164795
Epoche: 10; regular: 2.4627368450164795: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.42
Epoch 11
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 84.68 (21896/25856)
Train | Batch (196/196) | Top-1: 84.43 (42217/50000)
Regular: 2.4063477516174316
Epoche: 11; regular: 2.4063477516174316: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.75
Epoch 12
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 84.41 (21825/25856)
Train | Batch (196/196) | Top-1: 84.58 (42290/50000)
Regular: 2.356480121612549
Epoche: 12; regular: 2.356480121612549: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.91
Epoch 13
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 84.55 (21862/25856)
Train | Batch (196/196) | Top-1: 84.56 (42279/50000)
Regular: 2.3111205101013184
Epoche: 13; regular: 2.3111205101013184: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.95
Epoch 14
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 85.09 (22001/25856)
Train | Batch (196/196) | Top-1: 84.70 (42349/50000)
Regular: 2.267418384552002
Epoche: 14; regular: 2.267418384552002: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.43
Epoch 15
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 84.96 (21966/25856)
Train | Batch (196/196) | Top-1: 84.91 (42453/50000)
Regular: 2.224771022796631
Epoche: 15; regular: 2.224771022796631: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.84
Epoch 16
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 85.00 (21978/25856)
Train | Batch (196/196) | Top-1: 85.10 (42549/50000)
Regular: 2.183954954147339
Epoche: 16; regular: 2.183954954147339: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.13
Epoch 17
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 85.28 (22049/25856)
Train | Batch (196/196) | Top-1: 85.10 (42549/50000)
Regular: 2.141958236694336
Epoche: 17; regular: 2.141958236694336: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.28
Epoch 18
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 85.66 (22148/25856)
Train | Batch (196/196) | Top-1: 85.67 (42836/50000)
Regular: 2.101182222366333
Epoche: 18; regular: 2.101182222366333: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.92
Epoch 19
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 85.59 (22129/25856)
Train | Batch (196/196) | Top-1: 85.55 (42776/50000)
Regular: 2.0613863468170166
Epoche: 19; regular: 2.0613863468170166: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.96
Epoch 20
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 86.00 (22236/25856)
Train | Batch (196/196) | Top-1: 85.91 (42954/50000)
Regular: 2.0225753784179688
Epoche: 20; regular: 2.0225753784179688: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.12
Epoch 21
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 86.04 (22246/25856)
Train | Batch (196/196) | Top-1: 85.88 (42939/50000)
Regular: 1.9836198091506958
Epoche: 21; regular: 1.9836198091506958: flops 68862592
#Filters: 1135, #FLOPs: 68.72M | Top-1: 78.21
Epoch 22
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 85.89 (22207/25856)
Train | Batch (196/196) | Top-1: 85.98 (42989/50000)
Regular: 1.9460114240646362
Epoche: 22; regular: 1.9460114240646362: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 80.35
Epoch 23
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 86.39 (22338/25856)
Train | Batch (196/196) | Top-1: 86.21 (43106/50000)
Regular: 1.9098511934280396
Epoche: 23; regular: 1.9098511934280396: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 78.46
Epoch 24
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 86.52 (22371/25856)
Train | Batch (196/196) | Top-1: 86.36 (43179/50000)
Regular: 1.874826192855835
Epoche: 24; regular: 1.874826192855835: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 72.31
Epoch 25
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 87.02 (22500/25856)
Train | Batch (196/196) | Top-1: 86.81 (43404/50000)
Regular: 1.8395847082138062
Epoche: 25; regular: 1.8395847082138062: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 80.99
Epoch 26
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 86.71 (22420/25856)
Train | Batch (196/196) | Top-1: 86.54 (43272/50000)
Regular: 1.8059849739074707
Epoche: 26; regular: 1.8059849739074707: flops 68862592
#Filters: 1133, #FLOPs: 68.42M | Top-1: 74.62
Epoch 27
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 86.66 (22406/25856)
Train | Batch (196/196) | Top-1: 86.58 (43291/50000)
Regular: 1.7733708620071411
Epoche: 27; regular: 1.7733708620071411: flops 68862592
#Filters: 1133, #FLOPs: 68.42M | Top-1: 78.79
Epoch 28
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 87.26 (22562/25856)
Train | Batch (196/196) | Top-1: 87.11 (43557/50000)
Regular: 1.7407760620117188
Epoche: 28; regular: 1.7407760620117188: flops 68862592
#Filters: 1134, #FLOPs: 68.57M | Top-1: 79.95
Epoch 29
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 87.59 (22646/25856)
Train | Batch (196/196) | Top-1: 87.21 (43603/50000)
Regular: 1.708559274673462
Epoche: 29; regular: 1.708559274673462: flops 68862592
#Filters: 1133, #FLOPs: 68.42M | Top-1: 79.76
Drin!!
Layers that will be prunned: [(3, 1), (5, 1), (7, 1)]
Prunning filters..
Layer index: 3; Pruned filters: 1
Layer index: 5; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 67.978M | #Params: 0.461M
1.0065076990073354
After Growth | FLOPs: 67.978M | #Params: 0.461M
I: 2
flops: 67977856
Before Pruning | FLOPs: 67.978M | #Params: 0.461M
Epoch 0
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 26.98 (6976/25856)
Train | Batch (196/196) | Top-1: 32.76 (16379/50000)
Regular: 1.6646193265914917
Epoche: 0; regular: 1.6646193265914917: flops 67977856
#Filters: 1106, #FLOPs: 64.00M | Top-1: 36.57
Epoch 1
Train | Batch (1/196) | Top-1: 45.70 (117/256)
Train | Batch (101/196) | Top-1: 43.69 (11296/25856)
Train | Batch (196/196) | Top-1: 45.99 (22996/50000)
Regular: 1.625868558883667
Epoche: 1; regular: 1.625868558883667: flops 67977856
#Filters: 1103, #FLOPs: 63.55M | Top-1: 41.66
Epoch 2
Train | Batch (1/196) | Top-1: 48.83 (125/256)
Train | Batch (101/196) | Top-1: 51.95 (13431/25856)
Train | Batch (196/196) | Top-1: 53.20 (26600/50000)
Regular: 1.593960165977478
Epoche: 2; regular: 1.593960165977478: flops 67977856
#Filters: 1102, #FLOPs: 63.41M | Top-1: 52.21
Epoch 3
Train | Batch (1/196) | Top-1: 51.56 (132/256)
Train | Batch (101/196) | Top-1: 57.05 (14750/25856)
Train | Batch (196/196) | Top-1: 57.87 (28937/50000)
Regular: 1.5620081424713135
Epoche: 3; regular: 1.5620081424713135: flops 67977856
#Filters: 1096, #FLOPs: 62.52M | Top-1: 38.35
Epoch 4
Train | Batch (1/196) | Top-1: 64.45 (165/256)
Train | Batch (101/196) | Top-1: 61.36 (15864/25856)
Train | Batch (196/196) | Top-1: 61.85 (30927/50000)
Regular: 1.529941439628601
Epoche: 4; regular: 1.529941439628601: flops 67977856
#Filters: 1094, #FLOPs: 62.23M | Top-1: 62.12
Epoch 5
Train | Batch (1/196) | Top-1: 62.50 (160/256)
Train | Batch (101/196) | Top-1: 64.27 (16617/25856)
Train | Batch (196/196) | Top-1: 64.80 (32398/50000)
Regular: 1.4972546100616455
Epoche: 5; regular: 1.4972546100616455: flops 67977856
#Filters: 1091, #FLOPs: 61.78M | Top-1: 59.37
Epoch 6
Train | Batch (1/196) | Top-1: 60.94 (156/256)
Train | Batch (101/196) | Top-1: 66.84 (17282/25856)
Train | Batch (196/196) | Top-1: 67.26 (33629/50000)
Regular: 1.4653974771499634
Epoche: 6; regular: 1.4653974771499634: flops 67977856
#Filters: 1092, #FLOPs: 61.93M | Top-1: 65.54
Epoch 7
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 68.76 (17779/25856)
Train | Batch (196/196) | Top-1: 69.20 (34599/50000)
Regular: 1.433587670326233
Epoche: 7; regular: 1.433587670326233: flops 67977856
#Filters: 1089, #FLOPs: 61.49M | Top-1: 54.54
Epoch 8
Train | Batch (1/196) | Top-1: 70.31 (180/256)
Train | Batch (101/196) | Top-1: 70.70 (18279/25856)
Train | Batch (196/196) | Top-1: 70.80 (35401/50000)
Regular: 1.4020837545394897
Epoche: 8; regular: 1.4020837545394897: flops 67977856
#Filters: 1088, #FLOPs: 61.34M | Top-1: 66.80
Epoch 9
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 71.29 (18434/25856)
Train | Batch (196/196) | Top-1: 72.07 (36034/50000)
Regular: 1.3709431886672974
Epoche: 9; regular: 1.3709431886672974: flops 67977856
#Filters: 1087, #FLOPs: 61.19M | Top-1: 65.01
Epoch 10
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 72.58 (18767/25856)
Train | Batch (196/196) | Top-1: 73.17 (36587/50000)
Regular: 1.3404659032821655
Epoche: 10; regular: 1.3404659032821655: flops 67977856
#Filters: 1087, #FLOPs: 61.19M | Top-1: 64.36
Epoch 11
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 74.42 (19243/25856)
Train | Batch (196/196) | Top-1: 74.50 (37249/50000)
Regular: 1.3105072975158691
Epoche: 11; regular: 1.3105072975158691: flops 67977856
#Filters: 1083, #FLOPs: 60.61M | Top-1: 71.05
Epoch 12
Train | Batch (1/196) | Top-1: 71.09 (182/256)
Train | Batch (101/196) | Top-1: 74.95 (19379/25856)
Train | Batch (196/196) | Top-1: 75.27 (37635/50000)
Regular: 1.2801599502563477
Epoche: 12; regular: 1.2801599502563477: flops 67977856
#Filters: 1083, #FLOPs: 60.61M | Top-1: 64.75
Epoch 13
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 75.93 (19633/25856)
Train | Batch (196/196) | Top-1: 75.97 (37983/50000)
Regular: 1.2507325410842896
Epoche: 13; regular: 1.2507325410842896: flops 67977856
#Filters: 1082, #FLOPs: 60.46M | Top-1: 70.83
Epoch 14
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 76.82 (19862/25856)
Train | Batch (196/196) | Top-1: 76.71 (38357/50000)
Regular: 1.2221779823303223
Epoche: 14; regular: 1.2221779823303223: flops 67977856
#Filters: 1082, #FLOPs: 60.46M | Top-1: 62.35
Epoch 15
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 77.53 (20047/25856)
Train | Batch (196/196) | Top-1: 77.31 (38654/50000)
Regular: 1.1935867071151733
Epoche: 15; regular: 1.1935867071151733: flops 67977856
#Filters: 1082, #FLOPs: 60.46M | Top-1: 71.07
Epoch 16
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 77.81 (20118/25856)
Train | Batch (196/196) | Top-1: 77.99 (38994/50000)
Regular: 1.1660112142562866
Epoche: 16; regular: 1.1660112142562866: flops 67977856
#Filters: 1080, #FLOPs: 60.16M | Top-1: 72.41
Epoch 17
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 78.81 (20378/25856)
Train | Batch (196/196) | Top-1: 78.62 (39309/50000)
Regular: 1.13991379737854
Epoche: 17; regular: 1.13991379737854: flops 67977856
#Filters: 1081, #FLOPs: 60.31M | Top-1: 66.34
Epoch 18
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 79.08 (20447/25856)
Train | Batch (196/196) | Top-1: 79.27 (39637/50000)
Regular: 1.1154214143753052
Epoche: 18; regular: 1.1154214143753052: flops 67977856
#Filters: 1079, #FLOPs: 60.02M | Top-1: 68.98
Epoch 19
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 79.10 (20453/25856)
Train | Batch (196/196) | Top-1: 79.36 (39682/50000)
Regular: 1.0918081998825073
Epoche: 19; regular: 1.0918081998825073: flops 67977856
#Filters: 1078, #FLOPs: 59.87M | Top-1: 59.46
Epoch 20
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 79.78 (20627/25856)
Train | Batch (196/196) | Top-1: 79.90 (39950/50000)
Regular: 1.0699186325073242
Epoche: 20; regular: 1.0699186325073242: flops 67977856
#Filters: 1078, #FLOPs: 59.87M | Top-1: 77.27
Epoch 21
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.07 (20703/25856)
Train | Batch (196/196) | Top-1: 80.02 (40012/50000)
Regular: 1.0499213933944702
Epoche: 21; regular: 1.0499213933944702: flops 67977856
#Filters: 1078, #FLOPs: 59.87M | Top-1: 74.60
Epoch 22
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.59 (20838/25856)
Train | Batch (196/196) | Top-1: 80.64 (40320/50000)
Regular: 1.0318135023117065
Epoche: 22; regular: 1.0318135023117065: flops 67977856
#Filters: 1077, #FLOPs: 59.72M | Top-1: 70.32
Epoch 23
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 81.07 (20961/25856)
Train | Batch (196/196) | Top-1: 80.92 (40460/50000)
Regular: 1.015656590461731
Epoche: 23; regular: 1.015656590461731: flops 67977856
#Filters: 1075, #FLOPs: 59.43M | Top-1: 72.04
Epoch 24
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 81.08 (20964/25856)
Train | Batch (196/196) | Top-1: 81.17 (40586/50000)
Regular: 1.0001718997955322
Epoche: 24; regular: 1.0001718997955322: flops 67977856
#Filters: 1078, #FLOPs: 59.87M | Top-1: 60.86
Epoch 25
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 81.25 (21009/25856)
Train | Batch (196/196) | Top-1: 81.37 (40683/50000)
Regular: 0.9848434925079346
Epoche: 25; regular: 0.9848434925079346: flops 67977856
#Filters: 1076, #FLOPs: 59.57M | Top-1: 70.85
Epoch 26
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 81.99 (21199/25856)
Train | Batch (196/196) | Top-1: 81.78 (40891/50000)
Regular: 0.970158040523529
Epoche: 26; regular: 0.970158040523529: flops 67977856
#Filters: 1075, #FLOPs: 59.43M | Top-1: 71.71
Epoch 27
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 82.00 (21203/25856)
Train | Batch (196/196) | Top-1: 82.20 (41099/50000)
Regular: 0.9558761119842529
Epoche: 27; regular: 0.9558761119842529: flops 67977856
#Filters: 1074, #FLOPs: 59.35M | Top-1: 71.10
Epoch 28
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 81.93 (21183/25856)
Train | Batch (196/196) | Top-1: 82.17 (41086/50000)
Regular: 0.9418901205062866
Epoche: 28; regular: 0.9418901205062866: flops 67977856
#Filters: 1074, #FLOPs: 59.35M | Top-1: 71.52
Epoch 29
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 82.53 (21338/25856)
Train | Batch (196/196) | Top-1: 82.62 (41310/50000)
Regular: 0.928631067276001
Epoche: 29; regular: 0.928631067276001: flops 67977856
#Filters: 1074, #FLOPs: 59.35M | Top-1: 75.27
Drin!!
Layers that will be prunned: [(1, 8), (3, 13), (5, 11), (7, 13), (9, 13), (17, 1)]
Prunning filters..
Layer index: 1; Pruned filters: 2
Layer index: 1; Pruned filters: 3
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 2
Layer index: 3; Pruned filters: 11
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 5; Pruned filters: 2
Layer index: 5; Pruned filters: 7
Layer index: 5; Pruned filters: 2
Layer index: 7; Pruned filters: 1
Layer index: 7; Pruned filters: 2
Layer index: 7; Pruned filters: 10
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 2
Layer index: 9; Pruned filters: 10
Layer index: 17; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 50.726M | #Params: 0.444M
1.1658655469135768
After Growth | FLOPs: 68.476M | #Params: 0.606M
I: 3
flops: 68475630
Before Pruning | FLOPs: 68.476M | #Params: 0.606M
Epoch 0
Train | Batch (1/196) | Top-1: 67.19 (172/256)
Train | Batch (101/196) | Top-1: 80.07 (20702/25856)
Train | Batch (196/196) | Top-1: 81.02 (40508/50000)
Regular: 1.8141926527023315
Epoche: 0; regular: 1.8141926527023315: flops 68475630
#Filters: 1250, #FLOPs: 67.78M | Top-1: 64.64
Epoch 1
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 82.44 (21315/25856)
Train | Batch (196/196) | Top-1: 82.76 (41381/50000)
Regular: 1.785325050354004
Epoche: 1; regular: 1.785325050354004: flops 68475630
#Filters: 1248, #FLOPs: 67.78M | Top-1: 59.30
Epoch 2
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 82.76 (21398/25856)
Train | Batch (196/196) | Top-1: 82.89 (41445/50000)
Regular: 1.7569910287857056
Epoche: 2; regular: 1.7569910287857056: flops 68475630
#Filters: 1248, #FLOPs: 67.78M | Top-1: 75.19
Epoch 3
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 83.48 (21585/25856)
Train | Batch (196/196) | Top-1: 83.38 (41688/50000)
Regular: 1.7309620380401611
Epoche: 3; regular: 1.7309620380401611: flops 68475630
#Filters: 1247, #FLOPs: 67.69M | Top-1: 73.01
Epoch 4
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 83.71 (21643/25856)
Train | Batch (196/196) | Top-1: 83.47 (41737/50000)
Regular: 1.7058684825897217
Epoche: 4; regular: 1.7058684825897217: flops 68475630
#Filters: 1248, #FLOPs: 67.78M | Top-1: 74.33
Epoch 5
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 84.10 (21744/25856)
Train | Batch (196/196) | Top-1: 83.91 (41957/50000)
Regular: 1.6799813508987427
Epoche: 5; regular: 1.6799813508987427: flops 68475630
#Filters: 1247, #FLOPs: 67.69M | Top-1: 63.98
Epoch 6
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 84.35 (21810/25856)
Train | Batch (196/196) | Top-1: 84.07 (42033/50000)
Regular: 1.6550769805908203
Epoche: 6; regular: 1.6550769805908203: flops 68475630
#Filters: 1247, #FLOPs: 67.69M | Top-1: 74.48
Epoch 7
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 84.11 (21747/25856)
Train | Batch (196/196) | Top-1: 84.26 (42130/50000)
Regular: 1.6306802034378052
Epoche: 7; regular: 1.6306802034378052: flops 68475630
#Filters: 1247, #FLOPs: 67.69M | Top-1: 70.59
Epoch 8
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 84.37 (21815/25856)
Train | Batch (196/196) | Top-1: 84.38 (42188/50000)
Regular: 1.6066184043884277
Epoche: 8; regular: 1.6066184043884277: flops 68475630
#Filters: 1248, #FLOPs: 67.78M | Top-1: 64.95
Epoch 9
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 84.62 (21879/25856)
Train | Batch (196/196) | Top-1: 84.76 (42380/50000)
Regular: 1.5816797018051147
Epoche: 9; regular: 1.5816797018051147: flops 68475630
#Filters: 1246, #FLOPs: 67.60M | Top-1: 77.69
Epoch 10
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 85.05 (21991/25856)
Train | Batch (196/196) | Top-1: 85.13 (42564/50000)
Regular: 1.557442545890808
Epoche: 10; regular: 1.557442545890808: flops 68475630
#Filters: 1246, #FLOPs: 67.60M | Top-1: 73.09
Epoch 11
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 85.12 (22009/25856)
Train | Batch (196/196) | Top-1: 85.03 (42514/50000)
Regular: 1.5330272912979126
Epoche: 11; regular: 1.5330272912979126: flops 68475630
#Filters: 1246, #FLOPs: 67.60M | Top-1: 67.66
Epoch 12
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 85.27 (22047/25856)
Train | Batch (196/196) | Top-1: 85.28 (42641/50000)
Regular: 1.509617805480957
Epoche: 12; regular: 1.509617805480957: flops 68475630
#Filters: 1244, #FLOPs: 67.43M | Top-1: 77.03
Epoch 13
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 85.52 (22111/25856)
Train | Batch (196/196) | Top-1: 85.37 (42686/50000)
Regular: 1.4864749908447266
Epoche: 13; regular: 1.4864749908447266: flops 68475630
#Filters: 1246, #FLOPs: 67.60M | Top-1: 78.69
Epoch 14
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 85.71 (22162/25856)
Train | Batch (196/196) | Top-1: 85.66 (42832/50000)
Regular: 1.4642577171325684
Epoche: 14; regular: 1.4642577171325684: flops 68475630
#Filters: 1246, #FLOPs: 67.60M | Top-1: 78.41
Epoch 15
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 86.15 (22276/25856)
Train | Batch (196/196) | Top-1: 85.97 (42986/50000)
Regular: 1.4413158893585205
Epoche: 15; regular: 1.4413158893585205: flops 68475630
#Filters: 1245, #FLOPs: 67.52M | Top-1: 78.36
Epoch 16
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 85.78 (22180/25856)
Train | Batch (196/196) | Top-1: 86.07 (43034/50000)
Regular: 1.4189220666885376
Epoche: 16; regular: 1.4189220666885376: flops 68475630
#Filters: 1245, #FLOPs: 67.52M | Top-1: 81.21
Epoch 17
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 86.31 (22316/25856)
Train | Batch (196/196) | Top-1: 86.15 (43074/50000)
Regular: 1.3962839841842651
Epoche: 17; regular: 1.3962839841842651: flops 68475630
#Filters: 1245, #FLOPs: 67.52M | Top-1: 76.74
Epoch 18
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 86.52 (22370/25856)
Train | Batch (196/196) | Top-1: 86.26 (43131/50000)
Regular: 1.3747104406356812
Epoche: 18; regular: 1.3747104406356812: flops 68475630
#Filters: 1245, #FLOPs: 67.61M | Top-1: 74.06
Epoch 19
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 86.36 (22328/25856)
Train | Batch (196/196) | Top-1: 86.37 (43184/50000)
Regular: 1.3529338836669922
Epoche: 19; regular: 1.3529338836669922: flops 68475630
#Filters: 1242, #FLOPs: 67.26M | Top-1: 59.89
Epoch 20
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 86.73 (22424/25856)
Train | Batch (196/196) | Top-1: 86.67 (43337/50000)
Regular: 1.3304375410079956
Epoche: 20; regular: 1.3304375410079956: flops 68475630
#Filters: 1240, #FLOPs: 67.09M | Top-1: 75.40
Epoch 21
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 86.75 (22430/25856)
Train | Batch (196/196) | Top-1: 86.62 (43308/50000)
Regular: 1.3085615634918213
Epoche: 21; regular: 1.3085615634918213: flops 68475630
#Filters: 1241, #FLOPs: 67.18M | Top-1: 71.48
Epoch 22
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 86.81 (22445/25856)
Train | Batch (196/196) | Top-1: 86.96 (43479/50000)
Regular: 1.286764144897461
Epoche: 22; regular: 1.286764144897461: flops 68475630
#Filters: 1240, #FLOPs: 67.09M | Top-1: 81.41
Epoch 23
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 86.97 (22488/25856)
Train | Batch (196/196) | Top-1: 87.11 (43556/50000)
Regular: 1.2652652263641357
Epoche: 23; regular: 1.2652652263641357: flops 68475630
#Filters: 1238, #FLOPs: 66.92M | Top-1: 73.20
Epoch 24
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 87.28 (22568/25856)
Train | Batch (196/196) | Top-1: 87.16 (43582/50000)
Regular: 1.2442671060562134
Epoche: 24; regular: 1.2442671060562134: flops 68475630
#Filters: 1241, #FLOPs: 67.18M | Top-1: 65.98
Epoch 25
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 87.38 (22593/25856)
Train | Batch (196/196) | Top-1: 87.30 (43648/50000)
Regular: 1.2226570844650269
Epoche: 25; regular: 1.2226570844650269: flops 68475630
#Filters: 1238, #FLOPs: 66.92M | Top-1: 79.17
Epoch 26
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 87.40 (22598/25856)
Train | Batch (196/196) | Top-1: 87.43 (43717/50000)
Regular: 1.2014164924621582
Epoche: 26; regular: 1.2014164924621582: flops 68475630
#Filters: 1240, #FLOPs: 67.09M | Top-1: 75.03
Epoch 27
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.60 (22649/25856)
Train | Batch (196/196) | Top-1: 87.47 (43735/50000)
Regular: 1.1799103021621704
Epoche: 27; regular: 1.1799103021621704: flops 68475630
#Filters: 1238, #FLOPs: 66.92M | Top-1: 74.36
Epoch 28
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 87.99 (22750/25856)
Train | Batch (196/196) | Top-1: 87.79 (43896/50000)
Regular: 1.1587945222854614
Epoche: 28; regular: 1.1587945222854614: flops 68475630
#Filters: 1239, #FLOPs: 67.01M | Top-1: 79.74
Epoch 29
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 87.80 (22702/25856)
Train | Batch (196/196) | Top-1: 87.84 (43919/50000)
Regular: 1.137342095375061
Epoche: 29; regular: 1.137342095375061: flops 68475630
#Filters: 1237, #FLOPs: 66.84M | Top-1: 80.18
Drin!!
Layers that will be prunned: [(1, 1), (3, 1), (7, 1), (9, 1), (13, 2), (15, 2), (17, 5), (19, 2)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 2
Layer index: 19; Pruned filters: 2
Target (flops): 68.863M
After Pruning | FLOPs: 65.199M | #Params: 0.597M
1.0278210460367576
After Growth | FLOPs: 68.834M | #Params: 0.630M
I: 4
flops: 68834498
Before Pruning | FLOPs: 68.834M | #Params: 0.630M
Epoch 0
Train | Batch (1/196) | Top-1: 32.42 (83/256)
Train | Batch (101/196) | Top-1: 77.39 (20009/25856)
Train | Batch (196/196) | Top-1: 80.69 (40346/50000)
Regular: 1.2792186737060547
Epoche: 0; regular: 1.2792186737060547: flops 68834498
#Filters: 1268, #FLOPs: 68.65M | Top-1: 80.10
Epoch 1
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 85.36 (22070/25856)
Train | Batch (196/196) | Top-1: 85.54 (42771/50000)
Regular: 1.2529242038726807
Epoche: 1; regular: 1.2529242038726807: flops 68834498
#Filters: 1266, #FLOPs: 68.38M | Top-1: 74.99
Epoch 2
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 86.34 (22323/25856)
Train | Batch (196/196) | Top-1: 86.49 (43247/50000)
Regular: 1.2261959314346313
Epoche: 2; regular: 1.2261959314346313: flops 68834498
#Filters: 1267, #FLOPs: 68.47M | Top-1: 76.54
Epoch 3
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 86.66 (22406/25856)
Train | Batch (196/196) | Top-1: 86.69 (43347/50000)
Regular: 1.2014484405517578
Epoche: 3; regular: 1.2014484405517578: flops 68834498
#Filters: 1266, #FLOPs: 68.38M | Top-1: 80.37
Epoch 4
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 87.03 (22503/25856)
Train | Batch (196/196) | Top-1: 86.94 (43471/50000)
Regular: 1.1775944232940674
Epoche: 4; regular: 1.1775944232940674: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 77.43
Epoch 5
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 87.45 (22611/25856)
Train | Batch (196/196) | Top-1: 87.53 (43767/50000)
Regular: 1.1542675495147705
Epoche: 5; regular: 1.1542675495147705: flops 68834498
#Filters: 1266, #FLOPs: 68.38M | Top-1: 71.43
Epoch 6
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 87.50 (22625/25856)
Train | Batch (196/196) | Top-1: 87.58 (43790/50000)
Regular: 1.1317927837371826
Epoche: 6; regular: 1.1317927837371826: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 71.44
Epoch 7
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 87.85 (22715/25856)
Train | Batch (196/196) | Top-1: 87.89 (43943/50000)
Regular: 1.1090795993804932
Epoche: 7; regular: 1.1090795993804932: flops 68834498
#Filters: 1266, #FLOPs: 68.38M | Top-1: 73.18
Epoch 8
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 88.21 (22808/25856)
Train | Batch (196/196) | Top-1: 88.21 (44106/50000)
Regular: 1.087499737739563
Epoche: 8; regular: 1.087499737739563: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 70.29
Epoch 9
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 88.37 (22849/25856)
Train | Batch (196/196) | Top-1: 88.17 (44084/50000)
Regular: 1.0655224323272705
Epoche: 9; regular: 1.0655224323272705: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 78.76
Epoch 10
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 88.25 (22818/25856)
Train | Batch (196/196) | Top-1: 88.10 (44049/50000)
Regular: 1.043940782546997
Epoche: 10; regular: 1.043940782546997: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 79.58
Epoch 11
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 88.11 (22782/25856)
Train | Batch (196/196) | Top-1: 87.96 (43978/50000)
Regular: 1.0226460695266724
Epoche: 11; regular: 1.0226460695266724: flops 68834498
#Filters: 1264, #FLOPs: 68.20M | Top-1: 82.44
Epoch 12
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 88.50 (22882/25856)
Train | Batch (196/196) | Top-1: 88.40 (44202/50000)
Regular: 1.0006154775619507
Epoche: 12; regular: 1.0006154775619507: flops 68834498
#Filters: 1262, #FLOPs: 68.03M | Top-1: 75.05
Epoch 13
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 88.67 (22926/25856)
Train | Batch (196/196) | Top-1: 88.56 (44279/50000)
Regular: 0.9788044691085815
Epoche: 13; regular: 0.9788044691085815: flops 68834498
#Filters: 1264, #FLOPs: 68.20M | Top-1: 71.97
Epoch 14
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 88.70 (22935/25856)
Train | Batch (196/196) | Top-1: 88.59 (44297/50000)
Regular: 0.9582415223121643
Epoche: 14; regular: 0.9582415223121643: flops 68834498
#Filters: 1264, #FLOPs: 68.20M | Top-1: 77.77
Epoch 15
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 88.85 (22974/25856)
Train | Batch (196/196) | Top-1: 88.89 (44446/50000)
Regular: 0.9377956986427307
Epoche: 15; regular: 0.9377956986427307: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 75.33
Epoch 16
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 88.83 (22968/25856)
Train | Batch (196/196) | Top-1: 88.78 (44391/50000)
Regular: 0.9163373112678528
Epoche: 16; regular: 0.9163373112678528: flops 68834498
#Filters: 1263, #FLOPs: 68.12M | Top-1: 79.88
Epoch 17
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 89.09 (23035/25856)
Train | Batch (196/196) | Top-1: 88.59 (44294/50000)
Regular: 0.8962721824645996
Epoche: 17; regular: 0.8962721824645996: flops 68834498
#Filters: 1264, #FLOPs: 68.20M | Top-1: 80.25
Epoch 18
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 89.04 (23023/25856)
Train | Batch (196/196) | Top-1: 89.06 (44528/50000)
Regular: 0.8774241805076599
Epoche: 18; regular: 0.8774241805076599: flops 68834498
#Filters: 1264, #FLOPs: 68.20M | Top-1: 70.65
Epoch 19
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 89.05 (23025/25856)
Train | Batch (196/196) | Top-1: 88.92 (44461/50000)
Regular: 0.8569390177726746
Epoche: 19; regular: 0.8569390177726746: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 70.51
Epoch 20
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 88.81 (22963/25856)
Train | Batch (196/196) | Top-1: 89.10 (44550/50000)
Regular: 0.837135910987854
Epoche: 20; regular: 0.837135910987854: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 76.21
Epoch 21
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 89.30 (23090/25856)
Train | Batch (196/196) | Top-1: 89.22 (44609/50000)
Regular: 0.8183479309082031
Epoche: 21; regular: 0.8183479309082031: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 77.15
Epoch 22
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 89.64 (23178/25856)
Train | Batch (196/196) | Top-1: 89.33 (44666/50000)
Regular: 0.7996208667755127
Epoche: 22; regular: 0.7996208667755127: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 75.36
Epoch 23
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 89.24 (23073/25856)
Train | Batch (196/196) | Top-1: 89.29 (44643/50000)
Regular: 0.7820910811424255
Epoche: 23; regular: 0.7820910811424255: flops 68834498
#Filters: 1264, #FLOPs: 68.20M | Top-1: 77.95
Epoch 24
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 89.72 (23197/25856)
Train | Batch (196/196) | Top-1: 89.27 (44634/50000)
Regular: 0.7643147706985474
Epoche: 24; regular: 0.7643147706985474: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 71.29
Epoch 25
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 89.16 (23052/25856)
Train | Batch (196/196) | Top-1: 89.35 (44676/50000)
Regular: 0.7489489316940308
Epoche: 25; regular: 0.7489489316940308: flops 68834498
#Filters: 1265, #FLOPs: 68.29M | Top-1: 74.41
Epoch 26
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 89.37 (23107/25856)
Train | Batch (196/196) | Top-1: 89.35 (44673/50000)
Regular: 0.732377827167511
Epoche: 26; regular: 0.732377827167511: flops 68834498
#Filters: 1260, #FLOPs: 67.98M | Top-1: 66.96
Epoch 27
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 89.32 (23094/25856)
Train | Batch (196/196) | Top-1: 89.26 (44628/50000)
Regular: 0.7175065875053406
Epoche: 27; regular: 0.7175065875053406: flops 68834498
#Filters: 1261, #FLOPs: 68.11M | Top-1: 71.22
Epoch 28
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 89.70 (23194/25856)
Train | Batch (196/196) | Top-1: 89.43 (44714/50000)
Regular: 0.7034122943878174
Epoche: 28; regular: 0.7034122943878174: flops 68834498
#Filters: 1260, #FLOPs: 68.07M | Top-1: 74.89
Epoch 29
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 89.64 (23177/25856)
Train | Batch (196/196) | Top-1: 89.57 (44785/50000)
Regular: 0.6894451975822449
Epoche: 29; regular: 0.6894451975822449: flops 68834498
#Filters: 1259, #FLOPs: 68.02M | Top-1: 79.77
Drin!!
Layers that will be prunned: [(1, 3), (5, 1), (15, 1), (17, 1), (27, 2), (29, 4)]
Prunning filters..
Layer index: 1; Pruned filters: 3
Layer index: 5; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 66.478M | #Params: 0.618M
1.017855330543455
After Growth | FLOPs: 68.919M | #Params: 0.638M
I: 5
flops: 68919180
Before Pruning | FLOPs: 68.919M | #Params: 0.638M
Epoch 0
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 34.12 (8821/25856)
Train | Batch (196/196) | Top-1: 39.89 (19945/50000)
Regular: 0.7510861754417419
Epoche: 0; regular: 0.7510861754417419: flops 68919180
#Filters: 1216, #FLOPs: 65.14M | Top-1: 23.57
Epoch 1
Train | Batch (1/196) | Top-1: 53.52 (137/256)
Train | Batch (101/196) | Top-1: 52.56 (13589/25856)
Train | Batch (196/196) | Top-1: 55.30 (27651/50000)
Regular: 0.7284080386161804
Epoche: 1; regular: 0.7284080386161804: flops 68919180
#Filters: 1185, #FLOPs: 62.90M | Top-1: 38.91
Epoch 2
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 64.75 (16742/25856)
Train | Batch (196/196) | Top-1: 67.99 (33996/50000)
Regular: 0.72540283203125
Epoche: 2; regular: 0.72540283203125: flops 68919180
#Filters: 1202, #FLOPs: 64.02M | Top-1: 62.36
Epoch 3
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 76.90 (19884/25856)
Train | Batch (196/196) | Top-1: 78.23 (39113/50000)
Regular: 0.7227417826652527
Epoche: 3; regular: 0.7227417826652527: flops 68919180
#Filters: 1222, #FLOPs: 65.41M | Top-1: 66.99
Epoch 4
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 81.79 (21147/25856)
Train | Batch (196/196) | Top-1: 82.28 (41140/50000)
Regular: 0.7116947770118713
Epoche: 4; regular: 0.7116947770118713: flops 68919180
#Filters: 1238, #FLOPs: 66.44M | Top-1: 70.57
Epoch 5
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.13 (21752/25856)
Train | Batch (196/196) | Top-1: 84.53 (42263/50000)
Regular: 0.6981117129325867
Epoche: 5; regular: 0.6981117129325867: flops 68919180
#Filters: 1248, #FLOPs: 67.21M | Top-1: 73.80
Epoch 6
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 85.47 (22100/25856)
Train | Batch (196/196) | Top-1: 85.75 (42873/50000)
Regular: 0.6840943098068237
Epoche: 6; regular: 0.6840943098068237: flops 68919180
#Filters: 1253, #FLOPs: 67.44M | Top-1: 81.31
Epoch 7
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 86.04 (22246/25856)
Train | Batch (196/196) | Top-1: 86.13 (43064/50000)
Regular: 0.6705442070960999
Epoche: 7; regular: 0.6705442070960999: flops 68919180
#Filters: 1257, #FLOPs: 67.79M | Top-1: 78.54
Epoch 8
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 86.78 (22438/25856)
Train | Batch (196/196) | Top-1: 86.78 (43388/50000)
Regular: 0.656110942363739
Epoche: 8; regular: 0.656110942363739: flops 68919180
#Filters: 1256, #FLOPs: 67.75M | Top-1: 66.37
Epoch 9
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 87.21 (22548/25856)
Train | Batch (196/196) | Top-1: 87.08 (43541/50000)
Regular: 0.6421902179718018
Epoche: 9; regular: 0.6421902179718018: flops 68919180
#Filters: 1257, #FLOPs: 67.79M | Top-1: 72.70
Epoch 10
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.30 (22573/25856)
Train | Batch (196/196) | Top-1: 87.31 (43654/50000)
Regular: 0.629256010055542
Epoche: 10; regular: 0.629256010055542: flops 68919180
#Filters: 1234, #FLOPs: 65.63M | Top-1: 69.35
Epoch 11
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 87.18 (22542/25856)
Train | Batch (196/196) | Top-1: 87.37 (43684/50000)
Regular: 0.6174954771995544
Epoche: 11; regular: 0.6174954771995544: flops 68919180
#Filters: 1233, #FLOPs: 65.59M | Top-1: 75.08
Epoch 12
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 88.18 (22801/25856)
Train | Batch (196/196) | Top-1: 87.91 (43953/50000)
Regular: 0.6070874333381653
Epoche: 12; regular: 0.6070874333381653: flops 68919180
#Filters: 1236, #FLOPs: 65.86M | Top-1: 76.28
Epoch 13
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 87.86 (22716/25856)
Train | Batch (196/196) | Top-1: 87.95 (43974/50000)
Regular: 0.5974505543708801
Epoche: 13; regular: 0.5974505543708801: flops 68919180
#Filters: 1237, #FLOPs: 66.00M | Top-1: 73.71
Epoch 14
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 88.45 (22869/25856)
Train | Batch (196/196) | Top-1: 88.36 (44178/50000)
Regular: 0.5876145362854004
Epoche: 14; regular: 0.5876145362854004: flops 68919180
#Filters: 1237, #FLOPs: 66.00M | Top-1: 78.39
Epoch 15
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 88.32 (22836/25856)
Train | Batch (196/196) | Top-1: 88.20 (44101/50000)
Regular: 0.5785871148109436
Epoche: 15; regular: 0.5785871148109436: flops 68919180
#Filters: 1237, #FLOPs: 65.90M | Top-1: 76.33
Epoch 16
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 88.59 (22906/25856)
Train | Batch (196/196) | Top-1: 88.50 (44252/50000)
Regular: 0.5702421069145203
Epoche: 16; regular: 0.5702421069145203: flops 68919180
#Filters: 1237, #FLOPs: 65.90M | Top-1: 78.47
Epoch 17
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 88.85 (22972/25856)
Train | Batch (196/196) | Top-1: 88.53 (44265/50000)
Regular: 0.5616536736488342
Epoche: 17; regular: 0.5616536736488342: flops 68919180
#Filters: 1235, #FLOPs: 65.72M | Top-1: 79.05
Epoch 18
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 88.79 (22957/25856)
Train | Batch (196/196) | Top-1: 88.79 (44396/50000)
Regular: 0.5541208386421204
Epoche: 18; regular: 0.5541208386421204: flops 68919180
#Filters: 1234, #FLOPs: 65.68M | Top-1: 77.98
Epoch 19
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 88.67 (22926/25856)
Train | Batch (196/196) | Top-1: 88.75 (44373/50000)
Regular: 0.5465235114097595
Epoche: 19; regular: 0.5465235114097595: flops 68919180
#Filters: 1237, #FLOPs: 65.95M | Top-1: 71.70
Epoch 20
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 89.10 (23037/25856)
Train | Batch (196/196) | Top-1: 88.82 (44409/50000)
Regular: 0.5385962724685669
Epoche: 20; regular: 0.5385962724685669: flops 68919180
#Filters: 1238, #FLOPs: 66.13M | Top-1: 73.88
Epoch 21
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 89.28 (23083/25856)
Train | Batch (196/196) | Top-1: 89.27 (44635/50000)
Regular: 0.530943751335144
Epoche: 21; regular: 0.530943751335144: flops 68919180
#Filters: 1237, #FLOPs: 65.95M | Top-1: 80.18
Epoch 22
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 89.25 (23077/25856)
Train | Batch (196/196) | Top-1: 89.11 (44556/50000)
Regular: 0.5233110785484314
Epoche: 22; regular: 0.5233110785484314: flops 68919180
#Filters: 1236, #FLOPs: 65.86M | Top-1: 77.87
Epoch 23
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 89.80 (23218/25856)
Train | Batch (196/196) | Top-1: 89.38 (44689/50000)
Regular: 0.515638530254364
Epoche: 23; regular: 0.515638530254364: flops 68919180
#Filters: 1232, #FLOPs: 65.59M | Top-1: 81.59
Epoch 24
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 89.29 (23088/25856)
Train | Batch (196/196) | Top-1: 89.29 (44645/50000)
Regular: 0.5080593824386597
Epoche: 24; regular: 0.5080593824386597: flops 68919180
#Filters: 1232, #FLOPs: 65.59M | Top-1: 72.80
Epoch 25
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 89.50 (23142/25856)
Train | Batch (196/196) | Top-1: 89.32 (44661/50000)
Regular: 0.5009936690330505
Epoche: 25; regular: 0.5009936690330505: flops 68919180
#Filters: 1233, #FLOPs: 65.63M | Top-1: 76.59
Epoch 26
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 89.58 (23163/25856)
Train | Batch (196/196) | Top-1: 89.32 (44660/50000)
Regular: 0.4931996762752533
Epoche: 26; regular: 0.4931996762752533: flops 68919180
#Filters: 1232, #FLOPs: 65.54M | Top-1: 80.54
Epoch 27
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 90.18 (23316/25856)
Train | Batch (196/196) | Top-1: 89.79 (44895/50000)
Regular: 0.48546770215034485
Epoche: 27; regular: 0.48546770215034485: flops 68919180
#Filters: 1231, #FLOPs: 65.50M | Top-1: 76.06
Epoch 28
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 89.70 (23194/25856)
Train | Batch (196/196) | Top-1: 89.60 (44802/50000)
Regular: 0.478629469871521
Epoche: 28; regular: 0.478629469871521: flops 68919180
#Filters: 1233, #FLOPs: 65.59M | Top-1: 74.47
Epoch 29
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 89.80 (23219/25856)
Train | Batch (196/196) | Top-1: 89.72 (44860/50000)
Regular: 0.47185900807380676
Epoche: 29; regular: 0.47185900807380676: flops 68919180
#Filters: 1232, #FLOPs: 65.59M | Top-1: 79.27
Drin!!
Layers that will be prunned: [(1, 2), (5, 1), (11, 1), (13, 5), (15, 7), (17, 9), (19, 5), (25, 1), (27, 8), (29, 8)]
Prunning filters..
Layer index: 1; Pruned filters: 2
Layer index: 5; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 5
Layer index: 15; Pruned filters: 2
Layer index: 15; Pruned filters: 5
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 2
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 5
Layer index: 19; Pruned filters: 5
Layer index: 25; Pruned filters: 1
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 61.477M | #Params: 0.594M
1.0586265345946573
After Growth | FLOPs: 68.687M | #Params: 0.670M
Train | Batch (1/196) | Top-1: 30.86 (79/256)
Train | Batch (101/196) | Top-1: 29.09 (7522/25856)
Train | Batch (196/196) | Top-1: 34.71 (17354/50000)
Regular: nan
Epoche: 0; regular: nan: flops 68686526
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
#Filters: 1305, #FLOPs: 69.07M | Top-1: 39.13
Epoch 0 | Top-1: 39.13
Train | Batch (1/196) | Top-1: 39.06 (100/256)
Train | Batch (101/196) | Top-1: 45.83 (11850/25856)
Train | Batch (196/196) | Top-1: 48.45 (24223/50000)
Regular: nan
Epoche: 1; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 41.93
Epoch 1 | Top-1: 41.93
Train | Batch (1/196) | Top-1: 52.73 (135/256)
Train | Batch (101/196) | Top-1: 56.80 (14685/25856)
Train | Batch (196/196) | Top-1: 58.69 (29343/50000)
Regular: nan
Epoche: 2; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 44.46
Epoch 2 | Top-1: 44.46
Train | Batch (1/196) | Top-1: 56.25 (144/256)
Train | Batch (101/196) | Top-1: 63.79 (16493/25856)
Train | Batch (196/196) | Top-1: 64.72 (32358/50000)
Regular: nan
Epoche: 3; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 60.17
Epoch 3 | Top-1: 60.17
Train | Batch (1/196) | Top-1: 68.36 (175/256)
Train | Batch (101/196) | Top-1: 68.08 (17602/25856)
Train | Batch (196/196) | Top-1: 68.62 (34312/50000)
Regular: nan
Epoche: 4; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 61.05
Epoch 4 | Top-1: 61.05
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 71.34 (18445/25856)
Train | Batch (196/196) | Top-1: 71.90 (35949/50000)
Regular: nan
Epoche: 5; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 63.72
Epoch 5 | Top-1: 63.72
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 74.52 (19269/25856)
Train | Batch (196/196) | Top-1: 74.79 (37394/50000)
Regular: nan
Epoche: 6; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 62.42
Epoch 6 | Top-1: 62.42
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 76.51 (19782/25856)
Train | Batch (196/196) | Top-1: 76.65 (38325/50000)
Regular: nan
Epoche: 7; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 66.69
Epoch 7 | Top-1: 66.69
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 77.84 (20126/25856)
Train | Batch (196/196) | Top-1: 78.13 (39067/50000)
Regular: nan
Epoche: 8; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 69.33
Epoch 8 | Top-1: 69.33
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 79.21 (20480/25856)
Train | Batch (196/196) | Top-1: 79.19 (39594/50000)
Regular: nan
Epoche: 9; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 73.10
Epoch 9 | Top-1: 73.10
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 80.10 (20710/25856)
Train | Batch (196/196) | Top-1: 80.28 (40139/50000)
Regular: nan
Epoche: 10; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 71.09
Epoch 10 | Top-1: 71.09
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 80.55 (20828/25856)
Train | Batch (196/196) | Top-1: 80.97 (40485/50000)
Regular: nan
Epoche: 11; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 78.49
Epoch 11 | Top-1: 78.49
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 81.78 (21144/25856)
Train | Batch (196/196) | Top-1: 81.70 (40852/50000)
Regular: nan
Epoche: 12; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 71.37
Epoch 12 | Top-1: 71.37
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 82.31 (21283/25856)
Train | Batch (196/196) | Top-1: 82.52 (41260/50000)
Regular: nan
Epoche: 13; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 76.94
Epoch 13 | Top-1: 76.94
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 83.18 (21508/25856)
Train | Batch (196/196) | Top-1: 82.93 (41467/50000)
Regular: nan
Epoche: 14; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 77.00
Epoch 14 | Top-1: 77.00
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.30 (21539/25856)
Train | Batch (196/196) | Top-1: 83.23 (41617/50000)
Regular: nan
Epoche: 15; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 67.72
Epoch 15 | Top-1: 67.72
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 83.96 (21708/25856)
Train | Batch (196/196) | Top-1: 84.04 (42022/50000)
Regular: nan
Epoche: 16; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 78.18
Epoch 16 | Top-1: 78.18
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 84.12 (21749/25856)
Train | Batch (196/196) | Top-1: 83.96 (41980/50000)
Regular: nan
Epoche: 17; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 80.18
Epoch 17 | Top-1: 80.18
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 84.32 (21802/25856)
Train | Batch (196/196) | Top-1: 84.25 (42124/50000)
Regular: nan
Epoche: 18; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 65.53
Epoch 18 | Top-1: 65.53
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 84.95 (21965/25856)
Train | Batch (196/196) | Top-1: 84.68 (42342/50000)
Regular: nan
Epoche: 19; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 70.10
Epoch 19 | Top-1: 70.10
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 84.99 (21976/25856)
Train | Batch (196/196) | Top-1: 84.87 (42433/50000)
Regular: nan
Epoche: 20; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 78.74
Epoch 20 | Top-1: 78.74
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 85.17 (22022/25856)
Train | Batch (196/196) | Top-1: 85.29 (42645/50000)
Regular: nan
Epoche: 21; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 75.62
Epoch 21 | Top-1: 75.62
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 85.81 (22187/25856)
Train | Batch (196/196) | Top-1: 85.38 (42689/50000)
Regular: nan
Epoche: 22; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 79.06
Epoch 22 | Top-1: 79.06
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 85.88 (22205/25856)
Train | Batch (196/196) | Top-1: 85.69 (42846/50000)
Regular: nan
Epoche: 23; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 79.59
Epoch 23 | Top-1: 79.59
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 86.03 (22244/25856)
Train | Batch (196/196) | Top-1: 85.87 (42935/50000)
Regular: nan
Epoche: 24; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 74.35
Epoch 24 | Top-1: 74.35
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 85.89 (22208/25856)
Train | Batch (196/196) | Top-1: 85.91 (42953/50000)
Regular: nan
Epoche: 25; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 77.67
Epoch 25 | Top-1: 77.67
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 86.27 (22307/25856)
Train | Batch (196/196) | Top-1: 86.21 (43104/50000)
Regular: nan
Epoche: 26; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 72.14
Epoch 26 | Top-1: 72.14
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 86.30 (22314/25856)
Train | Batch (196/196) | Top-1: 86.31 (43155/50000)
Regular: nan
Epoche: 27; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 81.23
Epoch 27 | Top-1: 81.23
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 86.46 (22355/25856)
Train | Batch (196/196) | Top-1: 86.28 (43142/50000)
Regular: nan
Epoche: 28; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 76.71
Epoch 28 | Top-1: 76.71
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 86.93 (22477/25856)
Train | Batch (196/196) | Top-1: 86.55 (43275/50000)
Regular: nan
Epoche: 29; regular: nan: flops 68686526
#Filters: 1305, #FLOPs: 69.07M | Top-1: 80.39
Epoch 29 | Top-1: 80.39
model: ResNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(21, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(21, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(21, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(21, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(21, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(21, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(40, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(41, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(34, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(41, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(31, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(41, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(24, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(41, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(34, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(41, 83, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(83, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(83, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(83, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(83, 82, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(82, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(83, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(72, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(83, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(70, 83, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (4): AvgPool2d(kernel_size=8, stride=8, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=83, out_features=10, bias=True)
  )
)
Test acc: 80.39
