no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=1e-08, logger='MorphLogs/logMorphNetFlops1e-8_grow2.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 23.12 (5979/25856)
Train | Batch (196/196) | Top-1: 27.95 (13975/50000)
Regular: 2.727287530899048
Epoche: 0; regular: 2.727287530899048: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 37.31
Epoch 1
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 38.86 (10047/25856)
Train | Batch (196/196) | Top-1: 40.45 (20224/50000)
Regular: 2.7067298889160156
Epoche: 1; regular: 2.7067298889160156: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 44.02
Epoch 2
Train | Batch (1/196) | Top-1: 47.66 (122/256)
Train | Batch (101/196) | Top-1: 46.24 (11956/25856)
Train | Batch (196/196) | Top-1: 47.40 (23699/50000)
Regular: 2.686098575592041
Epoche: 2; regular: 2.686098575592041: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 49.84
Epoch 3
Train | Batch (1/196) | Top-1: 46.88 (120/256)
Train | Batch (101/196) | Top-1: 51.77 (13386/25856)
Train | Batch (196/196) | Top-1: 52.83 (26413/50000)
Regular: 2.665501117706299
Epoche: 3; regular: 2.665501117706299: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 54.17
Epoch 4
Train | Batch (1/196) | Top-1: 54.30 (139/256)
Train | Batch (101/196) | Top-1: 55.87 (14447/25856)
Train | Batch (196/196) | Top-1: 56.90 (28450/50000)
Regular: 2.6448850631713867
Epoche: 4; regular: 2.6448850631713867: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 56.95
Epoch 5
Train | Batch (1/196) | Top-1: 60.16 (154/256)
Train | Batch (101/196) | Top-1: 59.00 (15255/25856)
Train | Batch (196/196) | Top-1: 59.52 (29758/50000)
Regular: 2.6242597103118896
Epoche: 5; regular: 2.6242597103118896: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 59.81
Epoch 6
Train | Batch (1/196) | Top-1: 63.28 (162/256)
Train | Batch (101/196) | Top-1: 61.63 (15935/25856)
Train | Batch (196/196) | Top-1: 61.95 (30974/50000)
Regular: 2.603670835494995
Epoche: 6; regular: 2.603670835494995: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 61.20
Epoch 7
Train | Batch (1/196) | Top-1: 64.84 (166/256)
Train | Batch (101/196) | Top-1: 64.00 (16547/25856)
Train | Batch (196/196) | Top-1: 63.74 (31869/50000)
Regular: 2.5830936431884766
Epoche: 7; regular: 2.5830936431884766: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 63.16
Epoch 8
Train | Batch (1/196) | Top-1: 64.06 (164/256)
Train | Batch (101/196) | Top-1: 65.50 (16935/25856)
Train | Batch (196/196) | Top-1: 65.51 (32754/50000)
Regular: 2.5625195503234863
Epoche: 8; regular: 2.5625195503234863: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 64.09
Epoch 9
Train | Batch (1/196) | Top-1: 64.45 (165/256)
Train | Batch (101/196) | Top-1: 66.31 (17146/25856)
Train | Batch (196/196) | Top-1: 66.71 (33356/50000)
Regular: 2.5419883728027344
Epoche: 9; regular: 2.5419883728027344: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 64.56
Epoch 10
Train | Batch (1/196) | Top-1: 64.84 (166/256)
Train | Batch (101/196) | Top-1: 67.38 (17421/25856)
Train | Batch (196/196) | Top-1: 67.88 (33939/50000)
Regular: 2.5214455127716064
Epoche: 10; regular: 2.5214455127716064: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 65.52
Epoch 11
Train | Batch (1/196) | Top-1: 68.36 (175/256)
Train | Batch (101/196) | Top-1: 68.73 (17771/25856)
Train | Batch (196/196) | Top-1: 69.07 (34533/50000)
Regular: 2.500908136367798
Epoche: 11; regular: 2.500908136367798: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 67.62
Epoch 12
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 69.38 (17938/25856)
Train | Batch (196/196) | Top-1: 69.83 (34916/50000)
Regular: 2.4804272651672363
Epoche: 12; regular: 2.4804272651672363: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 67.40
Epoch 13
Train | Batch (1/196) | Top-1: 67.97 (174/256)
Train | Batch (101/196) | Top-1: 70.71 (18284/25856)
Train | Batch (196/196) | Top-1: 70.98 (35490/50000)
Regular: 2.4599242210388184
Epoche: 13; regular: 2.4599242210388184: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 68.38
Epoch 14
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 71.67 (18531/25856)
Train | Batch (196/196) | Top-1: 71.77 (35886/50000)
Regular: 2.4394350051879883
Epoche: 14; regular: 2.4394350051879883: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 68.92
Epoch 15
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 72.31 (18696/25856)
Train | Batch (196/196) | Top-1: 72.54 (36269/50000)
Regular: 2.418973445892334
Epoche: 15; regular: 2.418973445892334: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 68.16
Epoch 16
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 72.67 (18790/25856)
Train | Batch (196/196) | Top-1: 72.96 (36478/50000)
Regular: 2.3985419273376465
Epoche: 16; regular: 2.3985419273376465: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 70.28
Epoch 17
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 73.69 (19053/25856)
Train | Batch (196/196) | Top-1: 73.88 (36940/50000)
Regular: 2.37813401222229
Epoche: 17; regular: 2.37813401222229: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 70.08
Epoch 18
Train | Batch (1/196) | Top-1: 72.66 (186/256)
Train | Batch (101/196) | Top-1: 74.48 (19257/25856)
Train | Batch (196/196) | Top-1: 74.52 (37258/50000)
Regular: 2.3577239513397217
Epoche: 18; regular: 2.3577239513397217: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.23
Epoch 19
Train | Batch (1/196) | Top-1: 75.78 (194/256)
Train | Batch (101/196) | Top-1: 75.34 (19481/25856)
Train | Batch (196/196) | Top-1: 75.37 (37684/50000)
Regular: 2.33732533454895
Epoche: 19; regular: 2.33732533454895: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.94
Epoch 20
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 75.48 (19515/25856)
Train | Batch (196/196) | Top-1: 75.84 (37920/50000)
Regular: 2.316969871520996
Epoche: 20; regular: 2.316969871520996: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.24
Epoch 21
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 76.56 (19795/25856)
Train | Batch (196/196) | Top-1: 76.48 (38239/50000)
Regular: 2.296628952026367
Epoche: 21; regular: 2.296628952026367: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.39
Epoch 22
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 77.00 (19909/25856)
Train | Batch (196/196) | Top-1: 77.05 (38523/50000)
Regular: 2.2762997150421143
Epoche: 22; regular: 2.2762997150421143: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.28
Epoch 23
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 77.85 (20129/25856)
Train | Batch (196/196) | Top-1: 77.89 (38947/50000)
Regular: 2.255997657775879
Epoche: 23; regular: 2.255997657775879: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.31
Epoch 24
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 78.43 (20280/25856)
Train | Batch (196/196) | Top-1: 78.46 (39231/50000)
Regular: 2.2356812953948975
Epoche: 24; regular: 2.2356812953948975: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.53
Epoch 25
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 78.75 (20361/25856)
Train | Batch (196/196) | Top-1: 78.91 (39456/50000)
Regular: 2.2154130935668945
Epoche: 25; regular: 2.2154130935668945: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.14
Epoch 26
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 79.62 (20586/25856)
Train | Batch (196/196) | Top-1: 79.09 (39547/50000)
Regular: 2.195162057876587
Epoche: 26; regular: 2.195162057876587: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.73
Epoch 27
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 79.54 (20565/25856)
Train | Batch (196/196) | Top-1: 79.56 (39782/50000)
Regular: 2.174940347671509
Epoche: 27; regular: 2.174940347671509: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.05
Epoch 28
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 80.55 (20826/25856)
Train | Batch (196/196) | Top-1: 80.32 (40160/50000)
Regular: 2.154785394668579
Epoche: 28; regular: 2.154785394668579: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.77
Epoch 29
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 80.70 (20865/25856)
Train | Batch (196/196) | Top-1: 80.63 (40317/50000)
Regular: 2.134647846221924
Epoche: 29; regular: 2.134647846221924: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.98
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 1
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 81.25 (21008/25856)
Train | Batch (196/196) | Top-1: 81.13 (40563/50000)
Regular: 2.1153218746185303
Epoche: 0; regular: 2.1153218746185303: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.51
Epoch 1
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 81.43 (21055/25856)
Train | Batch (196/196) | Top-1: 81.39 (40695/50000)
Regular: 2.0952558517456055
Epoche: 1; regular: 2.0952558517456055: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.88
Epoch 2
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 81.71 (21127/25856)
Train | Batch (196/196) | Top-1: 81.63 (40816/50000)
Regular: 2.075162410736084
Epoche: 2; regular: 2.075162410736084: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.74
Epoch 3
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 82.22 (21259/25856)
Train | Batch (196/196) | Top-1: 82.09 (41047/50000)
Regular: 2.055110454559326
Epoche: 3; regular: 2.055110454559326: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.64
Epoch 4
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 82.47 (21323/25856)
Train | Batch (196/196) | Top-1: 82.63 (41317/50000)
Regular: 2.035071849822998
Epoche: 4; regular: 2.035071849822998: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.39
Epoch 5
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 82.85 (21422/25856)
Train | Batch (196/196) | Top-1: 83.23 (41615/50000)
Regular: 2.0150933265686035
Epoche: 5; regular: 2.0150933265686035: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.42
Epoch 6
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 83.10 (21486/25856)
Train | Batch (196/196) | Top-1: 83.32 (41658/50000)
Regular: 1.9951345920562744
Epoche: 6; regular: 1.9951345920562744: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.36
Epoch 7
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 83.62 (21622/25856)
Train | Batch (196/196) | Top-1: 83.52 (41758/50000)
Regular: 1.9751654863357544
Epoche: 7; regular: 1.9751654863357544: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.27
Epoch 8
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.84 (21678/25856)
Train | Batch (196/196) | Top-1: 83.67 (41835/50000)
Regular: 1.9552922248840332
Epoche: 8; regular: 1.9552922248840332: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.63
Epoch 9
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 84.17 (21763/25856)
Train | Batch (196/196) | Top-1: 84.02 (42012/50000)
Regular: 1.9353753328323364
Epoche: 9; regular: 1.9353753328323364: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.03
Epoch 10
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 84.52 (21853/25856)
Train | Batch (196/196) | Top-1: 84.40 (42198/50000)
Regular: 1.9155594110488892
Epoche: 10; regular: 1.9155594110488892: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.63
Epoch 11
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 84.44 (21832/25856)
Train | Batch (196/196) | Top-1: 84.31 (42153/50000)
Regular: 1.895764708518982
Epoche: 11; regular: 1.895764708518982: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.70
Epoch 12
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 84.97 (21969/25856)
Train | Batch (196/196) | Top-1: 84.86 (42431/50000)
Regular: 1.8760665655136108
Epoche: 12; regular: 1.8760665655136108: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.37
Epoch 13
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 85.32 (22060/25856)
Train | Batch (196/196) | Top-1: 84.98 (42488/50000)
Regular: 1.8563635349273682
Epoche: 13; regular: 1.8563635349273682: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.68
Epoch 14
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 85.18 (22024/25856)
Train | Batch (196/196) | Top-1: 85.15 (42575/50000)
Regular: 1.8367077112197876
Epoche: 14; regular: 1.8367077112197876: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.04
Epoch 15
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 85.81 (22186/25856)
Train | Batch (196/196) | Top-1: 85.65 (42823/50000)
Regular: 1.8170583248138428
Epoche: 15; regular: 1.8170583248138428: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.00
Epoch 16
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 86.00 (22237/25856)
Train | Batch (196/196) | Top-1: 85.94 (42968/50000)
Regular: 1.7974414825439453
Epoche: 16; regular: 1.7974414825439453: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.69
Epoch 17
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 85.78 (22180/25856)
Train | Batch (196/196) | Top-1: 85.78 (42891/50000)
Regular: 1.7778466939926147
Epoche: 17; regular: 1.7778466939926147: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.95
Epoch 18
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 86.36 (22329/25856)
Train | Batch (196/196) | Top-1: 86.25 (43127/50000)
Regular: 1.7583726644515991
Epoche: 18; regular: 1.7583726644515991: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.61
Epoch 19
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 86.44 (22351/25856)
Train | Batch (196/196) | Top-1: 86.21 (43105/50000)
Regular: 1.738845705986023
Epoche: 19; regular: 1.738845705986023: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.83
Epoch 20
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 86.69 (22414/25856)
Train | Batch (196/196) | Top-1: 86.61 (43305/50000)
Regular: 1.7193368673324585
Epoche: 20; regular: 1.7193368673324585: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.60
Epoch 21
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 86.86 (22458/25856)
Train | Batch (196/196) | Top-1: 86.71 (43355/50000)
Regular: 1.6999685764312744
Epoche: 21; regular: 1.6999685764312744: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.69
Epoch 22
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 87.09 (22519/25856)
Train | Batch (196/196) | Top-1: 86.92 (43459/50000)
Regular: 1.680671215057373
Epoche: 22; regular: 1.680671215057373: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.73
Epoch 23
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 87.26 (22563/25856)
Train | Batch (196/196) | Top-1: 87.10 (43548/50000)
Regular: 1.6613290309906006
Epoche: 23; regular: 1.6613290309906006: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.73
Epoch 24
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 87.47 (22615/25856)
Train | Batch (196/196) | Top-1: 87.44 (43720/50000)
Regular: 1.6420217752456665
Epoche: 24; regular: 1.6420217752456665: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.15
Epoch 25
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.91 (22731/25856)
Train | Batch (196/196) | Top-1: 87.54 (43768/50000)
Regular: 1.6228954792022705
Epoche: 25; regular: 1.6228954792022705: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.61
Epoch 26
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 87.94 (22737/25856)
Train | Batch (196/196) | Top-1: 87.62 (43812/50000)
Regular: 1.6036996841430664
Epoche: 26; regular: 1.6036996841430664: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.95
Epoch 27
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 88.28 (22826/25856)
Train | Batch (196/196) | Top-1: 87.98 (43989/50000)
Regular: 1.5846177339553833
Epoche: 27; regular: 1.5846177339553833: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.63
Epoch 28
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 87.93 (22735/25856)
Train | Batch (196/196) | Top-1: 87.96 (43978/50000)
Regular: 1.5655617713928223
Epoche: 28; regular: 1.5655617713928223: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.40
Epoch 29
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 88.23 (22814/25856)
Train | Batch (196/196) | Top-1: 88.10 (44050/50000)
Regular: 1.5466266870498657
Epoche: 29; regular: 1.5466266870498657: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.69
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 2
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 88.26 (22821/25856)
Train | Batch (196/196) | Top-1: 88.33 (44167/50000)
Regular: 1.5284647941589355
Epoche: 0; regular: 1.5284647941589355: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.95
Epoch 1
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 88.76 (22951/25856)
Train | Batch (196/196) | Top-1: 88.59 (44296/50000)
Regular: 1.509719729423523
Epoche: 1; regular: 1.509719729423523: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.39
Epoch 2
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 88.72 (22939/25856)
Train | Batch (196/196) | Top-1: 88.59 (44296/50000)
Regular: 1.4909698963165283
Epoche: 2; regular: 1.4909698963165283: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.99
Epoch 3
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 89.00 (23013/25856)
Train | Batch (196/196) | Top-1: 88.71 (44353/50000)
Regular: 1.4723211526870728
Epoche: 3; regular: 1.4723211526870728: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.21
Epoch 4
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 88.60 (22909/25856)
Train | Batch (196/196) | Top-1: 88.73 (44365/50000)
Regular: 1.453697681427002
Epoche: 4; regular: 1.453697681427002: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.05
Epoch 5
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 88.72 (22939/25856)
Train | Batch (196/196) | Top-1: 89.01 (44506/50000)
Regular: 1.4352285861968994
Epoche: 5; regular: 1.4352285861968994: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.10
Epoch 6
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 89.06 (23028/25856)
Train | Batch (196/196) | Top-1: 89.08 (44538/50000)
Regular: 1.4168487787246704
Epoche: 6; regular: 1.4168487787246704: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.88
Epoch 7
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 89.24 (23075/25856)
Train | Batch (196/196) | Top-1: 89.19 (44594/50000)
Regular: 1.3986862897872925
Epoche: 7; regular: 1.3986862897872925: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.90
Epoch 8
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 89.41 (23117/25856)
Train | Batch (196/196) | Top-1: 89.40 (44700/50000)
Regular: 1.3805043697357178
Epoche: 8; regular: 1.3805043697357178: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.35
Epoch 9
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 89.69 (23190/25856)
Train | Batch (196/196) | Top-1: 89.41 (44704/50000)
Regular: 1.3624327182769775
Epoche: 9; regular: 1.3624327182769775: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.34
Epoch 10
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 89.53 (23149/25856)
Train | Batch (196/196) | Top-1: 89.48 (44740/50000)
Regular: 1.3444453477859497
Epoche: 10; regular: 1.3444453477859497: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.82
Epoch 11
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 89.56 (23156/25856)
Train | Batch (196/196) | Top-1: 89.48 (44739/50000)
Regular: 1.326735019683838
Epoche: 11; regular: 1.326735019683838: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.83
Epoch 12
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 90.00 (23271/25856)
Train | Batch (196/196) | Top-1: 89.88 (44941/50000)
Regular: 1.3092824220657349
Epoche: 12; regular: 1.3092824220657349: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.77
Epoch 13
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 89.79 (23215/25856)
Train | Batch (196/196) | Top-1: 89.83 (44915/50000)
Regular: 1.2918404340744019
Epoche: 13; regular: 1.2918404340744019: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.58
Epoch 14
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 90.10 (23296/25856)
Train | Batch (196/196) | Top-1: 89.87 (44935/50000)
Regular: 1.274840235710144
Epoche: 14; regular: 1.274840235710144: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.74
Epoch 15
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 90.39 (23371/25856)
Train | Batch (196/196) | Top-1: 90.30 (45150/50000)
Regular: 1.2580583095550537
Epoche: 15; regular: 1.2580583095550537: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.63
Epoch 16
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 89.80 (23218/25856)
Train | Batch (196/196) | Top-1: 89.84 (44922/50000)
Regular: 1.2416478395462036
Epoche: 16; regular: 1.2416478395462036: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.27
Epoch 17
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 90.19 (23320/25856)
Train | Batch (196/196) | Top-1: 90.33 (45165/50000)
Regular: 1.2256956100463867
Epoche: 17; regular: 1.2256956100463867: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.43
Epoch 18
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 90.56 (23415/25856)
Train | Batch (196/196) | Top-1: 90.28 (45139/50000)
Regular: 1.2100205421447754
Epoche: 18; regular: 1.2100205421447754: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.58
Epoch 19
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 90.77 (23469/25856)
Train | Batch (196/196) | Top-1: 90.54 (45270/50000)
Regular: 1.1949827671051025
Epoche: 19; regular: 1.1949827671051025: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.87
Epoch 20
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 90.56 (23414/25856)
Train | Batch (196/196) | Top-1: 90.32 (45159/50000)
Regular: 1.1809147596359253
Epoche: 20; regular: 1.1809147596359253: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.11
Epoch 21
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 90.62 (23430/25856)
Train | Batch (196/196) | Top-1: 90.41 (45204/50000)
Regular: 1.1674716472625732
Epoche: 21; regular: 1.1674716472625732: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.49
Epoch 22
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 90.74 (23463/25856)
Train | Batch (196/196) | Top-1: 90.43 (45215/50000)
Regular: 1.154667854309082
Epoche: 22; regular: 1.154667854309082: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.51
Epoch 23
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 90.75 (23465/25856)
Train | Batch (196/196) | Top-1: 90.57 (45285/50000)
Regular: 1.1424362659454346
Epoche: 23; regular: 1.1424362659454346: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.54
Epoch 24
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 90.97 (23520/25856)
Train | Batch (196/196) | Top-1: 90.70 (45351/50000)
Regular: 1.1303519010543823
Epoche: 24; regular: 1.1303519010543823: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.96
Epoch 25
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 91.10 (23555/25856)
Train | Batch (196/196) | Top-1: 90.95 (45473/50000)
Regular: 1.118644118309021
Epoche: 25; regular: 1.118644118309021: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.66
Epoch 26
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 90.83 (23484/25856)
Train | Batch (196/196) | Top-1: 90.72 (45360/50000)
Regular: 1.1073039770126343
Epoche: 26; regular: 1.1073039770126343: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.10
Epoch 27
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 91.15 (23568/25856)
Train | Batch (196/196) | Top-1: 91.02 (45510/50000)
Regular: 1.0959020853042603
Epoche: 27; regular: 1.0959020853042603: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.87
Epoch 28
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 90.89 (23501/25856)
Train | Batch (196/196) | Top-1: 90.83 (45413/50000)
Regular: 1.0845521688461304
Epoche: 28; regular: 1.0845521688461304: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.71
Epoch 29
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 91.14 (23564/25856)
Train | Batch (196/196) | Top-1: 91.05 (45526/50000)
Regular: 1.0735493898391724
Epoche: 29; regular: 1.0735493898391724: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.12
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 3
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 91.61 (23687/25856)
Train | Batch (196/196) | Top-1: 91.38 (45690/50000)
Regular: 1.0627092123031616
Epoche: 0; regular: 1.0627092123031616: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.02
Epoch 1
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 91.41 (23634/25856)
Train | Batch (196/196) | Top-1: 91.30 (45648/50000)
Regular: 1.051992416381836
Epoche: 1; regular: 1.051992416381836: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.05
Epoch 2
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 91.50 (23658/25856)
Train | Batch (196/196) | Top-1: 91.43 (45715/50000)
Regular: 1.0414235591888428
Epoche: 2; regular: 1.0414235591888428: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.89
Epoch 3
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 91.44 (23644/25856)
Train | Batch (196/196) | Top-1: 91.47 (45736/50000)
Regular: 1.0308756828308105
Epoche: 3; regular: 1.0308756828308105: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.94
Epoch 4
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.59 (23681/25856)
Train | Batch (196/196) | Top-1: 91.63 (45816/50000)
Regular: 1.020636796951294
Epoche: 4; regular: 1.020636796951294: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.42
Epoch 5
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 91.76 (23726/25856)
Train | Batch (196/196) | Top-1: 91.60 (45801/50000)
Regular: 1.0104869604110718
Epoche: 5; regular: 1.0104869604110718: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.57
Epoch 6
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 91.75 (23723/25856)
Train | Batch (196/196) | Top-1: 91.53 (45764/50000)
Regular: 1.0003788471221924
Epoche: 6; regular: 1.0003788471221924: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.58
Epoch 7
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 91.45 (23646/25856)
Train | Batch (196/196) | Top-1: 91.70 (45851/50000)
Regular: 0.99052894115448
Epoche: 7; regular: 0.99052894115448: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.76
Epoch 8
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 91.87 (23754/25856)
Train | Batch (196/196) | Top-1: 91.76 (45881/50000)
Regular: 0.980491042137146
Epoche: 8; regular: 0.980491042137146: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.96
Epoch 9
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 91.99 (23784/25856)
Train | Batch (196/196) | Top-1: 91.86 (45932/50000)
Regular: 0.9706891775131226
Epoche: 9; regular: 0.9706891775131226: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.41
Epoch 10
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 92.26 (23855/25856)
Train | Batch (196/196) | Top-1: 92.11 (46054/50000)
Regular: 0.9610382318496704
Epoche: 10; regular: 0.9610382318496704: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.24
Epoch 11
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 91.85 (23749/25856)
Train | Batch (196/196) | Top-1: 91.91 (45957/50000)
Regular: 0.9514480829238892
Epoche: 11; regular: 0.9514480829238892: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.72
Epoch 12
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 92.24 (23850/25856)
Train | Batch (196/196) | Top-1: 92.11 (46057/50000)
Regular: 0.9420653581619263
Epoche: 12; regular: 0.9420653581619263: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.47
Epoch 13
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 92.02 (23793/25856)
Train | Batch (196/196) | Top-1: 92.01 (46007/50000)
Regular: 0.9330858588218689
Epoche: 13; regular: 0.9330858588218689: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.80
Epoch 14
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 92.07 (23805/25856)
Train | Batch (196/196) | Top-1: 92.05 (46026/50000)
Regular: 0.9241951107978821
Epoche: 14; regular: 0.9241951107978821: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.98
Epoch 15
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 92.33 (23873/25856)
Train | Batch (196/196) | Top-1: 92.44 (46218/50000)
Regular: 0.9149235486984253
Epoche: 15; regular: 0.9149235486984253: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.72
Epoch 16
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 92.21 (23842/25856)
Train | Batch (196/196) | Top-1: 92.04 (46020/50000)
Regular: 0.9058791399002075
Epoche: 16; regular: 0.9058791399002075: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.09
Epoch 17
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 92.32 (23871/25856)
Train | Batch (196/196) | Top-1: 92.15 (46076/50000)
Regular: 0.8972666263580322
Epoche: 17; regular: 0.8972666263580322: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.42
Epoch 18
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 92.38 (23887/25856)
Train | Batch (196/196) | Top-1: 92.44 (46219/50000)
Regular: 0.8884643316268921
Epoche: 18; regular: 0.8884643316268921: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.16
Epoch 19
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 92.42 (23895/25856)
Train | Batch (196/196) | Top-1: 92.52 (46261/50000)
Regular: 0.87989342212677
Epoche: 19; regular: 0.87989342212677: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.61
Epoch 20
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 92.81 (23998/25856)
Train | Batch (196/196) | Top-1: 92.53 (46267/50000)
Regular: 0.8711946606636047
Epoche: 20; regular: 0.8711946606636047: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.07
Epoch 21
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 92.73 (23976/25856)
Train | Batch (196/196) | Top-1: 92.64 (46322/50000)
Regular: 0.8627897500991821
Epoche: 21; regular: 0.8627897500991821: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.04
Epoch 22
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 92.70 (23968/25856)
Train | Batch (196/196) | Top-1: 92.49 (46245/50000)
Regular: 0.8549540042877197
Epoche: 22; regular: 0.8549540042877197: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.03
Epoch 23
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 93.12 (24077/25856)
Train | Batch (196/196) | Top-1: 92.95 (46475/50000)
Regular: 0.8466398119926453
Epoche: 23; regular: 0.8466398119926453: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.77
Epoch 24
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 93.09 (24069/25856)
Train | Batch (196/196) | Top-1: 92.99 (46494/50000)
Regular: 0.8381660580635071
Epoche: 24; regular: 0.8381660580635071: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.55
Epoch 25
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 92.59 (23940/25856)
Train | Batch (196/196) | Top-1: 92.61 (46303/50000)
Regular: 0.8298683762550354
Epoche: 25; regular: 0.8298683762550354: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.93
Epoch 26
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 93.10 (24073/25856)
Train | Batch (196/196) | Top-1: 92.97 (46483/50000)
Regular: 0.8217455744743347
Epoche: 26; regular: 0.8217455744743347: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.35
Epoch 27
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 93.43 (24158/25856)
Train | Batch (196/196) | Top-1: 93.23 (46613/50000)
Regular: 0.8133152723312378
Epoche: 27; regular: 0.8133152723312378: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.81
Epoch 28
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 93.05 (24058/25856)
Train | Batch (196/196) | Top-1: 93.04 (46519/50000)
Regular: 0.804936408996582
Epoche: 28; regular: 0.804936408996582: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.00
Epoch 29
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 93.31 (24127/25856)
Train | Batch (196/196) | Top-1: 93.04 (46522/50000)
Regular: 0.7966766357421875
Epoche: 29; regular: 0.7966766357421875: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.41
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 4
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 93.54 (24185/25856)
Train | Batch (196/196) | Top-1: 93.30 (46648/50000)
Regular: 0.7889901995658875
Epoche: 0; regular: 0.7889901995658875: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.48
Epoch 1
Train | Batch (1/196) | Top-1: 96.09 (246/256)
Train | Batch (101/196) | Top-1: 93.33 (24132/25856)
Train | Batch (196/196) | Top-1: 93.32 (46659/50000)
Regular: 0.7810091376304626
Epoche: 1; regular: 0.7810091376304626: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.79
Epoch 2
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 93.41 (24151/25856)
Train | Batch (196/196) | Top-1: 93.33 (46664/50000)
Regular: 0.7731542587280273
Epoche: 2; regular: 0.7731542587280273: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.93
Epoch 3
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 93.49 (24174/25856)
Train | Batch (196/196) | Top-1: 93.40 (46698/50000)
Regular: 0.765135645866394
Epoche: 3; regular: 0.765135645866394: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.57
Epoch 4
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 93.41 (24151/25856)
Train | Batch (196/196) | Top-1: 93.30 (46651/50000)
Regular: 0.7573918104171753
Epoche: 4; regular: 0.7573918104171753: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.08
Epoch 5
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 93.48 (24170/25856)
Train | Batch (196/196) | Top-1: 93.31 (46653/50000)
Regular: 0.7499772906303406
Epoche: 5; regular: 0.7499772906303406: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.22
Epoch 6
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 93.73 (24236/25856)
Train | Batch (196/196) | Top-1: 93.48 (46739/50000)
Regular: 0.7423187494277954
Epoche: 6; regular: 0.7423187494277954: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.88
Epoch 7
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 93.59 (24198/25856)
Train | Batch (196/196) | Top-1: 93.43 (46713/50000)
Regular: 0.7340871095657349
Epoche: 7; regular: 0.7340871095657349: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.58
Epoch 8
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 93.63 (24210/25856)
Train | Batch (196/196) | Top-1: 93.46 (46732/50000)
Regular: 0.7261835336685181
Epoche: 8; regular: 0.7261835336685181: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.96
Epoch 9
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 93.78 (24248/25856)
Train | Batch (196/196) | Top-1: 93.58 (46792/50000)
Regular: 0.7186232805252075
Epoche: 9; regular: 0.7186232805252075: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.42
Epoch 10
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 94.02 (24310/25856)
Train | Batch (196/196) | Top-1: 93.73 (46863/50000)
Regular: 0.7108888626098633
Epoche: 10; regular: 0.7108888626098633: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.13
Epoch 11
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 93.89 (24275/25856)
Train | Batch (196/196) | Top-1: 93.78 (46891/50000)
Regular: 0.7033221125602722
Epoche: 11; regular: 0.7033221125602722: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.60
Epoch 12
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 93.78 (24249/25856)
Train | Batch (196/196) | Top-1: 93.67 (46836/50000)
Regular: 0.6955347061157227
Epoche: 12; regular: 0.6955347061157227: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.60
Epoch 13
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 94.19 (24353/25856)
Train | Batch (196/196) | Top-1: 93.91 (46954/50000)
Regular: 0.6879374384880066
Epoche: 13; regular: 0.6879374384880066: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.55
Epoch 14
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 94.09 (24327/25856)
Train | Batch (196/196) | Top-1: 93.91 (46954/50000)
Regular: 0.6807383298873901
Epoche: 14; regular: 0.6807383298873901: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.95
Epoch 15
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 93.83 (24261/25856)
Train | Batch (196/196) | Top-1: 93.63 (46816/50000)
Regular: 0.6735042929649353
Epoche: 15; regular: 0.6735042929649353: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.57
Epoch 16
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 94.19 (24354/25856)
Train | Batch (196/196) | Top-1: 94.01 (47004/50000)
Regular: 0.6665171384811401
Epoche: 16; regular: 0.6665171384811401: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.97
Epoch 17
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 93.87 (24272/25856)
Train | Batch (196/196) | Top-1: 93.82 (46910/50000)
Regular: 0.6593437790870667
Epoche: 17; regular: 0.6593437790870667: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.50
Epoch 18
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 94.23 (24365/25856)
Train | Batch (196/196) | Top-1: 94.06 (47032/50000)
Regular: 0.6520494222640991
Epoche: 18; regular: 0.6520494222640991: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.38
Epoch 19
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 93.99 (24303/25856)
Train | Batch (196/196) | Top-1: 93.99 (46994/50000)
Regular: 0.6450208425521851
Epoche: 19; regular: 0.6450208425521851: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.43
Epoch 20
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 93.90 (24280/25856)
Train | Batch (196/196) | Top-1: 93.87 (46936/50000)
Regular: 0.6381330490112305
Epoche: 20; regular: 0.6381330490112305: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.62
Epoch 21
Train | Batch (1/196) | Top-1: 96.09 (246/256)
Train | Batch (101/196) | Top-1: 94.15 (24344/25856)
Train | Batch (196/196) | Top-1: 94.13 (47064/50000)
Regular: 0.6311948299407959
Epoche: 21; regular: 0.6311948299407959: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.73
Epoch 22
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 94.01 (24307/25856)
Train | Batch (196/196) | Top-1: 93.90 (46948/50000)
Regular: 0.624508798122406
Epoche: 22; regular: 0.624508798122406: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.27
Epoch 23
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 93.85 (24267/25856)
Train | Batch (196/196) | Top-1: 93.99 (46993/50000)
Regular: 0.6181116104125977
Epoche: 23; regular: 0.6181116104125977: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.43
Epoch 24
Train | Batch (1/196) | Top-1: 97.27 (249/256)
Train | Batch (101/196) | Top-1: 94.08 (24326/25856)
Train | Batch (196/196) | Top-1: 94.10 (47052/50000)
Regular: 0.6113467216491699
Epoche: 24; regular: 0.6113467216491699: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.31
Epoch 25
Train | Batch (1/196) | Top-1: 96.09 (246/256)
Train | Batch (101/196) | Top-1: 94.31 (24386/25856)
Train | Batch (196/196) | Top-1: 94.30 (47151/50000)
Regular: 0.6043121814727783
Epoche: 25; regular: 0.6043121814727783: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.21
Epoch 26
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 94.48 (24428/25856)
Train | Batch (196/196) | Top-1: 94.21 (47104/50000)
Regular: 0.5977914929389954
Epoche: 26; regular: 0.5977914929389954: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.70
Epoch 27
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 94.18 (24350/25856)
Train | Batch (196/196) | Top-1: 94.12 (47058/50000)
Regular: 0.5918750166893005
Epoche: 27; regular: 0.5918750166893005: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.77
Epoch 28
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 94.56 (24450/25856)
Train | Batch (196/196) | Top-1: 94.30 (47150/50000)
Regular: 0.5856028199195862
Epoche: 28; regular: 0.5856028199195862: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.79
Epoch 29
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 93.54 (24186/25856)
Train | Batch (196/196) | Top-1: 93.70 (46850/50000)
Regular: 0.5800063014030457
Epoche: 29; regular: 0.5800063014030457: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.31
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 5
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 94.31 (24386/25856)
Train | Batch (196/196) | Top-1: 94.25 (47125/50000)
Regular: 0.5740470886230469
Epoche: 0; regular: 0.5740470886230469: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.21
Epoch 1
Train | Batch (1/196) | Top-1: 96.09 (246/256)
Train | Batch (101/196) | Top-1: 94.16 (24347/25856)
Train | Batch (196/196) | Top-1: 94.19 (47093/50000)
Regular: 0.5684242248535156
Epoche: 1; regular: 0.5684242248535156: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.30
Epoch 2
Train | Batch (1/196) | Top-1: 95.70 (245/256)
Train | Batch (101/196) | Top-1: 93.91 (24282/25856)
Train | Batch (196/196) | Top-1: 94.04 (47019/50000)
Regular: 0.563490629196167
Epoche: 2; regular: 0.563490629196167: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.33
Epoch 3
Train | Batch (1/196) | Top-1: 96.48 (247/256)
Train | Batch (101/196) | Top-1: 94.39 (24405/25856)
Train | Batch (196/196) | Top-1: 94.34 (47168/50000)
Regular: 0.55828458070755
Epoche: 3; regular: 0.55828458070755: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.26
Epoch 4
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 94.19 (24355/25856)
Train | Batch (196/196) | Top-1: 94.34 (47170/50000)
Regular: 0.5529261827468872
Epoche: 4; regular: 0.5529261827468872: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.95
Epoch 5
Train | Batch (1/196) | Top-1: 97.27 (249/256)
Train | Batch (101/196) | Top-1: 94.24 (24367/25856)
Train | Batch (196/196) | Top-1: 94.04 (47018/50000)
Regular: 0.5482028126716614
Epoche: 5; regular: 0.5482028126716614: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.94
Epoch 6
Train | Batch (1/196) | Top-1: 96.88 (248/256)
Train | Batch (101/196) | Top-1: 94.48 (24430/25856)
Train | Batch (196/196) | Top-1: 94.49 (47245/50000)
Regular: 0.5437670350074768
Epoche: 6; regular: 0.5437670350074768: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.16
Epoch 7
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 94.43 (24416/25856)
Train | Batch (196/196) | Top-1: 94.19 (47093/50000)
Regular: 0.538831353187561
Epoche: 7; regular: 0.538831353187561: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.89
Epoch 8
Train | Batch (1/196) | Top-1: 96.48 (247/256)
Train | Batch (101/196) | Top-1: 94.44 (24419/25856)
Train | Batch (196/196) | Top-1: 94.31 (47155/50000)
Regular: 0.5344772338867188
Epoche: 8; regular: 0.5344772338867188: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.99
Epoch 9
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 94.69 (24484/25856)
Train | Batch (196/196) | Top-1: 94.41 (47206/50000)
Regular: 0.5297905206680298
Epoche: 9; regular: 0.5297905206680298: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.42
Epoch 10
Train | Batch (1/196) | Top-1: 96.09 (246/256)
Train | Batch (101/196) | Top-1: 94.59 (24457/25856)
Train | Batch (196/196) | Top-1: 94.40 (47198/50000)
Regular: 0.5253527760505676
Epoche: 10; regular: 0.5253527760505676: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.01
Epoch 11
Train | Batch (1/196) | Top-1: 97.27 (249/256)
Train | Batch (101/196) | Top-1: 94.22 (24361/25856)
Train | Batch (196/196) | Top-1: 94.35 (47174/50000)
Regular: 0.5213061571121216
Epoche: 11; regular: 0.5213061571121216: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.54
Epoch 12
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 94.57 (24451/25856)
Train | Batch (196/196) | Top-1: 94.49 (47244/50000)
Regular: 0.5172648429870605
Epoche: 12; regular: 0.5172648429870605: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.98
Epoch 13
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 94.52 (24440/25856)
Train | Batch (196/196) | Top-1: 94.48 (47239/50000)
Regular: 0.5129450559616089
Epoche: 13; regular: 0.5129450559616089: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.91
Epoch 14
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 94.46 (24423/25856)
Train | Batch (196/196) | Top-1: 94.34 (47170/50000)
Regular: 0.5092722177505493
Epoche: 14; regular: 0.5092722177505493: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.25
Epoch 15
Train | Batch (1/196) | Top-1: 95.70 (245/256)
Train | Batch (101/196) | Top-1: 94.75 (24499/25856)
Train | Batch (196/196) | Top-1: 94.61 (47303/50000)
Regular: 0.5052585601806641
Epoche: 15; regular: 0.5052585601806641: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.66
Epoch 16
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 94.42 (24414/25856)
Train | Batch (196/196) | Top-1: 94.53 (47265/50000)
Regular: 0.5015881061553955
Epoche: 16; regular: 0.5015881061553955: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.33
Epoch 17
Train | Batch (1/196) | Top-1: 95.70 (245/256)
Train | Batch (101/196) | Top-1: 94.41 (24410/25856)
Train | Batch (196/196) | Top-1: 94.41 (47205/50000)
Regular: 0.49765607714653015
Epoche: 17; regular: 0.49765607714653015: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.77
Epoch 18
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 94.77 (24505/25856)
Train | Batch (196/196) | Top-1: 94.59 (47294/50000)
Regular: 0.4937291741371155
Epoche: 18; regular: 0.4937291741371155: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.38
Epoch 19
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 94.82 (24516/25856)
Train | Batch (196/196) | Top-1: 94.56 (47279/50000)
Regular: 0.49024584889411926
Epoche: 19; regular: 0.49024584889411926: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.02
Epoch 20
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 94.73 (24494/25856)
Train | Batch (196/196) | Top-1: 94.60 (47300/50000)
Regular: 0.48677611351013184
Epoche: 20; regular: 0.48677611351013184: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.41
Epoch 21
Train | Batch (1/196) | Top-1: 96.09 (246/256)
Train | Batch (101/196) | Top-1: 94.60 (24460/25856)
Train | Batch (196/196) | Top-1: 94.61 (47304/50000)
Regular: 0.48379188776016235
Epoche: 21; regular: 0.48379188776016235: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.80
Epoch 22
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 95.12 (24595/25856)
Train | Batch (196/196) | Top-1: 94.85 (47425/50000)
Regular: 0.4795319736003876
Epoche: 22; regular: 0.4795319736003876: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.19
Epoch 23
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 94.42 (24414/25856)
Train | Batch (196/196) | Top-1: 94.55 (47274/50000)
Regular: 0.47607025504112244
Epoche: 23; regular: 0.47607025504112244: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.95
Epoch 24
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 94.57 (24452/25856)
Train | Batch (196/196) | Top-1: 94.52 (47261/50000)
Regular: 0.4726126194000244
Epoche: 24; regular: 0.4726126194000244: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.96
Epoch 25
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 94.72 (24491/25856)
Train | Batch (196/196) | Top-1: 94.83 (47416/50000)
Regular: 0.46944597363471985
Epoche: 25; regular: 0.46944597363471985: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.88
Epoch 26
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 94.73 (24494/25856)
Train | Batch (196/196) | Top-1: 94.76 (47382/50000)
Regular: 0.46567583084106445
Epoche: 26; regular: 0.46567583084106445: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.22
Epoch 27
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 95.12 (24595/25856)
Train | Batch (196/196) | Top-1: 94.85 (47423/50000)
Regular: 0.4619785249233246
Epoche: 27; regular: 0.4619785249233246: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.04
Epoch 28
Train | Batch (1/196) | Top-1: 96.88 (248/256)
Train | Batch (101/196) | Top-1: 94.65 (24473/25856)
Train | Batch (196/196) | Top-1: 94.70 (47352/50000)
Regular: 0.4588718116283417
Epoche: 28; regular: 0.4588718116283417: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.06
Epoch 29
Train | Batch (1/196) | Top-1: 96.48 (247/256)
Train | Batch (101/196) | Top-1: 95.21 (24617/25856)
Train | Batch (196/196) | Top-1: 95.09 (47546/50000)
Regular: 0.45513442158699036
Epoche: 29; regular: 0.45513442158699036: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.87
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
Train | Batch (1/196) | Top-1: 95.70 (245/256)
Train | Batch (101/196) | Top-1: 26.85 (6942/25856)
Train | Batch (196/196) | Top-1: 31.77 (15886/50000)
Regular: nan
Epoche: 0; regular: nan: flops 68862592
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
#Filters: 1136, #FLOPs: 68.86M | Top-1: 31.44
Epoch 0 | Top-1: 31.44
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 43.84 (11334/25856)
Train | Batch (196/196) | Top-1: 46.82 (23412/50000)
Regular: nan
Epoche: 1; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 42.14
Epoch 1 | Top-1: 42.14
Train | Batch (1/196) | Top-1: 52.73 (135/256)
Train | Batch (101/196) | Top-1: 56.17 (14523/25856)
Train | Batch (196/196) | Top-1: 58.17 (29083/50000)
Regular: nan
Epoche: 2; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 51.46
Epoch 2 | Top-1: 51.46
Train | Batch (1/196) | Top-1: 63.67 (163/256)
Train | Batch (101/196) | Top-1: 64.30 (16625/25856)
Train | Batch (196/196) | Top-1: 65.96 (32981/50000)
Regular: nan
Epoche: 3; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 60.75
Epoch 3 | Top-1: 60.75
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 70.48 (18223/25856)
Train | Batch (196/196) | Top-1: 71.61 (35804/50000)
Regular: nan
Epoche: 4; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 66.80
Epoch 4 | Top-1: 66.80
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 74.56 (19277/25856)
Train | Batch (196/196) | Top-1: 75.12 (37558/50000)
Regular: nan
Epoche: 5; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 66.34
Epoch 5 | Top-1: 66.34
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 77.46 (20029/25856)
Train | Batch (196/196) | Top-1: 77.79 (38894/50000)
Regular: nan
Epoche: 6; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 70.89
Epoch 6 | Top-1: 70.89
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 79.02 (20432/25856)
Train | Batch (196/196) | Top-1: 79.49 (39743/50000)
Regular: nan
Epoche: 7; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 69.61
Epoch 7 | Top-1: 69.61
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.57 (20833/25856)
Train | Batch (196/196) | Top-1: 80.79 (40397/50000)
Regular: nan
Epoche: 8; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.67
Epoch 8 | Top-1: 74.67
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 81.93 (21183/25856)
Train | Batch (196/196) | Top-1: 81.82 (40909/50000)
Regular: nan
Epoche: 9; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.60
Epoch 9 | Top-1: 73.60
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 82.56 (21348/25856)
Train | Batch (196/196) | Top-1: 82.51 (41255/50000)
Regular: nan
Epoche: 10; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.43
Epoch 10 | Top-1: 74.43
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 82.50 (21330/25856)
Train | Batch (196/196) | Top-1: 82.63 (41316/50000)
Regular: nan
Epoche: 11; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.44
Epoch 11 | Top-1: 76.44
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 84.34 (21807/25856)
Train | Batch (196/196) | Top-1: 83.80 (41898/50000)
Regular: nan
Epoche: 12; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.75
Epoch 12 | Top-1: 72.75
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.94 (21703/25856)
Train | Batch (196/196) | Top-1: 83.93 (41965/50000)
Regular: nan
Epoche: 13; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.26
Epoch 13 | Top-1: 79.26
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 84.40 (21823/25856)
Train | Batch (196/196) | Top-1: 84.41 (42207/50000)
Regular: nan
Epoche: 14; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.06
Epoch 14 | Top-1: 76.06
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 84.80 (21927/25856)
Train | Batch (196/196) | Top-1: 84.90 (42451/50000)
Regular: nan
Epoche: 15; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.19
Epoch 15 | Top-1: 79.19
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 85.21 (22031/25856)
Train | Batch (196/196) | Top-1: 85.16 (42580/50000)
Regular: nan
Epoche: 16; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.35
Epoch 16 | Top-1: 81.35
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 85.79 (22182/25856)
Train | Batch (196/196) | Top-1: 85.66 (42831/50000)
Regular: nan
Epoche: 17; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.09
Epoch 17 | Top-1: 75.09
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 85.86 (22200/25856)
Train | Batch (196/196) | Top-1: 85.76 (42881/50000)
Regular: nan
Epoche: 18; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.07
Epoch 18 | Top-1: 75.07
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 86.21 (22291/25856)
Train | Batch (196/196) | Top-1: 86.27 (43134/50000)
Regular: nan
Epoche: 19; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.50
Epoch 19 | Top-1: 76.50
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 86.76 (22433/25856)
Train | Batch (196/196) | Top-1: 86.44 (43222/50000)
Regular: nan
Epoche: 20; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.21
Epoch 20 | Top-1: 76.21
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 86.95 (22482/25856)
Train | Batch (196/196) | Top-1: 86.55 (43275/50000)
Regular: nan
Epoche: 21; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.50
Epoch 21 | Top-1: 74.50
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 87.06 (22510/25856)
Train | Batch (196/196) | Top-1: 86.87 (43433/50000)
Regular: nan
Epoche: 22; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.41
Epoch 22 | Top-1: 72.41
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 87.24 (22557/25856)
Train | Batch (196/196) | Top-1: 86.91 (43455/50000)
Regular: nan
Epoche: 23; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.20
Epoch 23 | Top-1: 78.20
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 87.45 (22611/25856)
Train | Batch (196/196) | Top-1: 87.32 (43662/50000)
Regular: nan
Epoche: 24; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.69
Epoch 24 | Top-1: 76.69
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 87.35 (22584/25856)
Train | Batch (196/196) | Top-1: 87.32 (43660/50000)
Regular: nan
Epoche: 25; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.09
Epoch 25 | Top-1: 84.09
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.51 (22627/25856)
Train | Batch (196/196) | Top-1: 87.37 (43685/50000)
Regular: nan
Epoche: 26; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.42
Epoch 26 | Top-1: 77.42
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 87.88 (22722/25856)
Train | Batch (196/196) | Top-1: 87.79 (43894/50000)
Regular: nan
Epoche: 27; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.10
Epoch 27 | Top-1: 76.10
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 87.90 (22727/25856)
Train | Batch (196/196) | Top-1: 87.69 (43845/50000)
Regular: nan
Epoche: 28; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 71.59
Epoch 28 | Top-1: 71.59
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 88.13 (22786/25856)
Train | Batch (196/196) | Top-1: 87.95 (43975/50000)
Regular: nan
Epoche: 29; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.33
Epoch 29 | Top-1: 79.33
model: ResNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (4): AvgPool2d(kernel_size=8, stride=8, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=10, bias=True)
  )
)
Test acc: 79.33
