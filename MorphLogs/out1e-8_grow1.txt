no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=1e-08, logger='MorphLogs/logMorphNetFlops1e-8_grow1.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 24.45 (6321/25856)
Train | Batch (196/196) | Top-1: 30.18 (15088/50000)
Regular: 2.7272582054138184
Epoche: 0; regular: 2.7272582054138184: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 39.32
Epoch 1
Train | Batch (1/196) | Top-1: 40.23 (103/256)
Train | Batch (101/196) | Top-1: 41.08 (10621/25856)
Train | Batch (196/196) | Top-1: 42.90 (21449/50000)
Regular: 2.706669330596924
Epoche: 1; regular: 2.706669330596924: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 42.64
Epoch 2
Train | Batch (1/196) | Top-1: 46.88 (120/256)
Train | Batch (101/196) | Top-1: 47.89 (12382/25856)
Train | Batch (196/196) | Top-1: 49.11 (24557/50000)
Regular: 2.6860058307647705
Epoche: 2; regular: 2.6860058307647705: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 51.82
Epoch 3
Train | Batch (1/196) | Top-1: 53.12 (136/256)
Train | Batch (101/196) | Top-1: 52.87 (13670/25856)
Train | Batch (196/196) | Top-1: 53.77 (26884/50000)
Regular: 2.665334701538086
Epoche: 3; regular: 2.665334701538086: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 56.00
Epoch 4
Train | Batch (1/196) | Top-1: 55.86 (143/256)
Train | Batch (101/196) | Top-1: 56.95 (14724/25856)
Train | Batch (196/196) | Top-1: 57.85 (28924/50000)
Regular: 2.644690990447998
Epoche: 4; regular: 2.644690990447998: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 58.36
Epoch 5
Train | Batch (1/196) | Top-1: 57.42 (147/256)
Train | Batch (101/196) | Top-1: 60.36 (15607/25856)
Train | Batch (196/196) | Top-1: 60.67 (30337/50000)
Regular: 2.6240782737731934
Epoche: 5; regular: 2.6240782737731934: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 58.14
Epoch 6
Train | Batch (1/196) | Top-1: 58.59 (150/256)
Train | Batch (101/196) | Top-1: 62.44 (16144/25856)
Train | Batch (196/196) | Top-1: 63.01 (31504/50000)
Regular: 2.603468179702759
Epoche: 6; regular: 2.603468179702759: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 62.03
Epoch 7
Train | Batch (1/196) | Top-1: 61.72 (158/256)
Train | Batch (101/196) | Top-1: 64.54 (16687/25856)
Train | Batch (196/196) | Top-1: 64.79 (32396/50000)
Regular: 2.582879066467285
Epoche: 7; regular: 2.582879066467285: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 60.34
Epoch 8
Train | Batch (1/196) | Top-1: 66.80 (171/256)
Train | Batch (101/196) | Top-1: 66.22 (17122/25856)
Train | Batch (196/196) | Top-1: 66.47 (33237/50000)
Regular: 2.5623013973236084
Epoche: 8; regular: 2.5623013973236084: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 62.63
Epoch 9
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 67.85 (17544/25856)
Train | Batch (196/196) | Top-1: 68.07 (34033/50000)
Regular: 2.5417520999908447
Epoche: 9; regular: 2.5417520999908447: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 67.47
Epoch 10
Train | Batch (1/196) | Top-1: 73.44 (188/256)
Train | Batch (101/196) | Top-1: 69.24 (17903/25856)
Train | Batch (196/196) | Top-1: 69.57 (34787/50000)
Regular: 2.521209239959717
Epoche: 10; regular: 2.521209239959717: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 67.36
Epoch 11
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 70.80 (18306/25856)
Train | Batch (196/196) | Top-1: 71.03 (35516/50000)
Regular: 2.500675678253174
Epoche: 11; regular: 2.500675678253174: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 67.04
Epoch 12
Train | Batch (1/196) | Top-1: 69.92 (179/256)
Train | Batch (101/196) | Top-1: 72.25 (18682/25856)
Train | Batch (196/196) | Top-1: 72.30 (36149/50000)
Regular: 2.4801697731018066
Epoche: 12; regular: 2.4801697731018066: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 70.90
Epoch 13
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 73.12 (18906/25856)
Train | Batch (196/196) | Top-1: 73.66 (36831/50000)
Regular: 2.459674835205078
Epoche: 13; regular: 2.459674835205078: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.87
Epoch 14
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 74.28 (19207/25856)
Train | Batch (196/196) | Top-1: 74.52 (37258/50000)
Regular: 2.439175605773926
Epoche: 14; regular: 2.439175605773926: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.87
Epoch 15
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 75.41 (19499/25856)
Train | Batch (196/196) | Top-1: 75.39 (37697/50000)
Regular: 2.4187023639678955
Epoche: 15; regular: 2.4187023639678955: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.08
Epoch 16
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 76.39 (19752/25856)
Train | Batch (196/196) | Top-1: 76.53 (38264/50000)
Regular: 2.3982596397399902
Epoche: 16; regular: 2.3982596397399902: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.22
Epoch 17
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 77.52 (20043/25856)
Train | Batch (196/196) | Top-1: 77.01 (38507/50000)
Regular: 2.3778107166290283
Epoche: 17; regular: 2.3778107166290283: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 71.94
Epoch 18
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 77.71 (20092/25856)
Train | Batch (196/196) | Top-1: 77.65 (38826/50000)
Regular: 2.3574085235595703
Epoche: 18; regular: 2.3574085235595703: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.75
Epoch 19
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 78.45 (20283/25856)
Train | Batch (196/196) | Top-1: 78.53 (39263/50000)
Regular: 2.337052345275879
Epoche: 19; regular: 2.337052345275879: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.25
Epoch 20
Train | Batch (1/196) | Top-1: 75.78 (194/256)
Train | Batch (101/196) | Top-1: 78.86 (20389/25856)
Train | Batch (196/196) | Top-1: 79.05 (39527/50000)
Regular: 2.3167195320129395
Epoche: 20; regular: 2.3167195320129395: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.32
Epoch 21
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 80.20 (20737/25856)
Train | Batch (196/196) | Top-1: 79.81 (39904/50000)
Regular: 2.2964084148406982
Epoche: 21; regular: 2.2964084148406982: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.14
Epoch 22
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 80.30 (20762/25856)
Train | Batch (196/196) | Top-1: 80.27 (40133/50000)
Regular: 2.2760772705078125
Epoche: 22; regular: 2.2760772705078125: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.87
Epoch 23
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 80.82 (20897/25856)
Train | Batch (196/196) | Top-1: 80.77 (40385/50000)
Regular: 2.2557754516601562
Epoche: 23; regular: 2.2557754516601562: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.47
Epoch 24
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 81.09 (20967/25856)
Train | Batch (196/196) | Top-1: 81.13 (40563/50000)
Regular: 2.235517740249634
Epoche: 24; regular: 2.235517740249634: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.31
Epoch 25
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 81.62 (21104/25856)
Train | Batch (196/196) | Top-1: 81.54 (40771/50000)
Regular: 2.215275287628174
Epoche: 25; regular: 2.215275287628174: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.23
Epoch 26
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 81.71 (21127/25856)
Train | Batch (196/196) | Top-1: 81.92 (40962/50000)
Regular: 2.1950340270996094
Epoche: 26; regular: 2.1950340270996094: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.56
Epoch 27
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 82.14 (21238/25856)
Train | Batch (196/196) | Top-1: 82.21 (41106/50000)
Regular: 2.1748197078704834
Epoche: 27; regular: 2.1748197078704834: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.65
Epoch 28
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 82.89 (21433/25856)
Train | Batch (196/196) | Top-1: 82.43 (41214/50000)
Regular: 2.154656171798706
Epoche: 28; regular: 2.154656171798706: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.05
Epoch 29
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 83.17 (21505/25856)
Train | Batch (196/196) | Top-1: 83.00 (41499/50000)
Regular: 2.134519577026367
Epoche: 29; regular: 2.134519577026367: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.65
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 1
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 83.29 (21535/25856)
Train | Batch (196/196) | Top-1: 83.21 (41607/50000)
Regular: 2.115216016769409
Epoche: 0; regular: 2.115216016769409: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.04
Epoch 1
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 83.76 (21658/25856)
Train | Batch (196/196) | Top-1: 83.74 (41872/50000)
Regular: 2.095127582550049
Epoche: 1; regular: 2.095127582550049: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.98
Epoch 2
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 84.02 (21725/25856)
Train | Batch (196/196) | Top-1: 83.92 (41960/50000)
Regular: 2.075047016143799
Epoche: 2; regular: 2.075047016143799: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.89
Epoch 3
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.79 (21665/25856)
Train | Batch (196/196) | Top-1: 83.97 (41984/50000)
Regular: 2.0549912452697754
Epoche: 3; regular: 2.0549912452697754: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.60
Epoch 4
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 84.42 (21828/25856)
Train | Batch (196/196) | Top-1: 84.38 (42189/50000)
Regular: 2.034980297088623
Epoche: 4; regular: 2.034980297088623: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.54
Epoch 5
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 84.53 (21856/25856)
Train | Batch (196/196) | Top-1: 84.48 (42239/50000)
Regular: 2.015012264251709
Epoche: 5; regular: 2.015012264251709: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.83
Epoch 6
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 85.19 (22027/25856)
Train | Batch (196/196) | Top-1: 84.96 (42482/50000)
Regular: 1.9950439929962158
Epoche: 6; regular: 1.9950439929962158: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.00
Epoch 7
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 85.44 (22092/25856)
Train | Batch (196/196) | Top-1: 85.26 (42632/50000)
Regular: 1.9751302003860474
Epoche: 7; regular: 1.9751302003860474: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.34
Epoch 8
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 85.67 (22152/25856)
Train | Batch (196/196) | Top-1: 85.47 (42733/50000)
Regular: 1.9552733898162842
Epoche: 8; regular: 1.9552733898162842: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.52
Epoch 9
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 85.66 (22149/25856)
Train | Batch (196/196) | Top-1: 85.61 (42807/50000)
Regular: 1.935404896736145
Epoche: 9; regular: 1.935404896736145: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.39
Epoch 10
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 86.27 (22307/25856)
Train | Batch (196/196) | Top-1: 85.92 (42961/50000)
Regular: 1.915591835975647
Epoche: 10; regular: 1.915591835975647: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.21
Epoch 11
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 86.42 (22345/25856)
Train | Batch (196/196) | Top-1: 86.25 (43123/50000)
Regular: 1.895804524421692
Epoche: 11; regular: 1.895804524421692: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.01
Epoch 12
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 86.34 (22325/25856)
Train | Batch (196/196) | Top-1: 86.27 (43135/50000)
Regular: 1.8760920763015747
Epoche: 12; regular: 1.8760920763015747: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.35
Epoch 13
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 86.70 (22418/25856)
Train | Batch (196/196) | Top-1: 86.63 (43317/50000)
Regular: 1.8563847541809082
Epoche: 13; regular: 1.8563847541809082: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.96
Epoch 14
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 86.53 (22372/25856)
Train | Batch (196/196) | Top-1: 86.60 (43302/50000)
Regular: 1.8366599082946777
Epoche: 14; regular: 1.8366599082946777: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.26
Epoch 15
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 86.83 (22452/25856)
Train | Batch (196/196) | Top-1: 86.86 (43429/50000)
Regular: 1.8169795274734497
Epoche: 15; regular: 1.8169795274734497: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.02
Epoch 16
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 87.28 (22566/25856)
Train | Batch (196/196) | Top-1: 86.99 (43495/50000)
Regular: 1.7973642349243164
Epoche: 16; regular: 1.7973642349243164: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.76
Epoch 17
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 87.16 (22536/25856)
Train | Batch (196/196) | Top-1: 87.15 (43575/50000)
Regular: 1.7778055667877197
Epoche: 17; regular: 1.7778055667877197: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.62
Epoch 18
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.81 (22705/25856)
Train | Batch (196/196) | Top-1: 87.38 (43692/50000)
Regular: 1.7583036422729492
Epoche: 18; regular: 1.7583036422729492: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.83
Epoch 19
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.63 (22657/25856)
Train | Batch (196/196) | Top-1: 87.33 (43666/50000)
Regular: 1.7388559579849243
Epoche: 19; regular: 1.7388559579849243: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.24
Epoch 20
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 87.67 (22668/25856)
Train | Batch (196/196) | Top-1: 87.75 (43873/50000)
Regular: 1.719378113746643
Epoche: 20; regular: 1.719378113746643: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.02
Epoch 21
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 87.87 (22720/25856)
Train | Batch (196/196) | Top-1: 87.95 (43973/50000)
Regular: 1.6999973058700562
Epoche: 21; regular: 1.6999973058700562: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.38
Epoch 22
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 88.35 (22843/25856)
Train | Batch (196/196) | Top-1: 88.25 (44124/50000)
Regular: 1.6806671619415283
Epoche: 22; regular: 1.6806671619415283: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.13
Epoch 23
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 87.92 (22733/25856)
Train | Batch (196/196) | Top-1: 87.94 (43970/50000)
Regular: 1.661384105682373
Epoche: 23; regular: 1.661384105682373: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.85
Epoch 24
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 88.47 (22876/25856)
Train | Batch (196/196) | Top-1: 88.29 (44146/50000)
Regular: 1.642215371131897
Epoche: 24; regular: 1.642215371131897: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.03
Epoch 25
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 88.98 (23006/25856)
Train | Batch (196/196) | Top-1: 88.65 (44327/50000)
Regular: 1.6229727268218994
Epoche: 25; regular: 1.6229727268218994: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.69
Epoch 26
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 88.75 (22948/25856)
Train | Batch (196/196) | Top-1: 88.75 (44373/50000)
Regular: 1.6038247346878052
Epoche: 26; regular: 1.6038247346878052: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.77
Epoch 27
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 89.00 (23012/25856)
Train | Batch (196/196) | Top-1: 88.91 (44457/50000)
Regular: 1.5847043991088867
Epoche: 27; regular: 1.5847043991088867: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.13
Epoch 28
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 88.88 (22981/25856)
Train | Batch (196/196) | Top-1: 88.96 (44480/50000)
Regular: 1.5657453536987305
Epoche: 28; regular: 1.5657453536987305: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.96
Epoch 29
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 89.45 (23127/25856)
Train | Batch (196/196) | Top-1: 89.13 (44567/50000)
Regular: 1.5468261241912842
Epoche: 29; regular: 1.5468261241912842: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.13
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 2
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 89.80 (23218/25856)
Train | Batch (196/196) | Top-1: 89.37 (44684/50000)
Regular: 1.5286551713943481
Epoche: 0; regular: 1.5286551713943481: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.51
Epoch 1
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 89.81 (23220/25856)
Train | Batch (196/196) | Top-1: 89.35 (44673/50000)
Regular: 1.5098687410354614
Epoche: 1; regular: 1.5098687410354614: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.96
Epoch 2
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 89.58 (23162/25856)
Train | Batch (196/196) | Top-1: 89.54 (44771/50000)
Regular: 1.4910975694656372
Epoche: 2; regular: 1.4910975694656372: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.13
Epoch 3
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 89.83 (23227/25856)
Train | Batch (196/196) | Top-1: 89.58 (44788/50000)
Regular: 1.4725391864776611
Epoche: 3; regular: 1.4725391864776611: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.74
Epoch 4
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 89.81 (23222/25856)
Train | Batch (196/196) | Top-1: 89.81 (44907/50000)
Regular: 1.4539357423782349
Epoche: 4; regular: 1.4539357423782349: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.20
Epoch 5
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 90.06 (23286/25856)
Train | Batch (196/196) | Top-1: 89.79 (44895/50000)
Regular: 1.4354640245437622
Epoche: 5; regular: 1.4354640245437622: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.70
Epoch 6
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.20 (23321/25856)
Train | Batch (196/196) | Top-1: 90.02 (45011/50000)
Regular: 1.417148470878601
Epoche: 6; regular: 1.417148470878601: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.62
Epoch 7
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.50 (23400/25856)
Train | Batch (196/196) | Top-1: 90.06 (45028/50000)
Regular: 1.3987340927124023
Epoche: 7; regular: 1.3987340927124023: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.89
Epoch 8
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 90.29 (23345/25856)
Train | Batch (196/196) | Top-1: 90.22 (45108/50000)
Regular: 1.380613088607788
Epoche: 8; regular: 1.380613088607788: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.72
Epoch 9
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 90.29 (23346/25856)
Train | Batch (196/196) | Top-1: 90.31 (45156/50000)
Regular: 1.3626046180725098
Epoche: 9; regular: 1.3626046180725098: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.82
Epoch 10
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 90.25 (23335/25856)
Train | Batch (196/196) | Top-1: 90.41 (45203/50000)
Regular: 1.3447754383087158
Epoche: 10; regular: 1.3447754383087158: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.45
Epoch 11
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 90.88 (23497/25856)
Train | Batch (196/196) | Top-1: 90.62 (45311/50000)
Regular: 1.3271496295928955
Epoche: 11; regular: 1.3271496295928955: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.51
Epoch 12
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 90.90 (23503/25856)
Train | Batch (196/196) | Top-1: 90.59 (45295/50000)
Regular: 1.3096342086791992
Epoche: 12; regular: 1.3096342086791992: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.47
Epoch 13
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 90.79 (23474/25856)
Train | Batch (196/196) | Top-1: 90.59 (45295/50000)
Regular: 1.2921428680419922
Epoche: 13; regular: 1.2921428680419922: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.54
Epoch 14
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 90.89 (23501/25856)
Train | Batch (196/196) | Top-1: 90.85 (45427/50000)
Regular: 1.2749687433242798
Epoche: 14; regular: 1.2749687433242798: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.24
Epoch 15
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 90.81 (23479/25856)
Train | Batch (196/196) | Top-1: 90.87 (45435/50000)
Regular: 1.2581496238708496
Epoche: 15; regular: 1.2581496238708496: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.95
Epoch 16
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 91.27 (23600/25856)
Train | Batch (196/196) | Top-1: 90.98 (45491/50000)
Regular: 1.2417235374450684
Epoche: 16; regular: 1.2417235374450684: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.58
Epoch 17
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 91.05 (23542/25856)
Train | Batch (196/196) | Top-1: 90.94 (45472/50000)
Regular: 1.2257130146026611
Epoche: 17; regular: 1.2257130146026611: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.80
Epoch 18
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 91.24 (23592/25856)
Train | Batch (196/196) | Top-1: 90.97 (45486/50000)
Regular: 1.210099458694458
Epoche: 18; regular: 1.210099458694458: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.61
Epoch 19
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 90.93 (23510/25856)
Train | Batch (196/196) | Top-1: 90.94 (45472/50000)
Regular: 1.1952357292175293
Epoche: 19; regular: 1.1952357292175293: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.70
Epoch 20
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 91.16 (23571/25856)
Train | Batch (196/196) | Top-1: 91.02 (45510/50000)
Regular: 1.181028127670288
Epoche: 20; regular: 1.181028127670288: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.47
Epoch 21
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 91.39 (23631/25856)
Train | Batch (196/196) | Top-1: 91.22 (45612/50000)
Regular: 1.1673822402954102
Epoche: 21; regular: 1.1673822402954102: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.35
Epoch 22
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 91.21 (23582/25856)
Train | Batch (196/196) | Top-1: 91.18 (45591/50000)
Regular: 1.1548019647598267
Epoche: 22; regular: 1.1548019647598267: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.88
Epoch 23
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 91.00 (23530/25856)
Train | Batch (196/196) | Top-1: 91.04 (45519/50000)
Regular: 1.1425286531448364
Epoche: 23; regular: 1.1425286531448364: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.10
Epoch 24
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 91.26 (23595/25856)
Train | Batch (196/196) | Top-1: 91.22 (45609/50000)
Regular: 1.130483627319336
Epoche: 24; regular: 1.130483627319336: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.51
Epoch 25
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 91.68 (23704/25856)
Train | Batch (196/196) | Top-1: 91.41 (45707/50000)
Regular: 1.1187117099761963
Epoche: 25; regular: 1.1187117099761963: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.09
Epoch 26
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 91.56 (23673/25856)
Train | Batch (196/196) | Top-1: 91.48 (45738/50000)
Regular: 1.1071659326553345
Epoche: 26; regular: 1.1071659326553345: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.94
Epoch 27
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 91.69 (23707/25856)
Train | Batch (196/196) | Top-1: 91.55 (45774/50000)
Regular: 1.0956813097000122
Epoche: 27; regular: 1.0956813097000122: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.55
Epoch 28
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 91.74 (23721/25856)
Train | Batch (196/196) | Top-1: 91.62 (45810/50000)
Regular: 1.0844734907150269
Epoche: 28; regular: 1.0844734907150269: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.28
Epoch 29
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 91.73 (23718/25856)
Train | Batch (196/196) | Top-1: 91.66 (45830/50000)
Regular: 1.0736582279205322
Epoche: 29; regular: 1.0736582279205322: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.79
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 3
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 92.18 (23835/25856)
Train | Batch (196/196) | Top-1: 91.94 (45972/50000)
Regular: 1.0629538297653198
Epoche: 0; regular: 1.0629538297653198: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.40
Epoch 1
Train | Batch (1/196) | Top-1: 95.70 (245/256)
Train | Batch (101/196) | Top-1: 92.11 (23815/25856)
Train | Batch (196/196) | Top-1: 91.88 (45940/50000)
Regular: 1.0521622896194458
Epoche: 1; regular: 1.0521622896194458: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.87
Epoch 2
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 92.14 (23825/25856)
Train | Batch (196/196) | Top-1: 91.94 (45970/50000)
Regular: 1.0415078401565552
Epoche: 2; regular: 1.0415078401565552: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.35
Epoch 3
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 92.21 (23842/25856)
Train | Batch (196/196) | Top-1: 92.03 (46013/50000)
Regular: 1.030745267868042
Epoche: 3; regular: 1.030745267868042: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.73
Epoch 4
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 92.09 (23812/25856)
Train | Batch (196/196) | Top-1: 92.04 (46021/50000)
Regular: 1.0204740762710571
Epoche: 4; regular: 1.0204740762710571: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.96
Epoch 5
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 92.28 (23859/25856)
Train | Batch (196/196) | Top-1: 91.98 (45988/50000)
Regular: 1.0104323625564575
Epoche: 5; regular: 1.0104323625564575: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.18
Epoch 6
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 92.65 (23956/25856)
Train | Batch (196/196) | Top-1: 92.34 (46170/50000)
Regular: 1.0002951622009277
Epoche: 6; regular: 1.0002951622009277: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.94
Epoch 7
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 92.35 (23879/25856)
Train | Batch (196/196) | Top-1: 92.25 (46123/50000)
Regular: 0.9902793765068054
Epoche: 7; regular: 0.9902793765068054: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.06
Epoch 8
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 92.82 (24000/25856)
Train | Batch (196/196) | Top-1: 92.38 (46191/50000)
Regular: 0.9804438352584839
Epoche: 8; regular: 0.9804438352584839: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.38
Epoch 9
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 92.50 (23917/25856)
Train | Batch (196/196) | Top-1: 92.30 (46149/50000)
Regular: 0.970933735370636
Epoche: 9; regular: 0.970933735370636: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.73
Epoch 10
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 92.60 (23943/25856)
Train | Batch (196/196) | Top-1: 92.45 (46226/50000)
Regular: 0.9613350033760071
Epoche: 10; regular: 0.9613350033760071: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.71
Epoch 11
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 92.61 (23945/25856)
Train | Batch (196/196) | Top-1: 92.39 (46195/50000)
Regular: 0.9518762826919556
Epoche: 11; regular: 0.9518762826919556: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.79
Epoch 12
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 92.64 (23953/25856)
Train | Batch (196/196) | Top-1: 92.52 (46259/50000)
Regular: 0.9428833723068237
Epoche: 12; regular: 0.9428833723068237: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.89
Epoch 13
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 92.71 (23971/25856)
Train | Batch (196/196) | Top-1: 92.64 (46319/50000)
Regular: 0.9332849979400635
Epoche: 13; regular: 0.9332849979400635: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.06
Epoch 14
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 92.62 (23948/25856)
Train | Batch (196/196) | Top-1: 92.59 (46297/50000)
Regular: 0.9243071675300598
Epoche: 14; regular: 0.9243071675300598: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.10
Epoch 15
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 92.93 (24028/25856)
Train | Batch (196/196) | Top-1: 92.71 (46355/50000)
Regular: 0.9153498411178589
Epoche: 15; regular: 0.9153498411178589: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.90
Epoch 16
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 93.07 (24065/25856)
Train | Batch (196/196) | Top-1: 92.93 (46466/50000)
Regular: 0.906337320804596
Epoche: 16; regular: 0.906337320804596: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.25
Epoch 17
Train | Batch (1/196) | Top-1: 96.09 (246/256)
Train | Batch (101/196) | Top-1: 92.87 (24013/25856)
Train | Batch (196/196) | Top-1: 92.74 (46368/50000)
Regular: 0.8977447152137756
Epoche: 17; regular: 0.8977447152137756: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.97
Epoch 18
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 93.11 (24075/25856)
Train | Batch (196/196) | Top-1: 92.84 (46419/50000)
Regular: 0.8891113996505737
Epoche: 18; regular: 0.8891113996505737: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.94
Epoch 19
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 93.21 (24100/25856)
Train | Batch (196/196) | Top-1: 92.82 (46412/50000)
Regular: 0.8802060484886169
Epoche: 19; regular: 0.8802060484886169: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.24
Epoch 20
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 92.92 (24025/25856)
Train | Batch (196/196) | Top-1: 92.87 (46437/50000)
Regular: 0.8721016049385071
Epoche: 20; regular: 0.8721016049385071: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.44
Epoch 21
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 93.19 (24096/25856)
Train | Batch (196/196) | Top-1: 93.16 (46578/50000)
Regular: 0.8635162115097046
Epoche: 21; regular: 0.8635162115097046: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.67
Epoch 22
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 93.50 (24175/25856)
Train | Batch (196/196) | Top-1: 93.13 (46564/50000)
Regular: 0.8550271987915039
Epoche: 22; regular: 0.8550271987915039: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.47
Epoch 23
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 93.38 (24145/25856)
Train | Batch (196/196) | Top-1: 93.14 (46569/50000)
Regular: 0.8465487360954285
Epoche: 23; regular: 0.8465487360954285: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.78
Epoch 24
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 93.10 (24071/25856)
Train | Batch (196/196) | Top-1: 93.12 (46560/50000)
Regular: 0.8384016156196594
Epoche: 24; regular: 0.8384016156196594: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.30
Epoch 25
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 93.53 (24183/25856)
Train | Batch (196/196) | Top-1: 93.21 (46607/50000)
Regular: 0.829902172088623
Epoche: 25; regular: 0.829902172088623: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.53
Epoch 26
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 93.61 (24205/25856)
Train | Batch (196/196) | Top-1: 93.37 (46686/50000)
Regular: 0.8214898109436035
Epoche: 26; regular: 0.8214898109436035: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.05
Epoch 27
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 93.37 (24142/25856)
Train | Batch (196/196) | Top-1: 93.44 (46720/50000)
Regular: 0.8133307695388794
Epoche: 27; regular: 0.8133307695388794: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.36
Epoch 28
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 93.59 (24199/25856)
Train | Batch (196/196) | Top-1: 93.36 (46679/50000)
Regular: 0.8054074645042419
Epoche: 28; regular: 0.8054074645042419: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.05
Epoch 29
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 93.93 (24287/25856)
Train | Batch (196/196) | Top-1: 93.64 (46819/50000)
Regular: 0.7973460555076599
Epoche: 29; regular: 0.7973460555076599: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.97
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 4
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 93.97 (24297/25856)
Train | Batch (196/196) | Top-1: 93.63 (46817/50000)
Regular: 0.7889981865882874
Epoche: 0; regular: 0.7889981865882874: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.08
Epoch 1
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 93.39 (24146/25856)
Train | Batch (196/196) | Top-1: 93.52 (46762/50000)
Regular: 0.7813933491706848
Epoche: 1; regular: 0.7813933491706848: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.02
Epoch 2
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 93.47 (24168/25856)
Train | Batch (196/196) | Top-1: 93.47 (46733/50000)
Regular: 0.7733800411224365
Epoche: 2; regular: 0.7733800411224365: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.69
Epoch 3
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 93.75 (24240/25856)
Train | Batch (196/196) | Top-1: 93.55 (46777/50000)
Regular: 0.7654174566268921
Epoche: 3; regular: 0.7654174566268921: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.26
Epoch 4
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 93.78 (24249/25856)
Train | Batch (196/196) | Top-1: 93.51 (46757/50000)
Regular: 0.7576169967651367
Epoche: 4; regular: 0.7576169967651367: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.54
Epoch 5
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 93.77 (24245/25856)
Train | Batch (196/196) | Top-1: 93.71 (46854/50000)
Regular: 0.7501212954521179
Epoche: 5; regular: 0.7501212954521179: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.80
Epoch 6
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 93.91 (24282/25856)
Train | Batch (196/196) | Top-1: 93.79 (46895/50000)
Regular: 0.7419447898864746
Epoche: 6; regular: 0.7419447898864746: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.98
Epoch 7
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 94.26 (24372/25856)
Train | Batch (196/196) | Top-1: 94.10 (47050/50000)
Regular: 0.7339078783988953
Epoche: 7; regular: 0.7339078783988953: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.88
Epoch 8
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 94.07 (24322/25856)
Train | Batch (196/196) | Top-1: 93.98 (46991/50000)
Regular: 0.7261255979537964
Epoche: 8; regular: 0.7261255979537964: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.44
Epoch 9
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 94.08 (24325/25856)
Train | Batch (196/196) | Top-1: 93.86 (46928/50000)
Regular: 0.7186956405639648
Epoche: 9; regular: 0.7186956405639648: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.40
Epoch 10
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 94.09 (24329/25856)
Train | Batch (196/196) | Top-1: 94.06 (47029/50000)
Regular: 0.7109591960906982
Epoche: 10; regular: 0.7109591960906982: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.17
Epoch 11
Train | Batch (1/196) | Top-1: 95.70 (245/256)
Train | Batch (101/196) | Top-1: 94.03 (24313/25856)
Train | Batch (196/196) | Top-1: 93.96 (46981/50000)
Regular: 0.7034388780593872
Epoche: 11; regular: 0.7034388780593872: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.76
Epoch 12
Train | Batch (1/196) | Top-1: 96.09 (246/256)
Train | Batch (101/196) | Top-1: 94.26 (24373/25856)
Train | Batch (196/196) | Top-1: 94.09 (47043/50000)
Regular: 0.6956028938293457
Epoche: 12; regular: 0.6956028938293457: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.22
Epoch 13
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 94.36 (24397/25856)
Train | Batch (196/196) | Top-1: 94.10 (47049/50000)
Regular: 0.6884353756904602
Epoche: 13; regular: 0.6884353756904602: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.00
Epoch 14
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 94.22 (24361/25856)
Train | Batch (196/196) | Top-1: 94.16 (47080/50000)
Regular: 0.6812244057655334
Epoche: 14; regular: 0.6812244057655334: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.67
Epoch 15
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 94.17 (24348/25856)
Train | Batch (196/196) | Top-1: 94.17 (47083/50000)
Regular: 0.6737762689590454
Epoche: 15; regular: 0.6737762689590454: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.78
Epoch 16
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 94.33 (24391/25856)
Train | Batch (196/196) | Top-1: 94.21 (47106/50000)
Regular: 0.6664533019065857
Epoche: 16; regular: 0.6664533019065857: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.04
Epoch 17
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 94.34 (24392/25856)
Train | Batch (196/196) | Top-1: 94.24 (47118/50000)
Regular: 0.658869206905365
Epoche: 17; regular: 0.658869206905365: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.98
Epoch 18
Train | Batch (1/196) | Top-1: 96.88 (248/256)
Train | Batch (101/196) | Top-1: 94.32 (24388/25856)
Train | Batch (196/196) | Top-1: 94.34 (47171/50000)
Regular: 0.6513111591339111
Epoche: 18; regular: 0.6513111591339111: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.37
Epoch 19
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 94.52 (24438/25856)
Train | Batch (196/196) | Top-1: 94.35 (47173/50000)
Regular: 0.6445645093917847
Epoche: 19; regular: 0.6445645093917847: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.31
Epoch 20
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 94.37 (24401/25856)
Train | Batch (196/196) | Top-1: 94.28 (47141/50000)
Regular: 0.6378123164176941
Epoche: 20; regular: 0.6378123164176941: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.74
Epoch 21
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 94.42 (24412/25856)
Train | Batch (196/196) | Top-1: 94.39 (47195/50000)
Regular: 0.6312677264213562
Epoche: 21; regular: 0.6312677264213562: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.59
Epoch 22
Train | Batch (1/196) | Top-1: 96.09 (246/256)
Train | Batch (101/196) | Top-1: 94.44 (24418/25856)
Train | Batch (196/196) | Top-1: 94.33 (47167/50000)
Regular: 0.624622106552124
Epoche: 22; regular: 0.624622106552124: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.39
Epoch 23
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 94.41 (24410/25856)
Train | Batch (196/196) | Top-1: 94.36 (47181/50000)
Regular: 0.6176111698150635
Epoche: 23; regular: 0.6176111698150635: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.77
Epoch 24
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 94.33 (24390/25856)
Train | Batch (196/196) | Top-1: 94.24 (47118/50000)
Regular: 0.6110717058181763
Epoche: 24; regular: 0.6110717058181763: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.03
Epoch 25
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 94.81 (24514/25856)
Train | Batch (196/196) | Top-1: 94.68 (47341/50000)
Regular: 0.6042266488075256
Epoche: 25; regular: 0.6042266488075256: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.93
Epoch 26
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 94.41 (24411/25856)
Train | Batch (196/196) | Top-1: 94.33 (47166/50000)
Regular: 0.5973969101905823
Epoche: 26; regular: 0.5973969101905823: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.19
Epoch 27
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 94.70 (24486/25856)
Train | Batch (196/196) | Top-1: 94.43 (47215/50000)
Regular: 0.5912175178527832
Epoche: 27; regular: 0.5912175178527832: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.08
Epoch 28
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 94.28 (24377/25856)
Train | Batch (196/196) | Top-1: 94.23 (47113/50000)
Regular: 0.5855095386505127
Epoche: 28; regular: 0.5855095386505127: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.73
Epoch 29
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 94.37 (24401/25856)
Train | Batch (196/196) | Top-1: 94.37 (47184/50000)
Regular: 0.5797672867774963
Epoche: 29; regular: 0.5797672867774963: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.06
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 5
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 94.91 (24541/25856)
Train | Batch (196/196) | Top-1: 94.68 (47339/50000)
Regular: 0.5734398365020752
Epoche: 0; regular: 0.5734398365020752: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.52
Epoch 1
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 94.33 (24391/25856)
Train | Batch (196/196) | Top-1: 94.32 (47160/50000)
Regular: 0.5681989789009094
Epoche: 1; regular: 0.5681989789009094: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.57
Epoch 2
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 94.61 (24463/25856)
Train | Batch (196/196) | Top-1: 94.62 (47309/50000)
Regular: 0.5627844929695129
Epoche: 2; regular: 0.5627844929695129: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.63
Epoch 3
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 94.76 (24500/25856)
Train | Batch (196/196) | Top-1: 94.35 (47174/50000)
Regular: 0.5575613379478455
Epoche: 3; regular: 0.5575613379478455: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.20
Epoch 4
Train | Batch (1/196) | Top-1: 95.70 (245/256)
Train | Batch (101/196) | Top-1: 94.34 (24393/25856)
Train | Batch (196/196) | Top-1: 94.28 (47142/50000)
Regular: 0.5528688430786133
Epoche: 4; regular: 0.5528688430786133: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.80
Epoch 5
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 94.66 (24475/25856)
Train | Batch (196/196) | Top-1: 94.59 (47293/50000)
Regular: 0.5478096604347229
Epoche: 5; regular: 0.5478096604347229: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.86
Epoch 6
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 94.78 (24506/25856)
Train | Batch (196/196) | Top-1: 94.60 (47298/50000)
Regular: 0.5426937937736511
Epoche: 6; regular: 0.5426937937736511: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.47
Epoch 7
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 94.53 (24442/25856)
Train | Batch (196/196) | Top-1: 94.52 (47262/50000)
Regular: 0.5377525091171265
Epoche: 7; regular: 0.5377525091171265: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.10
Epoch 8
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 94.93 (24544/25856)
Train | Batch (196/196) | Top-1: 94.63 (47314/50000)
Regular: 0.5330466628074646
Epoche: 8; regular: 0.5330466628074646: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.11
Epoch 9
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 94.76 (24501/25856)
Train | Batch (196/196) | Top-1: 94.69 (47345/50000)
Regular: 0.5289701223373413
Epoche: 9; regular: 0.5289701223373413: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.34
Epoch 10
Train | Batch (1/196) | Top-1: 96.09 (246/256)
Train | Batch (101/196) | Top-1: 94.79 (24509/25856)
Train | Batch (196/196) | Top-1: 94.76 (47378/50000)
Regular: 0.5243600010871887
Epoche: 10; regular: 0.5243600010871887: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.89
Epoch 11
Train | Batch (1/196) | Top-1: 95.70 (245/256)
Train | Batch (101/196) | Top-1: 94.56 (24450/25856)
Train | Batch (196/196) | Top-1: 94.42 (47212/50000)
Regular: 0.5200343132019043
Epoche: 11; regular: 0.5200343132019043: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.92
Epoch 12
Train | Batch (1/196) | Top-1: 97.66 (250/256)
Train | Batch (101/196) | Top-1: 94.61 (24462/25856)
Train | Batch (196/196) | Top-1: 94.70 (47348/50000)
Regular: 0.5159556269645691
Epoche: 12; regular: 0.5159556269645691: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.53
Epoch 13
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 94.84 (24522/25856)
Train | Batch (196/196) | Top-1: 94.55 (47277/50000)
Regular: 0.512124240398407
Epoche: 13; regular: 0.512124240398407: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.29
Epoch 14
Train | Batch (1/196) | Top-1: 96.48 (247/256)
Train | Batch (101/196) | Top-1: 94.47 (24425/25856)
Train | Batch (196/196) | Top-1: 94.63 (47314/50000)
Regular: 0.5089142322540283
Epoche: 14; regular: 0.5089142322540283: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.41
Epoch 15
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 95.12 (24595/25856)
Train | Batch (196/196) | Top-1: 94.89 (47444/50000)
Regular: 0.5044090747833252
Epoche: 15; regular: 0.5044090747833252: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.88
Epoch 16
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 94.55 (24446/25856)
Train | Batch (196/196) | Top-1: 94.49 (47246/50000)
Regular: 0.5007972121238708
Epoche: 16; regular: 0.5007972121238708: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.31
Epoch 17
Train | Batch (1/196) | Top-1: 96.48 (247/256)
Train | Batch (101/196) | Top-1: 94.83 (24518/25856)
Train | Batch (196/196) | Top-1: 94.66 (47332/50000)
Regular: 0.49723321199417114
Epoche: 17; regular: 0.49723321199417114: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.33
Epoch 18
Train | Batch (1/196) | Top-1: 96.48 (247/256)
Train | Batch (101/196) | Top-1: 95.06 (24580/25856)
Train | Batch (196/196) | Top-1: 95.01 (47505/50000)
Regular: 0.493166983127594
Epoche: 18; regular: 0.493166983127594: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.06
Epoch 19
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 94.76 (24501/25856)
Train | Batch (196/196) | Top-1: 94.80 (47400/50000)
Regular: 0.4894846975803375
Epoche: 19; regular: 0.4894846975803375: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.32
Epoch 20
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 94.93 (24546/25856)
Train | Batch (196/196) | Top-1: 95.01 (47503/50000)
Regular: 0.48582911491394043
Epoche: 20; regular: 0.48582911491394043: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.72
Epoch 21
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 95.15 (24603/25856)
Train | Batch (196/196) | Top-1: 94.96 (47481/50000)
Regular: 0.48168447613716125
Epoche: 21; regular: 0.48168447613716125: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.58
Epoch 22
Train | Batch (1/196) | Top-1: 98.05 (251/256)
Train | Batch (101/196) | Top-1: 95.44 (24678/25856)
Train | Batch (196/196) | Top-1: 95.11 (47556/50000)
Regular: 0.4775402247905731
Epoche: 22; regular: 0.4775402247905731: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.51
Epoch 23
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 94.83 (24519/25856)
Train | Batch (196/196) | Top-1: 94.75 (47377/50000)
Regular: 0.4746473729610443
Epoche: 23; regular: 0.4746473729610443: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.53
Epoch 24
Train | Batch (1/196) | Top-1: 95.70 (245/256)
Train | Batch (101/196) | Top-1: 95.08 (24584/25856)
Train | Batch (196/196) | Top-1: 94.92 (47462/50000)
Regular: 0.47141435742378235
Epoche: 24; regular: 0.47141435742378235: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.89
Epoch 25
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 95.35 (24653/25856)
Train | Batch (196/196) | Top-1: 95.07 (47537/50000)
Regular: 0.4675394296646118
Epoche: 25; regular: 0.4675394296646118: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 71.99
Epoch 26
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 95.31 (24644/25856)
Train | Batch (196/196) | Top-1: 95.14 (47571/50000)
Regular: 0.4638412296772003
Epoche: 26; regular: 0.4638412296772003: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.79
Epoch 27
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 94.99 (24560/25856)
Train | Batch (196/196) | Top-1: 94.97 (47485/50000)
Regular: 0.4606355130672455
Epoche: 27; regular: 0.4606355130672455: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.71
Epoch 28
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 94.91 (24539/25856)
Train | Batch (196/196) | Top-1: 94.76 (47382/50000)
Regular: 0.45783933997154236
Epoche: 28; regular: 0.45783933997154236: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.99
Epoch 29
Train | Batch (1/196) | Top-1: 95.70 (245/256)
Train | Batch (101/196) | Top-1: 95.39 (24664/25856)
Train | Batch (196/196) | Top-1: 95.28 (47639/50000)
Regular: 0.4543989896774292
Epoche: 29; regular: 0.4543989896774292: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.52
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 25.19 (6512/25856)
Train | Batch (196/196) | Top-1: 30.07 (15035/50000)
Regular: nan
Epoche: 0; regular: nan: flops 68862592
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
#Filters: 1136, #FLOPs: 68.86M | Top-1: 32.61
Epoch 0 | Top-1: 32.61
Train | Batch (1/196) | Top-1: 32.03 (82/256)
Train | Batch (101/196) | Top-1: 42.59 (11011/25856)
Train | Batch (196/196) | Top-1: 45.25 (22627/50000)
Regular: nan
Epoche: 1; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 45.77
Epoch 1 | Top-1: 45.77
Train | Batch (1/196) | Top-1: 47.66 (122/256)
Train | Batch (101/196) | Top-1: 53.56 (13848/25856)
Train | Batch (196/196) | Top-1: 56.28 (28139/50000)
Regular: nan
Epoche: 2; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 50.16
Epoch 2 | Top-1: 50.16
Train | Batch (1/196) | Top-1: 53.91 (138/256)
Train | Batch (101/196) | Top-1: 62.43 (16142/25856)
Train | Batch (196/196) | Top-1: 63.83 (31915/50000)
Regular: nan
Epoche: 3; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 59.08
Epoch 3 | Top-1: 59.08
Train | Batch (1/196) | Top-1: 64.06 (164/256)
Train | Batch (101/196) | Top-1: 67.39 (17425/25856)
Train | Batch (196/196) | Top-1: 68.47 (34235/50000)
Regular: nan
Epoche: 4; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 59.66
Epoch 4 | Top-1: 59.66
Train | Batch (1/196) | Top-1: 64.84 (166/256)
Train | Batch (101/196) | Top-1: 71.07 (18375/25856)
Train | Batch (196/196) | Top-1: 71.99 (35993/50000)
Regular: nan
Epoche: 5; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 55.61
Epoch 5 | Top-1: 55.61
Train | Batch (1/196) | Top-1: 66.80 (171/256)
Train | Batch (101/196) | Top-1: 73.91 (19110/25856)
Train | Batch (196/196) | Top-1: 74.60 (37302/50000)
Regular: nan
Epoche: 6; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 66.18
Epoch 6 | Top-1: 66.18
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 76.28 (19724/25856)
Train | Batch (196/196) | Top-1: 76.88 (38438/50000)
Regular: nan
Epoche: 7; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 66.97
Epoch 7 | Top-1: 66.97
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 78.24 (20230/25856)
Train | Batch (196/196) | Top-1: 78.66 (39330/50000)
Regular: nan
Epoche: 8; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 66.38
Epoch 8 | Top-1: 66.38
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 80.19 (20734/25856)
Train | Batch (196/196) | Top-1: 80.15 (40076/50000)
Regular: nan
Epoche: 9; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.61
Epoch 9 | Top-1: 74.61
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 81.42 (21053/25856)
Train | Batch (196/196) | Top-1: 81.48 (40738/50000)
Regular: nan
Epoche: 10; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.06
Epoch 10 | Top-1: 76.06
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 81.30 (21022/25856)
Train | Batch (196/196) | Top-1: 81.56 (40781/50000)
Regular: nan
Epoche: 11; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 68.58
Epoch 11 | Top-1: 68.58
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 82.57 (21350/25856)
Train | Batch (196/196) | Top-1: 82.58 (41292/50000)
Regular: nan
Epoche: 12; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.34
Epoch 12 | Top-1: 74.34
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 83.10 (21487/25856)
Train | Batch (196/196) | Top-1: 83.03 (41513/50000)
Regular: nan
Epoche: 13; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.27
Epoch 13 | Top-1: 75.27
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.48 (21584/25856)
Train | Batch (196/196) | Top-1: 83.80 (41898/50000)
Regular: nan
Epoche: 14; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.97
Epoch 14 | Top-1: 75.97
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.85 (21680/25856)
Train | Batch (196/196) | Top-1: 84.09 (42047/50000)
Regular: nan
Epoche: 15; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.28
Epoch 15 | Top-1: 74.28
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 84.38 (21817/25856)
Train | Batch (196/196) | Top-1: 84.58 (42291/50000)
Regular: nan
Epoche: 16; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 65.17
Epoch 16 | Top-1: 65.17
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 84.54 (21859/25856)
Train | Batch (196/196) | Top-1: 84.51 (42256/50000)
Regular: nan
Epoche: 17; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.03
Epoch 17 | Top-1: 75.03
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 85.45 (22094/25856)
Train | Batch (196/196) | Top-1: 85.02 (42512/50000)
Regular: nan
Epoche: 18; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.43
Epoch 18 | Top-1: 80.43
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 85.92 (22215/25856)
Train | Batch (196/196) | Top-1: 85.54 (42770/50000)
Regular: nan
Epoche: 19; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.26
Epoch 19 | Top-1: 79.26
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 86.20 (22287/25856)
Train | Batch (196/196) | Top-1: 85.79 (42896/50000)
Regular: nan
Epoche: 20; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.84
Epoch 20 | Top-1: 75.84
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 85.94 (22220/25856)
Train | Batch (196/196) | Top-1: 85.66 (42829/50000)
Regular: nan
Epoche: 21; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.99
Epoch 21 | Top-1: 80.99
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 85.97 (22228/25856)
Train | Batch (196/196) | Top-1: 86.05 (43024/50000)
Regular: nan
Epoche: 22; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.14
Epoch 22 | Top-1: 75.14
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 86.56 (22382/25856)
Train | Batch (196/196) | Top-1: 86.49 (43244/50000)
Regular: nan
Epoche: 23; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.16
Epoch 23 | Top-1: 78.16
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 86.34 (22325/25856)
Train | Batch (196/196) | Top-1: 86.42 (43210/50000)
Regular: nan
Epoche: 24; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.31
Epoch 24 | Top-1: 80.31
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 86.71 (22419/25856)
Train | Batch (196/196) | Top-1: 86.60 (43298/50000)
Regular: nan
Epoche: 25; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.96
Epoch 25 | Top-1: 79.96
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.23 (22553/25856)
Train | Batch (196/196) | Top-1: 86.95 (43475/50000)
Regular: nan
Epoche: 26; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.05
Epoch 26 | Top-1: 76.05
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 87.13 (22529/25856)
Train | Batch (196/196) | Top-1: 87.19 (43595/50000)
Regular: nan
Epoche: 27; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.87
Epoch 27 | Top-1: 80.87
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 87.41 (22600/25856)
Train | Batch (196/196) | Top-1: 87.23 (43616/50000)
Regular: nan
Epoche: 28; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.35
Epoch 28 | Top-1: 73.35
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 87.33 (22579/25856)
Train | Batch (196/196) | Top-1: 87.22 (43610/50000)
Regular: nan
Epoche: 29; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 68.98
Epoch 29 | Top-1: 68.98
model: ResNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (4): AvgPool2d(kernel_size=8, stride=8, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=10, bias=True)
  )
)
Test acc: 68.97999999999999
