no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=1e-08, logger='MorphLogs/logMorphNetFlops1e-8', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=4.0, pruner='FilterPrunnerResNet')
I: 0
flops: 17326400
Before Pruning | FLOPs: 17.326M | #Params: 0.116M
Epoch 0
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 29.13 (7531/25856)
Train | Batch (196/196) | Top-1: 35.38 (17690/50000)
Regular: 0.5428292155265808
Epoche: 0; regular: 0.5428292155265808: flops 17326400
#Filters: 568, #FLOPs: 17.33M | Top-1: 41.13
Epoch 1
Train | Batch (1/196) | Top-1: 45.31 (116/256)
Train | Batch (101/196) | Top-1: 50.76 (13125/25856)
Train | Batch (196/196) | Top-1: 53.86 (26930/50000)
Regular: 0.3002820611000061
Epoche: 1; regular: 0.3002820611000061: flops 17326400
#Filters: 554, #FLOPs: 16.29M | Top-1: 50.79
Epoch 2
Train | Batch (1/196) | Top-1: 57.42 (147/256)
Train | Batch (101/196) | Top-1: 61.59 (15926/25856)
Train | Batch (196/196) | Top-1: 63.31 (31657/50000)
Regular: 0.18073490262031555
Epoche: 2; regular: 0.18073490262031555: flops 17326400
#Filters: 537, #FLOPs: 15.26M | Top-1: 51.87
Epoch 3
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 67.55 (17467/25856)
Train | Batch (196/196) | Top-1: 68.15 (34074/50000)
Regular: 0.13806737959384918
Epoche: 3; regular: 0.13806737959384918: flops 17326400
#Filters: 532, #FLOPs: 15.11M | Top-1: 61.73
Epoch 4
Train | Batch (1/196) | Top-1: 68.36 (175/256)
Train | Batch (101/196) | Top-1: 70.16 (18140/25856)
Train | Batch (196/196) | Top-1: 70.56 (35281/50000)
Regular: 0.12153097987174988
Epoche: 4; regular: 0.12153097987174988: flops 17326400
#Filters: 525, #FLOPs: 14.71M | Top-1: 48.73
Epoch 5
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 72.21 (18670/25856)
Train | Batch (196/196) | Top-1: 72.34 (36172/50000)
Regular: 0.11615097522735596
Epoche: 5; regular: 0.11615097522735596: flops 17326400
#Filters: 518, #FLOPs: 14.51M | Top-1: 59.82
Epoch 6
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 73.30 (18953/25856)
Train | Batch (196/196) | Top-1: 73.67 (36833/50000)
Regular: 0.11433395743370056
Epoche: 6; regular: 0.11433395743370056: flops 17326400
#Filters: 522, #FLOPs: 14.87M | Top-1: 42.82
Epoch 7
Train | Batch (1/196) | Top-1: 71.09 (182/256)
Train | Batch (101/196) | Top-1: 74.95 (19380/25856)
Train | Batch (196/196) | Top-1: 74.71 (37357/50000)
Regular: 0.1138685792684555
Epoche: 7; regular: 0.1138685792684555: flops 17326400
#Filters: 519, #FLOPs: 14.64M | Top-1: 52.05
Epoch 8
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 75.34 (19481/25856)
Train | Batch (196/196) | Top-1: 75.28 (37638/50000)
Regular: 0.11264742910861969
Epoche: 8; regular: 0.11264742910861969: flops 17326400
#Filters: 513, #FLOPs: 14.38M | Top-1: 60.00
Epoch 9
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 75.84 (19610/25856)
Train | Batch (196/196) | Top-1: 75.99 (37996/50000)
Regular: 0.11206533759832382
Epoche: 9; regular: 0.11206533759832382: flops 17326400
#Filters: 510, #FLOPs: 14.19M | Top-1: 61.09
Epoch 10
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 76.39 (19751/25856)
Train | Batch (196/196) | Top-1: 76.38 (38192/50000)
Regular: 0.11222794651985168
Epoche: 10; regular: 0.11222794651985168: flops 17326400
#Filters: 513, #FLOPs: 14.49M | Top-1: 71.41
Epoch 11
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 77.10 (19935/25856)
Train | Batch (196/196) | Top-1: 77.01 (38503/50000)
Regular: 0.11198706179857254
Epoche: 11; regular: 0.11198706179857254: flops 17326400
#Filters: 509, #FLOPs: 14.08M | Top-1: 57.49
Epoch 12
Train | Batch (1/196) | Top-1: 75.78 (194/256)
Train | Batch (101/196) | Top-1: 77.56 (20054/25856)
Train | Batch (196/196) | Top-1: 77.40 (38701/50000)
Regular: 0.11157771199941635
Epoche: 12; regular: 0.11157771199941635: flops 17326400
#Filters: 504, #FLOPs: 14.01M | Top-1: 65.64
Epoch 13
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 77.70 (20089/25856)
Train | Batch (196/196) | Top-1: 77.72 (38858/50000)
Regular: 0.11221002042293549
Epoche: 13; regular: 0.11221002042293549: flops 17326400
#Filters: 502, #FLOPs: 13.79M | Top-1: 64.58
Epoch 14
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 78.06 (20184/25856)
Train | Batch (196/196) | Top-1: 78.08 (39038/50000)
Regular: 0.1129310205578804
Epoche: 14; regular: 0.1129310205578804: flops 17326400
#Filters: 506, #FLOPs: 14.06M | Top-1: 58.64
Epoch 15
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 78.51 (20300/25856)
Train | Batch (196/196) | Top-1: 78.36 (39181/50000)
Regular: 0.11214421689510345
Epoche: 15; regular: 0.11214421689510345: flops 17326400
#Filters: 502, #FLOPs: 13.75M | Top-1: 59.26
Epoch 16
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 78.83 (20382/25856)
Train | Batch (196/196) | Top-1: 78.70 (39349/50000)
Regular: 0.11172451078891754
Epoche: 16; regular: 0.11172451078891754: flops 17326400
#Filters: 503, #FLOPs: 13.84M | Top-1: 56.45
Epoch 17
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 78.43 (20278/25856)
Train | Batch (196/196) | Top-1: 78.55 (39276/50000)
Regular: 0.11290781199932098
Epoche: 17; regular: 0.11290781199932098: flops 17326400
#Filters: 505, #FLOPs: 13.97M | Top-1: 56.11
Epoch 18
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 78.82 (20379/25856)
Train | Batch (196/196) | Top-1: 79.06 (39531/50000)
Regular: 0.11168000847101212
Epoche: 18; regular: 0.11168000847101212: flops 17326400
#Filters: 502, #FLOPs: 13.81M | Top-1: 65.07
Epoch 19
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 79.03 (20435/25856)
Train | Batch (196/196) | Top-1: 78.85 (39423/50000)
Regular: 0.11215081810951233
Epoche: 19; regular: 0.11215081810951233: flops 17326400
#Filters: 495, #FLOPs: 13.47M | Top-1: 57.25
Epoch 20
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 79.22 (20482/25856)
Train | Batch (196/196) | Top-1: 79.42 (39711/50000)
Regular: 0.11218956857919693
Epoche: 20; regular: 0.11218956857919693: flops 17326400
#Filters: 498, #FLOPs: 13.68M | Top-1: 60.13
Epoch 21
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 79.43 (20537/25856)
Train | Batch (196/196) | Top-1: 79.44 (39720/50000)
Regular: 0.11255484074354172
Epoche: 21; regular: 0.11255484074354172: flops 17326400
#Filters: 495, #FLOPs: 13.49M | Top-1: 62.19
Epoch 22
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 80.07 (20703/25856)
Train | Batch (196/196) | Top-1: 79.69 (39843/50000)
Regular: 0.11186475306749344
Epoche: 22; regular: 0.11186475306749344: flops 17326400
#Filters: 497, #FLOPs: 13.66M | Top-1: 61.14
Epoch 23
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.80 (20632/25856)
Train | Batch (196/196) | Top-1: 79.61 (39805/50000)
Regular: 0.11324203014373779
Epoche: 23; regular: 0.11324203014373779: flops 17326400
#Filters: 498, #FLOPs: 13.58M | Top-1: 58.71
Epoch 24
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.52 (20561/25856)
Train | Batch (196/196) | Top-1: 79.70 (39852/50000)
Regular: 0.11198166757822037
Epoche: 24; regular: 0.11198166757822037: flops 17326400
#Filters: 500, #FLOPs: 13.70M | Top-1: 68.62
Epoch 25
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 80.18 (20731/25856)
Train | Batch (196/196) | Top-1: 80.14 (40069/50000)
Regular: 0.11073211580514908
Epoche: 25; regular: 0.11073211580514908: flops 17326400
#Filters: 493, #FLOPs: 13.47M | Top-1: 67.40
Epoch 26
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 79.88 (20655/25856)
Train | Batch (196/196) | Top-1: 79.82 (39910/50000)
Regular: 0.1099267527461052
Epoche: 26; regular: 0.1099267527461052: flops 17326400
#Filters: 488, #FLOPs: 13.31M | Top-1: 70.98
Epoch 27
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 79.88 (20655/25856)
Train | Batch (196/196) | Top-1: 80.07 (40036/50000)
Regular: 0.11272414773702621
Epoche: 27; regular: 0.11272414773702621: flops 17326400
#Filters: 487, #FLOPs: 13.27M | Top-1: 56.43
Epoch 28
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 80.19 (20733/25856)
Train | Batch (196/196) | Top-1: 80.13 (40066/50000)
Regular: 0.11616379767656326
Epoche: 28; regular: 0.11616379767656326: flops 17326400
#Filters: 488, #FLOPs: 13.33M | Top-1: 66.08
Epoch 29
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.58 (20834/25856)
Train | Batch (196/196) | Top-1: 80.55 (40273/50000)
Regular: 0.11420869082212448
Epoche: 29; regular: 0.11420869082212448: flops 17326400
#Filters: 488, #FLOPs: 13.35M | Top-1: 63.87
Drin!!
Layers that will be prunned: [(1, 7), (3, 7), (5, 6), (7, 7), (9, 7), (11, 2), (13, 10), (15, 13), (17, 12), (19, 1), (27, 4), (29, 2)]
Prunning filters..
Layer index: 1; Pruned filters: 4
Layer index: 1; Pruned filters: 3
Layer index: 3; Pruned filters: 3
Layer index: 3; Pruned filters: 4
Layer index: 5; Pruned filters: 3
Layer index: 5; Pruned filters: 3
Layer index: 7; Pruned filters: 4
Layer index: 7; Pruned filters: 3
Layer index: 9; Pruned filters: 4
Layer index: 9; Pruned filters: 3
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 3
Layer index: 13; Pruned filters: 3
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 2
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 8
Layer index: 15; Pruned filters: 4
Layer index: 17; Pruned filters: 3
Layer index: 17; Pruned filters: 3
Layer index: 17; Pruned filters: 2
Layer index: 17; Pruned filters: 3
Layer index: 17; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Target (flops): 69.306M
After Pruning | FLOPs: 9.327M | #Params: 0.097M
2.7467539584359013
After Growth | FLOPs: 69.342M | #Params: 0.726M
I: 1
flops: 69342064
Before Pruning | FLOPs: 69.342M | #Params: 0.726M
Epoch 0
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 80.54 (20824/25856)
Train | Batch (196/196) | Top-1: 80.13 (40066/50000)
Regular: 1.5138561725616455
Epoche: 0; regular: 1.5138561725616455: flops 69342064
#Filters: 1325, #FLOPs: 66.81M | Top-1: 61.71
Epoch 1
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.22 (20484/25856)
Train | Batch (196/196) | Top-1: 78.98 (39488/50000)
Regular: 0.6992611885070801
Epoche: 1; regular: 0.6992611885070801: flops 69342064
#Filters: 1244, #FLOPs: 60.47M | Top-1: 61.65
Epoch 2
Train | Batch (1/196) | Top-1: 75.78 (194/256)
Train | Batch (101/196) | Top-1: 79.42 (20534/25856)
Train | Batch (196/196) | Top-1: 79.05 (39527/50000)
Regular: 0.2921568751335144
Epoche: 2; regular: 0.2921568751335144: flops 69342064
#Filters: 976, #FLOPs: 47.65M | Top-1: 62.47
Epoch 3
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 78.76 (20363/25856)
Train | Batch (196/196) | Top-1: 78.97 (39483/50000)
Regular: 0.2151648849248886
Epoche: 3; regular: 0.2151648849248886: flops 69342064
#Filters: 975, #FLOPs: 47.55M | Top-1: 61.52
Epoch 4
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 78.69 (20347/25856)
Train | Batch (196/196) | Top-1: 78.92 (39459/50000)
Regular: 0.19151845574378967
Epoche: 4; regular: 0.19151845574378967: flops 69342064
#Filters: 632, #FLOPs: 30.70M | Top-1: 38.83
Epoch 5
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 79.04 (20436/25856)
Train | Batch (196/196) | Top-1: 78.98 (39492/50000)
Regular: 0.18814930319786072
Epoche: 5; regular: 0.18814930319786072: flops 69342064
#Filters: 627, #FLOPs: 30.45M | Top-1: 57.73
Epoch 6
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.28 (20498/25856)
Train | Batch (196/196) | Top-1: 79.27 (39635/50000)
Regular: 0.18766656517982483
Epoche: 6; regular: 0.18766656517982483: flops 69342064
#Filters: 627, #FLOPs: 30.95M | Top-1: 59.41
Epoch 7
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 79.22 (20483/25856)
Train | Batch (196/196) | Top-1: 79.22 (39609/50000)
Regular: 0.1871410608291626
Epoche: 7; regular: 0.1871410608291626: flops 69342064
#Filters: 627, #FLOPs: 30.70M | Top-1: 48.70
Epoch 8
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 79.58 (20577/25856)
Train | Batch (196/196) | Top-1: 79.50 (39751/50000)
Regular: 0.18356573581695557
Epoche: 8; regular: 0.18356573581695557: flops 69342064
#Filters: 624, #FLOPs: 30.80M | Top-1: 55.89
Epoch 9
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.38 (20524/25856)
Train | Batch (196/196) | Top-1: 79.14 (39572/50000)
Regular: 0.18247860670089722
Epoche: 9; regular: 0.18247860670089722: flops 69342064
#Filters: 615, #FLOPs: 29.94M | Top-1: 58.95
Epoch 10
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.25 (20491/25856)
Train | Batch (196/196) | Top-1: 79.44 (39721/50000)
Regular: 0.18152128159999847
Epoche: 10; regular: 0.18152128159999847: flops 69342064
#Filters: 618, #FLOPs: 30.24M | Top-1: 64.68
Epoch 11
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 79.46 (20546/25856)
Train | Batch (196/196) | Top-1: 79.32 (39658/50000)
Regular: 0.18841922283172607
Epoche: 11; regular: 0.18841922283172607: flops 69342064
#Filters: 623, #FLOPs: 30.39M | Top-1: 48.25
Epoch 12
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 79.60 (20582/25856)
Train | Batch (196/196) | Top-1: 79.17 (39583/50000)
Regular: 0.18078339099884033
Epoche: 12; regular: 0.18078339099884033: flops 69342064
#Filters: 619, #FLOPs: 30.09M | Top-1: 50.90
Epoch 13
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 79.49 (20552/25856)
Train | Batch (196/196) | Top-1: 79.50 (39751/50000)
Regular: 0.17838361859321594
Epoche: 13; regular: 0.17838361859321594: flops 69342064
#Filters: 613, #FLOPs: 29.99M | Top-1: 59.72
Epoch 14
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 79.68 (20603/25856)
Train | Batch (196/196) | Top-1: 79.50 (39749/50000)
Regular: 0.17908422648906708
Epoche: 14; regular: 0.17908422648906708: flops 69342064
#Filters: 619, #FLOPs: 30.45M | Top-1: 66.62
Epoch 15
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 79.33 (20511/25856)
Train | Batch (196/196) | Top-1: 79.41 (39704/50000)
Regular: 0.17785948514938354
Epoche: 15; regular: 0.17785948514938354: flops 69342064
#Filters: 615, #FLOPs: 30.34M | Top-1: 58.04
Epoch 16
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 79.44 (20540/25856)
Train | Batch (196/196) | Top-1: 79.39 (39694/50000)
Regular: 0.17867411673069
Epoche: 16; regular: 0.17867411673069: flops 69342064
#Filters: 615, #FLOPs: 30.29M | Top-1: 52.68
Epoch 17
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 79.47 (20548/25856)
Train | Batch (196/196) | Top-1: 79.62 (39808/50000)
Regular: 0.17898514866828918
Epoche: 17; regular: 0.17898514866828918: flops 69342064
#Filters: 611, #FLOPs: 29.99M | Top-1: 56.49
Epoch 18
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 79.46 (20546/25856)
Train | Batch (196/196) | Top-1: 79.39 (39697/50000)
Regular: 0.17991860210895538
Epoche: 18; regular: 0.17991860210895538: flops 69342064
#Filters: 611, #FLOPs: 29.58M | Top-1: 47.42
Epoch 19
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 79.18 (20472/25856)
Train | Batch (196/196) | Top-1: 79.35 (39676/50000)
Regular: 0.17973065376281738
Epoche: 19; regular: 0.17973065376281738: flops 69342064
#Filters: 614, #FLOPs: 29.84M | Top-1: 44.04
Epoch 20
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 79.66 (20596/25856)
Train | Batch (196/196) | Top-1: 79.61 (39804/50000)
Regular: 0.18502649664878845
Epoche: 20; regular: 0.18502649664878845: flops 69342064
#Filters: 613, #FLOPs: 29.79M | Top-1: 48.01
Epoch 21
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.76 (20622/25856)
Train | Batch (196/196) | Top-1: 79.78 (39892/50000)
Regular: 0.1784418821334839
Epoche: 21; regular: 0.1784418821334839: flops 69342064
#Filters: 610, #FLOPs: 29.89M | Top-1: 70.03
Epoch 22
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 79.25 (20491/25856)
Train | Batch (196/196) | Top-1: 79.37 (39686/50000)
Regular: 0.1779748499393463
Epoche: 22; regular: 0.1779748499393463: flops 69342064
#Filters: 607, #FLOPs: 29.74M | Top-1: 62.78
Epoch 23
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 79.92 (20663/25856)
Train | Batch (196/196) | Top-1: 79.68 (39839/50000)
Regular: 0.17652449011802673
Epoche: 23; regular: 0.17652449011802673: flops 69342064
#Filters: 606, #FLOPs: 29.58M | Top-1: 42.00
Epoch 24
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.92 (20664/25856)
Train | Batch (196/196) | Top-1: 79.75 (39875/50000)
Regular: 0.17584489285945892
Epoche: 24; regular: 0.17584489285945892: flops 69342064
#Filters: 603, #FLOPs: 29.58M | Top-1: 57.02
Epoch 25
Train | Batch (1/196) | Top-1: 72.66 (186/256)
Train | Batch (101/196) | Top-1: 79.74 (20618/25856)
Train | Batch (196/196) | Top-1: 79.68 (39842/50000)
Regular: 0.1744225025177002
Epoche: 25; regular: 0.1744225025177002: flops 69342064
#Filters: 603, #FLOPs: 29.89M | Top-1: 38.32
Epoch 26
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.75 (20621/25856)
Train | Batch (196/196) | Top-1: 79.65 (39825/50000)
Regular: 0.17501527070999146
Epoche: 26; regular: 0.17501527070999146: flops 69342064
#Filters: 602, #FLOPs: 29.79M | Top-1: 65.69
Epoch 27
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.69 (20604/25856)
Train | Batch (196/196) | Top-1: 79.87 (39933/50000)
Regular: 0.17385171353816986
Epoche: 27; regular: 0.17385171353816986: flops 69342064
#Filters: 600, #FLOPs: 29.79M | Top-1: 58.42
Epoch 28
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 79.83 (20640/25856)
Train | Batch (196/196) | Top-1: 79.66 (39832/50000)
Regular: 0.1721772998571396
Epoche: 28; regular: 0.1721772998571396: flops 69342064
#Filters: 593, #FLOPs: 29.38M | Top-1: 53.55
Epoch 29
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 79.55 (20569/25856)
Train | Batch (196/196) | Top-1: 79.40 (39698/50000)
Regular: 0.17135240137577057
Epoche: 29; regular: 0.17135240137577057: flops 69342064
#Filters: 597, #FLOPs: 29.23M | Top-1: 39.97
Drin!!
Layers that will be prunned: [(1, 2), (3, 2), (5, 4), (7, 2), (9, 1), (11, 26), (12, 12), (13, 15), (14, 12), (15, 6), (16, 12), (17, 10), (18, 12), (19, 31), (20, 12), (21, 56), (22, 56), (23, 57), (24, 56), (25, 58), (26, 56), (27, 65), (28, 56), (29, 58), (30, 56)]
Prunning filters..
Layer index: 12; Pruned filters: 12
Layer index: 14; Pruned filters: 12
Layer index: 16; Pruned filters: 12
Layer index: 18; Pruned filters: 12
Layer index: 20; Pruned filters: 12
Layer index: 22; Pruned filters: 56
Layer index: 24; Pruned filters: 56
Layer index: 26; Pruned filters: 56
Layer index: 28; Pruned filters: 56
Layer index: 30; Pruned filters: 56
Layer index: 1; Pruned filters: 2
Layer index: 3; Pruned filters: 2
Layer index: 5; Pruned filters: 1
Layer index: 5; Pruned filters: 3
Layer index: 7; Pruned filters: 2
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 24
Layer index: 13; Pruned filters: 5
Layer index: 13; Pruned filters: 10
Layer index: 15; Pruned filters: 6
Layer index: 17; Pruned filters: 3
Layer index: 17; Pruned filters: 7
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 28
Layer index: 21; Pruned filters: 56
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 56
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 56
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 49
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 52
Target (flops): 69.306M
After Pruning | FLOPs: 11.354M | #Params: 0.091M
2.511431363316917
After Growth | FLOPs: 71.125M | #Params: 0.573M
I: 2
flops: 71125280
Before Pruning | FLOPs: 71.125M | #Params: 0.573M
Epoch 0
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.92 (20663/25856)
Train | Batch (196/196) | Top-1: 79.52 (39762/50000)
Regular: 1.3603944778442383
Epoche: 0; regular: 1.3603944778442383: flops 71125280
#Filters: 1467, #FLOPs: 59.12M | Top-1: 56.60
Epoch 1
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.76 (20624/25856)
Train | Batch (196/196) | Top-1: 79.41 (39707/50000)
Regular: 0.7973113059997559
Epoche: 1; regular: 0.7973113059997559: flops 71125280
#Filters: 1463, #FLOPs: 58.95M | Top-1: 43.14
Epoch 2
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 79.52 (20561/25856)
Train | Batch (196/196) | Top-1: 79.48 (39738/50000)
Regular: 0.36565542221069336
Epoche: 2; regular: 0.36565542221069336: flops 71125280
#Filters: 1268, #FLOPs: 49.82M | Top-1: 47.69
Epoch 3
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 79.88 (20654/25856)
Train | Batch (196/196) | Top-1: 80.07 (40037/50000)
Regular: 0.2233850359916687
Epoche: 3; regular: 0.2233850359916687: flops 71125280
#Filters: 1268, #FLOPs: 50.60M | Top-1: 55.75
Epoch 4
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 78.50 (20298/25856)
Train | Batch (196/196) | Top-1: 78.64 (39319/50000)
Regular: 0.1933509111404419
Epoche: 4; regular: 0.1933509111404419: flops 71125280
#Filters: 1266, #FLOPs: 49.81M | Top-1: 22.36
Epoch 5
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 79.23 (20485/25856)
Train | Batch (196/196) | Top-1: 79.38 (39692/50000)
Regular: 0.18682058155536652
Epoche: 5; regular: 0.18682058155536652: flops 71125280
#Filters: 1221, #FLOPs: 49.95M | Top-1: 62.02
Epoch 6
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 79.38 (20524/25856)
Train | Batch (196/196) | Top-1: 79.50 (39752/50000)
Regular: 0.1769348382949829
Epoche: 6; regular: 0.1769348382949829: flops 71125280
#Filters: 1218, #FLOPs: 49.86M | Top-1: 25.73
Epoch 7
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 78.36 (20260/25856)
Train | Batch (196/196) | Top-1: 78.75 (39375/50000)
Regular: 0.27203649282455444
Epoche: 7; regular: 0.27203649282455444: flops 71125280
#Filters: 1218, #FLOPs: 50.04M | Top-1: 49.33
Epoch 8
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 79.61 (20583/25856)
Train | Batch (196/196) | Top-1: 79.56 (39781/50000)
Regular: 0.1903401017189026
Epoche: 8; regular: 0.1903401017189026: flops 71125280
#Filters: 1220, #FLOPs: 50.14M | Top-1: 62.40
Epoch 9
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 79.68 (20602/25856)
Train | Batch (196/196) | Top-1: 79.49 (39745/50000)
Regular: 0.18894460797309875
Epoche: 9; regular: 0.18894460797309875: flops 71125280
#Filters: 1217, #FLOPs: 49.86M | Top-1: 65.49
Epoch 10
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.56 (20570/25856)
Train | Batch (196/196) | Top-1: 79.59 (39794/50000)
Regular: 0.27045702934265137
Epoche: 10; regular: 0.27045702934265137: flops 71125280
#Filters: 649, #FLOPs: 29.48M | Top-1: 49.24
Epoch 11
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 79.46 (20546/25856)
Train | Batch (196/196) | Top-1: 79.47 (39734/50000)
Regular: 0.2562350630760193
Epoche: 11; regular: 0.2562350630760193: flops 71125280
#Filters: 649, #FLOPs: 29.48M | Top-1: 51.57
Epoch 12
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 79.96 (20674/25856)
Train | Batch (196/196) | Top-1: 79.81 (39903/50000)
Regular: 0.2294052243232727
Epoche: 12; regular: 0.2294052243232727: flops 71125280
#Filters: 646, #FLOPs: 29.90M | Top-1: 38.43
Epoch 13
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.63 (20590/25856)
Train | Batch (196/196) | Top-1: 79.48 (39739/50000)
Regular: 0.23732662200927734
Epoche: 13; regular: 0.23732662200927734: flops 71125280
#Filters: 647, #FLOPs: 29.94M | Top-1: 62.07
Epoch 14
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.79 (20631/25856)
Train | Batch (196/196) | Top-1: 79.55 (39776/50000)
Regular: 0.17895184457302094
Epoche: 14; regular: 0.17895184457302094: flops 71125280
#Filters: 645, #FLOPs: 29.85M | Top-1: 52.27
Epoch 15
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 79.76 (20622/25856)
Train | Batch (196/196) | Top-1: 79.70 (39850/50000)
Regular: 0.17063915729522705
Epoche: 15; regular: 0.17063915729522705: flops 71125280
#Filters: 648, #FLOPs: 29.44M | Top-1: 61.22
Epoch 16
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 80.09 (20708/25856)
Train | Batch (196/196) | Top-1: 79.94 (39971/50000)
Regular: 0.18940840661525726
Epoche: 16; regular: 0.18940840661525726: flops 71125280
#Filters: 648, #FLOPs: 29.99M | Top-1: 53.17
Epoch 17
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 79.75 (20620/25856)
Train | Batch (196/196) | Top-1: 79.63 (39816/50000)
Regular: 0.17328301072120667
Epoche: 17; regular: 0.17328301072120667: flops 71125280
#Filters: 646, #FLOPs: 30.13M | Top-1: 63.30
Epoch 18
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 79.70 (20606/25856)
Train | Batch (196/196) | Top-1: 79.67 (39833/50000)
Regular: 0.16867667436599731
Epoche: 18; regular: 0.16867667436599731: flops 71125280
#Filters: 646, #FLOPs: 30.05M | Top-1: 65.22
Epoch 19
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 79.78 (20627/25856)
Train | Batch (196/196) | Top-1: 79.76 (39881/50000)
Regular: 0.1732373684644699
Epoche: 19; regular: 0.1732373684644699: flops 71125280
#Filters: 645, #FLOPs: 30.05M | Top-1: 49.76
Epoch 20
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 80.00 (20685/25856)
Train | Batch (196/196) | Top-1: 79.92 (39959/50000)
Regular: 0.1646701991558075
Epoche: 20; regular: 0.1646701991558075: flops 71125280
#Filters: 645, #FLOPs: 30.05M | Top-1: 57.29
Epoch 21
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 79.97 (20678/25856)
Train | Batch (196/196) | Top-1: 79.81 (39907/50000)
Regular: 0.17444375157356262
Epoche: 21; regular: 0.17444375157356262: flops 71125280
#Filters: 642, #FLOPs: 29.62M | Top-1: 57.37
Epoch 22
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.35 (20516/25856)
Train | Batch (196/196) | Top-1: 79.51 (39754/50000)
Regular: 0.16947010159492493
Epoche: 22; regular: 0.16947010159492493: flops 71125280
#Filters: 642, #FLOPs: 29.72M | Top-1: 52.32
Epoch 23
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 79.88 (20653/25856)
Train | Batch (196/196) | Top-1: 79.67 (39837/50000)
Regular: 0.17162539064884186
Epoche: 23; regular: 0.17162539064884186: flops 71125280
#Filters: 639, #FLOPs: 29.45M | Top-1: 69.60
Epoch 24
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 79.48 (20550/25856)
Train | Batch (196/196) | Top-1: 79.57 (39785/50000)
Regular: 0.16985099017620087
Epoche: 24; regular: 0.16985099017620087: flops 71125280
#Filters: 637, #FLOPs: 29.45M | Top-1: 42.18
Epoch 25
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 79.72 (20612/25856)
Train | Batch (196/196) | Top-1: 79.76 (39882/50000)
Regular: 0.17068085074424744
Epoche: 25; regular: 0.17068085074424744: flops 71125280
#Filters: 642, #FLOPs: 29.63M | Top-1: 65.10
Epoch 26
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 79.56 (20570/25856)
Train | Batch (196/196) | Top-1: 79.79 (39893/50000)
Regular: 0.17311826348304749
Epoche: 26; regular: 0.17311826348304749: flops 71125280
#Filters: 644, #FLOPs: 29.68M | Top-1: 64.97
Epoch 27
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 79.62 (20586/25856)
Train | Batch (196/196) | Top-1: 79.57 (39787/50000)
Regular: 0.17075876891613007
Epoche: 27; regular: 0.17075876891613007: flops 71125280
#Filters: 644, #FLOPs: 29.68M | Top-1: 58.47
Epoch 28
