no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=1e-08, logger='MorphLogs/logMorphNetFlops1e-8', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=4.0, pruner='FilterPrunnerResNet')
I: 0
flops: 17326400
Before Pruning | FLOPs: 17.326M | #Params: 0.116M
Epoch 0
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 29.13 (7531/25856)
Train | Batch (196/196) | Top-1: 35.38 (17690/50000)
Regular: 0.5428292155265808
Epoche: 0; regular: 0.5428292155265808: flops 17326400
#Filters: 568, #FLOPs: 17.33M | Top-1: 41.13
Epoch 1
Train | Batch (1/196) | Top-1: 45.31 (116/256)
Train | Batch (101/196) | Top-1: 50.76 (13125/25856)
Train | Batch (196/196) | Top-1: 53.86 (26930/50000)
Regular: 0.3002820611000061
Epoche: 1; regular: 0.3002820611000061: flops 17326400
#Filters: 554, #FLOPs: 16.29M | Top-1: 50.79
Epoch 2
Train | Batch (1/196) | Top-1: 57.42 (147/256)
Train | Batch (101/196) | Top-1: 61.59 (15926/25856)
Train | Batch (196/196) | Top-1: 63.31 (31657/50000)
Regular: 0.18073490262031555
Epoche: 2; regular: 0.18073490262031555: flops 17326400
#Filters: 537, #FLOPs: 15.26M | Top-1: 51.87
Epoch 3
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 67.55 (17467/25856)
Train | Batch (196/196) | Top-1: 68.15 (34074/50000)
Regular: 0.13806737959384918
Epoche: 3; regular: 0.13806737959384918: flops 17326400
#Filters: 532, #FLOPs: 15.11M | Top-1: 61.73
Epoch 4
Train | Batch (1/196) | Top-1: 68.36 (175/256)
Train | Batch (101/196) | Top-1: 70.16 (18140/25856)
Train | Batch (196/196) | Top-1: 70.56 (35281/50000)
Regular: 0.12153097987174988
Epoche: 4; regular: 0.12153097987174988: flops 17326400
#Filters: 525, #FLOPs: 14.71M | Top-1: 48.73
Epoch 5
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 72.21 (18670/25856)
Train | Batch (196/196) | Top-1: 72.34 (36172/50000)
Regular: 0.11615097522735596
Epoche: 5; regular: 0.11615097522735596: flops 17326400
#Filters: 518, #FLOPs: 14.51M | Top-1: 59.82
Epoch 6
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 73.30 (18953/25856)
Train | Batch (196/196) | Top-1: 73.67 (36833/50000)
Regular: 0.11433395743370056
Epoche: 6; regular: 0.11433395743370056: flops 17326400
#Filters: 522, #FLOPs: 14.87M | Top-1: 42.82
Epoch 7
Train | Batch (1/196) | Top-1: 71.09 (182/256)
Train | Batch (101/196) | Top-1: 74.95 (19380/25856)
Train | Batch (196/196) | Top-1: 74.71 (37357/50000)
Regular: 0.1138685792684555
Epoche: 7; regular: 0.1138685792684555: flops 17326400
#Filters: 519, #FLOPs: 14.64M | Top-1: 52.05
Epoch 8
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 75.34 (19481/25856)
Train | Batch (196/196) | Top-1: 75.28 (37638/50000)
Regular: 0.11264742910861969
Epoche: 8; regular: 0.11264742910861969: flops 17326400
#Filters: 513, #FLOPs: 14.38M | Top-1: 60.00
Epoch 9
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 75.84 (19610/25856)
Train | Batch (196/196) | Top-1: 75.99 (37996/50000)
Regular: 0.11206533759832382
Epoche: 9; regular: 0.11206533759832382: flops 17326400
#Filters: 510, #FLOPs: 14.19M | Top-1: 61.09
Epoch 10
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 76.39 (19751/25856)
Train | Batch (196/196) | Top-1: 76.38 (38192/50000)
Regular: 0.11222794651985168
Epoche: 10; regular: 0.11222794651985168: flops 17326400
#Filters: 513, #FLOPs: 14.49M | Top-1: 71.41
Epoch 11
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 77.10 (19935/25856)
Train | Batch (196/196) | Top-1: 77.01 (38503/50000)
Regular: 0.11198706179857254
Epoche: 11; regular: 0.11198706179857254: flops 17326400
#Filters: 509, #FLOPs: 14.08M | Top-1: 57.49
Epoch 12
Train | Batch (1/196) | Top-1: 75.78 (194/256)
Train | Batch (101/196) | Top-1: 77.56 (20054/25856)
Train | Batch (196/196) | Top-1: 77.40 (38701/50000)
Regular: 0.11157771199941635
Epoche: 12; regular: 0.11157771199941635: flops 17326400
#Filters: 504, #FLOPs: 14.01M | Top-1: 65.64
Epoch 13
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 77.70 (20089/25856)
Train | Batch (196/196) | Top-1: 77.72 (38858/50000)
Regular: 0.11221002042293549
Epoche: 13; regular: 0.11221002042293549: flops 17326400
#Filters: 502, #FLOPs: 13.79M | Top-1: 64.58
Epoch 14
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 78.06 (20184/25856)
Train | Batch (196/196) | Top-1: 78.08 (39038/50000)
Regular: 0.1129310205578804
Epoche: 14; regular: 0.1129310205578804: flops 17326400
#Filters: 506, #FLOPs: 14.06M | Top-1: 58.64
Epoch 15
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 78.51 (20300/25856)
Train | Batch (196/196) | Top-1: 78.36 (39181/50000)
Regular: 0.11214421689510345
Epoche: 15; regular: 0.11214421689510345: flops 17326400
#Filters: 502, #FLOPs: 13.75M | Top-1: 59.26
Epoch 16
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 78.83 (20382/25856)
Train | Batch (196/196) | Top-1: 78.70 (39349/50000)
Regular: 0.11172451078891754
Epoche: 16; regular: 0.11172451078891754: flops 17326400
#Filters: 503, #FLOPs: 13.84M | Top-1: 56.45
Epoch 17
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 78.43 (20278/25856)
Train | Batch (196/196) | Top-1: 78.55 (39276/50000)
Regular: 0.11290781199932098
Epoche: 17; regular: 0.11290781199932098: flops 17326400
#Filters: 505, #FLOPs: 13.97M | Top-1: 56.11
Epoch 18
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 78.82 (20379/25856)
Train | Batch (196/196) | Top-1: 79.06 (39531/50000)
Regular: 0.11168000847101212
Epoche: 18; regular: 0.11168000847101212: flops 17326400
#Filters: 502, #FLOPs: 13.81M | Top-1: 65.07
Epoch 19
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 79.03 (20435/25856)
Train | Batch (196/196) | Top-1: 78.85 (39423/50000)
Regular: 0.11215081810951233
Epoche: 19; regular: 0.11215081810951233: flops 17326400
#Filters: 495, #FLOPs: 13.47M | Top-1: 57.25
Epoch 20
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 79.22 (20482/25856)
Train | Batch (196/196) | Top-1: 79.42 (39711/50000)
Regular: 0.11218956857919693
Epoche: 20; regular: 0.11218956857919693: flops 17326400
#Filters: 498, #FLOPs: 13.68M | Top-1: 60.13
Epoch 21
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 79.43 (20537/25856)
Train | Batch (196/196) | Top-1: 79.44 (39720/50000)
Regular: 0.11255484074354172
Epoche: 21; regular: 0.11255484074354172: flops 17326400
#Filters: 495, #FLOPs: 13.49M | Top-1: 62.19
Epoch 22
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 80.07 (20703/25856)
Train | Batch (196/196) | Top-1: 79.69 (39843/50000)
Regular: 0.11186475306749344
Epoche: 22; regular: 0.11186475306749344: flops 17326400
#Filters: 497, #FLOPs: 13.66M | Top-1: 61.14
Epoch 23
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.80 (20632/25856)
Train | Batch (196/196) | Top-1: 79.61 (39805/50000)
Regular: 0.11324203014373779
Epoche: 23; regular: 0.11324203014373779: flops 17326400
#Filters: 498, #FLOPs: 13.58M | Top-1: 58.71
Epoch 24
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.52 (20561/25856)
Train | Batch (196/196) | Top-1: 79.70 (39852/50000)
Regular: 0.11198166757822037
Epoche: 24; regular: 0.11198166757822037: flops 17326400
#Filters: 500, #FLOPs: 13.70M | Top-1: 68.62
Epoch 25
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 80.18 (20731/25856)
Train | Batch (196/196) | Top-1: 80.14 (40069/50000)
Regular: 0.11073211580514908
Epoche: 25; regular: 0.11073211580514908: flops 17326400
#Filters: 493, #FLOPs: 13.47M | Top-1: 67.40
Epoch 26
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 79.88 (20655/25856)
Train | Batch (196/196) | Top-1: 79.82 (39910/50000)
Regular: 0.1099267527461052
Epoche: 26; regular: 0.1099267527461052: flops 17326400
#Filters: 488, #FLOPs: 13.31M | Top-1: 70.98
Epoch 27
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 79.88 (20655/25856)
Train | Batch (196/196) | Top-1: 80.07 (40036/50000)
Regular: 0.11272414773702621
Epoche: 27; regular: 0.11272414773702621: flops 17326400
#Filters: 487, #FLOPs: 13.27M | Top-1: 56.43
Epoch 28
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 80.19 (20733/25856)
Train | Batch (196/196) | Top-1: 80.13 (40066/50000)
Regular: 0.11616379767656326
Epoche: 28; regular: 0.11616379767656326: flops 17326400
#Filters: 488, #FLOPs: 13.33M | Top-1: 66.08
Epoch 29
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.58 (20834/25856)
Train | Batch (196/196) | Top-1: 80.55 (40273/50000)
Regular: 0.11420869082212448
Epoche: 29; regular: 0.11420869082212448: flops 17326400
#Filters: 488, #FLOPs: 13.35M | Top-1: 63.87
Drin!!
Layers that will be prunned: [(1, 7), (3, 7), (5, 6), (7, 7), (9, 7), (11, 2), (13, 10), (15, 13), (17, 12), (19, 1), (27, 4), (29, 2)]
Prunning filters..
Layer index: 1; Pruned filters: 4
Layer index: 1; Pruned filters: 3
Layer index: 3; Pruned filters: 3
Layer index: 3; Pruned filters: 4
Layer index: 5; Pruned filters: 3
Layer index: 5; Pruned filters: 3
Layer index: 7; Pruned filters: 4
Layer index: 7; Pruned filters: 3
Layer index: 9; Pruned filters: 4
Layer index: 9; Pruned filters: 3
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 3
Layer index: 13; Pruned filters: 3
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 2
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 8
Layer index: 15; Pruned filters: 4
Layer index: 17; Pruned filters: 3
Layer index: 17; Pruned filters: 3
Layer index: 17; Pruned filters: 2
Layer index: 17; Pruned filters: 3
Layer index: 17; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Target (flops): 69.306M
After Pruning | FLOPs: 9.327M | #Params: 0.097M
2.7467539584359013
After Growth | FLOPs: 69.342M | #Params: 0.726M
I: 1
flops: 69342064
Before Pruning | FLOPs: 69.342M | #Params: 0.726M
Epoch 0
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 80.54 (20824/25856)
Train | Batch (196/196) | Top-1: 80.13 (40066/50000)
Regular: 1.5138561725616455
Epoche: 0; regular: 1.5138561725616455: flops 69342064
#Filters: 1325, #FLOPs: 66.81M | Top-1: 61.71
Epoch 1
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.22 (20484/25856)
Train | Batch (196/196) | Top-1: 78.98 (39488/50000)
Regular: 0.6992611885070801
Epoche: 1; regular: 0.6992611885070801: flops 69342064
#Filters: 1244, #FLOPs: 60.47M | Top-1: 61.65
Epoch 2
Train | Batch (1/196) | Top-1: 75.78 (194/256)
Train | Batch (101/196) | Top-1: 79.42 (20534/25856)
Train | Batch (196/196) | Top-1: 79.05 (39527/50000)
Regular: 0.2921568751335144
Epoche: 2; regular: 0.2921568751335144: flops 69342064
#Filters: 976, #FLOPs: 47.65M | Top-1: 62.47
Epoch 3
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 78.76 (20363/25856)
Train | Batch (196/196) | Top-1: 78.97 (39483/50000)
Regular: 0.2151648849248886
Epoche: 3; regular: 0.2151648849248886: flops 69342064
#Filters: 975, #FLOPs: 47.55M | Top-1: 61.52
Epoch 4
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 78.69 (20347/25856)
Train | Batch (196/196) | Top-1: 78.92 (39459/50000)
Regular: 0.19151845574378967
Epoche: 4; regular: 0.19151845574378967: flops 69342064
#Filters: 632, #FLOPs: 30.70M | Top-1: 38.83
Epoch 5
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 79.04 (20436/25856)
Train | Batch (196/196) | Top-1: 78.98 (39492/50000)
Regular: 0.18814930319786072
Epoche: 5; regular: 0.18814930319786072: flops 69342064
#Filters: 627, #FLOPs: 30.45M | Top-1: 57.73
Epoch 6
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.28 (20498/25856)
Train | Batch (196/196) | Top-1: 79.27 (39635/50000)
Regular: 0.18766656517982483
Epoche: 6; regular: 0.18766656517982483: flops 69342064
#Filters: 627, #FLOPs: 30.95M | Top-1: 59.41
Epoch 7
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 79.22 (20483/25856)
Train | Batch (196/196) | Top-1: 79.22 (39609/50000)
Regular: 0.1871410608291626
Epoche: 7; regular: 0.1871410608291626: flops 69342064
#Filters: 627, #FLOPs: 30.70M | Top-1: 48.70
Epoch 8
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 79.58 (20577/25856)
Train | Batch (196/196) | Top-1: 79.50 (39751/50000)
Regular: 0.18356573581695557
Epoche: 8; regular: 0.18356573581695557: flops 69342064
#Filters: 624, #FLOPs: 30.80M | Top-1: 55.89
Epoch 9
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.38 (20524/25856)
Train | Batch (196/196) | Top-1: 79.14 (39572/50000)
Regular: 0.18247860670089722
Epoche: 9; regular: 0.18247860670089722: flops 69342064
#Filters: 615, #FLOPs: 29.94M | Top-1: 58.95
Epoch 10
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.25 (20491/25856)
Train | Batch (196/196) | Top-1: 79.44 (39721/50000)
Regular: 0.18152128159999847
Epoche: 10; regular: 0.18152128159999847: flops 69342064
#Filters: 618, #FLOPs: 30.24M | Top-1: 64.68
Epoch 11
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 79.46 (20546/25856)
Train | Batch (196/196) | Top-1: 79.32 (39658/50000)
Regular: 0.18841922283172607
Epoche: 11; regular: 0.18841922283172607: flops 69342064
#Filters: 623, #FLOPs: 30.39M | Top-1: 48.25
Epoch 12
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 79.60 (20582/25856)
Train | Batch (196/196) | Top-1: 79.17 (39583/50000)
Regular: 0.18078339099884033
Epoche: 12; regular: 0.18078339099884033: flops 69342064
#Filters: 619, #FLOPs: 30.09M | Top-1: 50.90
Epoch 13
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 79.49 (20552/25856)
Train | Batch (196/196) | Top-1: 79.50 (39751/50000)
Regular: 0.17838361859321594
Epoche: 13; regular: 0.17838361859321594: flops 69342064
#Filters: 613, #FLOPs: 29.99M | Top-1: 59.72
Epoch 14
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 79.68 (20603/25856)
Train | Batch (196/196) | Top-1: 79.50 (39749/50000)
Regular: 0.17908422648906708
Epoche: 14; regular: 0.17908422648906708: flops 69342064
#Filters: 619, #FLOPs: 30.45M | Top-1: 66.62
Epoch 15
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 79.33 (20511/25856)
Train | Batch (196/196) | Top-1: 79.41 (39704/50000)
Regular: 0.17785948514938354
Epoche: 15; regular: 0.17785948514938354: flops 69342064
#Filters: 615, #FLOPs: 30.34M | Top-1: 58.04
Epoch 16
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 79.44 (20540/25856)
Train | Batch (196/196) | Top-1: 79.39 (39694/50000)
Regular: 0.17867411673069
Epoche: 16; regular: 0.17867411673069: flops 69342064
#Filters: 615, #FLOPs: 30.29M | Top-1: 52.68
Epoch 17
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 79.47 (20548/25856)
Train | Batch (196/196) | Top-1: 79.62 (39808/50000)
Regular: 0.17898514866828918
Epoche: 17; regular: 0.17898514866828918: flops 69342064
#Filters: 611, #FLOPs: 29.99M | Top-1: 56.49
Epoch 18
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 79.46 (20546/25856)
Train | Batch (196/196) | Top-1: 79.39 (39697/50000)
Regular: 0.17991860210895538
Epoche: 18; regular: 0.17991860210895538: flops 69342064
#Filters: 611, #FLOPs: 29.58M | Top-1: 47.42
Epoch 19
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 79.18 (20472/25856)
Train | Batch (196/196) | Top-1: 79.35 (39676/50000)
Regular: 0.17973065376281738
Epoche: 19; regular: 0.17973065376281738: flops 69342064
#Filters: 614, #FLOPs: 29.84M | Top-1: 44.04
Epoch 20
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 79.66 (20596/25856)
Train | Batch (196/196) | Top-1: 79.61 (39804/50000)
Regular: 0.18502649664878845
Epoche: 20; regular: 0.18502649664878845: flops 69342064
#Filters: 613, #FLOPs: 29.79M | Top-1: 48.01
Epoch 21
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.76 (20622/25856)
Train | Batch (196/196) | Top-1: 79.78 (39892/50000)
Regular: 0.1784418821334839
Epoche: 21; regular: 0.1784418821334839: flops 69342064
#Filters: 610, #FLOPs: 29.89M | Top-1: 70.03
Epoch 22
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 79.25 (20491/25856)
Train | Batch (196/196) | Top-1: 79.37 (39686/50000)
Regular: 0.1779748499393463
Epoche: 22; regular: 0.1779748499393463: flops 69342064
#Filters: 607, #FLOPs: 29.74M | Top-1: 62.78
Epoch 23
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 79.92 (20663/25856)
Train | Batch (196/196) | Top-1: 79.68 (39839/50000)
Regular: 0.17652449011802673
Epoche: 23; regular: 0.17652449011802673: flops 69342064
#Filters: 606, #FLOPs: 29.58M | Top-1: 42.00
Epoch 24
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.92 (20664/25856)
Train | Batch (196/196) | Top-1: 79.75 (39875/50000)
Regular: 0.17584489285945892
Epoche: 24; regular: 0.17584489285945892: flops 69342064
#Filters: 603, #FLOPs: 29.58M | Top-1: 57.02
Epoch 25
Train | Batch (1/196) | Top-1: 72.66 (186/256)
Train | Batch (101/196) | Top-1: 79.74 (20618/25856)
Train | Batch (196/196) | Top-1: 79.68 (39842/50000)
Regular: 0.1744225025177002
Epoche: 25; regular: 0.1744225025177002: flops 69342064
#Filters: 603, #FLOPs: 29.89M | Top-1: 38.32
Epoch 26
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.75 (20621/25856)
Train | Batch (196/196) | Top-1: 79.65 (39825/50000)
Regular: 0.17501527070999146
Epoche: 26; regular: 0.17501527070999146: flops 69342064
#Filters: 602, #FLOPs: 29.79M | Top-1: 65.69
Epoch 27
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.69 (20604/25856)
Train | Batch (196/196) | Top-1: 79.87 (39933/50000)
Regular: 0.17385171353816986
Epoche: 27; regular: 0.17385171353816986: flops 69342064
#Filters: 600, #FLOPs: 29.79M | Top-1: 58.42
Epoch 28
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 79.83 (20640/25856)
Train | Batch (196/196) | Top-1: 79.66 (39832/50000)
Regular: 0.1721772998571396
Epoche: 28; regular: 0.1721772998571396: flops 69342064
#Filters: 593, #FLOPs: 29.38M | Top-1: 53.55
Epoch 29
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 79.55 (20569/25856)
Train | Batch (196/196) | Top-1: 79.40 (39698/50000)
Regular: 0.17135240137577057
Epoche: 29; regular: 0.17135240137577057: flops 69342064
#Filters: 597, #FLOPs: 29.23M | Top-1: 39.97
Drin!!
Layers that will be prunned: [(1, 2), (3, 2), (5, 4), (7, 2), (9, 1), (11, 26), (12, 12), (13, 15), (14, 12), (15, 6), (16, 12), (17, 10), (18, 12), (19, 31), (20, 12), (21, 56), (22, 56), (23, 57), (24, 56), (25, 58), (26, 56), (27, 65), (28, 56), (29, 58), (30, 56)]
Prunning filters..
Layer index: 12; Pruned filters: 12
Layer index: 14; Pruned filters: 12
Layer index: 16; Pruned filters: 12
Layer index: 18; Pruned filters: 12
Layer index: 20; Pruned filters: 12
Layer index: 22; Pruned filters: 56
Layer index: 24; Pruned filters: 56
Layer index: 26; Pruned filters: 56
Layer index: 28; Pruned filters: 56
Layer index: 30; Pruned filters: 56
Layer index: 1; Pruned filters: 2
Layer index: 3; Pruned filters: 2
Layer index: 5; Pruned filters: 1
Layer index: 5; Pruned filters: 3
Layer index: 7; Pruned filters: 2
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 24
Layer index: 13; Pruned filters: 5
Layer index: 13; Pruned filters: 10
Layer index: 15; Pruned filters: 6
Layer index: 17; Pruned filters: 3
Layer index: 17; Pruned filters: 7
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 28
Layer index: 21; Pruned filters: 56
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 56
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 56
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 49
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 52
Target (flops): 69.306M
After Pruning | FLOPs: 11.354M | #Params: 0.091M
2.511431363316917
After Growth | FLOPs: 71.125M | #Params: 0.573M
I: 2
flops: 71125280
Before Pruning | FLOPs: 71.125M | #Params: 0.573M
Epoch 0
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.92 (20663/25856)
Train | Batch (196/196) | Top-1: 79.52 (39762/50000)
Regular: 1.3603944778442383
Epoche: 0; regular: 1.3603944778442383: flops 71125280
#Filters: 1467, #FLOPs: 59.12M | Top-1: 56.60
Epoch 1
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.76 (20624/25856)
Train | Batch (196/196) | Top-1: 79.41 (39707/50000)
Regular: 0.7973113059997559
Epoche: 1; regular: 0.7973113059997559: flops 71125280
#Filters: 1463, #FLOPs: 58.95M | Top-1: 43.14
Epoch 2
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 79.52 (20561/25856)
Train | Batch (196/196) | Top-1: 79.48 (39738/50000)
Regular: 0.36565542221069336
Epoche: 2; regular: 0.36565542221069336: flops 71125280
#Filters: 1268, #FLOPs: 49.82M | Top-1: 47.69
Epoch 3
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 79.88 (20654/25856)
Train | Batch (196/196) | Top-1: 80.07 (40037/50000)
Regular: 0.2233850359916687
Epoche: 3; regular: 0.2233850359916687: flops 71125280
#Filters: 1268, #FLOPs: 50.60M | Top-1: 55.75
Epoch 4
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 78.50 (20298/25856)
Train | Batch (196/196) | Top-1: 78.64 (39319/50000)
Regular: 0.1933509111404419
Epoche: 4; regular: 0.1933509111404419: flops 71125280
#Filters: 1266, #FLOPs: 49.81M | Top-1: 22.36
Epoch 5
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 79.23 (20485/25856)
Train | Batch (196/196) | Top-1: 79.38 (39692/50000)
Regular: 0.18682058155536652
Epoche: 5; regular: 0.18682058155536652: flops 71125280
#Filters: 1221, #FLOPs: 49.95M | Top-1: 62.02
Epoch 6
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 79.38 (20524/25856)
Train | Batch (196/196) | Top-1: 79.50 (39752/50000)
Regular: 0.1769348382949829
Epoche: 6; regular: 0.1769348382949829: flops 71125280
#Filters: 1218, #FLOPs: 49.86M | Top-1: 25.73
Epoch 7
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 78.36 (20260/25856)
Train | Batch (196/196) | Top-1: 78.75 (39375/50000)
Regular: 0.27203649282455444
Epoche: 7; regular: 0.27203649282455444: flops 71125280
#Filters: 1218, #FLOPs: 50.04M | Top-1: 49.33
Epoch 8
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 79.61 (20583/25856)
Train | Batch (196/196) | Top-1: 79.56 (39781/50000)
Regular: 0.1903401017189026
Epoche: 8; regular: 0.1903401017189026: flops 71125280
#Filters: 1220, #FLOPs: 50.14M | Top-1: 62.40
Epoch 9
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 79.68 (20602/25856)
Train | Batch (196/196) | Top-1: 79.49 (39745/50000)
Regular: 0.18894460797309875
Epoche: 9; regular: 0.18894460797309875: flops 71125280
#Filters: 1217, #FLOPs: 49.86M | Top-1: 65.49
Epoch 10
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 79.56 (20570/25856)
Train | Batch (196/196) | Top-1: 79.59 (39794/50000)
Regular: 0.27045702934265137
Epoche: 10; regular: 0.27045702934265137: flops 71125280
#Filters: 649, #FLOPs: 29.48M | Top-1: 49.24
Epoch 11
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 79.46 (20546/25856)
Train | Batch (196/196) | Top-1: 79.47 (39734/50000)
Regular: 0.2562350630760193
Epoche: 11; regular: 0.2562350630760193: flops 71125280
#Filters: 649, #FLOPs: 29.48M | Top-1: 51.57
Epoch 12
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 79.96 (20674/25856)
Train | Batch (196/196) | Top-1: 79.81 (39903/50000)
Regular: 0.2294052243232727
Epoche: 12; regular: 0.2294052243232727: flops 71125280
#Filters: 646, #FLOPs: 29.90M | Top-1: 38.43
Epoch 13
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.63 (20590/25856)
Train | Batch (196/196) | Top-1: 79.48 (39739/50000)
Regular: 0.23732662200927734
Epoche: 13; regular: 0.23732662200927734: flops 71125280
#Filters: 647, #FLOPs: 29.94M | Top-1: 62.07
Epoch 14
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.79 (20631/25856)
Train | Batch (196/196) | Top-1: 79.55 (39776/50000)
Regular: 0.17895184457302094
Epoche: 14; regular: 0.17895184457302094: flops 71125280
#Filters: 645, #FLOPs: 29.85M | Top-1: 52.27
Epoch 15
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 79.76 (20622/25856)
Train | Batch (196/196) | Top-1: 79.70 (39850/50000)
Regular: 0.17063915729522705
Epoche: 15; regular: 0.17063915729522705: flops 71125280
#Filters: 648, #FLOPs: 29.44M | Top-1: 61.22
Epoch 16
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 80.09 (20708/25856)
Train | Batch (196/196) | Top-1: 79.94 (39971/50000)
Regular: 0.18940840661525726
Epoche: 16; regular: 0.18940840661525726: flops 71125280
#Filters: 648, #FLOPs: 29.99M | Top-1: 53.17
Epoch 17
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 79.75 (20620/25856)
Train | Batch (196/196) | Top-1: 79.63 (39816/50000)
Regular: 0.17328301072120667
Epoche: 17; regular: 0.17328301072120667: flops 71125280
#Filters: 646, #FLOPs: 30.13M | Top-1: 63.30
Epoch 18
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 79.70 (20606/25856)
Train | Batch (196/196) | Top-1: 79.67 (39833/50000)
Regular: 0.16867667436599731
Epoche: 18; regular: 0.16867667436599731: flops 71125280
#Filters: 646, #FLOPs: 30.05M | Top-1: 65.22
Epoch 19
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 79.78 (20627/25856)
Train | Batch (196/196) | Top-1: 79.76 (39881/50000)
Regular: 0.1732373684644699
Epoche: 19; regular: 0.1732373684644699: flops 71125280
#Filters: 645, #FLOPs: 30.05M | Top-1: 49.76
Epoch 20
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 80.00 (20685/25856)
Train | Batch (196/196) | Top-1: 79.92 (39959/50000)
Regular: 0.1646701991558075
Epoche: 20; regular: 0.1646701991558075: flops 71125280
#Filters: 645, #FLOPs: 30.05M | Top-1: 57.29
Epoch 21
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 79.97 (20678/25856)
Train | Batch (196/196) | Top-1: 79.81 (39907/50000)
Regular: 0.17444375157356262
Epoche: 21; regular: 0.17444375157356262: flops 71125280
#Filters: 642, #FLOPs: 29.62M | Top-1: 57.37
Epoch 22
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.35 (20516/25856)
Train | Batch (196/196) | Top-1: 79.51 (39754/50000)
Regular: 0.16947010159492493
Epoche: 22; regular: 0.16947010159492493: flops 71125280
#Filters: 642, #FLOPs: 29.72M | Top-1: 52.32
Epoch 23
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 79.88 (20653/25856)
Train | Batch (196/196) | Top-1: 79.67 (39837/50000)
Regular: 0.17162539064884186
Epoche: 23; regular: 0.17162539064884186: flops 71125280
#Filters: 639, #FLOPs: 29.45M | Top-1: 69.60
Epoch 24
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 79.48 (20550/25856)
Train | Batch (196/196) | Top-1: 79.57 (39785/50000)
Regular: 0.16985099017620087
Epoche: 24; regular: 0.16985099017620087: flops 71125280
#Filters: 637, #FLOPs: 29.45M | Top-1: 42.18
Epoch 25
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 79.72 (20612/25856)
Train | Batch (196/196) | Top-1: 79.76 (39882/50000)
Regular: 0.17068085074424744
Epoche: 25; regular: 0.17068085074424744: flops 71125280
#Filters: 642, #FLOPs: 29.63M | Top-1: 65.10
Epoch 26
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 79.56 (20570/25856)
Train | Batch (196/196) | Top-1: 79.79 (39893/50000)
Regular: 0.17311826348304749
Epoche: 26; regular: 0.17311826348304749: flops 71125280
#Filters: 644, #FLOPs: 29.68M | Top-1: 64.97
Epoch 27
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 79.62 (20586/25856)
Train | Batch (196/196) | Top-1: 79.57 (39787/50000)
Regular: 0.17075876891613007
Epoche: 27; regular: 0.17075876891613007: flops 71125280
#Filters: 644, #FLOPs: 29.68M | Top-1: 58.47
Epoch 28
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 79.82 (20639/25856)
Train | Batch (196/196) | Top-1: 79.98 (39988/50000)
Regular: 0.16995295882225037
Epoche: 28; regular: 0.16995295882225037: flops 71125280
#Filters: 643, #FLOPs: 30.27M | Top-1: 27.21
Epoch 29
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 79.81 (20636/25856)
Train | Batch (196/196) | Top-1: 79.76 (39881/50000)
Regular: 0.16852791607379913
Epoche: 29; regular: 0.16852791607379913: flops 71125280
#Filters: 642, #FLOPs: 29.57M | Top-1: 51.61
Drin!!
Layers that will be prunned: [(0, 23), (1, 2), (2, 23), (3, 2), (4, 23), (5, 2), (6, 23), (7, 2), (8, 23), (9, 3), (10, 23), (11, 18), (12, 48), (13, 1), (14, 48), (15, 3), (16, 48), (17, 1), (18, 48), (19, 17), (20, 48), (21, 48), (22, 48), (23, 48), (24, 48), (25, 45), (26, 48), (27, 21), (28, 48), (29, 37), (30, 48)]
Prunning filters..
Layer index: 0; Pruned filters: 23
Layer index: 2; Pruned filters: 23
Layer index: 4; Pruned filters: 23
Layer index: 6; Pruned filters: 23
Layer index: 8; Pruned filters: 23
Layer index: 10; Pruned filters: 23
Layer index: 12; Pruned filters: 48
Layer index: 14; Pruned filters: 48
Layer index: 16; Pruned filters: 48
Layer index: 18; Pruned filters: 48
Layer index: 20; Pruned filters: 48
Layer index: 22; Pruned filters: 48
Layer index: 24; Pruned filters: 48
Layer index: 26; Pruned filters: 48
Layer index: 28; Pruned filters: 48
Layer index: 30; Pruned filters: 48
Layer index: 1; Pruned filters: 2
Layer index: 3; Pruned filters: 2
Layer index: 5; Pruned filters: 2
Layer index: 7; Pruned filters: 2
Layer index: 9; Pruned filters: 3
Layer index: 11; Pruned filters: 18
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 3
Layer index: 17; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 15
Layer index: 21; Pruned filters: 48
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 47
Layer index: 25; Pruned filters: 45
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 18
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 36
Target (flops): 69.306M
After Pruning | FLOPs: 12.829M | #Params: 0.091M
2.3720882176278013
After Growth | FLOPs: 67.766M | #Params: 0.508M
I: 3
flops: 67766008
Before Pruning | FLOPs: 67.766M | #Params: 0.508M
Epoch 0
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 80.47 (20807/25856)
Train | Batch (196/196) | Top-1: 79.89 (39946/50000)
Regular: 1.1190720796585083
Epoche: 0; regular: 1.1190720796585083: flops 67766008
#Filters: 1514, #FLOPs: 56.47M | Top-1: 74.96
Epoch 1
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 79.64 (20593/25856)
Train | Batch (196/196) | Top-1: 79.85 (39924/50000)
Regular: 0.6517416834831238
Epoche: 1; regular: 0.6517416834831238: flops 67766008
#Filters: 1513, #FLOPs: 57.04M | Top-1: 67.68
Epoch 2
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 80.13 (20718/25856)
Train | Batch (196/196) | Top-1: 80.19 (40094/50000)
Regular: 0.36369603872299194
Epoche: 2; regular: 0.36369603872299194: flops 67766008
#Filters: 1345, #FLOPs: 49.77M | Top-1: 57.44
Epoch 3
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 79.87 (20651/25856)
Train | Batch (196/196) | Top-1: 79.85 (39926/50000)
Regular: 0.25058162212371826
Epoche: 3; regular: 0.25058162212371826: flops 67766008
#Filters: 1343, #FLOPs: 49.60M | Top-1: 29.55
Epoch 4
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 79.73 (20615/25856)
Train | Batch (196/196) | Top-1: 79.65 (39825/50000)
Regular: 0.6175222992897034
Epoche: 4; regular: 0.6175222992897034: flops 67766008
#Filters: 1342, #FLOPs: 49.42M | Top-1: 65.00
Epoch 5
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 80.05 (20699/25856)
Train | Batch (196/196) | Top-1: 79.91 (39954/50000)
Regular: 1.241243600845337
Epoche: 5; regular: 1.241243600845337: flops 67766008
#Filters: 1298, #FLOPs: 49.29M | Top-1: 46.52
Epoch 6
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 80.33 (20769/25856)
Train | Batch (196/196) | Top-1: 79.94 (39968/50000)
Regular: 0.9929633736610413
Epoche: 6; regular: 0.9929633736610413: flops 67766008
#Filters: 1297, #FLOPs: 49.25M | Top-1: 52.88
Epoch 7
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 80.26 (20753/25856)
Train | Batch (196/196) | Top-1: 80.02 (40012/50000)
Regular: 0.7628647089004517
Epoche: 7; regular: 0.7628647089004517: flops 67766008
#Filters: 1299, #FLOPs: 49.47M | Top-1: 50.54
Epoch 8
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 80.33 (20769/25856)
Train | Batch (196/196) | Top-1: 79.98 (39989/50000)
Regular: 0.5576772689819336
Epoche: 8; regular: 0.5576772689819336: flops 67766008
#Filters: 1297, #FLOPs: 49.38M | Top-1: 48.52
Epoch 9
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 80.40 (20788/25856)
Train | Batch (196/196) | Top-1: 80.12 (40059/50000)
Regular: 0.37576767802238464
Epoche: 9; regular: 0.37576767802238464: flops 67766008
#Filters: 1296, #FLOPs: 49.20M | Top-1: 57.14
Epoch 10
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 79.86 (20648/25856)
Train | Batch (196/196) | Top-1: 79.67 (39837/50000)
Regular: 0.20393739640712738
Epoche: 10; regular: 0.20393739640712738: flops 67766008
#Filters: 634, #FLOPs: 28.76M | Top-1: 38.85
Epoch 11
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 79.94 (20670/25856)
Train | Batch (196/196) | Top-1: 79.98 (39989/50000)
Regular: 0.2110898196697235
Epoche: 11; regular: 0.2110898196697235: flops 67766008
#Filters: 635, #FLOPs: 28.94M | Top-1: 54.48
Epoch 12
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 79.87 (20651/25856)
Train | Batch (196/196) | Top-1: 80.03 (40016/50000)
Regular: 0.16835415363311768
Epoche: 12; regular: 0.16835415363311768: flops 67766008
#Filters: 634, #FLOPs: 29.64M | Top-1: 55.06
Epoch 13
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 80.00 (20686/25856)
Train | Batch (196/196) | Top-1: 80.14 (40069/50000)
Regular: 0.16141696274280548
Epoche: 13; regular: 0.16141696274280548: flops 67766008
#Filters: 634, #FLOPs: 29.51M | Top-1: 67.15
Epoch 14
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 80.31 (20765/25856)
Train | Batch (196/196) | Top-1: 80.17 (40087/50000)
Regular: 0.1702205091714859
Epoche: 14; regular: 0.1702205091714859: flops 67766008
#Filters: 633, #FLOPs: 29.46M | Top-1: 56.04
Epoch 15
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 79.94 (20669/25856)
Train | Batch (196/196) | Top-1: 80.23 (40117/50000)
Regular: 0.15538273751735687
Epoche: 15; regular: 0.15538273751735687: flops 67766008
#Filters: 633, #FLOPs: 29.46M | Top-1: 19.17
Epoch 16
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 79.83 (20640/25856)
Train | Batch (196/196) | Top-1: 80.08 (40042/50000)
Regular: 0.17540989816188812
Epoche: 16; regular: 0.17540989816188812: flops 67766008
#Filters: 637, #FLOPs: 29.02M | Top-1: 59.18
Epoch 17
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 80.12 (20716/25856)
Train | Batch (196/196) | Top-1: 79.94 (39972/50000)
Regular: 0.1644163280725479
Epoche: 17; regular: 0.1644163280725479: flops 67766008
#Filters: 636, #FLOPs: 28.98M | Top-1: 62.34
Epoch 18
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 80.16 (20726/25856)
Train | Batch (196/196) | Top-1: 80.17 (40087/50000)
Regular: 0.16048023104667664
Epoche: 18; regular: 0.16048023104667664: flops 67766008
#Filters: 635, #FLOPs: 29.68M | Top-1: 47.50
Epoch 19
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 79.91 (20662/25856)
Train | Batch (196/196) | Top-1: 80.02 (40012/50000)
Regular: 0.16041019558906555
Epoche: 19; regular: 0.16041019558906555: flops 67766008
#Filters: 635, #FLOPs: 29.68M | Top-1: 59.48
Epoch 20
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 79.79 (20630/25856)
Train | Batch (196/196) | Top-1: 79.75 (39875/50000)
Regular: 0.1754305511713028
Epoche: 20; regular: 0.1754305511713028: flops 67766008
#Filters: 636, #FLOPs: 29.72M | Top-1: 62.64
Epoch 21
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 80.16 (20725/25856)
Train | Batch (196/196) | Top-1: 80.31 (40153/50000)
Regular: 0.19343984127044678
Epoche: 21; regular: 0.19343984127044678: flops 67766008
#Filters: 634, #FLOPs: 29.51M | Top-1: 67.01
Epoch 22
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 80.25 (20749/25856)
Train | Batch (196/196) | Top-1: 80.14 (40068/50000)
Regular: 0.16791655123233795
Epoche: 22; regular: 0.16791655123233795: flops 67766008
#Filters: 634, #FLOPs: 29.51M | Top-1: 52.03
Epoch 23
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 80.05 (20699/25856)
Train | Batch (196/196) | Top-1: 80.08 (40038/50000)
Regular: 0.15555226802825928
Epoche: 23; regular: 0.15555226802825928: flops 67766008
#Filters: 634, #FLOPs: 29.37M | Top-1: 63.34
Epoch 24
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 80.59 (20838/25856)
Train | Batch (196/196) | Top-1: 80.36 (40179/50000)
Regular: 0.1568278670310974
Epoche: 24; regular: 0.1568278670310974: flops 67766008
#Filters: 635, #FLOPs: 29.42M | Top-1: 39.43
Epoch 25
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 80.13 (20718/25856)
Train | Batch (196/196) | Top-1: 80.01 (40003/50000)
Regular: 0.15653574466705322
Epoche: 25; regular: 0.15653574466705322: flops 67766008
#Filters: 632, #FLOPs: 29.42M | Top-1: 64.61
Epoch 26
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 80.07 (20702/25856)
Train | Batch (196/196) | Top-1: 80.01 (40006/50000)
Regular: 0.1574281007051468
Epoche: 26; regular: 0.1574281007051468: flops 67766008
#Filters: 634, #FLOPs: 28.63M | Top-1: 26.32
Epoch 27
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 80.11 (20714/25856)
Train | Batch (196/196) | Top-1: 80.18 (40089/50000)
Regular: 0.1671774983406067
Epoche: 27; regular: 0.1671774983406067: flops 67766008
#Filters: 634, #FLOPs: 28.63M | Top-1: 22.28
Epoch 28
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 80.50 (20814/25856)
Train | Batch (196/196) | Top-1: 80.37 (40185/50000)
Regular: 0.19102244079113007
Epoche: 28; regular: 0.19102244079113007: flops 67766008
#Filters: 635, #FLOPs: 28.80M | Top-1: 43.18
Epoch 29
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.02 (20689/25856)
Train | Batch (196/196) | Top-1: 80.24 (40122/50000)
Regular: 0.15961894392967224
Epoche: 29; regular: 0.15961894392967224: flops 67766008
#Filters: 634, #FLOPs: 28.63M | Top-1: 58.87
Drin!!
Layers that will be prunned: [(0, 44), (1, 1), (2, 44), (4, 44), (5, 1), (6, 44), (7, 1), (8, 44), (9, 3), (10, 44), (11, 18), (12, 44), (13, 3), (14, 44), (15, 3), (16, 44), (17, 3), (18, 44), (19, 14), (20, 44), (21, 44), (22, 44), (23, 41), (24, 44), (25, 41), (26, 44), (27, 13), (28, 44), (29, 33), (30, 44)]
Prunning filters..
Layer index: 0; Pruned filters: 44
Layer index: 2; Pruned filters: 44
Layer index: 4; Pruned filters: 44
Layer index: 6; Pruned filters: 44
Layer index: 8; Pruned filters: 44
Layer index: 10; Pruned filters: 44
Layer index: 12; Pruned filters: 44
Layer index: 14; Pruned filters: 44
Layer index: 16; Pruned filters: 44
Layer index: 18; Pruned filters: 44
Layer index: 20; Pruned filters: 44
Layer index: 22; Pruned filters: 44
Layer index: 24; Pruned filters: 44
Layer index: 26; Pruned filters: 44
Layer index: 28; Pruned filters: 44
Layer index: 30; Pruned filters: 44
Layer index: 1; Pruned filters: 1
Layer index: 5; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 3
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 16
Layer index: 13; Pruned filters: 3
Layer index: 15; Pruned filters: 3
Layer index: 17; Pruned filters: 3
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 12
Layer index: 21; Pruned filters: 44
Layer index: 23; Pruned filters: 41
Layer index: 25; Pruned filters: 41
Layer index: 27; Pruned filters: 13
Layer index: 29; Pruned filters: 33
Target (flops): 69.306M
After Pruning | FLOPs: 12.608M | #Params: 0.088M
2.3940160953107785
After Growth | FLOPs: 68.924M | #Params: 0.502M
I: 4
flops: 68923778
Before Pruning | FLOPs: 68.924M | #Params: 0.502M
Epoch 0
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 80.29 (20760/25856)
Train | Batch (196/196) | Top-1: 80.32 (40159/50000)
Regular: 1.1106113195419312
Epoche: 0; regular: 1.1106113195419312: flops 68923778
#Filters: 1523, #FLOPs: 57.13M | Top-1: 64.42
Epoch 1
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 80.00 (20686/25856)
Train | Batch (196/196) | Top-1: 80.09 (40046/50000)
Regular: 0.6543148756027222
Epoche: 1; regular: 0.6543148756027222: flops 68923778
#Filters: 1523, #FLOPs: 56.99M | Top-1: 47.49
Epoch 2
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 80.38 (20784/25856)
Train | Batch (196/196) | Top-1: 80.32 (40162/50000)
Regular: 0.37163710594177246
Epoche: 2; regular: 0.37163710594177246: flops 68923778
#Filters: 1350, #FLOPs: 50.07M | Top-1: 44.41
Epoch 3
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 79.72 (20612/25856)
Train | Batch (196/196) | Top-1: 80.29 (40147/50000)
Regular: 0.27442076802253723
Epoche: 3; regular: 0.27442076802253723: flops 68923778
#Filters: 1352, #FLOPs: 50.30M | Top-1: 62.45
Epoch 4
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.29 (20759/25856)
Train | Batch (196/196) | Top-1: 80.14 (40069/50000)
Regular: 0.19137917459011078
Epoche: 4; regular: 0.19137917459011078: flops 68923778
#Filters: 1352, #FLOPs: 50.30M | Top-1: 56.03
Epoch 5
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 79.99 (20682/25856)
Train | Batch (196/196) | Top-1: 80.18 (40090/50000)
Regular: 0.1743239164352417
Epoche: 5; regular: 0.1743239164352417: flops 68923778
#Filters: 1353, #FLOPs: 49.59M | Top-1: 53.10
Epoch 6
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 80.07 (20704/25856)
Train | Batch (196/196) | Top-1: 80.24 (40119/50000)
Regular: 0.16437487304210663
Epoche: 6; regular: 0.16437487304210663: flops 68923778
#Filters: 1307, #FLOPs: 50.16M | Top-1: 61.34
Epoch 7
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 80.06 (20700/25856)
Train | Batch (196/196) | Top-1: 80.17 (40083/50000)
Regular: 0.16526027023792267
Epoche: 7; regular: 0.16526027023792267: flops 68923778
#Filters: 1307, #FLOPs: 49.54M | Top-1: 68.06
Epoch 8
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 80.11 (20712/25856)
Train | Batch (196/196) | Top-1: 80.48 (40241/50000)
Regular: 0.16158299148082733
Epoche: 8; regular: 0.16158299148082733: flops 68923778
#Filters: 1308, #FLOPs: 49.59M | Top-1: 53.76
Epoch 9
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 79.79 (20630/25856)
Train | Batch (196/196) | Top-1: 80.03 (40014/50000)
Regular: 0.16654209792613983
Epoche: 9; regular: 0.16654209792613983: flops 68923778
#Filters: 1306, #FLOPs: 50.25M | Top-1: 46.36
Epoch 10
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 80.45 (20801/25856)
Train | Batch (196/196) | Top-1: 80.37 (40187/50000)
Regular: 0.15685734152793884
Epoche: 10; regular: 0.15685734152793884: flops 68923778
#Filters: 632, #FLOPs: 29.53M | Top-1: 28.49
Epoch 11
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 80.06 (20700/25856)
Train | Batch (196/196) | Top-1: 80.19 (40094/50000)
Regular: 0.24127139151096344
Epoche: 11; regular: 0.24127139151096344: flops 68923778
#Filters: 632, #FLOPs: 29.53M | Top-1: 44.57
Epoch 12
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 80.33 (20769/25856)
Train | Batch (196/196) | Top-1: 80.43 (40215/50000)
Regular: 0.15629026293754578
Epoche: 12; regular: 0.15629026293754578: flops 68923778
#Filters: 631, #FLOPs: 29.49M | Top-1: 53.61
Epoch 13
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 80.35 (20776/25856)
Train | Batch (196/196) | Top-1: 80.33 (40166/50000)
Regular: 0.1544579118490219
Epoche: 13; regular: 0.1544579118490219: flops 68923778
#Filters: 629, #FLOPs: 29.27M | Top-1: 34.91
Epoch 14
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 80.30 (20762/25856)
Train | Batch (196/196) | Top-1: 80.52 (40258/50000)
Regular: 0.15683862566947937
Epoche: 14; regular: 0.15683862566947937: flops 68923778
#Filters: 629, #FLOPs: 29.13M | Top-1: 45.16
Epoch 15
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 79.59 (20578/25856)
Train | Batch (196/196) | Top-1: 79.96 (39982/50000)
Regular: 0.1652928739786148
Epoche: 15; regular: 0.1652928739786148: flops 68923778
#Filters: 630, #FLOPs: 29.31M | Top-1: 21.16
Epoch 16
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 80.09 (20709/25856)
Train | Batch (196/196) | Top-1: 80.35 (40177/50000)
Regular: 0.17045332491397858
Epoche: 16; regular: 0.17045332491397858: flops 68923778
#Filters: 631, #FLOPs: 28.47M | Top-1: 47.21
Epoch 17
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 80.16 (20725/25856)
Train | Batch (196/196) | Top-1: 80.16 (40080/50000)
Regular: 0.1800982803106308
Epoche: 17; regular: 0.1800982803106308: flops 68923778
#Filters: 629, #FLOPs: 29.13M | Top-1: 67.73
Epoch 18
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 80.38 (20782/25856)
Train | Batch (196/196) | Top-1: 80.44 (40220/50000)
Regular: 0.15408988296985626
Epoche: 18; regular: 0.15408988296985626: flops 68923778
#Filters: 631, #FLOPs: 29.36M | Top-1: 59.36
Epoch 19
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 80.72 (20872/25856)
Train | Batch (196/196) | Top-1: 80.56 (40281/50000)
Regular: 0.15493197739124298
Epoche: 19; regular: 0.15493197739124298: flops 68923778
#Filters: 630, #FLOPs: 29.36M | Top-1: 40.19
Epoch 20
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 80.42 (20793/25856)
Train | Batch (196/196) | Top-1: 80.43 (40217/50000)
Regular: 0.15672573447227478
Epoche: 20; regular: 0.15672573447227478: flops 68923778
#Filters: 631, #FLOPs: 29.53M | Top-1: 44.69
Epoch 21
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 80.74 (20875/25856)
Train | Batch (196/196) | Top-1: 80.53 (40265/50000)
Regular: 0.15799911320209503
Epoche: 21; regular: 0.15799911320209503: flops 68923778
#Filters: 629, #FLOPs: 29.18M | Top-1: 37.67
Epoch 22
Train | Batch (1/196) | Top-1: 76.56 (196/256)
Train | Batch (101/196) | Top-1: 80.58 (20835/25856)
Train | Batch (196/196) | Top-1: 80.36 (40180/50000)
Regular: 0.16918574273586273
Epoche: 22; regular: 0.16918574273586273: flops 68923778
#Filters: 630, #FLOPs: 28.47M | Top-1: 58.35
Epoch 23
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 80.52 (20818/25856)
Train | Batch (196/196) | Top-1: 80.32 (40161/50000)
Regular: 0.15659409761428833
Epoche: 23; regular: 0.15659409761428833: flops 68923778
#Filters: 629, #FLOPs: 28.29M | Top-1: 55.69
Epoch 24
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 80.40 (20787/25856)
Train | Batch (196/196) | Top-1: 80.23 (40116/50000)
Regular: 0.229597806930542
Epoche: 24; regular: 0.229597806930542: flops 68923778
#Filters: 630, #FLOPs: 28.47M | Top-1: 54.75
Epoch 25
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 80.03 (20692/25856)
Train | Batch (196/196) | Top-1: 80.42 (40208/50000)
Regular: 0.22281105816364288
Epoche: 25; regular: 0.22281105816364288: flops 68923778
#Filters: 630, #FLOPs: 29.36M | Top-1: 38.13
Epoch 26
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 80.34 (20772/25856)
Train | Batch (196/196) | Top-1: 80.37 (40184/50000)
Regular: 0.15203939378261566
Epoche: 26; regular: 0.15203939378261566: flops 68923778
#Filters: 628, #FLOPs: 29.13M | Top-1: 48.06
Epoch 27
Train | Batch (1/196) | Top-1: 75.39 (193/256)
Train | Batch (101/196) | Top-1: 80.34 (20773/25856)
Train | Batch (196/196) | Top-1: 80.15 (40077/50000)
Regular: 0.15732048451900482
Epoche: 27; regular: 0.15732048451900482: flops 68923778
#Filters: 629, #FLOPs: 29.18M | Top-1: 48.89
Epoch 28
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 80.51 (20816/25856)
Train | Batch (196/196) | Top-1: 80.28 (40140/50000)
Regular: 0.1566832959651947
Epoche: 28; regular: 0.1566832959651947: flops 68923778
#Filters: 630, #FLOPs: 28.47M | Top-1: 65.02
Epoch 29
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 80.50 (20815/25856)
Train | Batch (196/196) | Top-1: 80.47 (40234/50000)
Regular: 0.1565195918083191
Epoche: 29; regular: 0.1565195918083191: flops 68923778
#Filters: 631, #FLOPs: 28.65M | Top-1: 58.76
Drin!!
Layers that will be prunned: [(0, 45), (1, 1), (2, 45), (3, 3), (4, 45), (5, 1), (6, 45), (7, 1), (8, 45), (9, 3), (10, 45), (11, 14), (12, 45), (13, 3), (14, 45), (15, 3), (16, 45), (17, 3), (18, 45), (19, 8), (20, 45), (21, 45), (22, 45), (23, 43), (24, 45), (25, 42), (26, 45), (27, 11), (28, 45), (29, 31), (30, 45)]
Prunning filters..
Layer index: 0; Pruned filters: 45
Layer index: 2; Pruned filters: 45
Layer index: 4; Pruned filters: 45
Layer index: 6; Pruned filters: 45
Layer index: 8; Pruned filters: 45
Layer index: 10; Pruned filters: 45
Layer index: 12; Pruned filters: 45
Layer index: 14; Pruned filters: 45
Layer index: 16; Pruned filters: 45
Layer index: 18; Pruned filters: 45
Layer index: 20; Pruned filters: 45
Layer index: 22; Pruned filters: 45
Layer index: 24; Pruned filters: 45
Layer index: 26; Pruned filters: 45
Layer index: 28; Pruned filters: 45
Layer index: 30; Pruned filters: 45
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 3
Layer index: 5; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 3
Layer index: 11; Pruned filters: 14
Layer index: 13; Pruned filters: 3
Layer index: 15; Pruned filters: 3
Layer index: 17; Pruned filters: 3
Layer index: 19; Pruned filters: 8
Layer index: 21; Pruned filters: 45
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 42
Layer index: 25; Pruned filters: 42
Layer index: 27; Pruned filters: 11
Layer index: 29; Pruned filters: 31
Target (flops): 69.306M
After Pruning | FLOPs: 12.423M | #Params: 0.086M
2.4127638791874984
After Growth | FLOPs: 68.037M | #Params: 0.496M
I: 5
flops: 68036738
Before Pruning | FLOPs: 68.037M | #Params: 0.496M
Epoch 0
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 80.87 (20909/25856)
Train | Batch (196/196) | Top-1: 80.53 (40266/50000)
Regular: 1.0908268690109253
Epoche: 0; regular: 1.0908268690109253: flops 68036738
#Filters: 1520, #FLOPs: 56.33M | Top-1: 54.83
Epoch 1
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 80.93 (20924/25856)
Train | Batch (196/196) | Top-1: 80.44 (40218/50000)
Regular: 0.6509584784507751
Epoche: 1; regular: 0.6509584784507751: flops 68036738
#Filters: 1519, #FLOPs: 56.42M | Top-1: 54.11
Epoch 2
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 80.34 (20774/25856)
Train | Batch (196/196) | Top-1: 80.22 (40109/50000)
Regular: 0.3814293146133423
Epoche: 2; regular: 0.3814293146133423: flops 68036738
#Filters: 1349, #FLOPs: 48.74M | Top-1: 54.03
Epoch 3
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.17 (20730/25856)
Train | Batch (196/196) | Top-1: 80.13 (40066/50000)
Regular: 0.25215983390808105
Epoche: 3; regular: 0.25215983390808105: flops 68036738
#Filters: 1350, #FLOPs: 48.79M | Top-1: 65.40
Epoch 4
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 80.73 (20874/25856)
Train | Batch (196/196) | Top-1: 80.59 (40293/50000)
Regular: 0.20235513150691986
Epoche: 4; regular: 0.20235513150691986: flops 68036738
#Filters: 1348, #FLOPs: 49.45M | Top-1: 55.25
Epoch 5
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 80.81 (20894/25856)
Train | Batch (196/196) | Top-1: 80.55 (40276/50000)
Regular: 0.1899482160806656
Epoche: 5; regular: 0.1899482160806656: flops 68036738
#Filters: 1349, #FLOPs: 49.50M | Top-1: 44.95
Epoch 6
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 80.84 (20902/25856)
Train | Batch (196/196) | Top-1: 80.67 (40334/50000)
Regular: 0.1728525161743164
Epoche: 6; regular: 0.1728525161743164: flops 68036738
#Filters: 1305, #FLOPs: 48.79M | Top-1: 36.77
Epoch 7
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 80.36 (20778/25856)
Train | Batch (196/196) | Top-1: 80.28 (40141/50000)
Regular: 0.5152406692504883
Epoche: 7; regular: 0.5152406692504883: flops 68036738
#Filters: 1304, #FLOPs: 48.74M | Top-1: 57.98
Epoch 8
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 80.32 (20768/25856)
Train | Batch (196/196) | Top-1: 80.34 (40172/50000)
Regular: 0.3736285865306854
Epoche: 8; regular: 0.3736285865306854: flops 68036738
#Filters: 1303, #FLOPs: 49.45M | Top-1: 56.94
Epoch 9
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 80.48 (20810/25856)
Train | Batch (196/196) | Top-1: 80.05 (40023/50000)
Regular: 0.2025747150182724
Epoche: 9; regular: 0.2025747150182724: flops 68036738
#Filters: 1302, #FLOPs: 49.41M | Top-1: 42.23
Epoch 10
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 79.76 (20622/25856)
Train | Batch (196/196) | Top-1: 79.74 (39869/50000)
Regular: 0.5373659729957581
Epoche: 10; regular: 0.5373659729957581: flops 68036738
#Filters: 629, #FLOPs: 28.24M | Top-1: 45.54
Epoch 11
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 80.68 (20861/25856)
Train | Batch (196/196) | Top-1: 79.46 (39729/50000)
Regular: 0.7553281784057617
Epoche: 11; regular: 0.7553281784057617: flops 68036738
#Filters: 630, #FLOPs: 28.28M | Top-1: 45.42
Epoch 12
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 79.66 (20598/25856)
Train | Batch (196/196) | Top-1: 79.67 (39836/50000)
Regular: 0.5621543526649475
Epoche: 12; regular: 0.5621543526649475: flops 68036738
#Filters: 628, #FLOPs: 28.82M | Top-1: 56.23
Epoch 13
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 80.43 (20796/25856)
Train | Batch (196/196) | Top-1: 80.12 (40061/50000)
Regular: 0.33479243516921997
Epoche: 13; regular: 0.33479243516921997: flops 68036738
#Filters: 628, #FLOPs: 28.82M | Top-1: 40.39
Epoch 14
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.73 (20614/25856)
Train | Batch (196/196) | Top-1: 80.04 (40018/50000)
Regular: 0.2596578896045685
Epoche: 14; regular: 0.2596578896045685: flops 68036738
#Filters: 629, #FLOPs: 28.11M | Top-1: 63.27
Epoch 15
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 79.84 (20644/25856)
Train | Batch (196/196) | Top-1: 80.00 (39998/50000)
Regular: 0.18330204486846924
Epoche: 15; regular: 0.18330204486846924: flops 68036738
#Filters: 629, #FLOPs: 28.11M | Top-1: 55.18
Epoch 16
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 80.67 (20857/25856)
Train | Batch (196/196) | Top-1: 80.04 (40022/50000)
Regular: 0.25079506635665894
Epoche: 16; regular: 0.25079506635665894: flops 68036738
#Filters: 627, #FLOPs: 28.02M | Top-1: 46.95
Epoch 17
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 79.66 (20598/25856)
Train | Batch (196/196) | Top-1: 79.74 (39872/50000)
Regular: 0.41995006799697876
Epoche: 17; regular: 0.41995006799697876: flops 68036738
#Filters: 629, #FLOPs: 28.11M | Top-1: 41.40
Epoch 18
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 80.11 (20714/25856)
Train | Batch (196/196) | Top-1: 80.20 (40098/50000)
Regular: 0.24727478623390198
Epoche: 18; regular: 0.24727478623390198: flops 68036738
#Filters: 628, #FLOPs: 28.82M | Top-1: 49.12
Epoch 19
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.43 (20797/25856)
Train | Batch (196/196) | Top-1: 80.16 (40082/50000)
Regular: 0.20296326279640198
Epoche: 19; regular: 0.20296326279640198: flops 68036738
#Filters: 628, #FLOPs: 28.82M | Top-1: 61.23
Epoch 20
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 79.82 (20638/25856)
Train | Batch (196/196) | Top-1: 79.88 (39940/50000)
Regular: 0.41633668541908264
Epoche: 20; regular: 0.41633668541908264: flops 68036738
#Filters: 629, #FLOPs: 28.99M | Top-1: 59.70
Epoch 21
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 80.51 (20816/25856)
Train | Batch (196/196) | Top-1: 80.29 (40146/50000)
Regular: 0.5507569313049316
Epoche: 21; regular: 0.5507569313049316: flops 68036738
#Filters: 629, #FLOPs: 28.99M | Top-1: 71.80
Epoch 22
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.59 (20837/25856)
Train | Batch (196/196) | Top-1: 80.56 (40279/50000)
Regular: 0.22731000185012817
Epoche: 22; regular: 0.22731000185012817: flops 68036738
#Filters: 626, #FLOPs: 28.73M | Top-1: 47.14
Epoch 23
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 80.21 (20740/25856)
Train | Batch (196/196) | Top-1: 80.27 (40133/50000)
Regular: 0.15180440247058868
Epoche: 23; regular: 0.15180440247058868: flops 68036738
#Filters: 627, #FLOPs: 28.77M | Top-1: 35.35
Epoch 24
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 79.89 (20657/25856)
Train | Batch (196/196) | Top-1: 79.81 (39905/50000)
Regular: 0.1810404360294342
Epoche: 24; regular: 0.1810404360294342: flops 68036738
#Filters: 628, #FLOPs: 28.82M | Top-1: 44.69
Epoch 25
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 80.26 (20753/25856)
Train | Batch (196/196) | Top-1: 80.52 (40262/50000)
Regular: 0.15268883109092712
Epoche: 25; regular: 0.15268883109092712: flops 68036738
#Filters: 627, #FLOPs: 28.77M | Top-1: 27.74
Epoch 26
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 80.09 (20708/25856)
Train | Batch (196/196) | Top-1: 80.13 (40065/50000)
Regular: 0.2491055279970169
Epoche: 26; regular: 0.2491055279970169: flops 68036738
#Filters: 628, #FLOPs: 28.82M | Top-1: 69.79
Epoch 27
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 80.32 (20768/25856)
Train | Batch (196/196) | Top-1: 80.30 (40152/50000)
Regular: 0.16109593212604523
Epoche: 27; regular: 0.16109593212604523: flops 68036738
#Filters: 627, #FLOPs: 28.77M | Top-1: 52.35
Epoch 28
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 79.91 (20662/25856)
Train | Batch (196/196) | Top-1: 79.95 (39975/50000)
Regular: 0.7333847284317017
Epoche: 28; regular: 0.7333847284317017: flops 68036738
#Filters: 627, #FLOPs: 28.77M | Top-1: 42.23
Epoch 29
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 80.00 (20684/25856)
Train | Batch (196/196) | Top-1: 80.13 (40067/50000)
Regular: 0.5490120053291321
Epoche: 29; regular: 0.5490120053291321: flops 68036738
#Filters: 627, #FLOPs: 28.77M | Top-1: 57.34
Drin!!
Layers that will be prunned: [(0, 45), (1, 1), (2, 45), (3, 3), (4, 45), (6, 45), (7, 1), (8, 45), (9, 3), (10, 45), (11, 16), (12, 45), (13, 3), (14, 45), (15, 3), (16, 45), (17, 3), (18, 45), (19, 6), (20, 45), (21, 45), (22, 45), (23, 41), (24, 45), (25, 42), (26, 45), (27, 12), (28, 45), (29, 31), (30, 45)]
Prunning filters..
Layer index: 0; Pruned filters: 45
Layer index: 2; Pruned filters: 45
Layer index: 4; Pruned filters: 45
Layer index: 6; Pruned filters: 45
Layer index: 8; Pruned filters: 45
Layer index: 10; Pruned filters: 45
Layer index: 12; Pruned filters: 45
Layer index: 14; Pruned filters: 45
Layer index: 16; Pruned filters: 45
Layer index: 18; Pruned filters: 45
Layer index: 20; Pruned filters: 45
Layer index: 22; Pruned filters: 45
Layer index: 24; Pruned filters: 45
Layer index: 26; Pruned filters: 45
Layer index: 28; Pruned filters: 45
Layer index: 30; Pruned filters: 45
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 3
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 3
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 14
Layer index: 13; Pruned filters: 3
Layer index: 15; Pruned filters: 3
Layer index: 17; Pruned filters: 3
Layer index: 19; Pruned filters: 6
Layer index: 21; Pruned filters: 45
Layer index: 23; Pruned filters: 41
Layer index: 25; Pruned filters: 42
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 11
Layer index: 29; Pruned filters: 31
Target (flops): 69.306M
After Pruning | FLOPs: 12.682M | #Params: 0.085M
2.386639025554724
After Growth | FLOPs: 69.254M | #Params: 0.482M
Epoch 0
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 81.34 (21032/25856)
Train | Batch (196/196) | Top-1: 81.39 (40697/50000)
Regular: nan
Epoche: 0; regular: nan: flops 69254392
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
#Filters: 1554, #FLOPs: 64.00M | Top-1: 73.29
Epoch 1
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 81.74 (21134/25856)
Train | Batch (196/196) | Top-1: 81.49 (40746/50000)
Regular: nan
Epoche: 1; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 73.24
Epoch 2
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 81.61 (21102/25856)
Train | Batch (196/196) | Top-1: 81.80 (40902/50000)
Regular: nan
Epoche: 2; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 65.01
Epoch 3
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 82.28 (21274/25856)
Train | Batch (196/196) | Top-1: 82.29 (41143/50000)
Regular: nan
Epoche: 3; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 71.66
Epoch 4
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 82.50 (21332/25856)
Train | Batch (196/196) | Top-1: 82.37 (41185/50000)
Regular: nan
Epoche: 4; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 68.50
Epoch 5
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 82.58 (21351/25856)
Train | Batch (196/196) | Top-1: 82.34 (41172/50000)
Regular: nan
Epoche: 5; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 78.44
Epoch 6
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 82.33 (21287/25856)
Train | Batch (196/196) | Top-1: 82.26 (41132/50000)
Regular: nan
Epoche: 6; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 65.59
Epoch 7
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 82.68 (21379/25856)
Train | Batch (196/196) | Top-1: 82.39 (41196/50000)
Regular: nan
Epoche: 7; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 74.84
Epoch 8
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 82.48 (21327/25856)
Train | Batch (196/196) | Top-1: 82.38 (41190/50000)
Regular: nan
Epoche: 8; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 64.65
Epoch 9
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 82.61 (21360/25856)
Train | Batch (196/196) | Top-1: 82.69 (41347/50000)
Regular: nan
Epoche: 9; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 75.04
Epoch 10
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 83.08 (21481/25856)
Train | Batch (196/196) | Top-1: 82.62 (41310/50000)
Regular: nan
Epoche: 10; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 73.98
Epoch 11
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 82.96 (21450/25856)
Train | Batch (196/196) | Top-1: 82.59 (41295/50000)
Regular: nan
Epoche: 11; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 71.88
Epoch 12
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 83.01 (21463/25856)
Train | Batch (196/196) | Top-1: 82.92 (41460/50000)
Regular: nan
Epoche: 12; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 73.65
Epoch 13
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.05 (21474/25856)
Train | Batch (196/196) | Top-1: 82.87 (41434/50000)
Regular: nan
Epoche: 13; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 75.19
Epoch 14
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 82.74 (21393/25856)
Train | Batch (196/196) | Top-1: 82.73 (41366/50000)
Regular: nan
Epoche: 14; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 58.31
Epoch 15
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 82.74 (21394/25856)
Train | Batch (196/196) | Top-1: 82.84 (41422/50000)
Regular: nan
Epoche: 15; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 54.34
Epoch 16
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 83.23 (21520/25856)
Train | Batch (196/196) | Top-1: 83.02 (41510/50000)
Regular: nan
Epoche: 16; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 72.62
Epoch 17
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 83.08 (21480/25856)
Train | Batch (196/196) | Top-1: 82.78 (41392/50000)
Regular: nan
Epoche: 17; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 70.68
Epoch 18
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 82.92 (21440/25856)
Train | Batch (196/196) | Top-1: 82.72 (41359/50000)
Regular: nan
Epoche: 18; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 69.97
Epoch 19
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 83.04 (21472/25856)
Train | Batch (196/196) | Top-1: 83.03 (41513/50000)
Regular: nan
Epoche: 19; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 65.54
Epoch 20
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 82.92 (21439/25856)
Train | Batch (196/196) | Top-1: 82.70 (41349/50000)
Regular: nan
Epoche: 20; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 71.62
Epoch 21
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 83.05 (21473/25856)
Train | Batch (196/196) | Top-1: 82.74 (41371/50000)
Regular: nan
Epoche: 21; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 64.66
Epoch 22
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 83.14 (21496/25856)
Train | Batch (196/196) | Top-1: 82.93 (41467/50000)
Regular: nan
Epoche: 22; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 75.72
Epoch 23
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 82.85 (21422/25856)
Train | Batch (196/196) | Top-1: 82.87 (41437/50000)
Regular: nan
Epoche: 23; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 55.50
Epoch 24
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 83.05 (21474/25856)
Train | Batch (196/196) | Top-1: 82.82 (41410/50000)
Regular: nan
Epoche: 24; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 70.69
Epoch 25
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 82.75 (21397/25856)
Train | Batch (196/196) | Top-1: 82.73 (41364/50000)
Regular: nan
Epoche: 25; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 62.67
Epoch 26
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 83.27 (21530/25856)
Train | Batch (196/196) | Top-1: 82.89 (41444/50000)
Regular: nan
Epoche: 26; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 58.41
Epoch 27
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 82.97 (21454/25856)
Train | Batch (196/196) | Top-1: 82.74 (41368/50000)
Regular: nan
Epoche: 27; regular: nan: flops 69254392
#Filters: 1554, #FLOPs: 64.00M | Top-1: 73.39
Epoch 28
