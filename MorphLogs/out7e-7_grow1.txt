no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=7e-07, logger='MorphLogs/logMorphNetFlops7e-7_grow1.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 10.28 (2659/25856)
Train | Batch (196/196) | Top-1: 10.10 (5049/50000)
Regular: 12.284767150878906
Epoche: 0; regular: 12.284767150878906: flops 68862592
#Filters: 796, #FLOPs: 57.32M | Top-1: 10.11
Epoch 1
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 9.98 (2581/25856)
Train | Batch (196/196) | Top-1: 9.88 (4938/50000)
Regular: 4.037823677062988
Epoche: 1; regular: 4.037823677062988: flops 68862592
#Filters: 910, #FLOPs: 62.23M | Top-1: 10.00
Epoch 2
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 9.74 (2518/25856)
Train | Batch (196/196) | Top-1: 9.96 (4980/50000)
Regular: 4.0328497886657715
Epoche: 2; regular: 4.0328497886657715: flops 68862592
#Filters: 884, #FLOPs: 60.99M | Top-1: 10.00
Epoch 3
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 10.06 (2602/25856)
Train | Batch (196/196) | Top-1: 10.09 (5045/50000)
Regular: 4.045732498168945
Epoche: 3; regular: 4.045732498168945: flops 68862592
#Filters: 883, #FLOPs: 60.55M | Top-1: 9.92
Epoch 4
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.42 (2693/25856)
Train | Batch (196/196) | Top-1: 10.26 (5130/50000)
Regular: 4.074751377105713
Epoche: 4; regular: 4.074751377105713: flops 68862592
#Filters: 887, #FLOPs: 61.21M | Top-1: 13.74
Epoch 5
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 10.59 (2739/25856)
Train | Batch (196/196) | Top-1: 10.73 (5367/50000)
Regular: 4.093151092529297
Epoche: 5; regular: 4.093151092529297: flops 68862592
#Filters: 865, #FLOPs: 60.49M | Top-1: 10.35
Epoch 6
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.02 (2590/25856)
Train | Batch (196/196) | Top-1: 10.21 (5103/50000)
Regular: 4.051717758178711
Epoche: 6; regular: 4.051717758178711: flops 68862592
#Filters: 872, #FLOPs: 60.13M | Top-1: 10.00
Epoch 7
Train | Batch (1/196) | Top-1: 6.64 (17/256)
Train | Batch (101/196) | Top-1: 10.89 (2815/25856)
Train | Batch (196/196) | Top-1: 11.00 (5498/50000)
Regular: 4.055624485015869
Epoche: 7; regular: 4.055624485015869: flops 68862592
#Filters: 859, #FLOPs: 59.76M | Top-1: 8.93
Epoch 8
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 11.13 (2878/25856)
Train | Batch (196/196) | Top-1: 11.64 (5819/50000)
Regular: 4.091033458709717
Epoche: 8; regular: 4.091033458709717: flops 68862592
#Filters: 830, #FLOPs: 58.36M | Top-1: 15.55
Epoch 9
Train | Batch (1/196) | Top-1: 14.06 (36/256)
Train | Batch (101/196) | Top-1: 10.82 (2797/25856)
Train | Batch (196/196) | Top-1: 10.49 (5247/50000)
Regular: 4.082545280456543
Epoche: 9; regular: 4.082545280456543: flops 68862592
#Filters: 874, #FLOPs: 60.03M | Top-1: 10.00
Epoch 10
Train | Batch (1/196) | Top-1: 7.42 (19/256)
Train | Batch (101/196) | Top-1: 13.25 (3425/25856)
Train | Batch (196/196) | Top-1: 11.70 (5849/50000)
Regular: 4.2858171463012695
Epoche: 10; regular: 4.2858171463012695: flops 68862592
#Filters: 845, #FLOPs: 58.47M | Top-1: 10.00
Epoch 11
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.09 (2610/25856)
Train | Batch (196/196) | Top-1: 9.83 (4915/50000)
Regular: 4.042229652404785
Epoche: 11; regular: 4.042229652404785: flops 68862592
#Filters: 918, #FLOPs: 62.71M | Top-1: 10.00
Epoch 12
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 9.87 (2553/25856)
Train | Batch (196/196) | Top-1: 9.95 (4974/50000)
Regular: 4.038232326507568
Epoche: 12; regular: 4.038232326507568: flops 68862592
#Filters: 876, #FLOPs: 60.25M | Top-1: 10.00
Epoch 13
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 11.78 (3046/25856)
Train | Batch (196/196) | Top-1: 13.85 (6925/50000)
Regular: 4.333057403564453
Epoche: 13; regular: 4.333057403564453: flops 68862592
#Filters: 829, #FLOPs: 58.50M | Top-1: 17.65
Epoch 14
Train | Batch (1/196) | Top-1: 17.97 (46/256)
Train | Batch (101/196) | Top-1: 16.80 (4345/25856)
Train | Batch (196/196) | Top-1: 15.87 (7937/50000)
Regular: 5.067816734313965
Epoche: 14; regular: 5.067816734313965: flops 68862592
#Filters: 841, #FLOPs: 58.67M | Top-1: 9.99
Epoch 15
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 9.97 (2578/25856)
Train | Batch (196/196) | Top-1: 9.82 (4908/50000)
Regular: 4.031290054321289
Epoche: 15; regular: 4.031290054321289: flops 68862592
#Filters: 921, #FLOPs: 62.43M | Top-1: 10.00
Epoch 16
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 9.89 (2558/25856)
Train | Batch (196/196) | Top-1: 9.95 (4977/50000)
Regular: 4.023270130157471
Epoche: 16; regular: 4.023270130157471: flops 68862592
#Filters: 925, #FLOPs: 62.78M | Top-1: 9.99
Epoch 17
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 9.86 (2549/25856)
Train | Batch (196/196) | Top-1: 10.02 (5012/50000)
Regular: 4.035617351531982
Epoche: 17; regular: 4.035617351531982: flops 68862592
#Filters: 909, #FLOPs: 62.17M | Top-1: 10.47
Epoch 18
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 10.35 (2677/25856)
Train | Batch (196/196) | Top-1: 10.17 (5087/50000)
Regular: 4.020493507385254
Epoche: 18; regular: 4.020493507385254: flops 68862592
#Filters: 899, #FLOPs: 61.47M | Top-1: 10.00
Epoch 19
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 10.21 (2641/25856)
Train | Batch (196/196) | Top-1: 10.14 (5069/50000)
Regular: 4.030167579650879
Epoche: 19; regular: 4.030167579650879: flops 68862592
#Filters: 882, #FLOPs: 60.97M | Top-1: 10.00
Epoch 20
Train | Batch (1/196) | Top-1: 12.11 (31/256)
Train | Batch (101/196) | Top-1: 10.28 (2658/25856)
Train | Batch (196/196) | Top-1: 10.28 (5142/50000)
Regular: 4.03208065032959
Epoche: 20; regular: 4.03208065032959: flops 68862592
#Filters: 858, #FLOPs: 59.98M | Top-1: 10.00
Epoch 21
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.20 (2638/25856)
Train | Batch (196/196) | Top-1: 10.70 (5349/50000)
Regular: 4.03964900970459
Epoche: 21; regular: 4.03964900970459: flops 68862592
#Filters: 837, #FLOPs: 58.76M | Top-1: 10.00
Epoch 22
Train | Batch (1/196) | Top-1: 7.81 (20/256)
Train | Batch (101/196) | Top-1: 10.16 (2627/25856)
Train | Batch (196/196) | Top-1: 10.18 (5090/50000)
Regular: 4.0449700355529785
Epoche: 22; regular: 4.0449700355529785: flops 68862592
#Filters: 869, #FLOPs: 59.81M | Top-1: 10.00
Epoch 23
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.52 (2721/25856)
Train | Batch (196/196) | Top-1: 10.29 (5146/50000)
Regular: 4.031461238861084
Epoche: 23; regular: 4.031461238861084: flops 68862592
#Filters: 823, #FLOPs: 58.71M | Top-1: 10.00
Epoch 24
Train | Batch (1/196) | Top-1: 14.06 (36/256)
Train | Batch (101/196) | Top-1: 10.58 (2736/25856)
Train | Batch (196/196) | Top-1: 10.81 (5407/50000)
Regular: 4.028677940368652
Epoche: 24; regular: 4.028677940368652: flops 68862592
#Filters: 816, #FLOPs: 57.18M | Top-1: 13.59
Epoch 25
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 12.89 (3333/25856)
Train | Batch (196/196) | Top-1: 11.59 (5793/50000)
Regular: 4.148392677307129
Epoche: 25; regular: 4.148392677307129: flops 68862592
#Filters: 838, #FLOPs: 59.00M | Top-1: 10.00
Epoch 26
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.35 (2677/25856)
Train | Batch (196/196) | Top-1: 10.44 (5220/50000)
Regular: 4.002527236938477
Epoche: 26; regular: 4.002527236938477: flops 68862592
#Filters: 835, #FLOPs: 57.99M | Top-1: 10.00
Epoch 27
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.88 (2814/25856)
Train | Batch (196/196) | Top-1: 10.64 (5319/50000)
Regular: 4.01871395111084
Epoche: 27; regular: 4.01871395111084: flops 68862592
#Filters: 827, #FLOPs: 58.58M | Top-1: 9.97
Epoch 28
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.17 (2629/25856)
Train | Batch (196/196) | Top-1: 10.55 (5273/50000)
Regular: 4.031083106994629
Epoche: 28; regular: 4.031083106994629: flops 68862592
#Filters: 823, #FLOPs: 58.56M | Top-1: 10.00
Epoch 29
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 10.19 (2636/25856)
Train | Batch (196/196) | Top-1: 11.24 (5618/50000)
Regular: 4.060194969177246
Epoche: 29; regular: 4.060194969177246: flops 68862592
#Filters: 829, #FLOPs: 57.90M | Top-1: 17.72
Drin!!
Layers that will be prunned: [(1, 4), (3, 2), (5, 1), (7, 4), (9, 3), (11, 5), (13, 7), (15, 5), (17, 7), (19, 10), (21, 47), (22, 2), (23, 37), (24, 2), (25, 39), (26, 2), (27, 39), (28, 2), (29, 30), (30, 2)]
Prunning filters..
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 1
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 1
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 1
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 1
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 1
Layer index: 1; Pruned filters: 2
Layer index: 1; Pruned filters: 2
Layer index: 3; Pruned filters: 2
Layer index: 5; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 7; Pruned filters: 2
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 2
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 2
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 2
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 6
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 3
Layer index: 21; Pruned filters: 11
Layer index: 21; Pruned filters: 7
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 9
Layer index: 21; Pruned filters: 4
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 4
Layer index: 23; Pruned filters: 4
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 3
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 4
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 5
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 3
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 3
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 6
Layer index: 29; Pruned filters: 3
Target (flops): 68.863M
After Pruning | FLOPs: 46.340M | #Params: 0.227M
1.2200827312370237
After Growth | FLOPs: 70.122M | #Params: 0.341M
I: 1
flops: 70122424
Before Pruning | FLOPs: 70.122M | #Params: 0.341M
Epoch 0
Train | Batch (1/196) | Top-1: 16.80 (43/256)
Train | Batch (101/196) | Top-1: 14.67 (3793/25856)
Train | Batch (196/196) | Top-1: 16.30 (8148/50000)
Regular: 8.741996765136719
Epoche: 0; regular: 8.741996765136719: flops 70122424
#Filters: 729, #FLOPs: 58.30M | Top-1: 10.01
Epoch 1
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 18.30 (4731/25856)
Train | Batch (196/196) | Top-1: 18.19 (9093/50000)
Regular: 5.490346431732178
Epoche: 1; regular: 5.490346431732178: flops 70122424
#Filters: 777, #FLOPs: 59.03M | Top-1: 10.07
Epoch 2
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 15.94 (4122/25856)
Train | Batch (196/196) | Top-1: 16.93 (8464/50000)
Regular: 7.319733142852783
Epoche: 2; regular: 7.319733142852783: flops 70122424
#Filters: 778, #FLOPs: 61.14M | Top-1: 20.64
Epoch 3
Train | Batch (1/196) | Top-1: 23.83 (61/256)
Train | Batch (101/196) | Top-1: 17.92 (4634/25856)
Train | Batch (196/196) | Top-1: 17.58 (8789/50000)
Regular: 7.219808578491211
Epoche: 3; regular: 7.219808578491211: flops 70122424
#Filters: 799, #FLOPs: 62.56M | Top-1: 14.19
Epoch 4
Train | Batch (1/196) | Top-1: 20.70 (53/256)
Train | Batch (101/196) | Top-1: 14.08 (3641/25856)
Train | Batch (196/196) | Top-1: 12.19 (6096/50000)
Regular: 5.945249080657959
Epoche: 4; regular: 5.945249080657959: flops 70122424
#Filters: 847, #FLOPs: 61.54M | Top-1: 10.00
Epoch 5
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 9.90 (2559/25856)
Train | Batch (196/196) | Top-1: 9.93 (4965/50000)
Regular: 5.1418561935424805
Epoche: 5; regular: 5.1418561935424805: flops 70122424
#Filters: 830, #FLOPs: 64.33M | Top-1: 9.83
Epoch 6
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 9.84 (2545/25856)
Train | Batch (196/196) | Top-1: 10.01 (5003/50000)
Regular: 5.146744728088379
Epoche: 6; regular: 5.146744728088379: flops 70122424
#Filters: 894, #FLOPs: 65.45M | Top-1: 10.00
Epoch 7
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.03 (2593/25856)
Train | Batch (196/196) | Top-1: 10.09 (5044/50000)
Regular: 5.166961669921875
Epoche: 7; regular: 5.166961669921875: flops 70122424
#Filters: 841, #FLOPs: 64.67M | Top-1: 9.65
Epoch 8
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 9.89 (2558/25856)
Train | Batch (196/196) | Top-1: 9.83 (4913/50000)
Regular: 5.176715850830078
Epoche: 8; regular: 5.176715850830078: flops 70122424
#Filters: 820, #FLOPs: 63.95M | Top-1: 10.00
Epoch 9
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 10.21 (2641/25856)
Train | Batch (196/196) | Top-1: 10.36 (5178/50000)
Regular: 5.214579105377197
Epoche: 9; regular: 5.214579105377197: flops 70122424
#Filters: 861, #FLOPs: 64.13M | Top-1: 10.98
Epoch 10
Train | Batch (1/196) | Top-1: 7.81 (20/256)
Train | Batch (101/196) | Top-1: 10.42 (2695/25856)
Train | Batch (196/196) | Top-1: 10.21 (5104/50000)
Regular: 5.2248334884643555
Epoche: 10; regular: 5.2248334884643555: flops 70122424
#Filters: 794, #FLOPs: 62.11M | Top-1: 10.00
Epoch 11
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 10.42 (2693/25856)
Train | Batch (196/196) | Top-1: 10.48 (5240/50000)
Regular: 5.218369960784912
Epoche: 11; regular: 5.218369960784912: flops 70122424
#Filters: 813, #FLOPs: 63.05M | Top-1: 10.00
Epoch 12
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 10.44 (2700/25856)
Train | Batch (196/196) | Top-1: 11.27 (5633/50000)
Regular: 5.241001605987549
Epoche: 12; regular: 5.241001605987549: flops 70122424
#Filters: 881, #FLOPs: 64.81M | Top-1: 10.00
Epoch 13
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 16.26 (4204/25856)
Train | Batch (196/196) | Top-1: 16.62 (8312/50000)
Regular: 6.498840808868408
Epoche: 13; regular: 6.498840808868408: flops 70122424
#Filters: 793, #FLOPs: 61.61M | Top-1: 10.61
Epoch 14
Train | Batch (1/196) | Top-1: 21.09 (54/256)
Train | Batch (101/196) | Top-1: 18.25 (4718/25856)
Train | Batch (196/196) | Top-1: 18.04 (9018/50000)
Regular: 5.516345024108887
Epoche: 14; regular: 5.516345024108887: flops 70122424
#Filters: 821, #FLOPs: 63.13M | Top-1: 19.78
Epoch 15
Train | Batch (1/196) | Top-1: 17.19 (44/256)
Train | Batch (101/196) | Top-1: 17.64 (4561/25856)
Train | Batch (196/196) | Top-1: 17.93 (8966/50000)
Regular: 5.3453898429870605
Epoche: 15; regular: 5.3453898429870605: flops 70122424
#Filters: 872, #FLOPs: 63.99M | Top-1: 18.59
Epoch 16
Train | Batch (1/196) | Top-1: 18.36 (47/256)
Train | Batch (101/196) | Top-1: 17.93 (4637/25856)
Train | Batch (196/196) | Top-1: 17.42 (8708/50000)
Regular: 5.917109489440918
Epoche: 16; regular: 5.917109489440918: flops 70122424
#Filters: 807, #FLOPs: 62.75M | Top-1: 16.30
Epoch 17
Train | Batch (1/196) | Top-1: 18.75 (48/256)
Train | Batch (101/196) | Top-1: 16.79 (4341/25856)
Train | Batch (196/196) | Top-1: 16.44 (8221/50000)
Regular: 8.406947135925293
Epoche: 17; regular: 8.406947135925293: flops 70122424
#Filters: 788, #FLOPs: 61.79M | Top-1: 9.18
Epoch 18
Train | Batch (1/196) | Top-1: 6.64 (17/256)
Train | Batch (101/196) | Top-1: 17.49 (4522/25856)
Train | Batch (196/196) | Top-1: 17.79 (8896/50000)
Regular: 10.134061813354492
Epoche: 18; regular: 10.134061813354492: flops 70122424
#Filters: 870, #FLOPs: 63.90M | Top-1: 12.42
Epoch 19
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 18.33 (4740/25856)
Train | Batch (196/196) | Top-1: 17.83 (8916/50000)
Regular: 5.596043109893799
Epoche: 19; regular: 5.596043109893799: flops 70122424
#Filters: 824, #FLOPs: 63.80M | Top-1: 10.00
Epoch 20
Train | Batch (1/196) | Top-1: 16.02 (41/256)
Train | Batch (101/196) | Top-1: 18.00 (4655/25856)
Train | Batch (196/196) | Top-1: 18.40 (9200/50000)
Regular: 7.137914657592773
Epoche: 20; regular: 7.137914657592773: flops 70122424
#Filters: 792, #FLOPs: 61.76M | Top-1: 18.39
Epoch 21
Train | Batch (1/196) | Top-1: 13.67 (35/256)
Train | Batch (101/196) | Top-1: 18.36 (4746/25856)
Train | Batch (196/196) | Top-1: 18.25 (9124/50000)
Regular: 6.338092803955078
Epoche: 21; regular: 6.338092803955078: flops 70122424
#Filters: 855, #FLOPs: 64.25M | Top-1: 15.84
Epoch 22
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 18.31 (4735/25856)
Train | Batch (196/196) | Top-1: 18.50 (9249/50000)
Regular: 5.956258773803711
Epoche: 22; regular: 5.956258773803711: flops 70122424
#Filters: 865, #FLOPs: 64.88M | Top-1: 19.13
Epoch 23
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 18.41 (4760/25856)
Train | Batch (196/196) | Top-1: 18.02 (9012/50000)
Regular: 5.689986228942871
Epoche: 23; regular: 5.689986228942871: flops 70122424
#Filters: 806, #FLOPs: 63.31M | Top-1: 16.97
Epoch 24
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 18.27 (4723/25856)
Train | Batch (196/196) | Top-1: 14.29 (7147/50000)
Regular: 5.339967250823975
Epoche: 24; regular: 5.339967250823975: flops 70122424
#Filters: 806, #FLOPs: 63.07M | Top-1: 10.00
Epoch 25
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 9.90 (2561/25856)
Train | Batch (196/196) | Top-1: 9.79 (4897/50000)
Regular: 5.214701175689697
Epoche: 25; regular: 5.214701175689697: flops 70122424
#Filters: 814, #FLOPs: 63.53M | Top-1: 10.00
Epoch 26
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.06 (2601/25856)
Train | Batch (196/196) | Top-1: 9.80 (4898/50000)
Regular: 5.216831207275391
Epoche: 26; regular: 5.216831207275391: flops 70122424
#Filters: 828, #FLOPs: 64.17M | Top-1: 10.00
Epoch 27
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 9.78 (2528/25856)
Train | Batch (196/196) | Top-1: 10.04 (5022/50000)
Regular: 5.222322940826416
Epoche: 27; regular: 5.222322940826416: flops 70122424
#Filters: 837, #FLOPs: 64.96M | Top-1: 10.00
Epoch 28
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 9.84 (2544/25856)
Train | Batch (196/196) | Top-1: 9.89 (4944/50000)
Regular: 5.2233567237854
Epoche: 28; regular: 5.2233567237854: flops 70122424
#Filters: 869, #FLOPs: 65.25M | Top-1: 10.00
Epoch 29
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 9.73 (2517/25856)
Train | Batch (196/196) | Top-1: 9.85 (4927/50000)
Regular: 5.2267069816589355
Epoche: 29; regular: 5.2267069816589355: flops 70122424
#Filters: 832, #FLOPs: 65.12M | Top-1: 10.00
Drin!!
Layers that will be prunned: [(1, 1), (11, 1), (15, 1), (19, 2), (21, 13), (22, 34), (23, 7), (24, 34), (25, 3), (26, 34), (27, 7), (28, 34), (29, 9), (30, 34)]
Prunning filters..
Layer index: 22; Pruned filters: 9
Layer index: 22; Pruned filters: 4
Layer index: 22; Pruned filters: 21
Layer index: 24; Pruned filters: 9
Layer index: 24; Pruned filters: 4
Layer index: 24; Pruned filters: 21
Layer index: 26; Pruned filters: 9
Layer index: 26; Pruned filters: 4
Layer index: 26; Pruned filters: 21
Layer index: 28; Pruned filters: 9
Layer index: 28; Pruned filters: 4
Layer index: 28; Pruned filters: 21
Layer index: 30; Pruned filters: 9
Layer index: 30; Pruned filters: 4
Layer index: 30; Pruned filters: 21
Layer index: 1; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 19; Pruned filters: 2
Layer index: 21; Pruned filters: 3
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 8
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 61.476M | #Params: 0.219M
1.0586394426672048
After Growth | FLOPs: 68.466M | #Params: 0.244M
I: 2
flops: 68466104
Before Pruning | FLOPs: 68.466M | #Params: 0.244M
Epoch 0
Train | Batch (1/196) | Top-1: 7.03 (18/256)
Train | Batch (101/196) | Top-1: 9.80 (2533/25856)
Train | Batch (196/196) | Top-1: 9.75 (4877/50000)
Regular: 6.400760173797607
Epoche: 0; regular: 6.400760173797607: flops 68466104
#Filters: 600, #FLOPs: 49.99M | Top-1: 10.00
Epoch 1
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.31 (2665/25856)
Train | Batch (196/196) | Top-1: 10.16 (5080/50000)
Regular: 5.704391956329346
Epoche: 1; regular: 5.704391956329346: flops 68466104
#Filters: 764, #FLOPs: 60.88M | Top-1: 10.00
Epoch 2
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 9.63 (2491/25856)
Train | Batch (196/196) | Top-1: 9.76 (4879/50000)
Regular: 5.874438285827637
Epoche: 2; regular: 5.874438285827637: flops 68466104
#Filters: 751, #FLOPs: 62.40M | Top-1: 10.00
Epoch 3
Train | Batch (1/196) | Top-1: 8.20 (21/256)
Train | Batch (101/196) | Top-1: 10.57 (2734/25856)
Train | Batch (196/196) | Top-1: 10.62 (5308/50000)
Regular: 5.909640789031982
Epoche: 3; regular: 5.909640789031982: flops 68466104
#Filters: 783, #FLOPs: 65.36M | Top-1: 11.94
Epoch 4
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 10.70 (2767/25856)
Train | Batch (196/196) | Top-1: 11.14 (5572/50000)
Regular: 5.8558268547058105
Epoche: 4; regular: 5.8558268547058105: flops 68466104
#Filters: 766, #FLOPs: 63.85M | Top-1: 10.00
Epoch 5
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.93 (2826/25856)
Train | Batch (196/196) | Top-1: 10.97 (5483/50000)
Regular: 5.802757263183594
Epoche: 5; regular: 5.802757263183594: flops 68466104
#Filters: 745, #FLOPs: 63.01M | Top-1: 10.00
Epoch 6
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.96 (2833/25856)
Train | Batch (196/196) | Top-1: 10.84 (5420/50000)
Regular: 5.823372840881348
Epoche: 6; regular: 5.823372840881348: flops 68466104
#Filters: 737, #FLOPs: 62.27M | Top-1: 4.91
Epoch 7
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 11.01 (2846/25856)
Train | Batch (196/196) | Top-1: 12.67 (6336/50000)
Regular: 5.983268737792969
Epoche: 7; regular: 5.983268737792969: flops 68466104
#Filters: 765, #FLOPs: 63.27M | Top-1: 6.68
Epoch 8
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 17.65 (4563/25856)
Train | Batch (196/196) | Top-1: 17.74 (8872/50000)
Regular: 7.777811050415039
Epoche: 8; regular: 7.777811050415039: flops 68466104
#Filters: 733, #FLOPs: 61.92M | Top-1: 19.49
Epoch 9
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 18.85 (4875/25856)
Train | Batch (196/196) | Top-1: 18.55 (9276/50000)
Regular: 6.783295154571533
Epoche: 9; regular: 6.783295154571533: flops 68466104
#Filters: 743, #FLOPs: 63.14M | Top-1: 14.41
Epoch 10
Train | Batch (1/196) | Top-1: 22.27 (57/256)
Train | Batch (101/196) | Top-1: 18.35 (4744/25856)
Train | Batch (196/196) | Top-1: 18.52 (9260/50000)
Regular: 7.1917901039123535
Epoche: 10; regular: 7.1917901039123535: flops 68466104
#Filters: 744, #FLOPs: 62.61M | Top-1: 10.01
Epoch 11
Train | Batch (1/196) | Top-1: 23.44 (60/256)
Train | Batch (101/196) | Top-1: 18.60 (4808/25856)
Train | Batch (196/196) | Top-1: 18.54 (9268/50000)
Regular: 7.139847755432129
Epoche: 11; regular: 7.139847755432129: flops 68466104
#Filters: 771, #FLOPs: 63.29M | Top-1: 22.15
Epoch 12
Train | Batch (1/196) | Top-1: 18.36 (47/256)
Train | Batch (101/196) | Top-1: 18.98 (4908/25856)
Train | Batch (196/196) | Top-1: 17.53 (8764/50000)
Regular: 7.213559627532959
Epoche: 12; regular: 7.213559627532959: flops 68466104
#Filters: 756, #FLOPs: 63.70M | Top-1: 10.00
Epoch 13
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 16.28 (4209/25856)
Train | Batch (196/196) | Top-1: 16.09 (8043/50000)
Regular: 8.690069198608398
Epoche: 13; regular: 8.690069198608398: flops 68466104
#Filters: 748, #FLOPs: 63.16M | Top-1: 10.00
Epoch 14
Train | Batch (1/196) | Top-1: 21.48 (55/256)
Train | Batch (101/196) | Top-1: 18.43 (4765/25856)
Train | Batch (196/196) | Top-1: 18.30 (9152/50000)
Regular: 7.034309387207031
Epoche: 14; regular: 7.034309387207031: flops 68466104
#Filters: 771, #FLOPs: 63.71M | Top-1: 10.00
Epoch 15
Train | Batch (1/196) | Top-1: 16.80 (43/256)
Train | Batch (101/196) | Top-1: 11.79 (3049/25856)
Train | Batch (196/196) | Top-1: 10.94 (5470/50000)
Regular: 7.2243828773498535
Epoche: 15; regular: 7.2243828773498535: flops 68466104
#Filters: 737, #FLOPs: 62.79M | Top-1: 10.00
Epoch 16
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 9.85 (2546/25856)
Train | Batch (196/196) | Top-1: 9.86 (4929/50000)
Regular: 5.84586763381958
Epoche: 16; regular: 5.84586763381958: flops 68466104
#Filters: 775, #FLOPs: 64.59M | Top-1: 9.24
Epoch 17
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 9.72 (2514/25856)
Train | Batch (196/196) | Top-1: 9.89 (4943/50000)
Regular: 5.77241849899292
Epoche: 17; regular: 5.77241849899292: flops 68466104
#Filters: 796, #FLOPs: 65.16M | Top-1: 10.00
Epoch 18
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 9.89 (2557/25856)
Train | Batch (196/196) | Top-1: 9.96 (4980/50000)
Regular: 5.796270847320557
Epoche: 18; regular: 5.796270847320557: flops 68466104
#Filters: 781, #FLOPs: 65.46M | Top-1: 12.55
Epoch 19
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.00 (2586/25856)
Train | Batch (196/196) | Top-1: 10.02 (5009/50000)
Regular: 5.806528568267822
Epoche: 19; regular: 5.806528568267822: flops 68466104
#Filters: 766, #FLOPs: 64.66M | Top-1: 10.00
Epoch 20
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.29 (2661/25856)
Train | Batch (196/196) | Top-1: 10.40 (5200/50000)
Regular: 5.7887163162231445
Epoche: 20; regular: 5.7887163162231445: flops 68466104
#Filters: 781, #FLOPs: 64.80M | Top-1: 10.00
Epoch 21
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 10.42 (2694/25856)
Train | Batch (196/196) | Top-1: 10.35 (5176/50000)
Regular: 5.75775146484375
Epoche: 21; regular: 5.75775146484375: flops 68466104
#Filters: 753, #FLOPs: 63.40M | Top-1: 10.00
Epoch 22
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 10.30 (2663/25856)
Train | Batch (196/196) | Top-1: 10.41 (5205/50000)
Regular: 5.772947311401367
Epoche: 22; regular: 5.772947311401367: flops 68466104
#Filters: 747, #FLOPs: 63.86M | Top-1: 10.00
Epoch 23
Train | Batch (1/196) | Top-1: 16.02 (41/256)
Train | Batch (101/196) | Top-1: 11.09 (2867/25856)
Train | Batch (196/196) | Top-1: 10.89 (5446/50000)
Regular: 5.748373031616211
Epoche: 23; regular: 5.748373031616211: flops 68466104
#Filters: 763, #FLOPs: 63.44M | Top-1: 10.00
Epoch 24
Train | Batch (1/196) | Top-1: 5.86 (15/256)
Train | Batch (101/196) | Top-1: 13.65 (3530/25856)
Train | Batch (196/196) | Top-1: 14.80 (7398/50000)
Regular: 6.453548908233643
Epoche: 24; regular: 6.453548908233643: flops 68466104
#Filters: 759, #FLOPs: 62.77M | Top-1: 11.03
Epoch 25
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 17.95 (4640/25856)
Train | Batch (196/196) | Top-1: 17.71 (8853/50000)
Regular: 6.894751071929932
Epoche: 25; regular: 6.894751071929932: flops 68466104
#Filters: 741, #FLOPs: 62.80M | Top-1: 10.10
Epoch 26
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 16.97 (4389/25856)
Train | Batch (196/196) | Top-1: 17.05 (8525/50000)
Regular: 7.640944957733154
Epoche: 26; regular: 7.640944957733154: flops 68466104
#Filters: 769, #FLOPs: 63.54M | Top-1: 10.29
Epoch 27
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 18.36 (4748/25856)
Train | Batch (196/196) | Top-1: 18.24 (9118/50000)
Regular: 6.569687366485596
Epoche: 27; regular: 6.569687366485596: flops 68466104
#Filters: 767, #FLOPs: 64.35M | Top-1: 11.81
Epoch 28
Train | Batch (1/196) | Top-1: 21.88 (56/256)
Train | Batch (101/196) | Top-1: 10.05 (2598/25856)
Train | Batch (196/196) | Top-1: 10.07 (5037/50000)
Regular: 5.955003261566162
Epoche: 28; regular: 5.955003261566162: flops 68466104
#Filters: 749, #FLOPs: 63.54M | Top-1: 10.00
Epoch 29
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 9.98 (2580/25856)
Train | Batch (196/196) | Top-1: 9.94 (4968/50000)
Regular: 5.734017372131348
Epoche: 29; regular: 5.734017372131348: flops 68466104
#Filters: 787, #FLOPs: 64.82M | Top-1: 10.33
Drin!!
Layers that will be prunned: [(1, 1), (3, 2), (7, 2), (13, 1), (15, 6), (17, 2), (21, 4), (22, 3), (23, 9), (24, 3), (25, 15), (26, 3), (27, 9), (28, 3), (29, 27), (30, 3)]
Prunning filters..
Layer index: 22; Pruned filters: 3
Layer index: 24; Pruned filters: 3
Layer index: 26; Pruned filters: 3
Layer index: 28; Pruned filters: 3
Layer index: 30; Pruned filters: 3
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 7; Pruned filters: 2
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 2
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 2
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 7
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 5
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 7
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 10
Target (flops): 68.863M
After Pruning | FLOPs: 61.389M | #Params: 0.182M
1.0594012321795279
After Growth | FLOPs: 68.406M | #Params: 0.202M
I: 3
flops: 68406190
Before Pruning | FLOPs: 68.406M | #Params: 0.202M
Epoch 0
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 9.92 (2565/25856)
Train | Batch (196/196) | Top-1: 9.95 (4974/50000)
Regular: 6.793432712554932
Epoche: 0; regular: 6.793432712554932: flops 68406190
#Filters: 533, #FLOPs: 46.53M | Top-1: 10.00
Epoch 1
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.21 (2641/25856)
Train | Batch (196/196) | Top-1: 10.45 (5224/50000)
Regular: 6.031591892242432
Epoche: 1; regular: 6.031591892242432: flops 68406190
#Filters: 743, #FLOPs: 61.08M | Top-1: 10.00
Epoch 2
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.49 (2713/25856)
Train | Batch (196/196) | Top-1: 10.76 (5381/50000)
Regular: 6.160427093505859
Epoche: 2; regular: 6.160427093505859: flops 68406190
#Filters: 756, #FLOPs: 64.28M | Top-1: 14.16
Epoch 3
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 11.28 (2917/25856)
Train | Batch (196/196) | Top-1: 10.98 (5492/50000)
Regular: 6.157285213470459
Epoche: 3; regular: 6.157285213470459: flops 68406190
#Filters: 762, #FLOPs: 64.22M | Top-1: 10.00
Epoch 4
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.38 (2683/25856)
Train | Batch (196/196) | Top-1: 10.50 (5252/50000)
Regular: 6.126031875610352
Epoche: 4; regular: 6.126031875610352: flops 68406190
#Filters: 739, #FLOPs: 62.98M | Top-1: 13.68
Epoch 5
Train | Batch (1/196) | Top-1: 15.23 (39/256)
Train | Batch (101/196) | Top-1: 10.59 (2738/25856)
Train | Batch (196/196) | Top-1: 10.68 (5341/50000)
Regular: 6.136279106140137
Epoche: 5; regular: 6.136279106140137: flops 68406190
#Filters: 744, #FLOPs: 63.60M | Top-1: 9.99
Epoch 6
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 10.91 (2821/25856)
Train | Batch (196/196) | Top-1: 10.67 (5334/50000)
Regular: 6.160379886627197
Epoche: 6; regular: 6.160379886627197: flops 68406190
#Filters: 760, #FLOPs: 63.82M | Top-1: 10.00
Epoch 7
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 10.39 (2686/25856)
Train | Batch (196/196) | Top-1: 10.86 (5428/50000)
Regular: 6.169563293457031
Epoche: 7; regular: 6.169563293457031: flops 68406190
#Filters: 745, #FLOPs: 63.98M | Top-1: 10.00
Epoch 8
Train | Batch (1/196) | Top-1: 8.20 (21/256)
Train | Batch (101/196) | Top-1: 10.66 (2756/25856)
Train | Batch (196/196) | Top-1: 10.69 (5344/50000)
Regular: 6.137699127197266
Epoche: 8; regular: 6.137699127197266: flops 68406190
#Filters: 751, #FLOPs: 63.91M | Top-1: 10.00
Epoch 9
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.59 (2739/25856)
Train | Batch (196/196) | Top-1: 10.82 (5408/50000)
Regular: 6.154770851135254
Epoche: 9; regular: 6.154770851135254: flops 68406190
#Filters: 771, #FLOPs: 64.53M | Top-1: 9.03
Epoch 10
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 10.25 (2650/25856)
Train | Batch (196/196) | Top-1: 10.38 (5192/50000)
Regular: 6.138687610626221
Epoche: 10; regular: 6.138687610626221: flops 68406190
#Filters: 737, #FLOPs: 62.63M | Top-1: 10.00
Epoch 11
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 10.87 (2810/25856)
Train | Batch (196/196) | Top-1: 10.79 (5396/50000)
Regular: 6.148536682128906
Epoche: 11; regular: 6.148536682128906: flops 68406190
#Filters: 747, #FLOPs: 63.16M | Top-1: 10.00
Epoch 12
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.75 (2780/25856)
Train | Batch (196/196) | Top-1: 10.82 (5409/50000)
Regular: 6.1139326095581055
Epoche: 12; regular: 6.1139326095581055: flops 68406190
#Filters: 766, #FLOPs: 64.51M | Top-1: 10.74
Epoch 13
Train | Batch (1/196) | Top-1: 16.80 (43/256)
Train | Batch (101/196) | Top-1: 11.18 (2890/25856)
Train | Batch (196/196) | Top-1: 10.97 (5483/50000)
Regular: 6.124778747558594
Epoche: 13; regular: 6.124778747558594: flops 68406190
#Filters: 743, #FLOPs: 63.92M | Top-1: 11.56
Epoch 14
Train | Batch (1/196) | Top-1: 14.06 (36/256)
Train | Batch (101/196) | Top-1: 10.80 (2792/25856)
Train | Batch (196/196) | Top-1: 10.89 (5444/50000)
Regular: 6.139009475708008
Epoche: 14; regular: 6.139009475708008: flops 68406190
#Filters: 747, #FLOPs: 64.22M | Top-1: 10.00
Epoch 15
Train | Batch (1/196) | Top-1: 14.84 (38/256)
Train | Batch (101/196) | Top-1: 11.00 (2845/25856)
Train | Batch (196/196) | Top-1: 10.94 (5468/50000)
Regular: 6.100285530090332
Epoche: 15; regular: 6.100285530090332: flops 68406190
#Filters: 774, #FLOPs: 65.36M | Top-1: 9.76
Epoch 16
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 10.62 (2746/25856)
Train | Batch (196/196) | Top-1: 10.70 (5352/50000)
Regular: 6.1307454109191895
Epoche: 16; regular: 6.1307454109191895: flops 68406190
#Filters: 752, #FLOPs: 64.22M | Top-1: 10.00
Epoch 17
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 11.63 (3007/25856)
Train | Batch (196/196) | Top-1: 11.41 (5706/50000)
Regular: 6.155381679534912
Epoche: 17; regular: 6.155381679534912: flops 68406190
#Filters: 743, #FLOPs: 63.87M | Top-1: 16.55
Epoch 18
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.72 (2771/25856)
Train | Batch (196/196) | Top-1: 10.52 (5260/50000)
Regular: 6.168530464172363
Epoche: 18; regular: 6.168530464172363: flops 68406190
#Filters: 763, #FLOPs: 64.47M | Top-1: 13.25
Epoch 19
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 11.65 (3011/25856)
Train | Batch (196/196) | Top-1: 11.58 (5790/50000)
Regular: 6.159823894500732
Epoche: 19; regular: 6.159823894500732: flops 68406190
#Filters: 748, #FLOPs: 64.14M | Top-1: 10.35
Epoch 20
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 11.50 (2974/25856)
Train | Batch (196/196) | Top-1: 11.17 (5585/50000)
Regular: 6.163486480712891
Epoche: 20; regular: 6.163486480712891: flops 68406190
#Filters: 755, #FLOPs: 64.52M | Top-1: 7.51
Epoch 21
Train | Batch (1/196) | Top-1: 7.81 (20/256)
Train | Batch (101/196) | Top-1: 10.62 (2745/25856)
Train | Batch (196/196) | Top-1: 10.77 (5386/50000)
Regular: 6.147829532623291
Epoche: 21; regular: 6.147829532623291: flops 68406190
#Filters: 748, #FLOPs: 64.24M | Top-1: 10.03
Epoch 22
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.44 (2699/25856)
Train | Batch (196/196) | Top-1: 10.57 (5287/50000)
Regular: 6.120382308959961
Epoche: 22; regular: 6.120382308959961: flops 68406190
#Filters: 747, #FLOPs: 64.52M | Top-1: 10.00
Epoch 23
Train | Batch (1/196) | Top-1: 14.06 (36/256)
Train | Batch (101/196) | Top-1: 10.87 (2811/25856)
Train | Batch (196/196) | Top-1: 10.72 (5362/50000)
Regular: 6.178775787353516
Epoche: 23; regular: 6.178775787353516: flops 68406190
#Filters: 767, #FLOPs: 64.53M | Top-1: 13.97
Epoch 24
Train | Batch (1/196) | Top-1: 13.67 (35/256)
Train | Batch (101/196) | Top-1: 10.58 (2735/25856)
Train | Batch (196/196) | Top-1: 11.21 (5604/50000)
Regular: 6.195748329162598
Epoche: 24; regular: 6.195748329162598: flops 68406190
#Filters: 745, #FLOPs: 64.45M | Top-1: 10.00
Epoch 25
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.24 (2647/25856)
Train | Batch (196/196) | Top-1: 10.44 (5219/50000)
Regular: 6.134030818939209
Epoche: 25; regular: 6.134030818939209: flops 68406190
#Filters: 745, #FLOPs: 63.26M | Top-1: 10.00
Epoch 26
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 11.01 (2846/25856)
Train | Batch (196/196) | Top-1: 10.78 (5388/50000)
Regular: 6.181869983673096
Epoche: 26; regular: 6.181869983673096: flops 68406190
#Filters: 777, #FLOPs: 65.50M | Top-1: 10.00
Epoch 27
Train | Batch (1/196) | Top-1: 13.28 (34/256)
Train | Batch (101/196) | Top-1: 11.36 (2936/25856)
Train | Batch (196/196) | Top-1: 11.76 (5881/50000)
Regular: 6.1748762130737305
Epoche: 27; regular: 6.1748762130737305: flops 68406190
#Filters: 745, #FLOPs: 63.87M | Top-1: 12.08
Epoch 28
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 16.05 (4151/25856)
Train | Batch (196/196) | Top-1: 13.41 (6705/50000)
Regular: 6.677655220031738
Epoche: 28; regular: 6.677655220031738: flops 68406190
#Filters: 743, #FLOPs: 63.86M | Top-1: 10.00
Epoch 29
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 10.04 (2597/25856)
Train | Batch (196/196) | Top-1: 10.25 (5123/50000)
Regular: 6.077042579650879
Epoche: 29; regular: 6.077042579650879: flops 68406190
#Filters: 783, #FLOPs: 65.12M | Top-1: 10.00
Drin!!
Layers that will be prunned: [(1, 2), (5, 2), (9, 4), (11, 2), (13, 2), (15, 1), (17, 2), (19, 4), (21, 1), (23, 9), (25, 6), (27, 7), (29, 4)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 1
Layer index: 5; Pruned filters: 1
Layer index: 5; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 2
Target (flops): 68.863M
After Pruning | FLOPs: 61.742M | #Params: 0.170M
1.056369998556786
After Growth | FLOPs: 68.618M | #Params: 0.189M
I: 4
flops: 68618178
Before Pruning | FLOPs: 68.618M | #Params: 0.189M
Epoch 0
Train | Batch (1/196) | Top-1: 8.20 (21/256)
Train | Batch (101/196) | Top-1: 10.07 (2603/25856)
Train | Batch (196/196) | Top-1: 10.10 (5048/50000)
Regular: 6.986482620239258
Epoche: 0; regular: 6.986482620239258: flops 68618178
#Filters: 685, #FLOPs: 56.11M | Top-1: 10.00
Epoch 1
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.01 (2587/25856)
Train | Batch (196/196) | Top-1: 10.03 (5016/50000)
Regular: 6.264232635498047
Epoche: 1; regular: 6.264232635498047: flops 68618178
#Filters: 772, #FLOPs: 63.02M | Top-1: 10.00
Epoch 2
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 10.44 (2699/25856)
Train | Batch (196/196) | Top-1: 10.42 (5210/50000)
Regular: 6.378702163696289
Epoche: 2; regular: 6.378702163696289: flops 68618178
#Filters: 770, #FLOPs: 63.46M | Top-1: 10.00
Epoch 3
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 10.24 (2648/25856)
Train | Batch (196/196) | Top-1: 10.43 (5217/50000)
Regular: 6.450736999511719
Epoche: 3; regular: 6.450736999511719: flops 68618178
#Filters: 774, #FLOPs: 65.78M | Top-1: 10.38
Epoch 4
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.67 (2760/25856)
Train | Batch (196/196) | Top-1: 10.66 (5328/50000)
Regular: 6.429798126220703
Epoche: 4; regular: 6.429798126220703: flops 68618178
#Filters: 759, #FLOPs: 65.17M | Top-1: 10.00
Epoch 5
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.91 (2822/25856)
Train | Batch (196/196) | Top-1: 10.72 (5361/50000)
Regular: 6.397833347320557
Epoche: 5; regular: 6.397833347320557: flops 68618178
#Filters: 782, #FLOPs: 65.43M | Top-1: 10.00
Epoch 6
Train | Batch (1/196) | Top-1: 17.19 (44/256)
Train | Batch (101/196) | Top-1: 11.85 (3065/25856)
Train | Batch (196/196) | Top-1: 11.16 (5580/50000)
Regular: 6.400480270385742
Epoche: 6; regular: 6.400480270385742: flops 68618178
#Filters: 741, #FLOPs: 62.66M | Top-1: 10.83
Epoch 7
Train | Batch (1/196) | Top-1: 14.84 (38/256)
Train | Batch (101/196) | Top-1: 10.65 (2754/25856)
Train | Batch (196/196) | Top-1: 10.66 (5328/50000)
Regular: 6.383181095123291
Epoche: 7; regular: 6.383181095123291: flops 68618178
#Filters: 765, #FLOPs: 65.30M | Top-1: 10.17
Epoch 8
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 10.71 (2768/25856)
Train | Batch (196/196) | Top-1: 10.63 (5314/50000)
Regular: 6.379384517669678
Epoche: 8; regular: 6.379384517669678: flops 68618178
#Filters: 790, #FLOPs: 66.33M | Top-1: 10.00
Epoch 9
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 12.33 (3188/25856)
Train | Batch (196/196) | Top-1: 14.32 (7161/50000)
Regular: 6.581241607666016
Epoche: 9; regular: 6.581241607666016: flops 68618178
#Filters: 762, #FLOPs: 64.99M | Top-1: 10.00
Epoch 10
Train | Batch (1/196) | Top-1: 15.23 (39/256)
Train | Batch (101/196) | Top-1: 15.17 (3922/25856)
Train | Batch (196/196) | Top-1: 16.20 (8100/50000)
Regular: 6.667671203613281
Epoche: 10; regular: 6.667671203613281: flops 68618178
#Filters: 763, #FLOPs: 64.88M | Top-1: 16.95
Epoch 11
Train | Batch (1/196) | Top-1: 17.97 (46/256)
Train | Batch (101/196) | Top-1: 18.34 (4743/25856)
Train | Batch (196/196) | Top-1: 18.20 (9102/50000)
Regular: 6.65108585357666
Epoche: 11; regular: 6.65108585357666: flops 68618178
#Filters: 797, #FLOPs: 66.42M | Top-1: 15.13
Epoch 12
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 18.66 (4824/25856)
Train | Batch (196/196) | Top-1: 18.43 (9214/50000)
Regular: 6.571086883544922
Epoche: 12; regular: 6.571086883544922: flops 68618178
#Filters: 783, #FLOPs: 66.58M | Top-1: 13.83
Epoch 13
Train | Batch (1/196) | Top-1: 14.84 (38/256)
Train | Batch (101/196) | Top-1: 18.73 (4842/25856)
Train | Batch (196/196) | Top-1: 18.60 (9299/50000)
Regular: 6.494358062744141
Epoche: 13; regular: 6.494358062744141: flops 68618178
#Filters: 776, #FLOPs: 66.66M | Top-1: 10.00
Epoch 14
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 18.44 (4769/25856)
Train | Batch (196/196) | Top-1: 18.49 (9247/50000)
Regular: 6.695058822631836
Epoche: 14; regular: 6.695058822631836: flops 68618178
#Filters: 808, #FLOPs: 66.99M | Top-1: 16.53
Epoch 15
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 18.54 (4794/25856)
Train | Batch (196/196) | Top-1: 18.75 (9376/50000)
Regular: 7.53756046295166
Epoche: 15; regular: 7.53756046295166: flops 68618178
#Filters: 776, #FLOPs: 66.24M | Top-1: 18.73
Epoch 16
Train | Batch (1/196) | Top-1: 16.80 (43/256)
Train | Batch (101/196) | Top-1: 18.54 (4794/25856)
Train | Batch (196/196) | Top-1: 18.71 (9355/50000)
Regular: 6.5484795570373535
Epoche: 16; regular: 6.5484795570373535: flops 68618178
#Filters: 768, #FLOPs: 65.45M | Top-1: 10.36
Epoch 17
Train | Batch (1/196) | Top-1: 21.88 (56/256)
Train | Batch (101/196) | Top-1: 19.05 (4925/25856)
Train | Batch (196/196) | Top-1: 18.88 (9441/50000)
Regular: 6.9426093101501465
Epoche: 17; regular: 6.9426093101501465: flops 68618178
#Filters: 797, #FLOPs: 66.77M | Top-1: 11.20
Epoch 18
Train | Batch (1/196) | Top-1: 16.80 (43/256)
Train | Batch (101/196) | Top-1: 18.71 (4837/25856)
Train | Batch (196/196) | Top-1: 18.54 (9271/50000)
Regular: 6.793910503387451
Epoche: 18; regular: 6.793910503387451: flops 68618178
#Filters: 773, #FLOPs: 65.91M | Top-1: 19.19
Epoch 19
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 19.62 (5073/25856)
Train | Batch (196/196) | Top-1: 19.16 (9582/50000)
Regular: 6.802652359008789
Epoche: 19; regular: 6.802652359008789: flops 68618178
#Filters: 776, #FLOPs: 66.48M | Top-1: 20.45
Epoch 20
Train | Batch (1/196) | Top-1: 19.92 (51/256)
Train | Batch (101/196) | Top-1: 16.07 (4154/25856)
Train | Batch (196/196) | Top-1: 16.94 (8472/50000)
Regular: 6.528164386749268
Epoche: 20; regular: 6.528164386749268: flops 68618178
#Filters: 791, #FLOPs: 65.13M | Top-1: 16.49
Epoch 21
Train | Batch (1/196) | Top-1: 19.92 (51/256)
Train | Batch (101/196) | Top-1: 18.78 (4856/25856)
Train | Batch (196/196) | Top-1: 18.65 (9326/50000)
Regular: 7.077234268188477
Epoche: 21; regular: 7.077234268188477: flops 68618178
#Filters: 772, #FLOPs: 65.16M | Top-1: 10.78
Epoch 22
Train | Batch (1/196) | Top-1: 20.70 (53/256)
Train | Batch (101/196) | Top-1: 17.52 (4530/25856)
Train | Batch (196/196) | Top-1: 17.44 (8722/50000)
Regular: 14.325222969055176
Epoche: 22; regular: 14.325222969055176: flops 68618178
#Filters: 773, #FLOPs: 65.62M | Top-1: 13.16
Epoch 23
Train | Batch (1/196) | Top-1: 17.19 (44/256)
Train | Batch (101/196) | Top-1: 17.95 (4641/25856)
Train | Batch (196/196) | Top-1: 18.40 (9198/50000)
Regular: 24.81561851501465
Epoche: 23; regular: 24.81561851501465: flops 68618178
#Filters: 793, #FLOPs: 65.98M | Top-1: 10.14
Epoch 24
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 19.04 (4923/25856)
Train | Batch (196/196) | Top-1: 18.91 (9454/50000)
Regular: 6.795860290527344
Epoche: 24; regular: 6.795860290527344: flops 68618178
#Filters: 775, #FLOPs: 65.72M | Top-1: 19.22
Epoch 25
Train | Batch (1/196) | Top-1: 21.48 (55/256)
Train | Batch (101/196) | Top-1: 18.45 (4770/25856)
Train | Batch (196/196) | Top-1: 18.67 (9333/50000)
Regular: 6.631172180175781
Epoche: 25; regular: 6.631172180175781: flops 68618178
#Filters: 771, #FLOPs: 65.83M | Top-1: 19.77
Epoch 26
Train | Batch (1/196) | Top-1: 17.97 (46/256)
Train | Batch (101/196) | Top-1: 14.89 (3849/25856)
Train | Batch (196/196) | Top-1: 13.89 (6946/50000)
Regular: 8.155108451843262
Epoche: 26; regular: 8.155108451843262: flops 68618178
#Filters: 801, #FLOPs: 66.85M | Top-1: 8.75
Epoch 27
Train | Batch (1/196) | Top-1: 4.69 (12/256)
Train | Batch (101/196) | Top-1: 10.85 (2806/25856)
Train | Batch (196/196) | Top-1: 10.48 (5240/50000)
Regular: 7.968654155731201
Epoche: 27; regular: 7.968654155731201: flops 68618178
#Filters: 771, #FLOPs: 65.85M | Top-1: 10.00
Epoch 28
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 9.75 (2521/25856)
Train | Batch (196/196) | Top-1: 9.84 (4919/50000)
Regular: 6.756970405578613
Epoche: 28; regular: 6.756970405578613: flops 68618178
#Filters: 782, #FLOPs: 66.79M | Top-1: 10.00
Epoch 29
Train | Batch (1/196) | Top-1: 7.81 (20/256)
Train | Batch (101/196) | Top-1: 9.93 (2568/25856)
Train | Batch (196/196) | Top-1: 10.02 (5010/50000)
Regular: 6.387160778045654
Epoche: 29; regular: 6.387160778045654: flops 68618178
#Filters: 813, #FLOPs: 68.07M | Top-1: 10.00
Drin!!
Layers that will be prunned: [(11, 1), (13, 2), (17, 1), (21, 2), (23, 2), (25, 1), (27, 2)]
Prunning filters..
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 21; Pruned filters: 2
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 67.477M | #Params: 0.180M
1.0102667814134259
After Growth | FLOPs: 67.477M | #Params: 0.180M
I: 5
flops: 67476546
Before Pruning | FLOPs: 67.477M | #Params: 0.180M
Epoch 0
Train | Batch (1/196) | Top-1: 7.42 (19/256)
Train | Batch (101/196) | Top-1: 9.76 (2523/25856)
Train | Batch (196/196) | Top-1: 9.88 (4938/50000)
Regular: 6.599250793457031
Epoche: 0; regular: 6.599250793457031: flops 67476546
#Filters: 647, #FLOPs: 51.31M | Top-1: 10.00
Epoch 1
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 9.73 (2516/25856)
Train | Batch (196/196) | Top-1: 9.96 (4981/50000)
Regular: 6.146799564361572
Epoche: 1; regular: 6.146799564361572: flops 67476546
#Filters: 761, #FLOPs: 62.68M | Top-1: 10.00
Epoch 2
Train | Batch (1/196) | Top-1: 8.20 (21/256)
Train | Batch (101/196) | Top-1: 10.08 (2606/25856)
Train | Batch (196/196) | Top-1: 10.13 (5063/50000)
Regular: 6.326662540435791
Epoche: 2; regular: 6.326662540435791: flops 67476546
#Filters: 764, #FLOPs: 63.83M | Top-1: 10.00
Epoch 3
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.13 (2620/25856)
Train | Batch (196/196) | Top-1: 9.98 (4990/50000)
Regular: 6.360519886016846
Epoche: 3; regular: 6.360519886016846: flops 67476546
#Filters: 784, #FLOPs: 64.74M | Top-1: 10.70
Epoch 4
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 10.29 (2661/25856)
Train | Batch (196/196) | Top-1: 10.38 (5190/50000)
Regular: 6.408193111419678
Epoche: 4; regular: 6.408193111419678: flops 67476546
#Filters: 784, #FLOPs: 65.70M | Top-1: 10.00
Epoch 5
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 9.84 (2545/25856)
Train | Batch (196/196) | Top-1: 10.18 (5091/50000)
Regular: 6.420309066772461
Epoche: 5; regular: 6.420309066772461: flops 67476546
#Filters: 786, #FLOPs: 65.82M | Top-1: 10.46
Epoch 6
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 10.76 (2781/25856)
Train | Batch (196/196) | Top-1: 10.82 (5412/50000)
Regular: 6.40775203704834
Epoche: 6; regular: 6.40775203704834: flops 67476546
#Filters: 778, #FLOPs: 65.20M | Top-1: 10.00
Epoch 7
Train | Batch (1/196) | Top-1: 7.42 (19/256)
Train | Batch (101/196) | Top-1: 15.58 (4028/25856)
Train | Batch (196/196) | Top-1: 16.35 (8174/50000)
Regular: 7.560053825378418
Epoche: 7; regular: 7.560053825378418: flops 67476546
#Filters: 751, #FLOPs: 62.15M | Top-1: 10.00
Epoch 8
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 17.71 (4578/25856)
Train | Batch (196/196) | Top-1: 17.96 (8979/50000)
Regular: 6.808074951171875
Epoche: 8; regular: 6.808074951171875: flops 67476546
#Filters: 768, #FLOPs: 64.11M | Top-1: 16.79
Epoch 9
Train | Batch (1/196) | Top-1: 7.03 (18/256)
Train | Batch (101/196) | Top-1: 16.97 (4387/25856)
Train | Batch (196/196) | Top-1: 17.71 (8857/50000)
Regular: 7.836472034454346
Epoche: 9; regular: 7.836472034454346: flops 67476546
#Filters: 776, #FLOPs: 63.99M | Top-1: 10.41
Epoch 10
Train | Batch (1/196) | Top-1: 19.92 (51/256)
Train | Batch (101/196) | Top-1: 20.05 (5185/25856)
Train | Batch (196/196) | Top-1: 19.99 (9996/50000)
Regular: 7.8875956535339355
Epoche: 10; regular: 7.8875956535339355: flops 67476546
#Filters: 770, #FLOPs: 64.27M | Top-1: 10.00
Epoch 11
Train | Batch (1/196) | Top-1: 19.53 (50/256)
Train | Batch (101/196) | Top-1: 19.12 (4943/25856)
Train | Batch (196/196) | Top-1: 19.00 (9499/50000)
Regular: 6.922399520874023
Epoche: 11; regular: 6.922399520874023: flops 67476546
#Filters: 776, #FLOPs: 64.65M | Top-1: 10.00
Epoch 12
Train | Batch (1/196) | Top-1: 17.97 (46/256)
Train | Batch (101/196) | Top-1: 18.98 (4907/25856)
Train | Batch (196/196) | Top-1: 14.83 (7416/50000)
Regular: 6.879842758178711
Epoche: 12; regular: 6.879842758178711: flops 67476546
#Filters: 752, #FLOPs: 62.58M | Top-1: 10.00
Epoch 13
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.11 (2613/25856)
Train | Batch (196/196) | Top-1: 9.93 (4967/50000)
Regular: 6.208919525146484
Epoche: 13; regular: 6.208919525146484: flops 67476546
#Filters: 779, #FLOPs: 65.22M | Top-1: 10.00
Epoch 14
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.18 (2631/25856)
Train | Batch (196/196) | Top-1: 10.03 (5017/50000)
Regular: 6.157423496246338
Epoche: 14; regular: 6.157423496246338: flops 67476546
#Filters: 779, #FLOPs: 65.62M | Top-1: 10.00
Epoch 15
Train | Batch (1/196) | Top-1: 12.11 (31/256)
Train | Batch (101/196) | Top-1: 10.00 (2586/25856)
Train | Batch (196/196) | Top-1: 10.08 (5038/50000)
Regular: 6.170686721801758
Epoche: 15; regular: 6.170686721801758: flops 67476546
#Filters: 784, #FLOPs: 66.52M | Top-1: 9.84
Epoch 16
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 10.20 (2637/25856)
Train | Batch (196/196) | Top-1: 9.95 (4974/50000)
Regular: 6.173140525817871
Epoche: 16; regular: 6.173140525817871: flops 67476546
#Filters: 777, #FLOPs: 65.17M | Top-1: 9.71
Epoch 17
Train | Batch (1/196) | Top-1: 7.03 (18/256)
Train | Batch (101/196) | Top-1: 9.88 (2554/25856)
Train | Batch (196/196) | Top-1: 9.85 (4927/50000)
Regular: 6.195298194885254
Epoche: 17; regular: 6.195298194885254: flops 67476546
#Filters: 781, #FLOPs: 65.72M | Top-1: 10.00
Epoch 18
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 10.12 (2617/25856)
Train | Batch (196/196) | Top-1: 10.09 (5046/50000)
Regular: 6.19773006439209
Epoche: 18; regular: 6.19773006439209: flops 67476546
#Filters: 784, #FLOPs: 66.15M | Top-1: 10.00
Epoch 19
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.35 (2676/25856)
Train | Batch (196/196) | Top-1: 10.30 (5149/50000)
Regular: 6.219959259033203
Epoche: 19; regular: 6.219959259033203: flops 67476546
#Filters: 781, #FLOPs: 65.51M | Top-1: 13.29
Epoch 20
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 16.51 (4268/25856)
Train | Batch (196/196) | Top-1: 17.82 (8909/50000)
Regular: 7.5201849937438965
Epoche: 20; regular: 7.5201849937438965: flops 67476546
#Filters: 768, #FLOPs: 63.75M | Top-1: 10.00
Epoch 21
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 19.07 (4930/25856)
Train | Batch (196/196) | Top-1: 19.24 (9620/50000)
Regular: 7.969792366027832
Epoche: 21; regular: 7.969792366027832: flops 67476546
#Filters: 756, #FLOPs: 62.58M | Top-1: 19.43
Epoch 22
Train | Batch (1/196) | Top-1: 21.09 (54/256)
Train | Batch (101/196) | Top-1: 19.86 (5135/25856)
Train | Batch (196/196) | Top-1: 19.80 (9900/50000)
Regular: 6.8676323890686035
Epoche: 22; regular: 6.8676323890686035: flops 67476546
#Filters: 785, #FLOPs: 65.21M | Top-1: 10.25
Epoch 23
Train | Batch (1/196) | Top-1: 13.67 (35/256)
Train | Batch (101/196) | Top-1: 18.75 (4849/25856)
Train | Batch (196/196) | Top-1: 19.16 (9580/50000)
Regular: 6.277308940887451
Epoche: 23; regular: 6.277308940887451: flops 67476546
#Filters: 776, #FLOPs: 64.41M | Top-1: 12.44
Epoch 24
Train | Batch (1/196) | Top-1: 16.80 (43/256)
Train | Batch (101/196) | Top-1: 19.86 (5135/25856)
Train | Batch (196/196) | Top-1: 19.66 (9828/50000)
Regular: 7.09592866897583
Epoche: 24; regular: 7.09592866897583: flops 67476546
#Filters: 770, #FLOPs: 63.64M | Top-1: 18.86
Epoch 25
Train | Batch (1/196) | Top-1: 14.84 (38/256)
Train | Batch (101/196) | Top-1: 19.95 (5157/25856)
Train | Batch (196/196) | Top-1: 19.96 (9981/50000)
Regular: 7.034639358520508
Epoche: 25; regular: 7.034639358520508: flops 67476546
#Filters: 778, #FLOPs: 65.28M | Top-1: 10.04
Epoch 26
Train | Batch (1/196) | Top-1: 17.19 (44/256)
Train | Batch (101/196) | Top-1: 20.76 (5367/25856)
Train | Batch (196/196) | Top-1: 20.91 (10453/50000)
Regular: 6.5603413581848145
Epoche: 26; regular: 6.5603413581848145: flops 67476546
#Filters: 762, #FLOPs: 62.90M | Top-1: 13.14
Epoch 27
Train | Batch (1/196) | Top-1: 21.88 (56/256)
Train | Batch (101/196) | Top-1: 21.73 (5619/25856)
Train | Batch (196/196) | Top-1: 21.92 (10961/50000)
Regular: 6.355757713317871
Epoche: 27; regular: 6.355757713317871: flops 67476546
#Filters: 769, #FLOPs: 64.59M | Top-1: 9.56
Epoch 28
Train | Batch (1/196) | Top-1: 23.05 (59/256)
Train | Batch (101/196) | Top-1: 22.06 (5705/25856)
Train | Batch (196/196) | Top-1: 22.39 (11194/50000)
Regular: 6.299301624298096
Epoche: 28; regular: 6.299301624298096: flops 67476546
#Filters: 783, #FLOPs: 65.41M | Top-1: 10.02
Epoch 29
Train | Batch (1/196) | Top-1: 21.88 (56/256)
Train | Batch (101/196) | Top-1: 22.95 (5935/25856)
Train | Batch (196/196) | Top-1: 23.01 (11506/50000)
Regular: 6.442086219787598
Epoche: 29; regular: 6.442086219787598: flops 67476546
#Filters: 768, #FLOPs: 63.62M | Top-1: 22.68
Drin!!
Layers that will be prunned: [(1, 1), (3, 1), (5, 4), (7, 1), (9, 1), (11, 4), (13, 1), (15, 6), (17, 3), (19, 3), (23, 6), (25, 9), (27, 6), (29, 2)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 5; Pruned filters: 1
Layer index: 5; Pruned filters: 1
Layer index: 5; Pruned filters: 2
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 23; Pruned filters: 4
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 25; Pruned filters: 5
Layer index: 25; Pruned filters: 4
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 2
Layer index: 29; Pruned filters: 2
Target (flops): 68.863M
After Pruning | FLOPs: 59.570M | #Params: 0.145M
1.0755722266692755
After Growth | FLOPs: 68.634M | #Params: 0.166M
Epoch 0
Train | Batch (1/196) | Top-1: 24.61 (63/256)
Train | Batch (101/196) | Top-1: 24.20 (6256/25856)
Train | Batch (196/196) | Top-1: 25.19 (12597/50000)
Regular: nan
Epoche: 0; regular: nan: flops 68634336
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
#Filters: 860, #FLOPs: 68.63M | Top-1: 15.39
Epoch 1
Train | Batch (1/196) | Top-1: 25.78 (66/256)
Train | Batch (101/196) | Top-1: 29.33 (7583/25856)
Train | Batch (196/196) | Top-1: 31.66 (15832/50000)
Regular: nan
Epoche: 1; regular: nan: flops 68634336
#Filters: 865, #FLOPs: 68.63M | Top-1: 28.98
Epoch 2
Train | Batch (1/196) | Top-1: 37.11 (95/256)
Train | Batch (101/196) | Top-1: 40.23 (10401/25856)
Train | Batch (196/196) | Top-1: 42.67 (21333/50000)
Regular: nan
Epoche: 2; regular: nan: flops 68634336
#Filters: 865, #FLOPs: 68.63M | Top-1: 47.77
Epoch 3
Train | Batch (1/196) | Top-1: 49.22 (126/256)
Train | Batch (101/196) | Top-1: 51.01 (13189/25856)
Train | Batch (196/196) | Top-1: 53.41 (26706/50000)
Regular: nan
Epoche: 3; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 45.09
Epoch 4
Train | Batch (1/196) | Top-1: 52.34 (134/256)
Train | Batch (101/196) | Top-1: 59.19 (15303/25856)
Train | Batch (196/196) | Top-1: 60.32 (30161/50000)
Regular: nan
Epoche: 4; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 50.09
Epoch 5
Train | Batch (1/196) | Top-1: 60.55 (155/256)
Train | Batch (101/196) | Top-1: 63.19 (16338/25856)
Train | Batch (196/196) | Top-1: 64.33 (32163/50000)
Regular: nan
Epoche: 5; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 56.01
Epoch 6
Train | Batch (1/196) | Top-1: 63.28 (162/256)
Train | Batch (101/196) | Top-1: 66.85 (17285/25856)
Train | Batch (196/196) | Top-1: 67.21 (33607/50000)
Regular: nan
Epoche: 6; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 60.54
Epoch 7
Train | Batch (1/196) | Top-1: 65.23 (167/256)
Train | Batch (101/196) | Top-1: 69.14 (17878/25856)
Train | Batch (196/196) | Top-1: 69.49 (34743/50000)
Regular: nan
Epoche: 7; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 47.62
Epoch 8
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 70.82 (18311/25856)
Train | Batch (196/196) | Top-1: 71.42 (35711/50000)
Regular: nan
Epoche: 8; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 61.43
Epoch 9
Train | Batch (1/196) | Top-1: 71.09 (182/256)
Train | Batch (101/196) | Top-1: 72.58 (18767/25856)
Train | Batch (196/196) | Top-1: 73.05 (36524/50000)
Regular: nan
Epoche: 9; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 63.97
Epoch 10
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 74.40 (19238/25856)
Train | Batch (196/196) | Top-1: 74.77 (37386/50000)
Regular: nan
Epoche: 10; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 54.26
Epoch 11
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 75.89 (19623/25856)
Train | Batch (196/196) | Top-1: 76.12 (38062/50000)
Regular: nan
Epoche: 11; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 56.69
Epoch 12
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 76.53 (19788/25856)
Train | Batch (196/196) | Top-1: 76.86 (38432/50000)
Regular: nan
Epoche: 12; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 53.99
Epoch 13
Train | Batch (1/196) | Top-1: 75.00 (192/256)
Train | Batch (101/196) | Top-1: 77.99 (20166/25856)
Train | Batch (196/196) | Top-1: 78.02 (39012/50000)
Regular: nan
Epoche: 13; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 64.36
Epoch 14
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 78.31 (20248/25856)
Train | Batch (196/196) | Top-1: 78.37 (39183/50000)
Regular: nan
Epoche: 14; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 73.16
Epoch 15
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 79.58 (20576/25856)
Train | Batch (196/196) | Top-1: 79.48 (39742/50000)
Regular: nan
Epoche: 15; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 71.33
Epoch 16
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.64 (20592/25856)
Train | Batch (196/196) | Top-1: 79.50 (39748/50000)
Regular: nan
Epoche: 16; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 69.69
Epoch 17
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 80.22 (20741/25856)
Train | Batch (196/196) | Top-1: 80.20 (40098/50000)
Regular: nan
Epoche: 17; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 59.84
Epoch 18
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 80.58 (20835/25856)
Train | Batch (196/196) | Top-1: 80.49 (40246/50000)
Regular: nan
Epoche: 18; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 55.68
Epoch 19
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.51 (20817/25856)
Train | Batch (196/196) | Top-1: 80.53 (40267/50000)
Regular: nan
Epoche: 19; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 53.41
Epoch 20
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 80.93 (20924/25856)
Train | Batch (196/196) | Top-1: 80.98 (40488/50000)
Regular: nan
Epoche: 20; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 72.88
Epoch 21
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 81.23 (21003/25856)
Train | Batch (196/196) | Top-1: 81.41 (40707/50000)
Regular: nan
Epoche: 21; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 71.23
Epoch 22
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 81.35 (21034/25856)
Train | Batch (196/196) | Top-1: 81.32 (40658/50000)
Regular: nan
Epoche: 22; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 65.22
Epoch 23
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 81.79 (21148/25856)
Train | Batch (196/196) | Top-1: 82.03 (41014/50000)
Regular: nan
Epoche: 23; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 61.67
Epoch 24
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 82.19 (21250/25856)
Train | Batch (196/196) | Top-1: 82.18 (41092/50000)
Regular: nan
Epoche: 24; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 64.69
Epoch 25
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 82.75 (21396/25856)
Train | Batch (196/196) | Top-1: 82.38 (41191/50000)
Regular: nan
Epoche: 25; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 68.96
Epoch 26
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 82.14 (21239/25856)
Train | Batch (196/196) | Top-1: 82.22 (41112/50000)
Regular: nan
Epoche: 26; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 61.03
Epoch 27
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 82.50 (21330/25856)
Train | Batch (196/196) | Top-1: 82.56 (41281/50000)
Regular: nan
Epoche: 27; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 77.81
Epoch 28
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 83.27 (21530/25856)
Train | Batch (196/196) | Top-1: 82.78 (41392/50000)
Regular: nan
Epoche: 28; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 75.19
Epoch 29
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 83.33 (21545/25856)
Train | Batch (196/196) | Top-1: 82.95 (41476/50000)
Regular: nan
Epoche: 29; regular: nan: flops 68634336
#Filters: 867, #FLOPs: 68.63M | Top-1: 75.27
model: ResNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(14, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(18, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(15, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(25, 33, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(33, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(48, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(33, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(48, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(27, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(48, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(31, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(48, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(48, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(4, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(48, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(48, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(48, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (4): AvgPool2d(kernel_size=8, stride=8, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=48, out_features=10, bias=True)
  )
)
Test acc: 75.27000000000001
