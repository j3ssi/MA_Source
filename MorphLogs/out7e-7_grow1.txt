no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=7e-07, logger='MorphLogs/logMorphNetFlops7e-7_grow1.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 10.28 (2659/25856)
Train | Batch (196/196) | Top-1: 10.10 (5049/50000)
Regular: 12.284767150878906
Epoche: 0; regular: 12.284767150878906: flops 68862592
#Filters: 796, #FLOPs: 57.32M | Top-1: 10.11
Epoch 1
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 9.98 (2581/25856)
Train | Batch (196/196) | Top-1: 9.88 (4938/50000)
Regular: 4.037823677062988
Epoche: 1; regular: 4.037823677062988: flops 68862592
#Filters: 910, #FLOPs: 62.23M | Top-1: 10.00
Epoch 2
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 9.74 (2518/25856)
Train | Batch (196/196) | Top-1: 9.96 (4980/50000)
Regular: 4.0328497886657715
Epoche: 2; regular: 4.0328497886657715: flops 68862592
#Filters: 884, #FLOPs: 60.99M | Top-1: 10.00
Epoch 3
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 10.06 (2602/25856)
Train | Batch (196/196) | Top-1: 10.09 (5045/50000)
Regular: 4.045732498168945
Epoche: 3; regular: 4.045732498168945: flops 68862592
#Filters: 883, #FLOPs: 60.55M | Top-1: 9.92
Epoch 4
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.42 (2693/25856)
Train | Batch (196/196) | Top-1: 10.26 (5130/50000)
Regular: 4.074751377105713
Epoche: 4; regular: 4.074751377105713: flops 68862592
#Filters: 887, #FLOPs: 61.21M | Top-1: 13.74
Epoch 5
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 10.59 (2739/25856)
Train | Batch (196/196) | Top-1: 10.73 (5367/50000)
Regular: 4.093151092529297
Epoche: 5; regular: 4.093151092529297: flops 68862592
#Filters: 865, #FLOPs: 60.49M | Top-1: 10.35
Epoch 6
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.02 (2590/25856)
Train | Batch (196/196) | Top-1: 10.21 (5103/50000)
Regular: 4.051717758178711
Epoche: 6; regular: 4.051717758178711: flops 68862592
#Filters: 872, #FLOPs: 60.13M | Top-1: 10.00
Epoch 7
Train | Batch (1/196) | Top-1: 6.64 (17/256)
Train | Batch (101/196) | Top-1: 10.89 (2815/25856)
Train | Batch (196/196) | Top-1: 11.00 (5498/50000)
Regular: 4.055624485015869
Epoche: 7; regular: 4.055624485015869: flops 68862592
#Filters: 859, #FLOPs: 59.76M | Top-1: 8.93
Epoch 8
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 11.13 (2878/25856)
Train | Batch (196/196) | Top-1: 11.64 (5819/50000)
Regular: 4.091033458709717
Epoche: 8; regular: 4.091033458709717: flops 68862592
#Filters: 830, #FLOPs: 58.36M | Top-1: 15.55
Epoch 9
Train | Batch (1/196) | Top-1: 14.06 (36/256)
Train | Batch (101/196) | Top-1: 10.82 (2797/25856)
Train | Batch (196/196) | Top-1: 10.49 (5247/50000)
Regular: 4.082545280456543
Epoche: 9; regular: 4.082545280456543: flops 68862592
#Filters: 874, #FLOPs: 60.03M | Top-1: 10.00
Epoch 10
Train | Batch (1/196) | Top-1: 7.42 (19/256)
Train | Batch (101/196) | Top-1: 13.25 (3425/25856)
Train | Batch (196/196) | Top-1: 11.70 (5849/50000)
Regular: 4.2858171463012695
Epoche: 10; regular: 4.2858171463012695: flops 68862592
#Filters: 845, #FLOPs: 58.47M | Top-1: 10.00
Epoch 11
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.09 (2610/25856)
Train | Batch (196/196) | Top-1: 9.83 (4915/50000)
Regular: 4.042229652404785
Epoche: 11; regular: 4.042229652404785: flops 68862592
#Filters: 918, #FLOPs: 62.71M | Top-1: 10.00
Epoch 12
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 9.87 (2553/25856)
Train | Batch (196/196) | Top-1: 9.95 (4974/50000)
Regular: 4.038232326507568
Epoche: 12; regular: 4.038232326507568: flops 68862592
#Filters: 876, #FLOPs: 60.25M | Top-1: 10.00
Epoch 13
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 11.78 (3046/25856)
Train | Batch (196/196) | Top-1: 13.85 (6925/50000)
Regular: 4.333057403564453
Epoche: 13; regular: 4.333057403564453: flops 68862592
#Filters: 829, #FLOPs: 58.50M | Top-1: 17.65
Epoch 14
Train | Batch (1/196) | Top-1: 17.97 (46/256)
Train | Batch (101/196) | Top-1: 16.80 (4345/25856)
Train | Batch (196/196) | Top-1: 15.87 (7937/50000)
Regular: 5.067816734313965
Epoche: 14; regular: 5.067816734313965: flops 68862592
#Filters: 841, #FLOPs: 58.67M | Top-1: 9.99
Epoch 15
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 9.97 (2578/25856)
Train | Batch (196/196) | Top-1: 9.82 (4908/50000)
Regular: 4.031290054321289
Epoche: 15; regular: 4.031290054321289: flops 68862592
#Filters: 921, #FLOPs: 62.43M | Top-1: 10.00
Epoch 16
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 9.89 (2558/25856)
Train | Batch (196/196) | Top-1: 9.95 (4977/50000)
Regular: 4.023270130157471
Epoche: 16; regular: 4.023270130157471: flops 68862592
#Filters: 925, #FLOPs: 62.78M | Top-1: 9.99
Epoch 17
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 9.86 (2549/25856)
Train | Batch (196/196) | Top-1: 10.02 (5012/50000)
Regular: 4.035617351531982
Epoche: 17; regular: 4.035617351531982: flops 68862592
#Filters: 909, #FLOPs: 62.17M | Top-1: 10.47
Epoch 18
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 10.35 (2677/25856)
Train | Batch (196/196) | Top-1: 10.17 (5087/50000)
Regular: 4.020493507385254
Epoche: 18; regular: 4.020493507385254: flops 68862592
#Filters: 899, #FLOPs: 61.47M | Top-1: 10.00
Epoch 19
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 10.21 (2641/25856)
Train | Batch (196/196) | Top-1: 10.14 (5069/50000)
Regular: 4.030167579650879
Epoche: 19; regular: 4.030167579650879: flops 68862592
#Filters: 882, #FLOPs: 60.97M | Top-1: 10.00
Epoch 20
Train | Batch (1/196) | Top-1: 12.11 (31/256)
Train | Batch (101/196) | Top-1: 10.28 (2658/25856)
Train | Batch (196/196) | Top-1: 10.28 (5142/50000)
Regular: 4.03208065032959
Epoche: 20; regular: 4.03208065032959: flops 68862592
#Filters: 858, #FLOPs: 59.98M | Top-1: 10.00
Epoch 21
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.20 (2638/25856)
Train | Batch (196/196) | Top-1: 10.70 (5349/50000)
Regular: 4.03964900970459
Epoche: 21; regular: 4.03964900970459: flops 68862592
#Filters: 837, #FLOPs: 58.76M | Top-1: 10.00
Epoch 22
Train | Batch (1/196) | Top-1: 7.81 (20/256)
Train | Batch (101/196) | Top-1: 10.16 (2627/25856)
Train | Batch (196/196) | Top-1: 10.18 (5090/50000)
Regular: 4.0449700355529785
Epoche: 22; regular: 4.0449700355529785: flops 68862592
#Filters: 869, #FLOPs: 59.81M | Top-1: 10.00
Epoch 23
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.52 (2721/25856)
Train | Batch (196/196) | Top-1: 10.29 (5146/50000)
Regular: 4.031461238861084
Epoche: 23; regular: 4.031461238861084: flops 68862592
#Filters: 823, #FLOPs: 58.71M | Top-1: 10.00
Epoch 24
Train | Batch (1/196) | Top-1: 14.06 (36/256)
Train | Batch (101/196) | Top-1: 10.58 (2736/25856)
Train | Batch (196/196) | Top-1: 10.81 (5407/50000)
Regular: 4.028677940368652
Epoche: 24; regular: 4.028677940368652: flops 68862592
#Filters: 816, #FLOPs: 57.18M | Top-1: 13.59
Epoch 25
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 12.89 (3333/25856)
Train | Batch (196/196) | Top-1: 11.59 (5793/50000)
Regular: 4.148392677307129
Epoche: 25; regular: 4.148392677307129: flops 68862592
#Filters: 838, #FLOPs: 59.00M | Top-1: 10.00
Epoch 26
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.35 (2677/25856)
Train | Batch (196/196) | Top-1: 10.44 (5220/50000)
Regular: 4.002527236938477
Epoche: 26; regular: 4.002527236938477: flops 68862592
#Filters: 835, #FLOPs: 57.99M | Top-1: 10.00
Epoch 27
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.88 (2814/25856)
Train | Batch (196/196) | Top-1: 10.64 (5319/50000)
Regular: 4.01871395111084
Epoche: 27; regular: 4.01871395111084: flops 68862592
#Filters: 827, #FLOPs: 58.58M | Top-1: 9.97
Epoch 28
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.17 (2629/25856)
Train | Batch (196/196) | Top-1: 10.55 (5273/50000)
Regular: 4.031083106994629
Epoche: 28; regular: 4.031083106994629: flops 68862592
#Filters: 823, #FLOPs: 58.56M | Top-1: 10.00
Epoch 29
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 10.19 (2636/25856)
Train | Batch (196/196) | Top-1: 11.24 (5618/50000)
Regular: 4.060194969177246
Epoche: 29; regular: 4.060194969177246: flops 68862592
#Filters: 829, #FLOPs: 57.90M | Top-1: 17.72
Drin!!
Layers that will be prunned: [(1, 4), (3, 2), (5, 1), (7, 4), (9, 3), (11, 5), (13, 7), (15, 5), (17, 7), (19, 10), (21, 47), (22, 2), (23, 37), (24, 2), (25, 39), (26, 2), (27, 39), (28, 2), (29, 30), (30, 2)]
Prunning filters..
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 1
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 1
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 1
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 1
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 1
Layer index: 1; Pruned filters: 2
Layer index: 1; Pruned filters: 2
Layer index: 3; Pruned filters: 2
Layer index: 5; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 7; Pruned filters: 2
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 2
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 2
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 2
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 6
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 3
Layer index: 21; Pruned filters: 11
Layer index: 21; Pruned filters: 7
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 9
Layer index: 21; Pruned filters: 4
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 4
Layer index: 23; Pruned filters: 4
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 3
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 4
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 5
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 3
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 3
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 6
Layer index: 29; Pruned filters: 3
Target (flops): 68.863M
After Pruning | FLOPs: 46.340M | #Params: 0.227M
1.2200827312370237
After Growth | FLOPs: 70.122M | #Params: 0.341M
I: 1
flops: 70122424
Before Pruning | FLOPs: 70.122M | #Params: 0.341M
Epoch 0
Train | Batch (1/196) | Top-1: 16.80 (43/256)
Train | Batch (101/196) | Top-1: 14.67 (3793/25856)
Train | Batch (196/196) | Top-1: 16.30 (8148/50000)
Regular: 8.741996765136719
Epoche: 0; regular: 8.741996765136719: flops 70122424
#Filters: 729, #FLOPs: 58.30M | Top-1: 10.01
Epoch 1
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 18.30 (4731/25856)
Train | Batch (196/196) | Top-1: 18.19 (9093/50000)
Regular: 5.490346431732178
Epoche: 1; regular: 5.490346431732178: flops 70122424
#Filters: 777, #FLOPs: 59.03M | Top-1: 10.07
Epoch 2
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 15.94 (4122/25856)
Train | Batch (196/196) | Top-1: 16.93 (8464/50000)
Regular: 7.319733142852783
Epoche: 2; regular: 7.319733142852783: flops 70122424
#Filters: 778, #FLOPs: 61.14M | Top-1: 20.64
Epoch 3
Train | Batch (1/196) | Top-1: 23.83 (61/256)
Train | Batch (101/196) | Top-1: 17.92 (4634/25856)
Train | Batch (196/196) | Top-1: 17.58 (8789/50000)
Regular: 7.219808578491211
Epoche: 3; regular: 7.219808578491211: flops 70122424
#Filters: 799, #FLOPs: 62.56M | Top-1: 14.19
Epoch 4
Train | Batch (1/196) | Top-1: 20.70 (53/256)
Train | Batch (101/196) | Top-1: 14.08 (3641/25856)
Train | Batch (196/196) | Top-1: 12.19 (6096/50000)
Regular: 5.945249080657959
Epoche: 4; regular: 5.945249080657959: flops 70122424
#Filters: 847, #FLOPs: 61.54M | Top-1: 10.00
Epoch 5
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 9.90 (2559/25856)
Train | Batch (196/196) | Top-1: 9.93 (4965/50000)
Regular: 5.1418561935424805
Epoche: 5; regular: 5.1418561935424805: flops 70122424
#Filters: 830, #FLOPs: 64.33M | Top-1: 9.83
Epoch 6
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 9.84 (2545/25856)
Train | Batch (196/196) | Top-1: 10.01 (5003/50000)
Regular: 5.146744728088379
Epoche: 6; regular: 5.146744728088379: flops 70122424
#Filters: 894, #FLOPs: 65.45M | Top-1: 10.00
Epoch 7
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.03 (2593/25856)
Train | Batch (196/196) | Top-1: 10.09 (5044/50000)
Regular: 5.166961669921875
Epoche: 7; regular: 5.166961669921875: flops 70122424
#Filters: 841, #FLOPs: 64.67M | Top-1: 9.65
Epoch 8
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 9.89 (2558/25856)
Train | Batch (196/196) | Top-1: 9.83 (4913/50000)
Regular: 5.176715850830078
Epoche: 8; regular: 5.176715850830078: flops 70122424
#Filters: 820, #FLOPs: 63.95M | Top-1: 10.00
Epoch 9
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 10.21 (2641/25856)
Train | Batch (196/196) | Top-1: 10.36 (5178/50000)
Regular: 5.214579105377197
Epoche: 9; regular: 5.214579105377197: flops 70122424
#Filters: 861, #FLOPs: 64.13M | Top-1: 10.98
Epoch 10
Train | Batch (1/196) | Top-1: 7.81 (20/256)
Train | Batch (101/196) | Top-1: 10.42 (2695/25856)
Train | Batch (196/196) | Top-1: 10.21 (5104/50000)
Regular: 5.2248334884643555
Epoche: 10; regular: 5.2248334884643555: flops 70122424
#Filters: 794, #FLOPs: 62.11M | Top-1: 10.00
Epoch 11
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 10.42 (2693/25856)
Train | Batch (196/196) | Top-1: 10.48 (5240/50000)
Regular: 5.218369960784912
Epoche: 11; regular: 5.218369960784912: flops 70122424
#Filters: 813, #FLOPs: 63.05M | Top-1: 10.00
Epoch 12
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 10.44 (2700/25856)
Train | Batch (196/196) | Top-1: 11.27 (5633/50000)
Regular: 5.241001605987549
Epoche: 12; regular: 5.241001605987549: flops 70122424
#Filters: 881, #FLOPs: 64.81M | Top-1: 10.00
Epoch 13
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 16.26 (4204/25856)
Train | Batch (196/196) | Top-1: 16.62 (8312/50000)
Regular: 6.498840808868408
Epoche: 13; regular: 6.498840808868408: flops 70122424
#Filters: 793, #FLOPs: 61.61M | Top-1: 10.61
Epoch 14
Train | Batch (1/196) | Top-1: 21.09 (54/256)
Train | Batch (101/196) | Top-1: 18.25 (4718/25856)
Train | Batch (196/196) | Top-1: 18.04 (9018/50000)
Regular: 5.516345024108887
Epoche: 14; regular: 5.516345024108887: flops 70122424
#Filters: 821, #FLOPs: 63.13M | Top-1: 19.78
Epoch 15
Train | Batch (1/196) | Top-1: 17.19 (44/256)
Train | Batch (101/196) | Top-1: 17.64 (4561/25856)
Train | Batch (196/196) | Top-1: 17.93 (8966/50000)
Regular: 5.3453898429870605
Epoche: 15; regular: 5.3453898429870605: flops 70122424
#Filters: 872, #FLOPs: 63.99M | Top-1: 18.59
Epoch 16
Train | Batch (1/196) | Top-1: 18.36 (47/256)
Train | Batch (101/196) | Top-1: 17.93 (4637/25856)
Train | Batch (196/196) | Top-1: 17.42 (8708/50000)
Regular: 5.917109489440918
Epoche: 16; regular: 5.917109489440918: flops 70122424
#Filters: 807, #FLOPs: 62.75M | Top-1: 16.30
Epoch 17
Train | Batch (1/196) | Top-1: 18.75 (48/256)
Train | Batch (101/196) | Top-1: 16.79 (4341/25856)
Train | Batch (196/196) | Top-1: 16.44 (8221/50000)
Regular: 8.406947135925293
Epoche: 17; regular: 8.406947135925293: flops 70122424
#Filters: 788, #FLOPs: 61.79M | Top-1: 9.18
Epoch 18
Train | Batch (1/196) | Top-1: 6.64 (17/256)
Train | Batch (101/196) | Top-1: 17.49 (4522/25856)
Train | Batch (196/196) | Top-1: 17.79 (8896/50000)
Regular: 10.134061813354492
Epoche: 18; regular: 10.134061813354492: flops 70122424
#Filters: 870, #FLOPs: 63.90M | Top-1: 12.42
Epoch 19
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 18.33 (4740/25856)
Train | Batch (196/196) | Top-1: 17.83 (8916/50000)
Regular: 5.596043109893799
Epoche: 19; regular: 5.596043109893799: flops 70122424
#Filters: 824, #FLOPs: 63.80M | Top-1: 10.00
Epoch 20
Train | Batch (1/196) | Top-1: 16.02 (41/256)
Train | Batch (101/196) | Top-1: 18.00 (4655/25856)
Train | Batch (196/196) | Top-1: 18.40 (9200/50000)
Regular: 7.137914657592773
Epoche: 20; regular: 7.137914657592773: flops 70122424
#Filters: 792, #FLOPs: 61.76M | Top-1: 18.39
Epoch 21
Train | Batch (1/196) | Top-1: 13.67 (35/256)
Train | Batch (101/196) | Top-1: 18.36 (4746/25856)
Train | Batch (196/196) | Top-1: 18.25 (9124/50000)
Regular: 6.338092803955078
Epoche: 21; regular: 6.338092803955078: flops 70122424
#Filters: 855, #FLOPs: 64.25M | Top-1: 15.84
Epoch 22
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 18.31 (4735/25856)
Train | Batch (196/196) | Top-1: 18.50 (9249/50000)
Regular: 5.956258773803711
Epoche: 22; regular: 5.956258773803711: flops 70122424
#Filters: 865, #FLOPs: 64.88M | Top-1: 19.13
Epoch 23
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 18.41 (4760/25856)
Train | Batch (196/196) | Top-1: 18.02 (9012/50000)
Regular: 5.689986228942871
Epoche: 23; regular: 5.689986228942871: flops 70122424
#Filters: 806, #FLOPs: 63.31M | Top-1: 16.97
Epoch 24
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 18.27 (4723/25856)
Train | Batch (196/196) | Top-1: 14.29 (7147/50000)
Regular: 5.339967250823975
Epoche: 24; regular: 5.339967250823975: flops 70122424
#Filters: 806, #FLOPs: 63.07M | Top-1: 10.00
Epoch 25
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 9.90 (2561/25856)
Train | Batch (196/196) | Top-1: 9.79 (4897/50000)
Regular: 5.214701175689697
Epoche: 25; regular: 5.214701175689697: flops 70122424
#Filters: 814, #FLOPs: 63.53M | Top-1: 10.00
Epoch 26
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.06 (2601/25856)
Train | Batch (196/196) | Top-1: 9.80 (4898/50000)
Regular: 5.216831207275391
Epoche: 26; regular: 5.216831207275391: flops 70122424
#Filters: 828, #FLOPs: 64.17M | Top-1: 10.00
Epoch 27
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 9.78 (2528/25856)
Train | Batch (196/196) | Top-1: 10.04 (5022/50000)
Regular: 5.222322940826416
Epoche: 27; regular: 5.222322940826416: flops 70122424
#Filters: 837, #FLOPs: 64.96M | Top-1: 10.00
Epoch 28
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 9.84 (2544/25856)
Train | Batch (196/196) | Top-1: 9.89 (4944/50000)
Regular: 5.2233567237854
Epoche: 28; regular: 5.2233567237854: flops 70122424
#Filters: 869, #FLOPs: 65.25M | Top-1: 10.00
Epoch 29
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 9.73 (2517/25856)
Train | Batch (196/196) | Top-1: 9.85 (4927/50000)
Regular: 5.2267069816589355
Epoche: 29; regular: 5.2267069816589355: flops 70122424
#Filters: 832, #FLOPs: 65.12M | Top-1: 10.00
Drin!!
Layers that will be prunned: [(1, 1), (11, 1), (15, 1), (19, 2), (21, 13), (22, 34), (23, 7), (24, 34), (25, 3), (26, 34), (27, 7), (28, 34), (29, 9), (30, 34)]
Prunning filters..
Layer index: 22; Pruned filters: 9
Layer index: 22; Pruned filters: 4
Layer index: 22; Pruned filters: 21
Layer index: 24; Pruned filters: 9
Layer index: 24; Pruned filters: 4
Layer index: 24; Pruned filters: 21
Layer index: 26; Pruned filters: 9
Layer index: 26; Pruned filters: 4
Layer index: 26; Pruned filters: 21
Layer index: 28; Pruned filters: 9
Layer index: 28; Pruned filters: 4
Layer index: 28; Pruned filters: 21
Layer index: 30; Pruned filters: 9
Layer index: 30; Pruned filters: 4
Layer index: 30; Pruned filters: 21
Layer index: 1; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 19; Pruned filters: 2
Layer index: 21; Pruned filters: 3
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 8
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 61.476M | #Params: 0.219M
1.0586394426672048
After Growth | FLOPs: 68.466M | #Params: 0.244M
I: 2
flops: 68466104
Before Pruning | FLOPs: 68.466M | #Params: 0.244M
Epoch 0
Train | Batch (1/196) | Top-1: 7.03 (18/256)
Train | Batch (101/196) | Top-1: 9.80 (2533/25856)
Train | Batch (196/196) | Top-1: 9.75 (4877/50000)
Regular: 6.400760173797607
Epoche: 0; regular: 6.400760173797607: flops 68466104
#Filters: 600, #FLOPs: 49.99M | Top-1: 10.00
Epoch 1
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.31 (2665/25856)
Train | Batch (196/196) | Top-1: 10.16 (5080/50000)
Regular: 5.704391956329346
Epoche: 1; regular: 5.704391956329346: flops 68466104
#Filters: 764, #FLOPs: 60.88M | Top-1: 10.00
Epoch 2
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 9.63 (2491/25856)
Train | Batch (196/196) | Top-1: 9.76 (4879/50000)
Regular: 5.874438285827637
Epoche: 2; regular: 5.874438285827637: flops 68466104
#Filters: 751, #FLOPs: 62.40M | Top-1: 10.00
Epoch 3
Train | Batch (1/196) | Top-1: 8.20 (21/256)
Train | Batch (101/196) | Top-1: 10.57 (2734/25856)
Train | Batch (196/196) | Top-1: 10.62 (5308/50000)
Regular: 5.909640789031982
Epoche: 3; regular: 5.909640789031982: flops 68466104
#Filters: 783, #FLOPs: 65.36M | Top-1: 11.94
Epoch 4
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 10.70 (2767/25856)
Train | Batch (196/196) | Top-1: 11.14 (5572/50000)
Regular: 5.8558268547058105
Epoche: 4; regular: 5.8558268547058105: flops 68466104
#Filters: 766, #FLOPs: 63.85M | Top-1: 10.00
Epoch 5
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.93 (2826/25856)
Train | Batch (196/196) | Top-1: 10.97 (5483/50000)
Regular: 5.802757263183594
Epoche: 5; regular: 5.802757263183594: flops 68466104
#Filters: 745, #FLOPs: 63.01M | Top-1: 10.00
Epoch 6
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.96 (2833/25856)
Train | Batch (196/196) | Top-1: 10.84 (5420/50000)
Regular: 5.823372840881348
Epoche: 6; regular: 5.823372840881348: flops 68466104
#Filters: 737, #FLOPs: 62.27M | Top-1: 4.91
Epoch 7
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 11.01 (2846/25856)
Train | Batch (196/196) | Top-1: 12.67 (6336/50000)
Regular: 5.983268737792969
Epoche: 7; regular: 5.983268737792969: flops 68466104
#Filters: 765, #FLOPs: 63.27M | Top-1: 6.68
Epoch 8
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 17.65 (4563/25856)
Train | Batch (196/196) | Top-1: 17.74 (8872/50000)
Regular: 7.777811050415039
Epoche: 8; regular: 7.777811050415039: flops 68466104
#Filters: 733, #FLOPs: 61.92M | Top-1: 19.49
Epoch 9
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 18.85 (4875/25856)
Train | Batch (196/196) | Top-1: 18.55 (9276/50000)
Regular: 6.783295154571533
Epoche: 9; regular: 6.783295154571533: flops 68466104
#Filters: 743, #FLOPs: 63.14M | Top-1: 14.41
Epoch 10
Train | Batch (1/196) | Top-1: 22.27 (57/256)
Train | Batch (101/196) | Top-1: 18.35 (4744/25856)
Train | Batch (196/196) | Top-1: 18.52 (9260/50000)
Regular: 7.1917901039123535
Epoche: 10; regular: 7.1917901039123535: flops 68466104
#Filters: 744, #FLOPs: 62.61M | Top-1: 10.01
Epoch 11
Train | Batch (1/196) | Top-1: 23.44 (60/256)
Train | Batch (101/196) | Top-1: 18.60 (4808/25856)
Train | Batch (196/196) | Top-1: 18.54 (9268/50000)
Regular: 7.139847755432129
Epoche: 11; regular: 7.139847755432129: flops 68466104
#Filters: 771, #FLOPs: 63.29M | Top-1: 22.15
Epoch 12
Train | Batch (1/196) | Top-1: 18.36 (47/256)
Train | Batch (101/196) | Top-1: 18.98 (4908/25856)
Train | Batch (196/196) | Top-1: 17.53 (8764/50000)
Regular: 7.213559627532959
Epoche: 12; regular: 7.213559627532959: flops 68466104
#Filters: 756, #FLOPs: 63.70M | Top-1: 10.00
Epoch 13
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 16.28 (4209/25856)
Train | Batch (196/196) | Top-1: 16.09 (8043/50000)
Regular: 8.690069198608398
Epoche: 13; regular: 8.690069198608398: flops 68466104
#Filters: 748, #FLOPs: 63.16M | Top-1: 10.00
Epoch 14
Train | Batch (1/196) | Top-1: 21.48 (55/256)
Train | Batch (101/196) | Top-1: 18.43 (4765/25856)
Train | Batch (196/196) | Top-1: 18.30 (9152/50000)
Regular: 7.034309387207031
Epoche: 14; regular: 7.034309387207031: flops 68466104
#Filters: 771, #FLOPs: 63.71M | Top-1: 10.00
Epoch 15
Train | Batch (1/196) | Top-1: 16.80 (43/256)
Train | Batch (101/196) | Top-1: 11.79 (3049/25856)
Train | Batch (196/196) | Top-1: 10.94 (5470/50000)
Regular: 7.2243828773498535
Epoche: 15; regular: 7.2243828773498535: flops 68466104
#Filters: 737, #FLOPs: 62.79M | Top-1: 10.00
Epoch 16
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 9.85 (2546/25856)
Train | Batch (196/196) | Top-1: 9.86 (4929/50000)
Regular: 5.84586763381958
Epoche: 16; regular: 5.84586763381958: flops 68466104
#Filters: 775, #FLOPs: 64.59M | Top-1: 9.24
Epoch 17
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 9.72 (2514/25856)
Train | Batch (196/196) | Top-1: 9.89 (4943/50000)
Regular: 5.77241849899292
Epoche: 17; regular: 5.77241849899292: flops 68466104
#Filters: 796, #FLOPs: 65.16M | Top-1: 10.00
Epoch 18
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 9.89 (2557/25856)
Train | Batch (196/196) | Top-1: 9.96 (4980/50000)
Regular: 5.796270847320557
Epoche: 18; regular: 5.796270847320557: flops 68466104
#Filters: 781, #FLOPs: 65.46M | Top-1: 12.55
Epoch 19
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.00 (2586/25856)
Train | Batch (196/196) | Top-1: 10.02 (5009/50000)
Regular: 5.806528568267822
Epoche: 19; regular: 5.806528568267822: flops 68466104
#Filters: 766, #FLOPs: 64.66M | Top-1: 10.00
Epoch 20
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.29 (2661/25856)
Train | Batch (196/196) | Top-1: 10.40 (5200/50000)
Regular: 5.7887163162231445
Epoche: 20; regular: 5.7887163162231445: flops 68466104
#Filters: 781, #FLOPs: 64.80M | Top-1: 10.00
Epoch 21
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 10.42 (2694/25856)
Train | Batch (196/196) | Top-1: 10.35 (5176/50000)
Regular: 5.75775146484375
Epoche: 21; regular: 5.75775146484375: flops 68466104
#Filters: 753, #FLOPs: 63.40M | Top-1: 10.00
Epoch 22
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 10.30 (2663/25856)
Train | Batch (196/196) | Top-1: 10.41 (5205/50000)
Regular: 5.772947311401367
Epoche: 22; regular: 5.772947311401367: flops 68466104
#Filters: 747, #FLOPs: 63.86M | Top-1: 10.00
Epoch 23
Train | Batch (1/196) | Top-1: 16.02 (41/256)
Train | Batch (101/196) | Top-1: 11.09 (2867/25856)
Train | Batch (196/196) | Top-1: 10.89 (5446/50000)
Regular: 5.748373031616211
Epoche: 23; regular: 5.748373031616211: flops 68466104
#Filters: 763, #FLOPs: 63.44M | Top-1: 10.00
Epoch 24
Train | Batch (1/196) | Top-1: 5.86 (15/256)
Train | Batch (101/196) | Top-1: 13.65 (3530/25856)
Train | Batch (196/196) | Top-1: 14.80 (7398/50000)
Regular: 6.453548908233643
Epoche: 24; regular: 6.453548908233643: flops 68466104
#Filters: 759, #FLOPs: 62.77M | Top-1: 11.03
Epoch 25
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 17.95 (4640/25856)
Train | Batch (196/196) | Top-1: 17.71 (8853/50000)
Regular: 6.894751071929932
Epoche: 25; regular: 6.894751071929932: flops 68466104
#Filters: 741, #FLOPs: 62.80M | Top-1: 10.10
Epoch 26
Train | Batch (1/196) | Top-1: 12.89 (33/256)
Train | Batch (101/196) | Top-1: 16.97 (4389/25856)
Train | Batch (196/196) | Top-1: 17.05 (8525/50000)
Regular: 7.640944957733154
Epoche: 26; regular: 7.640944957733154: flops 68466104
#Filters: 769, #FLOPs: 63.54M | Top-1: 10.29
Epoch 27
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 18.36 (4748/25856)
Train | Batch (196/196) | Top-1: 18.24 (9118/50000)
Regular: 6.569687366485596
Epoche: 27; regular: 6.569687366485596: flops 68466104
#Filters: 767, #FLOPs: 64.35M | Top-1: 11.81
Epoch 28
Train | Batch (1/196) | Top-1: 21.88 (56/256)
Train | Batch (101/196) | Top-1: 10.05 (2598/25856)
Train | Batch (196/196) | Top-1: 10.07 (5037/50000)
Regular: 5.955003261566162
Epoche: 28; regular: 5.955003261566162: flops 68466104
#Filters: 749, #FLOPs: 63.54M | Top-1: 10.00
Epoch 29
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 9.98 (2580/25856)
Train | Batch (196/196) | Top-1: 9.94 (4968/50000)
Regular: 5.734017372131348
Epoche: 29; regular: 5.734017372131348: flops 68466104
#Filters: 787, #FLOPs: 64.82M | Top-1: 10.33
Drin!!
Layers that will be prunned: [(1, 1), (3, 2), (7, 2), (13, 1), (15, 6), (17, 2), (21, 4), (22, 3), (23, 9), (24, 3), (25, 15), (26, 3), (27, 9), (28, 3), (29, 27), (30, 3)]
Prunning filters..
Layer index: 22; Pruned filters: 3
Layer index: 24; Pruned filters: 3
Layer index: 26; Pruned filters: 3
Layer index: 28; Pruned filters: 3
Layer index: 30; Pruned filters: 3
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 7; Pruned filters: 2
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 2
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 2
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 3
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 7
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 5
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 7
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 10
Target (flops): 68.863M
After Pruning | FLOPs: 61.389M | #Params: 0.182M
1.0594012321795279
After Growth | FLOPs: 68.406M | #Params: 0.202M
I: 3
flops: 68406190
Before Pruning | FLOPs: 68.406M | #Params: 0.202M
Epoch 0
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 9.92 (2565/25856)
Train | Batch (196/196) | Top-1: 9.95 (4974/50000)
Regular: 6.793432712554932
Epoche: 0; regular: 6.793432712554932: flops 68406190
#Filters: 533, #FLOPs: 46.53M | Top-1: 10.00
Epoch 1
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.21 (2641/25856)
Train | Batch (196/196) | Top-1: 10.45 (5224/50000)
Regular: 6.031591892242432
Epoche: 1; regular: 6.031591892242432: flops 68406190
#Filters: 743, #FLOPs: 61.08M | Top-1: 10.00
Epoch 2
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.49 (2713/25856)
Train | Batch (196/196) | Top-1: 10.76 (5381/50000)
Regular: 6.160427093505859
Epoche: 2; regular: 6.160427093505859: flops 68406190
#Filters: 756, #FLOPs: 64.28M | Top-1: 14.16
Epoch 3
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 11.28 (2917/25856)
Train | Batch (196/196) | Top-1: 10.98 (5492/50000)
Regular: 6.157285213470459
Epoche: 3; regular: 6.157285213470459: flops 68406190
#Filters: 762, #FLOPs: 64.22M | Top-1: 10.00
Epoch 4
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 10.38 (2683/25856)
Train | Batch (196/196) | Top-1: 10.50 (5252/50000)
Regular: 6.126031875610352
Epoche: 4; regular: 6.126031875610352: flops 68406190
#Filters: 739, #FLOPs: 62.98M | Top-1: 13.68
Epoch 5
Train | Batch (1/196) | Top-1: 15.23 (39/256)
Train | Batch (101/196) | Top-1: 10.59 (2738/25856)
Train | Batch (196/196) | Top-1: 10.68 (5341/50000)
Regular: 6.136279106140137
Epoche: 5; regular: 6.136279106140137: flops 68406190
#Filters: 744, #FLOPs: 63.60M | Top-1: 9.99
Epoch 6
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 10.91 (2821/25856)
Train | Batch (196/196) | Top-1: 10.67 (5334/50000)
Regular: 6.160379886627197
Epoche: 6; regular: 6.160379886627197: flops 68406190
#Filters: 760, #FLOPs: 63.82M | Top-1: 10.00
Epoch 7
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 10.39 (2686/25856)
Train | Batch (196/196) | Top-1: 10.86 (5428/50000)
Regular: 6.169563293457031
Epoche: 7; regular: 6.169563293457031: flops 68406190
#Filters: 745, #FLOPs: 63.98M | Top-1: 10.00
Epoch 8
Train | Batch (1/196) | Top-1: 8.20 (21/256)
Train | Batch (101/196) | Top-1: 10.66 (2756/25856)
Train | Batch (196/196) | Top-1: 10.69 (5344/50000)
Regular: 6.137699127197266
Epoche: 8; regular: 6.137699127197266: flops 68406190
#Filters: 751, #FLOPs: 63.91M | Top-1: 10.00
Epoch 9
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.59 (2739/25856)
Train | Batch (196/196) | Top-1: 10.82 (5408/50000)
Regular: 6.154770851135254
Epoche: 9; regular: 6.154770851135254: flops 68406190
#Filters: 771, #FLOPs: 64.53M | Top-1: 9.03
Epoch 10
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 10.25 (2650/25856)
Train | Batch (196/196) | Top-1: 10.38 (5192/50000)
Regular: 6.138687610626221
Epoche: 10; regular: 6.138687610626221: flops 68406190
#Filters: 737, #FLOPs: 62.63M | Top-1: 10.00
Epoch 11
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 10.87 (2810/25856)
Train | Batch (196/196) | Top-1: 10.79 (5396/50000)
Regular: 6.148536682128906
Epoche: 11; regular: 6.148536682128906: flops 68406190
#Filters: 747, #FLOPs: 63.16M | Top-1: 10.00
Epoch 12
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.75 (2780/25856)
Train | Batch (196/196) | Top-1: 10.82 (5409/50000)
Regular: 6.1139326095581055
Epoche: 12; regular: 6.1139326095581055: flops 68406190
#Filters: 766, #FLOPs: 64.51M | Top-1: 10.74
Epoch 13
Train | Batch (1/196) | Top-1: 16.80 (43/256)
Train | Batch (101/196) | Top-1: 11.18 (2890/25856)
Train | Batch (196/196) | Top-1: 10.97 (5483/50000)
Regular: 6.124778747558594
Epoche: 13; regular: 6.124778747558594: flops 68406190
#Filters: 743, #FLOPs: 63.92M | Top-1: 11.56
Epoch 14
Train | Batch (1/196) | Top-1: 14.06 (36/256)
Train | Batch (101/196) | Top-1: 10.80 (2792/25856)
Train | Batch (196/196) | Top-1: 10.89 (5444/50000)
Regular: 6.139009475708008
Epoche: 14; regular: 6.139009475708008: flops 68406190
#Filters: 747, #FLOPs: 64.22M | Top-1: 10.00
Epoch 15
Train | Batch (1/196) | Top-1: 14.84 (38/256)
Train | Batch (101/196) | Top-1: 11.00 (2845/25856)
Train | Batch (196/196) | Top-1: 10.94 (5468/50000)
Regular: 6.100285530090332
Epoche: 15; regular: 6.100285530090332: flops 68406190
#Filters: 774, #FLOPs: 65.36M | Top-1: 9.76
Epoch 16
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 10.62 (2746/25856)
Train | Batch (196/196) | Top-1: 10.70 (5352/50000)
Regular: 6.1307454109191895
Epoche: 16; regular: 6.1307454109191895: flops 68406190
#Filters: 752, #FLOPs: 64.22M | Top-1: 10.00
Epoch 17
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 11.63 (3007/25856)
Train | Batch (196/196) | Top-1: 11.41 (5706/50000)
Regular: 6.155381679534912
Epoche: 17; regular: 6.155381679534912: flops 68406190
#Filters: 743, #FLOPs: 63.87M | Top-1: 16.55
Epoch 18
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.72 (2771/25856)
Train | Batch (196/196) | Top-1: 10.52 (5260/50000)
Regular: 6.168530464172363
Epoche: 18; regular: 6.168530464172363: flops 68406190
#Filters: 763, #FLOPs: 64.47M | Top-1: 13.25
Epoch 19
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 11.65 (3011/25856)
Train | Batch (196/196) | Top-1: 11.58 (5790/50000)
Regular: 6.159823894500732
Epoche: 19; regular: 6.159823894500732: flops 68406190
#Filters: 748, #FLOPs: 64.14M | Top-1: 10.35
Epoch 20
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 11.50 (2974/25856)
Train | Batch (196/196) | Top-1: 11.17 (5585/50000)
Regular: 6.163486480712891
Epoche: 20; regular: 6.163486480712891: flops 68406190
#Filters: 755, #FLOPs: 64.52M | Top-1: 7.51
Epoch 21
Train | Batch (1/196) | Top-1: 7.81 (20/256)
Train | Batch (101/196) | Top-1: 10.62 (2745/25856)
Train | Batch (196/196) | Top-1: 10.77 (5386/50000)
Regular: 6.147829532623291
Epoche: 21; regular: 6.147829532623291: flops 68406190
#Filters: 748, #FLOPs: 64.24M | Top-1: 10.03
Epoch 22
Train | Batch (1/196) | Top-1: 10.94 (28/256)
Train | Batch (101/196) | Top-1: 10.44 (2699/25856)
Train | Batch (196/196) | Top-1: 10.57 (5287/50000)
Regular: 6.120382308959961
Epoche: 22; regular: 6.120382308959961: flops 68406190
#Filters: 747, #FLOPs: 64.52M | Top-1: 10.00
Epoch 23
Train | Batch (1/196) | Top-1: 14.06 (36/256)
Train | Batch (101/196) | Top-1: 10.87 (2811/25856)
Train | Batch (196/196) | Top-1: 10.72 (5362/50000)
Regular: 6.178775787353516
Epoche: 23; regular: 6.178775787353516: flops 68406190
#Filters: 767, #FLOPs: 64.53M | Top-1: 13.97
Epoch 24
Train | Batch (1/196) | Top-1: 13.67 (35/256)
Train | Batch (101/196) | Top-1: 10.58 (2735/25856)
Train | Batch (196/196) | Top-1: 11.21 (5604/50000)
Regular: 6.195748329162598
Epoche: 24; regular: 6.195748329162598: flops 68406190
#Filters: 745, #FLOPs: 64.45M | Top-1: 10.00
Epoch 25
Train | Batch (1/196) | Top-1: 10.16 (26/256)
Train | Batch (101/196) | Top-1: 10.24 (2647/25856)
Train | Batch (196/196) | Top-1: 10.44 (5219/50000)
Regular: 6.134030818939209
Epoche: 25; regular: 6.134030818939209: flops 68406190
#Filters: 745, #FLOPs: 63.26M | Top-1: 10.00
Epoch 26
Train | Batch (1/196) | Top-1: 11.33 (29/256)
Train | Batch (101/196) | Top-1: 11.01 (2846/25856)
Train | Batch (196/196) | Top-1: 10.78 (5388/50000)
Regular: 6.181869983673096
Epoche: 26; regular: 6.181869983673096: flops 68406190
#Filters: 777, #FLOPs: 65.50M | Top-1: 10.00
Epoch 27
Train | Batch (1/196) | Top-1: 13.28 (34/256)
Train | Batch (101/196) | Top-1: 11.36 (2936/25856)
Train | Batch (196/196) | Top-1: 11.76 (5881/50000)
Regular: 6.1748762130737305
Epoche: 27; regular: 6.1748762130737305: flops 68406190
#Filters: 745, #FLOPs: 63.87M | Top-1: 12.08
Epoch 28
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 16.05 (4151/25856)
Train | Batch (196/196) | Top-1: 13.41 (6705/50000)
Regular: 6.677655220031738
Epoche: 28; regular: 6.677655220031738: flops 68406190
#Filters: 743, #FLOPs: 63.86M | Top-1: 10.00
Epoch 29
Train | Batch (1/196) | Top-1: 8.98 (23/256)
Train | Batch (101/196) | Top-1: 10.04 (2597/25856)
Train | Batch (196/196) | Top-1: 10.25 (5123/50000)
Regular: 6.077042579650879
Epoche: 29; regular: 6.077042579650879: flops 68406190
#Filters: 783, #FLOPs: 65.12M | Top-1: 10.00
Drin!!
Layers that will be prunned: [(1, 2), (5, 2), (9, 4), (11, 2), (13, 2), (15, 1), (17, 2), (19, 4), (21, 1), (23, 9), (25, 6), (27, 7), (29, 4)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 1
Layer index: 5; Pruned filters: 1
Layer index: 5; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 2
Target (flops): 68.863M
After Pruning | FLOPs: 61.742M | #Params: 0.170M
1.056369998556786
After Growth | FLOPs: 68.618M | #Params: 0.189M
I: 4
flops: 68618178
Before Pruning | FLOPs: 68.618M | #Params: 0.189M
Epoch 0
Train | Batch (1/196) | Top-1: 8.20 (21/256)
Train | Batch (101/196) | Top-1: 10.07 (2603/25856)
Train | Batch (196/196) | Top-1: 10.10 (5048/50000)
Regular: 6.986482620239258
Epoche: 0; regular: 6.986482620239258: flops 68618178
#Filters: 685, #FLOPs: 56.11M | Top-1: 10.00
Epoch 1
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 10.01 (2587/25856)
Train | Batch (196/196) | Top-1: 10.03 (5016/50000)
Regular: 6.264232635498047
Epoche: 1; regular: 6.264232635498047: flops 68618178
#Filters: 772, #FLOPs: 63.02M | Top-1: 10.00
Epoch 2
