no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=0.0, logger='MorphLogs/logTest.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 11.72 (30/256)
Train | Batch (101/196) | Top-1: 17.32 (4478/25856)
Train | Batch (196/196) | Top-1: 24.92 (12458/50000)
Regular: 0.0
Epoche: 0; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 36.16
Epoch 1
Train | Batch (1/196) | Top-1: 40.23 (103/256)
Train | Batch (101/196) | Top-1: 43.76 (11314/25856)
Train | Batch (196/196) | Top-1: 47.68 (23840/50000)
Regular: 0.0
Epoche: 1; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 50.21
Epoch 2
Train | Batch (1/196) | Top-1: 54.69 (140/256)
Train | Batch (101/196) | Top-1: 58.50 (15126/25856)
Train | Batch (196/196) | Top-1: 60.39 (30195/50000)
Regular: 0.0
Epoche: 2; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 51.74
Epoch 3
Train | Batch (1/196) | Top-1: 58.59 (150/256)
Train | Batch (101/196) | Top-1: 66.40 (17169/25856)
Train | Batch (196/196) | Top-1: 67.83 (33914/50000)
Regular: 0.0
Epoche: 3; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 65.86
Epoch 4
Train | Batch (1/196) | Top-1: 70.31 (180/256)
Train | Batch (101/196) | Top-1: 72.20 (18667/25856)
Train | Batch (196/196) | Top-1: 73.07 (36533/50000)
Regular: 0.0
Epoche: 4; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 58.71
Epoch 5
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 75.79 (19596/25856)
Train | Batch (196/196) | Top-1: 76.35 (38173/50000)
Regular: 0.0
Epoche: 5; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 65.16
Epoch 6
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 78.18 (20215/25856)
Train | Batch (196/196) | Top-1: 78.59 (39295/50000)
Regular: 0.0
Epoche: 6; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.34
Epoch 7
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 80.07 (20703/25856)
Train | Batch (196/196) | Top-1: 80.33 (40167/50000)
Regular: 0.0
Epoche: 7; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.62
Epoch 8
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 81.68 (21120/25856)
Train | Batch (196/196) | Top-1: 81.73 (40864/50000)
Regular: 0.0
Epoche: 8; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.62
Epoch 9
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 82.50 (21332/25856)
Train | Batch (196/196) | Top-1: 82.41 (41204/50000)
Regular: 0.0
Epoche: 9; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.51
Epoch 10
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 83.44 (21574/25856)
Train | Batch (196/196) | Top-1: 83.42 (41711/50000)
Regular: 0.0
Epoche: 10; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.28
Epoch 11
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 84.24 (21780/25856)
Train | Batch (196/196) | Top-1: 84.10 (42048/50000)
Regular: 0.0
Epoche: 11; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.30
Epoch 12
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 84.65 (21886/25856)
Train | Batch (196/196) | Top-1: 84.61 (42303/50000)
Regular: 0.0
Epoche: 12; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.48
Epoch 13
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 85.57 (22126/25856)
Train | Batch (196/196) | Top-1: 85.18 (42588/50000)
Regular: 0.0
Epoche: 13; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.45
Epoch 14
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 85.52 (22111/25856)
Train | Batch (196/196) | Top-1: 85.30 (42648/50000)
Regular: 0.0
Epoche: 14; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 71.20
Epoch 15
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 86.24 (22298/25856)
Train | Batch (196/196) | Top-1: 85.95 (42973/50000)
Regular: 0.0
Epoche: 15; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.62
Epoch 16
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 85.98 (22232/25856)
Train | Batch (196/196) | Top-1: 86.03 (43013/50000)
Regular: 0.0
Epoche: 16; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.21
Epoch 17
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 86.68 (22412/25856)
Train | Batch (196/196) | Top-1: 86.56 (43279/50000)
Regular: 0.0
Epoche: 17; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.16
Epoch 18
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 86.77 (22434/25856)
Train | Batch (196/196) | Top-1: 86.74 (43372/50000)
Regular: 0.0
Epoche: 18; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.69
Epoch 19
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 86.85 (22457/25856)
Train | Batch (196/196) | Top-1: 87.00 (43502/50000)
Regular: 0.0
Epoche: 19; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.87
Epoch 20
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 87.24 (22556/25856)
Train | Batch (196/196) | Top-1: 87.24 (43622/50000)
Regular: 0.0
Epoche: 20; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.92
Epoch 21
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.40 (22598/25856)
Train | Batch (196/196) | Top-1: 87.33 (43665/50000)
Regular: 0.0
Epoche: 21; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.61
Epoch 22
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.39 (22595/25856)
Train | Batch (196/196) | Top-1: 87.41 (43704/50000)
Regular: 0.0
Epoche: 22; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.77
Epoch 23
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 87.65 (22664/25856)
Train | Batch (196/196) | Top-1: 87.52 (43759/50000)
Regular: 0.0
Epoche: 23; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.00
Epoch 24
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 88.01 (22755/25856)
Train | Batch (196/196) | Top-1: 87.78 (43891/50000)
Regular: 0.0
Epoche: 24; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.36
Epoch 25
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 88.04 (22763/25856)
Train | Batch (196/196) | Top-1: 88.05 (44024/50000)
Regular: 0.0
Epoche: 25; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.82
Epoch 26
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 88.05 (22766/25856)
Train | Batch (196/196) | Top-1: 87.97 (43984/50000)
Regular: 0.0
Epoche: 26; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.55
Epoch 27
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 88.47 (22875/25856)
Train | Batch (196/196) | Top-1: 88.24 (44119/50000)
Regular: 0.0
Epoche: 27; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.58
Epoch 28
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 88.58 (22904/25856)
Train | Batch (196/196) | Top-1: 88.53 (44266/50000)
Regular: 0.0
Epoche: 28; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.05
Epoch 29
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 88.54 (22894/25856)
Train | Batch (196/196) | Top-1: 88.52 (44258/50000)
Regular: 0.0
Epoche: 29; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.73
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 1
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 89.18 (23058/25856)
Train | Batch (196/196) | Top-1: 88.64 (44319/50000)
Regular: 0.0
Epoche: 0; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.44
Epoch 1
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 88.98 (23006/25856)
Train | Batch (196/196) | Top-1: 88.64 (44319/50000)
Regular: 0.0
Epoche: 1; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.37
Epoch 2
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 88.42 (22863/25856)
Train | Batch (196/196) | Top-1: 88.51 (44254/50000)
Regular: 0.0
Epoche: 2; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.47
Epoch 3
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 88.85 (22973/25856)
Train | Batch (196/196) | Top-1: 88.79 (44396/50000)
Regular: 0.0
Epoche: 3; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.35
Epoch 4
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 89.28 (23085/25856)
Train | Batch (196/196) | Top-1: 89.04 (44521/50000)
Regular: 0.0
Epoche: 4; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.51
Epoch 5
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 89.26 (23079/25856)
Train | Batch (196/196) | Top-1: 88.94 (44471/50000)
Regular: 0.0
Epoche: 5; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.83
Epoch 6
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 89.14 (23049/25856)
Train | Batch (196/196) | Top-1: 89.13 (44566/50000)
Regular: 0.0
Epoche: 6; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.98
Epoch 7
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 89.64 (23177/25856)
Train | Batch (196/196) | Top-1: 89.27 (44637/50000)
Regular: 0.0
Epoche: 7; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.76
Epoch 8
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 89.58 (23163/25856)
Train | Batch (196/196) | Top-1: 89.27 (44634/50000)
Regular: 0.0
Epoche: 8; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.56
Epoch 9
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 89.60 (23168/25856)
Train | Batch (196/196) | Top-1: 89.14 (44568/50000)
Regular: 0.0
Epoche: 9; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.78
Epoch 10
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 89.74 (23202/25856)
Train | Batch (196/196) | Top-1: 89.55 (44774/50000)
Regular: 0.0
Epoche: 10; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.44
Epoch 11
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 89.56 (23157/25856)
Train | Batch (196/196) | Top-1: 89.37 (44685/50000)
Regular: 0.0
Epoche: 11; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 73.62
Epoch 12
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 89.76 (23208/25856)
Train | Batch (196/196) | Top-1: 89.51 (44756/50000)
Regular: 0.0
Epoche: 12; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.23
Epoch 13
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 89.87 (23236/25856)
Train | Batch (196/196) | Top-1: 89.66 (44831/50000)
Regular: 0.0
Epoche: 13; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.05
Epoch 14
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 89.70 (23192/25856)
Train | Batch (196/196) | Top-1: 89.43 (44714/50000)
Regular: 0.0
Epoche: 14; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.72
Epoch 15
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 89.87 (23236/25856)
Train | Batch (196/196) | Top-1: 89.63 (44814/50000)
Regular: 0.0
Epoche: 15; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.74
Epoch 16
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 89.46 (23132/25856)
Train | Batch (196/196) | Top-1: 89.47 (44735/50000)
Regular: 0.0
Epoche: 16; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.71
Epoch 17
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 90.12 (23302/25856)
Train | Batch (196/196) | Top-1: 89.66 (44828/50000)
Regular: 0.0
Epoche: 17; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.96
Epoch 18
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 89.96 (23261/25856)
Train | Batch (196/196) | Top-1: 89.76 (44882/50000)
Regular: 0.0
Epoche: 18; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.29
Epoch 19
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 90.16 (23312/25856)
Train | Batch (196/196) | Top-1: 89.75 (44874/50000)
Regular: 0.0
Epoche: 19; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.10
Epoch 20
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 89.57 (23159/25856)
Train | Batch (196/196) | Top-1: 89.77 (44885/50000)
Regular: 0.0
Epoche: 20; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.49
Epoch 21
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 90.13 (23304/25856)
Train | Batch (196/196) | Top-1: 89.98 (44992/50000)
Regular: 0.0
Epoche: 21; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.70
Epoch 22
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 90.66 (23441/25856)
Train | Batch (196/196) | Top-1: 90.01 (45005/50000)
Regular: 0.0
Epoche: 22; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.58
Epoch 23
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 90.04 (23281/25856)
Train | Batch (196/196) | Top-1: 89.72 (44862/50000)
Regular: 0.0
Epoche: 23; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.44
Epoch 24
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.44 (23383/25856)
Train | Batch (196/196) | Top-1: 90.08 (45038/50000)
Regular: 0.0
Epoche: 24; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.41
Epoch 25
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 90.57 (23419/25856)
Train | Batch (196/196) | Top-1: 90.14 (45069/50000)
Regular: 0.0
Epoche: 25; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.33
Epoch 26
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 90.27 (23339/25856)
Train | Batch (196/196) | Top-1: 90.15 (45076/50000)
Regular: 0.0
Epoche: 26; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.29
Epoch 27
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 90.31 (23351/25856)
Train | Batch (196/196) | Top-1: 90.11 (45054/50000)
Regular: 0.0
Epoche: 27; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.70
Epoch 28
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 90.33 (23355/25856)
Train | Batch (196/196) | Top-1: 90.14 (45071/50000)
Regular: 0.0
Epoche: 28; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.11
Epoch 29
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.33 (23355/25856)
Train | Batch (196/196) | Top-1: 89.94 (44972/50000)
Regular: 0.0
Epoche: 29; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.51
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 2
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 90.33 (23357/25856)
Train | Batch (196/196) | Top-1: 90.22 (45109/50000)
Regular: 0.0
Epoche: 0; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.20
Epoch 1
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 90.11 (23300/25856)
Train | Batch (196/196) | Top-1: 90.03 (45014/50000)
Regular: 0.0
Epoche: 1; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.30
Epoch 2
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 90.69 (23448/25856)
Train | Batch (196/196) | Top-1: 90.12 (45059/50000)
Regular: 0.0
Epoche: 2; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.14
Epoch 3
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 90.35 (23361/25856)
Train | Batch (196/196) | Top-1: 90.19 (45095/50000)
Regular: 0.0
Epoche: 3; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.28
Epoch 4
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.32 (23353/25856)
Train | Batch (196/196) | Top-1: 90.14 (45069/50000)
Regular: 0.0
Epoche: 4; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.76
Epoch 5
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 90.56 (23415/25856)
Train | Batch (196/196) | Top-1: 90.12 (45062/50000)
Regular: 0.0
Epoche: 5; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.08
Epoch 6
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 90.61 (23427/25856)
Train | Batch (196/196) | Top-1: 90.35 (45177/50000)
Regular: 0.0
Epoche: 6; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.72
Epoch 7
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 90.36 (23363/25856)
Train | Batch (196/196) | Top-1: 90.24 (45118/50000)
Regular: 0.0
Epoche: 7; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.42
Epoch 8
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 90.69 (23449/25856)
Train | Batch (196/196) | Top-1: 90.30 (45149/50000)
Regular: 0.0
Epoche: 8; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.55
Epoch 9
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 90.55 (23412/25856)
Train | Batch (196/196) | Top-1: 90.46 (45229/50000)
Regular: 0.0
Epoche: 9; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.88
Epoch 10
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 90.25 (23334/25856)
Train | Batch (196/196) | Top-1: 90.37 (45187/50000)
Regular: 0.0
Epoche: 10; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.71
Epoch 11
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 90.68 (23446/25856)
Train | Batch (196/196) | Top-1: 90.31 (45156/50000)
Regular: 0.0
Epoche: 11; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.23
Epoch 12
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 91.11 (23557/25856)
Train | Batch (196/196) | Top-1: 90.76 (45381/50000)
Regular: 0.0
Epoche: 12; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 86.34
Epoch 13
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 90.67 (23444/25856)
Train | Batch (196/196) | Top-1: 90.23 (45115/50000)
Regular: 0.0
Epoche: 13; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.20
Epoch 14
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.90 (23504/25856)
Train | Batch (196/196) | Top-1: 90.54 (45269/50000)
Regular: 0.0
Epoche: 14; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.99
Epoch 15
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.99 (23527/25856)
Train | Batch (196/196) | Top-1: 90.65 (45323/50000)
Regular: 0.0
Epoche: 15; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.46
Epoch 16
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 90.68 (23445/25856)
Train | Batch (196/196) | Top-1: 90.37 (45184/50000)
Regular: 0.0
Epoche: 16; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.36
Epoch 17
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 90.58 (23421/25856)
Train | Batch (196/196) | Top-1: 90.28 (45142/50000)
Regular: 0.0
Epoche: 17; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.08
Epoch 18
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 90.93 (23512/25856)
Train | Batch (196/196) | Top-1: 90.55 (45274/50000)
Regular: 0.0
Epoche: 18; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.87
Epoch 19
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 90.69 (23450/25856)
Train | Batch (196/196) | Top-1: 90.28 (45142/50000)
Regular: 0.0
Epoche: 19; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.95
Epoch 20
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 90.89 (23501/25856)
Train | Batch (196/196) | Top-1: 90.66 (45328/50000)
Regular: 0.0
Epoche: 20; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.58
Epoch 21
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 90.59 (23424/25856)
Train | Batch (196/196) | Top-1: 90.51 (45253/50000)
Regular: 0.0
Epoche: 21; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.15
Epoch 22
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 90.64 (23435/25856)
Train | Batch (196/196) | Top-1: 90.53 (45264/50000)
Regular: 0.0
Epoche: 22; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.03
Epoch 23
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 90.82 (23483/25856)
Train | Batch (196/196) | Top-1: 90.37 (45185/50000)
Regular: 0.0
Epoche: 23; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.47
Epoch 24
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 90.96 (23519/25856)
Train | Batch (196/196) | Top-1: 90.62 (45310/50000)
Regular: 0.0
Epoche: 24; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.48
Epoch 25
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.75 (23465/25856)
Train | Batch (196/196) | Top-1: 90.57 (45284/50000)
Regular: 0.0
Epoche: 25; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.75
Epoch 26
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 90.88 (23499/25856)
Train | Batch (196/196) | Top-1: 90.70 (45351/50000)
Regular: 0.0
Epoche: 26; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.93
Epoch 27
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 91.07 (23547/25856)
Train | Batch (196/196) | Top-1: 90.76 (45380/50000)
Regular: 0.0
Epoche: 27; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.02
Epoch 28
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 91.06 (23545/25856)
Train | Batch (196/196) | Top-1: 90.81 (45404/50000)
Regular: 0.0
Epoche: 28; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.63
Epoch 29
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 90.60 (23426/25856)
Train | Batch (196/196) | Top-1: 90.44 (45222/50000)
Regular: 0.0
Epoche: 29; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.37
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 3
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 91.53 (23666/25856)
Train | Batch (196/196) | Top-1: 90.88 (45440/50000)
Regular: 0.0
Epoche: 0; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.82
Epoch 1
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 91.21 (23582/25856)
Train | Batch (196/196) | Top-1: 90.80 (45400/50000)
Regular: 0.0
Epoche: 1; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.88
Epoch 2
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 90.87 (23496/25856)
Train | Batch (196/196) | Top-1: 90.63 (45314/50000)
Regular: 0.0
Epoche: 2; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.91
Epoch 3
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 90.69 (23449/25856)
Train | Batch (196/196) | Top-1: 90.49 (45243/50000)
Regular: 0.0
Epoche: 3; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.79
Epoch 4
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 90.96 (23518/25856)
Train | Batch (196/196) | Top-1: 90.73 (45365/50000)
Regular: 0.0
Epoche: 4; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.52
Epoch 5
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 90.89 (23500/25856)
Train | Batch (196/196) | Top-1: 90.55 (45274/50000)
Regular: 0.0
Epoche: 5; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.65
Epoch 6
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 91.26 (23595/25856)
Train | Batch (196/196) | Top-1: 90.94 (45470/50000)
Regular: 0.0
Epoche: 6; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.17
Epoch 7
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 91.36 (23621/25856)
Train | Batch (196/196) | Top-1: 90.76 (45378/50000)
Regular: 0.0
Epoche: 7; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.11
Epoch 8
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.95 (23516/25856)
Train | Batch (196/196) | Top-1: 90.67 (45336/50000)
Regular: 0.0
Epoche: 8; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.15
Epoch 9
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 90.86 (23492/25856)
Train | Batch (196/196) | Top-1: 90.60 (45299/50000)
Regular: 0.0
Epoche: 9; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.18
Epoch 10
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 91.21 (23583/25856)
Train | Batch (196/196) | Top-1: 90.86 (45429/50000)
Regular: 0.0
Epoche: 10; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.29
Epoch 11
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 90.94 (23514/25856)
Train | Batch (196/196) | Top-1: 90.70 (45352/50000)
Regular: 0.0
Epoche: 11; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.33
Epoch 12
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 91.26 (23596/25856)
Train | Batch (196/196) | Top-1: 90.87 (45435/50000)
Regular: 0.0
Epoche: 12; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 71.04
Epoch 13
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 91.11 (23558/25856)
Train | Batch (196/196) | Top-1: 90.80 (45398/50000)
Regular: 0.0
Epoche: 13; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.90
Epoch 14
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 91.00 (23530/25856)
Train | Batch (196/196) | Top-1: 90.82 (45408/50000)
Regular: 0.0
Epoche: 14; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.86
Epoch 15
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 91.04 (23540/25856)
Train | Batch (196/196) | Top-1: 90.82 (45409/50000)
Regular: 0.0
Epoche: 15; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 86.33
Epoch 16
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 90.94 (23514/25856)
Train | Batch (196/196) | Top-1: 90.90 (45448/50000)
Regular: 0.0
Epoche: 16; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.50
Epoch 17
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.92 (23509/25856)
Train | Batch (196/196) | Top-1: 90.89 (45443/50000)
Regular: 0.0
Epoche: 17; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.97
Epoch 18
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 91.11 (23557/25856)
Train | Batch (196/196) | Top-1: 90.98 (45489/50000)
Regular: 0.0
Epoche: 18; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.29
Epoch 19
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.95 (23515/25856)
Train | Batch (196/196) | Top-1: 90.78 (45388/50000)
Regular: 0.0
Epoche: 19; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.87
Epoch 20
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 91.36 (23622/25856)
Train | Batch (196/196) | Top-1: 91.25 (45627/50000)
Regular: 0.0
Epoche: 20; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.04
Epoch 21
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 91.01 (23532/25856)
Train | Batch (196/196) | Top-1: 90.89 (45443/50000)
Regular: 0.0
Epoche: 21; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.77
Epoch 22
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 91.15 (23567/25856)
Train | Batch (196/196) | Top-1: 90.96 (45479/50000)
Regular: 0.0
Epoche: 22; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.42
Epoch 23
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 91.03 (23538/25856)
Train | Batch (196/196) | Top-1: 90.78 (45392/50000)
Regular: 0.0
Epoche: 23; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.43
Epoch 24
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 91.13 (23563/25856)
Train | Batch (196/196) | Top-1: 91.04 (45518/50000)
Regular: 0.0
Epoche: 24; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.15
Epoch 25
Train | Batch (1/196) | Top-1: 95.70 (245/256)
Train | Batch (101/196) | Top-1: 90.81 (23480/25856)
Train | Batch (196/196) | Top-1: 90.90 (45449/50000)
Regular: 0.0
Epoche: 25; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.87
Epoch 26
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 91.21 (23582/25856)
Train | Batch (196/196) | Top-1: 91.12 (45560/50000)
Regular: 0.0
Epoche: 26; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.72
Epoch 27
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 91.15 (23567/25856)
Train | Batch (196/196) | Top-1: 90.98 (45492/50000)
Regular: 0.0
Epoche: 27; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.66
Epoch 28
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 90.95 (23517/25856)
Train | Batch (196/196) | Top-1: 90.87 (45437/50000)
Regular: 0.0
Epoche: 28; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.18
Epoch 29
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 91.60 (23684/25856)
Train | Batch (196/196) | Top-1: 91.17 (45586/50000)
Regular: 0.0
Epoche: 29; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.12
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 4
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 91.96 (23777/25856)
Train | Batch (196/196) | Top-1: 91.37 (45684/50000)
Regular: 0.0
Epoche: 0; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.34
Epoch 1
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 90.87 (23496/25856)
Train | Batch (196/196) | Top-1: 90.78 (45389/50000)
Regular: 0.0
Epoche: 1; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.67
Epoch 2
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 91.10 (23555/25856)
Train | Batch (196/196) | Top-1: 90.99 (45497/50000)
Regular: 0.0
Epoche: 2; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.11
Epoch 3
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.34 (23618/25856)
Train | Batch (196/196) | Top-1: 90.96 (45482/50000)
Regular: 0.0
Epoche: 3; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.29
Epoch 4
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 91.26 (23595/25856)
Train | Batch (196/196) | Top-1: 90.83 (45415/50000)
Regular: 0.0
Epoche: 4; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.07
Epoch 5
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.28 (23601/25856)
Train | Batch (196/196) | Top-1: 91.10 (45551/50000)
Regular: 0.0
Epoche: 5; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.76
Epoch 6
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.48 (23652/25856)
Train | Batch (196/196) | Top-1: 90.93 (45466/50000)
Regular: 0.0
Epoche: 6; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.52
Epoch 7
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 91.09 (23551/25856)
Train | Batch (196/196) | Top-1: 90.95 (45477/50000)
Regular: 0.0
Epoche: 7; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.51
Epoch 8
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 91.18 (23575/25856)
Train | Batch (196/196) | Top-1: 90.95 (45474/50000)
Regular: 0.0
Epoche: 8; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.26
Epoch 9
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.76 (23726/25856)
Train | Batch (196/196) | Top-1: 91.39 (45694/50000)
Regular: 0.0
Epoche: 9; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.72
Epoch 10
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 91.11 (23557/25856)
Train | Batch (196/196) | Top-1: 91.16 (45582/50000)
Regular: 0.0
Epoche: 10; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.32
Epoch 11
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 91.33 (23614/25856)
Train | Batch (196/196) | Top-1: 91.07 (45536/50000)
Regular: 0.0
Epoche: 11; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.82
Epoch 12
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 91.13 (23563/25856)
Train | Batch (196/196) | Top-1: 91.07 (45537/50000)
Regular: 0.0
Epoche: 12; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.58
Epoch 13
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 91.72 (23716/25856)
Train | Batch (196/196) | Top-1: 91.26 (45631/50000)
Regular: 0.0
Epoche: 13; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.47
Epoch 14
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 91.05 (23543/25856)
Train | Batch (196/196) | Top-1: 91.09 (45543/50000)
Regular: 0.0
Epoche: 14; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 77.46
Epoch 15
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.35 (23619/25856)
Train | Batch (196/196) | Top-1: 91.12 (45558/50000)
Regular: 0.0
Epoche: 15; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.50
Epoch 16
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 91.50 (23659/25856)
Train | Batch (196/196) | Top-1: 91.18 (45591/50000)
Regular: 0.0
Epoche: 16; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 75.72
Epoch 17
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 91.15 (23567/25856)
Train | Batch (196/196) | Top-1: 91.09 (45543/50000)
Regular: 0.0
Epoche: 17; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.93
Epoch 18
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 91.49 (23656/25856)
Train | Batch (196/196) | Top-1: 91.13 (45567/50000)
Regular: 0.0
Epoche: 18; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.81
Epoch 19
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 91.56 (23675/25856)
Train | Batch (196/196) | Top-1: 91.22 (45608/50000)
Regular: 0.0
Epoche: 19; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 72.27
Epoch 20
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 91.54 (23668/25856)
Train | Batch (196/196) | Top-1: 91.21 (45605/50000)
Regular: 0.0
Epoche: 20; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.36
Epoch 21
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 91.36 (23622/25856)
Train | Batch (196/196) | Top-1: 91.14 (45571/50000)
Regular: 0.0
Epoche: 21; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.48
Epoch 22
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 91.51 (23662/25856)
Train | Batch (196/196) | Top-1: 91.30 (45648/50000)
Regular: 0.0
Epoche: 22; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 78.69
Epoch 23
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 91.32 (23611/25856)
Train | Batch (196/196) | Top-1: 91.09 (45543/50000)
Regular: 0.0
Epoche: 23; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.46
Epoch 24
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 91.07 (23546/25856)
Train | Batch (196/196) | Top-1: 91.08 (45540/50000)
Regular: 0.0
Epoche: 24; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.94
Epoch 25
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 90.84 (23487/25856)
Train | Batch (196/196) | Top-1: 90.87 (45435/50000)
Regular: 0.0
Epoche: 25; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.77
Epoch 26
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 91.36 (23623/25856)
Train | Batch (196/196) | Top-1: 91.18 (45592/50000)
Regular: 0.0
Epoche: 26; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.71
Epoch 27
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 91.35 (23619/25856)
Train | Batch (196/196) | Top-1: 90.99 (45495/50000)
Regular: 0.0
Epoche: 27; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.62
Epoch 28
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 91.38 (23626/25856)
Train | Batch (196/196) | Top-1: 91.06 (45532/50000)
Regular: 0.0
Epoche: 28; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.61
Epoch 29
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 90.86 (23494/25856)
Train | Batch (196/196) | Top-1: 91.07 (45535/50000)
Regular: 0.0
Epoche: 29; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.38
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
I: 5
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.91 (23763/25856)
Train | Batch (196/196) | Top-1: 91.50 (45748/50000)
Regular: 0.0
Epoche: 0; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.09
Epoch 1
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 90.93 (23512/25856)
Train | Batch (196/196) | Top-1: 90.89 (45447/50000)
Regular: 0.0
Epoche: 1; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 86.19
Epoch 2
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 91.40 (23632/25856)
Train | Batch (196/196) | Top-1: 91.23 (45615/50000)
Regular: 0.0
Epoche: 2; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.40
Epoch 3
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 91.29 (23605/25856)
Train | Batch (196/196) | Top-1: 91.29 (45644/50000)
Regular: 0.0
Epoche: 3; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.10
Epoch 4
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 91.96 (23776/25856)
Train | Batch (196/196) | Top-1: 91.25 (45626/50000)
Regular: 0.0
Epoche: 4; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.98
Epoch 5
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.46 (23649/25856)
Train | Batch (196/196) | Top-1: 91.33 (45667/50000)
Regular: 0.0
Epoche: 5; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.66
Epoch 6
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 91.57 (23676/25856)
Train | Batch (196/196) | Top-1: 91.19 (45594/50000)
Regular: 0.0
Epoche: 6; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.13
Epoch 7
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 91.19 (23577/25856)
Train | Batch (196/196) | Top-1: 91.18 (45590/50000)
Regular: 0.0
Epoche: 7; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.38
Epoch 8
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 91.39 (23629/25856)
Train | Batch (196/196) | Top-1: 91.23 (45614/50000)
Regular: 0.0
Epoche: 8; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.59
Epoch 9
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 91.46 (23649/25856)
Train | Batch (196/196) | Top-1: 91.22 (45612/50000)
Regular: 0.0
Epoche: 9; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.51
Epoch 10
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 91.24 (23591/25856)
Train | Batch (196/196) | Top-1: 91.14 (45570/50000)
Regular: 0.0
Epoche: 10; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.25
Epoch 11
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 91.27 (23600/25856)
Train | Batch (196/196) | Top-1: 91.13 (45567/50000)
Regular: 0.0
Epoche: 11; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.35
Epoch 12
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 91.76 (23725/25856)
Train | Batch (196/196) | Top-1: 91.47 (45734/50000)
Regular: 0.0
Epoche: 12; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.78
Epoch 13
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 91.51 (23662/25856)
Train | Batch (196/196) | Top-1: 91.44 (45721/50000)
Regular: 0.0
Epoche: 13; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.59
Epoch 14
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.30 (23607/25856)
Train | Batch (196/196) | Top-1: 91.24 (45619/50000)
Regular: 0.0
Epoche: 14; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.74
Epoch 15
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 91.48 (23652/25856)
Train | Batch (196/196) | Top-1: 91.35 (45675/50000)
Regular: 0.0
Epoche: 15; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.97
Epoch 16
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 91.52 (23664/25856)
Train | Batch (196/196) | Top-1: 91.25 (45623/50000)
Regular: 0.0
Epoche: 16; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.44
Epoch 17
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 91.47 (23650/25856)
Train | Batch (196/196) | Top-1: 91.20 (45602/50000)
Regular: 0.0
Epoche: 17; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.31
Epoch 18
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 91.72 (23716/25856)
Train | Batch (196/196) | Top-1: 91.45 (45727/50000)
Regular: 0.0
Epoche: 18; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.64
Epoch 19
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 91.97 (23780/25856)
Train | Batch (196/196) | Top-1: 91.43 (45715/50000)
Regular: 0.0
Epoche: 19; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.79
Epoch 20
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 91.08 (23549/25856)
Train | Batch (196/196) | Top-1: 91.22 (45609/50000)
Regular: 0.0
Epoche: 20; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.26
Epoch 21
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 91.70 (23711/25856)
Train | Batch (196/196) | Top-1: 91.48 (45741/50000)
Regular: 0.0
Epoche: 21; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.40
Epoch 22
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 91.38 (23627/25856)
Train | Batch (196/196) | Top-1: 91.24 (45618/50000)
Regular: 0.0
Epoche: 22; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.67
Epoch 23
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.31 (23609/25856)
Train | Batch (196/196) | Top-1: 91.18 (45592/50000)
Regular: 0.0
Epoche: 23; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 86.61
Epoch 24
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 91.36 (23621/25856)
Train | Batch (196/196) | Top-1: 91.19 (45595/50000)
Regular: 0.0
Epoche: 24; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.99
Epoch 25
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 91.46 (23648/25856)
Train | Batch (196/196) | Top-1: 91.35 (45675/50000)
Regular: 0.0
Epoche: 25; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.73
Epoch 26
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.62 (23689/25856)
Train | Batch (196/196) | Top-1: 91.34 (45672/50000)
Regular: 0.0
Epoche: 26; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.28
Epoch 27
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 91.32 (23611/25856)
Train | Batch (196/196) | Top-1: 91.11 (45557/50000)
Regular: 0.0
Epoche: 27; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 86.04
Epoch 28
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 91.26 (23596/25856)
Train | Batch (196/196) | Top-1: 91.22 (45608/50000)
Regular: 0.0
Epoche: 28; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.21
Epoch 29
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 91.56 (23675/25856)
Train | Batch (196/196) | Top-1: 91.36 (45682/50000)
Regular: 0.0
Epoche: 29; regular: 0.0: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.36
Drin!!
Layers that will be prunned: []
Prunning filters..
Target (flops): 68.863M
After Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 94.92 (243/256)
Train | Batch (101/196) | Top-1: 92.02 (23793/25856)
Train | Batch (196/196) | Top-1: 91.27 (45635/50000)
Regular: nan
Epoche: 0; regular: nan: flops 68862592
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.17
Epoch 1
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 91.85 (23749/25856)
Train | Batch (196/196) | Top-1: 91.41 (45703/50000)
Regular: nan
Epoche: 1; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.33
Epoch 2
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 91.44 (23644/25856)
Train | Batch (196/196) | Top-1: 91.32 (45661/50000)
Regular: nan
Epoche: 2; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.18
Epoch 3
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 91.26 (23595/25856)
Train | Batch (196/196) | Top-1: 91.15 (45576/50000)
Regular: nan
Epoche: 3; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.45
Epoch 4
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 91.41 (23636/25856)
Train | Batch (196/196) | Top-1: 91.31 (45656/50000)
Regular: nan
Epoche: 4; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.96
Epoch 5
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 91.62 (23690/25856)
Train | Batch (196/196) | Top-1: 91.38 (45690/50000)
Regular: nan
Epoche: 5; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.44
Epoch 6
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 91.38 (23627/25856)
Train | Batch (196/196) | Top-1: 91.22 (45611/50000)
Regular: nan
Epoche: 6; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.46
Epoch 7
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 91.64 (23695/25856)
Train | Batch (196/196) | Top-1: 91.46 (45731/50000)
Regular: nan
Epoche: 7; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.54
Epoch 8
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 91.46 (23648/25856)
Train | Batch (196/196) | Top-1: 91.47 (45736/50000)
Regular: nan
Epoche: 8; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.03
Epoch 9
Train | Batch (1/196) | Top-1: 91.80 (235/256)
Train | Batch (101/196) | Top-1: 91.53 (23667/25856)
Train | Batch (196/196) | Top-1: 91.30 (45652/50000)
Regular: nan
Epoche: 9; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.29
Epoch 10
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 91.97 (23780/25856)
Train | Batch (196/196) | Top-1: 91.50 (45751/50000)
Regular: nan
Epoche: 10; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.34
Epoch 11
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 91.72 (23716/25856)
Train | Batch (196/196) | Top-1: 91.45 (45727/50000)
Regular: nan
Epoche: 11; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.01
Epoch 12
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 91.44 (23642/25856)
Train | Batch (196/196) | Top-1: 91.22 (45611/50000)
Regular: nan
Epoche: 12; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.11
Epoch 13
Train | Batch (1/196) | Top-1: 92.19 (236/256)
Train | Batch (101/196) | Top-1: 91.28 (23602/25856)
Train | Batch (196/196) | Top-1: 91.19 (45594/50000)
Regular: nan
Epoche: 13; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.11
Epoch 14
Train | Batch (1/196) | Top-1: 91.02 (233/256)
Train | Batch (101/196) | Top-1: 91.58 (23680/25856)
Train | Batch (196/196) | Top-1: 91.44 (45720/50000)
Regular: nan
Epoche: 14; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 76.61
Epoch 15
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 91.50 (23657/25856)
Train | Batch (196/196) | Top-1: 91.38 (45690/50000)
Regular: nan
Epoche: 15; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.04
Epoch 16
Train | Batch (1/196) | Top-1: 92.97 (238/256)
Train | Batch (101/196) | Top-1: 91.60 (23683/25856)
Train | Batch (196/196) | Top-1: 91.39 (45694/50000)
Regular: nan
Epoche: 16; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.91
Epoch 17
Train | Batch (1/196) | Top-1: 92.58 (237/256)
Train | Batch (101/196) | Top-1: 91.60 (23684/25856)
Train | Batch (196/196) | Top-1: 91.36 (45681/50000)
Regular: nan
Epoche: 17; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.68
Epoch 18
Train | Batch (1/196) | Top-1: 93.36 (239/256)
Train | Batch (101/196) | Top-1: 91.14 (23565/25856)
Train | Batch (196/196) | Top-1: 91.19 (45596/50000)
Regular: nan
Epoche: 18; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 74.68
Epoch 19
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 91.49 (23656/25856)
Train | Batch (196/196) | Top-1: 91.18 (45590/50000)
Regular: nan
Epoche: 19; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.43
Epoch 20
Train | Batch (1/196) | Top-1: 94.53 (242/256)
Train | Batch (101/196) | Top-1: 91.63 (23692/25856)
Train | Batch (196/196) | Top-1: 91.44 (45721/50000)
Regular: nan
Epoche: 20; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.72
Epoch 21
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 91.54 (23669/25856)
Train | Batch (196/196) | Top-1: 91.36 (45679/50000)
Regular: nan
Epoche: 21; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 86.24
Epoch 22
Train | Batch (1/196) | Top-1: 95.31 (244/256)
Train | Batch (101/196) | Top-1: 91.66 (23699/25856)
Train | Batch (196/196) | Top-1: 91.44 (45722/50000)
Regular: nan
Epoche: 22; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 85.59
Epoch 23
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 91.88 (23756/25856)
Train | Batch (196/196) | Top-1: 91.38 (45689/50000)
Regular: nan
Epoche: 23; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.30
Epoch 24
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 91.48 (23653/25856)
Train | Batch (196/196) | Top-1: 91.40 (45701/50000)
Regular: nan
Epoche: 24; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 79.71
Epoch 25
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 91.60 (23683/25856)
Train | Batch (196/196) | Top-1: 91.27 (45637/50000)
Regular: nan
Epoche: 25; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 84.68
Epoch 26
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 91.30 (23606/25856)
Train | Batch (196/196) | Top-1: 91.28 (45640/50000)
Regular: nan
Epoche: 26; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 83.82
Epoch 27
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 91.73 (23717/25856)
Train | Batch (196/196) | Top-1: 91.44 (45718/50000)
Regular: nan
Epoche: 27; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 82.43
Epoch 28
Train | Batch (1/196) | Top-1: 93.75 (240/256)
Train | Batch (101/196) | Top-1: 91.70 (23711/25856)
Train | Batch (196/196) | Top-1: 91.47 (45737/50000)
Regular: nan
Epoche: 28; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 81.84
Epoch 29
Train | Batch (1/196) | Top-1: 94.14 (241/256)
Train | Batch (101/196) | Top-1: 91.51 (23660/25856)
Train | Batch (196/196) | Top-1: 91.39 (45694/50000)
Regular: nan
Epoche: 29; regular: nan: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 80.26
model: ResNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (4): AvgPool2d(kernel_size=8, stride=8, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=10, bias=True)
  )
)
Test acc: 80.25999999999999
