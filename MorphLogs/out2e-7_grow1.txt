no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=2e-07, logger='MorphLogs/logMorphNetFlops2e-7_grow2.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 17.57 (4544/25856)
Train | Batch (196/196) | Top-1: 18.26 (9132/50000)
Regular: 5.715904235839844
Epoche: 0; regular: 5.715904235839844: flops 68862592
#Filters: 394, #FLOPs: 33.90M | Top-1: 13.33
Epoch 1
Train | Batch (1/196) | Top-1: 14.84 (38/256)
Train | Batch (101/196) | Top-1: 19.84 (5129/25856)
Train | Batch (196/196) | Top-1: 19.97 (9985/50000)
Regular: 0.6809785962104797
Epoche: 1; regular: 0.6809785962104797: flops 68862592
#Filters: 373, #FLOPs: 33.55M | Top-1: 10.05
Epoch 2
Train | Batch (1/196) | Top-1: 17.97 (46/256)
Train | Batch (101/196) | Top-1: 19.68 (5088/25856)
Train | Batch (196/196) | Top-1: 19.62 (9810/50000)
Regular: 0.4971235692501068
Epoche: 2; regular: 0.4971235692501068: flops 68862592
#Filters: 408, #FLOPs: 36.59M | Top-1: 10.00
Epoch 3
Train | Batch (1/196) | Top-1: 23.83 (61/256)
Train | Batch (101/196) | Top-1: 20.13 (5204/25856)
Train | Batch (196/196) | Top-1: 20.06 (10029/50000)
Regular: 0.5530796647071838
Epoche: 3; regular: 0.5530796647071838: flops 68862592
#Filters: 356, #FLOPs: 32.92M | Top-1: 12.08
Epoch 4
Train | Batch (1/196) | Top-1: 23.44 (60/256)
Train | Batch (101/196) | Top-1: 20.13 (5205/25856)
Train | Batch (196/196) | Top-1: 20.28 (10140/50000)
Regular: 0.5015735030174255
Epoche: 4; regular: 0.5015735030174255: flops 68862592
#Filters: 369, #FLOPs: 33.31M | Top-1: 9.90
Epoch 5
Train | Batch (1/196) | Top-1: 16.02 (41/256)
Train | Batch (101/196) | Top-1: 19.63 (5076/25856)
Train | Batch (196/196) | Top-1: 19.98 (9988/50000)
Regular: 0.6042009592056274
Epoche: 5; regular: 0.6042009592056274: flops 68862592
#Filters: 393, #FLOPs: 34.74M | Top-1: 10.00
Epoch 6
Train | Batch (1/196) | Top-1: 14.06 (36/256)
Train | Batch (101/196) | Top-1: 19.57 (5059/25856)
Train | Batch (196/196) | Top-1: 20.25 (10124/50000)
Regular: 1.5685698986053467
Epoche: 6; regular: 1.5685698986053467: flops 68862592
#Filters: 354, #FLOPs: 32.39M | Top-1: 9.75
Epoch 7
Train | Batch (1/196) | Top-1: 16.02 (41/256)
Train | Batch (101/196) | Top-1: 19.72 (5099/25856)
Train | Batch (196/196) | Top-1: 19.98 (9991/50000)
Regular: 0.45114684104919434
Epoche: 7; regular: 0.45114684104919434: flops 68862592
#Filters: 351, #FLOPs: 32.55M | Top-1: 15.08
Epoch 8
Train | Batch (1/196) | Top-1: 14.84 (38/256)
Train | Batch (101/196) | Top-1: 20.91 (5407/25856)
Train | Batch (196/196) | Top-1: 20.17 (10085/50000)
Regular: 1.0289350748062134
Epoche: 8; regular: 1.0289350748062134: flops 68862592
#Filters: 390, #FLOPs: 34.98M | Top-1: 10.21
Epoch 9
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 20.07 (5190/25856)
Train | Batch (196/196) | Top-1: 20.00 (10001/50000)
Regular: 1.050163745880127
Epoche: 9; regular: 1.050163745880127: flops 68862592
#Filters: 397, #FLOPs: 35.63M | Top-1: 10.00
Epoch 10
Train | Batch (1/196) | Top-1: 19.53 (50/256)
Train | Batch (101/196) | Top-1: 19.96 (5162/25856)
Train | Batch (196/196) | Top-1: 19.75 (9875/50000)
Regular: 0.7383372783660889
Epoche: 10; regular: 0.7383372783660889: flops 68862592
#Filters: 397, #FLOPs: 35.43M | Top-1: 10.00
Epoch 11
Train | Batch (1/196) | Top-1: 15.62 (40/256)
Train | Batch (101/196) | Top-1: 18.62 (4814/25856)
Train | Batch (196/196) | Top-1: 19.03 (9514/50000)
Regular: 2.800755500793457
Epoche: 11; regular: 2.800755500793457: flops 68862592
#Filters: 369, #FLOPs: 33.71M | Top-1: 10.01
Epoch 12
Train | Batch (1/196) | Top-1: 21.09 (54/256)
Train | Batch (101/196) | Top-1: 18.63 (4818/25856)
Train | Batch (196/196) | Top-1: 19.03 (9515/50000)
Regular: 0.6342440247535706
Epoche: 12; regular: 0.6342440247535706: flops 68862592
#Filters: 400, #FLOPs: 35.54M | Top-1: 10.43
Epoch 13
Train | Batch (1/196) | Top-1: 18.36 (47/256)
Train | Batch (101/196) | Top-1: 19.98 (5166/25856)
Train | Batch (196/196) | Top-1: 19.79 (9895/50000)
Regular: 0.6698533892631531
Epoche: 13; regular: 0.6698533892631531: flops 68862592
#Filters: 392, #FLOPs: 35.17M | Top-1: 10.50
Epoch 14
Train | Batch (1/196) | Top-1: 19.92 (51/256)
Train | Batch (101/196) | Top-1: 20.29 (5246/25856)
Train | Batch (196/196) | Top-1: 19.85 (9925/50000)
Regular: 0.6805883646011353
Epoche: 14; regular: 0.6805883646011353: flops 68862592
#Filters: 343, #FLOPs: 31.35M | Top-1: 10.01
Epoch 15
Train | Batch (1/196) | Top-1: 23.83 (61/256)
Train | Batch (101/196) | Top-1: 19.26 (4981/25856)
Train | Batch (196/196) | Top-1: 19.37 (9685/50000)
Regular: 0.6242219805717468
Epoche: 15; regular: 0.6242219805717468: flops 68862592
#Filters: 387, #FLOPs: 34.06M | Top-1: 10.01
Epoch 16
Train | Batch (1/196) | Top-1: 16.41 (42/256)
Train | Batch (101/196) | Top-1: 19.72 (5100/25856)
Train | Batch (196/196) | Top-1: 20.11 (10054/50000)
Regular: 0.5382633805274963
Epoche: 16; regular: 0.5382633805274963: flops 68862592
#Filters: 354, #FLOPs: 32.50M | Top-1: 22.71
Epoch 17
Train | Batch (1/196) | Top-1: 18.75 (48/256)
Train | Batch (101/196) | Top-1: 22.07 (5706/25856)
Train | Batch (196/196) | Top-1: 22.04 (11021/50000)
Regular: 0.42483454942703247
Epoche: 17; regular: 0.42483454942703247: flops 68862592
#Filters: 356, #FLOPs: 32.68M | Top-1: 13.28
Epoch 18
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 22.53 (5825/25856)
Train | Batch (196/196) | Top-1: 22.34 (11170/50000)
Regular: 0.4163147211074829
Epoche: 18; regular: 0.4163147211074829: flops 68862592
#Filters: 419, #FLOPs: 36.70M | Top-1: 10.00
Epoch 19
Train | Batch (1/196) | Top-1: 21.09 (54/256)
Train | Batch (101/196) | Top-1: 22.61 (5847/25856)
Train | Batch (196/196) | Top-1: 22.28 (11141/50000)
Regular: 0.46537452936172485
Epoche: 19; regular: 0.46537452936172485: flops 68862592
#Filters: 386, #FLOPs: 34.65M | Top-1: 10.80
Epoch 20
Train | Batch (1/196) | Top-1: 22.66 (58/256)
Train | Batch (101/196) | Top-1: 22.26 (5755/25856)
Train | Batch (196/196) | Top-1: 22.72 (11360/50000)
Regular: 0.43377211689949036
Epoche: 20; regular: 0.43377211689949036: flops 68862592
#Filters: 352, #FLOPs: 33.23M | Top-1: 9.88
Epoch 21
Train | Batch (1/196) | Top-1: 25.78 (66/256)
Train | Batch (101/196) | Top-1: 23.26 (6015/25856)
Train | Batch (196/196) | Top-1: 23.18 (11590/50000)
Regular: 0.4442186951637268
Epoche: 21; regular: 0.4442186951637268: flops 68862592
#Filters: 371, #FLOPs: 34.38M | Top-1: 11.87
Epoch 22
Train | Batch (1/196) | Top-1: 19.92 (51/256)
Train | Batch (101/196) | Top-1: 22.51 (5819/25856)
Train | Batch (196/196) | Top-1: 22.78 (11390/50000)
Regular: 0.43337467312812805
Epoche: 22; regular: 0.43337467312812805: flops 68862592
#Filters: 373, #FLOPs: 33.71M | Top-1: 14.44
Epoch 23
Train | Batch (1/196) | Top-1: 24.61 (63/256)
Train | Batch (101/196) | Top-1: 22.45 (5805/25856)
Train | Batch (196/196) | Top-1: 22.55 (11274/50000)
Regular: 0.4433499574661255
Epoche: 23; regular: 0.4433499574661255: flops 68862592
#Filters: 406, #FLOPs: 35.74M | Top-1: 20.81
Epoch 24
Train | Batch (1/196) | Top-1: 21.88 (56/256)
Train | Batch (101/196) | Top-1: 23.40 (6051/25856)
Train | Batch (196/196) | Top-1: 23.17 (11585/50000)
Regular: 0.42685309052467346
Epoche: 24; regular: 0.42685309052467346: flops 68862592
#Filters: 369, #FLOPs: 33.44M | Top-1: 9.23
Epoch 25
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 21.40 (5533/25856)
Train | Batch (196/196) | Top-1: 21.66 (10830/50000)
Regular: 0.7019687294960022
Epoche: 25; regular: 0.7019687294960022: flops 68862592
#Filters: 391, #FLOPs: 34.19M | Top-1: 15.85
Epoch 26
Train | Batch (1/196) | Top-1: 24.22 (62/256)
Train | Batch (101/196) | Top-1: 22.20 (5740/25856)
Train | Batch (196/196) | Top-1: 22.27 (11136/50000)
Regular: 0.4259495735168457
Epoche: 26; regular: 0.4259495735168457: flops 68862592
#Filters: 389, #FLOPs: 34.95M | Top-1: 18.13
Epoch 27
Train | Batch (1/196) | Top-1: 20.70 (53/256)
Train | Batch (101/196) | Top-1: 22.91 (5924/25856)
Train | Batch (196/196) | Top-1: 22.87 (11433/50000)
Regular: 0.42654192447662354
Epoche: 27; regular: 0.42654192447662354: flops 68862592
#Filters: 407, #FLOPs: 35.81M | Top-1: 14.49
Epoch 28
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 22.80 (5895/25856)
Train | Batch (196/196) | Top-1: 23.08 (11538/50000)
Regular: 0.4245162904262543
Epoche: 28; regular: 0.4245162904262543: flops 68862592
#Filters: 369, #FLOPs: 32.90M | Top-1: 10.00
Epoch 29
Train | Batch (1/196) | Top-1: 15.23 (39/256)
Train | Batch (101/196) | Top-1: 22.85 (5909/25856)
Train | Batch (196/196) | Top-1: 22.58 (11291/50000)
Regular: 0.44170910120010376
Epoche: 29; regular: 0.44170910120010376: flops 68862592
#Filters: 373, #FLOPs: 33.12M | Top-1: 20.26
Drin!!
Layers that will be prunned: [(1, 6), (3, 8), (5, 10), (7, 7), (9, 4), (11, 28), (12, 10), (13, 27), (14, 10), (15, 25), (16, 10), (17, 26), (18, 10), (19, 31), (20, 10), (21, 63), (22, 41), (23, 62), (24, 41), (25, 62), (26, 41), (27, 62), (28, 41), (29, 61), (30, 41)]
Prunning filters..
Layer index: 12; Pruned filters: 2
Layer index: 12; Pruned filters: 4
Layer index: 12; Pruned filters: 1
Layer index: 12; Pruned filters: 2
Layer index: 12; Pruned filters: 1
Layer index: 14; Pruned filters: 2
Layer index: 14; Pruned filters: 4
Layer index: 14; Pruned filters: 1
Layer index: 14; Pruned filters: 2
Layer index: 14; Pruned filters: 1
Layer index: 16; Pruned filters: 2
Layer index: 16; Pruned filters: 4
Layer index: 16; Pruned filters: 1
Layer index: 16; Pruned filters: 2
Layer index: 16; Pruned filters: 1
Layer index: 18; Pruned filters: 2
Layer index: 18; Pruned filters: 4
Layer index: 18; Pruned filters: 1
Layer index: 18; Pruned filters: 2
Layer index: 18; Pruned filters: 1
Layer index: 20; Pruned filters: 2
Layer index: 20; Pruned filters: 4
Layer index: 20; Pruned filters: 1
Layer index: 20; Pruned filters: 2
Layer index: 20; Pruned filters: 1
Layer index: 22; Pruned filters: 2
Layer index: 22; Pruned filters: 4
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 2
Layer index: 22; Pruned filters: 13
Layer index: 22; Pruned filters: 19
Layer index: 24; Pruned filters: 2
Layer index: 24; Pruned filters: 4
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 2
Layer index: 24; Pruned filters: 13
Layer index: 24; Pruned filters: 19
Layer index: 26; Pruned filters: 2
Layer index: 26; Pruned filters: 4
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 2
Layer index: 26; Pruned filters: 13
Layer index: 26; Pruned filters: 19
Layer index: 28; Pruned filters: 2
Layer index: 28; Pruned filters: 4
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 2
Layer index: 28; Pruned filters: 13
Layer index: 28; Pruned filters: 19
Layer index: 30; Pruned filters: 2
Layer index: 30; Pruned filters: 4
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 2
Layer index: 30; Pruned filters: 13
Layer index: 30; Pruned filters: 19
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 3
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 3
Layer index: 5; Pruned filters: 2
Layer index: 5; Pruned filters: 5
Layer index: 5; Pruned filters: 3
Layer index: 7; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 7; Pruned filters: 2
Layer index: 7; Pruned filters: 2
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 2
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 7
Layer index: 11; Pruned filters: 12
Layer index: 11; Pruned filters: 5
Layer index: 11; Pruned filters: 4
Layer index: 13; Pruned filters: 4
Layer index: 13; Pruned filters: 5
Layer index: 13; Pruned filters: 7
Layer index: 13; Pruned filters: 9
Layer index: 13; Pruned filters: 2
Layer index: 15; Pruned filters: 5
Layer index: 15; Pruned filters: 5
Layer index: 15; Pruned filters: 11
Layer index: 15; Pruned filters: 4
Layer index: 17; Pruned filters: 9
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 4
Layer index: 17; Pruned filters: 5
Layer index: 17; Pruned filters: 6
Layer index: 17; Pruned filters: 1
Layer index: 19; Pruned filters: 10
Layer index: 19; Pruned filters: 21
Layer index: 21; Pruned filters: 19
Layer index: 21; Pruned filters: 44
Layer index: 23; Pruned filters: 62
Layer index: 25; Pruned filters: 62
Layer index: 27; Pruned filters: 62
Layer index: 29; Pruned filters: 7
Layer index: 29; Pruned filters: 8
Layer index: 29; Pruned filters: 44
Layer index: 29; Pruned filters: 2
Target (flops): 68.863M
After Pruning | FLOPs: 16.254M | #Params: 0.027M
2.072949737429516
After Growth | FLOPs: 68.915M | #Params: 0.112M
I: 1
flops: 68915424
Before Pruning | FLOPs: 68.915M | #Params: 0.112M
Epoch 0
Train | Batch (1/196) | Top-1: 21.48 (55/256)
Train | Batch (101/196) | Top-1: 17.01 (4399/25856)
Train | Batch (196/196) | Top-1: 17.77 (8885/50000)
Regular: 3.006479024887085
Epoche: 0; regular: 3.006479024887085: flops 68915424
#Filters: 663, #FLOPs: 44.72M | Top-1: 17.42
Epoch 1
Train | Batch (1/196) | Top-1: 22.27 (57/256)
Train | Batch (101/196) | Top-1: 21.93 (5671/25856)
Train | Batch (196/196) | Top-1: 22.54 (11268/50000)
Regular: 1.018241286277771
Epoche: 1; regular: 1.018241286277771: flops 68915424
#Filters: 767, #FLOPs: 63.78M | Top-1: 13.68
Epoch 2
Train | Batch (1/196) | Top-1: 22.66 (58/256)
Train | Batch (101/196) | Top-1: 23.89 (6176/25856)
Train | Batch (196/196) | Top-1: 24.42 (12210/50000)
Regular: 0.9486162662506104
Epoche: 2; regular: 0.9486162662506104: flops 68915424
#Filters: 763, #FLOPs: 63.29M | Top-1: 14.15
Epoch 3
Train | Batch (1/196) | Top-1: 29.30 (75/256)
Train | Batch (101/196) | Top-1: 26.12 (6753/25856)
Train | Batch (196/196) | Top-1: 26.67 (13334/50000)
Regular: 0.8997848033905029
Epoche: 3; regular: 0.8997848033905029: flops 68915424
#Filters: 752, #FLOPs: 65.36M | Top-1: 17.71
Epoch 4
Train | Batch (1/196) | Top-1: 26.17 (67/256)
Train | Batch (101/196) | Top-1: 24.66 (6375/25856)
Train | Batch (196/196) | Top-1: 26.34 (13170/50000)
Regular: 1.005030870437622
Epoche: 4; regular: 1.005030870437622: flops 68915424
#Filters: 628, #FLOPs: 63.70M | Top-1: 10.70
Epoch 5
Train | Batch (1/196) | Top-1: 24.22 (62/256)
Train | Batch (101/196) | Top-1: 22.86 (5911/25856)
Train | Batch (196/196) | Top-1: 23.99 (11993/50000)
Regular: 0.9102280139923096
Epoche: 5; regular: 0.9102280139923096: flops 68915424
#Filters: 624, #FLOPs: 63.48M | Top-1: 14.60
Epoch 6
Train | Batch (1/196) | Top-1: 26.17 (67/256)
Train | Batch (101/196) | Top-1: 25.58 (6614/25856)
Train | Batch (196/196) | Top-1: 26.17 (13087/50000)
Regular: 0.8571861386299133
Epoche: 6; regular: 0.8571861386299133: flops 68915424
#Filters: 601, #FLOPs: 61.28M | Top-1: 20.66
Epoch 7
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 26.60 (6877/25856)
Train | Batch (196/196) | Top-1: 26.65 (13327/50000)
Regular: 0.8558789491653442
Epoche: 7; regular: 0.8558789491653442: flops 68915424
#Filters: 605, #FLOPs: 61.25M | Top-1: 24.78
Epoch 8
Train | Batch (1/196) | Top-1: 25.78 (66/256)
Train | Batch (101/196) | Top-1: 27.12 (7012/25856)
Train | Batch (196/196) | Top-1: 27.41 (13705/50000)
Regular: 0.8622260689735413
Epoche: 8; regular: 0.8622260689735413: flops 68915424
#Filters: 625, #FLOPs: 63.45M | Top-1: 18.40
Epoch 9
Train | Batch (1/196) | Top-1: 23.83 (61/256)
Train | Batch (101/196) | Top-1: 26.81 (6932/25856)
Train | Batch (196/196) | Top-1: 27.16 (13578/50000)
Regular: 1.2142109870910645
Epoche: 9; regular: 1.2142109870910645: flops 68915424
#Filters: 601, #FLOPs: 60.57M | Top-1: 13.17
Epoch 10
Train | Batch (1/196) | Top-1: 25.00 (64/256)
Train | Batch (101/196) | Top-1: 28.05 (7253/25856)
Train | Batch (196/196) | Top-1: 28.39 (14194/50000)
Regular: 0.9100443124771118
Epoche: 10; regular: 0.9100443124771118: flops 68915424
#Filters: 599, #FLOPs: 61.17M | Top-1: 10.41
Epoch 11
Train | Batch (1/196) | Top-1: 25.39 (65/256)
Train | Batch (101/196) | Top-1: 27.82 (7193/25856)
Train | Batch (196/196) | Top-1: 28.10 (14050/50000)
Regular: 0.9694717526435852
Epoche: 11; regular: 0.9694717526435852: flops 68915424
#Filters: 629, #FLOPs: 63.67M | Top-1: 18.75
Epoch 12
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 28.50 (7368/25856)
Train | Batch (196/196) | Top-1: 29.03 (14516/50000)
Regular: 0.9022678732872009
Epoche: 12; regular: 0.9022678732872009: flops 68915424
#Filters: 611, #FLOPs: 62.64M | Top-1: 11.15
Epoch 13
Train | Batch (1/196) | Top-1: 30.47 (78/256)
Train | Batch (101/196) | Top-1: 27.79 (7186/25856)
Train | Batch (196/196) | Top-1: 28.72 (14361/50000)
Regular: 0.8830474615097046
Epoche: 13; regular: 0.8830474615097046: flops 68915424
#Filters: 631, #FLOPs: 65.23M | Top-1: 23.07
Epoch 14
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 22.52 (5822/25856)
Train | Batch (196/196) | Top-1: 25.11 (12556/50000)
Regular: 9.203866004943848
Epoche: 14; regular: 9.203866004943848: flops 68915424
#Filters: 606, #FLOPs: 61.37M | Top-1: 22.82
Epoch 15
Train | Batch (1/196) | Top-1: 26.17 (67/256)
Train | Batch (101/196) | Top-1: 29.00 (7497/25856)
Train | Batch (196/196) | Top-1: 29.21 (14605/50000)
Regular: 6.328812599182129
Epoche: 15; regular: 6.328812599182129: flops 68915424
#Filters: 600, #FLOPs: 60.46M | Top-1: 10.00
Epoch 16
Train | Batch (1/196) | Top-1: 24.61 (63/256)
Train | Batch (101/196) | Top-1: 29.92 (7736/25856)
Train | Batch (196/196) | Top-1: 29.88 (14938/50000)
Regular: 4.477097988128662
Epoche: 16; regular: 4.477097988128662: flops 68915424
#Filters: 621, #FLOPs: 62.85M | Top-1: 24.05
Epoch 17
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 30.45 (7873/25856)
Train | Batch (196/196) | Top-1: 30.37 (15185/50000)
Regular: 2.8122150897979736
Epoche: 17; regular: 2.8122150897979736: flops 68915424
#Filters: 606, #FLOPs: 61.77M | Top-1: 9.53
Epoch 18
Train | Batch (1/196) | Top-1: 25.39 (65/256)
Train | Batch (101/196) | Top-1: 27.44 (7094/25856)
Train | Batch (196/196) | Top-1: 29.15 (14576/50000)
Regular: 5.675146579742432
Epoche: 18; regular: 5.675146579742432: flops 68915424
#Filters: 624, #FLOPs: 64.01M | Top-1: 22.45
Epoch 19
Train | Batch (1/196) | Top-1: 31.25 (80/256)
Train | Batch (101/196) | Top-1: 28.26 (7307/25856)
Train | Batch (196/196) | Top-1: 29.39 (14693/50000)
Regular: 1.1493622064590454
Epoche: 19; regular: 1.1493622064590454: flops 68915424
#Filters: 602, #FLOPs: 60.91M | Top-1: 10.01
Epoch 20
Train | Batch (1/196) | Top-1: 28.12 (72/256)
Train | Batch (101/196) | Top-1: 31.24 (8078/25856)
Train | Batch (196/196) | Top-1: 30.96 (15481/50000)
Regular: 0.9148412942886353
Epoche: 20; regular: 0.9148412942886353: flops 68915424
#Filters: 607, #FLOPs: 61.72M | Top-1: 18.10
Epoch 21
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 31.11 (8045/25856)
Train | Batch (196/196) | Top-1: 30.87 (15433/50000)
Regular: 1.1883093118667603
Epoche: 21; regular: 1.1883093118667603: flops 68915424
#Filters: 625, #FLOPs: 63.75M | Top-1: 20.61
Epoch 22
Train | Batch (1/196) | Top-1: 26.56 (68/256)
Train | Batch (101/196) | Top-1: 30.67 (7929/25856)
Train | Batch (196/196) | Top-1: 31.14 (15572/50000)
Regular: 0.9618433713912964
Epoche: 22; regular: 0.9618433713912964: flops 68915424
#Filters: 599, #FLOPs: 61.59M | Top-1: 11.63
Epoch 23
Train | Batch (1/196) | Top-1: 30.47 (78/256)
Train | Batch (101/196) | Top-1: 31.59 (8168/25856)
Train | Batch (196/196) | Top-1: 31.53 (15766/50000)
Regular: 1.0894343852996826
Epoche: 23; regular: 1.0894343852996826: flops 68915424
#Filters: 630, #FLOPs: 64.55M | Top-1: 14.44
Epoch 24
Train | Batch (1/196) | Top-1: 26.95 (69/256)
Train | Batch (101/196) | Top-1: 31.79 (8219/25856)
Train | Batch (196/196) | Top-1: 31.82 (15908/50000)
Regular: 0.8949022889137268
Epoche: 24; regular: 0.8949022889137268: flops 68915424
#Filters: 603, #FLOPs: 61.80M | Top-1: 11.93
Epoch 25
Train | Batch (1/196) | Top-1: 29.69 (76/256)
Train | Batch (101/196) | Top-1: 31.76 (8211/25856)
Train | Batch (196/196) | Top-1: 31.82 (15910/50000)
Regular: 0.9086968302726746
Epoche: 25; regular: 0.9086968302726746: flops 68915424
#Filters: 607, #FLOPs: 62.42M | Top-1: 20.36
Epoch 26
Train | Batch (1/196) | Top-1: 33.98 (87/256)
Train | Batch (101/196) | Top-1: 31.46 (8134/25856)
Train | Batch (196/196) | Top-1: 31.72 (15860/50000)
Regular: 1.0098036527633667
Epoche: 26; regular: 1.0098036527633667: flops 68915424
#Filters: 625, #FLOPs: 63.40M | Top-1: 25.74
Epoch 27
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 32.11 (8303/25856)
Train | Batch (196/196) | Top-1: 32.26 (16129/50000)
Regular: 0.9195349812507629
Epoche: 27; regular: 0.9195349812507629: flops 68915424
#Filters: 607, #FLOPs: 62.25M | Top-1: 10.36
Epoch 28
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 32.07 (8292/25856)
Train | Batch (196/196) | Top-1: 32.21 (16104/50000)
Regular: 0.901463508605957
Epoche: 28; regular: 0.901463508605957: flops 68915424
#Filters: 625, #FLOPs: 64.11M | Top-1: 10.03
Epoch 29
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 32.62 (8433/25856)
Train | Batch (196/196) | Top-1: 31.28 (15642/50000)
Regular: 1.0297561883926392
Epoche: 29; regular: 1.0297561883926392: flops 68915424
#Filters: 610, #FLOPs: 61.75M | Top-1: 10.27
Drin!!
Layers that will be prunned: [(1, 1), (3, 2), (7, 1), (9, 3), (12, 13), (13, 8), (14, 13), (15, 12), (16, 13), (17, 9), (18, 13), (19, 1), (20, 13), (21, 1), (22, 15), (23, 2), (24, 15), (25, 2), (26, 15), (27, 2), (28, 15), (29, 3), (30, 15)]
Prunning filters..
Layer index: 12; Pruned filters: 13
Layer index: 14; Pruned filters: 13
Layer index: 16; Pruned filters: 13
Layer index: 18; Pruned filters: 13
Layer index: 20; Pruned filters: 13
Layer index: 22; Pruned filters: 15
Layer index: 24; Pruned filters: 15
Layer index: 26; Pruned filters: 15
Layer index: 28; Pruned filters: 15
Layer index: 30; Pruned filters: 15
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 2
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 2
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 7
Layer index: 15; Pruned filters: 2
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 9
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 2
Layer index: 17; Pruned filters: 6
Layer index: 19; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 25; Pruned filters: 2
Layer index: 27; Pruned filters: 2
Layer index: 29; Pruned filters: 3
Target (flops): 68.863M
After Pruning | FLOPs: 56.796M | #Params: 0.069M
1.101932387926491
After Growth | FLOPs: 68.097M | #Params: 0.082M
I: 2
flops: 68097384
Before Pruning | FLOPs: 68.097M | #Params: 0.082M
Epoch 0
Train | Batch (1/196) | Top-1: 15.62 (40/256)
Train | Batch (101/196) | Top-1: 19.18 (4960/25856)
Train | Batch (196/196) | Top-1: 20.17 (10083/50000)
Regular: 5.7106614112854
Epoche: 0; regular: 5.7106614112854: flops 68097384
#Filters: 581, #FLOPs: 46.24M | Top-1: 10.00
Epoch 1
Train | Batch (1/196) | Top-1: 25.39 (65/256)
Train | Batch (101/196) | Top-1: 16.14 (4173/25856)
Train | Batch (196/196) | Top-1: 19.08 (9539/50000)
Regular: 2.029085159301758
Epoche: 1; regular: 2.029085159301758: flops 68097384
#Filters: 641, #FLOPs: 62.15M | Top-1: 11.00
Epoch 2
Train | Batch (1/196) | Top-1: 25.00 (64/256)
Train | Batch (101/196) | Top-1: 24.52 (6340/25856)
Train | Batch (196/196) | Top-1: 24.73 (12367/50000)
Regular: 1.1142675876617432
Epoche: 2; regular: 1.1142675876617432: flops 68097384
#Filters: 650, #FLOPs: 65.13M | Top-1: 22.90
Epoch 3
Train | Batch (1/196) | Top-1: 23.05 (59/256)
Train | Batch (101/196) | Top-1: 26.31 (6803/25856)
Train | Batch (196/196) | Top-1: 26.20 (13102/50000)
Regular: 1.07846999168396
Epoche: 3; regular: 1.07846999168396: flops 68097384
#Filters: 637, #FLOPs: 65.31M | Top-1: 11.09
Epoch 4
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 28.08 (7261/25856)
Train | Batch (196/196) | Top-1: 28.30 (14148/50000)
Regular: 1.0965505838394165
Epoche: 4; regular: 1.0965505838394165: flops 68097384
#Filters: 654, #FLOPs: 66.52M | Top-1: 10.00
Epoch 5
Train | Batch (1/196) | Top-1: 20.70 (53/256)
Train | Batch (101/196) | Top-1: 28.71 (7424/25856)
Train | Batch (196/196) | Top-1: 28.87 (14433/50000)
Regular: 1.3245437145233154
Epoche: 5; regular: 1.3245437145233154: flops 68097384
#Filters: 656, #FLOPs: 66.94M | Top-1: 10.82
Epoch 6
Train | Batch (1/196) | Top-1: 30.47 (78/256)
Train | Batch (101/196) | Top-1: 27.83 (7197/25856)
Train | Batch (196/196) | Top-1: 28.81 (14406/50000)
Regular: 1.1570026874542236
Epoche: 6; regular: 1.1570026874542236: flops 68097384
#Filters: 645, #FLOPs: 64.95M | Top-1: 9.99
Epoch 7
Train | Batch (1/196) | Top-1: 25.78 (66/256)
Train | Batch (101/196) | Top-1: 26.45 (6839/25856)
Train | Batch (196/196) | Top-1: 27.56 (13780/50000)
Regular: 1.2606600522994995
Epoche: 7; regular: 1.2606600522994995: flops 68097384
#Filters: 647, #FLOPs: 65.44M | Top-1: 10.47
Epoch 8
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 29.35 (7588/25856)
Train | Batch (196/196) | Top-1: 30.10 (15050/50000)
Regular: 1.6735764741897583
Epoche: 8; regular: 1.6735764741897583: flops 68097384
#Filters: 655, #FLOPs: 65.94M | Top-1: 13.20
Epoch 9
Train | Batch (1/196) | Top-1: 34.38 (88/256)
Train | Batch (101/196) | Top-1: 30.89 (7987/25856)
Train | Batch (196/196) | Top-1: 31.23 (15614/50000)
Regular: 1.1400775909423828
Epoche: 9; regular: 1.1400775909423828: flops 68097384
#Filters: 638, #FLOPs: 62.87M | Top-1: 10.01
Epoch 10
Train | Batch (1/196) | Top-1: 28.52 (73/256)
Train | Batch (101/196) | Top-1: 31.56 (8161/25856)
Train | Batch (196/196) | Top-1: 27.22 (13611/50000)
Regular: 2.2815864086151123
Epoche: 10; regular: 2.2815864086151123: flops 68097384
#Filters: 646, #FLOPs: 66.25M | Top-1: 10.03
Epoch 11
Train | Batch (1/196) | Top-1: 17.19 (44/256)
Train | Batch (101/196) | Top-1: 24.11 (6235/25856)
Train | Batch (196/196) | Top-1: 25.16 (12578/50000)
Regular: 2.777834892272949
Epoche: 11; regular: 2.777834892272949: flops 68097384
#Filters: 653, #FLOPs: 65.77M | Top-1: 10.05
Epoch 12
Train | Batch (1/196) | Top-1: 28.12 (72/256)
Train | Batch (101/196) | Top-1: 28.17 (7284/25856)
Train | Batch (196/196) | Top-1: 29.01 (14507/50000)
Regular: 1.374233603477478
Epoche: 12; regular: 1.374233603477478: flops 68097384
#Filters: 654, #FLOPs: 66.11M | Top-1: 16.70
Epoch 13
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 30.00 (7756/25856)
Train | Batch (196/196) | Top-1: 30.33 (15166/50000)
Regular: 1.5483307838439941
Epoche: 13; regular: 1.5483307838439941: flops 68097384
#Filters: 648, #FLOPs: 64.95M | Top-1: 14.26
Epoch 14
Train | Batch (1/196) | Top-1: 32.03 (82/256)
Train | Batch (101/196) | Top-1: 31.27 (8086/25856)
Train | Batch (196/196) | Top-1: 31.14 (15569/50000)
Regular: 1.1503300666809082
Epoche: 14; regular: 1.1503300666809082: flops 68097384
#Filters: 650, #FLOPs: 65.28M | Top-1: 13.71
Epoch 15
Train | Batch (1/196) | Top-1: 30.08 (77/256)
Train | Batch (101/196) | Top-1: 31.79 (8219/25856)
Train | Batch (196/196) | Top-1: 32.47 (16234/50000)
Regular: 1.2084858417510986
Epoche: 15; regular: 1.2084858417510986: flops 68097384
#Filters: 647, #FLOPs: 65.03M | Top-1: 21.55
Epoch 16
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 31.74 (8207/25856)
Train | Batch (196/196) | Top-1: 32.49 (16247/50000)
Regular: 1.2514550685882568
Epoche: 16; regular: 1.2514550685882568: flops 68097384
#Filters: 646, #FLOPs: 64.28M | Top-1: 32.14
Epoch 17
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 32.58 (8423/25856)
Train | Batch (196/196) | Top-1: 32.57 (16285/50000)
Regular: 1.1200289726257324
Epoche: 17; regular: 1.1200289726257324: flops 68097384
#Filters: 655, #FLOPs: 66.19M | Top-1: 11.74
Epoch 18
Train | Batch (1/196) | Top-1: 26.56 (68/256)
Train | Batch (101/196) | Top-1: 33.36 (8626/25856)
Train | Batch (196/196) | Top-1: 33.35 (16676/50000)
Regular: 1.069481611251831
Epoche: 18; regular: 1.069481611251831: flops 68097384
#Filters: 654, #FLOPs: 65.77M | Top-1: 15.82
Epoch 19
Train | Batch (1/196) | Top-1: 18.75 (48/256)
Train | Batch (101/196) | Top-1: 31.71 (8200/25856)
Train | Batch (196/196) | Top-1: 32.84 (16420/50000)
Regular: 1.1243659257888794
Epoche: 19; regular: 1.1243659257888794: flops 68097384
#Filters: 648, #FLOPs: 66.02M | Top-1: 10.38
Epoch 20
Train | Batch (1/196) | Top-1: 33.98 (87/256)
Train | Batch (101/196) | Top-1: 33.71 (8715/25856)
Train | Batch (196/196) | Top-1: 34.27 (17134/50000)
Regular: 1.2088693380355835
Epoche: 20; regular: 1.2088693380355835: flops 68097384
#Filters: 649, #FLOPs: 64.53M | Top-1: 32.22
Epoch 21
Train | Batch (1/196) | Top-1: 40.23 (103/256)
Train | Batch (101/196) | Top-1: 32.31 (8354/25856)
Train | Batch (196/196) | Top-1: 33.38 (16691/50000)
Regular: 1.3984854221343994
Epoche: 21; regular: 1.3984854221343994: flops 68097384
#Filters: 649, #FLOPs: 65.69M | Top-1: 15.61
Epoch 22
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 35.74 (9241/25856)
Train | Batch (196/196) | Top-1: 35.41 (17704/50000)
Regular: 1.6721917390823364
Epoche: 22; regular: 1.6721917390823364: flops 68097384
#Filters: 653, #FLOPs: 66.36M | Top-1: 24.98
Epoch 23
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 35.05 (9063/25856)
Train | Batch (196/196) | Top-1: 35.86 (17930/50000)
Regular: 2.106534957885742
Epoche: 23; regular: 2.106534957885742: flops 68097384
#Filters: 655, #FLOPs: 66.27M | Top-1: 17.46
Epoch 24
Train | Batch (1/196) | Top-1: 34.77 (89/256)
Train | Batch (101/196) | Top-1: 36.26 (9375/25856)
Train | Batch (196/196) | Top-1: 36.51 (18253/50000)
Regular: 1.3053444623947144
Epoche: 24; regular: 1.3053444623947144: flops 68097384
#Filters: 649, #FLOPs: 65.19M | Top-1: 17.56
Epoch 25
Train | Batch (1/196) | Top-1: 32.81 (84/256)
Train | Batch (101/196) | Top-1: 33.37 (8627/25856)
Train | Batch (196/196) | Top-1: 34.24 (17119/50000)
Regular: 1.725955843925476
Epoche: 25; regular: 1.725955843925476: flops 68097384
#Filters: 650, #FLOPs: 65.19M | Top-1: 10.00
Epoch 26
Train | Batch (1/196) | Top-1: 37.11 (95/256)
Train | Batch (101/196) | Top-1: 35.73 (9239/25856)
Train | Batch (196/196) | Top-1: 35.96 (17978/50000)
Regular: 1.1928775310516357
Epoche: 26; regular: 1.1928775310516357: flops 68097384
#Filters: 656, #FLOPs: 65.61M | Top-1: 26.74
Epoch 27
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 37.05 (9579/25856)
Train | Batch (196/196) | Top-1: 36.51 (18255/50000)
Regular: 1.4854798316955566
Epoche: 27; regular: 1.4854798316955566: flops 68097384
#Filters: 655, #FLOPs: 66.52M | Top-1: 9.98
Epoch 28
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 37.87 (9791/25856)
Train | Batch (196/196) | Top-1: 37.82 (18909/50000)
Regular: 1.203898549079895
Epoche: 28; regular: 1.203898549079895: flops 68097384
#Filters: 653, #FLOPs: 65.94M | Top-1: 28.19
Epoch 29
Train | Batch (1/196) | Top-1: 30.08 (77/256)
Train | Batch (101/196) | Top-1: 37.56 (9711/25856)
Train | Batch (196/196) | Top-1: 37.54 (18768/50000)
Regular: 1.1248793601989746
Epoche: 29; regular: 1.1248793601989746: flops 68097384
#Filters: 653, #FLOPs: 66.11M | Top-1: 13.97
Drin!!
Layers that will be prunned: [(1, 2), (3, 1), (7, 1), (11, 4), (13, 1), (15, 2), (17, 1)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 11; Pruned filters: 3
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 2
Layer index: 17; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 64.116M | #Params: 0.074M
1.036638769872255
After Growth | FLOPs: 68.625M | #Params: 0.079M
I: 3
flops: 68625010
Before Pruning | FLOPs: 68.625M | #Params: 0.079M
Epoch 0
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 27.21 (7036/25856)
Train | Batch (196/196) | Top-1: 30.17 (15085/50000)
Regular: 1.2072418928146362
Epoche: 0; regular: 1.2072418928146362: flops 68625010
#Filters: 423, #FLOPs: 47.52M | Top-1: 27.81
Epoch 1
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 34.80 (8998/25856)
Train | Batch (196/196) | Top-1: 34.81 (17406/50000)
Regular: 1.2586438655853271
Epoche: 1; regular: 1.2586438655853271: flops 68625010
#Filters: 605, #FLOPs: 50.72M | Top-1: 10.00
Epoch 2
Train | Batch (1/196) | Top-1: 30.86 (79/256)
Train | Batch (101/196) | Top-1: 34.90 (9023/25856)
Train | Batch (196/196) | Top-1: 35.24 (17618/50000)
Regular: 1.5374125242233276
Epoche: 2; regular: 1.5374125242233276: flops 68625010
#Filters: 663, #FLOPs: 66.66M | Top-1: 10.00
Epoch 3
Train | Batch (1/196) | Top-1: 33.98 (87/256)
Train | Batch (101/196) | Top-1: 35.81 (9258/25856)
Train | Batch (196/196) | Top-1: 35.72 (17861/50000)
Regular: 2.0367026329040527
Epoche: 3; regular: 2.0367026329040527: flops 68625010
#Filters: 657, #FLOPs: 64.62M | Top-1: 12.71
Epoch 4
Train | Batch (1/196) | Top-1: 27.34 (70/256)
Train | Batch (101/196) | Top-1: 36.77 (9508/25856)
Train | Batch (196/196) | Top-1: 37.29 (18644/50000)
Regular: 2.392244338989258
Epoche: 4; regular: 2.392244338989258: flops 68625010
#Filters: 664, #FLOPs: 66.07M | Top-1: 23.12
Epoch 5
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 36.89 (9539/25856)
Train | Batch (196/196) | Top-1: 37.72 (18861/50000)
Regular: 1.408058762550354
Epoche: 5; regular: 1.408058762550354: flops 68625010
#Filters: 662, #FLOPs: 65.90M | Top-1: 15.21
Epoch 6
Train | Batch (1/196) | Top-1: 37.11 (95/256)
Train | Batch (101/196) | Top-1: 37.85 (9787/25856)
Train | Batch (196/196) | Top-1: 38.36 (19180/50000)
Regular: 1.8514108657836914
Epoche: 6; regular: 1.8514108657836914: flops 68625010
#Filters: 663, #FLOPs: 66.15M | Top-1: 30.37
Epoch 7
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 38.06 (9842/25856)
Train | Batch (196/196) | Top-1: 38.48 (19239/50000)
Regular: 1.3560723066329956
Epoche: 7; regular: 1.3560723066329956: flops 68625010
#Filters: 663, #FLOPs: 65.98M | Top-1: 10.90
Epoch 8
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 38.69 (10004/25856)
Train | Batch (196/196) | Top-1: 38.43 (19216/50000)
Regular: 1.3773701190948486
Epoche: 8; regular: 1.3773701190948486: flops 68625010
#Filters: 661, #FLOPs: 65.22M | Top-1: 11.05
Epoch 9
Train | Batch (1/196) | Top-1: 36.72 (94/256)
Train | Batch (101/196) | Top-1: 33.80 (8740/25856)
Train | Batch (196/196) | Top-1: 34.93 (17463/50000)
Regular: 6.344869136810303
Epoche: 9; regular: 6.344869136810303: flops 68625010
#Filters: 665, #FLOPs: 66.24M | Top-1: 25.31
Epoch 10
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 38.29 (9900/25856)
Train | Batch (196/196) | Top-1: 38.38 (19188/50000)
Regular: 4.544253349304199
Epoche: 10; regular: 4.544253349304199: flops 68625010
#Filters: 664, #FLOPs: 66.32M | Top-1: 11.47
Epoch 11
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 38.52 (9959/25856)
Train | Batch (196/196) | Top-1: 38.62 (19310/50000)
Regular: 2.961000442504883
Epoche: 11; regular: 2.961000442504883: flops 68625010
#Filters: 661, #FLOPs: 65.56M | Top-1: 20.26
Epoch 12
Train | Batch (1/196) | Top-1: 43.75 (112/256)
Train | Batch (101/196) | Top-1: 37.91 (9802/25856)
Train | Batch (196/196) | Top-1: 37.97 (18986/50000)
Regular: 2.168792963027954
Epoche: 12; regular: 2.168792963027954: flops 68625010
#Filters: 665, #FLOPs: 66.32M | Top-1: 28.38
Epoch 13
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 39.53 (10221/25856)
Train | Batch (196/196) | Top-1: 39.10 (19551/50000)
Regular: 1.9163267612457275
Epoche: 13; regular: 1.9163267612457275: flops 68625010
#Filters: 663, #FLOPs: 65.64M | Top-1: 15.25
Epoch 14
Train | Batch (1/196) | Top-1: 33.59 (86/256)
Train | Batch (101/196) | Top-1: 39.24 (10145/25856)
Train | Batch (196/196) | Top-1: 39.13 (19566/50000)
Regular: 2.375727415084839
Epoche: 14; regular: 2.375727415084839: flops 68625010
#Filters: 671, #FLOPs: 68.03M | Top-1: 11.78
Epoch 15
Train | Batch (1/196) | Top-1: 36.72 (94/256)
Train | Batch (101/196) | Top-1: 39.49 (10211/25856)
Train | Batch (196/196) | Top-1: 38.88 (19439/50000)
Regular: 1.4771490097045898
Epoche: 15; regular: 1.4771490097045898: flops 68625010
#Filters: 666, #FLOPs: 66.58M | Top-1: 8.82
Epoch 16
Train | Batch (1/196) | Top-1: 37.89 (97/256)
Train | Batch (101/196) | Top-1: 38.06 (9841/25856)
Train | Batch (196/196) | Top-1: 37.47 (18735/50000)
Regular: 1.4095803499221802
Epoche: 16; regular: 1.4095803499221802: flops 68625010
#Filters: 663, #FLOPs: 65.98M | Top-1: 23.09
Epoch 17
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 38.87 (10050/25856)
Train | Batch (196/196) | Top-1: 39.33 (19663/50000)
Regular: 2.2531018257141113
Epoche: 17; regular: 2.2531018257141113: flops 68625010
#Filters: 668, #FLOPs: 67.09M | Top-1: 14.90
Epoch 18
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 39.57 (10231/25856)
Train | Batch (196/196) | Top-1: 39.41 (19706/50000)
Regular: 1.3708735704421997
Epoche: 18; regular: 1.3708735704421997: flops 68625010
#Filters: 668, #FLOPs: 67.43M | Top-1: 24.58
Epoch 19
Train | Batch (1/196) | Top-1: 42.97 (110/256)
Train | Batch (101/196) | Top-1: 28.99 (7495/25856)
Train | Batch (196/196) | Top-1: 28.09 (14046/50000)
Regular: 1.7957561016082764
Epoche: 19; regular: 1.7957561016082764: flops 68625010
#Filters: 667, #FLOPs: 67.24M | Top-1: 12.75
Epoch 20
Train | Batch (1/196) | Top-1: 27.34 (70/256)
Train | Batch (101/196) | Top-1: 33.44 (8645/25856)
Train | Batch (196/196) | Top-1: 34.61 (17303/50000)
Regular: 1.4137318134307861
Epoche: 20; regular: 1.4137318134307861: flops 68625010
#Filters: 667, #FLOPs: 67.26M | Top-1: 14.24
Epoch 21
Train | Batch (1/196) | Top-1: 34.77 (89/256)
Train | Batch (101/196) | Top-1: 37.01 (9570/25856)
Train | Batch (196/196) | Top-1: 37.70 (18849/50000)
Regular: 1.1822811365127563
Epoche: 21; regular: 1.1822811365127563: flops 68625010
#Filters: 666, #FLOPs: 65.98M | Top-1: 10.74
Epoch 22
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 38.39 (9925/25856)
Train | Batch (196/196) | Top-1: 38.82 (19411/50000)
Regular: 1.2929141521453857
Epoche: 22; regular: 1.2929141521453857: flops 68625010
#Filters: 667, #FLOPs: 66.92M | Top-1: 15.56
Epoch 23
Train | Batch (1/196) | Top-1: 39.84 (102/256)
Train | Batch (101/196) | Top-1: 39.50 (10213/25856)
Train | Batch (196/196) | Top-1: 39.61 (19803/50000)
Regular: 1.315178394317627
Epoche: 23; regular: 1.315178394317627: flops 68625010
#Filters: 669, #FLOPs: 66.75M | Top-1: 29.97
Epoch 24
Train | Batch (1/196) | Top-1: 35.16 (90/256)
Train | Batch (101/196) | Top-1: 39.82 (10296/25856)
Train | Batch (196/196) | Top-1: 39.92 (19959/50000)
Regular: 1.287357211112976
Epoche: 24; regular: 1.287357211112976: flops 68625010
#Filters: 668, #FLOPs: 66.66M | Top-1: 15.51
Epoch 25
Train | Batch (1/196) | Top-1: 33.98 (87/256)
Train | Batch (101/196) | Top-1: 39.60 (10239/25856)
Train | Batch (196/196) | Top-1: 40.16 (20080/50000)
Regular: 1.173725962638855
Epoche: 25; regular: 1.173725962638855: flops 68625010
#Filters: 671, #FLOPs: 67.94M | Top-1: 10.02
Epoch 26
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 36.20 (9360/25856)
Train | Batch (196/196) | Top-1: 37.58 (18789/50000)
Regular: 1.7250854969024658
Epoche: 26; regular: 1.7250854969024658: flops 68625010
#Filters: 665, #FLOPs: 65.47M | Top-1: 10.00
Epoch 27
Train | Batch (1/196) | Top-1: 42.58 (109/256)
Train | Batch (101/196) | Top-1: 39.91 (10319/25856)
Train | Batch (196/196) | Top-1: 36.93 (18467/50000)
Regular: 2.8135769367218018
Epoche: 27; regular: 2.8135769367218018: flops 68625010
#Filters: 669, #FLOPs: 67.01M | Top-1: 9.78
Epoch 28
Train | Batch (1/196) | Top-1: 32.42 (83/256)
Train | Batch (101/196) | Top-1: 34.03 (8800/25856)
Train | Batch (196/196) | Top-1: 32.97 (16486/50000)
Regular: 1.8171850442886353
Epoche: 28; regular: 1.8171850442886353: flops 68625010
#Filters: 668, #FLOPs: 66.64M | Top-1: 28.17
Epoch 29
Train | Batch (1/196) | Top-1: 30.47 (78/256)
Train | Batch (101/196) | Top-1: 34.78 (8994/25856)
Train | Batch (196/196) | Top-1: 35.60 (17798/50000)
Regular: 1.2986159324645996
Epoche: 29; regular: 1.2986159324645996: flops 68625010
#Filters: 664, #FLOPs: 66.73M | Top-1: 11.43
Drin!!
Layers that will be prunned: [(1, 1), (5, 3), (9, 1), (11, 3), (29, 1)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 5; Pruned filters: 3
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 2
Layer index: 11; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 64.661M | #Params: 0.073M
1.0322333591322994
After Growth | FLOPs: 69.210M | #Params: 0.077M
I: 4
flops: 69210236
Before Pruning | FLOPs: 69.210M | #Params: 0.077M
Epoch 0
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 14.30 (3698/25856)
Train | Batch (196/196) | Top-1: 18.35 (9176/50000)
Regular: 1.2795286178588867
Epoche: 0; regular: 1.2795286178588867: flops 69210236
#Filters: 481, #FLOPs: 50.15M | Top-1: 10.07
Epoch 1
Train | Batch (1/196) | Top-1: 21.48 (55/256)
Train | Batch (101/196) | Top-1: 27.54 (7120/25856)
Train | Batch (196/196) | Top-1: 28.49 (14245/50000)
Regular: 1.0922832489013672
Epoche: 1; regular: 1.0922832489013672: flops 69210236
#Filters: 616, #FLOPs: 62.10M | Top-1: 11.50
Epoch 2
Train | Batch (1/196) | Top-1: 30.86 (79/256)
Train | Batch (101/196) | Top-1: 30.50 (7886/25856)
Train | Batch (196/196) | Top-1: 30.95 (15475/50000)
Regular: 1.1899049282073975
Epoche: 2; regular: 1.1899049282073975: flops 69210236
#Filters: 670, #FLOPs: 64.04M | Top-1: 11.73
Epoch 3
