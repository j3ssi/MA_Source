no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=2e-07, logger='MorphLogs/logMorphNetFlops2e-7_grow2.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 17.57 (4544/25856)
Train | Batch (196/196) | Top-1: 18.26 (9132/50000)
Regular: 5.715904235839844
Epoche: 0; regular: 5.715904235839844: flops 68862592
#Filters: 394, #FLOPs: 33.90M | Top-1: 13.33
Epoch 1
Train | Batch (1/196) | Top-1: 14.84 (38/256)
Train | Batch (101/196) | Top-1: 19.84 (5129/25856)
Train | Batch (196/196) | Top-1: 19.97 (9985/50000)
Regular: 0.6809785962104797
Epoche: 1; regular: 0.6809785962104797: flops 68862592
#Filters: 373, #FLOPs: 33.55M | Top-1: 10.05
Epoch 2
Train | Batch (1/196) | Top-1: 17.97 (46/256)
Train | Batch (101/196) | Top-1: 19.68 (5088/25856)
Train | Batch (196/196) | Top-1: 19.62 (9810/50000)
Regular: 0.4971235692501068
Epoche: 2; regular: 0.4971235692501068: flops 68862592
#Filters: 408, #FLOPs: 36.59M | Top-1: 10.00
Epoch 3
Train | Batch (1/196) | Top-1: 23.83 (61/256)
Train | Batch (101/196) | Top-1: 20.13 (5204/25856)
Train | Batch (196/196) | Top-1: 20.06 (10029/50000)
Regular: 0.5530796647071838
Epoche: 3; regular: 0.5530796647071838: flops 68862592
#Filters: 356, #FLOPs: 32.92M | Top-1: 12.08
Epoch 4
Train | Batch (1/196) | Top-1: 23.44 (60/256)
Train | Batch (101/196) | Top-1: 20.13 (5205/25856)
Train | Batch (196/196) | Top-1: 20.28 (10140/50000)
Regular: 0.5015735030174255
Epoche: 4; regular: 0.5015735030174255: flops 68862592
#Filters: 369, #FLOPs: 33.31M | Top-1: 9.90
Epoch 5
Train | Batch (1/196) | Top-1: 16.02 (41/256)
Train | Batch (101/196) | Top-1: 19.63 (5076/25856)
Train | Batch (196/196) | Top-1: 19.98 (9988/50000)
Regular: 0.6042009592056274
Epoche: 5; regular: 0.6042009592056274: flops 68862592
#Filters: 393, #FLOPs: 34.74M | Top-1: 10.00
Epoch 6
Train | Batch (1/196) | Top-1: 14.06 (36/256)
Train | Batch (101/196) | Top-1: 19.57 (5059/25856)
Train | Batch (196/196) | Top-1: 20.25 (10124/50000)
Regular: 1.5685698986053467
Epoche: 6; regular: 1.5685698986053467: flops 68862592
#Filters: 354, #FLOPs: 32.39M | Top-1: 9.75
Epoch 7
Train | Batch (1/196) | Top-1: 16.02 (41/256)
Train | Batch (101/196) | Top-1: 19.72 (5099/25856)
Train | Batch (196/196) | Top-1: 19.98 (9991/50000)
Regular: 0.45114684104919434
Epoche: 7; regular: 0.45114684104919434: flops 68862592
#Filters: 351, #FLOPs: 32.55M | Top-1: 15.08
Epoch 8
Train | Batch (1/196) | Top-1: 14.84 (38/256)
Train | Batch (101/196) | Top-1: 20.91 (5407/25856)
Train | Batch (196/196) | Top-1: 20.17 (10085/50000)
Regular: 1.0289350748062134
Epoche: 8; regular: 1.0289350748062134: flops 68862592
#Filters: 390, #FLOPs: 34.98M | Top-1: 10.21
Epoch 9
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 20.07 (5190/25856)
Train | Batch (196/196) | Top-1: 20.00 (10001/50000)
Regular: 1.050163745880127
Epoche: 9; regular: 1.050163745880127: flops 68862592
#Filters: 397, #FLOPs: 35.63M | Top-1: 10.00
Epoch 10
Train | Batch (1/196) | Top-1: 19.53 (50/256)
Train | Batch (101/196) | Top-1: 19.96 (5162/25856)
Train | Batch (196/196) | Top-1: 19.75 (9875/50000)
Regular: 0.7383372783660889
Epoche: 10; regular: 0.7383372783660889: flops 68862592
#Filters: 397, #FLOPs: 35.43M | Top-1: 10.00
Epoch 11
Train | Batch (1/196) | Top-1: 15.62 (40/256)
Train | Batch (101/196) | Top-1: 18.62 (4814/25856)
Train | Batch (196/196) | Top-1: 19.03 (9514/50000)
Regular: 2.800755500793457
Epoche: 11; regular: 2.800755500793457: flops 68862592
#Filters: 369, #FLOPs: 33.71M | Top-1: 10.01
Epoch 12
Train | Batch (1/196) | Top-1: 21.09 (54/256)
Train | Batch (101/196) | Top-1: 18.63 (4818/25856)
Train | Batch (196/196) | Top-1: 19.03 (9515/50000)
Regular: 0.6342440247535706
Epoche: 12; regular: 0.6342440247535706: flops 68862592
#Filters: 400, #FLOPs: 35.54M | Top-1: 10.43
Epoch 13
Train | Batch (1/196) | Top-1: 18.36 (47/256)
Train | Batch (101/196) | Top-1: 19.98 (5166/25856)
Train | Batch (196/196) | Top-1: 19.79 (9895/50000)
Regular: 0.6698533892631531
Epoche: 13; regular: 0.6698533892631531: flops 68862592
#Filters: 392, #FLOPs: 35.17M | Top-1: 10.50
Epoch 14
Train | Batch (1/196) | Top-1: 19.92 (51/256)
Train | Batch (101/196) | Top-1: 20.29 (5246/25856)
Train | Batch (196/196) | Top-1: 19.85 (9925/50000)
Regular: 0.6805883646011353
Epoche: 14; regular: 0.6805883646011353: flops 68862592
#Filters: 343, #FLOPs: 31.35M | Top-1: 10.01
Epoch 15
Train | Batch (1/196) | Top-1: 23.83 (61/256)
Train | Batch (101/196) | Top-1: 19.26 (4981/25856)
Train | Batch (196/196) | Top-1: 19.37 (9685/50000)
Regular: 0.6242219805717468
Epoche: 15; regular: 0.6242219805717468: flops 68862592
#Filters: 387, #FLOPs: 34.06M | Top-1: 10.01
Epoch 16
Train | Batch (1/196) | Top-1: 16.41 (42/256)
Train | Batch (101/196) | Top-1: 19.72 (5100/25856)
Train | Batch (196/196) | Top-1: 20.11 (10054/50000)
Regular: 0.5382633805274963
Epoche: 16; regular: 0.5382633805274963: flops 68862592
#Filters: 354, #FLOPs: 32.50M | Top-1: 22.71
Epoch 17
Train | Batch (1/196) | Top-1: 18.75 (48/256)
Train | Batch (101/196) | Top-1: 22.07 (5706/25856)
Train | Batch (196/196) | Top-1: 22.04 (11021/50000)
Regular: 0.42483454942703247
Epoche: 17; regular: 0.42483454942703247: flops 68862592
#Filters: 356, #FLOPs: 32.68M | Top-1: 13.28
Epoch 18
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 22.53 (5825/25856)
Train | Batch (196/196) | Top-1: 22.34 (11170/50000)
Regular: 0.4163147211074829
Epoche: 18; regular: 0.4163147211074829: flops 68862592
#Filters: 419, #FLOPs: 36.70M | Top-1: 10.00
Epoch 19
Train | Batch (1/196) | Top-1: 21.09 (54/256)
Train | Batch (101/196) | Top-1: 22.61 (5847/25856)
Train | Batch (196/196) | Top-1: 22.28 (11141/50000)
Regular: 0.46537452936172485
Epoche: 19; regular: 0.46537452936172485: flops 68862592
#Filters: 386, #FLOPs: 34.65M | Top-1: 10.80
Epoch 20
Train | Batch (1/196) | Top-1: 22.66 (58/256)
Train | Batch (101/196) | Top-1: 22.26 (5755/25856)
Train | Batch (196/196) | Top-1: 22.72 (11360/50000)
Regular: 0.43377211689949036
Epoche: 20; regular: 0.43377211689949036: flops 68862592
#Filters: 352, #FLOPs: 33.23M | Top-1: 9.88
Epoch 21
Train | Batch (1/196) | Top-1: 25.78 (66/256)
Train | Batch (101/196) | Top-1: 23.26 (6015/25856)
Train | Batch (196/196) | Top-1: 23.18 (11590/50000)
Regular: 0.4442186951637268
Epoche: 21; regular: 0.4442186951637268: flops 68862592
#Filters: 371, #FLOPs: 34.38M | Top-1: 11.87
Epoch 22
Train | Batch (1/196) | Top-1: 19.92 (51/256)
Train | Batch (101/196) | Top-1: 22.51 (5819/25856)
Train | Batch (196/196) | Top-1: 22.78 (11390/50000)
Regular: 0.43337467312812805
Epoche: 22; regular: 0.43337467312812805: flops 68862592
#Filters: 373, #FLOPs: 33.71M | Top-1: 14.44
Epoch 23
Train | Batch (1/196) | Top-1: 24.61 (63/256)
Train | Batch (101/196) | Top-1: 22.45 (5805/25856)
Train | Batch (196/196) | Top-1: 22.55 (11274/50000)
Regular: 0.4433499574661255
Epoche: 23; regular: 0.4433499574661255: flops 68862592
#Filters: 406, #FLOPs: 35.74M | Top-1: 20.81
Epoch 24
Train | Batch (1/196) | Top-1: 21.88 (56/256)
Train | Batch (101/196) | Top-1: 23.40 (6051/25856)
Train | Batch (196/196) | Top-1: 23.17 (11585/50000)
Regular: 0.42685309052467346
Epoche: 24; regular: 0.42685309052467346: flops 68862592
#Filters: 369, #FLOPs: 33.44M | Top-1: 9.23
Epoch 25
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 21.40 (5533/25856)
Train | Batch (196/196) | Top-1: 21.66 (10830/50000)
Regular: 0.7019687294960022
Epoche: 25; regular: 0.7019687294960022: flops 68862592
#Filters: 391, #FLOPs: 34.19M | Top-1: 15.85
Epoch 26
Train | Batch (1/196) | Top-1: 24.22 (62/256)
Train | Batch (101/196) | Top-1: 22.20 (5740/25856)
Train | Batch (196/196) | Top-1: 22.27 (11136/50000)
Regular: 0.4259495735168457
Epoche: 26; regular: 0.4259495735168457: flops 68862592
#Filters: 389, #FLOPs: 34.95M | Top-1: 18.13
Epoch 27
Train | Batch (1/196) | Top-1: 20.70 (53/256)
Train | Batch (101/196) | Top-1: 22.91 (5924/25856)
Train | Batch (196/196) | Top-1: 22.87 (11433/50000)
Regular: 0.42654192447662354
Epoche: 27; regular: 0.42654192447662354: flops 68862592
#Filters: 407, #FLOPs: 35.81M | Top-1: 14.49
Epoch 28
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 22.80 (5895/25856)
Train | Batch (196/196) | Top-1: 23.08 (11538/50000)
Regular: 0.4245162904262543
Epoche: 28; regular: 0.4245162904262543: flops 68862592
#Filters: 369, #FLOPs: 32.90M | Top-1: 10.00
Epoch 29
Train | Batch (1/196) | Top-1: 15.23 (39/256)
Train | Batch (101/196) | Top-1: 22.85 (5909/25856)
Train | Batch (196/196) | Top-1: 22.58 (11291/50000)
Regular: 0.44170910120010376
Epoche: 29; regular: 0.44170910120010376: flops 68862592
#Filters: 373, #FLOPs: 33.12M | Top-1: 20.26
Drin!!
Layers that will be prunned: [(1, 6), (3, 8), (5, 10), (7, 7), (9, 4), (11, 28), (12, 10), (13, 27), (14, 10), (15, 25), (16, 10), (17, 26), (18, 10), (19, 31), (20, 10), (21, 63), (22, 41), (23, 62), (24, 41), (25, 62), (26, 41), (27, 62), (28, 41), (29, 61), (30, 41)]
Prunning filters..
Layer index: 12; Pruned filters: 2
Layer index: 12; Pruned filters: 4
Layer index: 12; Pruned filters: 1
Layer index: 12; Pruned filters: 2
Layer index: 12; Pruned filters: 1
Layer index: 14; Pruned filters: 2
Layer index: 14; Pruned filters: 4
Layer index: 14; Pruned filters: 1
Layer index: 14; Pruned filters: 2
Layer index: 14; Pruned filters: 1
Layer index: 16; Pruned filters: 2
Layer index: 16; Pruned filters: 4
Layer index: 16; Pruned filters: 1
Layer index: 16; Pruned filters: 2
Layer index: 16; Pruned filters: 1
Layer index: 18; Pruned filters: 2
Layer index: 18; Pruned filters: 4
Layer index: 18; Pruned filters: 1
Layer index: 18; Pruned filters: 2
Layer index: 18; Pruned filters: 1
Layer index: 20; Pruned filters: 2
Layer index: 20; Pruned filters: 4
Layer index: 20; Pruned filters: 1
Layer index: 20; Pruned filters: 2
Layer index: 20; Pruned filters: 1
Layer index: 22; Pruned filters: 2
Layer index: 22; Pruned filters: 4
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 2
Layer index: 22; Pruned filters: 13
Layer index: 22; Pruned filters: 19
Layer index: 24; Pruned filters: 2
Layer index: 24; Pruned filters: 4
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 2
Layer index: 24; Pruned filters: 13
Layer index: 24; Pruned filters: 19
Layer index: 26; Pruned filters: 2
Layer index: 26; Pruned filters: 4
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 2
Layer index: 26; Pruned filters: 13
Layer index: 26; Pruned filters: 19
Layer index: 28; Pruned filters: 2
Layer index: 28; Pruned filters: 4
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 2
Layer index: 28; Pruned filters: 13
Layer index: 28; Pruned filters: 19
Layer index: 30; Pruned filters: 2
Layer index: 30; Pruned filters: 4
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 2
Layer index: 30; Pruned filters: 13
Layer index: 30; Pruned filters: 19
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 3
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 3
Layer index: 5; Pruned filters: 2
Layer index: 5; Pruned filters: 5
Layer index: 5; Pruned filters: 3
Layer index: 7; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 7; Pruned filters: 2
Layer index: 7; Pruned filters: 2
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 2
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 7
Layer index: 11; Pruned filters: 12
Layer index: 11; Pruned filters: 5
Layer index: 11; Pruned filters: 4
Layer index: 13; Pruned filters: 4
Layer index: 13; Pruned filters: 5
Layer index: 13; Pruned filters: 7
Layer index: 13; Pruned filters: 9
Layer index: 13; Pruned filters: 2
Layer index: 15; Pruned filters: 5
Layer index: 15; Pruned filters: 5
Layer index: 15; Pruned filters: 11
Layer index: 15; Pruned filters: 4
Layer index: 17; Pruned filters: 9
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 4
Layer index: 17; Pruned filters: 5
Layer index: 17; Pruned filters: 6
Layer index: 17; Pruned filters: 1
Layer index: 19; Pruned filters: 10
Layer index: 19; Pruned filters: 21
Layer index: 21; Pruned filters: 19
Layer index: 21; Pruned filters: 44
Layer index: 23; Pruned filters: 62
Layer index: 25; Pruned filters: 62
Layer index: 27; Pruned filters: 62
Layer index: 29; Pruned filters: 7
Layer index: 29; Pruned filters: 8
Layer index: 29; Pruned filters: 44
Layer index: 29; Pruned filters: 2
Target (flops): 68.863M
After Pruning | FLOPs: 16.254M | #Params: 0.027M
2.072949737429516
After Growth | FLOPs: 68.915M | #Params: 0.112M
I: 1
flops: 68915424
Before Pruning | FLOPs: 68.915M | #Params: 0.112M
Epoch 0
Train | Batch (1/196) | Top-1: 21.48 (55/256)
Train | Batch (101/196) | Top-1: 17.01 (4399/25856)
Train | Batch (196/196) | Top-1: 17.77 (8885/50000)
Regular: 3.006479024887085
Epoche: 0; regular: 3.006479024887085: flops 68915424
#Filters: 663, #FLOPs: 44.72M | Top-1: 17.42
Epoch 1
Train | Batch (1/196) | Top-1: 22.27 (57/256)
Train | Batch (101/196) | Top-1: 21.93 (5671/25856)
Train | Batch (196/196) | Top-1: 22.54 (11268/50000)
Regular: 1.018241286277771
Epoche: 1; regular: 1.018241286277771: flops 68915424
#Filters: 767, #FLOPs: 63.78M | Top-1: 13.68
Epoch 2
Train | Batch (1/196) | Top-1: 22.66 (58/256)
Train | Batch (101/196) | Top-1: 23.89 (6176/25856)
Train | Batch (196/196) | Top-1: 24.42 (12210/50000)
Regular: 0.9486162662506104
Epoche: 2; regular: 0.9486162662506104: flops 68915424
#Filters: 763, #FLOPs: 63.29M | Top-1: 14.15
Epoch 3
Train | Batch (1/196) | Top-1: 29.30 (75/256)
Train | Batch (101/196) | Top-1: 26.12 (6753/25856)
Train | Batch (196/196) | Top-1: 26.67 (13334/50000)
Regular: 0.8997848033905029
Epoche: 3; regular: 0.8997848033905029: flops 68915424
#Filters: 752, #FLOPs: 65.36M | Top-1: 17.71
Epoch 4
Train | Batch (1/196) | Top-1: 26.17 (67/256)
Train | Batch (101/196) | Top-1: 24.66 (6375/25856)
Train | Batch (196/196) | Top-1: 26.34 (13170/50000)
Regular: 1.005030870437622
Epoche: 4; regular: 1.005030870437622: flops 68915424
#Filters: 628, #FLOPs: 63.70M | Top-1: 10.70
Epoch 5
Train | Batch (1/196) | Top-1: 24.22 (62/256)
Train | Batch (101/196) | Top-1: 22.86 (5911/25856)
Train | Batch (196/196) | Top-1: 23.99 (11993/50000)
Regular: 0.9102280139923096
Epoche: 5; regular: 0.9102280139923096: flops 68915424
#Filters: 624, #FLOPs: 63.48M | Top-1: 14.60
Epoch 6
Train | Batch (1/196) | Top-1: 26.17 (67/256)
Train | Batch (101/196) | Top-1: 25.58 (6614/25856)
Train | Batch (196/196) | Top-1: 26.17 (13087/50000)
Regular: 0.8571861386299133
Epoche: 6; regular: 0.8571861386299133: flops 68915424
#Filters: 601, #FLOPs: 61.28M | Top-1: 20.66
Epoch 7
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 26.60 (6877/25856)
Train | Batch (196/196) | Top-1: 26.65 (13327/50000)
Regular: 0.8558789491653442
Epoche: 7; regular: 0.8558789491653442: flops 68915424
#Filters: 605, #FLOPs: 61.25M | Top-1: 24.78
Epoch 8
Train | Batch (1/196) | Top-1: 25.78 (66/256)
Train | Batch (101/196) | Top-1: 27.12 (7012/25856)
Train | Batch (196/196) | Top-1: 27.41 (13705/50000)
Regular: 0.8622260689735413
Epoche: 8; regular: 0.8622260689735413: flops 68915424
#Filters: 625, #FLOPs: 63.45M | Top-1: 18.40
Epoch 9
Train | Batch (1/196) | Top-1: 23.83 (61/256)
Train | Batch (101/196) | Top-1: 26.81 (6932/25856)
Train | Batch (196/196) | Top-1: 27.16 (13578/50000)
Regular: 1.2142109870910645
Epoche: 9; regular: 1.2142109870910645: flops 68915424
#Filters: 601, #FLOPs: 60.57M | Top-1: 13.17
Epoch 10
Train | Batch (1/196) | Top-1: 25.00 (64/256)
Train | Batch (101/196) | Top-1: 28.05 (7253/25856)
Train | Batch (196/196) | Top-1: 28.39 (14194/50000)
Regular: 0.9100443124771118
Epoche: 10; regular: 0.9100443124771118: flops 68915424
#Filters: 599, #FLOPs: 61.17M | Top-1: 10.41
Epoch 11
Train | Batch (1/196) | Top-1: 25.39 (65/256)
Train | Batch (101/196) | Top-1: 27.82 (7193/25856)
Train | Batch (196/196) | Top-1: 28.10 (14050/50000)
Regular: 0.9694717526435852
Epoche: 11; regular: 0.9694717526435852: flops 68915424
#Filters: 629, #FLOPs: 63.67M | Top-1: 18.75
Epoch 12
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 28.50 (7368/25856)
Train | Batch (196/196) | Top-1: 29.03 (14516/50000)
Regular: 0.9022678732872009
Epoche: 12; regular: 0.9022678732872009: flops 68915424
#Filters: 611, #FLOPs: 62.64M | Top-1: 11.15
Epoch 13
Train | Batch (1/196) | Top-1: 30.47 (78/256)
Train | Batch (101/196) | Top-1: 27.79 (7186/25856)
Train | Batch (196/196) | Top-1: 28.72 (14361/50000)
Regular: 0.8830474615097046
Epoche: 13; regular: 0.8830474615097046: flops 68915424
#Filters: 631, #FLOPs: 65.23M | Top-1: 23.07
Epoch 14
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 22.52 (5822/25856)
Train | Batch (196/196) | Top-1: 25.11 (12556/50000)
Regular: 9.203866004943848
Epoche: 14; regular: 9.203866004943848: flops 68915424
#Filters: 606, #FLOPs: 61.37M | Top-1: 22.82
Epoch 15
Train | Batch (1/196) | Top-1: 26.17 (67/256)
Train | Batch (101/196) | Top-1: 29.00 (7497/25856)
Train | Batch (196/196) | Top-1: 29.21 (14605/50000)
Regular: 6.328812599182129
Epoche: 15; regular: 6.328812599182129: flops 68915424
#Filters: 600, #FLOPs: 60.46M | Top-1: 10.00
Epoch 16
Train | Batch (1/196) | Top-1: 24.61 (63/256)
Train | Batch (101/196) | Top-1: 29.92 (7736/25856)
Train | Batch (196/196) | Top-1: 29.88 (14938/50000)
Regular: 4.477097988128662
Epoche: 16; regular: 4.477097988128662: flops 68915424
#Filters: 621, #FLOPs: 62.85M | Top-1: 24.05
Epoch 17
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 30.45 (7873/25856)
Train | Batch (196/196) | Top-1: 30.37 (15185/50000)
Regular: 2.8122150897979736
Epoche: 17; regular: 2.8122150897979736: flops 68915424
#Filters: 606, #FLOPs: 61.77M | Top-1: 9.53
Epoch 18
Train | Batch (1/196) | Top-1: 25.39 (65/256)
Train | Batch (101/196) | Top-1: 27.44 (7094/25856)
Train | Batch (196/196) | Top-1: 29.15 (14576/50000)
Regular: 5.675146579742432
Epoche: 18; regular: 5.675146579742432: flops 68915424
#Filters: 624, #FLOPs: 64.01M | Top-1: 22.45
Epoch 19
Train | Batch (1/196) | Top-1: 31.25 (80/256)
Train | Batch (101/196) | Top-1: 28.26 (7307/25856)
Train | Batch (196/196) | Top-1: 29.39 (14693/50000)
Regular: 1.1493622064590454
Epoche: 19; regular: 1.1493622064590454: flops 68915424
#Filters: 602, #FLOPs: 60.91M | Top-1: 10.01
Epoch 20
Train | Batch (1/196) | Top-1: 28.12 (72/256)
Train | Batch (101/196) | Top-1: 31.24 (8078/25856)
Train | Batch (196/196) | Top-1: 30.96 (15481/50000)
Regular: 0.9148412942886353
Epoche: 20; regular: 0.9148412942886353: flops 68915424
#Filters: 607, #FLOPs: 61.72M | Top-1: 18.10
Epoch 21
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 31.11 (8045/25856)
Train | Batch (196/196) | Top-1: 30.87 (15433/50000)
Regular: 1.1883093118667603
Epoche: 21; regular: 1.1883093118667603: flops 68915424
#Filters: 625, #FLOPs: 63.75M | Top-1: 20.61
Epoch 22
Train | Batch (1/196) | Top-1: 26.56 (68/256)
Train | Batch (101/196) | Top-1: 30.67 (7929/25856)
Train | Batch (196/196) | Top-1: 31.14 (15572/50000)
Regular: 0.9618433713912964
Epoche: 22; regular: 0.9618433713912964: flops 68915424
#Filters: 599, #FLOPs: 61.59M | Top-1: 11.63
Epoch 23
Train | Batch (1/196) | Top-1: 30.47 (78/256)
Train | Batch (101/196) | Top-1: 31.59 (8168/25856)
Train | Batch (196/196) | Top-1: 31.53 (15766/50000)
Regular: 1.0894343852996826
Epoche: 23; regular: 1.0894343852996826: flops 68915424
#Filters: 630, #FLOPs: 64.55M | Top-1: 14.44
Epoch 24
Train | Batch (1/196) | Top-1: 26.95 (69/256)
Train | Batch (101/196) | Top-1: 31.79 (8219/25856)
Train | Batch (196/196) | Top-1: 31.82 (15908/50000)
Regular: 0.8949022889137268
Epoche: 24; regular: 0.8949022889137268: flops 68915424
#Filters: 603, #FLOPs: 61.80M | Top-1: 11.93
Epoch 25
Train | Batch (1/196) | Top-1: 29.69 (76/256)
Train | Batch (101/196) | Top-1: 31.76 (8211/25856)
Train | Batch (196/196) | Top-1: 31.82 (15910/50000)
Regular: 0.9086968302726746
Epoche: 25; regular: 0.9086968302726746: flops 68915424
#Filters: 607, #FLOPs: 62.42M | Top-1: 20.36
Epoch 26
Train | Batch (1/196) | Top-1: 33.98 (87/256)
Train | Batch (101/196) | Top-1: 31.46 (8134/25856)
Train | Batch (196/196) | Top-1: 31.72 (15860/50000)
Regular: 1.0098036527633667
Epoche: 26; regular: 1.0098036527633667: flops 68915424
#Filters: 625, #FLOPs: 63.40M | Top-1: 25.74
Epoch 27
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 32.11 (8303/25856)
Train | Batch (196/196) | Top-1: 32.26 (16129/50000)
Regular: 0.9195349812507629
Epoche: 27; regular: 0.9195349812507629: flops 68915424
#Filters: 607, #FLOPs: 62.25M | Top-1: 10.36
Epoch 28
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 32.07 (8292/25856)
Train | Batch (196/196) | Top-1: 32.21 (16104/50000)
Regular: 0.901463508605957
Epoche: 28; regular: 0.901463508605957: flops 68915424
#Filters: 625, #FLOPs: 64.11M | Top-1: 10.03
Epoch 29
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 32.62 (8433/25856)
Train | Batch (196/196) | Top-1: 31.28 (15642/50000)
Regular: 1.0297561883926392
Epoche: 29; regular: 1.0297561883926392: flops 68915424
#Filters: 610, #FLOPs: 61.75M | Top-1: 10.27
Drin!!
Layers that will be prunned: [(1, 1), (3, 2), (7, 1), (9, 3), (12, 13), (13, 8), (14, 13), (15, 12), (16, 13), (17, 9), (18, 13), (19, 1), (20, 13), (21, 1), (22, 15), (23, 2), (24, 15), (25, 2), (26, 15), (27, 2), (28, 15), (29, 3), (30, 15)]
Prunning filters..
Layer index: 12; Pruned filters: 13
Layer index: 14; Pruned filters: 13
Layer index: 16; Pruned filters: 13
Layer index: 18; Pruned filters: 13
Layer index: 20; Pruned filters: 13
Layer index: 22; Pruned filters: 15
Layer index: 24; Pruned filters: 15
Layer index: 26; Pruned filters: 15
Layer index: 28; Pruned filters: 15
Layer index: 30; Pruned filters: 15
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 2
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 2
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 7
Layer index: 15; Pruned filters: 2
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 9
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 2
Layer index: 17; Pruned filters: 6
Layer index: 19; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 25; Pruned filters: 2
Layer index: 27; Pruned filters: 2
Layer index: 29; Pruned filters: 3
Target (flops): 68.863M
After Pruning | FLOPs: 56.796M | #Params: 0.069M
1.101932387926491
After Growth | FLOPs: 68.097M | #Params: 0.082M
I: 2
flops: 68097384
Before Pruning | FLOPs: 68.097M | #Params: 0.082M
Epoch 0
Train | Batch (1/196) | Top-1: 15.62 (40/256)
Train | Batch (101/196) | Top-1: 19.18 (4960/25856)
Train | Batch (196/196) | Top-1: 20.17 (10083/50000)
Regular: 5.7106614112854
Epoche: 0; regular: 5.7106614112854: flops 68097384
#Filters: 581, #FLOPs: 46.24M | Top-1: 10.00
Epoch 1
Train | Batch (1/196) | Top-1: 25.39 (65/256)
Train | Batch (101/196) | Top-1: 16.14 (4173/25856)
Train | Batch (196/196) | Top-1: 19.08 (9539/50000)
Regular: 2.029085159301758
Epoche: 1; regular: 2.029085159301758: flops 68097384
#Filters: 641, #FLOPs: 62.15M | Top-1: 11.00
Epoch 2
Train | Batch (1/196) | Top-1: 25.00 (64/256)
Train | Batch (101/196) | Top-1: 24.52 (6340/25856)
Train | Batch (196/196) | Top-1: 24.73 (12367/50000)
Regular: 1.1142675876617432
Epoche: 2; regular: 1.1142675876617432: flops 68097384
#Filters: 650, #FLOPs: 65.13M | Top-1: 22.90
Epoch 3
Train | Batch (1/196) | Top-1: 23.05 (59/256)
Train | Batch (101/196) | Top-1: 26.31 (6803/25856)
Train | Batch (196/196) | Top-1: 26.20 (13102/50000)
Regular: 1.07846999168396
Epoche: 3; regular: 1.07846999168396: flops 68097384
#Filters: 637, #FLOPs: 65.31M | Top-1: 11.09
Epoch 4
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 28.08 (7261/25856)
Train | Batch (196/196) | Top-1: 28.30 (14148/50000)
Regular: 1.0965505838394165
Epoche: 4; regular: 1.0965505838394165: flops 68097384
#Filters: 654, #FLOPs: 66.52M | Top-1: 10.00
Epoch 5
Train | Batch (1/196) | Top-1: 20.70 (53/256)
Train | Batch (101/196) | Top-1: 28.71 (7424/25856)
Train | Batch (196/196) | Top-1: 28.87 (14433/50000)
Regular: 1.3245437145233154
Epoche: 5; regular: 1.3245437145233154: flops 68097384
#Filters: 656, #FLOPs: 66.94M | Top-1: 10.82
Epoch 6
Train | Batch (1/196) | Top-1: 30.47 (78/256)
Train | Batch (101/196) | Top-1: 27.83 (7197/25856)
Train | Batch (196/196) | Top-1: 28.81 (14406/50000)
Regular: 1.1570026874542236
Epoche: 6; regular: 1.1570026874542236: flops 68097384
#Filters: 645, #FLOPs: 64.95M | Top-1: 9.99
Epoch 7
Train | Batch (1/196) | Top-1: 25.78 (66/256)
Train | Batch (101/196) | Top-1: 26.45 (6839/25856)
Train | Batch (196/196) | Top-1: 27.56 (13780/50000)
Regular: 1.2606600522994995
Epoche: 7; regular: 1.2606600522994995: flops 68097384
#Filters: 647, #FLOPs: 65.44M | Top-1: 10.47
Epoch 8
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 29.35 (7588/25856)
Train | Batch (196/196) | Top-1: 30.10 (15050/50000)
Regular: 1.6735764741897583
Epoche: 8; regular: 1.6735764741897583: flops 68097384
#Filters: 655, #FLOPs: 65.94M | Top-1: 13.20
Epoch 9
Train | Batch (1/196) | Top-1: 34.38 (88/256)
Train | Batch (101/196) | Top-1: 30.89 (7987/25856)
Train | Batch (196/196) | Top-1: 31.23 (15614/50000)
Regular: 1.1400775909423828
Epoche: 9; regular: 1.1400775909423828: flops 68097384
#Filters: 638, #FLOPs: 62.87M | Top-1: 10.01
Epoch 10
Train | Batch (1/196) | Top-1: 28.52 (73/256)
Train | Batch (101/196) | Top-1: 31.56 (8161/25856)
Train | Batch (196/196) | Top-1: 27.22 (13611/50000)
Regular: 2.2815864086151123
Epoche: 10; regular: 2.2815864086151123: flops 68097384
#Filters: 646, #FLOPs: 66.25M | Top-1: 10.03
Epoch 11
Train | Batch (1/196) | Top-1: 17.19 (44/256)
Train | Batch (101/196) | Top-1: 24.11 (6235/25856)
Train | Batch (196/196) | Top-1: 25.16 (12578/50000)
Regular: 2.777834892272949
Epoche: 11; regular: 2.777834892272949: flops 68097384
#Filters: 653, #FLOPs: 65.77M | Top-1: 10.05
Epoch 12
Train | Batch (1/196) | Top-1: 28.12 (72/256)
Train | Batch (101/196) | Top-1: 28.17 (7284/25856)
Train | Batch (196/196) | Top-1: 29.01 (14507/50000)
Regular: 1.374233603477478
Epoche: 12; regular: 1.374233603477478: flops 68097384
#Filters: 654, #FLOPs: 66.11M | Top-1: 16.70
Epoch 13
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 30.00 (7756/25856)
Train | Batch (196/196) | Top-1: 30.33 (15166/50000)
Regular: 1.5483307838439941
Epoche: 13; regular: 1.5483307838439941: flops 68097384
#Filters: 648, #FLOPs: 64.95M | Top-1: 14.26
Epoch 14
Train | Batch (1/196) | Top-1: 32.03 (82/256)
Train | Batch (101/196) | Top-1: 31.27 (8086/25856)
Train | Batch (196/196) | Top-1: 31.14 (15569/50000)
Regular: 1.1503300666809082
Epoche: 14; regular: 1.1503300666809082: flops 68097384
#Filters: 650, #FLOPs: 65.28M | Top-1: 13.71
Epoch 15
Train | Batch (1/196) | Top-1: 30.08 (77/256)
Train | Batch (101/196) | Top-1: 31.79 (8219/25856)
Train | Batch (196/196) | Top-1: 32.47 (16234/50000)
Regular: 1.2084858417510986
Epoche: 15; regular: 1.2084858417510986: flops 68097384
#Filters: 647, #FLOPs: 65.03M | Top-1: 21.55
Epoch 16
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 31.74 (8207/25856)
Train | Batch (196/196) | Top-1: 32.49 (16247/50000)
Regular: 1.2514550685882568
Epoche: 16; regular: 1.2514550685882568: flops 68097384
#Filters: 646, #FLOPs: 64.28M | Top-1: 32.14
Epoch 17
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 32.58 (8423/25856)
Train | Batch (196/196) | Top-1: 32.57 (16285/50000)
Regular: 1.1200289726257324
Epoche: 17; regular: 1.1200289726257324: flops 68097384
#Filters: 655, #FLOPs: 66.19M | Top-1: 11.74
Epoch 18
Train | Batch (1/196) | Top-1: 26.56 (68/256)
Train | Batch (101/196) | Top-1: 33.36 (8626/25856)
Train | Batch (196/196) | Top-1: 33.35 (16676/50000)
Regular: 1.069481611251831
Epoche: 18; regular: 1.069481611251831: flops 68097384
#Filters: 654, #FLOPs: 65.77M | Top-1: 15.82
Epoch 19
Train | Batch (1/196) | Top-1: 18.75 (48/256)
Train | Batch (101/196) | Top-1: 31.71 (8200/25856)
Train | Batch (196/196) | Top-1: 32.84 (16420/50000)
Regular: 1.1243659257888794
Epoche: 19; regular: 1.1243659257888794: flops 68097384
#Filters: 648, #FLOPs: 66.02M | Top-1: 10.38
Epoch 20
Train | Batch (1/196) | Top-1: 33.98 (87/256)
Train | Batch (101/196) | Top-1: 33.71 (8715/25856)
Train | Batch (196/196) | Top-1: 34.27 (17134/50000)
Regular: 1.2088693380355835
Epoche: 20; regular: 1.2088693380355835: flops 68097384
#Filters: 649, #FLOPs: 64.53M | Top-1: 32.22
Epoch 21
Train | Batch (1/196) | Top-1: 40.23 (103/256)
Train | Batch (101/196) | Top-1: 32.31 (8354/25856)
Train | Batch (196/196) | Top-1: 33.38 (16691/50000)
Regular: 1.3984854221343994
Epoche: 21; regular: 1.3984854221343994: flops 68097384
#Filters: 649, #FLOPs: 65.69M | Top-1: 15.61
Epoch 22
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 35.74 (9241/25856)
Train | Batch (196/196) | Top-1: 35.41 (17704/50000)
Regular: 1.6721917390823364
Epoche: 22; regular: 1.6721917390823364: flops 68097384
#Filters: 653, #FLOPs: 66.36M | Top-1: 24.98
Epoch 23
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 35.05 (9063/25856)
Train | Batch (196/196) | Top-1: 35.86 (17930/50000)
Regular: 2.106534957885742
Epoche: 23; regular: 2.106534957885742: flops 68097384
#Filters: 655, #FLOPs: 66.27M | Top-1: 17.46
Epoch 24
Train | Batch (1/196) | Top-1: 34.77 (89/256)
Train | Batch (101/196) | Top-1: 36.26 (9375/25856)
Train | Batch (196/196) | Top-1: 36.51 (18253/50000)
Regular: 1.3053444623947144
Epoche: 24; regular: 1.3053444623947144: flops 68097384
#Filters: 649, #FLOPs: 65.19M | Top-1: 17.56
Epoch 25
Train | Batch (1/196) | Top-1: 32.81 (84/256)
Train | Batch (101/196) | Top-1: 33.37 (8627/25856)
Train | Batch (196/196) | Top-1: 34.24 (17119/50000)
Regular: 1.725955843925476
Epoche: 25; regular: 1.725955843925476: flops 68097384
#Filters: 650, #FLOPs: 65.19M | Top-1: 10.00
Epoch 26
Train | Batch (1/196) | Top-1: 37.11 (95/256)
Train | Batch (101/196) | Top-1: 35.73 (9239/25856)
Train | Batch (196/196) | Top-1: 35.96 (17978/50000)
Regular: 1.1928775310516357
Epoche: 26; regular: 1.1928775310516357: flops 68097384
#Filters: 656, #FLOPs: 65.61M | Top-1: 26.74
Epoch 27
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 37.05 (9579/25856)
Train | Batch (196/196) | Top-1: 36.51 (18255/50000)
Regular: 1.4854798316955566
Epoche: 27; regular: 1.4854798316955566: flops 68097384
#Filters: 655, #FLOPs: 66.52M | Top-1: 9.98
Epoch 28
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 37.87 (9791/25856)
Train | Batch (196/196) | Top-1: 37.82 (18909/50000)
Regular: 1.203898549079895
Epoche: 28; regular: 1.203898549079895: flops 68097384
#Filters: 653, #FLOPs: 65.94M | Top-1: 28.19
Epoch 29
Train | Batch (1/196) | Top-1: 30.08 (77/256)
Train | Batch (101/196) | Top-1: 37.56 (9711/25856)
Train | Batch (196/196) | Top-1: 37.54 (18768/50000)
Regular: 1.1248793601989746
Epoche: 29; regular: 1.1248793601989746: flops 68097384
#Filters: 653, #FLOPs: 66.11M | Top-1: 13.97
Drin!!
Layers that will be prunned: [(1, 2), (3, 1), (7, 1), (11, 4), (13, 1), (15, 2), (17, 1)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 11; Pruned filters: 3
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 2
Layer index: 17; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 64.116M | #Params: 0.074M
1.036638769872255
After Growth | FLOPs: 68.625M | #Params: 0.079M
I: 3
flops: 68625010
Before Pruning | FLOPs: 68.625M | #Params: 0.079M
Epoch 0
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 27.21 (7036/25856)
Train | Batch (196/196) | Top-1: 30.17 (15085/50000)
Regular: 1.2072418928146362
Epoche: 0; regular: 1.2072418928146362: flops 68625010
#Filters: 423, #FLOPs: 47.52M | Top-1: 27.81
Epoch 1
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 34.80 (8998/25856)
Train | Batch (196/196) | Top-1: 34.81 (17406/50000)
Regular: 1.2586438655853271
Epoche: 1; regular: 1.2586438655853271: flops 68625010
#Filters: 605, #FLOPs: 50.72M | Top-1: 10.00
Epoch 2
Train | Batch (1/196) | Top-1: 30.86 (79/256)
Train | Batch (101/196) | Top-1: 34.90 (9023/25856)
Train | Batch (196/196) | Top-1: 35.24 (17618/50000)
Regular: 1.5374125242233276
Epoche: 2; regular: 1.5374125242233276: flops 68625010
#Filters: 663, #FLOPs: 66.66M | Top-1: 10.00
Epoch 3
Train | Batch (1/196) | Top-1: 33.98 (87/256)
Train | Batch (101/196) | Top-1: 35.81 (9258/25856)
Train | Batch (196/196) | Top-1: 35.72 (17861/50000)
Regular: 2.0367026329040527
Epoche: 3; regular: 2.0367026329040527: flops 68625010
#Filters: 657, #FLOPs: 64.62M | Top-1: 12.71
Epoch 4
Train | Batch (1/196) | Top-1: 27.34 (70/256)
Train | Batch (101/196) | Top-1: 36.77 (9508/25856)
Train | Batch (196/196) | Top-1: 37.29 (18644/50000)
Regular: 2.392244338989258
Epoche: 4; regular: 2.392244338989258: flops 68625010
#Filters: 664, #FLOPs: 66.07M | Top-1: 23.12
Epoch 5
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 36.89 (9539/25856)
Train | Batch (196/196) | Top-1: 37.72 (18861/50000)
Regular: 1.408058762550354
Epoche: 5; regular: 1.408058762550354: flops 68625010
#Filters: 662, #FLOPs: 65.90M | Top-1: 15.21
Epoch 6
Train | Batch (1/196) | Top-1: 37.11 (95/256)
Train | Batch (101/196) | Top-1: 37.85 (9787/25856)
Train | Batch (196/196) | Top-1: 38.36 (19180/50000)
Regular: 1.8514108657836914
Epoche: 6; regular: 1.8514108657836914: flops 68625010
#Filters: 663, #FLOPs: 66.15M | Top-1: 30.37
Epoch 7
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 38.06 (9842/25856)
Train | Batch (196/196) | Top-1: 38.48 (19239/50000)
Regular: 1.3560723066329956
Epoche: 7; regular: 1.3560723066329956: flops 68625010
#Filters: 663, #FLOPs: 65.98M | Top-1: 10.90
Epoch 8
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 38.69 (10004/25856)
Train | Batch (196/196) | Top-1: 38.43 (19216/50000)
Regular: 1.3773701190948486
Epoche: 8; regular: 1.3773701190948486: flops 68625010
#Filters: 661, #FLOPs: 65.22M | Top-1: 11.05
Epoch 9
Train | Batch (1/196) | Top-1: 36.72 (94/256)
Train | Batch (101/196) | Top-1: 33.80 (8740/25856)
Train | Batch (196/196) | Top-1: 34.93 (17463/50000)
Regular: 6.344869136810303
Epoche: 9; regular: 6.344869136810303: flops 68625010
#Filters: 665, #FLOPs: 66.24M | Top-1: 25.31
Epoch 10
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 38.29 (9900/25856)
Train | Batch (196/196) | Top-1: 38.38 (19188/50000)
Regular: 4.544253349304199
Epoche: 10; regular: 4.544253349304199: flops 68625010
#Filters: 664, #FLOPs: 66.32M | Top-1: 11.47
Epoch 11
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 38.52 (9959/25856)
Train | Batch (196/196) | Top-1: 38.62 (19310/50000)
Regular: 2.961000442504883
Epoche: 11; regular: 2.961000442504883: flops 68625010
#Filters: 661, #FLOPs: 65.56M | Top-1: 20.26
Epoch 12
Train | Batch (1/196) | Top-1: 43.75 (112/256)
Train | Batch (101/196) | Top-1: 37.91 (9802/25856)
Train | Batch (196/196) | Top-1: 37.97 (18986/50000)
Regular: 2.168792963027954
Epoche: 12; regular: 2.168792963027954: flops 68625010
#Filters: 665, #FLOPs: 66.32M | Top-1: 28.38
Epoch 13
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 39.53 (10221/25856)
Train | Batch (196/196) | Top-1: 39.10 (19551/50000)
Regular: 1.9163267612457275
Epoche: 13; regular: 1.9163267612457275: flops 68625010
#Filters: 663, #FLOPs: 65.64M | Top-1: 15.25
Epoch 14
Train | Batch (1/196) | Top-1: 33.59 (86/256)
Train | Batch (101/196) | Top-1: 39.24 (10145/25856)
Train | Batch (196/196) | Top-1: 39.13 (19566/50000)
Regular: 2.375727415084839
Epoche: 14; regular: 2.375727415084839: flops 68625010
#Filters: 671, #FLOPs: 68.03M | Top-1: 11.78
Epoch 15
Train | Batch (1/196) | Top-1: 36.72 (94/256)
Train | Batch (101/196) | Top-1: 39.49 (10211/25856)
Train | Batch (196/196) | Top-1: 38.88 (19439/50000)
Regular: 1.4771490097045898
Epoche: 15; regular: 1.4771490097045898: flops 68625010
#Filters: 666, #FLOPs: 66.58M | Top-1: 8.82
Epoch 16
Train | Batch (1/196) | Top-1: 37.89 (97/256)
Train | Batch (101/196) | Top-1: 38.06 (9841/25856)
Train | Batch (196/196) | Top-1: 37.47 (18735/50000)
Regular: 1.4095803499221802
Epoche: 16; regular: 1.4095803499221802: flops 68625010
#Filters: 663, #FLOPs: 65.98M | Top-1: 23.09
Epoch 17
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 38.87 (10050/25856)
Train | Batch (196/196) | Top-1: 39.33 (19663/50000)
Regular: 2.2531018257141113
Epoche: 17; regular: 2.2531018257141113: flops 68625010
#Filters: 668, #FLOPs: 67.09M | Top-1: 14.90
Epoch 18
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 39.57 (10231/25856)
Train | Batch (196/196) | Top-1: 39.41 (19706/50000)
Regular: 1.3708735704421997
Epoche: 18; regular: 1.3708735704421997: flops 68625010
#Filters: 668, #FLOPs: 67.43M | Top-1: 24.58
Epoch 19
Train | Batch (1/196) | Top-1: 42.97 (110/256)
Train | Batch (101/196) | Top-1: 28.99 (7495/25856)
Train | Batch (196/196) | Top-1: 28.09 (14046/50000)
Regular: 1.7957561016082764
Epoche: 19; regular: 1.7957561016082764: flops 68625010
#Filters: 667, #FLOPs: 67.24M | Top-1: 12.75
Epoch 20
Train | Batch (1/196) | Top-1: 27.34 (70/256)
Train | Batch (101/196) | Top-1: 33.44 (8645/25856)
Train | Batch (196/196) | Top-1: 34.61 (17303/50000)
Regular: 1.4137318134307861
Epoche: 20; regular: 1.4137318134307861: flops 68625010
#Filters: 667, #FLOPs: 67.26M | Top-1: 14.24
Epoch 21
Train | Batch (1/196) | Top-1: 34.77 (89/256)
Train | Batch (101/196) | Top-1: 37.01 (9570/25856)
Train | Batch (196/196) | Top-1: 37.70 (18849/50000)
Regular: 1.1822811365127563
Epoche: 21; regular: 1.1822811365127563: flops 68625010
#Filters: 666, #FLOPs: 65.98M | Top-1: 10.74
Epoch 22
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 38.39 (9925/25856)
Train | Batch (196/196) | Top-1: 38.82 (19411/50000)
Regular: 1.2929141521453857
Epoche: 22; regular: 1.2929141521453857: flops 68625010
#Filters: 667, #FLOPs: 66.92M | Top-1: 15.56
Epoch 23
Train | Batch (1/196) | Top-1: 39.84 (102/256)
Train | Batch (101/196) | Top-1: 39.50 (10213/25856)
Train | Batch (196/196) | Top-1: 39.61 (19803/50000)
Regular: 1.315178394317627
Epoche: 23; regular: 1.315178394317627: flops 68625010
#Filters: 669, #FLOPs: 66.75M | Top-1: 29.97
Epoch 24
Train | Batch (1/196) | Top-1: 35.16 (90/256)
Train | Batch (101/196) | Top-1: 39.82 (10296/25856)
Train | Batch (196/196) | Top-1: 39.92 (19959/50000)
Regular: 1.287357211112976
Epoche: 24; regular: 1.287357211112976: flops 68625010
#Filters: 668, #FLOPs: 66.66M | Top-1: 15.51
Epoch 25
Train | Batch (1/196) | Top-1: 33.98 (87/256)
Train | Batch (101/196) | Top-1: 39.60 (10239/25856)
Train | Batch (196/196) | Top-1: 40.16 (20080/50000)
Regular: 1.173725962638855
Epoche: 25; regular: 1.173725962638855: flops 68625010
#Filters: 671, #FLOPs: 67.94M | Top-1: 10.02
Epoch 26
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 36.20 (9360/25856)
Train | Batch (196/196) | Top-1: 37.58 (18789/50000)
Regular: 1.7250854969024658
Epoche: 26; regular: 1.7250854969024658: flops 68625010
#Filters: 665, #FLOPs: 65.47M | Top-1: 10.00
Epoch 27
Train | Batch (1/196) | Top-1: 42.58 (109/256)
Train | Batch (101/196) | Top-1: 39.91 (10319/25856)
Train | Batch (196/196) | Top-1: 36.93 (18467/50000)
Regular: 2.8135769367218018
Epoche: 27; regular: 2.8135769367218018: flops 68625010
#Filters: 669, #FLOPs: 67.01M | Top-1: 9.78
Epoch 28
Train | Batch (1/196) | Top-1: 32.42 (83/256)
Train | Batch (101/196) | Top-1: 34.03 (8800/25856)
Train | Batch (196/196) | Top-1: 32.97 (16486/50000)
Regular: 1.8171850442886353
Epoche: 28; regular: 1.8171850442886353: flops 68625010
#Filters: 668, #FLOPs: 66.64M | Top-1: 28.17
Epoch 29
Train | Batch (1/196) | Top-1: 30.47 (78/256)
Train | Batch (101/196) | Top-1: 34.78 (8994/25856)
Train | Batch (196/196) | Top-1: 35.60 (17798/50000)
Regular: 1.2986159324645996
Epoche: 29; regular: 1.2986159324645996: flops 68625010
#Filters: 664, #FLOPs: 66.73M | Top-1: 11.43
Drin!!
Layers that will be prunned: [(1, 1), (5, 3), (9, 1), (11, 3), (29, 1)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 5; Pruned filters: 3
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 2
Layer index: 11; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 64.661M | #Params: 0.073M
1.0322333591322994
After Growth | FLOPs: 69.210M | #Params: 0.077M
I: 4
flops: 69210236
Before Pruning | FLOPs: 69.210M | #Params: 0.077M
Epoch 0
Train | Batch (1/196) | Top-1: 8.59 (22/256)
Train | Batch (101/196) | Top-1: 14.30 (3698/25856)
Train | Batch (196/196) | Top-1: 18.35 (9176/50000)
Regular: 1.2795286178588867
Epoche: 0; regular: 1.2795286178588867: flops 69210236
#Filters: 481, #FLOPs: 50.15M | Top-1: 10.07
Epoch 1
Train | Batch (1/196) | Top-1: 21.48 (55/256)
Train | Batch (101/196) | Top-1: 27.54 (7120/25856)
Train | Batch (196/196) | Top-1: 28.49 (14245/50000)
Regular: 1.0922832489013672
Epoche: 1; regular: 1.0922832489013672: flops 69210236
#Filters: 616, #FLOPs: 62.10M | Top-1: 11.50
Epoch 2
Train | Batch (1/196) | Top-1: 30.86 (79/256)
Train | Batch (101/196) | Top-1: 30.50 (7886/25856)
Train | Batch (196/196) | Top-1: 30.95 (15475/50000)
Regular: 1.1899049282073975
Epoche: 2; regular: 1.1899049282073975: flops 69210236
#Filters: 670, #FLOPs: 64.04M | Top-1: 11.73
Epoch 3
Train | Batch (1/196) | Top-1: 33.98 (87/256)
Train | Batch (101/196) | Top-1: 30.80 (7963/25856)
Train | Batch (196/196) | Top-1: 31.23 (15617/50000)
Regular: 1.1733945608139038
Epoche: 3; regular: 1.1733945608139038: flops 69210236
#Filters: 682, #FLOPs: 68.25M | Top-1: 16.88
Epoch 4
Train | Batch (1/196) | Top-1: 30.08 (77/256)
Train | Batch (101/196) | Top-1: 33.06 (8548/25856)
Train | Batch (196/196) | Top-1: 32.83 (16416/50000)
Regular: 1.772730827331543
Epoche: 4; regular: 1.772730827331543: flops 69210236
#Filters: 640, #FLOPs: 53.54M | Top-1: 29.72
Epoch 5
Train | Batch (1/196) | Top-1: 34.77 (89/256)
Train | Batch (101/196) | Top-1: 33.82 (8745/25856)
Train | Batch (196/196) | Top-1: 33.85 (16925/50000)
Regular: 1.760340690612793
Epoche: 5; regular: 1.760340690612793: flops 69210236
#Filters: 683, #FLOPs: 68.60M | Top-1: 11.97
Epoch 6
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 33.21 (8587/25856)
Train | Batch (196/196) | Top-1: 33.91 (16953/50000)
Regular: 1.0203516483306885
Epoche: 6; regular: 1.0203516483306885: flops 69210236
#Filters: 676, #FLOPs: 65.80M | Top-1: 20.11
Epoch 7
Train | Batch (1/196) | Top-1: 33.98 (87/256)
Train | Batch (101/196) | Top-1: 35.35 (9139/25856)
Train | Batch (196/196) | Top-1: 35.27 (17637/50000)
Regular: 1.1321289539337158
Epoche: 7; regular: 1.1321289539337158: flops 69210236
#Filters: 638, #FLOPs: 52.40M | Top-1: 10.16
Epoch 8
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 35.93 (9291/25856)
Train | Batch (196/196) | Top-1: 35.38 (17692/50000)
Regular: 1.3870774507522583
Epoche: 8; regular: 1.3870774507522583: flops 69210236
#Filters: 682, #FLOPs: 67.81M | Top-1: 23.59
Epoch 9
Train | Batch (1/196) | Top-1: 27.73 (71/256)
Train | Batch (101/196) | Top-1: 34.19 (8839/25856)
Train | Batch (196/196) | Top-1: 34.54 (17272/50000)
Regular: 1.569350242614746
Epoche: 9; regular: 1.569350242614746: flops 69210236
#Filters: 678, #FLOPs: 67.20M | Top-1: 24.74
Epoch 10
Train | Batch (1/196) | Top-1: 33.59 (86/256)
Train | Batch (101/196) | Top-1: 36.44 (9422/25856)
Train | Batch (196/196) | Top-1: 36.49 (18246/50000)
Regular: 1.0017403364181519
Epoche: 10; regular: 1.0017403364181519: flops 69210236
#Filters: 682, #FLOPs: 67.81M | Top-1: 17.95
Epoch 11
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 36.69 (9487/25856)
Train | Batch (196/196) | Top-1: 36.90 (18450/50000)
Regular: 1.0158687829971313
Epoche: 11; regular: 1.0158687829971313: flops 69210236
#Filters: 681, #FLOPs: 67.90M | Top-1: 25.14
Epoch 12
Train | Batch (1/196) | Top-1: 39.84 (102/256)
Train | Batch (101/196) | Top-1: 36.42 (9417/25856)
Train | Batch (196/196) | Top-1: 36.80 (18402/50000)
Regular: 1.1578583717346191
Epoche: 12; regular: 1.1578583717346191: flops 69210236
#Filters: 637, #FLOPs: 52.49M | Top-1: 19.17
Epoch 13
Train | Batch (1/196) | Top-1: 32.42 (83/256)
Train | Batch (101/196) | Top-1: 35.83 (9265/25856)
Train | Batch (196/196) | Top-1: 36.39 (18193/50000)
Regular: 1.7153533697128296
Epoche: 13; regular: 1.7153533697128296: flops 69210236
#Filters: 680, #FLOPs: 67.55M | Top-1: 16.23
Epoch 14
Train | Batch (1/196) | Top-1: 38.67 (99/256)
Train | Batch (101/196) | Top-1: 36.55 (9450/25856)
Train | Batch (196/196) | Top-1: 36.86 (18428/50000)
Regular: 1.2331511974334717
Epoche: 14; regular: 1.2331511974334717: flops 69210236
#Filters: 639, #FLOPs: 52.75M | Top-1: 10.81
Epoch 15
Train | Batch (1/196) | Top-1: 39.45 (101/256)
Train | Batch (101/196) | Top-1: 37.43 (9677/25856)
Train | Batch (196/196) | Top-1: 37.45 (18723/50000)
Regular: 1.1305034160614014
Epoche: 15; regular: 1.1305034160614014: flops 69210236
#Filters: 682, #FLOPs: 68.16M | Top-1: 27.72
Epoch 16
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 37.72 (9754/25856)
Train | Batch (196/196) | Top-1: 37.90 (18950/50000)
Regular: 1.059661626815796
Epoche: 16; regular: 1.059661626815796: flops 69210236
#Filters: 682, #FLOPs: 67.81M | Top-1: 23.36
Epoch 17
Train | Batch (1/196) | Top-1: 41.02 (105/256)
Train | Batch (101/196) | Top-1: 38.40 (9928/25856)
Train | Batch (196/196) | Top-1: 38.24 (19122/50000)
Regular: 1.2958379983901978
Epoche: 17; regular: 1.2958379983901978: flops 69210236
#Filters: 676, #FLOPs: 66.41M | Top-1: 20.16
Epoch 18
Train | Batch (1/196) | Top-1: 32.81 (84/256)
Train | Batch (101/196) | Top-1: 38.34 (9914/25856)
Train | Batch (196/196) | Top-1: 38.63 (19316/50000)
Regular: 1.0626641511917114
Epoche: 18; regular: 1.0626641511917114: flops 69210236
#Filters: 680, #FLOPs: 67.81M | Top-1: 13.51
Epoch 19
Train | Batch (1/196) | Top-1: 40.62 (104/256)
Train | Batch (101/196) | Top-1: 37.06 (9583/25856)
Train | Batch (196/196) | Top-1: 37.27 (18634/50000)
Regular: 1.0820138454437256
Epoche: 19; regular: 1.0820138454437256: flops 69210236
#Filters: 678, #FLOPs: 67.55M | Top-1: 10.43
Epoch 20
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 37.24 (9628/25856)
Train | Batch (196/196) | Top-1: 37.59 (18795/50000)
Regular: 1.0375049114227295
Epoche: 20; regular: 1.0375049114227295: flops 69210236
#Filters: 676, #FLOPs: 66.85M | Top-1: 26.29
Epoch 21
Train | Batch (1/196) | Top-1: 34.77 (89/256)
Train | Batch (101/196) | Top-1: 38.57 (9973/25856)
Train | Batch (196/196) | Top-1: 39.04 (19519/50000)
Regular: 1.076749563217163
Epoche: 21; regular: 1.076749563217163: flops 69210236
#Filters: 684, #FLOPs: 68.95M | Top-1: 25.04
Epoch 22
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 38.96 (10073/25856)
Train | Batch (196/196) | Top-1: 39.24 (19619/50000)
Regular: 0.9827598929405212
Epoche: 22; regular: 0.9827598929405212: flops 69210236
#Filters: 681, #FLOPs: 68.16M | Top-1: 15.15
Epoch 23
Train | Batch (1/196) | Top-1: 39.45 (101/256)
Train | Batch (101/196) | Top-1: 39.82 (10295/25856)
Train | Batch (196/196) | Top-1: 39.41 (19706/50000)
Regular: 1.1098405122756958
Epoche: 23; regular: 1.1098405122756958: flops 69210236
#Filters: 682, #FLOPs: 68.51M | Top-1: 23.21
Epoch 24
Train | Batch (1/196) | Top-1: 39.06 (100/256)
Train | Batch (101/196) | Top-1: 39.71 (10267/25856)
Train | Batch (196/196) | Top-1: 39.63 (19815/50000)
Regular: 1.1515575647354126
Epoche: 24; regular: 1.1515575647354126: flops 69210236
#Filters: 681, #FLOPs: 67.90M | Top-1: 29.44
Epoch 25
Train | Batch (1/196) | Top-1: 44.14 (113/256)
Train | Batch (101/196) | Top-1: 38.57 (9973/25856)
Train | Batch (196/196) | Top-1: 39.22 (19609/50000)
Regular: 1.0516618490219116
Epoche: 25; regular: 1.0516618490219116: flops 69210236
#Filters: 684, #FLOPs: 68.95M | Top-1: 15.04
Epoch 26
Train | Batch (1/196) | Top-1: 41.41 (106/256)
Train | Batch (101/196) | Top-1: 39.87 (10310/25856)
Train | Batch (196/196) | Top-1: 40.17 (20087/50000)
Regular: 1.0002903938293457
Epoche: 26; regular: 1.0002903938293457: flops 69210236
#Filters: 679, #FLOPs: 67.46M | Top-1: 17.81
Epoch 27
Train | Batch (1/196) | Top-1: 35.16 (90/256)
Train | Batch (101/196) | Top-1: 39.22 (10141/25856)
Train | Batch (196/196) | Top-1: 39.67 (19836/50000)
Regular: 2.3905229568481445
Epoche: 27; regular: 2.3905229568481445: flops 69210236
#Filters: 684, #FLOPs: 68.95M | Top-1: 16.83
Epoch 28
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 39.77 (10284/25856)
Train | Batch (196/196) | Top-1: 40.00 (20000/50000)
Regular: 1.2438911199569702
Epoche: 28; regular: 1.2438911199569702: flops 69210236
#Filters: 682, #FLOPs: 68.16M | Top-1: 10.39
Epoch 29
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 39.58 (10234/25856)
Train | Batch (196/196) | Top-1: 39.50 (19752/50000)
Regular: 0.9772742390632629
Epoche: 29; regular: 0.9772742390632629: flops 69210236
#Filters: 679, #FLOPs: 67.20M | Top-1: 21.90
Drin!!
Layers that will be prunned: [(1, 2), (7, 3), (9, 1)]
Prunning filters..
Layer index: 1; Pruned filters: 2
Layer index: 7; Pruned filters: 3
Layer index: 9; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 65.008M | #Params: 0.073M
1.0294603844045336
After Growth | FLOPs: 69.594M | #Params: 0.078M
I: 5
flops: 69593862
Before Pruning | FLOPs: 69.594M | #Params: 0.078M
Epoch 0
Train | Batch (1/196) | Top-1: 6.64 (17/256)
Train | Batch (101/196) | Top-1: 17.68 (4571/25856)
Train | Batch (196/196) | Top-1: 19.01 (9505/50000)
Regular: 1.4705625772476196
Epoche: 0; regular: 1.4705625772476196: flops 69593862
#Filters: 670, #FLOPs: 63.75M | Top-1: 15.33
Epoch 1
Train | Batch (1/196) | Top-1: 16.02 (41/256)
Train | Batch (101/196) | Top-1: 21.68 (5606/25856)
Train | Batch (196/196) | Top-1: 22.85 (11425/50000)
Regular: 1.3017884492874146
Epoche: 1; regular: 1.3017884492874146: flops 69593862
#Filters: 606, #FLOPs: 44.24M | Top-1: 26.23
Epoch 2
Train | Batch (1/196) | Top-1: 27.73 (71/256)
Train | Batch (101/196) | Top-1: 25.73 (6654/25856)
Train | Batch (196/196) | Top-1: 25.50 (12750/50000)
Regular: 1.2461282014846802
Epoche: 2; regular: 1.2461282014846802: flops 69593862
#Filters: 685, #FLOPs: 65.10M | Top-1: 19.34
Epoch 3
Train | Batch (1/196) | Top-1: 26.17 (67/256)
Train | Batch (101/196) | Top-1: 24.28 (6278/25856)
Train | Batch (196/196) | Top-1: 25.71 (12853/50000)
Regular: 1.1474273204803467
Epoche: 3; regular: 1.1474273204803467: flops 69593862
#Filters: 695, #FLOPs: 68.70M | Top-1: 17.10
Epoch 4
Train | Batch (1/196) | Top-1: 23.44 (60/256)
Train | Batch (101/196) | Top-1: 27.82 (7194/25856)
Train | Batch (196/196) | Top-1: 28.35 (14176/50000)
Regular: 1.1603105068206787
Epoche: 4; regular: 1.1603105068206787: flops 69593862
#Filters: 695, #FLOPs: 68.25M | Top-1: 24.66
Epoch 5
Train | Batch (1/196) | Top-1: 30.08 (77/256)
Train | Batch (101/196) | Top-1: 29.13 (7533/25856)
Train | Batch (196/196) | Top-1: 29.63 (14814/50000)
Regular: 1.1101548671722412
Epoche: 5; regular: 1.1101548671722412: flops 69593862
#Filters: 695, #FLOPs: 67.98M | Top-1: 11.25
Epoch 6
Train | Batch (1/196) | Top-1: 28.12 (72/256)
Train | Batch (101/196) | Top-1: 30.37 (7852/25856)
Train | Batch (196/196) | Top-1: 31.00 (15502/50000)
Regular: 1.1357717514038086
Epoche: 6; regular: 1.1357717514038086: flops 69593862
#Filters: 695, #FLOPs: 67.80M | Top-1: 11.98
Epoch 7
Train | Batch (1/196) | Top-1: 25.78 (66/256)
Train | Batch (101/196) | Top-1: 31.78 (8217/25856)
Train | Batch (196/196) | Top-1: 32.19 (16096/50000)
Regular: 1.3358327150344849
Epoche: 7; regular: 1.3358327150344849: flops 69593862
#Filters: 690, #FLOPs: 66.09M | Top-1: 21.63
Epoch 8
Train | Batch (1/196) | Top-1: 28.12 (72/256)
Train | Batch (101/196) | Top-1: 32.66 (8444/25856)
Train | Batch (196/196) | Top-1: 32.84 (16422/50000)
Regular: 1.2606550455093384
Epoche: 8; regular: 1.2606550455093384: flops 69593862
#Filters: 691, #FLOPs: 67.17M | Top-1: 11.93
Epoch 9
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 33.05 (8545/25856)
Train | Batch (196/196) | Top-1: 34.15 (17074/50000)
Regular: 2.6497931480407715
Epoche: 9; regular: 2.6497931480407715: flops 69593862
#Filters: 696, #FLOPs: 68.96M | Top-1: 32.97
Epoch 10
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 34.92 (9028/25856)
Train | Batch (196/196) | Top-1: 35.03 (17514/50000)
Regular: 2.5550637245178223
Epoche: 10; regular: 2.5550637245178223: flops 69593862
#Filters: 692, #FLOPs: 67.53M | Top-1: 10.19
Epoch 11
Train | Batch (1/196) | Top-1: 32.81 (84/256)
Train | Batch (101/196) | Top-1: 34.87 (9017/25856)
Train | Batch (196/196) | Top-1: 35.40 (17699/50000)
Regular: 1.5165928602218628
Epoche: 11; regular: 1.5165928602218628: flops 69593862
#Filters: 696, #FLOPs: 68.52M | Top-1: 15.84
Epoch 12
Train | Batch (1/196) | Top-1: 32.81 (84/256)
Train | Batch (101/196) | Top-1: 34.27 (8861/25856)
Train | Batch (196/196) | Top-1: 35.35 (17673/50000)
Regular: 1.1187087297439575
Epoche: 12; regular: 1.1187087297439575: flops 69593862
#Filters: 694, #FLOPs: 68.25M | Top-1: 14.95
Epoch 13
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 36.74 (9500/25856)
Train | Batch (196/196) | Top-1: 36.88 (18441/50000)
Regular: 1.569661021232605
Epoche: 13; regular: 1.569661021232605: flops 69593862
#Filters: 695, #FLOPs: 67.89M | Top-1: 21.42
Epoch 14
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 36.52 (9442/25856)
Train | Batch (196/196) | Top-1: 36.57 (18284/50000)
Regular: 1.1773405075073242
Epoche: 14; regular: 1.1773405075073242: flops 69593862
#Filters: 693, #FLOPs: 67.89M | Top-1: 10.01
Epoch 15
Train | Batch (1/196) | Top-1: 35.16 (90/256)
Train | Batch (101/196) | Top-1: 37.64 (9732/25856)
Train | Batch (196/196) | Top-1: 37.64 (18819/50000)
Regular: 1.1721861362457275
Epoche: 15; regular: 1.1721861362457275: flops 69593862
#Filters: 693, #FLOPs: 67.89M | Top-1: 19.26
Epoch 16
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 37.34 (9655/25856)
Train | Batch (196/196) | Top-1: 36.87 (18434/50000)
Regular: 1.140190601348877
Epoche: 16; regular: 1.140190601348877: flops 69593862
#Filters: 694, #FLOPs: 68.25M | Top-1: 10.14
Epoch 17
Train | Batch (1/196) | Top-1: 36.72 (94/256)
Train | Batch (101/196) | Top-1: 32.51 (8406/25856)
Train | Batch (196/196) | Top-1: 33.60 (16800/50000)
Regular: 1.1294142007827759
Epoche: 17; regular: 1.1294142007827759: flops 69593862
#Filters: 698, #FLOPs: 69.41M | Top-1: 21.32
Epoch 18
Train | Batch (1/196) | Top-1: 32.81 (84/256)
Train | Batch (101/196) | Top-1: 36.03 (9316/25856)
Train | Batch (196/196) | Top-1: 36.50 (18251/50000)
Regular: 1.14156973361969
Epoche: 18; regular: 1.14156973361969: flops 69593862
#Filters: 691, #FLOPs: 67.35M | Top-1: 10.00
Epoch 19
Train | Batch (1/196) | Top-1: 33.59 (86/256)
Train | Batch (101/196) | Top-1: 37.85 (9787/25856)
Train | Batch (196/196) | Top-1: 35.21 (17605/50000)
Regular: 1.3731167316436768
Epoche: 19; regular: 1.3731167316436768: flops 69593862
#Filters: 697, #FLOPs: 69.05M | Top-1: 19.90
Epoch 20
Train | Batch (1/196) | Top-1: 33.59 (86/256)
Train | Batch (101/196) | Top-1: 34.91 (9027/25856)
Train | Batch (196/196) | Top-1: 35.59 (17794/50000)
Regular: 3.3950488567352295
Epoche: 20; regular: 3.3950488567352295: flops 69593862
#Filters: 692, #FLOPs: 67.08M | Top-1: 17.53
Epoch 21
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 36.83 (9522/25856)
Train | Batch (196/196) | Top-1: 37.16 (18579/50000)
Regular: 2.1199302673339844
Epoche: 21; regular: 2.1199302673339844: flops 69593862
#Filters: 691, #FLOPs: 67.62M | Top-1: 18.89
Epoch 22
Train | Batch (1/196) | Top-1: 37.11 (95/256)
Train | Batch (101/196) | Top-1: 36.88 (9535/25856)
Train | Batch (196/196) | Top-1: 35.25 (17625/50000)
Regular: 1.5418514013290405
Epoche: 22; regular: 1.5418514013290405: flops 69593862
#Filters: 692, #FLOPs: 67.17M | Top-1: 10.53
Epoch 23
Train | Batch (1/196) | Top-1: 25.39 (65/256)
Train | Batch (101/196) | Top-1: 30.98 (8010/25856)
Train | Batch (196/196) | Top-1: 31.93 (15964/50000)
Regular: 2.5726375579833984
Epoche: 23; regular: 2.5726375579833984: flops 69593862
#Filters: 694, #FLOPs: 68.13M | Top-1: 11.70
Epoch 24
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 34.63 (8954/25856)
Train | Batch (196/196) | Top-1: 35.34 (17671/50000)
Regular: 1.5254572629928589
Epoche: 24; regular: 1.5254572629928589: flops 69593862
#Filters: 695, #FLOPs: 68.16M | Top-1: 23.33
Epoch 25
Train | Batch (1/196) | Top-1: 34.77 (89/256)
Train | Batch (101/196) | Top-1: 34.50 (8921/25856)
Train | Batch (196/196) | Top-1: 35.78 (17888/50000)
Regular: 1.9194260835647583
Epoche: 25; regular: 1.9194260835647583: flops 69593862
#Filters: 698, #FLOPs: 68.96M | Top-1: 17.03
Epoch 26
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 38.60 (9981/25856)
Train | Batch (196/196) | Top-1: 37.19 (18594/50000)
Regular: 1.3776538372039795
Epoche: 26; regular: 1.3776538372039795: flops 69593862
#Filters: 696, #FLOPs: 68.25M | Top-1: 12.40
Epoch 27
Train | Batch (1/196) | Top-1: 30.86 (79/256)
Train | Batch (101/196) | Top-1: 38.56 (9971/25856)
Train | Batch (196/196) | Top-1: 37.72 (18859/50000)
Regular: 1.311837077140808
Epoche: 27; regular: 1.311837077140808: flops 69593862
#Filters: 694, #FLOPs: 67.80M | Top-1: 10.01
Epoch 28
Train | Batch (1/196) | Top-1: 32.81 (84/256)
Train | Batch (101/196) | Top-1: 35.53 (9187/25856)
Train | Batch (196/196) | Top-1: 36.94 (18472/50000)
Regular: 3.0888397693634033
Epoche: 28; regular: 3.0888397693634033: flops 69593862
#Filters: 691, #FLOPs: 67.08M | Top-1: 11.39
Epoch 29
Train | Batch (1/196) | Top-1: 27.73 (71/256)
Train | Batch (101/196) | Top-1: 34.98 (9045/25856)
Train | Batch (196/196) | Top-1: 36.41 (18203/50000)
Regular: 2.634243965148926
Epoche: 29; regular: 2.634243965148926: flops 69593862
#Filters: 692, #FLOPs: 67.53M | Top-1: 16.39
Drin!!
Layers that will be prunned: [(3, 2), (5, 2), (7, 1), (9, 1), (11, 1)]
Prunning filters..
Layer index: 3; Pruned filters: 2
Layer index: 5; Pruned filters: 1
Layer index: 5; Pruned filters: 1
Layer index: 7; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 65.101M | #Params: 0.073M
1.028722229179489
After Growth | FLOPs: 68.982M | #Params: 0.077M
Epoch 0
Train | Batch (1/196) | Top-1: 12.50 (32/256)
Train | Batch (101/196) | Top-1: 27.37 (7077/25856)
Train | Batch (196/196) | Top-1: 31.61 (15805/50000)
Regular: nan
Epoche: 0; regular: nan: flops 68982160
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
#Filters: 718, #FLOPs: 69.17M | Top-1: 21.32
Epoch 1
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 40.05 (10355/25856)
Train | Batch (196/196) | Top-1: 41.43 (20715/50000)
Regular: nan
Epoche: 1; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 33.07
Epoch 2
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 43.71 (11302/25856)
Train | Batch (196/196) | Top-1: 44.28 (22141/50000)
Regular: nan
Epoche: 2; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 28.12
Epoch 3
Train | Batch (1/196) | Top-1: 41.41 (106/256)
Train | Batch (101/196) | Top-1: 46.32 (11977/25856)
Train | Batch (196/196) | Top-1: 46.70 (23352/50000)
Regular: nan
Epoche: 3; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 35.61
Epoch 4
Train | Batch (1/196) | Top-1: 44.53 (114/256)
Train | Batch (101/196) | Top-1: 48.05 (12425/25856)
Train | Batch (196/196) | Top-1: 48.81 (24405/50000)
Regular: nan
Epoche: 4; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 41.82
Epoch 5
Train | Batch (1/196) | Top-1: 48.44 (124/256)
Train | Batch (101/196) | Top-1: 50.92 (13166/25856)
Train | Batch (196/196) | Top-1: 51.50 (25750/50000)
Regular: nan
Epoche: 5; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 37.43
Epoch 6
Train | Batch (1/196) | Top-1: 46.88 (120/256)
Train | Batch (101/196) | Top-1: 53.19 (13753/25856)
Train | Batch (196/196) | Top-1: 53.57 (26784/50000)
Regular: nan
Epoche: 6; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 32.11
Epoch 7
Train | Batch (1/196) | Top-1: 50.00 (128/256)
Train | Batch (101/196) | Top-1: 54.52 (14096/25856)
Train | Batch (196/196) | Top-1: 54.79 (27397/50000)
Regular: nan
Epoche: 7; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 45.56
Epoch 8
Train | Batch (1/196) | Top-1: 46.48 (119/256)
Train | Batch (101/196) | Top-1: 55.82 (14433/25856)
Train | Batch (196/196) | Top-1: 56.14 (28070/50000)
Regular: nan
Epoche: 8; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 28.85
Epoch 9
Train | Batch (1/196) | Top-1: 54.30 (139/256)
Train | Batch (101/196) | Top-1: 57.00 (14739/25856)
Train | Batch (196/196) | Top-1: 57.27 (28635/50000)
Regular: nan
Epoche: 9; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 46.32
Epoch 10
Train | Batch (1/196) | Top-1: 57.42 (147/256)
Train | Batch (101/196) | Top-1: 58.25 (15062/25856)
Train | Batch (196/196) | Top-1: 57.94 (28969/50000)
Regular: nan
Epoche: 10; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 33.82
Epoch 11
Train | Batch (1/196) | Top-1: 54.69 (140/256)
Train | Batch (101/196) | Top-1: 58.56 (15141/25856)
Train | Batch (196/196) | Top-1: 58.64 (29318/50000)
Regular: nan
Epoche: 11; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 39.33
Epoch 12
Train | Batch (1/196) | Top-1: 57.03 (146/256)
Train | Batch (101/196) | Top-1: 58.43 (15108/25856)
Train | Batch (196/196) | Top-1: 58.89 (29445/50000)
Regular: nan
Epoche: 12; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 45.06
Epoch 13
Train | Batch (1/196) | Top-1: 60.55 (155/256)
Train | Batch (101/196) | Top-1: 59.26 (15322/25856)
Train | Batch (196/196) | Top-1: 59.41 (29706/50000)
Regular: nan
Epoche: 13; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 45.63
Epoch 14
Train | Batch (1/196) | Top-1: 56.64 (145/256)
Train | Batch (101/196) | Top-1: 59.63 (15417/25856)
Train | Batch (196/196) | Top-1: 59.53 (29763/50000)
Regular: nan
Epoche: 14; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 53.40
Epoch 15
Train | Batch (1/196) | Top-1: 55.86 (143/256)
Train | Batch (101/196) | Top-1: 60.13 (15547/25856)
Train | Batch (196/196) | Top-1: 59.90 (29952/50000)
Regular: nan
Epoche: 15; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 53.01
Epoch 16
Train | Batch (1/196) | Top-1: 61.33 (157/256)
Train | Batch (101/196) | Top-1: 60.49 (15640/25856)
Train | Batch (196/196) | Top-1: 60.14 (30071/50000)
Regular: nan
Epoche: 16; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 52.80
Epoch 17
Train | Batch (1/196) | Top-1: 62.11 (159/256)
Train | Batch (101/196) | Top-1: 60.07 (15532/25856)
Train | Batch (196/196) | Top-1: 60.22 (30108/50000)
Regular: nan
Epoche: 17; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 56.74
Epoch 18
Train | Batch (1/196) | Top-1: 54.69 (140/256)
Train | Batch (101/196) | Top-1: 60.46 (15633/25856)
Train | Batch (196/196) | Top-1: 60.74 (30371/50000)
Regular: nan
Epoche: 18; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 52.81
Epoch 19
Train | Batch (1/196) | Top-1: 58.98 (151/256)
Train | Batch (101/196) | Top-1: 60.94 (15756/25856)
Train | Batch (196/196) | Top-1: 60.85 (30424/50000)
Regular: nan
Epoche: 19; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 45.73
Epoch 20
Train | Batch (1/196) | Top-1: 51.17 (131/256)
Train | Batch (101/196) | Top-1: 60.98 (15768/25856)
Train | Batch (196/196) | Top-1: 61.02 (30510/50000)
Regular: nan
Epoche: 20; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 43.02
Epoch 21
Train | Batch (1/196) | Top-1: 61.33 (157/256)
Train | Batch (101/196) | Top-1: 61.37 (15868/25856)
Train | Batch (196/196) | Top-1: 61.37 (30685/50000)
Regular: nan
Epoche: 21; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 39.07
Epoch 22
Train | Batch (1/196) | Top-1: 62.11 (159/256)
Train | Batch (101/196) | Top-1: 61.31 (15852/25856)
Train | Batch (196/196) | Top-1: 61.39 (30695/50000)
Regular: nan
Epoche: 22; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 48.50
Epoch 23
Train | Batch (1/196) | Top-1: 57.81 (148/256)
Train | Batch (101/196) | Top-1: 61.00 (15771/25856)
Train | Batch (196/196) | Top-1: 61.20 (30601/50000)
Regular: nan
Epoche: 23; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 54.67
Epoch 24
Train | Batch (1/196) | Top-1: 65.23 (167/256)
Train | Batch (101/196) | Top-1: 62.11 (16060/25856)
Train | Batch (196/196) | Top-1: 61.71 (30856/50000)
Regular: nan
Epoche: 24; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 55.78
Epoch 25
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 61.54 (15912/25856)
Train | Batch (196/196) | Top-1: 61.66 (30832/50000)
Regular: nan
Epoche: 25; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 47.63
Epoch 26
Train | Batch (1/196) | Top-1: 58.98 (151/256)
Train | Batch (101/196) | Top-1: 62.13 (16065/25856)
Train | Batch (196/196) | Top-1: 61.91 (30956/50000)
Regular: nan
Epoche: 26; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 43.31
Epoch 27
Train | Batch (1/196) | Top-1: 57.81 (148/256)
Train | Batch (101/196) | Top-1: 61.66 (15943/25856)
Train | Batch (196/196) | Top-1: 61.95 (30974/50000)
Regular: nan
Epoche: 27; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 49.56
Epoch 28
Train | Batch (1/196) | Top-1: 61.33 (157/256)
Train | Batch (101/196) | Top-1: 61.55 (15914/25856)
Train | Batch (196/196) | Top-1: 62.00 (31002/50000)
Regular: nan
Epoche: 28; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 54.03
Epoch 29
Train | Batch (1/196) | Top-1: 63.67 (163/256)
Train | Batch (101/196) | Top-1: 62.12 (16061/25856)
Train | Batch (196/196) | Top-1: 62.10 (31051/50000)
Regular: nan
Epoche: 29; regular: nan: flops 68982160
#Filters: 718, #FLOPs: 69.17M | Top-1: 54.17
model: ResNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(21, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(17, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(8, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(19, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(25, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(40, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (4): AvgPool2d(kernel_size=8, stride=8, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=40, out_features=10, bias=True)
  )
)
Test acc: 54.169999999999995
