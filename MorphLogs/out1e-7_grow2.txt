no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=1e-07, logger='MorphLogs/logMorphNetFlops1e-7_grow2.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 9.77 (25/256)
Train | Batch (101/196) | Top-1: 9.90 (2561/25856)
Train | Batch (196/196) | Top-1: 9.76 (4878/50000)
Regular: 3.9698076248168945
Epoche: 0; regular: 3.9698076248168945: flops 68862592
#Filters: 68, #FLOPs: 7.74M | Top-1: 10.00
Epoch 1
Train | Batch (1/196) | Top-1: 9.38 (24/256)
Train | Batch (101/196) | Top-1: 16.79 (4341/25856)
Train | Batch (196/196) | Top-1: 18.36 (9181/50000)
Regular: 0.6860769391059875
Epoche: 1; regular: 0.6860769391059875: flops 68862592
#Filters: 205, #FLOPs: 17.41M | Top-1: 10.38
Epoch 2
Train | Batch (1/196) | Top-1: 17.58 (45/256)
Train | Batch (101/196) | Top-1: 19.68 (5088/25856)
Train | Batch (196/196) | Top-1: 21.25 (10627/50000)
Regular: 0.5180855393409729
Epoche: 2; regular: 0.5180855393409729: flops 68862592
#Filters: 191, #FLOPs: 16.21M | Top-1: 17.65
Epoch 3
Train | Batch (1/196) | Top-1: 18.75 (48/256)
Train | Batch (101/196) | Top-1: 21.47 (5551/25856)
Train | Batch (196/196) | Top-1: 21.67 (10833/50000)
Regular: 2.065420627593994
Epoche: 3; regular: 2.065420627593994: flops 68862592
#Filters: 150, #FLOPs: 13.42M | Top-1: 17.72
Epoch 4
Train | Batch (1/196) | Top-1: 16.02 (41/256)
Train | Batch (101/196) | Top-1: 22.23 (5749/25856)
Train | Batch (196/196) | Top-1: 22.50 (11250/50000)
Regular: 0.38139912486076355
Epoche: 4; regular: 0.38139912486076355: flops 68862592
#Filters: 176, #FLOPs: 16.42M | Top-1: 12.70
Epoch 5
Train | Batch (1/196) | Top-1: 21.48 (55/256)
Train | Batch (101/196) | Top-1: 22.88 (5916/25856)
Train | Batch (196/196) | Top-1: 23.48 (11738/50000)
Regular: 0.19962437450885773
Epoche: 5; regular: 0.19962437450885773: flops 68862592
#Filters: 143, #FLOPs: 13.46M | Top-1: 9.87
Epoch 6
Train | Batch (1/196) | Top-1: 22.66 (58/256)
Train | Batch (101/196) | Top-1: 24.62 (6366/25856)
Train | Batch (196/196) | Top-1: 25.63 (12815/50000)
Regular: 0.20694783329963684
Epoche: 6; regular: 0.20694783329963684: flops 68862592
#Filters: 114, #FLOPs: 10.52M | Top-1: 13.97
Epoch 7
Train | Batch (1/196) | Top-1: 20.31 (52/256)
Train | Batch (101/196) | Top-1: 23.94 (6189/25856)
Train | Batch (196/196) | Top-1: 23.72 (11859/50000)
Regular: 0.34673017263412476
Epoche: 7; regular: 0.34673017263412476: flops 68862592
#Filters: 175, #FLOPs: 15.95M | Top-1: 20.77
Epoch 8
Train | Batch (1/196) | Top-1: 19.53 (50/256)
Train | Batch (101/196) | Top-1: 22.28 (5762/25856)
Train | Batch (196/196) | Top-1: 22.73 (11364/50000)
Regular: 0.9528521299362183
Epoche: 8; regular: 0.9528521299362183: flops 68862592
#Filters: 242, #FLOPs: 21.80M | Top-1: 10.00
Epoch 9
Train | Batch (1/196) | Top-1: 16.02 (41/256)
Train | Batch (101/196) | Top-1: 22.86 (5911/25856)
Train | Batch (196/196) | Top-1: 23.87 (11935/50000)
Regular: 0.2257653772830963
Epoche: 9; regular: 0.2257653772830963: flops 68862592
#Filters: 131, #FLOPs: 12.40M | Top-1: 17.22
Epoch 10
Train | Batch (1/196) | Top-1: 24.61 (63/256)
Train | Batch (101/196) | Top-1: 25.80 (6672/25856)
Train | Batch (196/196) | Top-1: 26.60 (13301/50000)
Regular: 0.16634641587734222
Epoche: 10; regular: 0.16634641587734222: flops 68862592
#Filters: 173, #FLOPs: 15.94M | Top-1: 10.78
Epoch 11
Train | Batch (1/196) | Top-1: 29.69 (76/256)
Train | Batch (101/196) | Top-1: 26.47 (6845/25856)
Train | Batch (196/196) | Top-1: 26.77 (13384/50000)
Regular: 0.27785825729370117
Epoche: 11; regular: 0.27785825729370117: flops 68862592
#Filters: 149, #FLOPs: 13.31M | Top-1: 24.93
Epoch 12
Train | Batch (1/196) | Top-1: 28.12 (72/256)
Train | Batch (101/196) | Top-1: 27.77 (7181/25856)
Train | Batch (196/196) | Top-1: 28.26 (14128/50000)
Regular: 0.1835380494594574
Epoche: 12; regular: 0.1835380494594574: flops 68862592
#Filters: 148, #FLOPs: 13.64M | Top-1: 13.37
Epoch 13
Train | Batch (1/196) | Top-1: 23.83 (61/256)
Train | Batch (101/196) | Top-1: 27.79 (7186/25856)
Train | Batch (196/196) | Top-1: 28.39 (14197/50000)
Regular: 0.19018873572349548
Epoche: 13; regular: 0.19018873572349548: flops 68862592
#Filters: 146, #FLOPs: 13.57M | Top-1: 14.67
Epoch 14
Train | Batch (1/196) | Top-1: 29.69 (76/256)
Train | Batch (101/196) | Top-1: 28.12 (7272/25856)
Train | Batch (196/196) | Top-1: 28.48 (14241/50000)
Regular: 0.17761123180389404
Epoche: 14; regular: 0.17761123180389404: flops 68862592
#Filters: 172, #FLOPs: 15.43M | Top-1: 10.28
Epoch 15
Train | Batch (1/196) | Top-1: 25.39 (65/256)
Train | Batch (101/196) | Top-1: 27.89 (7212/25856)
Train | Batch (196/196) | Top-1: 28.27 (14133/50000)
Regular: 0.25369083881378174
Epoche: 15; regular: 0.25369083881378174: flops 68862592
#Filters: 193, #FLOPs: 17.35M | Top-1: 19.74
Epoch 16
Train | Batch (1/196) | Top-1: 25.78 (66/256)
Train | Batch (101/196) | Top-1: 29.09 (7522/25856)
Train | Batch (196/196) | Top-1: 29.33 (14666/50000)
Regular: 0.1741117686033249
Epoche: 16; regular: 0.1741117686033249: flops 68862592
#Filters: 178, #FLOPs: 16.22M | Top-1: 14.61
Epoch 17
Train | Batch (1/196) | Top-1: 28.12 (72/256)
Train | Batch (101/196) | Top-1: 29.26 (7565/25856)
Train | Batch (196/196) | Top-1: 29.24 (14618/50000)
Regular: 0.18958567082881927
Epoche: 17; regular: 0.18958567082881927: flops 68862592
#Filters: 171, #FLOPs: 15.09M | Top-1: 22.06
Epoch 18
Train | Batch (1/196) | Top-1: 29.69 (76/256)
Train | Batch (101/196) | Top-1: 29.66 (7670/25856)
Train | Batch (196/196) | Top-1: 29.53 (14766/50000)
Regular: 0.1806640625
Epoche: 18; regular: 0.1806640625: flops 68862592
#Filters: 165, #FLOPs: 15.36M | Top-1: 11.96
Epoch 19
Train | Batch (1/196) | Top-1: 24.61 (63/256)
Train | Batch (101/196) | Top-1: 28.71 (7422/25856)
Train | Batch (196/196) | Top-1: 29.24 (14622/50000)
Regular: 0.24525895714759827
Epoche: 19; regular: 0.24525895714759827: flops 68862592
#Filters: 141, #FLOPs: 12.96M | Top-1: 19.18
Epoch 20
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 29.89 (7729/25856)
Train | Batch (196/196) | Top-1: 29.86 (14931/50000)
Regular: 0.20100869238376617
Epoche: 20; regular: 0.20100869238376617: flops 68862592
#Filters: 176, #FLOPs: 15.86M | Top-1: 28.89
Epoch 21
Train | Batch (1/196) | Top-1: 30.08 (77/256)
Train | Batch (101/196) | Top-1: 30.33 (7843/25856)
Train | Batch (196/196) | Top-1: 30.41 (15207/50000)
Regular: 0.18423613905906677
Epoche: 21; regular: 0.18423613905906677: flops 68862592
#Filters: 147, #FLOPs: 13.12M | Top-1: 17.65
Epoch 22
Train | Batch (1/196) | Top-1: 28.12 (72/256)
Train | Batch (101/196) | Top-1: 30.47 (7878/25856)
Train | Batch (196/196) | Top-1: 30.25 (15123/50000)
Regular: 0.19749870896339417
Epoche: 22; regular: 0.19749870896339417: flops 68862592
#Filters: 119, #FLOPs: 10.75M | Top-1: 22.55
Epoch 23
Train | Batch (1/196) | Top-1: 26.17 (67/256)
Train | Batch (101/196) | Top-1: 30.44 (7871/25856)
Train | Batch (196/196) | Top-1: 30.63 (15316/50000)
Regular: 0.19681809842586517
Epoche: 23; regular: 0.19681809842586517: flops 68862592
#Filters: 144, #FLOPs: 13.12M | Top-1: 18.86
Epoch 24
Train | Batch (1/196) | Top-1: 29.30 (75/256)
Train | Batch (101/196) | Top-1: 31.20 (8066/25856)
Train | Batch (196/196) | Top-1: 31.19 (15597/50000)
Regular: 0.18920527398586273
Epoche: 24; regular: 0.18920527398586273: flops 68862592
#Filters: 193, #FLOPs: 17.67M | Top-1: 23.95
Epoch 25
Train | Batch (1/196) | Top-1: 28.91 (74/256)
Train | Batch (101/196) | Top-1: 31.77 (8214/25856)
Train | Batch (196/196) | Top-1: 32.02 (16012/50000)
Regular: 0.197540283203125
Epoche: 25; regular: 0.197540283203125: flops 68862592
#Filters: 212, #FLOPs: 19.72M | Top-1: 15.58
Epoch 26
Train | Batch (1/196) | Top-1: 28.52 (73/256)
Train | Batch (101/196) | Top-1: 32.08 (8294/25856)
Train | Batch (196/196) | Top-1: 32.15 (16075/50000)
Regular: 0.1937657743692398
Epoche: 26; regular: 0.1937657743692398: flops 68862592
#Filters: 165, #FLOPs: 14.96M | Top-1: 17.73
Epoch 27
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 32.49 (8401/25856)
Train | Batch (196/196) | Top-1: 32.79 (16394/50000)
Regular: 0.204983189702034
Epoche: 27; regular: 0.204983189702034: flops 68862592
#Filters: 153, #FLOPs: 13.71M | Top-1: 19.56
Epoch 28
Train | Batch (1/196) | Top-1: 26.17 (67/256)
Train | Batch (101/196) | Top-1: 32.54 (8413/25856)
Train | Batch (196/196) | Top-1: 33.07 (16537/50000)
Regular: 0.2042621672153473
Epoche: 28; regular: 0.2042621672153473: flops 68862592
#Filters: 197, #FLOPs: 17.81M | Top-1: 17.17
Epoch 29
Train | Batch (1/196) | Top-1: 32.42 (83/256)
Train | Batch (101/196) | Top-1: 33.22 (8589/25856)
Train | Batch (196/196) | Top-1: 33.57 (16785/50000)
Regular: 0.23032036423683167
Epoche: 29; regular: 0.23032036423683167: flops 68862592
#Filters: 118, #FLOPs: 10.08M | Top-1: 23.13
Drin!!
Layers that will be prunned: [(0, 10), (1, 15), (2, 10), (3, 15), (4, 10), (5, 14), (6, 10), (7, 15), (8, 10), (9, 15), (10, 10), (11, 31), (12, 26), (13, 30), (14, 26), (15, 30), (16, 26), (17, 30), (18, 26), (19, 31), (20, 26), (21, 60), (22, 57), (23, 62), (24, 57), (25, 62), (26, 57), (27, 62), (28, 57), (29, 57), (30, 57)]
Prunning filters..
Layer index: 0; Pruned filters: 2
Layer index: 0; Pruned filters: 3
Layer index: 0; Pruned filters: 2
Layer index: 0; Pruned filters: 1
Layer index: 0; Pruned filters: 2
Layer index: 2; Pruned filters: 2
Layer index: 2; Pruned filters: 3
Layer index: 2; Pruned filters: 2
Layer index: 2; Pruned filters: 1
Layer index: 2; Pruned filters: 2
Layer index: 4; Pruned filters: 2
Layer index: 4; Pruned filters: 3
Layer index: 4; Pruned filters: 2
Layer index: 4; Pruned filters: 1
Layer index: 4; Pruned filters: 2
Layer index: 6; Pruned filters: 2
Layer index: 6; Pruned filters: 3
Layer index: 6; Pruned filters: 2
Layer index: 6; Pruned filters: 1
Layer index: 6; Pruned filters: 2
Layer index: 8; Pruned filters: 2
Layer index: 8; Pruned filters: 3
Layer index: 8; Pruned filters: 2
Layer index: 8; Pruned filters: 1
Layer index: 8; Pruned filters: 2
Layer index: 10; Pruned filters: 2
Layer index: 10; Pruned filters: 3
Layer index: 10; Pruned filters: 2
Layer index: 10; Pruned filters: 1
Layer index: 10; Pruned filters: 2
Layer index: 12; Pruned filters: 2
Layer index: 12; Pruned filters: 3
Layer index: 12; Pruned filters: 2
Layer index: 12; Pruned filters: 1
Layer index: 12; Pruned filters: 18
Layer index: 14; Pruned filters: 2
Layer index: 14; Pruned filters: 3
Layer index: 14; Pruned filters: 2
Layer index: 14; Pruned filters: 1
Layer index: 14; Pruned filters: 18
Layer index: 16; Pruned filters: 2
Layer index: 16; Pruned filters: 3
Layer index: 16; Pruned filters: 2
Layer index: 16; Pruned filters: 1
Layer index: 16; Pruned filters: 18
Layer index: 18; Pruned filters: 2
Layer index: 18; Pruned filters: 3
Layer index: 18; Pruned filters: 2
Layer index: 18; Pruned filters: 1
Layer index: 18; Pruned filters: 18
Layer index: 20; Pruned filters: 2
Layer index: 20; Pruned filters: 3
Layer index: 20; Pruned filters: 2
Layer index: 20; Pruned filters: 1
Layer index: 20; Pruned filters: 18
Layer index: 22; Pruned filters: 2
Layer index: 22; Pruned filters: 3
Layer index: 22; Pruned filters: 2
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 42
Layer index: 22; Pruned filters: 7
Layer index: 24; Pruned filters: 2
Layer index: 24; Pruned filters: 3
Layer index: 24; Pruned filters: 2
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 42
Layer index: 24; Pruned filters: 7
Layer index: 26; Pruned filters: 2
Layer index: 26; Pruned filters: 3
Layer index: 26; Pruned filters: 2
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 42
Layer index: 26; Pruned filters: 7
Layer index: 28; Pruned filters: 2
Layer index: 28; Pruned filters: 3
Layer index: 28; Pruned filters: 2
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 42
Layer index: 28; Pruned filters: 7
Layer index: 30; Pruned filters: 2
Layer index: 30; Pruned filters: 3
Layer index: 30; Pruned filters: 2
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 42
Layer index: 30; Pruned filters: 7
Layer index: 1; Pruned filters: 13
Layer index: 1; Pruned filters: 2
Layer index: 3; Pruned filters: 7
Layer index: 3; Pruned filters: 8
Layer index: 5; Pruned filters: 11
Layer index: 5; Pruned filters: 1
Layer index: 5; Pruned filters: 2
Layer index: 7; Pruned filters: 2
Layer index: 7; Pruned filters: 13
Layer index: 9; Pruned filters: 12
Layer index: 9; Pruned filters: 3
Layer index: 11; Pruned filters: 10
Layer index: 11; Pruned filters: 21
Layer index: 13; Pruned filters: 30
Layer index: 15; Pruned filters: 30
Layer index: 17; Pruned filters: 30
Layer index: 19; Pruned filters: 22
Layer index: 19; Pruned filters: 9
Layer index: 21; Pruned filters: 6
Layer index: 21; Pruned filters: 9
Layer index: 21; Pruned filters: 19
Layer index: 21; Pruned filters: 19
Layer index: 21; Pruned filters: 7
Layer index: 23; Pruned filters: 62
Layer index: 25; Pruned filters: 62
Layer index: 27; Pruned filters: 62
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 10
Layer index: 29; Pruned filters: 20
Layer index: 29; Pruned filters: 3
Layer index: 29; Pruned filters: 15
Layer index: 29; Pruned filters: 8
Target (flops): 68.863M
After Pruning | FLOPs: 1.185M | #Params: 0.004M
8.137536367714407
After Growth | FLOPs: 68.068M | #Params: 0.240M
I: 1
flops: 68067642
Before Pruning | FLOPs: 68.068M | #Params: 0.240M
Epoch 0
Train | Batch (1/196) | Top-1: 32.42 (83/256)
Train | Batch (101/196) | Top-1: 34.87 (9017/25856)
Train | Batch (196/196) | Top-1: 34.60 (17301/50000)
Regular: 3.417571783065796
Epoche: 0; regular: 3.417571783065796: flops 68067642
#Filters: 774, #FLOPs: 38.67M | Top-1: 13.51
Epoch 1
Train | Batch (1/196) | Top-1: 27.73 (71/256)
Train | Batch (101/196) | Top-1: 35.05 (9063/25856)
Train | Batch (196/196) | Top-1: 35.17 (17585/50000)
Regular: 0.28069567680358887
Epoche: 1; regular: 0.28069567680358887: flops 68067642
#Filters: 151, #FLOPs: 26.00M | Top-1: 18.73
Epoch 2
Train | Batch (1/196) | Top-1: 29.69 (76/256)
Train | Batch (101/196) | Top-1: 35.26 (9116/25856)
Train | Batch (196/196) | Top-1: 35.54 (17768/50000)
Regular: 0.2594020962715149
Epoche: 2; regular: 0.2594020962715149: flops 68067642
#Filters: 151, #FLOPs: 26.34M | Top-1: 19.08
Epoch 3
Train | Batch (1/196) | Top-1: 31.64 (81/256)
Train | Batch (101/196) | Top-1: 35.62 (9210/25856)
Train | Batch (196/196) | Top-1: 35.60 (17802/50000)
Regular: 0.2498674988746643
Epoche: 3; regular: 0.2498674988746643: flops 68067642
#Filters: 149, #FLOPs: 25.97M | Top-1: 22.95
Epoch 4
Train | Batch (1/196) | Top-1: 33.59 (86/256)
Train | Batch (101/196) | Top-1: 35.87 (9274/25856)
Train | Batch (196/196) | Top-1: 35.77 (17885/50000)
Regular: 0.2530153691768646
Epoche: 4; regular: 0.2530153691768646: flops 68067642
#Filters: 149, #FLOPs: 25.97M | Top-1: 29.35
Epoch 5
Train | Batch (1/196) | Top-1: 30.08 (77/256)
Train | Batch (101/196) | Top-1: 35.64 (9216/25856)
Train | Batch (196/196) | Top-1: 35.95 (17977/50000)
Regular: 0.2526806890964508
Epoche: 5; regular: 0.2526806890964508: flops 68067642
#Filters: 149, #FLOPs: 26.96M | Top-1: 22.51
Epoch 6
Train | Batch (1/196) | Top-1: 33.59 (86/256)
Train | Batch (101/196) | Top-1: 35.76 (9245/25856)
Train | Batch (196/196) | Top-1: 36.22 (18110/50000)
Regular: 0.2613852620124817
Epoche: 6; regular: 0.2613852620124817: flops 68067642
#Filters: 154, #FLOPs: 26.79M | Top-1: 22.11
Epoch 7
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 36.65 (9475/25856)
Train | Batch (196/196) | Top-1: 36.64 (18321/50000)
Regular: 0.25241968035697937
Epoche: 7; regular: 0.25241968035697937: flops 68067642
#Filters: 152, #FLOPs: 26.87M | Top-1: 35.70
Epoch 8
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 36.82 (9521/25856)
Train | Batch (196/196) | Top-1: 36.46 (18229/50000)
Regular: 0.2540707588195801
Epoche: 8; regular: 0.2540707588195801: flops 68067642
#Filters: 150, #FLOPs: 25.83M | Top-1: 18.31
Epoch 9
Train | Batch (1/196) | Top-1: 29.30 (75/256)
Train | Batch (101/196) | Top-1: 36.90 (9540/25856)
Train | Batch (196/196) | Top-1: 36.63 (18316/50000)
Regular: 0.2610706388950348
Epoche: 9; regular: 0.2610706388950348: flops 68067642
#Filters: 150, #FLOPs: 25.97M | Top-1: 30.80
Epoch 10
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 36.86 (9530/25856)
Train | Batch (196/196) | Top-1: 36.96 (18478/50000)
Regular: 0.2543976902961731
Epoche: 10; regular: 0.2543976902961731: flops 68067642
#Filters: 151, #FLOPs: 26.00M | Top-1: 32.37
Epoch 11
Train | Batch (1/196) | Top-1: 40.23 (103/256)
Train | Batch (101/196) | Top-1: 36.50 (9437/25856)
Train | Batch (196/196) | Top-1: 36.70 (18350/50000)
Regular: 0.2643565237522125
Epoche: 11; regular: 0.2643565237522125: flops 68067642
#Filters: 148, #FLOPs: 26.09M | Top-1: 23.22
Epoch 12
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 36.71 (9491/25856)
Train | Batch (196/196) | Top-1: 37.08 (18541/50000)
Regular: 0.26946547627449036
Epoche: 12; regular: 0.26946547627449036: flops 68067642
#Filters: 152, #FLOPs: 26.60M | Top-1: 31.92
Epoch 13
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 37.50 (9695/25856)
Train | Batch (196/196) | Top-1: 37.49 (18746/50000)
Regular: 0.266549289226532
Epoche: 13; regular: 0.266549289226532: flops 68067642
#Filters: 152, #FLOPs: 26.45M | Top-1: 15.40
Epoch 14
Train | Batch (1/196) | Top-1: 29.30 (75/256)
Train | Batch (101/196) | Top-1: 37.37 (9663/25856)
Train | Batch (196/196) | Top-1: 37.46 (18730/50000)
Regular: 0.2650754749774933
Epoche: 14; regular: 0.2650754749774933: flops 68067642
#Filters: 152, #FLOPs: 27.02M | Top-1: 26.42
Epoch 15
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 37.39 (9667/25856)
Train | Batch (196/196) | Top-1: 37.56 (18780/50000)
Regular: 0.2667428255081177
Epoche: 15; regular: 0.2667428255081177: flops 68067642
#Filters: 151, #FLOPs: 26.00M | Top-1: 19.66
Epoch 16
Train | Batch (1/196) | Top-1: 36.72 (94/256)
Train | Batch (101/196) | Top-1: 37.35 (9657/25856)
Train | Batch (196/196) | Top-1: 37.67 (18836/50000)
Regular: 0.2666592001914978
Epoche: 16; regular: 0.2666592001914978: flops 68067642
#Filters: 150, #FLOPs: 26.11M | Top-1: 24.24
Epoch 17
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 38.44 (9938/25856)
Train | Batch (196/196) | Top-1: 38.53 (19267/50000)
Regular: 0.26757532358169556
Epoche: 17; regular: 0.26757532358169556: flops 68067642
#Filters: 152, #FLOPs: 27.01M | Top-1: 26.38
Epoch 18
Train | Batch (1/196) | Top-1: 30.86 (79/256)
Train | Batch (101/196) | Top-1: 38.06 (9842/25856)
Train | Batch (196/196) | Top-1: 38.38 (19189/50000)
Regular: 0.2651354968547821
Epoche: 18; regular: 0.2651354968547821: flops 68067642
#Filters: 150, #FLOPs: 26.53M | Top-1: 34.34
Epoch 19
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 38.88 (10052/25856)
Train | Batch (196/196) | Top-1: 38.66 (19331/50000)
Regular: 0.26600155234336853
Epoche: 19; regular: 0.26600155234336853: flops 68067642
#Filters: 150, #FLOPs: 26.53M | Top-1: 22.16
Epoch 20
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 39.00 (10085/25856)
Train | Batch (196/196) | Top-1: 38.88 (19440/50000)
Regular: 0.2693931758403778
Epoche: 20; regular: 0.2693931758403778: flops 68067642
#Filters: 153, #FLOPs: 26.48M | Top-1: 23.09
Epoch 21
Train | Batch (1/196) | Top-1: 43.36 (111/256)
Train | Batch (101/196) | Top-1: 39.01 (10086/25856)
Train | Batch (196/196) | Top-1: 39.20 (19602/50000)
Regular: 0.26867613196372986
Epoche: 21; regular: 0.26867613196372986: flops 68067642
#Filters: 154, #FLOPs: 27.50M | Top-1: 15.49
Epoch 22
Train | Batch (1/196) | Top-1: 42.58 (109/256)
Train | Batch (101/196) | Top-1: 39.51 (10215/25856)
Train | Batch (196/196) | Top-1: 39.45 (19726/50000)
Regular: 0.27407950162887573
Epoche: 22; regular: 0.27407950162887573: flops 68067642
#Filters: 153, #FLOPs: 26.48M | Top-1: 24.86
Epoch 23
Train | Batch (1/196) | Top-1: 40.62 (104/256)
Train | Batch (101/196) | Top-1: 39.37 (10180/25856)
Train | Batch (196/196) | Top-1: 39.49 (19744/50000)
Regular: 0.2684394419193268
Epoche: 23; regular: 0.2684394419193268: flops 68067642
#Filters: 152, #FLOPs: 26.45M | Top-1: 19.16
Epoch 24
Train | Batch (1/196) | Top-1: 32.03 (82/256)
Train | Batch (101/196) | Top-1: 39.97 (10334/25856)
Train | Batch (196/196) | Top-1: 39.74 (19871/50000)
Regular: 0.2685317099094391
Epoche: 24; regular: 0.2685317099094391: flops 68067642
#Filters: 152, #FLOPs: 26.45M | Top-1: 24.96
Epoch 25
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 39.58 (10234/25856)
Train | Batch (196/196) | Top-1: 39.32 (19659/50000)
Regular: 0.27487069368362427
Epoche: 25; regular: 0.27487069368362427: flops 68067642
#Filters: 152, #FLOPs: 27.01M | Top-1: 29.76
Epoch 26
Train | Batch (1/196) | Top-1: 39.06 (100/256)
Train | Batch (101/196) | Top-1: 40.20 (10394/25856)
Train | Batch (196/196) | Top-1: 39.82 (19911/50000)
Regular: 0.27274051308631897
Epoche: 26; regular: 0.27274051308631897: flops 68067642
#Filters: 154, #FLOPs: 26.93M | Top-1: 20.80
Epoch 27
Train | Batch (1/196) | Top-1: 38.67 (99/256)
Train | Batch (101/196) | Top-1: 39.58 (10234/25856)
Train | Batch (196/196) | Top-1: 39.59 (19797/50000)
Regular: 0.2704026401042938
Epoche: 27; regular: 0.2704026401042938: flops 68067642
#Filters: 167, #FLOPs: 27.29M | Top-1: 24.56
Epoch 28
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 40.00 (10343/25856)
Train | Batch (196/196) | Top-1: 39.96 (19978/50000)
Regular: 0.27972105145454407
Epoche: 28; regular: 0.27972105145454407: flops 68067642
#Filters: 167, #FLOPs: 26.73M | Top-1: 32.12
Epoch 29
Train | Batch (1/196) | Top-1: 38.67 (99/256)
Train | Batch (101/196) | Top-1: 39.80 (10291/25856)
Train | Batch (196/196) | Top-1: 40.11 (20053/50000)
Regular: 0.27147284150123596
Epoche: 29; regular: 0.27147284150123596: flops 68067642
#Filters: 170, #FLOPs: 27.52M | Top-1: 16.47
Drin!!
Layers that will be prunned: [(0, 42), (2, 42), (4, 42), (6, 42), (8, 42), (9, 1), (10, 42), (11, 7), (12, 42), (13, 14), (14, 42), (15, 14), (16, 42), (17, 14), (18, 42), (19, 7), (20, 42), (21, 30), (22, 50), (23, 14), (24, 50), (25, 14), (26, 50), (27, 14), (28, 50), (29, 50), (30, 50)]
Prunning filters..
Layer index: 0; Pruned filters: 42
Layer index: 2; Pruned filters: 42
Layer index: 4; Pruned filters: 42
Layer index: 6; Pruned filters: 42
Layer index: 8; Pruned filters: 42
Layer index: 10; Pruned filters: 42
Layer index: 12; Pruned filters: 42
Layer index: 14; Pruned filters: 42
Layer index: 16; Pruned filters: 42
Layer index: 18; Pruned filters: 42
Layer index: 20; Pruned filters: 42
Layer index: 22; Pruned filters: 50
Layer index: 24; Pruned filters: 50
Layer index: 26; Pruned filters: 50
Layer index: 28; Pruned filters: 50
Layer index: 30; Pruned filters: 50
Layer index: 9; Pruned filters: 1
Layer index: 11; Pruned filters: 7
Layer index: 13; Pruned filters: 14
Layer index: 15; Pruned filters: 14
Layer index: 17; Pruned filters: 14
Layer index: 19; Pruned filters: 7
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 29
Layer index: 23; Pruned filters: 14
Layer index: 25; Pruned filters: 14
Layer index: 27; Pruned filters: 14
Layer index: 29; Pruned filters: 50
Target (flops): 68.863M
After Pruning | FLOPs: 6.645M | #Params: 0.009M
3.2521967544122083
After Growth | FLOPs: 69.791M | #Params: 0.098M
I: 2
flops: 69790694
Before Pruning | FLOPs: 69.791M | #Params: 0.098M
Epoch 0
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 40.20 (10394/25856)
Train | Batch (196/196) | Top-1: 40.31 (20157/50000)
Regular: 1.8787027597427368
Epoche: 0; regular: 1.8787027597427368: flops 69790694
#Filters: 379, #FLOPs: 37.65M | Top-1: 16.41
Epoch 1
Train | Batch (1/196) | Top-1: 41.80 (107/256)
Train | Batch (101/196) | Top-1: 40.43 (10454/25856)
Train | Batch (196/196) | Top-1: 40.79 (20394/50000)
Regular: 0.3930216431617737
Epoche: 1; regular: 0.3930216431617737: flops 69790694
#Filters: 512, #FLOPs: 67.75M | Top-1: 28.54
Epoch 2
Train | Batch (1/196) | Top-1: 37.89 (97/256)
Train | Batch (101/196) | Top-1: 41.04 (10611/25856)
Train | Batch (196/196) | Top-1: 41.43 (20714/50000)
Regular: 0.3425496220588684
Epoche: 2; regular: 0.3425496220588684: flops 69790694
#Filters: 514, #FLOPs: 68.17M | Top-1: 34.29
Epoch 3
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 41.77 (10800/25856)
Train | Batch (196/196) | Top-1: 41.58 (20790/50000)
Regular: 0.32784438133239746
Epoche: 3; regular: 0.32784438133239746: flops 69790694
#Filters: 514, #FLOPs: 67.91M | Top-1: 15.34
Epoch 4
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 41.54 (10740/25856)
Train | Batch (196/196) | Top-1: 41.77 (20884/50000)
Regular: 0.3249288499355316
Epoche: 4; regular: 0.3249288499355316: flops 69790694
#Filters: 512, #FLOPs: 67.49M | Top-1: 18.51
Epoch 5
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 41.77 (10799/25856)
Train | Batch (196/196) | Top-1: 42.14 (21071/50000)
Regular: 0.3292769491672516
Epoche: 5; regular: 0.3292769491672516: flops 69790694
#Filters: 513, #FLOPs: 67.43M | Top-1: 27.47
Epoch 6
Train | Batch (1/196) | Top-1: 41.02 (105/256)
Train | Batch (101/196) | Top-1: 42.22 (10917/25856)
Train | Batch (196/196) | Top-1: 42.20 (21100/50000)
Regular: 0.33151283860206604
Epoche: 6; regular: 0.33151283860206604: flops 69790694
#Filters: 275, #FLOPs: 43.63M | Top-1: 23.14
Epoch 7
Train | Batch (1/196) | Top-1: 44.53 (114/256)
Train | Batch (101/196) | Top-1: 41.54 (10741/25856)
Train | Batch (196/196) | Top-1: 41.94 (20969/50000)
Regular: 0.31381306052207947
Epoche: 7; regular: 0.31381306052207947: flops 69790694
#Filters: 515, #FLOPs: 68.12M | Top-1: 36.98
Epoch 8
Train | Batch (1/196) | Top-1: 41.80 (107/256)
Train | Batch (101/196) | Top-1: 41.67 (10773/25856)
Train | Batch (196/196) | Top-1: 42.25 (21125/50000)
Regular: 0.30703794956207275
Epoche: 8; regular: 0.30703794956207275: flops 69790694
#Filters: 366, #FLOPs: 37.39M | Top-1: 20.07
Epoch 9
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 42.20 (10910/25856)
Train | Batch (196/196) | Top-1: 42.36 (21181/50000)
Regular: 0.3127222955226898
Epoche: 9; regular: 0.3127222955226898: flops 69790694
#Filters: 513, #FLOPs: 67.70M | Top-1: 11.91
Epoch 10
Train | Batch (1/196) | Top-1: 44.14 (113/256)
Train | Batch (101/196) | Top-1: 42.96 (11107/25856)
Train | Batch (196/196) | Top-1: 42.82 (21412/50000)
Regular: 0.30591070652008057
Epoche: 10; regular: 0.30591070652008057: flops 69790694
#Filters: 512, #FLOPs: 67.75M | Top-1: 10.54
Epoch 11
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 41.92 (10840/25856)
Train | Batch (196/196) | Top-1: 42.11 (21056/50000)
Regular: 0.3148549199104309
Epoche: 11; regular: 0.3148549199104309: flops 69790694
#Filters: 512, #FLOPs: 67.49M | Top-1: 13.58
Epoch 12
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 43.39 (11220/25856)
Train | Batch (196/196) | Top-1: 43.22 (21609/50000)
Regular: 0.3037192225456238
Epoche: 12; regular: 0.3037192225456238: flops 69790694
#Filters: 512, #FLOPs: 67.75M | Top-1: 34.45
Epoch 13
Train | Batch (1/196) | Top-1: 39.84 (102/256)
Train | Batch (101/196) | Top-1: 42.77 (11059/25856)
Train | Batch (196/196) | Top-1: 42.10 (21052/50000)
Regular: 0.3323690593242645
Epoche: 13; regular: 0.3323690593242645: flops 69790694
#Filters: 513, #FLOPs: 67.96M | Top-1: 16.32
Epoch 14
Train | Batch (1/196) | Top-1: 40.23 (103/256)
Train | Batch (101/196) | Top-1: 42.84 (11078/25856)
Train | Batch (196/196) | Top-1: 43.10 (21552/50000)
Regular: 0.3112916648387909
Epoche: 14; regular: 0.3112916648387909: flops 69790694
#Filters: 126, #FLOPs: 13.05M | Top-1: 15.46
Epoch 15
Train | Batch (1/196) | Top-1: 32.03 (82/256)
Train | Batch (101/196) | Top-1: 42.54 (10998/25856)
Train | Batch (196/196) | Top-1: 42.82 (21408/50000)
Regular: 0.30127808451652527
Epoche: 15; regular: 0.30127808451652527: flops 69790694
#Filters: 511, #FLOPs: 67.54M | Top-1: 15.91
Epoch 16
Train | Batch (1/196) | Top-1: 39.06 (100/256)
Train | Batch (101/196) | Top-1: 42.83 (11074/25856)
Train | Batch (196/196) | Top-1: 43.08 (21540/50000)
Regular: 0.32440412044525146
Epoche: 16; regular: 0.32440412044525146: flops 69790694
#Filters: 513, #FLOPs: 67.96M | Top-1: 24.64
Epoch 17
Train | Batch (1/196) | Top-1: 38.67 (99/256)
Train | Batch (101/196) | Top-1: 42.93 (11099/25856)
Train | Batch (196/196) | Top-1: 43.14 (21569/50000)
Regular: 0.31231921911239624
Epoche: 17; regular: 0.31231921911239624: flops 69790694
#Filters: 515, #FLOPs: 68.12M | Top-1: 23.16
Epoch 18
Train | Batch (1/196) | Top-1: 42.58 (109/256)
Train | Batch (101/196) | Top-1: 42.89 (11090/25856)
Train | Batch (196/196) | Top-1: 42.72 (21361/50000)
Regular: 0.3116588294506073
Epoche: 18; regular: 0.3116588294506073: flops 69790694
#Filters: 514, #FLOPs: 67.91M | Top-1: 26.21
Epoch 19
Train | Batch (1/196) | Top-1: 44.53 (114/256)
Train | Batch (101/196) | Top-1: 43.00 (11118/25856)
Train | Batch (196/196) | Top-1: 43.14 (21572/50000)
Regular: 0.30775779485702515
Epoche: 19; regular: 0.30775779485702515: flops 69790694
#Filters: 514, #FLOPs: 67.91M | Top-1: 29.44
Epoch 20
Train | Batch (1/196) | Top-1: 43.36 (111/256)
Train | Batch (101/196) | Top-1: 42.38 (10959/25856)
Train | Batch (196/196) | Top-1: 42.66 (21328/50000)
Regular: 0.30978062748908997
Epoche: 20; regular: 0.30978062748908997: flops 69790694
#Filters: 365, #FLOPs: 37.60M | Top-1: 36.02
Epoch 21
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 42.38 (10957/25856)
Train | Batch (196/196) | Top-1: 42.51 (21254/50000)
Regular: 0.3238636553287506
Epoche: 21; regular: 0.3238636553287506: flops 69790694
#Filters: 274, #FLOPs: 43.41M | Top-1: 21.20
Epoch 22
Train | Batch (1/196) | Top-1: 42.58 (109/256)
Train | Batch (101/196) | Top-1: 43.23 (11178/25856)
Train | Batch (196/196) | Top-1: 43.25 (21626/50000)
Regular: 0.32383668422698975
Epoche: 22; regular: 0.32383668422698975: flops 69790694
#Filters: 365, #FLOPs: 37.86M | Top-1: 27.84
Epoch 23
Train | Batch (1/196) | Top-1: 41.41 (106/256)
Train | Batch (101/196) | Top-1: 42.54 (10998/25856)
Train | Batch (196/196) | Top-1: 42.98 (21489/50000)
Regular: 0.3252009153366089
Epoche: 23; regular: 0.3252009153366089: flops 69790694
#Filters: 366, #FLOPs: 37.23M | Top-1: 27.66
Epoch 24
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 42.88 (11088/25856)
Train | Batch (196/196) | Top-1: 42.99 (21495/50000)
Regular: 0.3299153447151184
Epoche: 24; regular: 0.3299153447151184: flops 69790694
#Filters: 514, #FLOPs: 67.64M | Top-1: 18.09
Epoch 25
Train | Batch (1/196) | Top-1: 39.84 (102/256)
Train | Batch (101/196) | Top-1: 42.86 (11082/25856)
Train | Batch (196/196) | Top-1: 43.02 (21509/50000)
Regular: 0.3311924934387207
Epoche: 25; regular: 0.3311924934387207: flops 69790694
#Filters: 366, #FLOPs: 37.39M | Top-1: 19.99
Epoch 26
Train | Batch (1/196) | Top-1: 37.11 (95/256)
Train | Batch (101/196) | Top-1: 43.19 (11168/25856)
Train | Batch (196/196) | Top-1: 43.09 (21544/50000)
Regular: 0.32650479674339294
Epoche: 26; regular: 0.32650479674339294: flops 69790694
#Filters: 126, #FLOPs: 12.89M | Top-1: 15.08
Epoch 27
Train | Batch (1/196) | Top-1: 44.14 (113/256)
Train | Batch (101/196) | Top-1: 42.80 (11066/25856)
Train | Batch (196/196) | Top-1: 42.74 (21372/50000)
Regular: 0.3254726529121399
Epoche: 27; regular: 0.3254726529121399: flops 69790694
#Filters: 511, #FLOPs: 67.27M | Top-1: 22.89
Epoch 28
Train | Batch (1/196) | Top-1: 43.75 (112/256)
Train | Batch (101/196) | Top-1: 42.93 (11100/25856)
Train | Batch (196/196) | Top-1: 43.22 (21610/50000)
Regular: 0.333177775144577
Epoche: 28; regular: 0.333177775144577: flops 69790694
#Filters: 364, #FLOPs: 37.65M | Top-1: 13.61
Epoch 29
Train | Batch (1/196) | Top-1: 39.06 (100/256)
Train | Batch (101/196) | Top-1: 41.09 (10624/25856)
Train | Batch (196/196) | Top-1: 41.73 (20863/50000)
Regular: 0.43680739402770996
Epoche: 29; regular: 0.43680739402770996: flops 69790694
#Filters: 126, #FLOPs: 12.89M | Top-1: 24.59
Drin!!
Layers that will be prunned: [(0, 16), (1, 25), (2, 16), (3, 25), (4, 16), (5, 51), (6, 16), (7, 24), (8, 16), (9, 21), (10, 16), (11, 1), (12, 16), (13, 5), (14, 16), (15, 5), (16, 16), (17, 5), (18, 16), (19, 2), (20, 16), (21, 7), (22, 16), (23, 5), (24, 16), (25, 5), (26, 16), (27, 5), (28, 16), (29, 16), (30, 16)]
Prunning filters..
Layer index: 0; Pruned filters: 16
Layer index: 2; Pruned filters: 16
Layer index: 4; Pruned filters: 16
Layer index: 6; Pruned filters: 16
Layer index: 8; Pruned filters: 16
Layer index: 10; Pruned filters: 16
Layer index: 12; Pruned filters: 16
Layer index: 14; Pruned filters: 16
Layer index: 16; Pruned filters: 16
Layer index: 18; Pruned filters: 16
Layer index: 20; Pruned filters: 16
Layer index: 22; Pruned filters: 16
Layer index: 24; Pruned filters: 16
Layer index: 26; Pruned filters: 16
Layer index: 28; Pruned filters: 16
Layer index: 30; Pruned filters: 16
Layer index: 1; Pruned filters: 25
Layer index: 3; Pruned filters: 25
Layer index: 5; Pruned filters: 51
Layer index: 7; Pruned filters: 24
Layer index: 9; Pruned filters: 21
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 5
Layer index: 15; Pruned filters: 5
Layer index: 17; Pruned filters: 5
Layer index: 19; Pruned filters: 2
Layer index: 21; Pruned filters: 7
Layer index: 23; Pruned filters: 5
Layer index: 25; Pruned filters: 5
Layer index: 27; Pruned filters: 5
Layer index: 29; Pruned filters: 16
Target (flops): 68.863M
After Pruning | FLOPs: 1.516M | #Params: 0.004M
7.14314039050531
After Growth | FLOPs: 67.565M | #Params: 0.204M
I: 3
flops: 67565300
Before Pruning | FLOPs: 67.565M | #Params: 0.204M
Epoch 0
Train | Batch (1/196) | Top-1: 44.92 (115/256)
Train | Batch (101/196) | Top-1: 42.10 (10885/25856)
Train | Batch (196/196) | Top-1: 41.97 (20985/50000)
Regular: 3.190781593322754
Epoche: 0; regular: 3.190781593322754: flops 67565300
#Filters: 810, #FLOPs: 55.33M | Top-1: 15.04
Epoch 1
Train | Batch (1/196) | Top-1: 39.06 (100/256)
Train | Batch (101/196) | Top-1: 41.31 (10682/25856)
Train | Batch (196/196) | Top-1: 41.51 (20754/50000)
Regular: 0.32437634468078613
Epoche: 1; regular: 0.32437634468078613: flops 67565300
#Filters: 123, #FLOPs: 10.59M | Top-1: 32.90
Epoch 2
Train | Batch (1/196) | Top-1: 44.14 (113/256)
Train | Batch (101/196) | Top-1: 41.94 (10843/25856)
Train | Batch (196/196) | Top-1: 41.75 (20875/50000)
Regular: 0.26558661460876465
Epoche: 2; regular: 0.26558661460876465: flops 67565300
#Filters: 126, #FLOPs: 9.55M | Top-1: 25.96
Epoch 3
Train | Batch (1/196) | Top-1: 39.06 (100/256)
Train | Batch (101/196) | Top-1: 42.01 (10861/25856)
Train | Batch (196/196) | Top-1: 41.73 (20866/50000)
Regular: 0.2562999427318573
Epoche: 3; regular: 0.2562999427318573: flops 67565300
#Filters: 167, #FLOPs: 27.18M | Top-1: 40.00
Epoch 4
Train | Batch (1/196) | Top-1: 44.14 (113/256)
Train | Batch (101/196) | Top-1: 41.92 (10838/25856)
Train | Batch (196/196) | Top-1: 41.68 (20842/50000)
Regular: 0.2574332058429718
Epoche: 4; regular: 0.2574332058429718: flops 67565300
#Filters: 167, #FLOPs: 27.18M | Top-1: 26.13
Epoch 5
Train | Batch (1/196) | Top-1: 37.11 (95/256)
Train | Batch (101/196) | Top-1: 41.69 (10780/25856)
Train | Batch (196/196) | Top-1: 41.61 (20804/50000)
Regular: 0.25594913959503174
Epoche: 5; regular: 0.25594913959503174: flops 67565300
#Filters: 165, #FLOPs: 26.25M | Top-1: 22.31
Epoch 6
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 42.04 (10870/25856)
Train | Batch (196/196) | Top-1: 41.76 (20878/50000)
Regular: 0.2551894783973694
Epoche: 6; regular: 0.2551894783973694: flops 67565300
#Filters: 166, #FLOPs: 26.14M | Top-1: 31.40
Epoch 7
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 41.90 (10834/25856)
Train | Batch (196/196) | Top-1: 41.93 (20965/50000)
Regular: 0.256756991147995
Epoche: 7; regular: 0.256756991147995: flops 67565300
#Filters: 168, #FLOPs: 27.06M | Top-1: 32.48
Epoch 8
Train | Batch (1/196) | Top-1: 40.62 (104/256)
Train | Batch (101/196) | Top-1: 41.50 (10729/25856)
Train | Batch (196/196) | Top-1: 41.63 (20817/50000)
Regular: 0.2563566565513611
Epoche: 8; regular: 0.2563566565513611: flops 67565300
#Filters: 167, #FLOPs: 27.18M | Top-1: 32.69
Epoch 9
Train | Batch (1/196) | Top-1: 45.31 (116/256)
Train | Batch (101/196) | Top-1: 41.65 (10768/25856)
Train | Batch (196/196) | Top-1: 41.75 (20873/50000)
Regular: 0.2566635012626648
Epoche: 9; regular: 0.2566635012626648: flops 67565300
#Filters: 166, #FLOPs: 26.71M | Top-1: 23.07
Epoch 10
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 41.58 (10752/25856)
Train | Batch (196/196) | Top-1: 41.78 (20890/50000)
Regular: 0.25610578060150146
Epoche: 10; regular: 0.25610578060150146: flops 67565300
#Filters: 168, #FLOPs: 27.06M | Top-1: 10.91
Epoch 11
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 41.62 (10761/25856)
Train | Batch (196/196) | Top-1: 41.78 (20892/50000)
Regular: 0.2548862397670746
Epoche: 11; regular: 0.2548862397670746: flops 67565300
#Filters: 167, #FLOPs: 26.60M | Top-1: 41.02
Epoch 12
Train | Batch (1/196) | Top-1: 48.83 (125/256)
Train | Batch (101/196) | Top-1: 41.43 (10712/25856)
Train | Batch (196/196) | Top-1: 41.66 (20832/50000)
Regular: 0.2567240595817566
Epoche: 12; regular: 0.2567240595817566: flops 67565300
#Filters: 167, #FLOPs: 26.60M | Top-1: 21.20
Epoch 13
Train | Batch (1/196) | Top-1: 40.23 (103/256)
Train | Batch (101/196) | Top-1: 41.31 (10681/25856)
Train | Batch (196/196) | Top-1: 41.73 (20867/50000)
Regular: 0.25563284754753113
Epoche: 13; regular: 0.25563284754753113: flops 67565300
#Filters: 167, #FLOPs: 27.18M | Top-1: 19.52
Epoch 14
Train | Batch (1/196) | Top-1: 41.80 (107/256)
Train | Batch (101/196) | Top-1: 41.85 (10821/25856)
Train | Batch (196/196) | Top-1: 41.93 (20966/50000)
Regular: 0.2545453608036041
Epoche: 14; regular: 0.2545453608036041: flops 67565300
#Filters: 166, #FLOPs: 26.71M | Top-1: 26.04
Epoch 15
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 41.96 (10849/25856)
Train | Batch (196/196) | Top-1: 41.96 (20980/50000)
Regular: 0.25424617528915405
Epoche: 15; regular: 0.25424617528915405: flops 67565300
#Filters: 167, #FLOPs: 27.18M | Top-1: 36.41
Epoch 16
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 41.92 (10838/25856)
Train | Batch (196/196) | Top-1: 42.07 (21033/50000)
Regular: 0.2531068027019501
Epoche: 16; regular: 0.2531068027019501: flops 67565300
#Filters: 166, #FLOPs: 26.71M | Top-1: 30.54
Epoch 17
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 41.65 (10768/25856)
Train | Batch (196/196) | Top-1: 42.06 (21031/50000)
Regular: 0.2544960081577301
Epoche: 17; regular: 0.2544960081577301: flops 67565300
#Filters: 166, #FLOPs: 26.71M | Top-1: 18.57
Epoch 18
Train | Batch (1/196) | Top-1: 36.72 (94/256)
Train | Batch (101/196) | Top-1: 41.81 (10810/25856)
Train | Batch (196/196) | Top-1: 41.89 (20943/50000)
Regular: 0.2540022134780884
Epoche: 18; regular: 0.2540022134780884: flops 67565300
#Filters: 166, #FLOPs: 26.71M | Top-1: 29.13
Epoch 19
Train | Batch (1/196) | Top-1: 34.38 (88/256)
Train | Batch (101/196) | Top-1: 42.03 (10866/25856)
Train | Batch (196/196) | Top-1: 41.98 (20988/50000)
Regular: 0.25283992290496826
Epoche: 19; regular: 0.25283992290496826: flops 67565300
#Filters: 166, #FLOPs: 26.71M | Top-1: 33.77
Epoch 20
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 42.23 (10920/25856)
Train | Batch (196/196) | Top-1: 42.04 (21018/50000)
Regular: 0.25566598773002625
Epoche: 20; regular: 0.25566598773002625: flops 67565300
#Filters: 165, #FLOPs: 26.25M | Top-1: 32.03
Epoch 21
Train | Batch (1/196) | Top-1: 40.62 (104/256)
Train | Batch (101/196) | Top-1: 41.62 (10761/25856)
Train | Batch (196/196) | Top-1: 41.89 (20943/50000)
Regular: 0.25311118364334106
Epoche: 21; regular: 0.25311118364334106: flops 67565300
#Filters: 165, #FLOPs: 26.25M | Top-1: 23.33
Epoch 22
Train | Batch (1/196) | Top-1: 42.97 (110/256)
Train | Batch (101/196) | Top-1: 42.24 (10921/25856)
Train | Batch (196/196) | Top-1: 41.97 (20986/50000)
Regular: 0.2557911276817322
Epoche: 22; regular: 0.2557911276817322: flops 67565300
#Filters: 167, #FLOPs: 26.60M | Top-1: 30.07
Epoch 23
Train | Batch (1/196) | Top-1: 39.84 (102/256)
Train | Batch (101/196) | Top-1: 41.83 (10816/25856)
Train | Batch (196/196) | Top-1: 41.99 (20995/50000)
Regular: 0.254651814699173
Epoche: 23; regular: 0.254651814699173: flops 67565300
#Filters: 167, #FLOPs: 27.18M | Top-1: 38.68
Epoch 24
Train | Batch (1/196) | Top-1: 48.83 (125/256)
Train | Batch (101/196) | Top-1: 42.25 (10925/25856)
Train | Batch (196/196) | Top-1: 42.19 (21095/50000)
Regular: 0.25370272994041443
Epoche: 24; regular: 0.25370272994041443: flops 67565300
#Filters: 166, #FLOPs: 26.71M | Top-1: 33.95
Epoch 25
Train | Batch (1/196) | Top-1: 42.58 (109/256)
Train | Batch (101/196) | Top-1: 42.29 (10934/25856)
Train | Batch (196/196) | Top-1: 42.22 (21112/50000)
Regular: 0.25582003593444824
Epoche: 25; regular: 0.25582003593444824: flops 67565300
#Filters: 166, #FLOPs: 26.14M | Top-1: 30.35
Epoch 26
Train | Batch (1/196) | Top-1: 41.80 (107/256)
Train | Batch (101/196) | Top-1: 42.27 (10930/25856)
Train | Batch (196/196) | Top-1: 42.19 (21094/50000)
Regular: 0.2577871084213257
Epoche: 26; regular: 0.2577871084213257: flops 67565300
#Filters: 167, #FLOPs: 27.18M | Top-1: 23.16
Epoch 27
Train | Batch (1/196) | Top-1: 37.89 (97/256)
Train | Batch (101/196) | Top-1: 42.26 (10928/25856)
Train | Batch (196/196) | Top-1: 42.15 (21077/50000)
Regular: 0.2554587423801422
Epoche: 27; regular: 0.2554587423801422: flops 67565300
#Filters: 168, #FLOPs: 27.06M | Top-1: 22.14
Epoch 28
Train | Batch (1/196) | Top-1: 43.36 (111/256)
Train | Batch (101/196) | Top-1: 42.23 (10919/25856)
Train | Batch (196/196) | Top-1: 42.27 (21135/50000)
Regular: 0.2543599307537079
Epoche: 28; regular: 0.2543599307537079: flops 67565300
#Filters: 167, #FLOPs: 27.18M | Top-1: 29.52
Epoch 29
Train | Batch (1/196) | Top-1: 38.67 (99/256)
Train | Batch (101/196) | Top-1: 42.34 (10947/25856)
Train | Batch (196/196) | Top-1: 42.38 (21190/50000)
Regular: 0.25613459944725037
Epoche: 29; regular: 0.25613459944725037: flops 67565300
#Filters: 167, #FLOPs: 27.18M | Top-1: 40.89
Drin!!
Layers that will be prunned: [(0, 43), (2, 43), (4, 43), (6, 43), (7, 2), (8, 43), (9, 2), (10, 43), (11, 12), (12, 43), (13, 12), (14, 43), (15, 12), (16, 43), (17, 12), (18, 43), (19, 5), (20, 43), (21, 18), (22, 43), (23, 12), (24, 43), (25, 12), (26, 43), (27, 12), (28, 43), (29, 43), (30, 43)]
Prunning filters..
Layer index: 0; Pruned filters: 43
Layer index: 2; Pruned filters: 43
Layer index: 4; Pruned filters: 43
Layer index: 6; Pruned filters: 43
Layer index: 8; Pruned filters: 43
Layer index: 10; Pruned filters: 43
Layer index: 12; Pruned filters: 43
Layer index: 14; Pruned filters: 43
Layer index: 16; Pruned filters: 43
Layer index: 18; Pruned filters: 43
Layer index: 20; Pruned filters: 43
Layer index: 22; Pruned filters: 43
Layer index: 24; Pruned filters: 43
Layer index: 26; Pruned filters: 43
Layer index: 28; Pruned filters: 43
Layer index: 30; Pruned filters: 43
Layer index: 7; Pruned filters: 2
Layer index: 9; Pruned filters: 2
Layer index: 11; Pruned filters: 12
Layer index: 13; Pruned filters: 12
Layer index: 15; Pruned filters: 12
Layer index: 17; Pruned filters: 12
Layer index: 19; Pruned filters: 5
Layer index: 21; Pruned filters: 18
Layer index: 23; Pruned filters: 12
Layer index: 25; Pruned filters: 12
Layer index: 27; Pruned filters: 12
Layer index: 29; Pruned filters: 43
Target (flops): 68.863M
After Pruning | FLOPs: 6.451M | #Params: 0.009M
3.301872775334812
After Growth | FLOPs: 68.943M | #Params: 0.099M
I: 4
flops: 68942822
Before Pruning | FLOPs: 68.943M | #Params: 0.099M
Epoch 0
Train | Batch (1/196) | Top-1: 44.53 (114/256)
Train | Batch (101/196) | Top-1: 42.92 (11097/25856)
Train | Batch (196/196) | Top-1: 42.42 (21211/50000)
Regular: 1.8821961879730225
Epoche: 0; regular: 1.8821961879730225: flops 68942822
#Filters: 379, #FLOPs: 37.44M | Top-1: 13.43
Epoch 1
Train | Batch (1/196) | Top-1: 36.72 (94/256)
Train | Batch (101/196) | Top-1: 42.55 (11003/25856)
Train | Batch (196/196) | Top-1: 42.29 (21145/50000)
Regular: 0.3405399024486542
Epoche: 1; regular: 0.3405399024486542: flops 68942822
#Filters: 509, #FLOPs: 66.69M | Top-1: 24.89
Epoch 2
Train | Batch (1/196) | Top-1: 35.94 (92/256)
Train | Batch (101/196) | Top-1: 42.23 (10918/25856)
Train | Batch (196/196) | Top-1: 42.75 (21374/50000)
Regular: 0.31311145424842834
Epoche: 2; regular: 0.31311145424842834: flops 68942822
#Filters: 469, #FLOPs: 58.21M | Top-1: 29.40
Epoch 3
Train | Batch (1/196) | Top-1: 41.41 (106/256)
Train | Batch (101/196) | Top-1: 42.26 (10928/25856)
Train | Batch (196/196) | Top-1: 42.57 (21285/50000)
Regular: 0.29375559091567993
Epoche: 3; regular: 0.29375559091567993: flops 68942822
#Filters: 467, #FLOPs: 57.79M | Top-1: 20.75
Epoch 4
Train | Batch (1/196) | Top-1: 40.62 (104/256)
Train | Batch (101/196) | Top-1: 42.41 (10966/25856)
Train | Batch (196/196) | Top-1: 42.77 (21387/50000)
Regular: 0.29144346714019775
Epoche: 4; regular: 0.29144346714019775: flops 68942822
#Filters: 510, #FLOPs: 66.90M | Top-1: 23.95
Epoch 5
Train | Batch (1/196) | Top-1: 40.23 (103/256)
Train | Batch (101/196) | Top-1: 42.13 (10892/25856)
Train | Batch (196/196) | Top-1: 42.52 (21260/50000)
Regular: 0.2991319000720978
Epoche: 5; regular: 0.2991319000720978: flops 68942822
#Filters: 508, #FLOPs: 66.48M | Top-1: 40.64
Epoch 6
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 42.23 (10920/25856)
Train | Batch (196/196) | Top-1: 42.53 (21264/50000)
Regular: 0.31978219747543335
Epoche: 6; regular: 0.31978219747543335: flops 68942822
#Filters: 509, #FLOPs: 66.69M | Top-1: 12.78
Epoch 7
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 42.28 (10932/25856)
Train | Batch (196/196) | Top-1: 42.61 (21303/50000)
Regular: 0.3049299120903015
Epoche: 7; regular: 0.3049299120903015: flops 68942822
#Filters: 510, #FLOPs: 66.90M | Top-1: 32.40
Epoch 8
Train | Batch (1/196) | Top-1: 45.70 (117/256)
Train | Batch (101/196) | Top-1: 42.83 (11074/25856)
Train | Batch (196/196) | Top-1: 42.75 (21373/50000)
Regular: 0.2964700162410736
Epoche: 8; regular: 0.2964700162410736: flops 68942822
#Filters: 509, #FLOPs: 66.69M | Top-1: 33.39
Epoch 9
Train | Batch (1/196) | Top-1: 39.45 (101/256)
Train | Batch (101/196) | Top-1: 42.64 (11025/25856)
Train | Batch (196/196) | Top-1: 42.82 (21412/50000)
Regular: 0.3021678924560547
Epoche: 9; regular: 0.3021678924560547: flops 68942822
#Filters: 467, #FLOPs: 57.79M | Top-1: 36.70
Epoch 10
Train | Batch (1/196) | Top-1: 45.70 (117/256)
Train | Batch (101/196) | Top-1: 42.83 (11073/25856)
Train | Batch (196/196) | Top-1: 42.69 (21343/50000)
Regular: 0.28767168521881104
Epoche: 10; regular: 0.28767168521881104: flops 68942822
#Filters: 510, #FLOPs: 66.90M | Top-1: 26.16
Epoch 11
Train | Batch (1/196) | Top-1: 42.97 (110/256)
Train | Batch (101/196) | Top-1: 43.24 (11180/25856)
Train | Batch (196/196) | Top-1: 42.96 (21479/50000)
Regular: 0.28978052735328674
Epoche: 11; regular: 0.28978052735328674: flops 68942822
#Filters: 510, #FLOPs: 66.90M | Top-1: 16.33
Epoch 12
Train | Batch (1/196) | Top-1: 39.45 (101/256)
Train | Batch (101/196) | Top-1: 42.93 (11099/25856)
Train | Batch (196/196) | Top-1: 42.92 (21458/50000)
Regular: 0.29001688957214355
Epoche: 12; regular: 0.29001688957214355: flops 68942822
#Filters: 509, #FLOPs: 66.69M | Top-1: 24.45
Epoch 13
Train | Batch (1/196) | Top-1: 35.16 (90/256)
Train | Batch (101/196) | Top-1: 42.89 (11089/25856)
Train | Batch (196/196) | Top-1: 43.00 (21502/50000)
Regular: 0.2868177890777588
Epoche: 13; regular: 0.2868177890777588: flops 68942822
#Filters: 508, #FLOPs: 66.48M | Top-1: 20.77
Epoch 14
Train | Batch (1/196) | Top-1: 41.02 (105/256)
Train | Batch (101/196) | Top-1: 42.77 (11058/25856)
Train | Batch (196/196) | Top-1: 42.80 (21399/50000)
Regular: 0.2891755700111389
Epoche: 14; regular: 0.2891755700111389: flops 68942822
#Filters: 510, #FLOPs: 66.90M | Top-1: 38.33
Epoch 15
Train | Batch (1/196) | Top-1: 41.02 (105/256)
Train | Batch (101/196) | Top-1: 41.76 (10798/25856)
Train | Batch (196/196) | Top-1: 42.19 (21096/50000)
Regular: 0.2975446283817291
Epoche: 15; regular: 0.2975446283817291: flops 68942822
#Filters: 467, #FLOPs: 57.79M | Top-1: 32.03
Epoch 16
Train | Batch (1/196) | Top-1: 43.36 (111/256)
Train | Batch (101/196) | Top-1: 42.97 (11110/25856)
Train | Batch (196/196) | Top-1: 42.67 (21336/50000)
Regular: 0.2948867678642273
Epoche: 16; regular: 0.2948867678642273: flops 68942822
#Filters: 509, #FLOPs: 66.69M | Top-1: 25.03
Epoch 17
Train | Batch (1/196) | Top-1: 42.58 (109/256)
Train | Batch (101/196) | Top-1: 43.59 (11270/25856)
Train | Batch (196/196) | Top-1: 43.44 (21719/50000)
Regular: 0.2987314462661743
Epoche: 17; regular: 0.2987314462661743: flops 68942822
#Filters: 510, #FLOPs: 66.90M | Top-1: 25.25
Epoch 18
Train | Batch (1/196) | Top-1: 39.45 (101/256)
Train | Batch (101/196) | Top-1: 43.47 (11239/25856)
Train | Batch (196/196) | Top-1: 43.07 (21536/50000)
Regular: 0.294625461101532
Epoche: 18; regular: 0.294625461101532: flops 68942822
#Filters: 509, #FLOPs: 66.69M | Top-1: 22.38
Epoch 19
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 42.78 (11060/25856)
Train | Batch (196/196) | Top-1: 42.95 (21476/50000)
Regular: 0.29494908452033997
Epoche: 19; regular: 0.29494908452033997: flops 68942822
#Filters: 510, #FLOPs: 66.90M | Top-1: 19.43
Epoch 20
Train | Batch (1/196) | Top-1: 41.41 (106/256)
Train | Batch (101/196) | Top-1: 42.55 (11001/25856)
Train | Batch (196/196) | Top-1: 42.31 (21154/50000)
Regular: 0.30236223340034485
Epoche: 20; regular: 0.30236223340034485: flops 68942822
#Filters: 405, #FLOPs: 44.65M | Top-1: 13.75
Epoch 21
Train | Batch (1/196) | Top-1: 43.75 (112/256)
Train | Batch (101/196) | Top-1: 42.91 (11095/25856)
Train | Batch (196/196) | Top-1: 42.89 (21445/50000)
Regular: 0.2871983051300049
Epoche: 21; regular: 0.2871983051300049: flops 68942822
#Filters: 364, #FLOPs: 37.23M | Top-1: 21.76
Epoch 22
Train | Batch (1/196) | Top-1: 38.67 (99/256)
Train | Batch (101/196) | Top-1: 42.98 (11113/25856)
Train | Batch (196/196) | Top-1: 42.87 (21433/50000)
Regular: 0.28732991218566895
Epoche: 22; regular: 0.28732991218566895: flops 68942822
#Filters: 509, #FLOPs: 66.69M | Top-1: 26.69
Epoch 23
Train | Batch (1/196) | Top-1: 45.70 (117/256)
Train | Batch (101/196) | Top-1: 43.00 (11118/25856)
Train | Batch (196/196) | Top-1: 43.03 (21515/50000)
Regular: 0.29548585414886475
Epoche: 23; regular: 0.29548585414886475: flops 68942822
#Filters: 405, #FLOPs: 44.65M | Top-1: 36.93
Epoch 24
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 43.28 (11191/25856)
Train | Batch (196/196) | Top-1: 43.28 (21638/50000)
Regular: 0.28048571944236755
Epoche: 24; regular: 0.28048571944236755: flops 68942822
#Filters: 164, #FLOPs: 20.23M | Top-1: 10.05
Epoch 25
Train | Batch (1/196) | Top-1: 42.97 (110/256)
Train | Batch (101/196) | Top-1: 40.91 (10578/25856)
Train | Batch (196/196) | Top-1: 41.79 (20896/50000)
Regular: 0.3498281240463257
Epoche: 25; regular: 0.3498281240463257: flops 68942822
#Filters: 467, #FLOPs: 57.79M | Top-1: 12.92
Epoch 26
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 42.57 (11008/25856)
Train | Batch (196/196) | Top-1: 42.76 (21381/50000)
Regular: 0.305890291929245
Epoche: 26; regular: 0.305890291929245: flops 68942822
#Filters: 364, #FLOPs: 37.23M | Top-1: 29.64
Epoch 27
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 42.09 (10882/25856)
Train | Batch (196/196) | Top-1: 42.74 (21370/50000)
Regular: 0.3173276484012604
Epoche: 27; regular: 0.3173276484012604: flops 68942822
#Filters: 167, #FLOPs: 20.87M | Top-1: 36.36
Epoch 28
Train | Batch (1/196) | Top-1: 43.75 (112/256)
Train | Batch (101/196) | Top-1: 42.91 (11095/25856)
Train | Batch (196/196) | Top-1: 42.76 (21381/50000)
Regular: 0.2979538142681122
Epoche: 28; regular: 0.2979538142681122: flops 68942822
#Filters: 469, #FLOPs: 58.21M | Top-1: 39.21
Epoch 29
Train | Batch (1/196) | Top-1: 37.89 (97/256)
Train | Batch (101/196) | Top-1: 42.71 (11042/25856)
Train | Batch (196/196) | Top-1: 42.85 (21426/50000)
Regular: 0.29623842239379883
Epoche: 29; regular: 0.29623842239379883: flops 68942822
#Filters: 407, #FLOPs: 45.07M | Top-1: 31.35
Drin!!
Layers that will be prunned: [(1, 16), (3, 16), (5, 16), (7, 28), (9, 28), (11, 5), (13, 5), (15, 5), (17, 5), (19, 5), (21, 7), (23, 5), (25, 5), (27, 5), (29, 16)]
Prunning filters..
Layer index: 1; Pruned filters: 16
Layer index: 3; Pruned filters: 16
Layer index: 5; Pruned filters: 16
Layer index: 7; Pruned filters: 28
Layer index: 9; Pruned filters: 28
Layer index: 11; Pruned filters: 5
Layer index: 13; Pruned filters: 5
Layer index: 15; Pruned filters: 5
Layer index: 17; Pruned filters: 5
Layer index: 19; Pruned filters: 5
Layer index: 21; Pruned filters: 7
Layer index: 23; Pruned filters: 5
Layer index: 25; Pruned filters: 5
Layer index: 27; Pruned filters: 5
Layer index: 29; Pruned filters: 16
Target (flops): 68.863M
After Pruning | FLOPs: 21.197M | #Params: 0.030M
1.8146783544642824
After Growth | FLOPs: 70.738M | #Params: 0.102M
I: 5
flops: 70737828
Before Pruning | FLOPs: 70.738M | #Params: 0.102M
Epoch 0
Train | Batch (1/196) | Top-1: 42.58 (109/256)
Train | Batch (101/196) | Top-1: 43.25 (11184/25856)
Train | Batch (196/196) | Top-1: 43.02 (21511/50000)
Regular: 1.3745883703231812
Epoche: 0; regular: 1.3745883703231812: flops 70737828
#Filters: 667, #FLOPs: 40.79M | Top-1: 35.63
Epoch 1
Train | Batch (1/196) | Top-1: 39.45 (101/256)
Train | Batch (101/196) | Top-1: 42.64 (11025/25856)
Train | Batch (196/196) | Top-1: 42.98 (21488/50000)
Regular: 0.4186663329601288
Epoche: 1; regular: 0.4186663329601288: flops 70737828
#Filters: 748, #FLOPs: 69.04M | Top-1: 27.36
Epoch 2
Train | Batch (1/196) | Top-1: 44.14 (113/256)
Train | Batch (101/196) | Top-1: 43.20 (11171/25856)
Train | Batch (196/196) | Top-1: 43.14 (21571/50000)
Regular: 0.3353067636489868
Epoche: 2; regular: 0.3353067636489868: flops 70737828
#Filters: 489, #FLOPs: 55.35M | Top-1: 13.18
Epoch 3
Train | Batch (1/196) | Top-1: 38.28 (98/256)
Train | Batch (101/196) | Top-1: 43.08 (11140/25856)
Train | Batch (196/196) | Top-1: 42.97 (21487/50000)
Regular: 0.31303638219833374
Epoche: 3; regular: 0.31303638219833374: flops 70737828
#Filters: 489, #FLOPs: 55.35M | Top-1: 40.56
Epoch 4
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 43.32 (11202/25856)
Train | Batch (196/196) | Top-1: 43.46 (21732/50000)
Regular: 0.31204041838645935
Epoche: 4; regular: 0.31204041838645935: flops 70737828
#Filters: 730, #FLOPs: 69.43M | Top-1: 26.78
Epoch 5
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 42.79 (11063/25856)
Train | Batch (196/196) | Top-1: 43.11 (21556/50000)
Regular: 0.30475685000419617
Epoche: 5; regular: 0.30475685000419617: flops 70737828
#Filters: 729, #FLOPs: 69.04M | Top-1: 19.02
Epoch 6
Train | Batch (1/196) | Top-1: 36.72 (94/256)
Train | Batch (101/196) | Top-1: 43.18 (11165/25856)
Train | Batch (196/196) | Top-1: 42.66 (21329/50000)
Regular: 0.31496089696884155
Epoche: 6; regular: 0.31496089696884155: flops 70737828
#Filters: 730, #FLOPs: 69.43M | Top-1: 32.05
Epoch 7
Train | Batch (1/196) | Top-1: 39.84 (102/256)
Train | Batch (101/196) | Top-1: 42.91 (11096/25856)
Train | Batch (196/196) | Top-1: 43.04 (21521/50000)
Regular: 0.3040148615837097
Epoche: 7; regular: 0.3040148615837097: flops 70737828
#Filters: 728, #FLOPs: 68.66M | Top-1: 28.34
Epoch 8
Train | Batch (1/196) | Top-1: 45.70 (117/256)
Train | Batch (101/196) | Top-1: 43.39 (11220/25856)
Train | Batch (196/196) | Top-1: 43.39 (21695/50000)
Regular: 0.30391785502433777
Epoche: 8; regular: 0.30391785502433777: flops 70737828
#Filters: 445, #FLOPs: 53.17M | Top-1: 20.68
Epoch 9
Train | Batch (1/196) | Top-1: 35.55 (91/256)
Train | Batch (101/196) | Top-1: 43.17 (11162/25856)
Train | Batch (196/196) | Top-1: 42.83 (21415/50000)
Regular: 0.29685845971107483
Epoche: 9; regular: 0.29685845971107483: flops 70737828
#Filters: 203, #FLOPs: 38.70M | Top-1: 19.63
Epoch 10
Train | Batch (1/196) | Top-1: 44.92 (115/256)
Train | Batch (101/196) | Top-1: 43.63 (11280/25856)
Train | Batch (196/196) | Top-1: 43.31 (21654/50000)
Regular: 0.30240365862846375
Epoche: 10; regular: 0.30240365862846375: flops 70737828
#Filters: 730, #FLOPs: 69.43M | Top-1: 21.84
Epoch 11
Train | Batch (1/196) | Top-1: 41.02 (105/256)
Train | Batch (101/196) | Top-1: 43.23 (11177/25856)
Train | Batch (196/196) | Top-1: 43.13 (21563/50000)
Regular: 0.3023095428943634
Epoche: 11; regular: 0.3023095428943634: flops 70737828
#Filters: 406, #FLOPs: 38.07M | Top-1: 26.63
Epoch 12
Train | Batch (1/196) | Top-1: 36.72 (94/256)
Train | Batch (101/196) | Top-1: 43.48 (11242/25856)
Train | Batch (196/196) | Top-1: 43.35 (21676/50000)
Regular: 0.31975361704826355
Epoche: 12; regular: 0.31975361704826355: flops 70737828
#Filters: 444, #FLOPs: 52.78M | Top-1: 20.87
Epoch 13
Train | Batch (1/196) | Top-1: 40.23 (103/256)
Train | Batch (101/196) | Top-1: 43.06 (11133/25856)
Train | Batch (196/196) | Top-1: 43.11 (21557/50000)
Regular: 0.2989822030067444
Epoche: 13; regular: 0.2989822030067444: flops 70737828
#Filters: 648, #FLOPs: 40.79M | Top-1: 39.03
Epoch 14
Train | Batch (1/196) | Top-1: 46.48 (119/256)
Train | Batch (101/196) | Top-1: 42.98 (11112/25856)
Train | Batch (196/196) | Top-1: 43.42 (21708/50000)
Regular: 0.2954833507537842
Epoche: 14; regular: 0.2954833507537842: flops 70737828
#Filters: 162, #FLOPs: 22.83M | Top-1: 34.71
Epoch 15
Train | Batch (1/196) | Top-1: 39.06 (100/256)
Train | Batch (101/196) | Top-1: 43.09 (11142/25856)
Train | Batch (196/196) | Top-1: 43.23 (21616/50000)
Regular: 0.2893927991390228
Epoche: 15; regular: 0.2893927991390228: flops 70737828
#Filters: 728, #FLOPs: 68.66M | Top-1: 24.85
Epoch 16
Train | Batch (1/196) | Top-1: 47.27 (121/256)
Train | Batch (101/196) | Top-1: 43.39 (11218/25856)
Train | Batch (196/196) | Top-1: 43.22 (21609/50000)
Regular: 0.2941339910030365
Epoche: 16; regular: 0.2941339910030365: flops 70737828
#Filters: 444, #FLOPs: 52.78M | Top-1: 26.20
Epoch 17
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 43.24 (11180/25856)
Train | Batch (196/196) | Top-1: 43.24 (21619/50000)
Regular: 0.35300561785697937
Epoche: 17; regular: 0.35300561785697937: flops 70737828
#Filters: 444, #FLOPs: 52.78M | Top-1: 23.02
Epoch 18
Train | Batch (1/196) | Top-1: 39.45 (101/256)
Train | Batch (101/196) | Top-1: 43.40 (11222/25856)
Train | Batch (196/196) | Top-1: 43.38 (21690/50000)
Regular: 0.2816367745399475
Epoche: 18; regular: 0.2816367745399475: flops 70737828
#Filters: 688, #FLOPs: 53.17M | Top-1: 38.18
Epoch 19
Train | Batch (1/196) | Top-1: 42.97 (110/256)
Train | Batch (101/196) | Top-1: 43.19 (11168/25856)
Train | Batch (196/196) | Top-1: 43.22 (21610/50000)
Regular: 0.2798943519592285
Epoche: 19; regular: 0.2798943519592285: flops 70737828
#Filters: 489, #FLOPs: 55.35M | Top-1: 26.43
Epoch 20
Train | Batch (1/196) | Top-1: 41.80 (107/256)
Train | Batch (101/196) | Top-1: 43.44 (11231/25856)
Train | Batch (196/196) | Top-1: 43.46 (21728/50000)
Regular: 0.32538339495658875
Epoche: 20; regular: 0.32538339495658875: flops 70737828
#Filters: 730, #FLOPs: 69.43M | Top-1: 28.99
Epoch 21
Train | Batch (1/196) | Top-1: 45.31 (116/256)
Train | Batch (101/196) | Top-1: 42.77 (11059/25856)
Train | Batch (196/196) | Top-1: 42.93 (21465/50000)
Regular: 0.325084388256073
Epoche: 21; regular: 0.325084388256073: flops 70737828
#Filters: 729, #FLOPs: 69.04M | Top-1: 23.72
Epoch 22
Train | Batch (1/196) | Top-1: 33.20 (85/256)
Train | Batch (101/196) | Top-1: 43.52 (11252/25856)
Train | Batch (196/196) | Top-1: 43.67 (21834/50000)
Regular: 0.3251608908176422
Epoche: 22; regular: 0.3251608908176422: flops 70737828
#Filters: 729, #FLOPs: 69.04M | Top-1: 33.37
Epoch 23
Train | Batch (1/196) | Top-1: 36.33 (93/256)
Train | Batch (101/196) | Top-1: 43.44 (11232/25856)
Train | Batch (196/196) | Top-1: 43.19 (21595/50000)
Regular: 0.3258886933326721
Epoche: 23; regular: 0.3258886933326721: flops 70737828
#Filters: 730, #FLOPs: 69.43M | Top-1: 36.25
Epoch 24
Train | Batch (1/196) | Top-1: 45.70 (117/256)
Train | Batch (101/196) | Top-1: 43.31 (11197/25856)
Train | Batch (196/196) | Top-1: 43.20 (21601/50000)
Regular: 0.323636919260025
Epoche: 24; regular: 0.323636919260025: flops 70737828
#Filters: 489, #FLOPs: 55.35M | Top-1: 27.44
Epoch 25
Train | Batch (1/196) | Top-1: 40.23 (103/256)
Train | Batch (101/196) | Top-1: 43.48 (11241/25856)
Train | Batch (196/196) | Top-1: 43.31 (21655/50000)
Regular: 0.2920985817909241
Epoche: 25; regular: 0.2920985817909241: flops 70737828
#Filters: 445, #FLOPs: 53.17M | Top-1: 25.09
Epoch 26
Train | Batch (1/196) | Top-1: 39.45 (101/256)
Train | Batch (101/196) | Top-1: 43.49 (11244/25856)
Train | Batch (196/196) | Top-1: 43.59 (21797/50000)
Regular: 0.32141539454460144
Epoche: 26; regular: 0.32141539454460144: flops 70737828
#Filters: 729, #FLOPs: 69.04M | Top-1: 23.80
Epoch 27
Train | Batch (1/196) | Top-1: 45.70 (117/256)
Train | Batch (101/196) | Top-1: 43.02 (11124/25856)
Train | Batch (196/196) | Top-1: 43.31 (21653/50000)
Regular: 0.32178056240081787
Epoche: 27; regular: 0.32178056240081787: flops 70737828
#Filters: 730, #FLOPs: 69.43M | Top-1: 16.87
Epoch 28
Train | Batch (1/196) | Top-1: 38.67 (99/256)
Train | Batch (101/196) | Top-1: 43.17 (11162/25856)
Train | Batch (196/196) | Top-1: 43.18 (21588/50000)
Regular: 0.32234105467796326
Epoche: 28; regular: 0.32234105467796326: flops 70737828
#Filters: 444, #FLOPs: 52.78M | Top-1: 30.21
Epoch 29
Train | Batch (1/196) | Top-1: 48.05 (123/256)
Train | Batch (101/196) | Top-1: 43.55 (11261/25856)
Train | Batch (196/196) | Top-1: 43.25 (21624/50000)
Regular: 0.2997256815433502
Epoche: 29; regular: 0.2997256815433502: flops 70737828
#Filters: 489, #FLOPs: 55.35M | Top-1: 24.63
Drin!!
Layers that will be prunned: [(0, 16), (2, 16), (4, 16), (5, 1), (6, 16), (8, 16), (10, 16), (11, 2), (12, 16), (13, 2), (14, 16), (15, 2), (16, 16), (17, 2), (18, 16), (19, 2), (20, 16), (21, 2), (22, 16), (23, 2), (24, 16), (25, 2), (26, 16), (27, 2), (28, 16), (29, 6), (30, 16)]
Prunning filters..
Layer index: 0; Pruned filters: 16
Layer index: 2; Pruned filters: 16
Layer index: 4; Pruned filters: 16
Layer index: 6; Pruned filters: 16
Layer index: 8; Pruned filters: 16
Layer index: 10; Pruned filters: 16
Layer index: 12; Pruned filters: 16
Layer index: 14; Pruned filters: 16
Layer index: 16; Pruned filters: 16
Layer index: 18; Pruned filters: 16
Layer index: 20; Pruned filters: 16
Layer index: 22; Pruned filters: 16
Layer index: 24; Pruned filters: 16
Layer index: 26; Pruned filters: 16
Layer index: 28; Pruned filters: 16
Layer index: 30; Pruned filters: 16
Layer index: 5; Pruned filters: 1
Layer index: 11; Pruned filters: 2
Layer index: 13; Pruned filters: 2
Layer index: 15; Pruned filters: 2
Layer index: 17; Pruned filters: 2
Layer index: 19; Pruned filters: 2
Layer index: 21; Pruned filters: 2
Layer index: 23; Pruned filters: 2
Layer index: 25; Pruned filters: 2
Layer index: 27; Pruned filters: 2
Layer index: 29; Pruned filters: 6
Target (flops): 68.863M
After Pruning | FLOPs: 41.693M | #Params: 0.052M
1.2876447398825612
After Growth | FLOPs: 67.897M | #Params: 0.086M
Epoch 0
Train | Batch (1/196) | Top-1: 35.16 (90/256)
Train | Batch (101/196) | Top-1: 43.26 (11186/25856)
Train | Batch (196/196) | Top-1: 43.79 (21895/50000)
Regular: nan
Epoche: 0; regular: nan: flops 67896906
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
#Filters: 635, #FLOPs: 67.02M | Top-1: 34.60
Epoch 1
Train | Batch (1/196) | Top-1: 40.62 (104/256)
Train | Batch (101/196) | Top-1: 43.97 (11370/25856)
Train | Batch (196/196) | Top-1: 43.94 (21971/50000)
Regular: nan
Epoche: 1; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 29.18
Epoch 2
Train | Batch (1/196) | Top-1: 41.41 (106/256)
Train | Batch (101/196) | Top-1: 43.48 (11242/25856)
Train | Batch (196/196) | Top-1: 43.84 (21920/50000)
Regular: nan
Epoche: 2; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 39.94
Epoch 3
Train | Batch (1/196) | Top-1: 42.58 (109/256)
Train | Batch (101/196) | Top-1: 43.79 (11322/25856)
Train | Batch (196/196) | Top-1: 43.96 (21980/50000)
Regular: nan
Epoche: 3; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 33.65
Epoch 4
Train | Batch (1/196) | Top-1: 46.09 (118/256)
Train | Batch (101/196) | Top-1: 44.06 (11391/25856)
Train | Batch (196/196) | Top-1: 43.93 (21964/50000)
Regular: nan
Epoche: 4; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 35.78
Epoch 5
Train | Batch (1/196) | Top-1: 42.97 (110/256)
Train | Batch (101/196) | Top-1: 43.78 (11320/25856)
Train | Batch (196/196) | Top-1: 43.97 (21985/50000)
Regular: nan
Epoche: 5; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 40.63
Epoch 6
Train | Batch (1/196) | Top-1: 40.62 (104/256)
Train | Batch (101/196) | Top-1: 44.28 (11448/25856)
Train | Batch (196/196) | Top-1: 44.21 (22107/50000)
Regular: nan
Epoche: 6; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 40.46
Epoch 7
Train | Batch (1/196) | Top-1: 42.97 (110/256)
Train | Batch (101/196) | Top-1: 44.47 (11498/25856)
Train | Batch (196/196) | Top-1: 44.35 (22175/50000)
Regular: nan
Epoche: 7; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 39.48
Epoch 8
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 44.31 (11458/25856)
Train | Batch (196/196) | Top-1: 44.38 (22192/50000)
Regular: nan
Epoche: 8; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 35.56
Epoch 9
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 44.04 (11386/25856)
Train | Batch (196/196) | Top-1: 44.08 (22042/50000)
Regular: nan
Epoche: 9; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 33.33
Epoch 10
Train | Batch (1/196) | Top-1: 39.84 (102/256)
Train | Batch (101/196) | Top-1: 44.17 (11420/25856)
Train | Batch (196/196) | Top-1: 44.50 (22249/50000)
Regular: nan
Epoche: 10; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 43.54
Epoch 11
Train | Batch (1/196) | Top-1: 44.53 (114/256)
Train | Batch (101/196) | Top-1: 44.48 (11502/25856)
Train | Batch (196/196) | Top-1: 44.33 (22163/50000)
Regular: nan
Epoche: 11; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 39.10
Epoch 12
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 44.36 (11471/25856)
Train | Batch (196/196) | Top-1: 44.17 (22084/50000)
Regular: nan
Epoche: 12; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 41.65
Epoch 13
Train | Batch (1/196) | Top-1: 46.09 (118/256)
Train | Batch (101/196) | Top-1: 43.91 (11354/25856)
Train | Batch (196/196) | Top-1: 44.34 (22172/50000)
Regular: nan
Epoche: 13; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 34.98
Epoch 14
Train | Batch (1/196) | Top-1: 45.70 (117/256)
Train | Batch (101/196) | Top-1: 44.25 (11441/25856)
Train | Batch (196/196) | Top-1: 44.41 (22205/50000)
Regular: nan
Epoche: 14; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 40.19
Epoch 15
Train | Batch (1/196) | Top-1: 41.41 (106/256)
Train | Batch (101/196) | Top-1: 44.23 (11435/25856)
Train | Batch (196/196) | Top-1: 44.21 (22105/50000)
Regular: nan
Epoche: 15; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 43.05
Epoch 16
Train | Batch (1/196) | Top-1: 49.61 (127/256)
Train | Batch (101/196) | Top-1: 44.45 (11493/25856)
Train | Batch (196/196) | Top-1: 44.28 (22138/50000)
Regular: nan
Epoche: 16; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 39.58
Epoch 17
Train | Batch (1/196) | Top-1: 47.27 (121/256)
Train | Batch (101/196) | Top-1: 44.73 (11565/25856)
Train | Batch (196/196) | Top-1: 44.61 (22303/50000)
Regular: nan
Epoche: 17; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 42.58
Epoch 18
Train | Batch (1/196) | Top-1: 42.19 (108/256)
Train | Batch (101/196) | Top-1: 44.19 (11427/25856)
Train | Batch (196/196) | Top-1: 44.28 (22141/50000)
Regular: nan
Epoche: 18; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 43.23
Epoch 19
Train | Batch (1/196) | Top-1: 45.70 (117/256)
Train | Batch (101/196) | Top-1: 44.58 (11526/25856)
Train | Batch (196/196) | Top-1: 44.18 (22089/50000)
Regular: nan
Epoche: 19; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 41.24
Epoch 20
Train | Batch (1/196) | Top-1: 43.75 (112/256)
Train | Batch (101/196) | Top-1: 44.23 (11435/25856)
Train | Batch (196/196) | Top-1: 44.29 (22147/50000)
Regular: nan
Epoche: 20; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 41.61
Epoch 21
Train | Batch (1/196) | Top-1: 41.80 (107/256)
Train | Batch (101/196) | Top-1: 43.87 (11344/25856)
Train | Batch (196/196) | Top-1: 44.15 (22073/50000)
Regular: nan
Epoche: 21; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 37.23
Epoch 22
Train | Batch (1/196) | Top-1: 37.89 (97/256)
Train | Batch (101/196) | Top-1: 44.13 (11411/25856)
Train | Batch (196/196) | Top-1: 44.30 (22152/50000)
Regular: nan
Epoche: 22; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 42.55
Epoch 23
Train | Batch (1/196) | Top-1: 41.41 (106/256)
Train | Batch (101/196) | Top-1: 44.42 (11486/25856)
Train | Batch (196/196) | Top-1: 44.39 (22196/50000)
Regular: nan
Epoche: 23; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 38.10
Epoch 24
Train | Batch (1/196) | Top-1: 46.09 (118/256)
Train | Batch (101/196) | Top-1: 44.49 (11503/25856)
Train | Batch (196/196) | Top-1: 44.36 (22180/50000)
Regular: nan
Epoche: 24; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 37.04
Epoch 25
Train | Batch (1/196) | Top-1: 47.66 (122/256)
Train | Batch (101/196) | Top-1: 44.47 (11498/25856)
Train | Batch (196/196) | Top-1: 44.39 (22193/50000)
Regular: nan
Epoche: 25; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 36.13
Epoch 26
Train | Batch (1/196) | Top-1: 43.36 (111/256)
Train | Batch (101/196) | Top-1: 44.96 (11625/25856)
Train | Batch (196/196) | Top-1: 44.62 (22310/50000)
Regular: nan
Epoche: 26; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 41.10
Epoch 27
Train | Batch (1/196) | Top-1: 39.45 (101/256)
Train | Batch (101/196) | Top-1: 43.92 (11355/25856)
Train | Batch (196/196) | Top-1: 44.17 (22084/50000)
Regular: nan
Epoche: 27; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 33.07
Epoch 28
Train | Batch (1/196) | Top-1: 43.75 (112/256)
Train | Batch (101/196) | Top-1: 44.76 (11574/25856)
Train | Batch (196/196) | Top-1: 44.68 (22340/50000)
Regular: nan
Epoche: 28; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 29.53
Epoch 29
Train | Batch (1/196) | Top-1: 48.44 (124/256)
Train | Batch (101/196) | Top-1: 44.59 (11528/25856)
Train | Batch (196/196) | Top-1: 44.63 (22315/50000)
Regular: nan
Epoche: 29; regular: nan: flops 67896906
#Filters: 635, #FLOPs: 67.02M | Top-1: 35.13
model: ResNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(17, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(17, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(15, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(28, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(28, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(4, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(33, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(9, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (4): AvgPool2d(kernel_size=8, stride=8, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=33, out_features=10, bias=True)
  )
)
Test acc: 35.13
