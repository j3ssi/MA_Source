no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=30, large_input=False, lbda=6e-08, logger='MorphLogs/logMorphNetFlops6e-8_grow1.txt', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=False, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 8.20 (21/256)
Train | Batch (101/196) | Top-1: 21.67 (5603/25856)
Train | Batch (196/196) | Top-1: 27.22 (13611/50000)
Regular: 16.114715576171875
Epoche: 0; regular: 16.114715576171875: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 35.24
Epoch 1
Train | Batch (1/196) | Top-1: 38.67 (99/256)
Train | Batch (101/196) | Top-1: 38.89 (10055/25856)
Train | Batch (196/196) | Top-1: 40.31 (20157/50000)
Regular: 15.450362205505371
Epoche: 1; regular: 15.450362205505371: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 43.95
Epoch 2
Train | Batch (1/196) | Top-1: 41.41 (106/256)
Train | Batch (101/196) | Top-1: 45.53 (11772/25856)
Train | Batch (196/196) | Top-1: 46.71 (23353/50000)
Regular: 14.785017967224121
Epoche: 2; regular: 14.785017967224121: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 48.08
Epoch 3
Train | Batch (1/196) | Top-1: 54.30 (139/256)
Train | Batch (101/196) | Top-1: 51.11 (13216/25856)
Train | Batch (196/196) | Top-1: 51.96 (25980/50000)
Regular: 14.12027645111084
Epoche: 3; regular: 14.12027645111084: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 52.58
Epoch 4
Train | Batch (1/196) | Top-1: 54.69 (140/256)
Train | Batch (101/196) | Top-1: 55.13 (14254/25856)
Train | Batch (196/196) | Top-1: 56.18 (28090/50000)
Regular: 13.456470489501953
Epoche: 4; regular: 13.456470489501953: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 57.88
Epoch 5
Train | Batch (1/196) | Top-1: 58.20 (149/256)
Train | Batch (101/196) | Top-1: 58.80 (15203/25856)
Train | Batch (196/196) | Top-1: 59.59 (29794/50000)
Regular: 12.793384552001953
Epoche: 5; regular: 12.793384552001953: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 60.24
Epoch 6
Train | Batch (1/196) | Top-1: 61.33 (157/256)
Train | Batch (101/196) | Top-1: 61.89 (16002/25856)
Train | Batch (196/196) | Top-1: 62.32 (31160/50000)
Regular: 12.131484985351562
Epoche: 6; regular: 12.131484985351562: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 62.09
Epoch 7
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 64.39 (16649/25856)
Train | Batch (196/196) | Top-1: 64.45 (32224/50000)
Regular: 11.470959663391113
Epoche: 7; regular: 11.470959663391113: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 65.19
Epoch 8
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 65.97 (17057/25856)
Train | Batch (196/196) | Top-1: 66.13 (33065/50000)
Regular: 10.812146186828613
Epoche: 8; regular: 10.812146186828613: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 60.38
Epoch 9
Train | Batch (1/196) | Top-1: 67.58 (173/256)
Train | Batch (101/196) | Top-1: 67.31 (17403/25856)
Train | Batch (196/196) | Top-1: 67.73 (33864/50000)
Regular: 10.155647277832031
Epoche: 9; regular: 10.155647277832031: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 65.90
Epoch 10
Train | Batch (1/196) | Top-1: 64.84 (166/256)
Train | Batch (101/196) | Top-1: 68.60 (17737/25856)
Train | Batch (196/196) | Top-1: 68.76 (34379/50000)
Regular: 9.503063201904297
Epoche: 10; regular: 9.503063201904297: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 67.36
Epoch 11
Train | Batch (1/196) | Top-1: 70.31 (180/256)
Train | Batch (101/196) | Top-1: 69.83 (18055/25856)
Train | Batch (196/196) | Top-1: 69.79 (34897/50000)
Regular: 8.857455253601074
Epoche: 11; regular: 8.857455253601074: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 67.30
Epoch 12
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 70.67 (18273/25856)
Train | Batch (196/196) | Top-1: 70.62 (35310/50000)
Regular: 8.223461151123047
Epoche: 12; regular: 8.223461151123047: flops 68862592
#Filters: 1136, #FLOPs: 68.86M | Top-1: 66.92
Epoch 13
Train | Batch (1/196) | Top-1: 75.78 (194/256)
Train | Batch (101/196) | Top-1: 71.30 (18435/25856)
Train | Batch (196/196) | Top-1: 71.09 (35543/50000)
Regular: 7.618850231170654
Epoche: 13; regular: 7.618850231170654: flops 68862592
#Filters: 1135, #FLOPs: 68.72M | Top-1: 68.05
Epoch 14
Train | Batch (1/196) | Top-1: 66.80 (171/256)
Train | Batch (101/196) | Top-1: 71.29 (18432/25856)
Train | Batch (196/196) | Top-1: 71.04 (35521/50000)
Regular: 7.1235761642456055
Epoche: 14; regular: 7.1235761642456055: flops 68862592
#Filters: 1127, #FLOPs: 67.54M | Top-1: 56.83
Epoch 15
Train | Batch (1/196) | Top-1: 62.89 (161/256)
Train | Batch (101/196) | Top-1: 71.36 (18451/25856)
Train | Batch (196/196) | Top-1: 71.61 (35803/50000)
Regular: 6.742099285125732
Epoche: 15; regular: 6.742099285125732: flops 68862592
#Filters: 1117, #FLOPs: 66.06M | Top-1: 51.27
Epoch 16
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 72.37 (18713/25856)
Train | Batch (196/196) | Top-1: 72.49 (36243/50000)
Regular: 6.388775825500488
Epoche: 16; regular: 6.388775825500488: flops 68862592
#Filters: 1110, #FLOPs: 65.03M | Top-1: 64.35
Epoch 17
Train | Batch (1/196) | Top-1: 72.66 (186/256)
Train | Batch (101/196) | Top-1: 73.17 (18919/25856)
Train | Batch (196/196) | Top-1: 73.51 (36753/50000)
Regular: 6.0553789138793945
Epoche: 17; regular: 6.0553789138793945: flops 68862592
#Filters: 1108, #FLOPs: 65.03M | Top-1: 61.65
Epoch 18
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 73.86 (19096/25856)
Train | Batch (196/196) | Top-1: 74.29 (37147/50000)
Regular: 5.733726501464844
Epoche: 18; regular: 5.733726501464844: flops 68862592
#Filters: 1103, #FLOPs: 64.29M | Top-1: 56.02
Epoch 19
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 74.41 (19239/25856)
Train | Batch (196/196) | Top-1: 74.96 (37482/50000)
Regular: 5.4275736808776855
Epoche: 19; regular: 5.4275736808776855: flops 68862592
#Filters: 1095, #FLOPs: 63.11M | Top-1: 61.97
Epoch 20
Train | Batch (1/196) | Top-1: 72.66 (186/256)
Train | Batch (101/196) | Top-1: 75.39 (19492/25856)
Train | Batch (196/196) | Top-1: 75.55 (37774/50000)
Regular: 5.133628845214844
Epoche: 20; regular: 5.133628845214844: flops 68862592
#Filters: 1092, #FLOPs: 62.67M | Top-1: 59.55
Epoch 21
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 75.97 (19644/25856)
Train | Batch (196/196) | Top-1: 76.32 (38162/50000)
Regular: 4.8520941734313965
Epoche: 21; regular: 4.8520941734313965: flops 68862592
#Filters: 1090, #FLOPs: 62.37M | Top-1: 58.92
Epoch 22
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 77.20 (19961/25856)
Train | Batch (196/196) | Top-1: 77.14 (38570/50000)
Regular: 4.579518795013428
Epoche: 22; regular: 4.579518795013428: flops 68862592
#Filters: 1089, #FLOPs: 62.23M | Top-1: 54.19
Epoch 23
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 77.75 (20104/25856)
Train | Batch (196/196) | Top-1: 77.72 (38861/50000)
Regular: 4.306742191314697
Epoche: 23; regular: 4.306742191314697: flops 68862592
#Filters: 1088, #FLOPs: 62.37M | Top-1: 60.34
Epoch 24
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 78.52 (20301/25856)
Train | Batch (196/196) | Top-1: 78.31 (39155/50000)
Regular: 4.038795471191406
Epoche: 24; regular: 4.038795471191406: flops 68862592
#Filters: 1086, #FLOPs: 62.08M | Top-1: 65.56
Epoch 25
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 78.68 (20343/25856)
Train | Batch (196/196) | Top-1: 78.75 (39375/50000)
Regular: 3.7740039825439453
Epoche: 25; regular: 3.7740039825439453: flops 68862592
#Filters: 1086, #FLOPs: 62.08M | Top-1: 68.18
Epoch 26
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 78.77 (20368/25856)
Train | Batch (196/196) | Top-1: 78.68 (39339/50000)
Regular: 3.518653154373169
Epoche: 26; regular: 3.518653154373169: flops 68862592
#Filters: 1087, #FLOPs: 62.23M | Top-1: 69.25
Epoch 27
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 79.12 (20456/25856)
Train | Batch (196/196) | Top-1: 78.86 (39429/50000)
Regular: 3.2820353507995605
Epoche: 27; regular: 3.2820353507995605: flops 68862592
#Filters: 1086, #FLOPs: 62.08M | Top-1: 64.13
Epoch 28
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 78.72 (20355/25856)
Train | Batch (196/196) | Top-1: 78.48 (39239/50000)
Regular: 3.0991005897521973
Epoche: 28; regular: 3.0991005897521973: flops 68862592
#Filters: 1080, #FLOPs: 61.34M | Top-1: 50.89
Epoch 29
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 78.36 (20261/25856)
Train | Batch (196/196) | Top-1: 78.35 (39175/50000)
Regular: 2.96266770362854
Epoche: 29; regular: 2.96266770362854: flops 68862592
#Filters: 1072, #FLOPs: 60.83M | Top-1: 29.38
Drin!!
Layers that will be prunned: [(1, 5), (3, 7), (5, 14), (7, 14), (9, 9), (13, 1), (15, 3), (17, 7)]
Prunning filters..
Layer index: 1; Pruned filters: 3
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 3
Layer index: 3; Pruned filters: 2
Layer index: 5; Pruned filters: 14
Layer index: 7; Pruned filters: 14
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 5
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 17; Pruned filters: 2
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 2
Layer index: 17; Pruned filters: 1
Layer index: 17; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 52.790M | #Params: 0.441M
1.1427310839813514
After Growth | FLOPs: 68.827M | #Params: 0.576M
I: 1
flops: 68826970
Before Pruning | FLOPs: 68.827M | #Params: 0.576M
Epoch 0
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 78.49 (20294/25856)
Train | Batch (196/196) | Top-1: 78.50 (39248/50000)
Regular: 5.210742950439453
Epoche: 0; regular: 5.210742950439453: flops 68826970
#Filters: 1217, #FLOPs: 67.98M | Top-1: 60.33
Epoch 1
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 79.07 (20444/25856)
Train | Batch (196/196) | Top-1: 78.96 (39478/50000)
Regular: 4.992549419403076
Epoche: 1; regular: 4.992549419403076: flops 68826970
#Filters: 1214, #FLOPs: 67.81M | Top-1: 71.10
Epoch 2
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 79.52 (20561/25856)
Train | Batch (196/196) | Top-1: 79.22 (39609/50000)
Regular: 4.777082920074463
Epoche: 2; regular: 4.777082920074463: flops 68826970
#Filters: 1209, #FLOPs: 67.30M | Top-1: 58.66
Epoch 3
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 79.57 (20574/25856)
Train | Batch (196/196) | Top-1: 79.39 (39694/50000)
Regular: 4.563715934753418
Epoche: 3; regular: 4.563715934753418: flops 68826970
#Filters: 1206, #FLOPs: 67.05M | Top-1: 44.32
Epoch 4
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 79.94 (20668/25856)
Train | Batch (196/196) | Top-1: 79.65 (39826/50000)
Regular: 4.355155944824219
Epoche: 4; regular: 4.355155944824219: flops 68826970
#Filters: 1207, #FLOPs: 67.13M | Top-1: 72.20
Epoch 5
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 80.05 (20697/25856)
Train | Batch (196/196) | Top-1: 80.04 (40019/50000)
Regular: 4.1490936279296875
Epoche: 5; regular: 4.1490936279296875: flops 68826970
#Filters: 1200, #FLOPs: 66.29M | Top-1: 47.43
Epoch 6
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 79.70 (20608/25856)
Train | Batch (196/196) | Top-1: 79.92 (39958/50000)
Regular: 3.9546291828155518
Epoche: 6; regular: 3.9546291828155518: flops 68826970
#Filters: 1199, #FLOPs: 66.37M | Top-1: 68.65
Epoch 7
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 80.07 (20702/25856)
Train | Batch (196/196) | Top-1: 80.04 (40019/50000)
Regular: 3.768871307373047
Epoche: 7; regular: 3.768871307373047: flops 68826970
#Filters: 1196, #FLOPs: 66.03M | Top-1: 48.63
Epoch 8
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 80.44 (20798/25856)
Train | Batch (196/196) | Top-1: 80.36 (40179/50000)
Regular: 3.5843822956085205
Epoche: 8; regular: 3.5843822956085205: flops 68826970
#Filters: 1191, #FLOPs: 65.61M | Top-1: 48.29
Epoch 9
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 80.38 (20784/25856)
Train | Batch (196/196) | Top-1: 80.61 (40305/50000)
Regular: 3.402893304824829
Epoche: 9; regular: 3.402893304824829: flops 68826970
#Filters: 1186, #FLOPs: 65.10M | Top-1: 56.75
Epoch 10
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 80.53 (20823/25856)
Train | Batch (196/196) | Top-1: 80.80 (40401/50000)
Regular: 3.223597526550293
Epoche: 10; regular: 3.223597526550293: flops 68826970
#Filters: 1185, #FLOPs: 65.01M | Top-1: 36.93
Epoch 11
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 81.05 (20955/25856)
Train | Batch (196/196) | Top-1: 80.86 (40429/50000)
Regular: 3.04117751121521
Epoche: 11; regular: 3.04117751121521: flops 68826970
#Filters: 1181, #FLOPs: 64.75M | Top-1: 61.03
Epoch 12
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 81.29 (21019/25856)
Train | Batch (196/196) | Top-1: 81.35 (40677/50000)
Regular: 2.862119674682617
Epoche: 12; regular: 2.862119674682617: flops 68826970
#Filters: 1174, #FLOPs: 63.83M | Top-1: 60.14
Epoch 13
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 81.52 (21078/25856)
Train | Batch (196/196) | Top-1: 81.52 (40758/50000)
Regular: 2.6921448707580566
Epoche: 13; regular: 2.6921448707580566: flops 68826970
#Filters: 1167, #FLOPs: 63.04M | Top-1: 53.88
Epoch 14
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 81.64 (21110/25856)
Train | Batch (196/196) | Top-1: 81.72 (40859/50000)
Regular: 2.526085615158081
Epoche: 14; regular: 2.526085615158081: flops 68826970
#Filters: 1167, #FLOPs: 63.12M | Top-1: 61.85
Epoch 15
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 82.02 (21206/25856)
Train | Batch (196/196) | Top-1: 81.83 (40916/50000)
Regular: 2.362579345703125
Epoche: 15; regular: 2.362579345703125: flops 68826970
#Filters: 1164, #FLOPs: 62.82M | Top-1: 51.29
Epoch 16
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 81.84 (21161/25856)
Train | Batch (196/196) | Top-1: 81.93 (40963/50000)
Regular: 2.2003695964813232
Epoche: 16; regular: 2.2003695964813232: flops 68826970
#Filters: 1162, #FLOPs: 62.73M | Top-1: 60.96
Epoch 17
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 82.24 (21265/25856)
Train | Batch (196/196) | Top-1: 82.18 (41090/50000)
Regular: 2.037722110748291
Epoche: 17; regular: 2.037722110748291: flops 68826970
#Filters: 1162, #FLOPs: 62.57M | Top-1: 65.96
Epoch 18
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 81.94 (21187/25856)
Train | Batch (196/196) | Top-1: 82.16 (41082/50000)
Regular: 1.878618836402893
Epoche: 18; regular: 1.878618836402893: flops 68826970
#Filters: 1162, #FLOPs: 62.77M | Top-1: 51.19
Epoch 19
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 82.30 (21279/25856)
Train | Batch (196/196) | Top-1: 82.10 (41052/50000)
Regular: 1.7217813730239868
Epoche: 19; regular: 1.7217813730239868: flops 68826970
#Filters: 1157, #FLOPs: 62.19M | Top-1: 45.36
Epoch 20
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 82.29 (21277/25856)
Train | Batch (196/196) | Top-1: 82.31 (41156/50000)
Regular: 1.5697401762008667
Epoche: 20; regular: 1.5697401762008667: flops 68826970
#Filters: 1160, #FLOPs: 62.60M | Top-1: 66.18
Epoch 21
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 82.30 (21279/25856)
Train | Batch (196/196) | Top-1: 82.22 (41108/50000)
Regular: 1.4301680326461792
Epoche: 21; regular: 1.4301680326461792: flops 68826970
#Filters: 1154, #FLOPs: 62.39M | Top-1: 65.49
Epoch 22
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 82.18 (21248/25856)
Train | Batch (196/196) | Top-1: 82.12 (41059/50000)
Regular: 1.3077279329299927
Epoche: 22; regular: 1.3077279329299927: flops 68826970
#Filters: 1136, #FLOPs: 61.59M | Top-1: 62.80
Epoch 23
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 82.10 (21229/25856)
Train | Batch (196/196) | Top-1: 81.86 (40928/50000)
Regular: 1.2082139253616333
Epoche: 23; regular: 1.2082139253616333: flops 68826970
#Filters: 1126, #FLOPs: 61.05M | Top-1: 47.22
Epoch 24
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 81.70 (21124/25856)
Train | Batch (196/196) | Top-1: 81.77 (40884/50000)
Regular: 1.1252027750015259
Epoche: 24; regular: 1.1252027750015259: flops 68826970
#Filters: 1092, #FLOPs: 58.80M | Top-1: 43.12
Epoch 25
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 82.27 (21271/25856)
Train | Batch (196/196) | Top-1: 81.95 (40975/50000)
Regular: 1.058627724647522
Epoche: 25; regular: 1.058627724647522: flops 68826970
#Filters: 1089, #FLOPs: 58.63M | Top-1: 48.02
Epoch 26
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 82.09 (21224/25856)
Train | Batch (196/196) | Top-1: 81.96 (40982/50000)
Regular: 1.0029150247573853
Epoche: 26; regular: 1.0029150247573853: flops 68826970
#Filters: 1088, #FLOPs: 58.47M | Top-1: 44.66
Epoch 27
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 82.12 (21234/25856)
Train | Batch (196/196) | Top-1: 81.93 (40964/50000)
Regular: 0.9531422853469849
Epoche: 27; regular: 0.9531422853469849: flops 68826970
#Filters: 1084, #FLOPs: 58.39M | Top-1: 58.51
Epoch 28
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 82.17 (21246/25856)
Train | Batch (196/196) | Top-1: 82.31 (41155/50000)
Regular: 0.9103711247444153
Epoche: 28; regular: 0.9103711247444153: flops 68826970
#Filters: 1077, #FLOPs: 57.93M | Top-1: 50.37
Epoch 29
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 82.21 (21257/25856)
Train | Batch (196/196) | Top-1: 82.10 (41049/50000)
Regular: 0.8710504770278931
Epoche: 29; regular: 0.8710504770278931: flops 68826970
#Filters: 1075, #FLOPs: 58.05M | Top-1: 49.38
Drin!!
Layers that will be prunned: [(1, 6), (3, 1), (9, 4), (11, 1), (13, 13), (15, 20), (17, 27), (19, 14), (23, 1), (25, 13), (27, 28), (29, 20)]
Prunning filters..
Layer index: 1; Pruned filters: 2
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 2
Layer index: 3; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 1
Layer index: 9; Pruned filters: 2
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 2
Layer index: 13; Pruned filters: 2
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 4
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 6
Layer index: 15; Pruned filters: 5
Layer index: 15; Pruned filters: 2
Layer index: 15; Pruned filters: 6
Layer index: 17; Pruned filters: 27
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 2
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 5
Layer index: 23; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 5
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 47.220M | #Params: 0.441M
1.2087167686522446
After Growth | FLOPs: 68.817M | #Params: 0.644M
I: 2
flops: 68816752
Before Pruning | FLOPs: 68.817M | #Params: 0.644M
Epoch 0
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 82.29 (21277/25856)
Train | Batch (196/196) | Top-1: 82.26 (41129/50000)
Regular: 3.5894949436187744
Epoche: 0; regular: 3.5894949436187744: flops 68816752
#Filters: 1289, #FLOPs: 67.79M | Top-1: 58.99
Epoch 1
Train | Batch (1/196) | Top-1: 75.78 (194/256)
Train | Batch (101/196) | Top-1: 81.70 (21125/25856)
Train | Batch (196/196) | Top-1: 81.99 (40993/50000)
Regular: 3.4578840732574463
Epoche: 1; regular: 3.4578840732574463: flops 68816752
#Filters: 1283, #FLOPs: 67.22M | Top-1: 61.18
Epoch 2
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 82.08 (21222/25856)
Train | Batch (196/196) | Top-1: 82.17 (41085/50000)
Regular: 3.327376365661621
Epoche: 2; regular: 3.327376365661621: flops 68816752
#Filters: 1270, #FLOPs: 66.46M | Top-1: 44.21
Epoch 3
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 82.43 (21314/25856)
Train | Batch (196/196) | Top-1: 82.41 (41204/50000)
Regular: 3.198155641555786
Epoche: 3; regular: 3.198155641555786: flops 68816752
#Filters: 1267, #FLOPs: 66.31M | Top-1: 57.73
Epoch 4
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 82.73 (21390/25856)
Train | Batch (196/196) | Top-1: 82.41 (41207/50000)
Regular: 3.0719974040985107
Epoche: 4; regular: 3.0719974040985107: flops 68816752
#Filters: 1263, #FLOPs: 65.85M | Top-1: 57.83
Epoch 5
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 82.18 (21249/25856)
Train | Batch (196/196) | Top-1: 82.23 (41114/50000)
Regular: 2.9507296085357666
Epoche: 5; regular: 2.9507296085357666: flops 68816752
#Filters: 1261, #FLOPs: 65.90M | Top-1: 62.52
Epoch 6
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 82.26 (21269/25856)
Train | Batch (196/196) | Top-1: 82.54 (41271/50000)
Regular: 2.8331432342529297
Epoche: 6; regular: 2.8331432342529297: flops 68816752
#Filters: 1258, #FLOPs: 65.65M | Top-1: 52.47
Epoch 7
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 82.65 (21369/25856)
Train | Batch (196/196) | Top-1: 82.67 (41333/50000)
Regular: 2.7150847911834717
Epoche: 7; regular: 2.7150847911834717: flops 68816752
#Filters: 1248, #FLOPs: 64.99M | Top-1: 58.06
Epoch 8
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 82.82 (21413/25856)
Train | Batch (196/196) | Top-1: 82.73 (41364/50000)
Regular: 2.598931074142456
Epoche: 8; regular: 2.598931074142456: flops 68816752
#Filters: 1246, #FLOPs: 65.03M | Top-1: 46.91
Epoch 9
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 82.78 (21404/25856)
Train | Batch (196/196) | Top-1: 82.74 (41372/50000)
Regular: 2.48576283454895
Epoche: 9; regular: 2.48576283454895: flops 68816752
#Filters: 1244, #FLOPs: 64.68M | Top-1: 56.03
Epoch 10
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 82.63 (21364/25856)
Train | Batch (196/196) | Top-1: 82.65 (41327/50000)
Regular: 2.3721566200256348
Epoche: 10; regular: 2.3721566200256348: flops 68816752
#Filters: 1242, #FLOPs: 63.82M | Top-1: 23.68
Epoch 11
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 82.84 (21418/25856)
Train | Batch (196/196) | Top-1: 82.75 (41377/50000)
Regular: 2.274515151977539
Epoche: 11; regular: 2.274515151977539: flops 68816752
#Filters: 1234, #FLOPs: 63.15M | Top-1: 63.96
Epoch 12
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 82.49 (21328/25856)
Train | Batch (196/196) | Top-1: 82.82 (41409/50000)
Regular: 2.1802401542663574
Epoche: 12; regular: 2.1802401542663574: flops 68816752
#Filters: 1235, #FLOPs: 63.56M | Top-1: 52.16
Epoch 13
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 83.15 (21499/25856)
Train | Batch (196/196) | Top-1: 83.16 (41578/50000)
Regular: 2.087360382080078
Epoche: 13; regular: 2.087360382080078: flops 68816752
#Filters: 1232, #FLOPs: 63.26M | Top-1: 44.21
Epoch 14
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 83.04 (21470/25856)
Train | Batch (196/196) | Top-1: 83.11 (41556/50000)
Regular: 1.9911006689071655
Epoche: 14; regular: 1.9911006689071655: flops 68816752
#Filters: 1231, #FLOPs: 63.05M | Top-1: 72.59
Epoch 15
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 82.92 (21440/25856)
Train | Batch (196/196) | Top-1: 83.03 (41514/50000)
Regular: 1.8989195823669434
Epoche: 15; regular: 1.8989195823669434: flops 68816752
#Filters: 1194, #FLOPs: 61.43M | Top-1: 68.03
Epoch 16
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 83.52 (21595/25856)
Train | Batch (196/196) | Top-1: 83.43 (41716/50000)
Regular: 1.8098057508468628
Epoche: 16; regular: 1.8098057508468628: flops 68816752
#Filters: 1191, #FLOPs: 61.28M | Top-1: 56.12
Epoch 17
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 83.61 (21617/25856)
Train | Batch (196/196) | Top-1: 83.72 (41861/50000)
Regular: 1.7294577360153198
Epoche: 17; regular: 1.7294577360153198: flops 68816752
#Filters: 1189, #FLOPs: 61.07M | Top-1: 59.50
Epoch 18
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 83.91 (21695/25856)
Train | Batch (196/196) | Top-1: 83.66 (41831/50000)
Regular: 1.6489912271499634
Epoche: 18; regular: 1.6489912271499634: flops 68816752
#Filters: 1189, #FLOPs: 61.02M | Top-1: 69.33
Epoch 19
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.62 (21621/25856)
Train | Batch (196/196) | Top-1: 83.81 (41904/50000)
Regular: 1.5733152627944946
Epoche: 19; regular: 1.5733152627944946: flops 68816752
#Filters: 1187, #FLOPs: 60.77M | Top-1: 48.48
Epoch 20
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 83.69 (21638/25856)
Train | Batch (196/196) | Top-1: 83.68 (41841/50000)
Regular: 1.4968211650848389
Epoche: 20; regular: 1.4968211650848389: flops 68816752
#Filters: 1172, #FLOPs: 59.40M | Top-1: 40.34
Epoch 21
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 83.56 (21605/25856)
Train | Batch (196/196) | Top-1: 83.82 (41911/50000)
Regular: 1.4356845617294312
Epoche: 21; regular: 1.4356845617294312: flops 68816752
#Filters: 1175, #FLOPs: 59.76M | Top-1: 51.57
Epoch 22
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 84.17 (21762/25856)
Train | Batch (196/196) | Top-1: 83.96 (41980/50000)
Regular: 1.3742105960845947
Epoche: 22; regular: 1.3742105960845947: flops 68816752
#Filters: 1171, #FLOPs: 59.24M | Top-1: 56.37
Epoch 23
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 83.83 (21676/25856)
Train | Batch (196/196) | Top-1: 83.76 (41880/50000)
Regular: 1.3141367435455322
Epoche: 23; regular: 1.3141367435455322: flops 68816752
#Filters: 1169, #FLOPs: 58.71M | Top-1: 52.23
Epoch 24
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 84.32 (21802/25856)
Train | Batch (196/196) | Top-1: 84.17 (42084/50000)
Regular: 1.2549718618392944
Epoche: 24; regular: 1.2549718618392944: flops 68816752
#Filters: 1166, #FLOPs: 58.90M | Top-1: 22.48
Epoch 25
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 84.17 (21764/25856)
Train | Batch (196/196) | Top-1: 84.15 (42075/50000)
Regular: 1.1992433071136475
Epoche: 25; regular: 1.1992433071136475: flops 68816752
#Filters: 1170, #FLOPs: 59.21M | Top-1: 41.82
Epoch 26
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 84.51 (21850/25856)
Train | Batch (196/196) | Top-1: 84.42 (42208/50000)
Regular: 1.1479648351669312
Epoche: 26; regular: 1.1479648351669312: flops 68816752
#Filters: 1159, #FLOPs: 58.45M | Top-1: 63.64
Epoch 27
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 83.88 (21687/25856)
Train | Batch (196/196) | Top-1: 84.32 (42159/50000)
Regular: 1.0999178886413574
Epoche: 27; regular: 1.0999178886413574: flops 68816752
#Filters: 1160, #FLOPs: 58.55M | Top-1: 64.76
Epoch 28
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 84.23 (21778/25856)
Train | Batch (196/196) | Top-1: 84.43 (42216/50000)
Regular: 1.0551230907440186
Epoche: 28; regular: 1.0551230907440186: flops 68816752
#Filters: 1148, #FLOPs: 58.12M | Top-1: 71.52
Epoch 29
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 84.50 (21848/25856)
Train | Batch (196/196) | Top-1: 84.64 (42321/50000)
Regular: 1.0143191814422607
Epoche: 29; regular: 1.0143191814422607: flops 68816752
#Filters: 1145, #FLOPs: 58.06M | Top-1: 61.66
Drin!!
Layers that will be prunned: [(1, 1), (3, 4), (9, 3), (11, 16), (13, 11), (15, 6), (19, 17), (21, 9), (23, 12), (25, 21), (27, 27), (29, 30)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 2
Layer index: 9; Pruned filters: 3
Layer index: 11; Pruned filters: 2
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 13
Layer index: 13; Pruned filters: 2
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 5
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 3
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 5
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 8
Layer index: 21; Pruned filters: 9
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 9
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 10
Layer index: 27; Pruned filters: 3
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 2
Layer index: 27; Pruned filters: 4
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 9
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 10
Target (flops): 68.863M
After Pruning | FLOPs: 46.239M | #Params: 0.450M
1.2218168391976667
After Growth | FLOPs: 68.874M | #Params: 0.676M
I: 3
flops: 68873976
Before Pruning | FLOPs: 68.874M | #Params: 0.676M
Epoch 0
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 83.58 (21611/25856)
Train | Batch (196/196) | Top-1: 83.91 (41953/50000)
Regular: 3.7522284984588623
Epoche: 0; regular: 3.7522284984588623: flops 68873976
#Filters: 1396, #FLOPs: 68.12M | Top-1: 33.18
Epoch 1
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 83.66 (21630/25856)
Train | Batch (196/196) | Top-1: 83.86 (41931/50000)
Regular: 3.60514235496521
Epoche: 1; regular: 3.60514235496521: flops 68873976
#Filters: 1389, #FLOPs: 67.28M | Top-1: 36.75
Epoch 2
Train | Batch (1/196) | Top-1: 79.69 (204/256)
Train | Batch (101/196) | Top-1: 83.88 (21689/25856)
Train | Batch (196/196) | Top-1: 84.18 (42091/50000)
Regular: 3.4589576721191406
Epoche: 2; regular: 3.4589576721191406: flops 68873976
#Filters: 1388, #FLOPs: 67.24M | Top-1: 45.97
Epoch 3
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 84.19 (21767/25856)
Train | Batch (196/196) | Top-1: 84.13 (42065/50000)
Regular: 3.3129782676696777
Epoche: 3; regular: 3.3129782676696777: flops 68873976
#Filters: 1390, #FLOPs: 67.68M | Top-1: 71.70
Epoch 4
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 84.36 (21811/25856)
Train | Batch (196/196) | Top-1: 84.40 (42202/50000)
Regular: 3.1686410903930664
Epoche: 4; regular: 3.1686410903930664: flops 68873976
#Filters: 1387, #FLOPs: 67.37M | Top-1: 60.02
Epoch 5
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.91 (21695/25856)
Train | Batch (196/196) | Top-1: 84.14 (42071/50000)
Regular: 3.0216076374053955
Epoche: 5; regular: 3.0216076374053955: flops 68873976
#Filters: 1385, #FLOPs: 67.31M | Top-1: 40.72
Epoch 6
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 84.20 (21771/25856)
Train | Batch (196/196) | Top-1: 84.28 (42140/50000)
Regular: 2.8752856254577637
Epoche: 6; regular: 2.8752856254577637: flops 68873976
#Filters: 1386, #FLOPs: 67.30M | Top-1: 55.68
Epoch 7
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 84.39 (21821/25856)
Train | Batch (196/196) | Top-1: 84.32 (42160/50000)
Regular: 2.7295479774475098
Epoche: 7; regular: 2.7295479774475098: flops 68873976
#Filters: 1385, #FLOPs: 67.24M | Top-1: 52.59
Epoch 8
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 84.46 (21838/25856)
Train | Batch (196/196) | Top-1: 84.58 (42291/50000)
Regular: 2.5871284008026123
Epoche: 8; regular: 2.5871284008026123: flops 68873976
#Filters: 1333, #FLOPs: 63.14M | Top-1: 42.95
Epoch 9
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 84.77 (21918/25856)
Train | Batch (196/196) | Top-1: 84.67 (42334/50000)
Regular: 2.474287986755371
Epoche: 9; regular: 2.474287986755371: flops 68873976
#Filters: 1333, #FLOPs: 63.26M | Top-1: 60.96
Epoch 10
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 85.05 (21991/25856)
Train | Batch (196/196) | Top-1: 84.80 (42399/50000)
Regular: 2.381615161895752
Epoche: 10; regular: 2.381615161895752: flops 68873976
#Filters: 1330, #FLOPs: 62.64M | Top-1: 56.16
Epoch 11
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 84.76 (21916/25856)
Train | Batch (196/196) | Top-1: 84.75 (42373/50000)
Regular: 2.289628744125366
Epoche: 11; regular: 2.289628744125366: flops 68873976
#Filters: 1331, #FLOPs: 62.92M | Top-1: 37.51
Epoch 12
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.79 (21924/25856)
Train | Batch (196/196) | Top-1: 84.98 (42491/50000)
Regular: 2.202967882156372
Epoche: 12; regular: 2.202967882156372: flops 68873976
#Filters: 1329, #FLOPs: 62.91M | Top-1: 58.89
Epoch 13
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 85.27 (22047/25856)
Train | Batch (196/196) | Top-1: 85.09 (42544/50000)
Regular: 2.112985610961914
Epoche: 13; regular: 2.112985610961914: flops 68873976
#Filters: 1329, #FLOPs: 62.98M | Top-1: 45.81
Epoch 14
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 84.91 (21955/25856)
Train | Batch (196/196) | Top-1: 84.99 (42497/50000)
Regular: 2.024052619934082
Epoche: 14; regular: 2.024052619934082: flops 68873976
#Filters: 1326, #FLOPs: 62.73M | Top-1: 63.55
Epoch 15
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 85.40 (22082/25856)
Train | Batch (196/196) | Top-1: 85.16 (42579/50000)
Regular: 1.937928318977356
Epoche: 15; regular: 1.937928318977356: flops 68873976
#Filters: 1327, #FLOPs: 62.73M | Top-1: 54.95
Epoch 16
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 85.55 (22120/25856)
Train | Batch (196/196) | Top-1: 85.46 (42732/50000)
Regular: 1.8536467552185059
Epoche: 16; regular: 1.8536467552185059: flops 68873976
#Filters: 1321, #FLOPs: 61.81M | Top-1: 49.80
Epoch 17
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 85.28 (22049/25856)
Train | Batch (196/196) | Top-1: 85.40 (42702/50000)
Regular: 1.7831813097000122
Epoche: 17; regular: 1.7831813097000122: flops 68873976
#Filters: 1319, #FLOPs: 61.59M | Top-1: 46.64
Epoch 18
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 85.48 (22102/25856)
Train | Batch (196/196) | Top-1: 85.54 (42769/50000)
Regular: 1.7127981185913086
Epoche: 18; regular: 1.7127981185913086: flops 68873976
#Filters: 1318, #FLOPs: 61.46M | Top-1: 55.08
Epoch 19
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 85.38 (22077/25856)
Train | Batch (196/196) | Top-1: 85.54 (42771/50000)
Regular: 1.644312858581543
Epoche: 19; regular: 1.644312858581543: flops 68873976
#Filters: 1302, #FLOPs: 61.27M | Top-1: 71.45
Epoch 20
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 85.32 (22060/25856)
Train | Batch (196/196) | Top-1: 85.42 (42711/50000)
Regular: 1.5780131816864014
Epoche: 20; regular: 1.5780131816864014: flops 68873976
#Filters: 1301, #FLOPs: 61.21M | Top-1: 76.66
Epoch 21
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 85.65 (22145/25856)
Train | Batch (196/196) | Top-1: 85.64 (42819/50000)
Regular: 1.5128650665283203
Epoche: 21; regular: 1.5128650665283203: flops 68873976
#Filters: 1302, #FLOPs: 61.31M | Top-1: 61.42
Epoch 22
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 85.78 (22178/25856)
Train | Batch (196/196) | Top-1: 85.84 (42919/50000)
Regular: 1.4504902362823486
Epoche: 22; regular: 1.4504902362823486: flops 68873976
#Filters: 1294, #FLOPs: 60.71M | Top-1: 71.95
Epoch 23
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 85.77 (22177/25856)
Train | Batch (196/196) | Top-1: 85.77 (42884/50000)
Regular: 1.3895841836929321
Epoche: 23; regular: 1.3895841836929321: flops 68873976
#Filters: 1296, #FLOPs: 60.93M | Top-1: 52.22
Epoch 24
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 85.76 (22174/25856)
Train | Batch (196/196) | Top-1: 85.74 (42870/50000)
Regular: 1.3352386951446533
Epoche: 24; regular: 1.3352386951446533: flops 68873976
#Filters: 1291, #FLOPs: 60.37M | Top-1: 65.57
Epoch 25
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 86.13 (22271/25856)
Train | Batch (196/196) | Top-1: 85.92 (42962/50000)
Regular: 1.2773104906082153
Epoche: 25; regular: 1.2773104906082153: flops 68873976
#Filters: 1288, #FLOPs: 60.00M | Top-1: 63.46
Epoch 26
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 85.98 (22230/25856)
Train | Batch (196/196) | Top-1: 85.94 (42971/50000)
Regular: 1.224403977394104
Epoche: 26; regular: 1.224403977394104: flops 68873976
#Filters: 1297, #FLOPs: 60.87M | Top-1: 77.05
Epoch 27
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 86.04 (22247/25856)
Train | Batch (196/196) | Top-1: 86.01 (43007/50000)
Regular: 1.1696484088897705
Epoche: 27; regular: 1.1696484088897705: flops 68873976
#Filters: 1289, #FLOPs: 59.93M | Top-1: 64.76
Epoch 28
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 86.14 (22273/25856)
Train | Batch (196/196) | Top-1: 86.03 (43013/50000)
Regular: 1.1164453029632568
Epoche: 28; regular: 1.1164453029632568: flops 68873976
#Filters: 1289, #FLOPs: 59.93M | Top-1: 70.93
Epoch 29
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 86.14 (22272/25856)
Train | Batch (196/196) | Top-1: 86.07 (43037/50000)
Regular: 1.0632450580596924
Epoche: 29; regular: 1.0632450580596924: flops 68873976
#Filters: 1289, #FLOPs: 59.83M | Top-1: 43.89
Drin!!
Layers that will be prunned: [(1, 4), (3, 3), (11, 6), (13, 6), (15, 3), (19, 9), (21, 15), (23, 18), (25, 24), (27, 11), (29, 14)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 2
Layer index: 3; Pruned filters: 1
Layer index: 3; Pruned filters: 2
Layer index: 11; Pruned filters: 6
Layer index: 13; Pruned filters: 2
Layer index: 13; Pruned filters: 4
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 2
Layer index: 19; Pruned filters: 2
Layer index: 19; Pruned filters: 3
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 1
Layer index: 19; Pruned filters: 2
Layer index: 21; Pruned filters: 15
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 15
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 4
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 13
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 9
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 11
Target (flops): 68.863M
After Pruning | FLOPs: 49.951M | #Params: 0.499M
1.1754570460624623
After Growth | FLOPs: 68.505M | #Params: 0.689M
I: 4
flops: 68504950
Before Pruning | FLOPs: 68.505M | #Params: 0.689M
Epoch 0
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 86.26 (22304/25856)
Train | Batch (196/196) | Top-1: 86.03 (43013/50000)
Regular: 3.151460647583008
Epoche: 0; regular: 3.151460647583008: flops 68504950
#Filters: 1512, #FLOPs: 67.87M | Top-1: 52.38
Epoch 1
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 86.24 (22299/25856)
Train | Batch (196/196) | Top-1: 85.95 (42973/50000)
Regular: 3.0139272212982178
Epoche: 1; regular: 3.0139272212982178: flops 68504950
#Filters: 1512, #FLOPs: 67.65M | Top-1: 72.58
Epoch 2
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 86.22 (22293/25856)
Train | Batch (196/196) | Top-1: 86.15 (43075/50000)
Regular: 2.8747987747192383
Epoche: 2; regular: 2.8747987747192383: flops 68504950
#Filters: 1467, #FLOPs: 64.44M | Top-1: 57.69
Epoch 3
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 86.15 (22275/25856)
Train | Batch (196/196) | Top-1: 85.85 (42926/50000)
Regular: 2.745802164077759
Epoche: 3; regular: 2.745802164077759: flops 68504950
#Filters: 1464, #FLOPs: 64.10M | Top-1: 55.59
Epoch 4
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 86.07 (22253/25856)
Train | Batch (196/196) | Top-1: 86.12 (43062/50000)
Regular: 2.633424997329712
Epoche: 4; regular: 2.633424997329712: flops 68504950
#Filters: 1467, #FLOPs: 64.28M | Top-1: 57.48
Epoch 5
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 86.19 (22285/25856)
Train | Batch (196/196) | Top-1: 86.07 (43036/50000)
Regular: 2.52317476272583
Epoche: 5; regular: 2.52317476272583: flops 68504950
#Filters: 1468, #FLOPs: 64.66M | Top-1: 63.08
Epoch 6
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 86.15 (22275/25856)
Train | Batch (196/196) | Top-1: 86.05 (43024/50000)
Regular: 2.4157462120056152
Epoche: 6; regular: 2.4157462120056152: flops 68504950
#Filters: 1466, #FLOPs: 64.21M | Top-1: 53.88
Epoch 7
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 86.44 (22350/25856)
Train | Batch (196/196) | Top-1: 86.27 (43136/50000)
Regular: 2.3120248317718506
Epoche: 7; regular: 2.3120248317718506: flops 68504950
#Filters: 1462, #FLOPs: 63.10M | Top-1: 76.43
Epoch 8
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 86.17 (22281/25856)
Train | Batch (196/196) | Top-1: 86.31 (43155/50000)
Regular: 2.2239911556243896
Epoche: 8; regular: 2.2239911556243896: flops 68504950
#Filters: 1463, #FLOPs: 63.66M | Top-1: 50.69
Epoch 9
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 86.43 (22347/25856)
Train | Batch (196/196) | Top-1: 86.34 (43170/50000)
Regular: 2.1421761512756348
Epoche: 9; regular: 2.1421761512756348: flops 68504950
#Filters: 1463, #FLOPs: 63.51M | Top-1: 39.02
Epoch 10
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 86.46 (22354/25856)
Train | Batch (196/196) | Top-1: 86.11 (43053/50000)
Regular: 2.0596654415130615
Epoche: 10; regular: 2.0596654415130615: flops 68504950
#Filters: 1461, #FLOPs: 63.55M | Top-1: 67.68
Epoch 11
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 86.84 (22454/25856)
Train | Batch (196/196) | Top-1: 86.59 (43295/50000)
Regular: 1.976361632347107
Epoche: 11; regular: 1.976361632347107: flops 68504950
#Filters: 1441, #FLOPs: 62.39M | Top-1: 50.91
Epoch 12
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 86.59 (22388/25856)
Train | Batch (196/196) | Top-1: 86.51 (43254/50000)
Regular: 1.8971772193908691
Epoche: 12; regular: 1.8971772193908691: flops 68504950
#Filters: 1443, #FLOPs: 62.62M | Top-1: 71.84
Epoch 13
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 86.22 (22293/25856)
Train | Batch (196/196) | Top-1: 86.44 (43220/50000)
Regular: 1.8224061727523804
Epoche: 13; regular: 1.8224061727523804: flops 68504950
#Filters: 1439, #FLOPs: 61.98M | Top-1: 66.55
Epoch 14
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 86.75 (22431/25856)
Train | Batch (196/196) | Top-1: 86.64 (43318/50000)
Regular: 1.751988172531128
Epoche: 14; regular: 1.751988172531128: flops 68504950
#Filters: 1441, #FLOPs: 62.34M | Top-1: 62.97
Epoch 15
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 86.61 (22394/25856)
Train | Batch (196/196) | Top-1: 86.66 (43332/50000)
Regular: 1.6904091835021973
Epoche: 15; regular: 1.6904091835021973: flops 68504950
#Filters: 1438, #FLOPs: 61.97M | Top-1: 63.32
Epoch 16
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 87.04 (22505/25856)
Train | Batch (196/196) | Top-1: 86.70 (43350/50000)
Regular: 1.6268892288208008
Epoche: 16; regular: 1.6268892288208008: flops 68504950
#Filters: 1437, #FLOPs: 61.72M | Top-1: 56.49
Epoch 17
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 86.71 (22419/25856)
Train | Batch (196/196) | Top-1: 86.75 (43376/50000)
Regular: 1.566713571548462
Epoche: 17; regular: 1.566713571548462: flops 68504950
#Filters: 1439, #FLOPs: 62.12M | Top-1: 56.82
Epoch 18
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 86.92 (22475/25856)
Train | Batch (196/196) | Top-1: 86.85 (43427/50000)
Regular: 1.5053175687789917
Epoche: 18; regular: 1.5053175687789917: flops 68504950
#Filters: 1434, #FLOPs: 61.86M | Top-1: 49.19
Epoch 19
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 86.82 (22448/25856)
Train | Batch (196/196) | Top-1: 86.76 (43378/50000)
Regular: 1.4487214088439941
Epoche: 19; regular: 1.4487214088439941: flops 68504950
#Filters: 1430, #FLOPs: 61.50M | Top-1: 53.48
Epoch 20
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 87.00 (22496/25856)
Train | Batch (196/196) | Top-1: 86.88 (43442/50000)
Regular: 1.398287057876587
Epoche: 20; regular: 1.398287057876587: flops 68504950
#Filters: 1429, #FLOPs: 61.31M | Top-1: 51.44
Epoch 21
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 86.59 (22389/25856)
Train | Batch (196/196) | Top-1: 86.78 (43392/50000)
Regular: 1.3484269380569458
Epoche: 21; regular: 1.3484269380569458: flops 68504950
#Filters: 1428, #FLOPs: 61.09M | Top-1: 72.73
Epoch 22
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 86.74 (22427/25856)
Train | Batch (196/196) | Top-1: 86.91 (43454/50000)
Regular: 1.2948212623596191
Epoche: 22; regular: 1.2948212623596191: flops 68504950
#Filters: 1429, #FLOPs: 61.32M | Top-1: 69.09
Epoch 23
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 86.94 (22480/25856)
Train | Batch (196/196) | Top-1: 86.96 (43482/50000)
Regular: 1.2469429969787598
Epoche: 23; regular: 1.2469429969787598: flops 68504950
#Filters: 1432, #FLOPs: 61.61M | Top-1: 70.79
Epoch 24
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 87.05 (22507/25856)
Train | Batch (196/196) | Top-1: 87.25 (43625/50000)
Regular: 1.1973872184753418
Epoche: 24; regular: 1.1973872184753418: flops 68504950
#Filters: 1432, #FLOPs: 61.68M | Top-1: 60.49
Epoch 25
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 87.23 (22555/25856)
Train | Batch (196/196) | Top-1: 87.03 (43514/50000)
Regular: 1.1484278440475464
Epoche: 25; regular: 1.1484278440475464: flops 68504950
#Filters: 1430, #FLOPs: 61.38M | Top-1: 75.26
Epoch 26
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 87.17 (22538/25856)
Train | Batch (196/196) | Top-1: 87.28 (43641/50000)
Regular: 1.0998834371566772
Epoche: 26; regular: 1.0998834371566772: flops 68504950
#Filters: 1428, #FLOPs: 61.24M | Top-1: 41.17
Epoch 27
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 87.32 (22578/25856)
Train | Batch (196/196) | Top-1: 87.27 (43637/50000)
Regular: 1.0514200925827026
Epoche: 27; regular: 1.0514200925827026: flops 68504950
#Filters: 1430, #FLOPs: 61.61M | Top-1: 62.94
Epoch 28
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 87.39 (22595/25856)
Train | Batch (196/196) | Top-1: 87.34 (43672/50000)
Regular: 1.0048059225082397
Epoche: 28; regular: 1.0048059225082397: flops 68504950
#Filters: 1400, #FLOPs: 59.27M | Top-1: 40.45
Epoch 29
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 87.08 (22515/25856)
Train | Batch (196/196) | Top-1: 87.13 (43565/50000)
Regular: 0.9688677787780762
Epoche: 29; regular: 0.9688677787780762: flops 68504950
#Filters: 1400, #FLOPs: 59.19M | Top-1: 35.36
Drin!!
Layers that will be prunned: [(1, 2), (3, 1), (11, 6), (13, 4), (15, 3), (19, 3), (21, 19), (23, 30), (25, 20), (27, 13), (29, 16)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 5
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 2
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 2
Layer index: 19; Pruned filters: 3
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 18
Layer index: 23; Pruned filters: 30
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 19
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 11
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 13
Target (flops): 68.863M
After Pruning | FLOPs: 48.740M | #Params: 0.457M
1.1903665813889264
After Growth | FLOPs: 67.639M | #Params: 0.642M
I: 5
flops: 67638886
Before Pruning | FLOPs: 67.639M | #Params: 0.642M
Epoch 0
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.35 (22584/25856)
Train | Batch (196/196) | Top-1: 87.14 (43568/50000)
Regular: 3.032109260559082
Epoche: 0; regular: 3.032109260559082: flops 67638886
#Filters: 1658, #FLOPs: 66.98M | Top-1: 41.35
Epoch 1
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 86.77 (22434/25856)
Train | Batch (196/196) | Top-1: 86.88 (43440/50000)
Regular: 2.912039041519165
Epoche: 1; regular: 2.912039041519165: flops 67638886
#Filters: 1655, #FLOPs: 66.94M | Top-1: 61.16
Epoch 2
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 86.95 (22481/25856)
Train | Batch (196/196) | Top-1: 86.81 (43405/50000)
Regular: 2.7893314361572266
Epoche: 2; regular: 2.7893314361572266: flops 67638886
#Filters: 1655, #FLOPs: 66.94M | Top-1: 54.17
Epoch 3
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 87.25 (22559/25856)
Train | Batch (196/196) | Top-1: 87.27 (43636/50000)
Regular: 2.6687300205230713
Epoche: 3; regular: 2.6687300205230713: flops 67638886
#Filters: 1655, #FLOPs: 66.90M | Top-1: 53.99
Epoch 4
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 87.45 (22610/25856)
Train | Batch (196/196) | Top-1: 87.27 (43635/50000)
Regular: 2.5478568077087402
Epoche: 4; regular: 2.5478568077087402: flops 67638886
#Filters: 1654, #FLOPs: 66.59M | Top-1: 39.69
Epoch 5
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 86.98 (22489/25856)
Train | Batch (196/196) | Top-1: 87.07 (43536/50000)
Regular: 2.429028034210205
Epoche: 5; regular: 2.429028034210205: flops 67638886
#Filters: 1655, #FLOPs: 66.67M | Top-1: 27.96
Epoch 6
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 86.78 (22437/25856)
Train | Batch (196/196) | Top-1: 86.98 (43489/50000)
Regular: 2.3206136226654053
Epoche: 6; regular: 2.3206136226654053: flops 67638886
#Filters: 1636, #FLOPs: 64.96M | Top-1: 40.15
Epoch 7
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 87.33 (22580/25856)
Train | Batch (196/196) | Top-1: 87.29 (43646/50000)
Regular: 2.2336976528167725
Epoche: 7; regular: 2.2336976528167725: flops 67638886
#Filters: 1638, #FLOPs: 65.62M | Top-1: 61.77
Epoch 8
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 87.53 (22633/25856)
Train | Batch (196/196) | Top-1: 87.46 (43732/50000)
Regular: 2.150956869125366
Epoche: 8; regular: 2.150956869125366: flops 67638886
#Filters: 1635, #FLOPs: 64.74M | Top-1: 51.62
Epoch 9
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 87.19 (22544/25856)
Train | Batch (196/196) | Top-1: 87.32 (43659/50000)
Regular: 2.068995475769043
Epoche: 9; regular: 2.068995475769043: flops 67638886
#Filters: 1634, #FLOPs: 64.69M | Top-1: 24.05
Epoch 10
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 87.44 (22608/25856)
Train | Batch (196/196) | Top-1: 87.34 (43671/50000)
Regular: 1.989326000213623
Epoche: 10; regular: 1.989326000213623: flops 67638886
#Filters: 1637, #FLOPs: 65.31M | Top-1: 67.42
Epoch 11
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 87.56 (22640/25856)
Train | Batch (196/196) | Top-1: 87.49 (43745/50000)
Regular: 1.909003734588623
Epoche: 11; regular: 1.909003734588623: flops 67638886
#Filters: 1632, #FLOPs: 64.34M | Top-1: 66.05
Epoch 12
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.21 (22548/25856)
Train | Batch (196/196) | Top-1: 87.28 (43642/50000)
Regular: 1.8386377096176147
Epoche: 12; regular: 1.8386377096176147: flops 67638886
#Filters: 1632, #FLOPs: 64.21M | Top-1: 67.91
Epoch 13
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 87.55 (22636/25856)
Train | Batch (196/196) | Top-1: 87.59 (43795/50000)
Regular: 1.7755041122436523
Epoche: 13; regular: 1.7755041122436523: flops 67638886
#Filters: 1630, #FLOPs: 63.98M | Top-1: 39.94
Epoch 14
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 87.96 (22743/25856)
Train | Batch (196/196) | Top-1: 87.54 (43772/50000)
Regular: 1.7125120162963867
Epoche: 14; regular: 1.7125120162963867: flops 67638886
#Filters: 1631, #FLOPs: 64.33M | Top-1: 46.99
Epoch 15
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 87.60 (22651/25856)
Train | Batch (196/196) | Top-1: 87.75 (43874/50000)
Regular: 1.6514612436294556
Epoche: 15; regular: 1.6514612436294556: flops 67638886
#Filters: 1628, #FLOPs: 64.12M | Top-1: 76.78
Epoch 16
Train | Batch (1/196) | Top-1: 90.23 (231/256)
Train | Batch (101/196) | Top-1: 87.76 (22690/25856)
Train | Batch (196/196) | Top-1: 87.85 (43923/50000)
Regular: 1.5954238176345825
Epoche: 16; regular: 1.5954238176345825: flops 67638886
#Filters: 1628, #FLOPs: 64.12M | Top-1: 40.22
Epoch 17
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 87.74 (22685/25856)
Train | Batch (196/196) | Top-1: 87.92 (43959/50000)
Regular: 1.5410975217819214
Epoche: 17; regular: 1.5410975217819214: flops 67638886
#Filters: 1629, #FLOPs: 64.43M | Top-1: 65.97
Epoch 18
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 87.51 (22627/25856)
Train | Batch (196/196) | Top-1: 87.70 (43848/50000)
Regular: 1.4897704124450684
Epoche: 18; regular: 1.4897704124450684: flops 67638886
#Filters: 1627, #FLOPs: 64.16M | Top-1: 55.90
Epoch 19
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 88.00 (22752/25856)
Train | Batch (196/196) | Top-1: 87.88 (43938/50000)
Regular: 1.438474416732788
Epoche: 19; regular: 1.438474416732788: flops 67638886
#Filters: 1626, #FLOPs: 64.03M | Top-1: 69.08
Epoch 20
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 87.84 (22713/25856)
Train | Batch (196/196) | Top-1: 87.87 (43935/50000)
Regular: 1.384414792060852
Epoche: 20; regular: 1.384414792060852: flops 67638886
#Filters: 1628, #FLOPs: 63.94M | Top-1: 56.75
Epoch 21
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 87.53 (22633/25856)
Train | Batch (196/196) | Top-1: 87.67 (43834/50000)
Regular: 1.3331949710845947
Epoche: 21; regular: 1.3331949710845947: flops 67638886
#Filters: 1624, #FLOPs: 63.81M | Top-1: 57.16
Epoch 22
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 87.69 (22673/25856)
Train | Batch (196/196) | Top-1: 87.73 (43863/50000)
Regular: 1.281406044960022
Epoche: 22; regular: 1.281406044960022: flops 67638886
#Filters: 1627, #FLOPs: 64.34M | Top-1: 64.48
Epoch 23
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 87.92 (22732/25856)
Train | Batch (196/196) | Top-1: 88.12 (44059/50000)
Regular: 1.2287189960479736
Epoche: 23; regular: 1.2287189960479736: flops 67638886
#Filters: 1605, #FLOPs: 62.25M | Top-1: 54.40
Epoch 24
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 88.17 (22798/25856)
Train | Batch (196/196) | Top-1: 88.08 (44041/50000)
Regular: 1.1812549829483032
Epoche: 24; regular: 1.1812549829483032: flops 67638886
#Filters: 1607, #FLOPs: 62.34M | Top-1: 62.69
Epoch 25
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 88.10 (22779/25856)
Train | Batch (196/196) | Top-1: 88.13 (44063/50000)
Regular: 1.1468092203140259
Epoche: 25; regular: 1.1468092203140259: flops 67638886
#Filters: 1605, #FLOPs: 62.20M | Top-1: 40.93
Epoch 26
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 88.05 (22767/25856)
Train | Batch (196/196) | Top-1: 88.11 (44057/50000)
Regular: 1.1147598028182983
Epoche: 26; regular: 1.1147598028182983: flops 67638886
#Filters: 1602, #FLOPs: 62.03M | Top-1: 68.54
Epoch 27
Train | Batch (1/196) | Top-1: 91.41 (234/256)
Train | Batch (101/196) | Top-1: 87.81 (22705/25856)
Train | Batch (196/196) | Top-1: 87.95 (43975/50000)
Regular: 1.0802592039108276
Epoche: 27; regular: 1.0802592039108276: flops 67638886
#Filters: 1592, #FLOPs: 61.94M | Top-1: 71.44
Epoch 28
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 88.12 (22784/25856)
Train | Batch (196/196) | Top-1: 88.14 (44069/50000)
Regular: 1.046683430671692
Epoche: 28; regular: 1.046683430671692: flops 67638886
#Filters: 1597, #FLOPs: 62.20M | Top-1: 60.78
Epoch 29
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 88.37 (22849/25856)
Train | Batch (196/196) | Top-1: 88.30 (44152/50000)
Regular: 1.014890432357788
Epoche: 29; regular: 1.014890432357788: flops 67638886
#Filters: 1596, #FLOPs: 62.34M | Top-1: 60.26
Drin!!
Layers that will be prunned: [(1, 1), (3, 1), (11, 5), (13, 2), (15, 3), (21, 17), (23, 15), (25, 5), (27, 4), (29, 5)]
Prunning filters..
Layer index: 1; Pruned filters: 1
Layer index: 3; Pruned filters: 1
Layer index: 11; Pruned filters: 5
Layer index: 13; Pruned filters: 2
Layer index: 15; Pruned filters: 1
Layer index: 15; Pruned filters: 2
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 14
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 11
Layer index: 25; Pruned filters: 5
Layer index: 27; Pruned filters: 1
Layer index: 27; Pruned filters: 3
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 3
Target (flops): 68.863M
After Pruning | FLOPs: 55.862M | #Params: 0.515M
1.1113313982418103
After Growth | FLOPs: 68.058M | #Params: 0.635M
Train | Batch (1/196) | Top-1: 70.31 (180/256)
Train | Batch (101/196) | Top-1: 29.45 (7615/25856)
Train | Batch (196/196) | Top-1: 34.49 (17244/50000)
Regular: nan
Epoche: 0; regular: nan: flops 68058384
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/jessica.buehler/venv/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
#Filters: 1776, #FLOPs: 68.06M | Top-1: 37.52
Epoch 0 | Top-1: 37.52
Train | Batch (1/196) | Top-1: 42.58 (109/256)
Train | Batch (101/196) | Top-1: 45.58 (11786/25856)
Train | Batch (196/196) | Top-1: 48.02 (24008/50000)
Regular: nan
Epoche: 1; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 44.83
Epoch 1 | Top-1: 44.83
Train | Batch (1/196) | Top-1: 52.34 (134/256)
Train | Batch (101/196) | Top-1: 55.41 (14327/25856)
Train | Batch (196/196) | Top-1: 57.45 (28725/50000)
Regular: nan
Epoche: 2; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 53.06
Epoch 2 | Top-1: 53.06
Train | Batch (1/196) | Top-1: 58.59 (150/256)
Train | Batch (101/196) | Top-1: 62.33 (16116/25856)
Train | Batch (196/196) | Top-1: 63.32 (31658/50000)
Regular: nan
Epoche: 3; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 57.10
Epoch 3 | Top-1: 57.10
Train | Batch (1/196) | Top-1: 62.89 (161/256)
Train | Batch (101/196) | Top-1: 66.09 (17089/25856)
Train | Batch (196/196) | Top-1: 67.00 (33501/50000)
Regular: nan
Epoche: 4; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 56.71
Epoch 4 | Top-1: 56.71
Train | Batch (1/196) | Top-1: 67.58 (173/256)
Train | Batch (101/196) | Top-1: 70.26 (18166/25856)
Train | Batch (196/196) | Top-1: 70.93 (35465/50000)
Regular: nan
Epoche: 5; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 63.76
Epoch 5 | Top-1: 63.76
Train | Batch (1/196) | Top-1: 71.09 (182/256)
Train | Batch (101/196) | Top-1: 73.67 (19048/25856)
Train | Batch (196/196) | Top-1: 74.12 (37059/50000)
Regular: nan
Epoche: 6; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 71.00
Epoch 6 | Top-1: 71.00
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 75.50 (19520/25856)
Train | Batch (196/196) | Top-1: 75.76 (37882/50000)
Regular: nan
Epoche: 7; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 69.07
Epoch 7 | Top-1: 69.07
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 76.84 (19867/25856)
Train | Batch (196/196) | Top-1: 77.22 (38608/50000)
Regular: nan
Epoche: 8; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 65.12
Epoch 8 | Top-1: 65.12
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 78.17 (20211/25856)
Train | Batch (196/196) | Top-1: 78.15 (39075/50000)
Regular: nan
Epoche: 9; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 65.58
Epoch 9 | Top-1: 65.58
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 79.34 (20513/25856)
Train | Batch (196/196) | Top-1: 79.13 (39564/50000)
Regular: nan
Epoche: 10; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 67.43
Epoch 10 | Top-1: 67.43
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 79.76 (20624/25856)
Train | Batch (196/196) | Top-1: 79.67 (39835/50000)
Regular: nan
Epoche: 11; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 76.25
Epoch 11 | Top-1: 76.25
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 80.40 (20787/25856)
Train | Batch (196/196) | Top-1: 80.58 (40292/50000)
Regular: nan
Epoche: 12; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 75.38
Epoch 12 | Top-1: 75.38
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 81.13 (20977/25856)
Train | Batch (196/196) | Top-1: 81.07 (40535/50000)
Regular: nan
Epoche: 13; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 71.01
Epoch 13 | Top-1: 71.01
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 81.44 (21056/25856)
Train | Batch (196/196) | Top-1: 81.34 (40670/50000)
Regular: nan
Epoche: 14; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 74.36
Epoch 14 | Top-1: 74.36
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 81.78 (21146/25856)
Train | Batch (196/196) | Top-1: 81.78 (40888/50000)
Regular: nan
Epoche: 15; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 73.97
Epoch 15 | Top-1: 73.97
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 82.64 (21367/25856)
Train | Batch (196/196) | Top-1: 82.03 (41015/50000)
Regular: nan
Epoche: 16; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 69.15
Epoch 16 | Top-1: 69.15
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 82.76 (21399/25856)
Train | Batch (196/196) | Top-1: 82.51 (41253/50000)
Regular: nan
Epoche: 17; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 67.05
Epoch 17 | Top-1: 67.05
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 82.67 (21375/25856)
Train | Batch (196/196) | Top-1: 82.61 (41303/50000)
Regular: nan
Epoche: 18; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 69.82
Epoch 18 | Top-1: 69.82
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 82.74 (21393/25856)
Train | Batch (196/196) | Top-1: 82.74 (41368/50000)
Regular: nan
Epoche: 19; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 67.52
Epoch 19 | Top-1: 67.52
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 83.38 (21560/25856)
Train | Batch (196/196) | Top-1: 83.17 (41585/50000)
Regular: nan
Epoche: 20; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 71.86
Epoch 20 | Top-1: 71.86
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 83.73 (21650/25856)
Train | Batch (196/196) | Top-1: 83.41 (41705/50000)
Regular: nan
Epoche: 21; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 79.60
Epoch 21 | Top-1: 79.60
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 83.44 (21575/25856)
Train | Batch (196/196) | Top-1: 83.50 (41748/50000)
Regular: nan
Epoche: 22; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 76.50
Epoch 22 | Top-1: 76.50
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 83.98 (21713/25856)
Train | Batch (196/196) | Top-1: 83.81 (41906/50000)
Regular: nan
Epoche: 23; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 66.60
Epoch 23 | Top-1: 66.60
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 83.91 (21697/25856)
Train | Batch (196/196) | Top-1: 83.78 (41890/50000)
Regular: nan
Epoche: 24; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 77.32
Epoch 24 | Top-1: 77.32
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 84.31 (21799/25856)
Train | Batch (196/196) | Top-1: 84.03 (42015/50000)
Regular: nan
Epoche: 25; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 72.54
Epoch 25 | Top-1: 72.54
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 84.28 (21792/25856)
Train | Batch (196/196) | Top-1: 84.19 (42096/50000)
Regular: nan
Epoche: 26; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 74.76
Epoch 26 | Top-1: 74.76
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.31 (21799/25856)
Train | Batch (196/196) | Top-1: 84.12 (42058/50000)
Regular: nan
Epoche: 27; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 73.54
Epoch 27 | Top-1: 73.54
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 84.31 (21798/25856)
Train | Batch (196/196) | Top-1: 84.43 (42217/50000)
Regular: nan
Epoche: 28; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 77.85
Epoch 28 | Top-1: 77.85
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 84.48 (21842/25856)
Train | Batch (196/196) | Top-1: 84.64 (42318/50000)
Regular: nan
Epoche: 29; regular: nan: flops 68058384
#Filters: 1776, #FLOPs: 68.06M | Top-1: 75.74
Epoch 29 | Top-1: 75.74
model: ResNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(42, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(4, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(42, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(7, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(42, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(42, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(42, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(42, 30, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(30, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(86, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(13, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(86, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(8, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(86, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(86, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(86, 83, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(83, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(168, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(59, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(168, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(30, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(168, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(12, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(168, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(17, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (4): AvgPool2d(kernel_size=8, stride=8, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=168, out_features=10, bias=True)
  )
)
Test acc: 75.74
