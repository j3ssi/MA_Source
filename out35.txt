no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.415 (0.415)	Data 0.160 (0.160)	Loss 3.3580 (3.3580)	Acc@1 10.938 (10.938)	Acc@5 58.594 (58.594)
Epoch: [1][64/391]	Time 0.408 (0.349)	Data 0.002 (0.004)	Loss 2.5356 (2.7954)	Acc@1 25.000 (19.760)	Acc@5 84.375 (73.642)
Epoch: [1][128/391]	Time 0.540 (0.419)	Data 0.002 (0.003)	Loss 2.4512 (2.6168)	Acc@1 28.906 (24.170)	Acc@5 78.906 (78.634)
Epoch: [1][192/391]	Time 0.507 (0.451)	Data 0.002 (0.003)	Loss 2.2762 (2.5204)	Acc@1 34.375 (27.149)	Acc@5 85.938 (81.404)
Epoch: [1][256/391]	Time 0.462 (0.468)	Data 0.001 (0.002)	Loss 2.0582 (2.4415)	Acc@1 47.656 (29.602)	Acc@5 91.406 (83.424)
Epoch: [1][320/391]	Time 0.515 (0.481)	Data 0.002 (0.002)	Loss 2.0013 (2.3729)	Acc@1 42.969 (31.985)	Acc@5 92.188 (84.818)
Epoch: [1][384/391]	Time 0.560 (0.490)	Data 0.002 (0.002)	Loss 1.9019 (2.3125)	Acc@1 58.594 (33.975)	Acc@5 89.062 (85.842)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.484 (0.484)	Data 0.233 (0.233)	Loss 2.0972 (2.0972)	Acc@1 41.406 (41.406)	Acc@5 87.500 (87.500)
Epoch: [2][64/391]	Time 0.460 (0.527)	Data 0.002 (0.005)	Loss 1.7811 (1.9168)	Acc@1 53.906 (47.079)	Acc@5 95.312 (92.488)
Epoch: [2][128/391]	Time 0.525 (0.515)	Data 0.002 (0.004)	Loss 1.8298 (1.8713)	Acc@1 46.094 (49.437)	Acc@5 92.969 (93.090)
Epoch: [2][192/391]	Time 0.571 (0.519)	Data 0.002 (0.003)	Loss 1.7738 (1.8409)	Acc@1 53.906 (50.518)	Acc@5 92.188 (93.572)
Epoch: [2][256/391]	Time 0.533 (0.523)	Data 0.002 (0.003)	Loss 1.7340 (1.8010)	Acc@1 56.250 (51.936)	Acc@5 92.969 (93.844)
Epoch: [2][320/391]	Time 0.507 (0.525)	Data 0.002 (0.003)	Loss 1.5489 (1.7679)	Acc@1 67.188 (53.047)	Acc@5 96.875 (94.156)
Epoch: [2][384/391]	Time 0.490 (0.525)	Data 0.002 (0.003)	Loss 1.5293 (1.7319)	Acc@1 59.375 (54.093)	Acc@5 98.438 (94.466)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.542 (0.542)	Data 0.205 (0.205)	Loss 1.5577 (1.5577)	Acc@1 59.375 (59.375)	Acc@5 97.656 (97.656)
Epoch: [3][64/391]	Time 0.484 (0.530)	Data 0.003 (0.005)	Loss 1.4589 (1.4677)	Acc@1 63.281 (62.356)	Acc@5 96.875 (96.526)
Epoch: [3][128/391]	Time 0.519 (0.520)	Data 0.002 (0.004)	Loss 1.2907 (1.4434)	Acc@1 69.531 (62.960)	Acc@5 99.219 (96.711)
Epoch: [3][192/391]	Time 0.529 (0.522)	Data 0.002 (0.003)	Loss 1.3388 (1.4196)	Acc@1 68.750 (63.589)	Acc@5 96.875 (96.822)
Epoch: [3][256/391]	Time 0.547 (0.525)	Data 0.002 (0.003)	Loss 1.3098 (1.3929)	Acc@1 68.750 (64.540)	Acc@5 98.438 (96.899)
Epoch: [3][320/391]	Time 0.508 (0.526)	Data 0.002 (0.003)	Loss 1.3727 (1.3765)	Acc@1 67.969 (64.926)	Acc@5 99.219 (96.975)
Epoch: [3][384/391]	Time 0.524 (0.527)	Data 0.002 (0.003)	Loss 1.1460 (1.3531)	Acc@1 71.875 (65.645)	Acc@5 99.219 (97.112)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.585 (0.585)	Data 0.229 (0.229)	Loss 1.1631 (1.1631)	Acc@1 70.312 (70.312)	Acc@5 97.656 (97.656)
Epoch: [4][64/391]	Time 0.513 (0.541)	Data 0.002 (0.006)	Loss 1.0414 (1.1915)	Acc@1 73.438 (70.781)	Acc@5 100.000 (97.849)
Epoch: [4][128/391]	Time 0.459 (0.506)	Data 0.002 (0.004)	Loss 1.1896 (1.1919)	Acc@1 72.656 (70.422)	Acc@5 98.438 (97.687)
Epoch: [4][192/391]	Time 0.479 (0.490)	Data 0.002 (0.003)	Loss 1.3123 (1.1828)	Acc@1 71.094 (70.758)	Acc@5 95.312 (97.697)
Epoch: [4][256/391]	Time 0.468 (0.484)	Data 0.002 (0.003)	Loss 1.1634 (1.1668)	Acc@1 70.312 (71.167)	Acc@5 99.219 (97.763)
Epoch: [4][320/391]	Time 0.471 (0.480)	Data 0.002 (0.003)	Loss 1.0588 (1.1539)	Acc@1 76.562 (71.644)	Acc@5 98.438 (97.817)
Epoch: [4][384/391]	Time 0.457 (0.478)	Data 0.002 (0.003)	Loss 1.1228 (1.1459)	Acc@1 71.875 (71.830)	Acc@5 96.094 (97.849)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.510 (0.510)	Data 0.240 (0.240)	Loss 1.0293 (1.0293)	Acc@1 75.000 (75.000)	Acc@5 98.438 (98.438)
Epoch: [5][64/391]	Time 0.446 (0.467)	Data 0.002 (0.006)	Loss 1.2054 (1.0732)	Acc@1 67.969 (73.630)	Acc@5 99.219 (98.329)
Epoch: [5][128/391]	Time 0.498 (0.467)	Data 0.002 (0.004)	Loss 0.8922 (1.0544)	Acc@1 83.594 (74.328)	Acc@5 96.094 (98.298)
Epoch: [5][192/391]	Time 0.482 (0.464)	Data 0.001 (0.003)	Loss 1.0098 (1.0506)	Acc@1 77.344 (74.676)	Acc@5 98.438 (98.324)
Epoch: [5][256/391]	Time 0.457 (0.466)	Data 0.002 (0.003)	Loss 1.0150 (1.0433)	Acc@1 73.438 (74.851)	Acc@5 97.656 (98.340)
Epoch: [5][320/391]	Time 0.431 (0.467)	Data 0.002 (0.003)	Loss 0.9318 (1.0390)	Acc@1 75.000 (74.961)	Acc@5 100.000 (98.350)
Epoch: [5][384/391]	Time 0.392 (0.466)	Data 0.002 (0.003)	Loss 1.0291 (1.0366)	Acc@1 75.000 (74.939)	Acc@5 97.656 (98.316)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.466 (0.466)	Data 0.195 (0.195)	Loss 0.9139 (0.9139)	Acc@1 78.125 (78.125)	Acc@5 98.438 (98.438)
Epoch: [6][64/391]	Time 0.464 (0.469)	Data 0.002 (0.005)	Loss 0.8869 (0.9779)	Acc@1 78.906 (76.719)	Acc@5 100.000 (98.462)
Epoch: [6][128/391]	Time 0.444 (0.465)	Data 0.002 (0.003)	Loss 0.8305 (0.9960)	Acc@1 81.250 (76.066)	Acc@5 100.000 (98.341)
Epoch: [6][192/391]	Time 0.485 (0.462)	Data 0.002 (0.003)	Loss 1.0529 (0.9937)	Acc@1 74.219 (76.154)	Acc@5 98.438 (98.365)
Epoch: [6][256/391]	Time 0.464 (0.463)	Data 0.002 (0.003)	Loss 0.9547 (0.9963)	Acc@1 77.344 (76.088)	Acc@5 98.438 (98.404)
Epoch: [6][320/391]	Time 0.477 (0.464)	Data 0.002 (0.003)	Loss 1.0474 (0.9958)	Acc@1 78.906 (76.061)	Acc@5 99.219 (98.469)
Epoch: [6][384/391]	Time 0.430 (0.464)	Data 0.001 (0.002)	Loss 0.9574 (0.9948)	Acc@1 78.906 (76.094)	Acc@5 98.438 (98.466)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.499 (0.499)	Data 0.205 (0.205)	Loss 0.9227 (0.9227)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [7][64/391]	Time 0.472 (0.469)	Data 0.002 (0.005)	Loss 0.9038 (0.9719)	Acc@1 79.688 (76.695)	Acc@5 99.219 (98.618)
Epoch: [7][128/391]	Time 0.418 (0.466)	Data 0.002 (0.004)	Loss 0.9928 (0.9687)	Acc@1 76.562 (76.841)	Acc@5 96.875 (98.583)
Epoch: [7][192/391]	Time 0.463 (0.465)	Data 0.002 (0.003)	Loss 1.1619 (0.9672)	Acc@1 68.750 (76.822)	Acc@5 97.656 (98.563)
Epoch: [7][256/391]	Time 0.502 (0.465)	Data 0.002 (0.003)	Loss 0.9336 (0.9658)	Acc@1 79.688 (76.985)	Acc@5 100.000 (98.577)
Epoch: [7][320/391]	Time 0.503 (0.466)	Data 0.002 (0.003)	Loss 0.9127 (0.9640)	Acc@1 80.469 (77.049)	Acc@5 97.656 (98.613)
Epoch: [7][384/391]	Time 0.460 (0.466)	Data 0.002 (0.003)	Loss 0.9112 (0.9650)	Acc@1 77.344 (77.058)	Acc@5 98.438 (98.608)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.518 (0.518)	Data 0.189 (0.189)	Loss 0.7781 (0.7781)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [8][64/391]	Time 0.459 (0.471)	Data 0.002 (0.005)	Loss 0.9746 (0.9411)	Acc@1 80.469 (78.065)	Acc@5 97.656 (98.738)
Epoch: [8][128/391]	Time 0.491 (0.468)	Data 0.002 (0.003)	Loss 0.9723 (0.9430)	Acc@1 78.906 (77.883)	Acc@5 98.438 (98.631)
Epoch: [8][192/391]	Time 0.466 (0.466)	Data 0.002 (0.003)	Loss 0.8596 (0.9399)	Acc@1 79.688 (77.801)	Acc@5 100.000 (98.664)
Epoch: [8][256/391]	Time 0.510 (0.465)	Data 0.002 (0.003)	Loss 1.0187 (0.9416)	Acc@1 72.656 (77.775)	Acc@5 99.219 (98.690)
Epoch: [8][320/391]	Time 0.486 (0.466)	Data 0.002 (0.003)	Loss 0.8759 (0.9450)	Acc@1 80.469 (77.704)	Acc@5 99.219 (98.681)
Epoch: [8][384/391]	Time 0.450 (0.466)	Data 0.002 (0.003)	Loss 0.7871 (0.9442)	Acc@1 82.812 (77.703)	Acc@5 100.000 (98.653)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.504 (0.504)	Data 0.233 (0.233)	Loss 1.0247 (1.0247)	Acc@1 75.000 (75.000)	Acc@5 98.438 (98.438)
Epoch: [9][64/391]	Time 0.420 (0.470)	Data 0.002 (0.006)	Loss 0.8873 (0.9152)	Acc@1 82.031 (78.774)	Acc@5 98.438 (98.834)
Epoch: [9][128/391]	Time 0.456 (0.470)	Data 0.001 (0.004)	Loss 0.8905 (0.9165)	Acc@1 83.594 (78.664)	Acc@5 99.219 (98.831)
Epoch: [9][192/391]	Time 0.460 (0.466)	Data 0.001 (0.003)	Loss 0.9521 (0.9178)	Acc@1 78.125 (78.821)	Acc@5 97.656 (98.806)
Epoch: [9][256/391]	Time 0.509 (0.465)	Data 0.002 (0.003)	Loss 0.9337 (0.9202)	Acc@1 75.781 (78.757)	Acc@5 99.219 (98.769)
Epoch: [9][320/391]	Time 0.452 (0.466)	Data 0.002 (0.003)	Loss 0.9903 (0.9254)	Acc@1 79.688 (78.575)	Acc@5 97.656 (98.764)
Epoch: [9][384/391]	Time 0.449 (0.466)	Data 0.002 (0.003)	Loss 1.0916 (0.9267)	Acc@1 75.781 (78.506)	Acc@5 96.875 (98.742)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.436 (0.436)	Data 0.227 (0.227)	Loss 0.8001 (0.8001)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.506 (0.464)	Data 0.002 (0.005)	Loss 0.9862 (0.9206)	Acc@1 78.125 (78.534)	Acc@5 97.656 (98.750)
Epoch: [10][128/391]	Time 0.431 (0.465)	Data 0.002 (0.004)	Loss 0.7397 (0.9141)	Acc@1 83.594 (79.052)	Acc@5 98.438 (98.734)
Epoch: [10][192/391]	Time 0.451 (0.463)	Data 0.013 (0.003)	Loss 0.7736 (0.9023)	Acc@1 87.500 (79.594)	Acc@5 99.219 (98.838)
Epoch: [10][256/391]	Time 0.388 (0.461)	Data 0.002 (0.003)	Loss 0.9883 (0.9115)	Acc@1 75.000 (79.207)	Acc@5 97.656 (98.845)
Epoch: [10][320/391]	Time 0.488 (0.464)	Data 0.002 (0.003)	Loss 1.0153 (0.9171)	Acc@1 79.688 (79.035)	Acc@5 96.094 (98.781)
Epoch: [10][384/391]	Time 0.456 (0.463)	Data 0.002 (0.003)	Loss 0.8578 (0.9186)	Acc@1 79.688 (78.971)	Acc@5 97.656 (98.724)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 470074 ; 487386 ; 0.9644798988891762

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.449 (0.449)	Data 0.219 (0.219)	Loss 0.9121 (0.9121)	Acc@1 78.125 (78.125)	Acc@5 97.656 (97.656)
Epoch: [11][64/391]	Time 0.473 (0.462)	Data 0.002 (0.005)	Loss 0.7995 (0.8934)	Acc@1 83.594 (80.000)	Acc@5 99.219 (98.882)
Epoch: [11][128/391]	Time 0.460 (0.460)	Data 0.002 (0.004)	Loss 0.7189 (0.9028)	Acc@1 86.719 (79.512)	Acc@5 100.000 (98.934)
Epoch: [11][192/391]	Time 0.473 (0.461)	Data 0.002 (0.003)	Loss 0.8756 (0.9015)	Acc@1 82.812 (79.574)	Acc@5 99.219 (98.899)
Epoch: [11][256/391]	Time 0.481 (0.461)	Data 0.002 (0.003)	Loss 0.9822 (0.9045)	Acc@1 80.469 (79.405)	Acc@5 98.438 (98.836)
Epoch: [11][320/391]	Time 0.475 (0.464)	Data 0.002 (0.003)	Loss 0.8535 (0.9011)	Acc@1 82.031 (79.430)	Acc@5 98.438 (98.839)
Epoch: [11][384/391]	Time 0.453 (0.465)	Data 0.002 (0.003)	Loss 0.9072 (0.9014)	Acc@1 80.469 (79.399)	Acc@5 97.656 (98.862)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.495 (0.495)	Data 0.223 (0.223)	Loss 0.9217 (0.9217)	Acc@1 77.344 (77.344)	Acc@5 98.438 (98.438)
Epoch: [12][64/391]	Time 0.473 (0.464)	Data 0.002 (0.005)	Loss 0.8538 (0.8879)	Acc@1 82.031 (79.375)	Acc@5 100.000 (98.726)
Epoch: [12][128/391]	Time 0.454 (0.462)	Data 0.002 (0.004)	Loss 0.7374 (0.9027)	Acc@1 83.594 (78.979)	Acc@5 99.219 (98.686)
Epoch: [12][192/391]	Time 0.460 (0.461)	Data 0.002 (0.003)	Loss 0.8485 (0.9002)	Acc@1 82.031 (79.080)	Acc@5 98.438 (98.688)
Epoch: [12][256/391]	Time 0.465 (0.461)	Data 0.002 (0.003)	Loss 0.8727 (0.8957)	Acc@1 78.906 (79.283)	Acc@5 100.000 (98.751)
Epoch: [12][320/391]	Time 0.462 (0.461)	Data 0.002 (0.003)	Loss 0.7458 (0.8961)	Acc@1 85.938 (79.296)	Acc@5 98.438 (98.744)
Epoch: [12][384/391]	Time 0.463 (0.462)	Data 0.002 (0.003)	Loss 0.7884 (0.8957)	Acc@1 83.594 (79.334)	Acc@5 98.438 (98.752)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.520 (0.520)	Data 0.224 (0.224)	Loss 0.8203 (0.8203)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [13][64/391]	Time 0.475 (0.474)	Data 0.002 (0.005)	Loss 0.7829 (0.8614)	Acc@1 81.250 (80.733)	Acc@5 99.219 (99.038)
Epoch: [13][128/391]	Time 0.436 (0.466)	Data 0.002 (0.004)	Loss 0.9209 (0.8796)	Acc@1 76.562 (80.057)	Acc@5 98.438 (98.952)
Epoch: [13][192/391]	Time 0.451 (0.465)	Data 0.002 (0.003)	Loss 0.9858 (0.8806)	Acc@1 78.906 (79.987)	Acc@5 97.656 (98.923)
Epoch: [13][256/391]	Time 0.484 (0.465)	Data 0.002 (0.003)	Loss 0.8212 (0.8827)	Acc@1 84.375 (79.897)	Acc@5 98.438 (98.878)
Epoch: [13][320/391]	Time 0.452 (0.466)	Data 0.002 (0.003)	Loss 0.8819 (0.8826)	Acc@1 82.031 (79.972)	Acc@5 97.656 (98.859)
Epoch: [13][384/391]	Time 0.464 (0.466)	Data 0.002 (0.003)	Loss 0.7975 (0.8807)	Acc@1 82.031 (80.026)	Acc@5 100.000 (98.890)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.482 (0.482)	Data 0.216 (0.216)	Loss 1.0226 (1.0226)	Acc@1 72.656 (72.656)	Acc@5 96.094 (96.094)
Epoch: [14][64/391]	Time 0.463 (0.469)	Data 0.002 (0.005)	Loss 0.8601 (0.8825)	Acc@1 78.906 (79.663)	Acc@5 100.000 (99.014)
Epoch: [14][128/391]	Time 0.486 (0.465)	Data 0.002 (0.004)	Loss 0.8885 (0.8731)	Acc@1 79.688 (80.087)	Acc@5 98.438 (99.007)
Epoch: [14][192/391]	Time 0.443 (0.464)	Data 0.002 (0.003)	Loss 0.8941 (0.8807)	Acc@1 81.250 (79.975)	Acc@5 100.000 (98.972)
Epoch: [14][256/391]	Time 0.451 (0.463)	Data 0.002 (0.003)	Loss 0.8892 (0.8870)	Acc@1 85.938 (79.873)	Acc@5 96.875 (98.921)
Epoch: [14][320/391]	Time 0.454 (0.464)	Data 0.002 (0.003)	Loss 0.8467 (0.8847)	Acc@1 79.688 (79.868)	Acc@5 99.219 (98.888)
Epoch: [14][384/391]	Time 0.450 (0.464)	Data 0.001 (0.003)	Loss 0.8838 (0.8806)	Acc@1 78.906 (80.043)	Acc@5 100.000 (98.898)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.459 (0.459)	Data 0.190 (0.190)	Loss 0.9415 (0.9415)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [15][64/391]	Time 0.474 (0.467)	Data 0.002 (0.005)	Loss 0.7258 (0.8559)	Acc@1 84.375 (81.082)	Acc@5 100.000 (98.990)
Epoch: [15][128/391]	Time 0.467 (0.463)	Data 0.002 (0.004)	Loss 0.8843 (0.8646)	Acc@1 80.469 (80.747)	Acc@5 99.219 (98.964)
Epoch: [15][192/391]	Time 0.454 (0.461)	Data 0.002 (0.003)	Loss 0.9456 (0.8765)	Acc@1 75.000 (80.315)	Acc@5 99.219 (98.960)
Epoch: [15][256/391]	Time 0.463 (0.461)	Data 0.002 (0.003)	Loss 0.8246 (0.8734)	Acc@1 82.031 (80.359)	Acc@5 99.219 (98.939)
Epoch: [15][320/391]	Time 0.427 (0.461)	Data 0.002 (0.003)	Loss 0.7997 (0.8763)	Acc@1 84.375 (80.306)	Acc@5 100.000 (98.924)
Epoch: [15][384/391]	Time 0.463 (0.461)	Data 0.002 (0.003)	Loss 0.6955 (0.8750)	Acc@1 88.281 (80.418)	Acc@5 100.000 (98.933)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 382334 ; 487386 ; 0.7844583143545363

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.451 (0.451)	Data 0.179 (0.179)	Loss 1.0694 (1.0694)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [16][64/391]	Time 0.459 (0.458)	Data 0.002 (0.005)	Loss 0.9797 (0.8513)	Acc@1 79.688 (81.082)	Acc@5 97.656 (99.038)
Epoch: [16][128/391]	Time 0.461 (0.458)	Data 0.002 (0.003)	Loss 0.9439 (0.8742)	Acc@1 75.781 (80.378)	Acc@5 97.656 (98.886)
Epoch: [16][192/391]	Time 0.466 (0.459)	Data 0.002 (0.003)	Loss 0.7810 (0.8686)	Acc@1 84.375 (80.594)	Acc@5 100.000 (98.952)
Epoch: [16][256/391]	Time 0.414 (0.458)	Data 0.001 (0.003)	Loss 0.9504 (0.8693)	Acc@1 78.906 (80.496)	Acc@5 99.219 (98.948)
Epoch: [16][320/391]	Time 0.512 (0.458)	Data 0.002 (0.003)	Loss 0.7413 (0.8694)	Acc@1 83.594 (80.547)	Acc@5 97.656 (98.961)
Epoch: [16][384/391]	Time 0.491 (0.458)	Data 0.002 (0.002)	Loss 0.7801 (0.8694)	Acc@1 84.375 (80.475)	Acc@5 98.438 (98.971)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.490 (0.490)	Data 0.290 (0.290)	Loss 0.7277 (0.7277)	Acc@1 89.844 (89.844)	Acc@5 99.219 (99.219)
Epoch: [17][64/391]	Time 0.448 (0.460)	Data 0.002 (0.006)	Loss 0.7398 (0.8499)	Acc@1 82.031 (81.058)	Acc@5 99.219 (99.099)
Epoch: [17][128/391]	Time 0.449 (0.460)	Data 0.002 (0.004)	Loss 0.8158 (0.8676)	Acc@1 80.469 (80.372)	Acc@5 98.438 (98.995)
Epoch: [17][192/391]	Time 0.447 (0.459)	Data 0.002 (0.003)	Loss 0.8382 (0.8673)	Acc@1 81.250 (80.404)	Acc@5 99.219 (99.028)
Epoch: [17][256/391]	Time 0.451 (0.460)	Data 0.002 (0.003)	Loss 0.8667 (0.8651)	Acc@1 80.469 (80.502)	Acc@5 99.219 (99.030)
Epoch: [17][320/391]	Time 0.448 (0.460)	Data 0.001 (0.003)	Loss 0.9904 (0.8671)	Acc@1 77.344 (80.420)	Acc@5 98.438 (99.014)
Epoch: [17][384/391]	Time 0.416 (0.460)	Data 0.002 (0.003)	Loss 0.8477 (0.8679)	Acc@1 79.688 (80.404)	Acc@5 98.438 (99.020)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.481 (0.481)	Data 0.230 (0.230)	Loss 0.9887 (0.9887)	Acc@1 77.344 (77.344)	Acc@5 96.875 (96.875)
Epoch: [18][64/391]	Time 0.480 (0.467)	Data 0.002 (0.006)	Loss 0.8627 (0.8404)	Acc@1 78.906 (81.767)	Acc@5 99.219 (98.762)
Epoch: [18][128/391]	Time 0.453 (0.460)	Data 0.002 (0.004)	Loss 0.9859 (0.8516)	Acc@1 75.000 (81.214)	Acc@5 98.438 (98.886)
Epoch: [18][192/391]	Time 0.445 (0.461)	Data 0.002 (0.003)	Loss 0.8104 (0.8572)	Acc@1 80.469 (80.906)	Acc@5 100.000 (98.899)
Epoch: [18][256/391]	Time 0.430 (0.459)	Data 0.002 (0.003)	Loss 0.8699 (0.8606)	Acc@1 80.469 (80.697)	Acc@5 99.219 (98.909)
Epoch: [18][320/391]	Time 0.440 (0.460)	Data 0.002 (0.003)	Loss 0.8413 (0.8582)	Acc@1 84.375 (80.724)	Acc@5 97.656 (98.910)
Epoch: [18][384/391]	Time 0.441 (0.461)	Data 0.002 (0.003)	Loss 0.9116 (0.8576)	Acc@1 78.906 (80.795)	Acc@5 98.438 (98.914)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.451 (0.451)	Data 0.199 (0.199)	Loss 1.1379 (1.1379)	Acc@1 69.531 (69.531)	Acc@5 98.438 (98.438)
Epoch: [19][64/391]	Time 0.445 (0.468)	Data 0.002 (0.005)	Loss 0.7996 (0.8605)	Acc@1 84.375 (80.228)	Acc@5 100.000 (98.930)
Epoch: [19][128/391]	Time 0.473 (0.463)	Data 0.002 (0.004)	Loss 0.7779 (0.8710)	Acc@1 82.031 (80.136)	Acc@5 100.000 (98.807)
Epoch: [19][192/391]	Time 0.468 (0.461)	Data 0.002 (0.003)	Loss 0.6597 (0.8651)	Acc@1 89.062 (80.416)	Acc@5 99.219 (98.907)
Epoch: [19][256/391]	Time 0.494 (0.461)	Data 0.002 (0.003)	Loss 0.9397 (0.8614)	Acc@1 75.000 (80.630)	Acc@5 98.438 (98.951)
Epoch: [19][320/391]	Time 0.459 (0.460)	Data 0.001 (0.003)	Loss 0.7004 (0.8571)	Acc@1 82.812 (80.802)	Acc@5 99.219 (98.949)
Epoch: [19][384/391]	Time 0.476 (0.460)	Data 0.002 (0.002)	Loss 0.6901 (0.8577)	Acc@1 89.062 (80.733)	Acc@5 100.000 (98.945)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.501 (0.501)	Data 0.202 (0.202)	Loss 0.9045 (0.9045)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [20][64/391]	Time 0.470 (0.465)	Data 0.002 (0.005)	Loss 0.8962 (0.8440)	Acc@1 81.250 (81.310)	Acc@5 98.438 (98.966)
Epoch: [20][128/391]	Time 0.430 (0.463)	Data 0.002 (0.004)	Loss 0.9678 (0.8508)	Acc@1 75.781 (80.899)	Acc@5 98.438 (99.061)
Epoch: [20][192/391]	Time 0.475 (0.462)	Data 0.002 (0.003)	Loss 0.7141 (0.8529)	Acc@1 82.812 (80.874)	Acc@5 100.000 (99.000)
Epoch: [20][256/391]	Time 0.512 (0.462)	Data 0.002 (0.003)	Loss 0.8193 (0.8541)	Acc@1 79.688 (80.803)	Acc@5 99.219 (98.997)
Epoch: [20][320/391]	Time 0.471 (0.459)	Data 0.002 (0.003)	Loss 0.9069 (0.8546)	Acc@1 75.781 (80.756)	Acc@5 100.000 (98.985)
Epoch: [20][384/391]	Time 0.459 (0.460)	Data 0.002 (0.002)	Loss 0.8507 (0.8555)	Acc@1 80.469 (80.812)	Acc@5 100.000 (98.998)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv30.weight

 RM:  module.conv31.weight

Module List Length:  68
Index1: 60
Index: 31
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 32; k: 32
Bool1: False
Bool2: True
j: 33; k: 32
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(25, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(51, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 58: Conv2d(64, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 59: BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 60: Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 61: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 62 gegen 66: Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 63 gegen 67: BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(25, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(51, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  64

Module List Length:  64
Index1: 58
Index: 30
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 31; k: 30
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(25, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(51, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 58 gegen 62: Conv2d(64, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 59 gegen 63: BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(25, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(51, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): AdaptiveAvgPool2d(output_size=(1, 1))
    (59): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  60
Count: 270228 ; 487386 ; 0.5544435006339944

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.415 (0.415)	Data 0.214 (0.214)	Loss 2.0530 (2.0530)	Acc@1 31.250 (31.250)	Acc@5 91.406 (91.406)
Epoch: [21][64/391]	Time 0.388 (0.411)	Data 0.002 (0.005)	Loss 0.8291 (1.1301)	Acc@1 80.469 (71.899)	Acc@5 100.000 (98.101)
Epoch: [21][128/391]	Time 0.390 (0.410)	Data 0.002 (0.004)	Loss 0.8324 (1.0054)	Acc@1 78.906 (75.115)	Acc@5 97.656 (98.474)
Epoch: [21][192/391]	Time 0.414 (0.411)	Data 0.001 (0.003)	Loss 0.8023 (0.9530)	Acc@1 82.812 (76.429)	Acc@5 97.656 (98.579)
Epoch: [21][256/391]	Time 0.450 (0.412)	Data 0.002 (0.003)	Loss 0.8998 (0.9347)	Acc@1 80.469 (76.876)	Acc@5 96.875 (98.620)
Epoch: [21][320/391]	Time 0.442 (0.414)	Data 0.002 (0.003)	Loss 0.7736 (0.9168)	Acc@1 83.594 (77.431)	Acc@5 98.438 (98.661)
Epoch: [21][384/391]	Time 0.421 (0.415)	Data 0.001 (0.003)	Loss 0.7913 (0.9033)	Acc@1 84.375 (77.865)	Acc@5 99.219 (98.687)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.426 (0.426)	Data 0.209 (0.209)	Loss 0.7188 (0.7188)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [22][64/391]	Time 0.386 (0.419)	Data 0.002 (0.005)	Loss 0.7640 (0.8216)	Acc@1 76.562 (80.084)	Acc@5 100.000 (99.002)
Epoch: [22][128/391]	Time 0.424 (0.416)	Data 0.002 (0.004)	Loss 0.7761 (0.8204)	Acc@1 83.594 (80.560)	Acc@5 98.438 (98.946)
Epoch: [22][192/391]	Time 0.414 (0.415)	Data 0.001 (0.003)	Loss 0.8831 (0.8239)	Acc@1 76.562 (80.359)	Acc@5 99.219 (98.996)
Epoch: [22][256/391]	Time 0.422 (0.416)	Data 0.002 (0.003)	Loss 0.8388 (0.8259)	Acc@1 82.031 (80.390)	Acc@5 98.438 (98.966)
Epoch: [22][320/391]	Time 0.490 (0.417)	Data 0.002 (0.003)	Loss 0.8903 (0.8275)	Acc@1 81.250 (80.335)	Acc@5 100.000 (98.975)
Epoch: [22][384/391]	Time 0.470 (0.417)	Data 0.002 (0.002)	Loss 0.8100 (0.8287)	Acc@1 80.469 (80.270)	Acc@5 98.438 (98.963)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.452 (0.452)	Data 0.267 (0.267)	Loss 0.7021 (0.7021)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [23][64/391]	Time 0.350 (0.418)	Data 0.001 (0.006)	Loss 0.8124 (0.8046)	Acc@1 82.812 (81.142)	Acc@5 99.219 (99.123)
Epoch: [23][128/391]	Time 0.404 (0.420)	Data 0.002 (0.004)	Loss 0.8890 (0.8103)	Acc@1 76.562 (81.086)	Acc@5 99.219 (99.098)
Epoch: [23][192/391]	Time 0.413 (0.419)	Data 0.003 (0.003)	Loss 0.8523 (0.8119)	Acc@1 79.688 (81.056)	Acc@5 98.438 (99.069)
Epoch: [23][256/391]	Time 0.436 (0.418)	Data 0.002 (0.003)	Loss 0.9191 (0.8104)	Acc@1 78.906 (81.098)	Acc@5 97.656 (99.094)
Epoch: [23][320/391]	Time 0.392 (0.419)	Data 0.002 (0.003)	Loss 0.8521 (0.8166)	Acc@1 75.781 (80.878)	Acc@5 100.000 (99.080)
Epoch: [23][384/391]	Time 0.445 (0.419)	Data 0.001 (0.003)	Loss 0.6379 (0.8223)	Acc@1 86.719 (80.664)	Acc@5 100.000 (99.020)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.460 (0.460)	Data 0.183 (0.183)	Loss 0.9078 (0.9078)	Acc@1 78.906 (78.906)	Acc@5 96.875 (96.875)
Epoch: [24][64/391]	Time 0.414 (0.423)	Data 0.001 (0.005)	Loss 0.8011 (0.7904)	Acc@1 79.688 (81.262)	Acc@5 98.438 (99.147)
Epoch: [24][128/391]	Time 0.408 (0.423)	Data 0.002 (0.003)	Loss 0.7892 (0.8037)	Acc@1 78.906 (80.984)	Acc@5 99.219 (98.934)
Epoch: [24][192/391]	Time 0.441 (0.421)	Data 0.002 (0.003)	Loss 0.8484 (0.8184)	Acc@1 79.688 (80.683)	Acc@5 96.875 (98.915)
Epoch: [24][256/391]	Time 0.430 (0.420)	Data 0.002 (0.003)	Loss 0.6351 (0.8225)	Acc@1 85.938 (80.636)	Acc@5 100.000 (98.945)
Epoch: [24][320/391]	Time 0.416 (0.419)	Data 0.002 (0.003)	Loss 0.7779 (0.8223)	Acc@1 81.250 (80.683)	Acc@5 100.000 (98.927)
Epoch: [24][384/391]	Time 0.414 (0.418)	Data 0.002 (0.002)	Loss 0.7954 (0.8197)	Acc@1 86.719 (80.814)	Acc@5 99.219 (98.963)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.421 (0.421)	Data 0.200 (0.200)	Loss 0.8480 (0.8480)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [25][64/391]	Time 0.383 (0.420)	Data 0.002 (0.005)	Loss 0.7109 (0.8102)	Acc@1 85.938 (81.562)	Acc@5 100.000 (98.930)
Epoch: [25][128/391]	Time 0.447 (0.422)	Data 0.002 (0.004)	Loss 0.7433 (0.8067)	Acc@1 80.469 (81.632)	Acc@5 98.438 (98.995)
Epoch: [25][192/391]	Time 0.410 (0.422)	Data 0.002 (0.003)	Loss 0.7488 (0.8149)	Acc@1 82.812 (81.266)	Acc@5 99.219 (98.943)
Epoch: [25][256/391]	Time 0.388 (0.420)	Data 0.002 (0.003)	Loss 0.7450 (0.8172)	Acc@1 85.156 (81.235)	Acc@5 100.000 (98.985)
Epoch: [25][320/391]	Time 0.399 (0.419)	Data 0.002 (0.003)	Loss 0.9101 (0.8175)	Acc@1 77.344 (81.201)	Acc@5 99.219 (98.988)
Epoch: [25][384/391]	Time 0.421 (0.417)	Data 0.001 (0.002)	Loss 0.7154 (0.8210)	Acc@1 84.375 (81.110)	Acc@5 98.438 (98.949)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.533 (0.533)	Data 0.222 (0.222)	Loss 3.2874 (3.2874)	Acc@1 8.594 (8.594)	Acc@5 42.969 (42.969)
Epoch: [1][64/391]	Time 0.495 (0.448)	Data 0.002 (0.005)	Loss 2.6938 (2.7912)	Acc@1 24.219 (20.962)	Acc@5 83.594 (72.776)
Epoch: [1][128/391]	Time 0.499 (0.454)	Data 0.002 (0.003)	Loss 2.3667 (2.6216)	Acc@1 30.469 (25.551)	Acc@5 86.719 (79.403)
Epoch: [1][192/391]	Time 0.398 (0.453)	Data 0.002 (0.003)	Loss 2.3384 (2.5071)	Acc@1 32.031 (29.550)	Acc@5 82.812 (82.533)
Epoch: [1][256/391]	Time 0.487 (0.455)	Data 0.001 (0.003)	Loss 2.1581 (2.4136)	Acc@1 45.312 (32.603)	Acc@5 95.312 (84.688)
Epoch: [1][320/391]	Time 0.470 (0.457)	Data 0.002 (0.003)	Loss 2.0967 (2.3341)	Acc@1 45.312 (35.324)	Acc@5 92.188 (86.161)
Epoch: [1][384/391]	Time 0.462 (0.458)	Data 0.001 (0.002)	Loss 1.8461 (2.2615)	Acc@1 51.562 (37.831)	Acc@5 96.094 (87.364)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.469 (0.469)	Data 0.194 (0.194)	Loss 1.7358 (1.7358)	Acc@1 53.906 (53.906)	Acc@5 93.750 (93.750)
Epoch: [2][64/391]	Time 0.468 (0.467)	Data 0.002 (0.005)	Loss 1.5451 (1.7724)	Acc@1 64.062 (54.327)	Acc@5 95.312 (94.303)
Epoch: [2][128/391]	Time 0.459 (0.464)	Data 0.001 (0.003)	Loss 1.7485 (1.7367)	Acc@1 53.125 (55.039)	Acc@5 93.750 (94.695)
Epoch: [2][192/391]	Time 0.455 (0.464)	Data 0.002 (0.003)	Loss 1.5004 (1.7057)	Acc@1 64.062 (55.898)	Acc@5 95.312 (94.904)
Epoch: [2][256/391]	Time 0.488 (0.464)	Data 0.002 (0.003)	Loss 1.6579 (1.6615)	Acc@1 59.375 (57.338)	Acc@5 94.531 (95.200)
Epoch: [2][320/391]	Time 0.476 (0.464)	Data 0.002 (0.002)	Loss 1.4501 (1.6269)	Acc@1 64.062 (58.467)	Acc@5 96.875 (95.507)
Epoch: [2][384/391]	Time 0.474 (0.463)	Data 0.002 (0.002)	Loss 1.3898 (1.5879)	Acc@1 62.500 (59.584)	Acc@5 96.875 (95.777)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.508 (0.508)	Data 0.214 (0.214)	Loss 1.2621 (1.2621)	Acc@1 75.000 (75.000)	Acc@5 96.875 (96.875)
Epoch: [3][64/391]	Time 0.486 (0.466)	Data 0.002 (0.005)	Loss 1.2347 (1.3556)	Acc@1 71.094 (66.899)	Acc@5 99.219 (96.827)
Epoch: [3][128/391]	Time 0.446 (0.465)	Data 0.003 (0.004)	Loss 1.3620 (1.3481)	Acc@1 68.750 (66.818)	Acc@5 97.656 (97.154)
Epoch: [3][192/391]	Time 0.468 (0.463)	Data 0.001 (0.003)	Loss 1.2108 (1.3287)	Acc@1 67.969 (67.208)	Acc@5 99.219 (97.284)
Epoch: [3][256/391]	Time 0.451 (0.464)	Data 0.002 (0.003)	Loss 1.3208 (1.3083)	Acc@1 64.844 (67.774)	Acc@5 98.438 (97.389)
Epoch: [3][320/391]	Time 0.459 (0.464)	Data 0.001 (0.003)	Loss 1.1229 (1.2922)	Acc@1 71.094 (68.200)	Acc@5 98.438 (97.415)
Epoch: [3][384/391]	Time 0.437 (0.464)	Data 0.002 (0.002)	Loss 1.0938 (1.2720)	Acc@1 74.219 (68.707)	Acc@5 99.219 (97.520)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.545 (0.545)	Data 0.212 (0.212)	Loss 1.2183 (1.2183)	Acc@1 70.312 (70.312)	Acc@5 97.656 (97.656)
Epoch: [4][64/391]	Time 0.446 (0.466)	Data 0.002 (0.005)	Loss 1.1721 (1.1613)	Acc@1 71.875 (71.526)	Acc@5 98.438 (98.041)
Epoch: [4][128/391]	Time 0.473 (0.462)	Data 0.002 (0.004)	Loss 1.0793 (1.1485)	Acc@1 78.125 (71.942)	Acc@5 100.000 (98.086)
Epoch: [4][192/391]	Time 0.502 (0.465)	Data 0.002 (0.003)	Loss 1.1206 (1.1401)	Acc@1 72.656 (72.170)	Acc@5 98.438 (98.081)
Epoch: [4][256/391]	Time 0.465 (0.464)	Data 0.002 (0.003)	Loss 1.1279 (1.1271)	Acc@1 76.562 (72.559)	Acc@5 97.656 (98.109)
Epoch: [4][320/391]	Time 0.474 (0.463)	Data 0.002 (0.003)	Loss 1.1238 (1.1188)	Acc@1 71.094 (72.912)	Acc@5 96.875 (98.104)
Epoch: [4][384/391]	Time 0.467 (0.463)	Data 0.001 (0.002)	Loss 1.0642 (1.1116)	Acc@1 75.000 (73.030)	Acc@5 96.875 (98.127)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.471 (0.471)	Data 0.197 (0.197)	Loss 0.9526 (0.9526)	Acc@1 75.000 (75.000)	Acc@5 99.219 (99.219)
Epoch: [5][64/391]	Time 0.477 (0.468)	Data 0.002 (0.005)	Loss 1.1874 (1.0496)	Acc@1 70.312 (74.087)	Acc@5 97.656 (98.353)
Epoch: [5][128/391]	Time 0.473 (0.464)	Data 0.002 (0.003)	Loss 1.1422 (1.0367)	Acc@1 74.219 (74.939)	Acc@5 98.438 (98.438)
Epoch: [5][192/391]	Time 0.472 (0.466)	Data 0.002 (0.003)	Loss 0.9221 (1.0337)	Acc@1 78.125 (74.972)	Acc@5 99.219 (98.409)
Epoch: [5][256/391]	Time 0.455 (0.467)	Data 0.002 (0.003)	Loss 1.0357 (1.0288)	Acc@1 71.875 (75.237)	Acc@5 98.438 (98.431)
Epoch: [5][320/391]	Time 0.437 (0.466)	Data 0.002 (0.003)	Loss 1.0474 (1.0265)	Acc@1 72.656 (75.190)	Acc@5 97.656 (98.423)
Epoch: [5][384/391]	Time 0.489 (0.466)	Data 0.002 (0.002)	Loss 0.9621 (1.0215)	Acc@1 80.469 (75.341)	Acc@5 97.656 (98.454)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.434 (0.434)	Data 0.212 (0.212)	Loss 0.9968 (0.9968)	Acc@1 78.125 (78.125)	Acc@5 96.875 (96.875)
Epoch: [6][64/391]	Time 0.446 (0.462)	Data 0.002 (0.005)	Loss 0.9997 (0.9700)	Acc@1 76.562 (76.659)	Acc@5 98.438 (98.558)
Epoch: [6][128/391]	Time 0.459 (0.463)	Data 0.002 (0.004)	Loss 0.8032 (0.9684)	Acc@1 84.375 (76.956)	Acc@5 100.000 (98.547)
Epoch: [6][192/391]	Time 0.458 (0.463)	Data 0.002 (0.003)	Loss 0.9595 (0.9758)	Acc@1 76.562 (76.635)	Acc@5 99.219 (98.559)
Epoch: [6][256/391]	Time 0.473 (0.463)	Data 0.002 (0.003)	Loss 1.0621 (0.9753)	Acc@1 71.875 (76.730)	Acc@5 97.656 (98.568)
Epoch: [6][320/391]	Time 0.459 (0.465)	Data 0.002 (0.003)	Loss 0.9082 (0.9733)	Acc@1 79.688 (76.816)	Acc@5 98.438 (98.596)
Epoch: [6][384/391]	Time 0.463 (0.463)	Data 0.002 (0.002)	Loss 1.0778 (0.9736)	Acc@1 73.438 (76.826)	Acc@5 99.219 (98.590)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.513 (0.513)	Data 0.250 (0.250)	Loss 0.9053 (0.9053)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [7][64/391]	Time 0.470 (0.465)	Data 0.002 (0.006)	Loss 0.9451 (0.9372)	Acc@1 75.781 (78.017)	Acc@5 100.000 (98.786)
Epoch: [7][128/391]	Time 0.469 (0.465)	Data 0.002 (0.004)	Loss 0.9145 (0.9534)	Acc@1 78.906 (77.386)	Acc@5 98.438 (98.710)
Epoch: [7][192/391]	Time 0.455 (0.466)	Data 0.002 (0.003)	Loss 0.8590 (0.9531)	Acc@1 80.469 (77.453)	Acc@5 98.438 (98.660)
Epoch: [7][256/391]	Time 0.474 (0.467)	Data 0.002 (0.003)	Loss 1.0209 (0.9518)	Acc@1 74.219 (77.636)	Acc@5 99.219 (98.574)
Epoch: [7][320/391]	Time 0.510 (0.466)	Data 0.002 (0.003)	Loss 0.9805 (0.9522)	Acc@1 82.812 (77.585)	Acc@5 98.438 (98.586)
Epoch: [7][384/391]	Time 0.451 (0.464)	Data 0.002 (0.003)	Loss 0.9272 (0.9496)	Acc@1 80.469 (77.646)	Acc@5 99.219 (98.586)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.507 (0.507)	Data 0.226 (0.226)	Loss 0.8489 (0.8489)	Acc@1 82.031 (82.031)	Acc@5 100.000 (100.000)
Epoch: [8][64/391]	Time 0.457 (0.465)	Data 0.002 (0.005)	Loss 0.7899 (0.9192)	Acc@1 83.594 (78.690)	Acc@5 100.000 (98.798)
Epoch: [8][128/391]	Time 0.487 (0.462)	Data 0.002 (0.004)	Loss 0.9538 (0.9196)	Acc@1 72.656 (78.609)	Acc@5 99.219 (98.795)
Epoch: [8][192/391]	Time 0.472 (0.463)	Data 0.002 (0.003)	Loss 1.0298 (0.9175)	Acc@1 72.656 (78.627)	Acc@5 97.656 (98.757)
Epoch: [8][256/391]	Time 0.413 (0.462)	Data 0.002 (0.003)	Loss 0.8651 (0.9200)	Acc@1 77.344 (78.687)	Acc@5 99.219 (98.748)
Epoch: [8][320/391]	Time 0.457 (0.462)	Data 0.002 (0.003)	Loss 0.9331 (0.9262)	Acc@1 79.688 (78.427)	Acc@5 98.438 (98.766)
Epoch: [8][384/391]	Time 0.451 (0.462)	Data 0.002 (0.003)	Loss 1.0395 (0.9237)	Acc@1 69.531 (78.421)	Acc@5 98.438 (98.795)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.502 (0.502)	Data 0.266 (0.266)	Loss 0.8612 (0.8612)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [9][64/391]	Time 0.447 (0.465)	Data 0.002 (0.006)	Loss 1.0587 (0.8791)	Acc@1 73.438 (79.856)	Acc@5 100.000 (98.906)
Epoch: [9][128/391]	Time 0.457 (0.463)	Data 0.002 (0.004)	Loss 0.8759 (0.8954)	Acc@1 75.781 (78.967)	Acc@5 100.000 (98.904)
Epoch: [9][192/391]	Time 0.460 (0.465)	Data 0.002 (0.003)	Loss 0.9779 (0.9022)	Acc@1 77.344 (78.870)	Acc@5 96.875 (98.782)
Epoch: [9][256/391]	Time 0.458 (0.464)	Data 0.002 (0.003)	Loss 0.9029 (0.9097)	Acc@1 72.656 (78.572)	Acc@5 100.000 (98.778)
Epoch: [9][320/391]	Time 0.435 (0.464)	Data 0.002 (0.003)	Loss 1.0107 (0.9092)	Acc@1 76.562 (78.690)	Acc@5 99.219 (98.756)
Epoch: [9][384/391]	Time 0.467 (0.463)	Data 0.002 (0.003)	Loss 0.8230 (0.9044)	Acc@1 84.375 (78.959)	Acc@5 98.438 (98.762)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.498 (0.498)	Data 0.258 (0.258)	Loss 0.9485 (0.9485)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.457 (0.466)	Data 0.002 (0.006)	Loss 0.8667 (0.8960)	Acc@1 77.344 (78.858)	Acc@5 99.219 (98.762)
Epoch: [10][128/391]	Time 0.486 (0.464)	Data 0.002 (0.004)	Loss 1.0417 (0.8995)	Acc@1 75.000 (79.003)	Acc@5 94.531 (98.783)
Epoch: [10][192/391]	Time 0.468 (0.464)	Data 0.002 (0.003)	Loss 1.0005 (0.9006)	Acc@1 74.219 (79.238)	Acc@5 99.219 (98.741)
Epoch: [10][256/391]	Time 0.437 (0.463)	Data 0.002 (0.003)	Loss 0.9639 (0.8984)	Acc@1 78.906 (79.216)	Acc@5 98.438 (98.802)
Epoch: [10][320/391]	Time 0.481 (0.463)	Data 0.002 (0.003)	Loss 0.8952 (0.8999)	Acc@1 78.906 (79.198)	Acc@5 99.219 (98.773)
Epoch: [10][384/391]	Time 0.428 (0.463)	Data 0.002 (0.003)	Loss 0.8550 (0.8985)	Acc@1 78.906 (79.249)	Acc@5 99.219 (98.801)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 444110 ; 487386 ; 0.911207954270332

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.558 (0.558)	Data 0.255 (0.255)	Loss 0.8543 (0.8543)	Acc@1 80.469 (80.469)	Acc@5 97.656 (97.656)
Epoch: [11][64/391]	Time 0.432 (0.462)	Data 0.002 (0.006)	Loss 0.8862 (0.8395)	Acc@1 76.562 (80.709)	Acc@5 99.219 (98.966)
Epoch: [11][128/391]	Time 0.419 (0.461)	Data 0.002 (0.004)	Loss 0.8223 (0.8769)	Acc@1 81.250 (79.512)	Acc@5 99.219 (98.867)
Epoch: [11][192/391]	Time 0.449 (0.463)	Data 0.002 (0.003)	Loss 0.6649 (0.8760)	Acc@1 86.719 (79.858)	Acc@5 100.000 (98.895)
Epoch: [11][256/391]	Time 0.428 (0.461)	Data 0.002 (0.003)	Loss 0.9948 (0.8789)	Acc@1 78.125 (79.675)	Acc@5 99.219 (98.872)
Epoch: [11][320/391]	Time 0.441 (0.461)	Data 0.002 (0.003)	Loss 0.9231 (0.8803)	Acc@1 81.250 (79.585)	Acc@5 99.219 (98.861)
Epoch: [11][384/391]	Time 0.452 (0.461)	Data 0.001 (0.003)	Loss 0.9484 (0.8827)	Acc@1 75.781 (79.454)	Acc@5 100.000 (98.858)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.462 (0.462)	Data 0.238 (0.238)	Loss 0.8857 (0.8857)	Acc@1 76.562 (76.562)	Acc@5 100.000 (100.000)
Epoch: [12][64/391]	Time 0.454 (0.456)	Data 0.002 (0.006)	Loss 1.0069 (0.8566)	Acc@1 75.000 (80.865)	Acc@5 100.000 (98.882)
Epoch: [12][128/391]	Time 0.444 (0.458)	Data 0.002 (0.004)	Loss 0.8743 (0.8712)	Acc@1 82.812 (80.269)	Acc@5 98.438 (98.861)
Epoch: [12][192/391]	Time 0.460 (0.460)	Data 0.002 (0.003)	Loss 0.9348 (0.8735)	Acc@1 80.469 (80.088)	Acc@5 98.438 (98.871)
Epoch: [12][256/391]	Time 0.446 (0.460)	Data 0.002 (0.003)	Loss 0.9328 (0.8739)	Acc@1 81.250 (79.931)	Acc@5 96.875 (98.848)
Epoch: [12][320/391]	Time 0.477 (0.461)	Data 0.002 (0.003)	Loss 0.7597 (0.8742)	Acc@1 82.031 (79.907)	Acc@5 100.000 (98.827)
Epoch: [12][384/391]	Time 0.470 (0.461)	Data 0.002 (0.003)	Loss 0.7846 (0.8772)	Acc@1 80.469 (79.907)	Acc@5 99.219 (98.827)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.433 (0.433)	Data 0.305 (0.305)	Loss 0.7601 (0.7601)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [13][64/391]	Time 0.427 (0.459)	Data 0.002 (0.007)	Loss 0.9932 (0.8534)	Acc@1 78.906 (80.601)	Acc@5 99.219 (98.966)
Epoch: [13][128/391]	Time 0.479 (0.458)	Data 0.002 (0.004)	Loss 1.0193 (0.8598)	Acc@1 78.906 (80.426)	Acc@5 97.656 (98.898)
Epoch: [13][192/391]	Time 0.449 (0.460)	Data 0.002 (0.004)	Loss 1.0414 (0.8643)	Acc@1 80.469 (80.307)	Acc@5 98.438 (98.903)
Epoch: [13][256/391]	Time 0.457 (0.460)	Data 0.002 (0.003)	Loss 0.8742 (0.8648)	Acc@1 78.125 (80.174)	Acc@5 100.000 (98.909)
Epoch: [13][320/391]	Time 0.477 (0.461)	Data 0.002 (0.003)	Loss 0.8563 (0.8712)	Acc@1 78.906 (79.975)	Acc@5 98.438 (98.868)
Epoch: [13][384/391]	Time 0.468 (0.460)	Data 0.002 (0.003)	Loss 0.9020 (0.8722)	Acc@1 78.906 (79.959)	Acc@5 99.219 (98.870)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.451 (0.451)	Data 0.216 (0.216)	Loss 0.8051 (0.8051)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [14][64/391]	Time 0.472 (0.463)	Data 0.002 (0.005)	Loss 0.8655 (0.8371)	Acc@1 78.125 (81.130)	Acc@5 98.438 (99.026)
Epoch: [14][128/391]	Time 0.452 (0.459)	Data 0.002 (0.004)	Loss 0.9049 (0.8529)	Acc@1 82.031 (80.935)	Acc@5 100.000 (99.079)
Epoch: [14][192/391]	Time 0.452 (0.462)	Data 0.002 (0.003)	Loss 0.8304 (0.8486)	Acc@1 78.125 (80.959)	Acc@5 98.438 (99.053)
Epoch: [14][256/391]	Time 0.486 (0.460)	Data 0.002 (0.003)	Loss 0.8871 (0.8494)	Acc@1 82.812 (80.943)	Acc@5 98.438 (99.036)
Epoch: [14][320/391]	Time 0.431 (0.460)	Data 0.002 (0.003)	Loss 0.7564 (0.8543)	Acc@1 83.594 (80.761)	Acc@5 99.219 (98.980)
Epoch: [14][384/391]	Time 0.461 (0.460)	Data 0.001 (0.003)	Loss 0.9526 (0.8531)	Acc@1 78.125 (80.787)	Acc@5 96.094 (98.987)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.463 (0.463)	Data 0.201 (0.201)	Loss 0.7499 (0.7499)	Acc@1 87.500 (87.500)	Acc@5 98.438 (98.438)
Epoch: [15][64/391]	Time 0.499 (0.463)	Data 0.002 (0.005)	Loss 0.8249 (0.8284)	Acc@1 81.250 (81.130)	Acc@5 98.438 (99.075)
Epoch: [15][128/391]	Time 0.483 (0.460)	Data 0.002 (0.004)	Loss 0.7666 (0.8377)	Acc@1 84.375 (80.996)	Acc@5 100.000 (99.025)
Epoch: [15][192/391]	Time 0.512 (0.463)	Data 0.002 (0.003)	Loss 0.8584 (0.8495)	Acc@1 84.375 (80.687)	Acc@5 100.000 (98.972)
Epoch: [15][256/391]	Time 0.476 (0.461)	Data 0.002 (0.003)	Loss 0.8130 (0.8511)	Acc@1 83.594 (80.721)	Acc@5 99.219 (98.979)
Epoch: [15][320/391]	Time 0.459 (0.460)	Data 0.002 (0.003)	Loss 0.7341 (0.8526)	Acc@1 83.594 (80.673)	Acc@5 99.219 (98.995)
Epoch: [15][384/391]	Time 0.457 (0.461)	Data 0.002 (0.003)	Loss 0.7986 (0.8538)	Acc@1 85.938 (80.621)	Acc@5 99.219 (98.985)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 375414 ; 487386 ; 0.7702601223670766

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.505 (0.505)	Data 0.222 (0.222)	Loss 0.8333 (0.8333)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [16][64/391]	Time 0.419 (0.457)	Data 0.002 (0.005)	Loss 0.8087 (0.7833)	Acc@1 82.812 (83.113)	Acc@5 99.219 (99.147)
Epoch: [16][128/391]	Time 0.533 (0.455)	Data 0.002 (0.004)	Loss 0.8569 (0.8111)	Acc@1 80.469 (82.098)	Acc@5 99.219 (99.158)
Epoch: [16][192/391]	Time 0.495 (0.460)	Data 0.002 (0.003)	Loss 0.8993 (0.8215)	Acc@1 80.469 (81.918)	Acc@5 97.656 (99.073)
Epoch: [16][256/391]	Time 0.454 (0.458)	Data 0.002 (0.003)	Loss 0.8586 (0.8314)	Acc@1 78.125 (81.481)	Acc@5 98.438 (99.021)
Epoch: [16][320/391]	Time 0.430 (0.458)	Data 0.002 (0.003)	Loss 0.7720 (0.8378)	Acc@1 81.250 (81.189)	Acc@5 99.219 (98.995)
Epoch: [16][384/391]	Time 0.442 (0.458)	Data 0.001 (0.003)	Loss 0.8377 (0.8383)	Acc@1 78.906 (81.167)	Acc@5 98.438 (98.965)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.512 (0.512)	Data 0.219 (0.219)	Loss 0.8508 (0.8508)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [17][64/391]	Time 0.442 (0.455)	Data 0.002 (0.005)	Loss 0.7965 (0.8238)	Acc@1 80.469 (81.575)	Acc@5 100.000 (99.183)
Epoch: [17][128/391]	Time 0.459 (0.457)	Data 0.002 (0.004)	Loss 0.7617 (0.8316)	Acc@1 83.594 (81.292)	Acc@5 99.219 (99.122)
Epoch: [17][192/391]	Time 0.438 (0.458)	Data 0.002 (0.003)	Loss 0.7976 (0.8333)	Acc@1 82.031 (81.250)	Acc@5 100.000 (99.146)
Epoch: [17][256/391]	Time 0.481 (0.456)	Data 0.002 (0.003)	Loss 0.8589 (0.8378)	Acc@1 77.344 (81.001)	Acc@5 100.000 (99.088)
Epoch: [17][320/391]	Time 0.453 (0.456)	Data 0.002 (0.003)	Loss 0.8365 (0.8420)	Acc@1 82.031 (80.861)	Acc@5 99.219 (99.036)
Epoch: [17][384/391]	Time 0.437 (0.456)	Data 0.002 (0.003)	Loss 0.8069 (0.8411)	Acc@1 82.812 (80.982)	Acc@5 97.656 (99.036)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.514 (0.514)	Data 0.324 (0.324)	Loss 0.7859 (0.7859)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [18][64/391]	Time 0.463 (0.457)	Data 0.002 (0.007)	Loss 0.8080 (0.8225)	Acc@1 82.812 (81.334)	Acc@5 97.656 (99.099)
Epoch: [18][128/391]	Time 0.452 (0.457)	Data 0.002 (0.005)	Loss 0.8053 (0.8343)	Acc@1 82.031 (80.959)	Acc@5 98.438 (99.092)
Epoch: [18][192/391]	Time 0.443 (0.458)	Data 0.002 (0.004)	Loss 0.7063 (0.8304)	Acc@1 85.938 (81.088)	Acc@5 100.000 (99.114)
Epoch: [18][256/391]	Time 0.393 (0.457)	Data 0.002 (0.003)	Loss 0.8104 (0.8413)	Acc@1 82.031 (80.706)	Acc@5 98.438 (99.055)
Epoch: [18][320/391]	Time 0.460 (0.456)	Data 0.002 (0.003)	Loss 0.7374 (0.8406)	Acc@1 84.375 (80.756)	Acc@5 100.000 (99.041)
Epoch: [18][384/391]	Time 0.454 (0.456)	Data 0.001 (0.003)	Loss 0.8112 (0.8453)	Acc@1 84.375 (80.629)	Acc@5 99.219 (99.008)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.483 (0.483)	Data 0.281 (0.281)	Loss 0.9691 (0.9691)	Acc@1 77.344 (77.344)	Acc@5 99.219 (99.219)
Epoch: [19][64/391]	Time 0.435 (0.450)	Data 0.002 (0.006)	Loss 0.7110 (0.8299)	Acc@1 89.062 (81.238)	Acc@5 100.000 (98.918)
Epoch: [19][128/391]	Time 0.477 (0.454)	Data 0.001 (0.004)	Loss 0.7190 (0.8301)	Acc@1 84.375 (81.111)	Acc@5 99.219 (98.934)
Epoch: [19][192/391]	Time 0.489 (0.456)	Data 0.002 (0.003)	Loss 0.8453 (0.8297)	Acc@1 82.031 (81.153)	Acc@5 99.219 (98.972)
Epoch: [19][256/391]	Time 0.468 (0.457)	Data 0.002 (0.003)	Loss 0.8879 (0.8315)	Acc@1 82.031 (81.028)	Acc@5 98.438 (98.957)
Epoch: [19][320/391]	Time 0.471 (0.457)	Data 0.002 (0.003)	Loss 0.8807 (0.8326)	Acc@1 79.688 (81.048)	Acc@5 100.000 (98.963)
Epoch: [19][384/391]	Time 0.413 (0.456)	Data 0.002 (0.003)	Loss 0.8979 (0.8325)	Acc@1 79.688 (81.043)	Acc@5 100.000 (98.975)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.483 (0.483)	Data 0.269 (0.269)	Loss 0.7154 (0.7154)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [20][64/391]	Time 0.450 (0.456)	Data 0.002 (0.006)	Loss 0.7483 (0.8024)	Acc@1 82.812 (81.971)	Acc@5 98.438 (99.038)
Epoch: [20][128/391]	Time 0.455 (0.457)	Data 0.002 (0.004)	Loss 0.9719 (0.8340)	Acc@1 77.344 (81.026)	Acc@5 97.656 (98.898)
Epoch: [20][192/391]	Time 0.436 (0.456)	Data 0.002 (0.003)	Loss 0.7904 (0.8369)	Acc@1 81.250 (81.003)	Acc@5 100.000 (98.948)
Epoch: [20][256/391]	Time 0.430 (0.456)	Data 0.002 (0.003)	Loss 0.8357 (0.8310)	Acc@1 80.469 (81.177)	Acc@5 98.438 (98.966)
Epoch: [20][320/391]	Time 0.484 (0.456)	Data 0.002 (0.003)	Loss 1.0640 (0.8358)	Acc@1 72.656 (81.011)	Acc@5 100.000 (98.978)
Epoch: [20][384/391]	Time 0.476 (0.456)	Data 0.001 (0.003)	Loss 0.8004 (0.8347)	Acc@1 82.812 (81.071)	Acc@5 99.219 (99.002)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 328938 ; 487386 ; 0.6749024387241324

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.382 (0.382)	Data 0.231 (0.231)	Loss 0.8586 (0.8586)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [21][64/391]	Time 0.479 (0.448)	Data 0.002 (0.006)	Loss 0.8721 (0.8147)	Acc@1 80.469 (81.875)	Acc@5 98.438 (99.111)
Epoch: [21][128/391]	Time 0.453 (0.447)	Data 0.002 (0.004)	Loss 0.8554 (0.8091)	Acc@1 77.344 (81.825)	Acc@5 98.438 (99.007)
Epoch: [21][192/391]	Time 0.423 (0.452)	Data 0.002 (0.003)	Loss 0.7684 (0.8106)	Acc@1 83.594 (81.375)	Acc@5 99.219 (99.016)
Epoch: [21][256/391]	Time 0.479 (0.452)	Data 0.002 (0.003)	Loss 0.7515 (0.8120)	Acc@1 82.812 (81.262)	Acc@5 98.438 (99.055)
Epoch: [21][320/391]	Time 0.466 (0.453)	Data 0.002 (0.003)	Loss 0.9717 (0.8127)	Acc@1 78.125 (81.269)	Acc@5 98.438 (99.034)
Epoch: [21][384/391]	Time 0.429 (0.454)	Data 0.002 (0.003)	Loss 0.8074 (0.8141)	Acc@1 79.688 (81.360)	Acc@5 98.438 (99.034)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.456 (0.456)	Data 0.292 (0.292)	Loss 0.9091 (0.9091)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [22][64/391]	Time 0.460 (0.460)	Data 0.002 (0.007)	Loss 0.7528 (0.7847)	Acc@1 82.031 (82.728)	Acc@5 99.219 (99.195)
Epoch: [22][128/391]	Time 0.506 (0.461)	Data 0.002 (0.004)	Loss 0.9119 (0.7966)	Acc@1 82.812 (82.225)	Acc@5 98.438 (99.128)
Epoch: [22][192/391]	Time 0.448 (0.461)	Data 0.002 (0.004)	Loss 0.7071 (0.8025)	Acc@1 84.375 (81.975)	Acc@5 99.219 (99.114)
Epoch: [22][256/391]	Time 0.446 (0.459)	Data 0.002 (0.003)	Loss 0.8592 (0.8022)	Acc@1 81.250 (81.967)	Acc@5 98.438 (99.058)
Epoch: [22][320/391]	Time 0.453 (0.459)	Data 0.002 (0.003)	Loss 0.9022 (0.8119)	Acc@1 78.906 (81.620)	Acc@5 98.438 (99.019)
Epoch: [22][384/391]	Time 0.436 (0.458)	Data 0.002 (0.003)	Loss 0.8555 (0.8146)	Acc@1 82.812 (81.451)	Acc@5 99.219 (99.006)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.583 (0.583)	Data 0.256 (0.256)	Loss 0.6399 (0.6399)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [23][64/391]	Time 0.428 (0.460)	Data 0.002 (0.006)	Loss 0.7161 (0.7621)	Acc@1 87.500 (82.668)	Acc@5 97.656 (99.303)
Epoch: [23][128/391]	Time 0.498 (0.461)	Data 0.002 (0.004)	Loss 0.7151 (0.7876)	Acc@1 85.156 (82.092)	Acc@5 100.000 (99.201)
Epoch: [23][192/391]	Time 0.407 (0.457)	Data 0.002 (0.003)	Loss 0.8054 (0.7940)	Acc@1 82.812 (81.930)	Acc@5 98.438 (99.182)
Epoch: [23][256/391]	Time 0.399 (0.441)	Data 0.002 (0.003)	Loss 0.7306 (0.7957)	Acc@1 84.375 (81.837)	Acc@5 99.219 (99.164)
Epoch: [23][320/391]	Time 0.407 (0.433)	Data 0.002 (0.003)	Loss 0.7519 (0.8003)	Acc@1 84.375 (81.683)	Acc@5 99.219 (99.126)
Epoch: [23][384/391]	Time 0.376 (0.427)	Data 0.002 (0.003)	Loss 0.8798 (0.8058)	Acc@1 79.688 (81.565)	Acc@5 99.219 (99.073)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.479 (0.479)	Data 0.210 (0.210)	Loss 0.8098 (0.8098)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [24][64/391]	Time 0.398 (0.402)	Data 0.002 (0.005)	Loss 0.7277 (0.8111)	Acc@1 87.500 (81.394)	Acc@5 98.438 (99.075)
Epoch: [24][128/391]	Time 0.398 (0.401)	Data 0.002 (0.004)	Loss 0.9060 (0.8068)	Acc@1 78.125 (81.365)	Acc@5 99.219 (99.079)
Epoch: [24][192/391]	Time 0.371 (0.399)	Data 0.001 (0.003)	Loss 0.8395 (0.8065)	Acc@1 78.906 (81.319)	Acc@5 97.656 (99.162)
Epoch: [24][256/391]	Time 0.428 (0.397)	Data 0.002 (0.003)	Loss 0.8182 (0.8049)	Acc@1 78.906 (81.545)	Acc@5 99.219 (99.137)
Epoch: [24][320/391]	Time 0.416 (0.397)	Data 0.002 (0.003)	Loss 0.7512 (0.8062)	Acc@1 84.375 (81.566)	Acc@5 100.000 (99.121)
Epoch: [24][384/391]	Time 0.377 (0.397)	Data 0.002 (0.003)	Loss 0.8127 (0.8108)	Acc@1 82.812 (81.459)	Acc@5 100.000 (99.117)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.471 (0.471)	Data 0.251 (0.251)	Loss 0.8638 (0.8638)	Acc@1 75.781 (75.781)	Acc@5 100.000 (100.000)
Epoch: [25][64/391]	Time 0.449 (0.402)	Data 0.002 (0.006)	Loss 0.8283 (0.8099)	Acc@1 79.688 (81.671)	Acc@5 99.219 (98.942)
Epoch: [25][128/391]	Time 0.395 (0.401)	Data 0.001 (0.004)	Loss 0.7539 (0.7974)	Acc@1 83.594 (81.953)	Acc@5 100.000 (99.061)
Epoch: [25][192/391]	Time 0.425 (0.396)	Data 0.002 (0.003)	Loss 0.9206 (0.8007)	Acc@1 77.344 (81.853)	Acc@5 96.094 (99.037)
Epoch: [25][256/391]	Time 0.391 (0.396)	Data 0.002 (0.003)	Loss 0.8840 (0.8003)	Acc@1 76.562 (81.897)	Acc@5 97.656 (99.052)
Epoch: [25][320/391]	Time 0.381 (0.396)	Data 0.002 (0.003)	Loss 0.9363 (0.8003)	Acc@1 76.562 (81.861)	Acc@5 99.219 (99.061)
Epoch: [25][384/391]	Time 0.423 (0.395)	Data 0.001 (0.003)	Loss 0.7618 (0.8017)	Acc@1 82.031 (81.767)	Acc@5 99.219 (99.069)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv30.weight

 RM:  module.conv31.weight

Module List Length:  68
Index1: 60
Index: 31
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 32; k: 32
Bool1: False
Bool2: True
j: 33; k: 32
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(18, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(20, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 58: Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 59: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 60: Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 61: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 62 gegen 66: Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 63 gegen 67: BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(18, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(20, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  64

Module List Length:  64
Index1: 58
Index: 30
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 31; k: 30
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(18, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(20, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 58 gegen 62: Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 59 gegen 63: BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(18, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(20, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): AdaptiveAvgPool2d(output_size=(1, 1))
    (59): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  60
Count: 239992 ; 487386 ; 0.49240642940092655

Epoch: [26 | 180] LR: 0.100000
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 318, in main
    use_gpu_num)
  File "main.py", line 442, in train
    outputs = model.forward(inputs)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 503, in forward
    _x = _x + x
RuntimeError: The size of tensor a (64) must match the size of tensor b (61) at non-singleton dimension 1
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.486 (0.486)	Data 0.168 (0.168)	Loss 3.1082 (3.1082)	Acc@1 11.719 (11.719)	Acc@5 46.875 (46.875)
Epoch: [1][64/391]	Time 0.442 (0.401)	Data 0.002 (0.004)	Loss 2.4128 (2.7258)	Acc@1 28.906 (20.204)	Acc@5 79.688 (73.870)
Epoch: [1][128/391]	Time 0.422 (0.403)	Data 0.003 (0.003)	Loss 2.3269 (2.5562)	Acc@1 30.469 (25.418)	Acc@5 89.844 (79.809)
Epoch: [1][192/391]	Time 0.423 (0.402)	Data 0.002 (0.003)	Loss 2.1278 (2.4621)	Acc@1 35.156 (28.295)	Acc@5 90.625 (82.578)
Epoch: [1][256/391]	Time 0.439 (0.401)	Data 0.002 (0.002)	Loss 2.1274 (2.3882)	Acc@1 42.188 (30.940)	Acc@5 89.844 (84.311)
Epoch: [1][320/391]	Time 0.406 (0.400)	Data 0.002 (0.002)	Loss 2.0007 (2.3133)	Acc@1 42.188 (33.861)	Acc@5 91.406 (85.733)
Epoch: [1][384/391]	Time 0.408 (0.401)	Data 0.002 (0.002)	Loss 1.8206 (2.2414)	Acc@1 54.688 (36.353)	Acc@5 89.062 (87.029)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.357 (0.357)	Data 0.204 (0.204)	Loss 1.7835 (1.7835)	Acc@1 56.250 (56.250)	Acc@5 98.438 (98.438)
Epoch: [2][64/391]	Time 0.426 (0.396)	Data 0.002 (0.005)	Loss 1.8812 (1.7746)	Acc@1 48.438 (53.317)	Acc@5 92.188 (94.579)
Epoch: [2][128/391]	Time 0.405 (0.399)	Data 0.002 (0.003)	Loss 1.4693 (1.7409)	Acc@1 63.281 (54.124)	Acc@5 95.312 (94.677)
Epoch: [2][192/391]	Time 0.399 (0.397)	Data 0.003 (0.003)	Loss 1.5197 (1.7104)	Acc@1 62.500 (54.955)	Acc@5 96.094 (94.738)
Epoch: [2][256/391]	Time 0.376 (0.397)	Data 0.002 (0.003)	Loss 1.5583 (1.6755)	Acc@1 64.844 (56.159)	Acc@5 93.750 (94.926)
Epoch: [2][320/391]	Time 0.383 (0.397)	Data 0.002 (0.002)	Loss 1.6317 (1.6433)	Acc@1 60.156 (57.009)	Acc@5 92.969 (95.181)
Epoch: [2][384/391]	Time 0.396 (0.398)	Data 0.001 (0.002)	Loss 1.3953 (1.6089)	Acc@1 65.625 (58.111)	Acc@5 96.875 (95.422)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.287 (0.287)	Data 0.272 (0.272)	Loss 1.3403 (1.3403)	Acc@1 65.625 (65.625)	Acc@5 96.094 (96.094)
Epoch: [3][64/391]	Time 0.412 (0.402)	Data 0.002 (0.006)	Loss 1.3578 (1.3916)	Acc@1 62.500 (64.663)	Acc@5 97.656 (96.550)
Epoch: [3][128/391]	Time 0.429 (0.404)	Data 0.002 (0.004)	Loss 1.2842 (1.3723)	Acc@1 69.531 (65.577)	Acc@5 97.656 (96.778)
Epoch: [3][192/391]	Time 0.406 (0.403)	Data 0.002 (0.003)	Loss 1.1902 (1.3503)	Acc@1 66.406 (65.961)	Acc@5 98.438 (96.899)
Epoch: [3][256/391]	Time 0.407 (0.403)	Data 0.002 (0.003)	Loss 1.4324 (1.3280)	Acc@1 64.844 (66.534)	Acc@5 97.656 (97.021)
Epoch: [3][320/391]	Time 0.360 (0.402)	Data 0.002 (0.003)	Loss 1.2485 (1.3020)	Acc@1 66.406 (67.365)	Acc@5 96.094 (97.206)
Epoch: [3][384/391]	Time 0.414 (0.401)	Data 0.002 (0.003)	Loss 1.2236 (1.2847)	Acc@1 68.750 (67.806)	Acc@5 98.438 (97.344)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.428 (0.428)	Data 0.255 (0.255)	Loss 1.3147 (1.3147)	Acc@1 71.094 (71.094)	Acc@5 96.875 (96.875)
Epoch: [4][64/391]	Time 0.351 (0.403)	Data 0.002 (0.006)	Loss 1.2381 (1.1402)	Acc@1 66.406 (72.151)	Acc@5 97.656 (98.053)
Epoch: [4][128/391]	Time 0.373 (0.404)	Data 0.002 (0.004)	Loss 1.1541 (1.1292)	Acc@1 72.656 (72.602)	Acc@5 96.875 (98.020)
Epoch: [4][192/391]	Time 0.438 (0.403)	Data 0.003 (0.003)	Loss 0.8845 (1.1183)	Acc@1 84.375 (72.891)	Acc@5 100.000 (98.097)
Epoch: [4][256/391]	Time 0.382 (0.402)	Data 0.002 (0.003)	Loss 1.0567 (1.1082)	Acc@1 69.531 (73.015)	Acc@5 98.438 (98.082)
Epoch: [4][320/391]	Time 0.389 (0.402)	Data 0.002 (0.003)	Loss 1.0861 (1.1012)	Acc@1 75.000 (73.192)	Acc@5 98.438 (98.116)
Epoch: [4][384/391]	Time 0.438 (0.403)	Data 0.002 (0.003)	Loss 1.0879 (1.0959)	Acc@1 71.094 (73.291)	Acc@5 96.875 (98.149)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.513 (0.513)	Data 0.320 (0.320)	Loss 1.1160 (1.1160)	Acc@1 71.875 (71.875)	Acc@5 97.656 (97.656)
Epoch: [5][64/391]	Time 0.410 (0.400)	Data 0.002 (0.007)	Loss 1.1864 (1.0486)	Acc@1 72.656 (74.459)	Acc@5 97.656 (98.329)
Epoch: [5][128/391]	Time 0.378 (0.400)	Data 0.002 (0.004)	Loss 0.8452 (1.0268)	Acc@1 81.250 (75.133)	Acc@5 100.000 (98.407)
Epoch: [5][192/391]	Time 0.412 (0.400)	Data 0.002 (0.004)	Loss 1.0620 (1.0303)	Acc@1 71.094 (75.081)	Acc@5 98.438 (98.357)
Epoch: [5][256/391]	Time 0.408 (0.400)	Data 0.002 (0.003)	Loss 1.0270 (1.0238)	Acc@1 73.438 (75.185)	Acc@5 100.000 (98.383)
Epoch: [5][320/391]	Time 0.387 (0.400)	Data 0.001 (0.003)	Loss 0.8890 (1.0165)	Acc@1 77.344 (75.348)	Acc@5 99.219 (98.455)
Epoch: [5][384/391]	Time 0.308 (0.399)	Data 0.013 (0.003)	Loss 0.9997 (1.0141)	Acc@1 73.438 (75.448)	Acc@5 99.219 (98.417)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.440 (0.440)	Data 0.223 (0.223)	Loss 0.9244 (0.9244)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [6][64/391]	Time 0.384 (0.407)	Data 0.006 (0.005)	Loss 0.9355 (0.9742)	Acc@1 82.031 (77.200)	Acc@5 96.875 (98.413)
Epoch: [6][128/391]	Time 0.404 (0.409)	Data 0.002 (0.004)	Loss 1.1553 (0.9914)	Acc@1 69.531 (76.417)	Acc@5 97.656 (98.383)
Epoch: [6][192/391]	Time 0.404 (0.406)	Data 0.002 (0.003)	Loss 1.0665 (0.9866)	Acc@1 75.781 (76.526)	Acc@5 98.438 (98.466)
Epoch: [6][256/391]	Time 0.425 (0.405)	Data 0.002 (0.003)	Loss 0.8743 (0.9797)	Acc@1 78.125 (76.742)	Acc@5 99.219 (98.495)
Epoch: [6][320/391]	Time 0.417 (0.406)	Data 0.001 (0.003)	Loss 1.0376 (0.9758)	Acc@1 74.219 (76.954)	Acc@5 98.438 (98.518)
Epoch: [6][384/391]	Time 0.552 (0.408)	Data 0.002 (0.002)	Loss 1.0030 (0.9789)	Acc@1 74.219 (76.778)	Acc@5 96.875 (98.490)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.445 (0.445)	Data 0.203 (0.203)	Loss 0.8625 (0.8625)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [7][64/391]	Time 0.388 (0.411)	Data 0.002 (0.005)	Loss 0.9654 (0.9848)	Acc@1 71.094 (76.418)	Acc@5 98.438 (98.185)
Epoch: [7][128/391]	Time 0.414 (0.406)	Data 0.003 (0.004)	Loss 0.8785 (0.9610)	Acc@1 78.906 (77.313)	Acc@5 98.438 (98.450)
Epoch: [7][192/391]	Time 0.399 (0.406)	Data 0.002 (0.003)	Loss 1.0015 (0.9570)	Acc@1 78.125 (77.453)	Acc@5 98.438 (98.579)
Epoch: [7][256/391]	Time 0.401 (0.405)	Data 0.002 (0.003)	Loss 0.9027 (0.9616)	Acc@1 78.906 (77.386)	Acc@5 100.000 (98.550)
Epoch: [7][320/391]	Time 0.372 (0.405)	Data 0.001 (0.003)	Loss 0.8030 (0.9592)	Acc@1 79.688 (77.378)	Acc@5 98.438 (98.569)
Epoch: [7][384/391]	Time 0.432 (0.405)	Data 0.002 (0.002)	Loss 0.9393 (0.9548)	Acc@1 78.906 (77.559)	Acc@5 99.219 (98.592)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.422 (0.422)	Data 0.227 (0.227)	Loss 0.9812 (0.9812)	Acc@1 74.219 (74.219)	Acc@5 99.219 (99.219)
Epoch: [8][64/391]	Time 0.393 (0.405)	Data 0.001 (0.005)	Loss 0.8564 (0.9223)	Acc@1 84.375 (78.630)	Acc@5 97.656 (98.666)
Epoch: [8][128/391]	Time 0.390 (0.408)	Data 0.002 (0.004)	Loss 0.8548 (0.9346)	Acc@1 81.250 (78.398)	Acc@5 99.219 (98.625)
Epoch: [8][192/391]	Time 0.410 (0.405)	Data 0.002 (0.003)	Loss 0.9589 (0.9390)	Acc@1 75.781 (78.246)	Acc@5 98.438 (98.599)
Epoch: [8][256/391]	Time 0.429 (0.405)	Data 0.002 (0.003)	Loss 0.8846 (0.9400)	Acc@1 80.469 (78.186)	Acc@5 99.219 (98.596)
Epoch: [8][320/391]	Time 0.461 (0.404)	Data 0.002 (0.003)	Loss 1.0753 (0.9409)	Acc@1 71.875 (78.157)	Acc@5 99.219 (98.586)
Epoch: [8][384/391]	Time 0.339 (0.405)	Data 0.002 (0.003)	Loss 0.8055 (0.9379)	Acc@1 85.156 (78.249)	Acc@5 98.438 (98.608)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.473 (0.473)	Data 0.325 (0.325)	Loss 0.8849 (0.8849)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [9][64/391]	Time 0.375 (0.413)	Data 0.002 (0.007)	Loss 0.9959 (0.9407)	Acc@1 77.344 (77.776)	Acc@5 97.656 (98.678)
Epoch: [9][128/391]	Time 0.440 (0.407)	Data 0.002 (0.004)	Loss 0.8851 (0.9341)	Acc@1 79.688 (78.131)	Acc@5 99.219 (98.734)
Epoch: [9][192/391]	Time 0.413 (0.404)	Data 0.002 (0.004)	Loss 0.9762 (0.9344)	Acc@1 79.688 (78.242)	Acc@5 99.219 (98.656)
Epoch: [9][256/391]	Time 0.418 (0.402)	Data 0.002 (0.003)	Loss 0.8716 (0.9283)	Acc@1 79.688 (78.517)	Acc@5 100.000 (98.738)
Epoch: [9][320/391]	Time 0.450 (0.403)	Data 0.002 (0.003)	Loss 0.8995 (0.9244)	Acc@1 78.125 (78.600)	Acc@5 99.219 (98.747)
Epoch: [9][384/391]	Time 0.453 (0.403)	Data 0.002 (0.003)	Loss 0.8956 (0.9232)	Acc@1 78.906 (78.724)	Acc@5 99.219 (98.756)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.451 (0.451)	Data 0.249 (0.249)	Loss 0.7836 (0.7836)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.400 (0.406)	Data 0.002 (0.006)	Loss 0.9466 (0.9076)	Acc@1 78.125 (79.231)	Acc@5 99.219 (98.678)
Epoch: [10][128/391]	Time 0.368 (0.407)	Data 0.002 (0.004)	Loss 0.8102 (0.9153)	Acc@1 81.250 (79.070)	Acc@5 100.000 (98.728)
Epoch: [10][192/391]	Time 0.382 (0.405)	Data 0.002 (0.003)	Loss 1.0646 (0.9212)	Acc@1 71.875 (78.874)	Acc@5 100.000 (98.709)
Epoch: [10][256/391]	Time 0.411 (0.405)	Data 0.002 (0.003)	Loss 0.9193 (0.9147)	Acc@1 78.125 (79.034)	Acc@5 99.219 (98.738)
Epoch: [10][320/391]	Time 0.385 (0.405)	Data 0.002 (0.003)	Loss 1.0028 (0.9104)	Acc@1 74.219 (79.164)	Acc@5 99.219 (98.739)
Epoch: [10][384/391]	Time 0.473 (0.405)	Data 0.002 (0.003)	Loss 0.6971 (0.9106)	Acc@1 87.500 (79.142)	Acc@5 99.219 (98.732)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 479306 ; 487386 ; 0.9834217642689778

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.448 (0.448)	Data 0.252 (0.252)	Loss 0.8128 (0.8128)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [11][64/391]	Time 0.426 (0.407)	Data 0.002 (0.006)	Loss 0.9834 (0.8794)	Acc@1 75.781 (80.192)	Acc@5 100.000 (98.906)
Epoch: [11][128/391]	Time 0.417 (0.403)	Data 0.002 (0.004)	Loss 1.0508 (0.8876)	Acc@1 78.125 (79.887)	Acc@5 99.219 (98.843)
Epoch: [11][192/391]	Time 0.406 (0.401)	Data 0.002 (0.003)	Loss 1.0813 (0.8921)	Acc@1 74.219 (79.696)	Acc@5 99.219 (98.838)
Epoch: [11][256/391]	Time 0.396 (0.403)	Data 0.002 (0.003)	Loss 0.8754 (0.8964)	Acc@1 82.031 (79.599)	Acc@5 98.438 (98.793)
Epoch: [11][320/391]	Time 0.409 (0.404)	Data 0.002 (0.003)	Loss 1.0039 (0.8983)	Acc@1 76.562 (79.464)	Acc@5 97.656 (98.783)
Epoch: [11][384/391]	Time 0.566 (0.405)	Data 0.002 (0.003)	Loss 0.8567 (0.8953)	Acc@1 81.250 (79.550)	Acc@5 100.000 (98.793)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.444 (0.444)	Data 0.245 (0.245)	Loss 0.7826 (0.7826)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [12][64/391]	Time 0.444 (0.408)	Data 0.002 (0.006)	Loss 0.9413 (0.8992)	Acc@1 79.688 (80.096)	Acc@5 99.219 (98.822)
Epoch: [12][128/391]	Time 0.404 (0.406)	Data 0.002 (0.004)	Loss 0.8396 (0.8984)	Acc@1 79.688 (79.706)	Acc@5 97.656 (98.837)
Epoch: [12][192/391]	Time 0.380 (0.403)	Data 0.002 (0.003)	Loss 0.7296 (0.8959)	Acc@1 88.281 (79.829)	Acc@5 100.000 (98.911)
Epoch: [12][256/391]	Time 0.392 (0.403)	Data 0.002 (0.003)	Loss 0.8595 (0.8966)	Acc@1 81.250 (79.794)	Acc@5 98.438 (98.900)
Epoch: [12][320/391]	Time 0.385 (0.404)	Data 0.002 (0.003)	Loss 0.8617 (0.8955)	Acc@1 82.031 (79.636)	Acc@5 99.219 (98.910)
Epoch: [12][384/391]	Time 0.436 (0.405)	Data 0.002 (0.003)	Loss 0.8163 (0.8967)	Acc@1 82.031 (79.543)	Acc@5 99.219 (98.868)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.482 (0.482)	Data 0.222 (0.222)	Loss 0.9626 (0.9626)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [13][64/391]	Time 0.393 (0.408)	Data 0.002 (0.005)	Loss 0.8969 (0.8815)	Acc@1 81.250 (79.988)	Acc@5 99.219 (99.099)
Epoch: [13][128/391]	Time 0.423 (0.407)	Data 0.002 (0.004)	Loss 0.9747 (0.8801)	Acc@1 75.781 (80.002)	Acc@5 99.219 (99.086)
Epoch: [13][192/391]	Time 0.389 (0.405)	Data 0.002 (0.003)	Loss 0.8761 (0.8776)	Acc@1 79.688 (80.104)	Acc@5 99.219 (98.996)
Epoch: [13][256/391]	Time 0.434 (0.404)	Data 0.003 (0.003)	Loss 0.8623 (0.8752)	Acc@1 82.031 (80.207)	Acc@5 99.219 (98.909)
Epoch: [13][320/391]	Time 0.399 (0.405)	Data 0.002 (0.003)	Loss 0.9508 (0.8756)	Acc@1 79.688 (80.208)	Acc@5 97.656 (98.919)
Epoch: [13][384/391]	Time 0.418 (0.404)	Data 0.001 (0.003)	Loss 0.9557 (0.8816)	Acc@1 78.125 (80.099)	Acc@5 99.219 (98.868)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.428 (0.428)	Data 0.261 (0.261)	Loss 0.6918 (0.6918)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [14][64/391]	Time 0.402 (0.403)	Data 0.002 (0.006)	Loss 0.9448 (0.8596)	Acc@1 79.688 (80.553)	Acc@5 97.656 (99.038)
Epoch: [14][128/391]	Time 0.392 (0.406)	Data 0.002 (0.004)	Loss 0.7684 (0.8586)	Acc@1 80.469 (80.584)	Acc@5 100.000 (99.086)
Epoch: [14][192/391]	Time 0.421 (0.403)	Data 0.002 (0.003)	Loss 0.9178 (0.8676)	Acc@1 78.125 (80.372)	Acc@5 100.000 (99.065)
Epoch: [14][256/391]	Time 0.401 (0.402)	Data 0.002 (0.003)	Loss 0.8800 (0.8687)	Acc@1 77.344 (80.469)	Acc@5 100.000 (99.033)
Epoch: [14][320/391]	Time 0.424 (0.403)	Data 0.002 (0.003)	Loss 0.7185 (0.8729)	Acc@1 86.719 (80.298)	Acc@5 99.219 (98.975)
Epoch: [14][384/391]	Time 0.362 (0.402)	Data 0.002 (0.003)	Loss 0.8729 (0.8728)	Acc@1 81.250 (80.310)	Acc@5 99.219 (98.994)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.358 (0.358)	Data 0.216 (0.216)	Loss 0.8655 (0.8655)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [15][64/391]	Time 0.390 (0.399)	Data 0.002 (0.005)	Loss 0.8634 (0.8353)	Acc@1 78.125 (81.226)	Acc@5 98.438 (99.062)
Epoch: [15][128/391]	Time 0.375 (0.401)	Data 0.002 (0.004)	Loss 0.9153 (0.8536)	Acc@1 82.031 (80.887)	Acc@5 98.438 (98.922)
Epoch: [15][192/391]	Time 0.376 (0.403)	Data 0.002 (0.003)	Loss 0.8523 (0.8577)	Acc@1 82.812 (80.825)	Acc@5 100.000 (98.923)
Epoch: [15][256/391]	Time 0.391 (0.401)	Data 0.002 (0.003)	Loss 0.8331 (0.8605)	Acc@1 75.000 (80.712)	Acc@5 100.000 (98.939)
Epoch: [15][320/391]	Time 0.349 (0.401)	Data 0.002 (0.003)	Loss 0.9784 (0.8662)	Acc@1 75.781 (80.493)	Acc@5 98.438 (98.949)
Epoch: [15][384/391]	Time 0.284 (0.399)	Data 0.001 (0.003)	Loss 1.0015 (0.8703)	Acc@1 75.781 (80.343)	Acc@5 97.656 (98.929)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 401090 ; 487386 ; 0.8229411595737259

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.444 (0.444)	Data 0.243 (0.243)	Loss 0.8521 (0.8521)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [16][64/391]	Time 0.390 (0.403)	Data 0.002 (0.006)	Loss 0.8420 (0.8495)	Acc@1 81.250 (81.226)	Acc@5 99.219 (98.978)
Epoch: [16][128/391]	Time 0.399 (0.404)	Data 0.002 (0.004)	Loss 0.9705 (0.8613)	Acc@1 77.344 (81.020)	Acc@5 99.219 (99.013)
Epoch: [16][192/391]	Time 0.385 (0.402)	Data 0.002 (0.003)	Loss 0.9156 (0.8694)	Acc@1 79.688 (80.793)	Acc@5 98.438 (98.996)
Epoch: [16][256/391]	Time 0.458 (0.401)	Data 0.001 (0.003)	Loss 0.9110 (0.8756)	Acc@1 79.688 (80.776)	Acc@5 99.219 (98.960)
Epoch: [16][320/391]	Time 0.375 (0.402)	Data 0.002 (0.003)	Loss 0.8195 (0.8776)	Acc@1 84.375 (80.722)	Acc@5 100.000 (98.990)
Epoch: [16][384/391]	Time 0.341 (0.402)	Data 0.002 (0.003)	Loss 0.8424 (0.8815)	Acc@1 84.375 (80.599)	Acc@5 96.875 (98.963)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.415 (0.415)	Data 0.229 (0.229)	Loss 0.9894 (0.9894)	Acc@1 76.562 (76.562)	Acc@5 100.000 (100.000)
Epoch: [17][64/391]	Time 0.403 (0.395)	Data 0.002 (0.005)	Loss 0.6788 (0.8728)	Acc@1 87.500 (80.397)	Acc@5 100.000 (99.038)
Epoch: [17][128/391]	Time 0.393 (0.399)	Data 0.001 (0.004)	Loss 1.0413 (0.8686)	Acc@1 77.344 (80.638)	Acc@5 98.438 (99.049)
Epoch: [17][192/391]	Time 0.387 (0.401)	Data 0.001 (0.003)	Loss 0.9466 (0.8692)	Acc@1 77.344 (80.602)	Acc@5 97.656 (99.037)
Epoch: [17][256/391]	Time 0.395 (0.402)	Data 0.002 (0.003)	Loss 1.0735 (0.8703)	Acc@1 71.875 (80.648)	Acc@5 98.438 (98.969)
Epoch: [17][320/391]	Time 0.442 (0.403)	Data 0.002 (0.003)	Loss 0.7895 (0.8703)	Acc@1 84.375 (80.598)	Acc@5 99.219 (98.975)
Epoch: [17][384/391]	Time 0.369 (0.404)	Data 0.002 (0.003)	Loss 0.9689 (0.8706)	Acc@1 75.781 (80.564)	Acc@5 97.656 (98.985)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.403 (0.403)	Data 0.242 (0.242)	Loss 1.0165 (1.0165)	Acc@1 74.219 (74.219)	Acc@5 100.000 (100.000)
Epoch: [18][64/391]	Time 0.437 (0.405)	Data 0.002 (0.006)	Loss 0.8999 (0.8461)	Acc@1 77.344 (81.046)	Acc@5 98.438 (99.171)
Epoch: [18][128/391]	Time 0.410 (0.405)	Data 0.002 (0.004)	Loss 0.9778 (0.8618)	Acc@1 79.688 (80.838)	Acc@5 99.219 (98.989)
Epoch: [18][192/391]	Time 0.414 (0.404)	Data 0.002 (0.003)	Loss 0.8066 (0.8638)	Acc@1 84.375 (80.918)	Acc@5 100.000 (98.956)
Epoch: [18][256/391]	Time 0.410 (0.403)	Data 0.002 (0.003)	Loss 0.8292 (0.8606)	Acc@1 85.938 (81.065)	Acc@5 98.438 (98.963)
Epoch: [18][320/391]	Time 0.401 (0.402)	Data 0.001 (0.003)	Loss 0.7765 (0.8629)	Acc@1 82.812 (81.014)	Acc@5 100.000 (98.956)
Epoch: [18][384/391]	Time 0.405 (0.402)	Data 0.002 (0.003)	Loss 0.9565 (0.8652)	Acc@1 75.781 (80.810)	Acc@5 99.219 (98.959)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.428 (0.428)	Data 0.233 (0.233)	Loss 0.8725 (0.8725)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [19][64/391]	Time 0.414 (0.405)	Data 0.002 (0.006)	Loss 0.8607 (0.8468)	Acc@1 82.812 (81.803)	Acc@5 98.438 (99.026)
Epoch: [19][128/391]	Time 0.419 (0.406)	Data 0.002 (0.004)	Loss 0.9350 (0.8594)	Acc@1 79.688 (81.298)	Acc@5 98.438 (99.049)
Epoch: [19][192/391]	Time 0.390 (0.405)	Data 0.002 (0.003)	Loss 0.7433 (0.8569)	Acc@1 82.812 (81.246)	Acc@5 99.219 (99.065)
Epoch: [19][256/391]	Time 0.400 (0.404)	Data 0.002 (0.003)	Loss 0.7179 (0.8514)	Acc@1 82.031 (81.366)	Acc@5 100.000 (99.070)
Epoch: [19][320/391]	Time 0.321 (0.402)	Data 0.001 (0.003)	Loss 0.8274 (0.8523)	Acc@1 81.250 (81.308)	Acc@5 99.219 (99.075)
Epoch: [19][384/391]	Time 0.359 (0.403)	Data 0.001 (0.003)	Loss 0.7291 (0.8563)	Acc@1 82.812 (81.226)	Acc@5 100.000 (99.034)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.481 (0.481)	Data 0.232 (0.232)	Loss 0.9091 (0.9091)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [20][64/391]	Time 0.394 (0.412)	Data 0.002 (0.005)	Loss 1.0360 (0.8492)	Acc@1 72.656 (81.082)	Acc@5 100.000 (98.990)
Epoch: [20][128/391]	Time 0.403 (0.409)	Data 0.002 (0.004)	Loss 0.9081 (0.8453)	Acc@1 77.344 (81.117)	Acc@5 98.438 (99.073)
Epoch: [20][192/391]	Time 0.412 (0.408)	Data 0.003 (0.003)	Loss 1.0495 (0.8536)	Acc@1 72.656 (80.789)	Acc@5 97.656 (99.008)
Epoch: [20][256/391]	Time 0.383 (0.406)	Data 0.002 (0.003)	Loss 0.9413 (0.8516)	Acc@1 75.781 (80.916)	Acc@5 100.000 (99.039)
Epoch: [20][320/391]	Time 0.333 (0.406)	Data 0.002 (0.003)	Loss 1.0775 (0.8562)	Acc@1 74.219 (80.819)	Acc@5 99.219 (99.029)
Epoch: [20][384/391]	Time 0.379 (0.406)	Data 0.002 (0.003)	Loss 0.7520 (0.8521)	Acc@1 86.719 (80.990)	Acc@5 99.219 (99.044)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 359094 ; 487386 ; 0.7367753690093684

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.516 (0.516)	Data 0.343 (0.343)	Loss 0.9485 (0.9485)	Acc@1 82.031 (82.031)	Acc@5 96.875 (96.875)
Epoch: [21][64/391]	Time 0.384 (0.406)	Data 0.002 (0.007)	Loss 0.9201 (0.8276)	Acc@1 78.125 (81.791)	Acc@5 99.219 (99.159)
Epoch: [21][128/391]	Time 0.405 (0.401)	Data 0.002 (0.005)	Loss 0.8174 (0.8287)	Acc@1 81.250 (81.426)	Acc@5 99.219 (99.152)
Epoch: [21][192/391]	Time 0.391 (0.401)	Data 0.002 (0.004)	Loss 0.8901 (0.8303)	Acc@1 77.344 (81.185)	Acc@5 99.219 (99.146)
Epoch: [21][256/391]	Time 0.421 (0.403)	Data 0.002 (0.003)	Loss 1.0089 (0.8352)	Acc@1 72.656 (80.967)	Acc@5 100.000 (99.073)
Epoch: [21][320/391]	Time 0.407 (0.402)	Data 0.002 (0.003)	Loss 0.7590 (0.8366)	Acc@1 82.812 (80.951)	Acc@5 100.000 (99.075)
Epoch: [21][384/391]	Time 0.439 (0.403)	Data 0.002 (0.003)	Loss 0.8370 (0.8325)	Acc@1 82.812 (81.098)	Acc@5 99.219 (99.081)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.494 (0.494)	Data 0.285 (0.285)	Loss 0.8201 (0.8201)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [22][64/391]	Time 0.393 (0.404)	Data 0.002 (0.006)	Loss 0.8034 (0.8408)	Acc@1 82.031 (80.721)	Acc@5 99.219 (98.978)
Epoch: [22][128/391]	Time 0.382 (0.402)	Data 0.003 (0.004)	Loss 0.7783 (0.8339)	Acc@1 81.250 (80.911)	Acc@5 100.000 (98.989)
Epoch: [22][192/391]	Time 0.407 (0.401)	Data 0.003 (0.003)	Loss 0.7743 (0.8301)	Acc@1 79.688 (81.096)	Acc@5 100.000 (99.041)
Epoch: [22][256/391]	Time 0.427 (0.402)	Data 0.002 (0.003)	Loss 0.8175 (0.8350)	Acc@1 82.031 (80.946)	Acc@5 99.219 (99.003)
Epoch: [22][320/391]	Time 0.440 (0.403)	Data 0.002 (0.003)	Loss 0.8564 (0.8347)	Acc@1 79.688 (80.999)	Acc@5 98.438 (98.992)
Epoch: [22][384/391]	Time 0.398 (0.402)	Data 0.002 (0.003)	Loss 1.0942 (0.8371)	Acc@1 72.656 (80.940)	Acc@5 96.875 (99.004)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.384 (0.384)	Data 0.243 (0.243)	Loss 0.8103 (0.8103)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [23][64/391]	Time 0.429 (0.408)	Data 0.002 (0.006)	Loss 0.8995 (0.8195)	Acc@1 82.812 (81.767)	Acc@5 99.219 (99.147)
Epoch: [23][128/391]	Time 0.384 (0.405)	Data 0.002 (0.004)	Loss 0.8943 (0.8289)	Acc@1 79.688 (81.426)	Acc@5 98.438 (99.146)
Epoch: [23][192/391]	Time 0.373 (0.404)	Data 0.002 (0.003)	Loss 0.7524 (0.8313)	Acc@1 82.031 (81.359)	Acc@5 98.438 (99.045)
Epoch: [23][256/391]	Time 0.417 (0.402)	Data 0.002 (0.003)	Loss 0.9229 (0.8267)	Acc@1 77.344 (81.508)	Acc@5 96.875 (99.039)
Epoch: [23][320/391]	Time 0.410 (0.401)	Data 0.002 (0.003)	Loss 0.8417 (0.8245)	Acc@1 78.125 (81.503)	Acc@5 99.219 (99.041)
Epoch: [23][384/391]	Time 0.399 (0.402)	Data 0.001 (0.003)	Loss 0.9184 (0.8259)	Acc@1 78.906 (81.380)	Acc@5 98.438 (99.071)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.420 (0.420)	Data 0.239 (0.239)	Loss 0.8029 (0.8029)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [24][64/391]	Time 0.385 (0.403)	Data 0.002 (0.006)	Loss 0.7519 (0.8117)	Acc@1 84.375 (82.163)	Acc@5 99.219 (99.087)
Epoch: [24][128/391]	Time 0.401 (0.405)	Data 0.002 (0.004)	Loss 0.8936 (0.8188)	Acc@1 77.344 (81.795)	Acc@5 99.219 (99.104)
Epoch: [24][192/391]	Time 0.384 (0.404)	Data 0.002 (0.003)	Loss 0.9150 (0.8237)	Acc@1 79.688 (81.570)	Acc@5 97.656 (99.041)
Epoch: [24][256/391]	Time 0.493 (0.403)	Data 0.002 (0.003)	Loss 0.8860 (0.8263)	Acc@1 80.469 (81.396)	Acc@5 99.219 (99.064)
Epoch: [24][320/391]	Time 0.391 (0.403)	Data 0.002 (0.003)	Loss 0.7367 (0.8278)	Acc@1 83.594 (81.282)	Acc@5 100.000 (99.058)
Epoch: [24][384/391]	Time 0.400 (0.404)	Data 0.001 (0.003)	Loss 0.9251 (0.8264)	Acc@1 76.562 (81.366)	Acc@5 99.219 (99.038)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.434 (0.434)	Data 0.257 (0.257)	Loss 0.6847 (0.6847)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [25][64/391]	Time 0.354 (0.409)	Data 0.002 (0.006)	Loss 0.7909 (0.8456)	Acc@1 78.906 (80.673)	Acc@5 100.000 (99.002)
Epoch: [25][128/391]	Time 0.392 (0.409)	Data 0.002 (0.004)	Loss 0.6935 (0.8202)	Acc@1 85.938 (81.625)	Acc@5 99.219 (98.989)
Epoch: [25][192/391]	Time 0.375 (0.405)	Data 0.003 (0.003)	Loss 0.8323 (0.8163)	Acc@1 81.250 (81.756)	Acc@5 99.219 (99.012)
Epoch: [25][256/391]	Time 0.453 (0.406)	Data 0.002 (0.003)	Loss 0.8383 (0.8236)	Acc@1 78.906 (81.411)	Acc@5 100.000 (98.997)
Epoch: [25][320/391]	Time 0.403 (0.405)	Data 0.002 (0.003)	Loss 0.8497 (0.8291)	Acc@1 79.688 (81.153)	Acc@5 97.656 (98.951)
Epoch: [25][384/391]	Time 0.406 (0.405)	Data 0.002 (0.003)	Loss 0.8487 (0.8265)	Acc@1 79.688 (81.358)	Acc@5 99.219 (98.965)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
no display found. Using non-interactive Agg backend
[5, 5, 5]
Cifar10: True; cifar100: False
False
Files already downloaded and verified
count0: 487386

Epoch: [1 | 180] LR: 0.100000
Epoch: [1][0/391]	Time 0.484 (0.484)	Data 0.216 (0.216)	Loss 3.3527 (3.3527)	Acc@1 8.594 (8.594)	Acc@5 52.344 (52.344)
Epoch: [1][64/391]	Time 0.439 (0.404)	Data 0.002 (0.005)	Loss 2.6428 (2.8265)	Acc@1 17.188 (19.026)	Acc@5 86.719 (73.486)
Epoch: [1][128/391]	Time 0.378 (0.398)	Data 0.002 (0.003)	Loss 2.4035 (2.6773)	Acc@1 30.469 (22.481)	Acc@5 85.156 (78.149)
Epoch: [1][192/391]	Time 0.391 (0.398)	Data 0.001 (0.003)	Loss 2.2048 (2.5828)	Acc@1 35.938 (25.433)	Acc@5 90.625 (80.687)
Epoch: [1][256/391]	Time 0.400 (0.398)	Data 0.002 (0.003)	Loss 2.2759 (2.5028)	Acc@1 28.906 (27.818)	Acc@5 87.500 (82.703)
Epoch: [1][320/391]	Time 0.361 (0.398)	Data 0.001 (0.002)	Loss 2.1934 (2.4302)	Acc@1 38.281 (30.199)	Acc@5 91.406 (84.236)
Epoch: [1][384/391]	Time 0.434 (0.399)	Data 0.002 (0.002)	Loss 1.8305 (2.3642)	Acc@1 49.219 (32.449)	Acc@5 97.656 (85.446)

Epoch: [2 | 180] LR: 0.100000
Epoch: [2][0/391]	Time 0.427 (0.427)	Data 0.277 (0.277)	Loss 1.9136 (1.9136)	Acc@1 48.438 (48.438)	Acc@5 92.188 (92.188)
Epoch: [2][64/391]	Time 0.381 (0.400)	Data 0.002 (0.006)	Loss 1.7621 (1.9174)	Acc@1 50.000 (48.281)	Acc@5 93.750 (92.993)
Epoch: [2][128/391]	Time 0.407 (0.401)	Data 0.002 (0.004)	Loss 1.6283 (1.8542)	Acc@1 59.375 (50.545)	Acc@5 94.531 (93.556)
Epoch: [2][192/391]	Time 0.415 (0.400)	Data 0.002 (0.003)	Loss 1.6831 (1.8081)	Acc@1 56.250 (51.854)	Acc@5 97.656 (93.973)
Epoch: [2][256/391]	Time 0.414 (0.401)	Data 0.002 (0.003)	Loss 1.6352 (1.7568)	Acc@1 57.812 (53.608)	Acc@5 96.094 (94.331)
Epoch: [2][320/391]	Time 0.446 (0.401)	Data 0.002 (0.003)	Loss 1.4909 (1.7180)	Acc@1 60.156 (54.880)	Acc@5 96.875 (94.607)
Epoch: [2][384/391]	Time 0.400 (0.402)	Data 0.002 (0.003)	Loss 1.3828 (1.6787)	Acc@1 69.531 (56.240)	Acc@5 96.094 (94.882)

Epoch: [3 | 180] LR: 0.100000
Epoch: [3][0/391]	Time 0.425 (0.425)	Data 0.231 (0.231)	Loss 1.5321 (1.5321)	Acc@1 63.281 (63.281)	Acc@5 96.094 (96.094)
Epoch: [3][64/391]	Time 0.399 (0.405)	Data 0.002 (0.005)	Loss 1.4968 (1.4509)	Acc@1 57.812 (62.825)	Acc@5 96.094 (96.334)
Epoch: [3][128/391]	Time 0.379 (0.407)	Data 0.003 (0.004)	Loss 1.2331 (1.4175)	Acc@1 74.219 (64.093)	Acc@5 96.875 (96.615)
Epoch: [3][192/391]	Time 0.332 (0.405)	Data 0.002 (0.003)	Loss 1.2890 (1.3950)	Acc@1 67.969 (64.755)	Acc@5 98.438 (96.802)
Epoch: [3][256/391]	Time 0.399 (0.404)	Data 0.002 (0.003)	Loss 1.3468 (1.3731)	Acc@1 64.062 (65.212)	Acc@5 96.875 (96.848)
Epoch: [3][320/391]	Time 0.381 (0.403)	Data 0.002 (0.003)	Loss 1.5446 (1.3579)	Acc@1 57.031 (65.588)	Acc@5 96.875 (96.929)
Epoch: [3][384/391]	Time 0.391 (0.403)	Data 0.002 (0.002)	Loss 1.3245 (1.3389)	Acc@1 67.188 (66.134)	Acc@5 97.656 (97.009)

Epoch: [4 | 180] LR: 0.100000
Epoch: [4][0/391]	Time 0.439 (0.439)	Data 0.244 (0.244)	Loss 1.0650 (1.0650)	Acc@1 75.000 (75.000)	Acc@5 98.438 (98.438)
Epoch: [4][64/391]	Time 0.431 (0.406)	Data 0.002 (0.006)	Loss 1.0341 (1.2067)	Acc@1 77.344 (69.724)	Acc@5 99.219 (97.969)
Epoch: [4][128/391]	Time 0.375 (0.407)	Data 0.002 (0.004)	Loss 1.3286 (1.2011)	Acc@1 71.094 (69.985)	Acc@5 96.094 (97.826)
Epoch: [4][192/391]	Time 0.403 (0.405)	Data 0.002 (0.003)	Loss 1.1589 (1.1862)	Acc@1 72.656 (70.466)	Acc@5 96.875 (97.794)
Epoch: [4][256/391]	Time 0.386 (0.404)	Data 0.001 (0.003)	Loss 1.0930 (1.1705)	Acc@1 68.750 (70.872)	Acc@5 99.219 (97.884)
Epoch: [4][320/391]	Time 0.374 (0.404)	Data 0.002 (0.003)	Loss 1.1915 (1.1598)	Acc@1 68.750 (71.138)	Acc@5 96.094 (97.931)
Epoch: [4][384/391]	Time 0.389 (0.405)	Data 0.001 (0.003)	Loss 1.0429 (1.1520)	Acc@1 75.000 (71.293)	Acc@5 96.094 (97.957)

Epoch: [5 | 180] LR: 0.100000
Epoch: [5][0/391]	Time 0.447 (0.447)	Data 0.256 (0.256)	Loss 0.9764 (0.9764)	Acc@1 77.344 (77.344)	Acc@5 97.656 (97.656)
Epoch: [5][64/391]	Time 0.369 (0.405)	Data 0.001 (0.006)	Loss 0.9958 (1.1074)	Acc@1 78.125 (72.740)	Acc@5 98.438 (97.728)
Epoch: [5][128/391]	Time 0.358 (0.402)	Data 0.002 (0.004)	Loss 1.0552 (1.0818)	Acc@1 74.219 (73.413)	Acc@5 99.219 (97.905)
Epoch: [5][192/391]	Time 0.446 (0.400)	Data 0.002 (0.003)	Loss 0.9253 (1.0754)	Acc@1 75.781 (73.616)	Acc@5 100.000 (98.025)
Epoch: [5][256/391]	Time 0.319 (0.399)	Data 0.002 (0.003)	Loss 1.0105 (1.0685)	Acc@1 76.562 (73.918)	Acc@5 98.438 (98.097)
Epoch: [5][320/391]	Time 0.346 (0.399)	Data 0.002 (0.003)	Loss 1.0053 (1.0548)	Acc@1 75.000 (74.304)	Acc@5 100.000 (98.165)
Epoch: [5][384/391]	Time 0.371 (0.400)	Data 0.002 (0.003)	Loss 1.0394 (1.0530)	Acc@1 78.906 (74.203)	Acc@5 96.875 (98.137)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

Epoch: [6 | 180] LR: 0.100000
Epoch: [6][0/391]	Time 0.388 (0.388)	Data 0.231 (0.231)	Loss 1.1173 (1.1173)	Acc@1 69.531 (69.531)	Acc@5 98.438 (98.438)
Epoch: [6][64/391]	Time 0.381 (0.397)	Data 0.002 (0.005)	Loss 0.9477 (0.9983)	Acc@1 77.344 (75.252)	Acc@5 97.656 (98.377)
Epoch: [6][128/391]	Time 0.406 (0.403)	Data 0.002 (0.004)	Loss 0.8450 (0.9805)	Acc@1 82.031 (75.799)	Acc@5 100.000 (98.595)
Epoch: [6][192/391]	Time 0.451 (0.402)	Data 0.002 (0.003)	Loss 0.9791 (0.9755)	Acc@1 77.344 (76.012)	Acc@5 96.875 (98.531)
Epoch: [6][256/391]	Time 0.402 (0.403)	Data 0.002 (0.003)	Loss 0.9933 (0.9783)	Acc@1 73.438 (75.854)	Acc@5 99.219 (98.486)
Epoch: [6][320/391]	Time 0.405 (0.403)	Data 0.002 (0.003)	Loss 0.8223 (0.9811)	Acc@1 79.688 (75.779)	Acc@5 99.219 (98.481)
Epoch: [6][384/391]	Time 0.360 (0.402)	Data 0.002 (0.003)	Loss 1.0309 (0.9784)	Acc@1 71.875 (75.889)	Acc@5 99.219 (98.466)

Epoch: [7 | 180] LR: 0.100000
Epoch: [7][0/391]	Time 0.459 (0.459)	Data 0.238 (0.238)	Loss 1.0462 (1.0462)	Acc@1 75.000 (75.000)	Acc@5 97.656 (97.656)
Epoch: [7][64/391]	Time 0.405 (0.407)	Data 0.002 (0.006)	Loss 1.0137 (0.9447)	Acc@1 75.000 (77.380)	Acc@5 98.438 (98.546)
Epoch: [7][128/391]	Time 0.400 (0.404)	Data 0.002 (0.004)	Loss 0.9448 (0.9442)	Acc@1 78.906 (77.416)	Acc@5 98.438 (98.528)
Epoch: [7][192/391]	Time 0.427 (0.403)	Data 0.001 (0.003)	Loss 1.1139 (0.9405)	Acc@1 71.094 (77.396)	Acc@5 96.875 (98.579)
Epoch: [7][256/391]	Time 0.420 (0.404)	Data 0.002 (0.003)	Loss 0.7735 (0.9358)	Acc@1 82.812 (77.444)	Acc@5 99.219 (98.559)
Epoch: [7][320/391]	Time 0.388 (0.405)	Data 0.002 (0.003)	Loss 0.7840 (0.9399)	Acc@1 84.375 (77.288)	Acc@5 99.219 (98.591)
Epoch: [7][384/391]	Time 0.414 (0.405)	Data 0.004 (0.003)	Loss 0.9213 (0.9415)	Acc@1 74.219 (77.127)	Acc@5 98.438 (98.600)

Epoch: [8 | 180] LR: 0.100000
Epoch: [8][0/391]	Time 0.415 (0.415)	Data 0.234 (0.234)	Loss 0.8488 (0.8488)	Acc@1 78.906 (78.906)	Acc@5 100.000 (100.000)
Epoch: [8][64/391]	Time 0.410 (0.407)	Data 0.002 (0.005)	Loss 0.9362 (0.9462)	Acc@1 78.906 (77.019)	Acc@5 99.219 (98.522)
Epoch: [8][128/391]	Time 0.388 (0.404)	Data 0.002 (0.004)	Loss 1.0325 (0.9309)	Acc@1 75.000 (77.828)	Acc@5 94.531 (98.504)
Epoch: [8][192/391]	Time 0.391 (0.404)	Data 0.002 (0.003)	Loss 0.9223 (0.9244)	Acc@1 82.812 (77.955)	Acc@5 99.219 (98.567)
Epoch: [8][256/391]	Time 0.438 (0.404)	Data 0.001 (0.003)	Loss 0.9383 (0.9269)	Acc@1 71.875 (77.620)	Acc@5 98.438 (98.586)
Epoch: [8][320/391]	Time 0.383 (0.403)	Data 0.002 (0.003)	Loss 0.8109 (0.9279)	Acc@1 78.125 (77.626)	Acc@5 100.000 (98.630)
Epoch: [8][384/391]	Time 0.403 (0.403)	Data 0.002 (0.003)	Loss 0.9078 (0.9247)	Acc@1 80.469 (77.717)	Acc@5 98.438 (98.665)

Epoch: [9 | 180] LR: 0.100000
Epoch: [9][0/391]	Time 0.464 (0.464)	Data 0.229 (0.229)	Loss 0.9186 (0.9186)	Acc@1 80.469 (80.469)	Acc@5 100.000 (100.000)
Epoch: [9][64/391]	Time 0.384 (0.413)	Data 0.002 (0.006)	Loss 0.8784 (0.8915)	Acc@1 77.344 (79.062)	Acc@5 98.438 (98.990)
Epoch: [9][128/391]	Time 0.395 (0.410)	Data 0.002 (0.004)	Loss 1.0546 (0.9046)	Acc@1 70.312 (78.597)	Acc@5 97.656 (98.680)
Epoch: [9][192/391]	Time 0.398 (0.406)	Data 0.001 (0.003)	Loss 1.0813 (0.9038)	Acc@1 75.000 (78.627)	Acc@5 97.656 (98.660)
Epoch: [9][256/391]	Time 0.409 (0.406)	Data 0.002 (0.003)	Loss 0.8223 (0.9045)	Acc@1 77.344 (78.523)	Acc@5 98.438 (98.650)
Epoch: [9][320/391]	Time 0.443 (0.404)	Data 0.002 (0.003)	Loss 0.9319 (0.9071)	Acc@1 75.781 (78.371)	Acc@5 94.531 (98.664)
Epoch: [9][384/391]	Time 0.415 (0.405)	Data 0.002 (0.003)	Loss 0.8694 (0.9056)	Acc@1 79.688 (78.500)	Acc@5 97.656 (98.683)

Epoch: [10 | 180] LR: 0.100000
Epoch: [10][0/391]	Time 0.458 (0.458)	Data 0.235 (0.235)	Loss 1.0092 (1.0092)	Acc@1 69.531 (69.531)	Acc@5 99.219 (99.219)
Epoch: [10][64/391]	Time 0.374 (0.411)	Data 0.002 (0.005)	Loss 0.9259 (0.8782)	Acc@1 78.906 (79.519)	Acc@5 98.438 (98.906)
Epoch: [10][128/391]	Time 0.411 (0.411)	Data 0.002 (0.004)	Loss 0.7429 (0.8889)	Acc@1 84.375 (79.288)	Acc@5 99.219 (98.807)
Epoch: [10][192/391]	Time 0.443 (0.408)	Data 0.002 (0.003)	Loss 0.9603 (0.8861)	Acc@1 75.781 (79.182)	Acc@5 98.438 (98.830)
Epoch: [10][256/391]	Time 0.404 (0.406)	Data 0.002 (0.003)	Loss 1.0277 (0.8985)	Acc@1 72.656 (78.706)	Acc@5 97.656 (98.833)
Epoch: [10][320/391]	Time 0.385 (0.405)	Data 0.002 (0.003)	Loss 0.8931 (0.8990)	Acc@1 73.438 (78.636)	Acc@5 99.219 (98.812)
Epoch: [10][384/391]	Time 0.342 (0.400)	Data 0.002 (0.003)	Loss 0.6569 (0.8991)	Acc@1 88.281 (78.640)	Acc@5 100.000 (98.801)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 445842 ; 487386 ; 0.9147616057908926

Epoch: [11 | 180] LR: 0.100000
Epoch: [11][0/391]	Time 0.351 (0.351)	Data 0.195 (0.195)	Loss 0.8473 (0.8473)	Acc@1 78.125 (78.125)	Acc@5 97.656 (97.656)
Epoch: [11][64/391]	Time 0.328 (0.350)	Data 0.002 (0.005)	Loss 0.9459 (0.8207)	Acc@1 78.125 (81.178)	Acc@5 99.219 (99.099)
Epoch: [11][128/391]	Time 0.352 (0.348)	Data 0.001 (0.003)	Loss 0.8205 (0.8523)	Acc@1 78.906 (80.390)	Acc@5 99.219 (98.934)
Epoch: [11][192/391]	Time 0.318 (0.345)	Data 0.002 (0.003)	Loss 0.9649 (0.8615)	Acc@1 76.562 (80.060)	Acc@5 99.219 (98.867)
Epoch: [11][256/391]	Time 0.412 (0.346)	Data 0.002 (0.003)	Loss 0.8798 (0.8707)	Acc@1 76.562 (79.621)	Acc@5 99.219 (98.796)
Epoch: [11][320/391]	Time 0.374 (0.345)	Data 0.002 (0.003)	Loss 1.0095 (0.8764)	Acc@1 72.656 (79.481)	Acc@5 96.875 (98.778)
Epoch: [11][384/391]	Time 0.332 (0.345)	Data 0.001 (0.002)	Loss 0.7753 (0.8763)	Acc@1 85.156 (79.505)	Acc@5 99.219 (98.768)

Epoch: [12 | 180] LR: 0.100000
Epoch: [12][0/391]	Time 0.402 (0.402)	Data 0.231 (0.231)	Loss 0.9835 (0.9835)	Acc@1 77.344 (77.344)	Acc@5 100.000 (100.000)
Epoch: [12][64/391]	Time 0.321 (0.309)	Data 0.002 (0.006)	Loss 0.7877 (0.8753)	Acc@1 81.250 (79.748)	Acc@5 99.219 (98.942)
Epoch: [12][128/391]	Time 0.285 (0.299)	Data 0.003 (0.004)	Loss 0.7521 (0.8795)	Acc@1 84.375 (79.282)	Acc@5 100.000 (98.874)
Epoch: [12][192/391]	Time 0.269 (0.297)	Data 0.002 (0.003)	Loss 0.9415 (0.8761)	Acc@1 75.781 (79.453)	Acc@5 97.656 (98.931)
Epoch: [12][256/391]	Time 0.278 (0.296)	Data 0.002 (0.003)	Loss 0.8975 (0.8786)	Acc@1 80.469 (79.420)	Acc@5 99.219 (98.881)
Epoch: [12][320/391]	Time 0.358 (0.296)	Data 0.003 (0.003)	Loss 0.9085 (0.8780)	Acc@1 79.688 (79.434)	Acc@5 98.438 (98.866)
Epoch: [12][384/391]	Time 0.335 (0.296)	Data 0.002 (0.003)	Loss 0.8206 (0.8750)	Acc@1 82.812 (79.580)	Acc@5 97.656 (98.870)

Epoch: [13 | 180] LR: 0.100000
Epoch: [13][0/391]	Time 0.297 (0.297)	Data 0.204 (0.204)	Loss 1.0127 (1.0127)	Acc@1 71.875 (71.875)	Acc@5 96.875 (96.875)
Epoch: [13][64/391]	Time 0.265 (0.291)	Data 0.002 (0.005)	Loss 0.9248 (0.8567)	Acc@1 78.906 (80.180)	Acc@5 98.438 (98.750)
Epoch: [13][128/391]	Time 0.320 (0.290)	Data 0.002 (0.004)	Loss 0.8041 (0.8499)	Acc@1 86.719 (80.438)	Acc@5 96.875 (98.777)
Epoch: [13][192/391]	Time 0.325 (0.291)	Data 0.002 (0.003)	Loss 0.8862 (0.8618)	Acc@1 79.688 (79.918)	Acc@5 98.438 (98.854)
Epoch: [13][256/391]	Time 0.323 (0.293)	Data 0.002 (0.003)	Loss 0.8001 (0.8651)	Acc@1 82.812 (79.846)	Acc@5 99.219 (98.839)
Epoch: [13][320/391]	Time 0.248 (0.294)	Data 0.003 (0.003)	Loss 0.9417 (0.8689)	Acc@1 76.562 (79.680)	Acc@5 97.656 (98.846)
Epoch: [13][384/391]	Time 0.285 (0.293)	Data 0.002 (0.003)	Loss 0.9288 (0.8695)	Acc@1 78.906 (79.712)	Acc@5 98.438 (98.819)

Epoch: [14 | 180] LR: 0.100000
Epoch: [14][0/391]	Time 0.253 (0.253)	Data 0.194 (0.194)	Loss 0.7996 (0.7996)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [14][64/391]	Time 0.313 (0.292)	Data 0.002 (0.005)	Loss 0.8905 (0.8638)	Acc@1 82.031 (79.459)	Acc@5 99.219 (98.894)
Epoch: [14][128/391]	Time 0.284 (0.291)	Data 0.002 (0.004)	Loss 0.9715 (0.8735)	Acc@1 77.344 (79.318)	Acc@5 98.438 (98.783)
Epoch: [14][192/391]	Time 0.333 (0.291)	Data 0.003 (0.003)	Loss 0.8117 (0.8714)	Acc@1 86.719 (79.546)	Acc@5 96.875 (98.782)
Epoch: [14][256/391]	Time 0.285 (0.291)	Data 0.002 (0.003)	Loss 0.8775 (0.8712)	Acc@1 78.125 (79.444)	Acc@5 100.000 (98.805)
Epoch: [14][320/391]	Time 0.312 (0.291)	Data 0.002 (0.003)	Loss 0.8599 (0.8692)	Acc@1 78.906 (79.573)	Acc@5 99.219 (98.815)
Epoch: [14][384/391]	Time 0.313 (0.292)	Data 0.002 (0.003)	Loss 0.7888 (0.8680)	Acc@1 82.812 (79.584)	Acc@5 99.219 (98.815)

Epoch: [15 | 180] LR: 0.100000
Epoch: [15][0/391]	Time 0.280 (0.280)	Data 0.212 (0.212)	Loss 0.9345 (0.9345)	Acc@1 79.688 (79.688)	Acc@5 96.875 (96.875)
Epoch: [15][64/391]	Time 0.279 (0.293)	Data 0.002 (0.005)	Loss 0.7613 (0.8401)	Acc@1 80.469 (80.661)	Acc@5 99.219 (98.966)
Epoch: [15][128/391]	Time 0.284 (0.292)	Data 0.002 (0.004)	Loss 0.7568 (0.8360)	Acc@1 85.156 (80.808)	Acc@5 100.000 (98.940)
Epoch: [15][192/391]	Time 0.326 (0.291)	Data 0.002 (0.003)	Loss 0.9848 (0.8443)	Acc@1 76.562 (80.562)	Acc@5 100.000 (98.923)
Epoch: [15][256/391]	Time 0.290 (0.292)	Data 0.002 (0.003)	Loss 0.6634 (0.8464)	Acc@1 85.156 (80.396)	Acc@5 99.219 (98.939)
Epoch: [15][320/391]	Time 0.305 (0.292)	Data 0.002 (0.003)	Loss 0.8937 (0.8498)	Acc@1 81.250 (80.245)	Acc@5 99.219 (98.888)
Epoch: [15][384/391]	Time 0.284 (0.291)	Data 0.002 (0.003)	Loss 0.9339 (0.8507)	Acc@1 80.469 (80.256)	Acc@5 98.438 (98.882)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Count: 367916 ; 487386 ; 0.7548760120315314

Epoch: [16 | 180] LR: 0.100000
Epoch: [16][0/391]	Time 0.322 (0.322)	Data 0.193 (0.193)	Loss 0.7515 (0.7515)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [16][64/391]	Time 0.313 (0.291)	Data 0.003 (0.005)	Loss 0.7327 (0.8236)	Acc@1 83.594 (80.685)	Acc@5 98.438 (98.990)
Epoch: [16][128/391]	Time 0.316 (0.279)	Data 0.002 (0.004)	Loss 0.8289 (0.8383)	Acc@1 78.125 (80.196)	Acc@5 98.438 (98.958)
Epoch: [16][192/391]	Time 0.302 (0.274)	Data 0.002 (0.003)	Loss 0.8423 (0.8389)	Acc@1 72.656 (80.283)	Acc@5 100.000 (98.927)
Epoch: [16][256/391]	Time 0.286 (0.273)	Data 0.002 (0.003)	Loss 0.8173 (0.8390)	Acc@1 82.812 (80.396)	Acc@5 97.656 (98.930)
Epoch: [16][320/391]	Time 0.324 (0.272)	Data 0.002 (0.003)	Loss 0.8538 (0.8432)	Acc@1 78.906 (80.276)	Acc@5 99.219 (98.936)
Epoch: [16][384/391]	Time 0.251 (0.270)	Data 0.003 (0.003)	Loss 0.7497 (0.8452)	Acc@1 85.156 (80.217)	Acc@5 99.219 (98.933)

Epoch: [17 | 180] LR: 0.100000
Epoch: [17][0/391]	Time 0.284 (0.284)	Data 0.195 (0.195)	Loss 0.7989 (0.7989)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [17][64/391]	Time 0.285 (0.259)	Data 0.003 (0.005)	Loss 0.8851 (0.8417)	Acc@1 80.469 (80.805)	Acc@5 99.219 (98.954)
Epoch: [17][128/391]	Time 0.314 (0.266)	Data 0.002 (0.004)	Loss 0.7865 (0.8366)	Acc@1 85.938 (80.875)	Acc@5 98.438 (98.928)
Epoch: [17][192/391]	Time 0.231 (0.262)	Data 0.003 (0.003)	Loss 0.8862 (0.8406)	Acc@1 78.125 (80.505)	Acc@5 99.219 (98.943)
Epoch: [17][256/391]	Time 0.273 (0.260)	Data 0.002 (0.003)	Loss 0.7693 (0.8400)	Acc@1 82.812 (80.542)	Acc@5 100.000 (98.960)
Epoch: [17][320/391]	Time 0.204 (0.256)	Data 0.003 (0.003)	Loss 0.8794 (0.8356)	Acc@1 82.812 (80.773)	Acc@5 96.875 (98.966)
Epoch: [17][384/391]	Time 0.952 (0.325)	Data 0.002 (0.003)	Loss 0.7777 (0.8336)	Acc@1 82.812 (80.816)	Acc@5 100.000 (98.959)

Epoch: [18 | 180] LR: 0.100000
Epoch: [18][0/391]	Time 0.976 (0.976)	Data 1.243 (1.243)	Loss 0.9737 (0.9737)	Acc@1 74.219 (74.219)	Acc@5 99.219 (99.219)
Epoch: [18][64/391]	Time 0.288 (0.563)	Data 0.002 (0.022)	Loss 0.6680 (0.8300)	Acc@1 89.062 (80.577)	Acc@5 99.219 (99.026)
Epoch: [18][128/391]	Time 0.345 (0.609)	Data 0.002 (0.012)	Loss 0.8488 (0.8248)	Acc@1 78.906 (81.093)	Acc@5 100.000 (99.079)
Epoch: [18][192/391]	Time 0.794 (0.598)	Data 0.002 (0.009)	Loss 0.8099 (0.8262)	Acc@1 82.812 (80.946)	Acc@5 99.219 (99.037)
Epoch: [18][256/391]	Time 0.437 (0.566)	Data 0.003 (0.008)	Loss 0.7443 (0.8323)	Acc@1 83.594 (80.806)	Acc@5 100.000 (98.960)
Epoch: [18][320/391]	Time 0.398 (0.552)	Data 0.002 (0.007)	Loss 0.8565 (0.8325)	Acc@1 78.125 (80.732)	Acc@5 98.438 (98.971)
Epoch: [18][384/391]	Time 0.447 (0.586)	Data 0.003 (0.006)	Loss 1.0355 (0.8352)	Acc@1 73.438 (80.619)	Acc@5 100.000 (98.947)

Epoch: [19 | 180] LR: 0.100000
Epoch: [19][0/391]	Time 0.696 (0.696)	Data 0.467 (0.467)	Loss 0.8847 (0.8847)	Acc@1 78.125 (78.125)	Acc@5 99.219 (99.219)
Epoch: [19][64/391]	Time 0.506 (0.448)	Data 0.004 (0.010)	Loss 0.8975 (0.8088)	Acc@1 78.125 (81.478)	Acc@5 97.656 (99.087)
Epoch: [19][128/391]	Time 0.429 (0.461)	Data 0.002 (0.006)	Loss 0.8810 (0.8256)	Acc@1 82.031 (80.984)	Acc@5 98.438 (98.861)
Epoch: [19][192/391]	Time 0.332 (0.466)	Data 0.002 (0.005)	Loss 0.8147 (0.8276)	Acc@1 82.812 (81.031)	Acc@5 100.000 (98.871)
Epoch: [19][256/391]	Time 0.316 (0.470)	Data 0.002 (0.004)	Loss 0.8467 (0.8319)	Acc@1 81.250 (80.761)	Acc@5 99.219 (98.906)
Epoch: [19][320/391]	Time 0.213 (0.492)	Data 0.002 (0.004)	Loss 0.7697 (0.8332)	Acc@1 80.469 (80.751)	Acc@5 100.000 (98.900)
Epoch: [19][384/391]	Time 0.657 (0.471)	Data 0.002 (0.004)	Loss 0.7895 (0.8325)	Acc@1 80.469 (80.769)	Acc@5 99.219 (98.886)

Epoch: [20 | 180] LR: 0.100000
Epoch: [20][0/391]	Time 0.777 (0.777)	Data 0.653 (0.653)	Loss 0.7961 (0.7961)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [20][64/391]	Time 0.431 (0.590)	Data 0.003 (0.013)	Loss 0.7274 (0.8087)	Acc@1 81.250 (81.322)	Acc@5 99.219 (99.050)
Epoch: [20][128/391]	Time 0.467 (0.559)	Data 0.002 (0.008)	Loss 0.7045 (0.8198)	Acc@1 82.812 (80.984)	Acc@5 99.219 (99.067)
Epoch: [20][192/391]	Time 0.473 (0.543)	Data 0.002 (0.006)	Loss 0.7409 (0.8202)	Acc@1 86.719 (80.959)	Acc@5 99.219 (99.057)
Epoch: [20][256/391]	Time 0.353 (0.549)	Data 0.002 (0.005)	Loss 0.8208 (0.8220)	Acc@1 82.812 (80.986)	Acc@5 100.000 (99.033)
Epoch: [20][320/391]	Time 0.557 (0.556)	Data 0.003 (0.005)	Loss 0.7330 (0.8284)	Acc@1 85.938 (80.795)	Acc@5 99.219 (99.000)
Epoch: [20][384/391]	Time 0.934 (0.581)	Data 0.002 (0.005)	Loss 0.8280 (0.8243)	Acc@1 83.594 (80.927)	Acc@5 99.219 (98.994)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv30.weight

 RM:  module.conv31.weight

Module List Length:  68
Index1: 60
Index: 31
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: False
Bool2: True
j: 31; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 32; k: 32
Bool1: False
Bool2: True
j: 33; k: 32
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(34, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 58: Conv2d(64, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 59: BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 60: Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 61: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 62 gegen 66: Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 63 gegen 67: BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(34, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  64

Module List Length:  64
Index1: 58
Index: 30
j: 2; k: 2
Bool1: False
Bool2: True
j: 3; k: 2
Bool1: False
Bool2: True
j: 4; k: 4
Bool1: False
Bool2: True
j: 5; k: 4
Bool1: False
Bool2: True
j: 6; k: 6
Bool1: False
Bool2: True
j: 7; k: 6
Bool1: False
Bool2: True
j: 8; k: 8
Bool1: False
Bool2: True
j: 9; k: 8
Bool1: False
Bool2: True
j: 10; k: 10
Bool1: False
Bool2: True
j: 11; k: 10
Bool1: False
Bool2: True
j: 12; k: 12
Bool1: False
Bool2: False
j: 13; k: 12
Bool1: False
Bool2: False
j: 14; k: 12
Bool1: False
Bool2: False
j: 15; k: 15
Bool1: False
Bool2: True
j: 16; k: 15
Bool1: False
Bool2: True
j: 17; k: 17
Bool1: False
Bool2: True
j: 18; k: 17
Bool1: False
Bool2: True
j: 19; k: 19
Bool1: False
Bool2: True
j: 20; k: 19
Bool1: False
Bool2: True
j: 21; k: 21
Bool1: False
Bool2: True
j: 22; k: 21
Bool1: False
Bool2: True
j: 23; k: 23
Bool1: False
Bool2: False
j: 24; k: 23
Bool1: False
Bool2: False
j: 25; k: 23
Bool1: False
Bool2: False
j: 26; k: 26
Bool1: False
Bool2: True
j: 27; k: 26
Bool1: False
Bool2: True
j: 28; k: 28
Bool1: False
Bool2: True
j: 29; k: 28
Bool1: False
Bool2: True
j: 30; k: 30
Bool1: True
Bool2: True
numDelete: 2
j: 31; k: 30
Bool1: False
Bool2: True
Kopiere 0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 2: Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 3: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 4: Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 5: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 6: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 7: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 8: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 9: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 10: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 11: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 12: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 13: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 14: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 15: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 16: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 17: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 18: Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 19: BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 20: Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 21: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 22: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 23: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 24: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 25: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 26: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 27: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 28: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 29: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 30: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 31: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 32: Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 33: BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 34: Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 35: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 36: Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 37: BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 38: Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 39: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 40: Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 41: BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 42: Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 43: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 44: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 45: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 46: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 47: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 48: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Kopiere 49: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 50: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 51: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 52: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 53: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 54: Conv2d(64, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 55: BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Kopiere 56: Conv2d(34, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Kopiere 57: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Ersetze 58 gegen 62: Conv2d(64, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) gegen AdaptiveAvgPool2d(output_size=(1, 1))
Ersetze 59 gegen 63: BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) gegen Linear(in_features=64, out_features=10, bias=True)
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(31, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(34, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): AdaptiveAvgPool2d(output_size=(1, 1))
    (59): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)

Module List Length After Delete:  60
Count: 254962 ; 487386 ; 0.5231213042639715

Epoch: [21 | 180] LR: 0.100000
Epoch: [21][0/391]	Time 0.815 (0.815)	Data 0.786 (0.786)	Loss 1.8831 (1.8831)	Acc@1 46.875 (46.875)	Acc@5 97.656 (97.656)
Epoch: [21][64/391]	Time 0.775 (0.745)	Data 0.003 (0.015)	Loss 0.9892 (1.1078)	Acc@1 79.688 (71.755)	Acc@5 98.438 (98.149)
Epoch: [21][128/391]	Time 1.371 (0.870)	Data 0.002 (0.009)	Loss 1.0265 (0.9889)	Acc@1 74.219 (74.952)	Acc@5 99.219 (98.462)
Epoch: [21][192/391]	Time 0.592 (0.791)	Data 0.002 (0.007)	Loss 0.9271 (0.9411)	Acc@1 77.344 (76.429)	Acc@5 99.219 (98.640)
Epoch: [21][256/391]	Time 0.523 (0.720)	Data 0.003 (0.006)	Loss 0.8703 (0.9141)	Acc@1 78.906 (77.146)	Acc@5 99.219 (98.672)
Epoch: [21][320/391]	Time 0.567 (0.673)	Data 0.002 (0.005)	Loss 0.7974 (0.9043)	Acc@1 82.812 (77.444)	Acc@5 98.438 (98.676)
Epoch: [21][384/391]	Time 0.592 (0.656)	Data 0.002 (0.005)	Loss 0.8756 (0.8936)	Acc@1 81.250 (77.766)	Acc@5 97.656 (98.693)

Epoch: [22 | 180] LR: 0.100000
Epoch: [22][0/391]	Time 0.610 (0.610)	Data 1.155 (1.155)	Loss 0.7692 (0.7692)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [22][64/391]	Time 0.662 (0.631)	Data 0.002 (0.021)	Loss 0.8502 (0.8009)	Acc@1 82.812 (81.190)	Acc@5 98.438 (98.966)
Epoch: [22][128/391]	Time 0.651 (0.654)	Data 0.002 (0.012)	Loss 0.7520 (0.8089)	Acc@1 84.375 (80.687)	Acc@5 98.438 (98.892)
Epoch: [22][192/391]	Time 0.473 (0.685)	Data 0.003 (0.009)	Loss 0.9119 (0.8148)	Acc@1 71.875 (80.428)	Acc@5 98.438 (98.879)
Epoch: [22][256/391]	Time 0.562 (0.650)	Data 0.002 (0.007)	Loss 0.8074 (0.8179)	Acc@1 81.250 (80.371)	Acc@5 98.438 (98.833)
Epoch: [22][320/391]	Time 0.418 (0.667)	Data 0.002 (0.006)	Loss 0.8140 (0.8211)	Acc@1 82.031 (80.286)	Acc@5 100.000 (98.815)
Epoch: [22][384/391]	Time 0.717 (0.636)	Data 0.005 (0.006)	Loss 0.9450 (0.8233)	Acc@1 76.562 (80.215)	Acc@5 99.219 (98.805)

Epoch: [23 | 180] LR: 0.100000
Epoch: [23][0/391]	Time 0.551 (0.551)	Data 0.641 (0.641)	Loss 0.8821 (0.8821)	Acc@1 75.781 (75.781)	Acc@5 99.219 (99.219)
Epoch: [23][64/391]	Time 0.508 (0.494)	Data 0.002 (0.013)	Loss 0.7220 (0.8067)	Acc@1 81.250 (81.010)	Acc@5 100.000 (99.014)
Epoch: [23][128/391]	Time 0.736 (0.540)	Data 0.003 (0.008)	Loss 0.7748 (0.8145)	Acc@1 82.031 (80.517)	Acc@5 97.656 (98.983)
Epoch: [23][192/391]	Time 0.383 (0.539)	Data 0.002 (0.006)	Loss 0.8978 (0.8095)	Acc@1 77.344 (80.566)	Acc@5 96.875 (98.972)
Epoch: [23][256/391]	Time 0.604 (0.529)	Data 0.002 (0.005)	Loss 0.8707 (0.8096)	Acc@1 80.469 (80.533)	Acc@5 100.000 (99.033)
Epoch: [23][320/391]	Time 0.388 (0.517)	Data 0.002 (0.005)	Loss 0.9428 (0.8139)	Acc@1 76.562 (80.449)	Acc@5 97.656 (99.022)
Epoch: [23][384/391]	Time 0.487 (0.504)	Data 0.002 (0.004)	Loss 1.0184 (0.8151)	Acc@1 70.312 (80.461)	Acc@5 99.219 (99.000)

Epoch: [24 | 180] LR: 0.100000
Epoch: [24][0/391]	Time 0.561 (0.561)	Data 0.540 (0.540)	Loss 0.7542 (0.7542)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [24][64/391]	Time 0.595 (0.493)	Data 0.002 (0.011)	Loss 0.7528 (0.7889)	Acc@1 85.938 (81.611)	Acc@5 99.219 (98.978)
Epoch: [24][128/391]	Time 0.596 (0.546)	Data 0.003 (0.007)	Loss 0.8891 (0.8046)	Acc@1 77.344 (80.929)	Acc@5 99.219 (99.019)
Epoch: [24][192/391]	Time 0.582 (0.535)	Data 0.002 (0.006)	Loss 0.8848 (0.8071)	Acc@1 75.781 (80.663)	Acc@5 98.438 (98.952)
Epoch: [24][256/391]	Time 0.617 (0.510)	Data 0.005 (0.005)	Loss 0.8524 (0.8105)	Acc@1 79.688 (80.517)	Acc@5 98.438 (98.985)
Epoch: [24][320/391]	Time 0.422 (0.500)	Data 0.002 (0.004)	Loss 0.8370 (0.8134)	Acc@1 80.469 (80.513)	Acc@5 99.219 (98.936)
Epoch: [24][384/391]	Time 0.512 (0.505)	Data 0.002 (0.004)	Loss 0.8763 (0.8147)	Acc@1 79.688 (80.503)	Acc@5 96.094 (98.937)

Epoch: [25 | 180] LR: 0.100000
Epoch: [25][0/391]	Time 0.551 (0.551)	Data 0.725 (0.725)	Loss 0.8371 (0.8371)	Acc@1 78.125 (78.125)	Acc@5 100.000 (100.000)
Epoch: [25][64/391]	Time 0.357 (0.437)	Data 0.003 (0.014)	Loss 0.7437 (0.8203)	Acc@1 84.375 (80.433)	Acc@5 99.219 (98.990)
Epoch: [25][128/391]	Time 0.399 (0.418)	Data 0.002 (0.009)	Loss 0.8377 (0.8149)	Acc@1 81.250 (80.814)	Acc@5 99.219 (98.989)
Epoch: [25][192/391]	Time 0.552 (0.415)	Data 0.002 (0.007)	Loss 0.9117 (0.8147)	Acc@1 79.688 (81.015)	Acc@5 97.656 (98.948)
Epoch: [25][256/391]	Time 0.470 (0.416)	Data 0.002 (0.006)	Loss 0.6993 (0.8101)	Acc@1 81.250 (81.049)	Acc@5 97.656 (98.963)
Epoch: [25][320/391]	Time 0.502 (0.416)	Data 0.003 (0.005)	Loss 0.8014 (0.8137)	Acc@1 82.031 (80.856)	Acc@5 99.219 (98.961)
Epoch: [25][384/391]	Time 0.482 (0.418)	Data 0.002 (0.005)	Loss 0.7559 (0.8131)	Acc@1 81.250 (80.852)	Acc@5 99.219 (98.983)
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
Traceback (most recent call last):
  File "main.py", line 725, in <module>
    main()
  File "main.py", line 332, in main
    genDenseModel(model, dense_chs, optimizer, 'cifar', use_gpu)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 236, in genDenseModel
    new_param[out_idx, in_idx, :, :] = param[out_ch, in_ch, :, :]
IndexError: index 32 is out of bounds for dimension 0 with size 32
























 ab hier mit pruneTrain
