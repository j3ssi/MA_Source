no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=180, inPlanes=16, large_input=False, lbda=3e-08, logger='MorphLogs1/logMorphNetFlops3e-8', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=True, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 10.55 (27/256)
Train | Batch (101/196) | Top-1: 11.26 (2911/25856)
Train | Batch (196/196) | Top-1: 16.48 (8240/50000)
Regular: 2.9652504920959473
Epoche: 0; regular: 2.9652504920959473: flops 68862592
#Filters: 943, #FLOPs: 50.17M | Top-1: 17.33
Epoch 1
Train | Batch (1/196) | Top-1: 25.00 (64/256)
Train | Batch (101/196) | Top-1: 32.63 (8436/25856)
Train | Batch (196/196) | Top-1: 35.28 (17641/50000)
Regular: 0.20303186774253845
Epoche: 1; regular: 0.20303186774253845: flops 68862592
#Filters: 659, #FLOPs: 38.51M | Top-1: 33.16
Epoch 2
Train | Batch (1/196) | Top-1: 35.16 (90/256)
Train | Batch (101/196) | Top-1: 43.48 (11242/25856)
Train | Batch (196/196) | Top-1: 45.65 (22825/50000)
Regular: 0.15660534799098969
Epoche: 2; regular: 0.15660534799098969: flops 68862592
#Filters: 475, #FLOPs: 28.52M | Top-1: 24.53
Epoch 3
Train | Batch (1/196) | Top-1: 50.39 (129/256)
Train | Batch (101/196) | Top-1: 50.38 (13027/25856)
Train | Batch (196/196) | Top-1: 51.32 (25658/50000)
Regular: 0.17889583110809326
Epoche: 3; regular: 0.17889583110809326: flops 68862592
#Filters: 409, #FLOPs: 23.59M | Top-1: 26.72
Epoch 4
Train | Batch (1/196) | Top-1: 56.25 (144/256)
Train | Batch (101/196) | Top-1: 53.50 (13834/25856)
Train | Batch (196/196) | Top-1: 54.43 (27215/50000)
Regular: 0.18014997243881226
Epoche: 4; regular: 0.18014997243881226: flops 68862592
#Filters: 397, #FLOPs: 23.24M | Top-1: 36.43
Epoch 5
Train | Batch (1/196) | Top-1: 51.95 (133/256)
Train | Batch (101/196) | Top-1: 56.27 (14550/25856)
Train | Batch (196/196) | Top-1: 56.77 (28383/50000)
Regular: 0.17832806706428528
Epoche: 5; regular: 0.17832806706428528: flops 68862592
#Filters: 374, #FLOPs: 21.81M | Top-1: 33.95
Epoch 6
Train | Batch (1/196) | Top-1: 55.47 (142/256)
Train | Batch (101/196) | Top-1: 57.69 (14917/25856)
Train | Batch (196/196) | Top-1: 57.97 (28983/50000)
Regular: 0.17722955346107483
Epoche: 6; regular: 0.17722955346107483: flops 68862592
#Filters: 329, #FLOPs: 19.19M | Top-1: 40.69
Epoch 7
Train | Batch (1/196) | Top-1: 59.38 (152/256)
Train | Batch (101/196) | Top-1: 59.06 (15271/25856)
Train | Batch (196/196) | Top-1: 59.48 (29738/50000)
Regular: 0.18051782250404358
Epoche: 7; regular: 0.18051782250404358: flops 68862592
#Filters: 326, #FLOPs: 18.80M | Top-1: 38.89
Epoch 8
Train | Batch (1/196) | Top-1: 51.95 (133/256)
Train | Batch (101/196) | Top-1: 60.07 (15531/25856)
Train | Batch (196/196) | Top-1: 60.37 (30186/50000)
Regular: 0.18127021193504333
Epoche: 8; regular: 0.18127021193504333: flops 68862592
#Filters: 302, #FLOPs: 17.62M | Top-1: 46.79
Epoch 9
Train | Batch (1/196) | Top-1: 57.42 (147/256)
Train | Batch (101/196) | Top-1: 61.11 (15801/25856)
Train | Batch (196/196) | Top-1: 61.07 (30536/50000)
Regular: 0.1789240539073944
Epoche: 9; regular: 0.1789240539073944: flops 68862592
#Filters: 289, #FLOPs: 16.40M | Top-1: 33.47
Epoch 10
Train | Batch (1/196) | Top-1: 60.55 (155/256)
Train | Batch (101/196) | Top-1: 61.05 (15784/25856)
Train | Batch (196/196) | Top-1: 61.62 (30810/50000)
Regular: 0.18072281777858734
Epoche: 10; regular: 0.18072281777858734: flops 68862592
#Filters: 290, #FLOPs: 17.11M | Top-1: 38.97
Epoch 11
Train | Batch (1/196) | Top-1: 59.77 (153/256)
Train | Batch (101/196) | Top-1: 61.81 (15981/25856)
Train | Batch (196/196) | Top-1: 62.31 (31157/50000)
Regular: 0.18606820702552795
Epoche: 11; regular: 0.18606820702552795: flops 68862592
#Filters: 280, #FLOPs: 16.49M | Top-1: 44.07
Epoch 12
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 62.63 (16193/25856)
Train | Batch (196/196) | Top-1: 63.10 (31552/50000)
Regular: 0.18792052567005157
Epoche: 12; regular: 0.18792052567005157: flops 68862592
#Filters: 278, #FLOPs: 16.41M | Top-1: 50.63
Epoch 13
Train | Batch (1/196) | Top-1: 63.28 (162/256)
Train | Batch (101/196) | Top-1: 63.90 (16522/25856)
Train | Batch (196/196) | Top-1: 63.75 (31875/50000)
Regular: 0.18698175251483917
Epoche: 13; regular: 0.18698175251483917: flops 68862592
#Filters: 270, #FLOPs: 15.75M | Top-1: 44.74
Epoch 14
Train | Batch (1/196) | Top-1: 61.72 (158/256)
Train | Batch (101/196) | Top-1: 64.10 (16573/25856)
Train | Batch (196/196) | Top-1: 64.06 (32031/50000)
Regular: 0.19505932927131653
Epoche: 14; regular: 0.19505932927131653: flops 68862592
#Filters: 249, #FLOPs: 14.52M | Top-1: 33.82
Epoch 15
Train | Batch (1/196) | Top-1: 60.94 (156/256)
Train | Batch (101/196) | Top-1: 64.02 (16553/25856)
Train | Batch (196/196) | Top-1: 64.29 (32146/50000)
Regular: 0.18924255669116974
Epoche: 15; regular: 0.18924255669116974: flops 68862592
#Filters: 253, #FLOPs: 14.76M | Top-1: 48.80
Epoch 16
Train | Batch (1/196) | Top-1: 65.23 (167/256)
Train | Batch (101/196) | Top-1: 64.32 (16630/25856)
Train | Batch (196/196) | Top-1: 64.62 (32309/50000)
Regular: 0.18863943219184875
Epoche: 16; regular: 0.18863943219184875: flops 68862592
#Filters: 253, #FLOPs: 14.38M | Top-1: 39.32
Epoch 17
Train | Batch (1/196) | Top-1: 59.77 (153/256)
Train | Batch (101/196) | Top-1: 65.07 (16824/25856)
Train | Batch (196/196) | Top-1: 65.22 (32610/50000)
Regular: 0.18789708614349365
Epoche: 17; regular: 0.18789708614349365: flops 68862592
#Filters: 261, #FLOPs: 15.02M | Top-1: 33.81
Epoch 18
Train | Batch (1/196) | Top-1: 62.11 (159/256)
Train | Batch (101/196) | Top-1: 65.54 (16945/25856)
Train | Batch (196/196) | Top-1: 65.47 (32735/50000)
Regular: 0.18766982853412628
Epoche: 18; regular: 0.18766982853412628: flops 68862592
#Filters: 250, #FLOPs: 14.41M | Top-1: 38.55
Epoch 19
Train | Batch (1/196) | Top-1: 63.67 (163/256)
Train | Batch (101/196) | Top-1: 65.08 (16828/25856)
Train | Batch (196/196) | Top-1: 65.13 (32564/50000)
Regular: 0.18577538430690765
Epoche: 19; regular: 0.18577538430690765: flops 68862592
#Filters: 247, #FLOPs: 14.34M | Top-1: 25.29
Epoch 20
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 65.86 (17030/25856)
Train | Batch (196/196) | Top-1: 65.68 (32840/50000)
Regular: 0.194487065076828
Epoche: 20; regular: 0.194487065076828: flops 68862592
#Filters: 246, #FLOPs: 14.51M | Top-1: 24.15
Epoch 21
Train | Batch (1/196) | Top-1: 67.97 (174/256)
Train | Batch (101/196) | Top-1: 66.40 (17169/25856)
Train | Batch (196/196) | Top-1: 66.17 (33087/50000)
Regular: 0.19003061950206757
Epoche: 21; regular: 0.19003061950206757: flops 68862592
#Filters: 244, #FLOPs: 14.21M | Top-1: 31.55
Epoch 22
Train | Batch (1/196) | Top-1: 62.89 (161/256)
Train | Batch (101/196) | Top-1: 65.63 (16970/25856)
Train | Batch (196/196) | Top-1: 65.64 (32820/50000)
Regular: 0.18946191668510437
Epoche: 22; regular: 0.18946191668510437: flops 68862592
#Filters: 249, #FLOPs: 14.17M | Top-1: 52.28
Epoch 23
Train | Batch (1/196) | Top-1: 61.33 (157/256)
Train | Batch (101/196) | Top-1: 65.81 (17017/25856)
Train | Batch (196/196) | Top-1: 65.89 (32947/50000)
Regular: 0.19449424743652344
Epoche: 23; regular: 0.19449424743652344: flops 68862592
#Filters: 250, #FLOPs: 14.08M | Top-1: 26.44
Epoch 24
Train | Batch (1/196) | Top-1: 63.67 (163/256)
Train | Batch (101/196) | Top-1: 66.35 (17156/25856)
Train | Batch (196/196) | Top-1: 66.33 (33165/50000)
Regular: 0.19006483256816864
Epoche: 24; regular: 0.19006483256816864: flops 68862592
#Filters: 251, #FLOPs: 14.62M | Top-1: 53.23
Epoch 25
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 66.40 (17169/25856)
Train | Batch (196/196) | Top-1: 66.30 (33151/50000)
Regular: 0.1948668658733368
Epoche: 25; regular: 0.1948668658733368: flops 68862592
#Filters: 250, #FLOPs: 14.47M | Top-1: 36.12
Epoch 26
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 66.10 (17090/25856)
Train | Batch (196/196) | Top-1: 66.41 (33206/50000)
Regular: 0.19455695152282715
Epoche: 26; regular: 0.19455695152282715: flops 68862592
#Filters: 248, #FLOPs: 14.01M | Top-1: 33.07
Epoch 27
Train | Batch (1/196) | Top-1: 58.98 (151/256)
Train | Batch (101/196) | Top-1: 66.69 (17243/25856)
Train | Batch (196/196) | Top-1: 66.67 (33335/50000)
Regular: 0.18875791132450104
Epoche: 27; regular: 0.18875791132450104: flops 68862592
#Filters: 246, #FLOPs: 14.12M | Top-1: 53.83
Epoch 28
Train | Batch (1/196) | Top-1: 69.14 (177/256)
Train | Batch (101/196) | Top-1: 66.66 (17236/25856)
Train | Batch (196/196) | Top-1: 66.71 (33353/50000)
Regular: 0.19200250506401062
Epoche: 28; regular: 0.19200250506401062: flops 68862592
#Filters: 247, #FLOPs: 13.86M | Top-1: 36.17
Epoch 29
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 66.72 (17251/25856)
Train | Batch (196/196) | Top-1: 66.26 (33129/50000)
Regular: 0.19172832369804382
Epoche: 29; regular: 0.19172832369804382: flops 68862592
#Filters: 243, #FLOPs: 14.43M | Top-1: 55.63
Epoch 30
Train | Batch (1/196) | Top-1: 62.89 (161/256)
Train | Batch (101/196) | Top-1: 66.10 (17090/25856)
Train | Batch (196/196) | Top-1: 66.37 (33185/50000)
Regular: 0.19328001141548157
Epoche: 30; regular: 0.19328001141548157: flops 68862592
#Filters: 245, #FLOPs: 14.32M | Top-1: 27.74
Epoch 31
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 66.38 (17162/25856)
Train | Batch (196/196) | Top-1: 66.73 (33363/50000)
Regular: 0.19516310095787048
Epoche: 31; regular: 0.19516310095787048: flops 68862592
#Filters: 249, #FLOPs: 14.56M | Top-1: 50.33
Epoch 32
Train | Batch (1/196) | Top-1: 67.97 (174/256)
Train | Batch (101/196) | Top-1: 67.00 (17323/25856)
Train | Batch (196/196) | Top-1: 66.70 (33350/50000)
Regular: 0.19183354079723358
Epoche: 32; regular: 0.19183354079723358: flops 68862592
#Filters: 248, #FLOPs: 14.03M | Top-1: 25.93
Epoch 33
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 67.33 (17410/25856)
Train | Batch (196/196) | Top-1: 67.40 (33699/50000)
Regular: 0.19108740985393524
Epoche: 33; regular: 0.19108740985393524: flops 68862592
#Filters: 244, #FLOPs: 14.21M | Top-1: 31.83
Epoch 34
Train | Batch (1/196) | Top-1: 63.67 (163/256)
Train | Batch (101/196) | Top-1: 66.75 (17259/25856)
Train | Batch (196/196) | Top-1: 67.02 (33508/50000)
Regular: 0.197280615568161
Epoche: 34; regular: 0.197280615568161: flops 68862592
#Filters: 247, #FLOPs: 14.29M | Top-1: 40.08
Epoch 35
Train | Batch (1/196) | Top-1: 62.11 (159/256)
Train | Batch (101/196) | Top-1: 66.88 (17292/25856)
Train | Batch (196/196) | Top-1: 67.16 (33580/50000)
Regular: 0.19325046241283417
Epoche: 35; regular: 0.19325046241283417: flops 68862592
#Filters: 248, #FLOPs: 14.21M | Top-1: 30.51
Epoch 36
Train | Batch (1/196) | Top-1: 61.33 (157/256)
Train | Batch (101/196) | Top-1: 66.99 (17321/25856)
Train | Batch (196/196) | Top-1: 67.11 (33556/50000)
Regular: 0.1936223804950714
Epoche: 36; regular: 0.1936223804950714: flops 68862592
#Filters: 245, #FLOPs: 14.29M | Top-1: 39.75
Epoch 37
Train | Batch (1/196) | Top-1: 65.23 (167/256)
Train | Batch (101/196) | Top-1: 67.24 (17386/25856)
Train | Batch (196/196) | Top-1: 67.05 (33524/50000)
Regular: 0.19395548105239868
Epoche: 37; regular: 0.19395548105239868: flops 68862592
#Filters: 241, #FLOPs: 14.19M | Top-1: 56.06
Epoch 38
Train | Batch (1/196) | Top-1: 64.45 (165/256)
Train | Batch (101/196) | Top-1: 67.59 (17477/25856)
Train | Batch (196/196) | Top-1: 67.23 (33613/50000)
Regular: 0.19340234994888306
Epoche: 38; regular: 0.19340234994888306: flops 68862592
#Filters: 248, #FLOPs: 14.32M | Top-1: 35.32
Epoch 39
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 67.32 (17405/25856)
Train | Batch (196/196) | Top-1: 67.46 (33728/50000)
Regular: 0.19032807648181915
Epoche: 39; regular: 0.19032807648181915: flops 68862592
#Filters: 245, #FLOPs: 14.41M | Top-1: 32.05
Epoch 40
Train | Batch (1/196) | Top-1: 64.45 (165/256)
Train | Batch (101/196) | Top-1: 67.17 (17367/25856)
Train | Batch (196/196) | Top-1: 67.31 (33653/50000)
Regular: 0.19318969547748566
Epoche: 40; regular: 0.19318969547748566: flops 68862592
#Filters: 246, #FLOPs: 14.06M | Top-1: 44.02
Epoch 41
Train | Batch (1/196) | Top-1: 62.89 (161/256)
Train | Batch (101/196) | Top-1: 67.20 (17374/25856)
Train | Batch (196/196) | Top-1: 67.55 (33774/50000)
Regular: 0.1998712569475174
Epoche: 41; regular: 0.1998712569475174: flops 68862592
#Filters: 244, #FLOPs: 14.29M | Top-1: 40.58
Epoch 42
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 67.35 (17414/25856)
Train | Batch (196/196) | Top-1: 67.51 (33754/50000)
Regular: 0.19999727606773376
Epoche: 42; regular: 0.19999727606773376: flops 68862592
#Filters: 246, #FLOPs: 14.16M | Top-1: 45.71
Epoch 43
Train | Batch (1/196) | Top-1: 64.84 (166/256)
Train | Batch (101/196) | Top-1: 67.68 (17499/25856)
Train | Batch (196/196) | Top-1: 67.63 (33816/50000)
Regular: 0.1987244337797165
Epoche: 43; regular: 0.1987244337797165: flops 68862592
#Filters: 244, #FLOPs: 14.27M | Top-1: 41.49
Epoch 44
Train | Batch (1/196) | Top-1: 69.14 (177/256)
Train | Batch (101/196) | Top-1: 67.46 (17443/25856)
Train | Batch (196/196) | Top-1: 67.60 (33799/50000)
Regular: 0.19405871629714966
Epoche: 44; regular: 0.19405871629714966: flops 68862592
#Filters: 244, #FLOPs: 14.27M | Top-1: 45.45
Epoch 45
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 67.52 (17457/25856)
Train | Batch (196/196) | Top-1: 67.69 (33843/50000)
Regular: 0.19718988239765167
Epoche: 45; regular: 0.19718988239765167: flops 68862592
#Filters: 249, #FLOPs: 14.23M | Top-1: 45.82
Epoch 46
Train | Batch (1/196) | Top-1: 67.97 (174/256)
Train | Batch (101/196) | Top-1: 67.83 (17538/25856)
Train | Batch (196/196) | Top-1: 67.87 (33933/50000)
Regular: 0.19388101994991302
Epoche: 46; regular: 0.19388101994991302: flops 68862592
#Filters: 246, #FLOPs: 14.43M | Top-1: 54.58
Epoch 47
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 68.14 (17619/25856)
Train | Batch (196/196) | Top-1: 68.01 (34007/50000)
Regular: 0.1955772340297699
Epoche: 47; regular: 0.1955772340297699: flops 68862592
#Filters: 241, #FLOPs: 14.25M | Top-1: 54.52
Epoch 48
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 67.86 (17547/25856)
Train | Batch (196/196) | Top-1: 67.76 (33878/50000)
Regular: 0.1995082050561905
Epoche: 48; regular: 0.1995082050561905: flops 68862592
#Filters: 247, #FLOPs: 14.34M | Top-1: 24.00
Epoch 49
Train | Batch (1/196) | Top-1: 64.84 (166/256)
Train | Batch (101/196) | Top-1: 67.92 (17562/25856)
Train | Batch (196/196) | Top-1: 67.86 (33931/50000)
Regular: 0.20637084543704987
Epoche: 49; regular: 0.20637084543704987: flops 68862592
#Filters: 243, #FLOPs: 14.34M | Top-1: 41.36
Epoch 50
Train | Batch (1/196) | Top-1: 67.58 (173/256)
Train | Batch (101/196) | Top-1: 67.72 (17510/25856)
Train | Batch (196/196) | Top-1: 67.72 (33861/50000)
Regular: 0.19971048831939697
Epoche: 50; regular: 0.19971048831939697: flops 68862592
#Filters: 246, #FLOPs: 14.34M | Top-1: 44.21
Epoch 51
Train | Batch (1/196) | Top-1: 62.89 (161/256)
Train | Batch (101/196) | Top-1: 68.32 (17664/25856)
Train | Batch (196/196) | Top-1: 68.09 (34046/50000)
Regular: 0.19615258276462555
Epoche: 51; regular: 0.19615258276462555: flops 68862592
#Filters: 246, #FLOPs: 14.19M | Top-1: 52.42
Epoch 52
Train | Batch (1/196) | Top-1: 66.80 (171/256)
Train | Batch (101/196) | Top-1: 68.45 (17699/25856)
Train | Batch (196/196) | Top-1: 68.29 (34144/50000)
Regular: 0.19459354877471924
Epoche: 52; regular: 0.19459354877471924: flops 68862592
#Filters: 242, #FLOPs: 14.21M | Top-1: 43.79
Epoch 53
Train | Batch (1/196) | Top-1: 67.19 (172/256)
Train | Batch (101/196) | Top-1: 67.62 (17483/25856)
Train | Batch (196/196) | Top-1: 67.86 (33931/50000)
Regular: 0.19967563450336456
Epoche: 53; regular: 0.19967563450336456: flops 68862592
#Filters: 244, #FLOPs: 14.17M | Top-1: 38.79
Epoch 54
Train | Batch (1/196) | Top-1: 62.50 (160/256)
Train | Batch (101/196) | Top-1: 68.51 (17715/25856)
Train | Batch (196/196) | Top-1: 68.28 (34139/50000)
Regular: 0.1958548128604889
Epoche: 54; regular: 0.1958548128604889: flops 68862592
#Filters: 245, #FLOPs: 14.23M | Top-1: 32.71
Epoch 55
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 68.44 (17695/25856)
Train | Batch (196/196) | Top-1: 68.51 (34253/50000)
Regular: 0.19769559800624847
Epoche: 55; regular: 0.19769559800624847: flops 68862592
#Filters: 249, #FLOPs: 14.19M | Top-1: 44.92
Epoch 56
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 68.15 (17621/25856)
Train | Batch (196/196) | Top-1: 68.17 (34083/50000)
Regular: 0.1978909820318222
Epoche: 56; regular: 0.1978909820318222: flops 68862592
#Filters: 251, #FLOPs: 14.52M | Top-1: 33.78
Epoch 57
Train | Batch (1/196) | Top-1: 63.28 (162/256)
Train | Batch (101/196) | Top-1: 68.33 (17668/25856)
Train | Batch (196/196) | Top-1: 68.47 (34233/50000)
Regular: 0.19833458960056305
Epoche: 57; regular: 0.19833458960056305: flops 68862592
#Filters: 247, #FLOPs: 14.05M | Top-1: 43.30
Epoch 58
Train | Batch (1/196) | Top-1: 67.19 (172/256)
Train | Batch (101/196) | Top-1: 68.19 (17632/25856)
Train | Batch (196/196) | Top-1: 68.23 (34117/50000)
Regular: 0.2009241282939911
Epoche: 58; regular: 0.2009241282939911: flops 68862592
#Filters: 244, #FLOPs: 14.34M | Top-1: 47.13
Epoch 59
Train | Batch (1/196) | Top-1: 69.92 (179/256)
Train | Batch (101/196) | Top-1: 68.70 (17762/25856)
Train | Batch (196/196) | Top-1: 68.59 (34296/50000)
Regular: 0.19842906296253204
Epoche: 59; regular: 0.19842906296253204: flops 68862592
#Filters: 243, #FLOPs: 14.16M | Top-1: 35.98
Epoch 60
Train | Batch (1/196) | Top-1: 67.58 (173/256)
Train | Batch (101/196) | Top-1: 67.98 (17577/25856)
Train | Batch (196/196) | Top-1: 68.41 (34203/50000)
Regular: 0.19611576199531555
Epoche: 60; regular: 0.19611576199531555: flops 68862592
#Filters: 242, #FLOPs: 14.10M | Top-1: 23.89
Epoch 61
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 68.47 (17703/25856)
Train | Batch (196/196) | Top-1: 68.67 (34336/50000)
Regular: 0.2083997279405594
Epoche: 61; regular: 0.2083997279405594: flops 68862592
#Filters: 243, #FLOPs: 14.32M | Top-1: 45.15
Epoch 62
Train | Batch (1/196) | Top-1: 63.28 (162/256)
Train | Batch (101/196) | Top-1: 68.65 (17751/25856)
Train | Batch (196/196) | Top-1: 68.78 (34389/50000)
Regular: 0.20006988942623138
Epoche: 62; regular: 0.20006988942623138: flops 68862592
#Filters: 249, #FLOPs: 14.14M | Top-1: 43.30
Epoch 63
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 68.58 (17733/25856)
Train | Batch (196/196) | Top-1: 68.63 (34316/50000)
Regular: 0.2033930867910385
Epoche: 63; regular: 0.2033930867910385: flops 68862592
#Filters: 248, #FLOPs: 14.21M | Top-1: 48.22
Epoch 64
Train | Batch (1/196) | Top-1: 69.92 (179/256)
Train | Batch (101/196) | Top-1: 68.41 (17689/25856)
Train | Batch (196/196) | Top-1: 68.72 (34358/50000)
Regular: 0.19724662601947784
Epoche: 64; regular: 0.19724662601947784: flops 68862592
#Filters: 247, #FLOPs: 14.08M | Top-1: 59.16
Epoch 65
Train | Batch (1/196) | Top-1: 72.66 (186/256)
Train | Batch (101/196) | Top-1: 69.08 (17862/25856)
Train | Batch (196/196) | Top-1: 68.93 (34465/50000)
Regular: 0.196676105260849
Epoche: 65; regular: 0.196676105260849: flops 68862592
#Filters: 243, #FLOPs: 14.08M | Top-1: 53.80
Epoch 66
Train | Batch (1/196) | Top-1: 70.31 (180/256)
Train | Batch (101/196) | Top-1: 68.99 (17839/25856)
Train | Batch (196/196) | Top-1: 68.91 (34455/50000)
Regular: 0.19430166482925415
Epoche: 66; regular: 0.19430166482925415: flops 68862592
#Filters: 241, #FLOPs: 14.49M | Top-1: 46.75
Epoch 67
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 68.81 (17792/25856)
Train | Batch (196/196) | Top-1: 68.75 (34376/50000)
Regular: 0.20360012352466583
Epoche: 67; regular: 0.20360012352466583: flops 68862592
#Filters: 246, #FLOPs: 14.19M | Top-1: 51.72
Epoch 68
Train | Batch (1/196) | Top-1: 67.58 (173/256)
Train | Batch (101/196) | Top-1: 68.76 (17778/25856)
Train | Batch (196/196) | Top-1: 68.88 (34438/50000)
Regular: 0.1998874545097351
Epoche: 68; regular: 0.1998874545097351: flops 68862592
#Filters: 245, #FLOPs: 14.19M | Top-1: 37.11
Epoch 69
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 69.01 (17843/25856)
Train | Batch (196/196) | Top-1: 69.17 (34585/50000)
Regular: 0.20181484520435333
Epoche: 69; regular: 0.20181484520435333: flops 68862592
#Filters: 241, #FLOPs: 14.38M | Top-1: 47.18
Epoch 70
Train | Batch (1/196) | Top-1: 68.36 (175/256)
Train | Batch (101/196) | Top-1: 68.28 (17655/25856)
Train | Batch (196/196) | Top-1: 68.30 (34148/50000)
Regular: 0.20517878234386444
Epoche: 70; regular: 0.20517878234386444: flops 68862592
#Filters: 251, #FLOPs: 14.34M | Top-1: 49.21
Epoch 71
Train | Batch (1/196) | Top-1: 69.14 (177/256)
Train | Batch (101/196) | Top-1: 69.19 (17890/25856)
Train | Batch (196/196) | Top-1: 68.88 (34442/50000)
Regular: 0.19814078509807587
Epoche: 71; regular: 0.19814078509807587: flops 68862592
#Filters: 244, #FLOPs: 14.38M | Top-1: 40.21
Epoch 72
Train | Batch (1/196) | Top-1: 67.58 (173/256)
Train | Batch (101/196) | Top-1: 69.08 (17861/25856)
Train | Batch (196/196) | Top-1: 68.93 (34466/50000)
Regular: 0.2034943848848343
Epoche: 72; regular: 0.2034943848848343: flops 68862592
#Filters: 246, #FLOPs: 14.45M | Top-1: 36.85
Epoch 73
Train | Batch (1/196) | Top-1: 61.72 (158/256)
Train | Batch (101/196) | Top-1: 68.68 (17759/25856)
Train | Batch (196/196) | Top-1: 68.62 (34310/50000)
Regular: 0.19943708181381226
Epoche: 73; regular: 0.19943708181381226: flops 68862592
#Filters: 245, #FLOPs: 14.19M | Top-1: 44.47
Epoch 74
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 68.63 (17746/25856)
Train | Batch (196/196) | Top-1: 68.97 (34483/50000)
Regular: 0.19902415573596954
Epoche: 74; regular: 0.19902415573596954: flops 68862592
#Filters: 244, #FLOPs: 14.08M | Top-1: 44.34
Epoch 75
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 69.07 (17859/25856)
Train | Batch (196/196) | Top-1: 69.21 (34606/50000)
Regular: 0.1989019513130188
Epoche: 75; regular: 0.1989019513130188: flops 68862592
#Filters: 247, #FLOPs: 14.08M | Top-1: 33.71
Epoch 76
Train | Batch (1/196) | Top-1: 69.14 (177/256)
Train | Batch (101/196) | Top-1: 68.91 (17817/25856)
Train | Batch (196/196) | Top-1: 68.95 (34473/50000)
Regular: 0.1952502578496933
Epoche: 76; regular: 0.1952502578496933: flops 68862592
#Filters: 245, #FLOPs: 14.38M | Top-1: 55.40
Epoch 77
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 69.25 (17906/25856)
Train | Batch (196/196) | Top-1: 69.04 (34520/50000)
Regular: 0.1997278928756714
Epoche: 77; regular: 0.1997278928756714: flops 68862592
#Filters: 250, #FLOPs: 13.97M | Top-1: 45.86
Epoch 78
Train | Batch (1/196) | Top-1: 69.14 (177/256)
Train | Batch (101/196) | Top-1: 69.26 (17908/25856)
Train | Batch (196/196) | Top-1: 68.90 (34449/50000)
Regular: 0.19904685020446777
Epoche: 78; regular: 0.19904685020446777: flops 68862592
#Filters: 241, #FLOPs: 14.01M | Top-1: 33.42
Epoch 79
Train | Batch (1/196) | Top-1: 64.84 (166/256)
Train | Batch (101/196) | Top-1: 68.82 (17795/25856)
Train | Batch (196/196) | Top-1: 68.92 (34458/50000)
Regular: 0.2042837142944336
Epoche: 79; regular: 0.2042837142944336: flops 68862592
#Filters: 240, #FLOPs: 14.23M | Top-1: 43.27
Epoch 80
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 68.83 (17796/25856)
Train | Batch (196/196) | Top-1: 68.96 (34480/50000)
Regular: 0.20242585241794586
Epoche: 80; regular: 0.20242585241794586: flops 68862592
#Filters: 248, #FLOPs: 14.19M | Top-1: 55.90
Epoch 81
Train | Batch (1/196) | Top-1: 62.89 (161/256)
Train | Batch (101/196) | Top-1: 68.53 (17719/25856)
Train | Batch (196/196) | Top-1: 68.91 (34457/50000)
Regular: 0.1967901885509491
Epoche: 81; regular: 0.1967901885509491: flops 68862592
#Filters: 243, #FLOPs: 14.27M | Top-1: 51.28
Epoch 82
Train | Batch (1/196) | Top-1: 67.58 (173/256)
Train | Batch (101/196) | Top-1: 68.85 (17802/25856)
Train | Batch (196/196) | Top-1: 69.10 (34550/50000)
Regular: 0.20020438730716705
Epoche: 82; regular: 0.20020438730716705: flops 68862592
#Filters: 245, #FLOPs: 14.08M | Top-1: 44.30
Epoch 83
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 68.84 (17799/25856)
Train | Batch (196/196) | Top-1: 69.09 (34544/50000)
Regular: 0.20020949840545654
Epoche: 83; regular: 0.20020949840545654: flops 68862592
#Filters: 244, #FLOPs: 14.05M | Top-1: 39.33
Epoch 84
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 69.37 (17936/25856)
Train | Batch (196/196) | Top-1: 69.36 (34680/50000)
Regular: 0.20280705392360687
Epoche: 84; regular: 0.20280705392360687: flops 68862592
#Filters: 244, #FLOPs: 14.23M | Top-1: 58.13
Epoch 85
Train | Batch (1/196) | Top-1: 65.23 (167/256)
Train | Batch (101/196) | Top-1: 68.95 (17829/25856)
Train | Batch (196/196) | Top-1: 69.34 (34671/50000)
Regular: 0.20045198500156403
Epoche: 85; regular: 0.20045198500156403: flops 68862592
#Filters: 244, #FLOPs: 14.45M | Top-1: 40.62
Epoch 86
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 68.84 (17800/25856)
Train | Batch (196/196) | Top-1: 69.26 (34628/50000)
Regular: 0.20212136209011078
Epoche: 86; regular: 0.20212136209011078: flops 68862592
#Filters: 247, #FLOPs: 13.68M | Top-1: 48.18
Epoch 87
Train | Batch (1/196) | Top-1: 66.80 (171/256)
Train | Batch (101/196) | Top-1: 69.01 (17842/25856)
Train | Batch (196/196) | Top-1: 68.94 (34470/50000)
Regular: 0.20078569650650024
Epoche: 87; regular: 0.20078569650650024: flops 68862592
#Filters: 246, #FLOPs: 14.23M | Top-1: 49.73
Epoch 88
Train | Batch (1/196) | Top-1: 73.44 (188/256)
Train | Batch (101/196) | Top-1: 68.83 (17797/25856)
Train | Batch (196/196) | Top-1: 69.16 (34580/50000)
Regular: 0.19944603741168976
Epoche: 88; regular: 0.19944603741168976: flops 68862592
#Filters: 244, #FLOPs: 14.16M | Top-1: 51.74
Epoch 89
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 69.25 (17905/25856)
Train | Batch (196/196) | Top-1: 69.30 (34650/50000)
Regular: 0.19971656799316406
Epoche: 89; regular: 0.19971656799316406: flops 68862592
#Filters: 245, #FLOPs: 14.19M | Top-1: 39.65
Epoch 90
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 69.48 (17965/25856)
Train | Batch (196/196) | Top-1: 69.25 (34623/50000)
Regular: 0.20370323956012726
Epoche: 90; regular: 0.20370323956012726: flops 68862592
#Filters: 246, #FLOPs: 14.38M | Top-1: 41.47
Epoch 91
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 68.99 (17838/25856)
Train | Batch (196/196) | Top-1: 69.25 (34623/50000)
Regular: 0.200985848903656
Epoche: 91; regular: 0.200985848903656: flops 68862592
#Filters: 245, #FLOPs: 14.45M | Top-1: 54.66
Epoch 92
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 69.11 (17869/25856)
Train | Batch (196/196) | Top-1: 69.42 (34708/50000)
Regular: 0.20211364328861237
Epoche: 92; regular: 0.20211364328861237: flops 68862592
#Filters: 240, #FLOPs: 14.19M | Top-1: 50.15
Epoch 93
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 69.29 (17916/25856)
Train | Batch (196/196) | Top-1: 69.19 (34597/50000)
Regular: 0.20069465041160583
Epoche: 93; regular: 0.20069465041160583: flops 68862592
#Filters: 247, #FLOPs: 14.32M | Top-1: 43.70
Epoch 94
Train | Batch (1/196) | Top-1: 61.33 (157/256)
Train | Batch (101/196) | Top-1: 69.04 (17851/25856)
Train | Batch (196/196) | Top-1: 68.98 (34489/50000)
Regular: 0.20554834604263306
Epoche: 94; regular: 0.20554834604263306: flops 68862592
#Filters: 245, #FLOPs: 13.79M | Top-1: 55.20
Epoch 95
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 69.19 (17891/25856)
Train | Batch (196/196) | Top-1: 69.25 (34627/50000)
Regular: 0.20101803541183472
Epoche: 95; regular: 0.20101803541183472: flops 68862592
#Filters: 242, #FLOPs: 14.01M | Top-1: 37.72
Epoch 96
Train | Batch (1/196) | Top-1: 66.80 (171/256)
Train | Batch (101/196) | Top-1: 68.94 (17826/25856)
Train | Batch (196/196) | Top-1: 69.22 (34608/50000)
Regular: 0.2046346813440323
Epoche: 96; regular: 0.2046346813440323: flops 68862592
#Filters: 247, #FLOPs: 14.23M | Top-1: 53.51
Epoch 97
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 69.46 (17960/25856)
Train | Batch (196/196) | Top-1: 69.33 (34666/50000)
Regular: 0.2084939330816269
Epoche: 97; regular: 0.2084939330816269: flops 68862592
#Filters: 248, #FLOPs: 14.34M | Top-1: 57.75
Epoch 98
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 69.07 (17858/25856)
Train | Batch (196/196) | Top-1: 69.25 (34626/50000)
Regular: 0.2066967636346817
Epoche: 98; regular: 0.2066967636346817: flops 68862592
#Filters: 246, #FLOPs: 14.19M | Top-1: 31.01
Epoch 99
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 69.06 (17856/25856)
Train | Batch (196/196) | Top-1: 69.16 (34578/50000)
Regular: 0.20677228271961212
Epoche: 99; regular: 0.20677228271961212: flops 68862592
#Filters: 247, #FLOPs: 14.30M | Top-1: 55.34
Epoch 100
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 68.99 (17838/25856)
Train | Batch (196/196) | Top-1: 69.20 (34600/50000)
Regular: 0.2018606811761856
Epoche: 100; regular: 0.2018606811761856: flops 68862592
#Filters: 249, #FLOPs: 14.49M | Top-1: 48.81
Epoch 101
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 69.68 (18017/25856)
Train | Batch (196/196) | Top-1: 69.47 (34737/50000)
Regular: 0.20516139268875122
Epoche: 101; regular: 0.20516139268875122: flops 68862592
#Filters: 253, #FLOPs: 14.78M | Top-1: 38.71
Epoch 102
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 69.31 (17922/25856)
Train | Batch (196/196) | Top-1: 69.35 (34675/50000)
Regular: 0.2014114260673523
Epoche: 102; regular: 0.2014114260673523: flops 68862592
#Filters: 248, #FLOPs: 14.67M | Top-1: 47.32
Epoch 103
Train | Batch (1/196) | Top-1: 67.58 (173/256)
Train | Batch (101/196) | Top-1: 69.63 (18004/25856)
Train | Batch (196/196) | Top-1: 69.76 (34880/50000)
Regular: 0.20416228473186493
Epoche: 103; regular: 0.20416228473186493: flops 68862592
#Filters: 246, #FLOPs: 14.23M | Top-1: 58.95
Epoch 104
Train | Batch (1/196) | Top-1: 72.66 (186/256)
Train | Batch (101/196) | Top-1: 69.38 (17939/25856)
Train | Batch (196/196) | Top-1: 69.44 (34722/50000)
Regular: 0.20289431512355804
Epoche: 104; regular: 0.20289431512355804: flops 68862592
#Filters: 251, #FLOPs: 14.71M | Top-1: 27.17
Epoch 105
Train | Batch (1/196) | Top-1: 62.89 (161/256)
Train | Batch (101/196) | Top-1: 69.57 (17988/25856)
Train | Batch (196/196) | Top-1: 69.34 (34670/50000)
Regular: 0.20806348323822021
Epoche: 105; regular: 0.20806348323822021: flops 68862592
#Filters: 250, #FLOPs: 14.82M | Top-1: 48.07
Epoch 106
Train | Batch (1/196) | Top-1: 65.23 (167/256)
Train | Batch (101/196) | Top-1: 69.85 (18061/25856)
Train | Batch (196/196) | Top-1: 69.66 (34829/50000)
Regular: 0.20388439297676086
Epoche: 106; regular: 0.20388439297676086: flops 68862592
#Filters: 252, #FLOPs: 14.27M | Top-1: 49.16
Epoch 107
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 69.60 (17996/25856)
Train | Batch (196/196) | Top-1: 69.58 (34788/50000)
Regular: 0.20730233192443848
Epoche: 107; regular: 0.20730233192443848: flops 68862592
#Filters: 246, #FLOPs: 14.64M | Top-1: 53.92
Epoch 108
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 69.31 (17920/25856)
Train | Batch (196/196) | Top-1: 69.70 (34848/50000)
Regular: 0.20410601794719696
Epoche: 108; regular: 0.20410601794719696: flops 68862592
#Filters: 245, #FLOPs: 14.45M | Top-1: 49.38
Epoch 109
Train | Batch (1/196) | Top-1: 68.36 (175/256)
Train | Batch (101/196) | Top-1: 69.83 (18056/25856)
Train | Batch (196/196) | Top-1: 69.91 (34957/50000)
Regular: 0.20695993304252625
Epoche: 109; regular: 0.20695993304252625: flops 68862592
#Filters: 247, #FLOPs: 14.49M | Top-1: 56.30
Epoch 110
Train | Batch (1/196) | Top-1: 67.97 (174/256)
Train | Batch (101/196) | Top-1: 69.53 (17978/25856)
Train | Batch (196/196) | Top-1: 69.95 (34975/50000)
Regular: 0.20467446744441986
Epoche: 110; regular: 0.20467446744441986: flops 68862592
#Filters: 245, #FLOPs: 13.90M | Top-1: 46.50
Epoch 111
Train | Batch (1/196) | Top-1: 64.06 (164/256)
Train | Batch (101/196) | Top-1: 69.90 (18073/25856)
Train | Batch (196/196) | Top-1: 69.74 (34871/50000)
Regular: 0.21404671669006348
Epoche: 111; regular: 0.21404671669006348: flops 68862592
#Filters: 244, #FLOPs: 14.41M | Top-1: 59.85
Epoch 112
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 69.66 (18011/25856)
Train | Batch (196/196) | Top-1: 69.93 (34967/50000)
Regular: 0.20992527902126312
Epoche: 112; regular: 0.20992527902126312: flops 68862592
#Filters: 248, #FLOPs: 14.71M | Top-1: 55.73
Epoch 113
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 69.84 (18059/25856)
Train | Batch (196/196) | Top-1: 70.03 (35015/50000)
Regular: 0.20663119852542877
Epoche: 113; regular: 0.20663119852542877: flops 68862592
#Filters: 248, #FLOPs: 14.49M | Top-1: 44.33
Epoch 114
Train | Batch (1/196) | Top-1: 67.19 (172/256)
Train | Batch (101/196) | Top-1: 70.14 (18136/25856)
Train | Batch (196/196) | Top-1: 69.95 (34975/50000)
Regular: 0.20963451266288757
Epoche: 114; regular: 0.20963451266288757: flops 68862592
#Filters: 244, #FLOPs: 14.16M | Top-1: 52.91
Epoch 115
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 69.86 (18063/25856)
Train | Batch (196/196) | Top-1: 69.64 (34819/50000)
Regular: 0.2058284878730774
Epoche: 115; regular: 0.2058284878730774: flops 68862592
#Filters: 250, #FLOPs: 14.49M | Top-1: 42.21
Epoch 116
Train | Batch (1/196) | Top-1: 68.36 (175/256)
Train | Batch (101/196) | Top-1: 69.95 (18086/25856)
Train | Batch (196/196) | Top-1: 70.01 (35004/50000)
Regular: 0.20605887472629547
Epoche: 116; regular: 0.20605887472629547: flops 68862592
#Filters: 251, #FLOPs: 14.49M | Top-1: 38.23
Epoch 117
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 69.61 (17999/25856)
Train | Batch (196/196) | Top-1: 69.96 (34979/50000)
Regular: 0.21106550097465515
Epoche: 117; regular: 0.21106550097465515: flops 68862592
#Filters: 246, #FLOPs: 14.23M | Top-1: 53.01
Epoch 118
Train | Batch (1/196) | Top-1: 69.14 (177/256)
Train | Batch (101/196) | Top-1: 69.46 (17960/25856)
Train | Batch (196/196) | Top-1: 69.77 (34883/50000)
Regular: 0.20873808860778809
Epoche: 118; regular: 0.20873808860778809: flops 68862592
#Filters: 250, #FLOPs: 14.54M | Top-1: 48.87
Epoch 119
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 69.56 (17985/25856)
Train | Batch (196/196) | Top-1: 69.65 (34826/50000)
Regular: 0.20739741623401642
Epoche: 119; regular: 0.20739741623401642: flops 68862592
#Filters: 246, #FLOPs: 14.49M | Top-1: 58.72
Epoch 120
Train | Batch (1/196) | Top-1: 67.58 (173/256)
Train | Batch (101/196) | Top-1: 70.32 (18183/25856)
Train | Batch (196/196) | Top-1: 70.08 (35042/50000)
Regular: 0.20652051270008087
Epoche: 120; regular: 0.20652051270008087: flops 68862592
#Filters: 244, #FLOPs: 14.21M | Top-1: 57.40
Epoch 121
Train | Batch (1/196) | Top-1: 70.31 (180/256)
Train | Batch (101/196) | Top-1: 69.77 (18041/25856)
Train | Batch (196/196) | Top-1: 69.81 (34903/50000)
Regular: 0.20351433753967285
Epoche: 121; regular: 0.20351433753967285: flops 68862592
#Filters: 244, #FLOPs: 14.41M | Top-1: 41.30
Epoch 122
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 69.97 (18092/25856)
Train | Batch (196/196) | Top-1: 70.12 (35060/50000)
Regular: 0.207275852560997
Epoche: 122; regular: 0.207275852560997: flops 68862592
#Filters: 249, #FLOPs: 14.67M | Top-1: 54.95
Epoch 123
Train | Batch (1/196) | Top-1: 64.06 (164/256)
Train | Batch (101/196) | Top-1: 70.10 (18126/25856)
Train | Batch (196/196) | Top-1: 70.12 (35062/50000)
Regular: 0.20688141882419586
Epoche: 123; regular: 0.20688141882419586: flops 68862592
#Filters: 249, #FLOPs: 14.34M | Top-1: 53.99
Epoch 124
Train | Batch (1/196) | Top-1: 67.19 (172/256)
Train | Batch (101/196) | Top-1: 69.68 (18017/25856)
Train | Batch (196/196) | Top-1: 70.05 (35026/50000)
Regular: 0.20814105868339539
Epoche: 124; regular: 0.20814105868339539: flops 68862592
#Filters: 248, #FLOPs: 14.23M | Top-1: 52.69
Epoch 125
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 70.04 (18109/25856)
Train | Batch (196/196) | Top-1: 70.15 (35074/50000)
Regular: 0.2047034651041031
Epoche: 125; regular: 0.2047034651041031: flops 68862592
#Filters: 251, #FLOPs: 14.27M | Top-1: 48.97
Epoch 126
Train | Batch (1/196) | Top-1: 68.36 (175/256)
Train | Batch (101/196) | Top-1: 69.79 (18046/25856)
Train | Batch (196/196) | Top-1: 70.08 (35040/50000)
Regular: 0.20301009714603424
Epoche: 126; regular: 0.20301009714603424: flops 68862592
#Filters: 248, #FLOPs: 14.52M | Top-1: 49.06
Epoch 127
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 69.36 (17935/25856)
Train | Batch (196/196) | Top-1: 69.78 (34890/50000)
Regular: 0.20685844123363495
Epoche: 127; regular: 0.20685844123363495: flops 68862592
#Filters: 255, #FLOPs: 14.64M | Top-1: 34.38
Epoch 128
Train | Batch (1/196) | Top-1: 66.80 (171/256)
Train | Batch (101/196) | Top-1: 70.33 (18185/25856)
Train | Batch (196/196) | Top-1: 70.37 (35184/50000)
Regular: 0.20247331261634827
Epoche: 128; regular: 0.20247331261634827: flops 68862592
#Filters: 251, #FLOPs: 14.19M | Top-1: 55.79
Epoch 129
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 70.41 (18204/25856)
Train | Batch (196/196) | Top-1: 70.17 (35083/50000)
Regular: 0.2009734958410263
Epoche: 129; regular: 0.2009734958410263: flops 68862592
#Filters: 248, #FLOPs: 14.41M | Top-1: 45.94
Epoch 130
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 70.36 (18192/25856)
Train | Batch (196/196) | Top-1: 70.41 (35205/50000)
Regular: 0.2048356533050537
Epoche: 130; regular: 0.2048356533050537: flops 68862592
#Filters: 251, #FLOPs: 14.52M | Top-1: 55.20
Epoch 131
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 69.91 (18075/25856)
Train | Batch (196/196) | Top-1: 69.87 (34937/50000)
Regular: 0.20774172246456146
Epoche: 131; regular: 0.20774172246456146: flops 68862592
#Filters: 248, #FLOPs: 14.41M | Top-1: 45.68
Epoch 132
Train | Batch (1/196) | Top-1: 64.06 (164/256)
Train | Batch (101/196) | Top-1: 70.03 (18107/25856)
Train | Batch (196/196) | Top-1: 69.97 (34985/50000)
Regular: 0.20546168088912964
Epoche: 132; regular: 0.20546168088912964: flops 68862592
#Filters: 246, #FLOPs: 14.36M | Top-1: 49.76
Epoch 133
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 69.96 (18089/25856)
Train | Batch (196/196) | Top-1: 70.00 (35001/50000)
Regular: 0.20638306438922882
Epoche: 133; regular: 0.20638306438922882: flops 68862592
#Filters: 252, #FLOPs: 14.12M | Top-1: 35.28
Epoch 134
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 69.76 (18038/25856)
Train | Batch (196/196) | Top-1: 69.90 (34950/50000)
Regular: 0.2053838074207306
Epoche: 134; regular: 0.2053838074207306: flops 68862592
#Filters: 249, #FLOPs: 14.56M | Top-1: 59.74
Epoch 135
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 69.92 (18079/25856)
Train | Batch (196/196) | Top-1: 70.05 (35023/50000)
Regular: 0.2042561024427414
Epoche: 135; regular: 0.2042561024427414: flops 68862592
#Filters: 255, #FLOPs: 14.16M | Top-1: 55.26
Epoch 136
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 70.33 (18185/25856)
Train | Batch (196/196) | Top-1: 70.33 (35164/50000)
Regular: 0.2031712681055069
Epoche: 136; regular: 0.2031712681055069: flops 68862592
#Filters: 246, #FLOPs: 14.38M | Top-1: 37.54
Epoch 137
Train | Batch (1/196) | Top-1: 69.92 (179/256)
Train | Batch (101/196) | Top-1: 70.16 (18140/25856)
Train | Batch (196/196) | Top-1: 70.11 (35055/50000)
Regular: 0.20592911541461945
Epoche: 137; regular: 0.20592911541461945: flops 68862592
#Filters: 248, #FLOPs: 14.41M | Top-1: 47.10
Epoch 138
Train | Batch (1/196) | Top-1: 69.92 (179/256)
Train | Batch (101/196) | Top-1: 70.73 (18288/25856)
Train | Batch (196/196) | Top-1: 70.48 (35241/50000)
Regular: 0.20632891356945038
Epoche: 138; regular: 0.20632891356945038: flops 68862592
#Filters: 254, #FLOPs: 14.41M | Top-1: 56.98
Epoch 139
Train | Batch (1/196) | Top-1: 71.09 (182/256)
Train | Batch (101/196) | Top-1: 70.21 (18153/25856)
Train | Batch (196/196) | Top-1: 70.27 (35134/50000)
Regular: 0.2032899707555771
Epoche: 139; regular: 0.2032899707555771: flops 68862592
#Filters: 246, #FLOPs: 14.34M | Top-1: 53.27
Epoch 140
Train | Batch (1/196) | Top-1: 67.19 (172/256)
Train | Batch (101/196) | Top-1: 70.03 (18108/25856)
Train | Batch (196/196) | Top-1: 70.21 (35106/50000)
Regular: 0.20858535170555115
Epoche: 140; regular: 0.20858535170555115: flops 68862592
#Filters: 248, #FLOPs: 14.41M | Top-1: 50.86
Epoch 141
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 70.46 (18218/25856)
Train | Batch (196/196) | Top-1: 70.40 (35201/50000)
Regular: 0.20414502918720245
Epoche: 141; regular: 0.20414502918720245: flops 68862592
#Filters: 248, #FLOPs: 14.30M | Top-1: 27.95
Epoch 142
Train | Batch (1/196) | Top-1: 64.84 (166/256)
Train | Batch (101/196) | Top-1: 70.51 (18232/25856)
Train | Batch (196/196) | Top-1: 70.21 (35106/50000)
Regular: 0.20794817805290222
Epoche: 142; regular: 0.20794817805290222: flops 68862592
#Filters: 245, #FLOPs: 14.30M | Top-1: 44.54
Epoch 143
Train | Batch (1/196) | Top-1: 62.50 (160/256)
Train | Batch (101/196) | Top-1: 70.30 (18177/25856)
Train | Batch (196/196) | Top-1: 69.90 (34951/50000)
Regular: 0.2050243318080902
Epoche: 143; regular: 0.2050243318080902: flops 68862592
#Filters: 248, #FLOPs: 14.30M | Top-1: 33.56
Epoch 144
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 70.16 (18141/25856)
Train | Batch (196/196) | Top-1: 70.20 (35099/50000)
Regular: 0.20376840233802795
Epoche: 144; regular: 0.20376840233802795: flops 68862592
#Filters: 245, #FLOPs: 13.97M | Top-1: 56.66
Epoch 145
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 70.05 (18113/25856)
Train | Batch (196/196) | Top-1: 70.10 (35052/50000)
Regular: 0.2036585509777069
Epoche: 145; regular: 0.2036585509777069: flops 68862592
#Filters: 264, #FLOPs: 15.70M | Top-1: 59.36
Epoch 146
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 70.35 (18190/25856)
Train | Batch (196/196) | Top-1: 70.50 (35250/50000)
Regular: 0.2010011076927185
Epoche: 146; regular: 0.2010011076927185: flops 68862592
#Filters: 260, #FLOPs: 15.22M | Top-1: 35.46
Epoch 147
Train | Batch (1/196) | Top-1: 63.67 (163/256)
Train | Batch (101/196) | Top-1: 69.97 (18092/25856)
Train | Batch (196/196) | Top-1: 69.91 (34953/50000)
Regular: 0.20470547676086426
Epoche: 147; regular: 0.20470547676086426: flops 68862592
#Filters: 264, #FLOPs: 15.55M | Top-1: 51.93
Epoch 148
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 69.64 (18006/25856)
Train | Batch (196/196) | Top-1: 69.94 (34969/50000)
Regular: 0.20410224795341492
Epoche: 148; regular: 0.20410224795341492: flops 68862592
#Filters: 280, #FLOPs: 16.79M | Top-1: 35.71
Epoch 149
Train | Batch (1/196) | Top-1: 71.09 (182/256)
Train | Batch (101/196) | Top-1: 70.13 (18133/25856)
Train | Batch (196/196) | Top-1: 70.12 (35061/50000)
Regular: 0.20434258878231049
Epoche: 149; regular: 0.20434258878231049: flops 68862592
#Filters: 247, #FLOPs: 14.16M | Top-1: 54.42
Epoch 150
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 69.99 (18096/25856)
Train | Batch (196/196) | Top-1: 70.01 (35004/50000)
Regular: 0.20293915271759033
Epoche: 150; regular: 0.20293915271759033: flops 68862592
#Filters: 263, #FLOPs: 15.81M | Top-1: 48.52
Epoch 151
Train | Batch (1/196) | Top-1: 68.36 (175/256)
Train | Batch (101/196) | Top-1: 70.01 (18102/25856)
Train | Batch (196/196) | Top-1: 70.08 (35040/50000)
Regular: 0.20353782176971436
Epoche: 151; regular: 0.20353782176971436: flops 68862592
#Filters: 262, #FLOPs: 15.66M | Top-1: 63.14
Epoch 152
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 70.06 (18115/25856)
Train | Batch (196/196) | Top-1: 70.15 (35075/50000)
Regular: 0.20468400418758392
Epoche: 152; regular: 0.20468400418758392: flops 68862592
#Filters: 263, #FLOPs: 15.44M | Top-1: 44.09
Epoch 153
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 69.83 (18056/25856)
Train | Batch (196/196) | Top-1: 69.88 (34939/50000)
Regular: 0.19807395339012146
Epoche: 153; regular: 0.19807395339012146: flops 68862592
#Filters: 261, #FLOPs: 15.51M | Top-1: 61.52
Epoch 154
Train | Batch (1/196) | Top-1: 70.31 (180/256)
Train | Batch (101/196) | Top-1: 70.10 (18124/25856)
Train | Batch (196/196) | Top-1: 70.05 (35027/50000)
Regular: 0.2065824717283249
Epoche: 154; regular: 0.2065824717283249: flops 68862592
#Filters: 250, #FLOPs: 14.34M | Top-1: 21.66
Epoch 155
Train | Batch (1/196) | Top-1: 67.19 (172/256)
Train | Batch (101/196) | Top-1: 70.27 (18168/25856)
Train | Batch (196/196) | Top-1: 70.30 (35152/50000)
Regular: 0.20777752995491028
Epoche: 155; regular: 0.20777752995491028: flops 68862592
#Filters: 279, #FLOPs: 16.79M | Top-1: 50.68
Epoch 156
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 70.36 (18193/25856)
Train | Batch (196/196) | Top-1: 70.41 (35204/50000)
Regular: 0.20696626603603363
Epoche: 156; regular: 0.20696626603603363: flops 68862592
#Filters: 277, #FLOPs: 16.79M | Top-1: 37.27
Epoch 157
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 70.28 (18171/25856)
Train | Batch (196/196) | Top-1: 70.14 (35068/50000)
Regular: 0.20643506944179535
Epoche: 157; regular: 0.20643506944179535: flops 68862592
#Filters: 268, #FLOPs: 15.25M | Top-1: 44.06
Epoch 158
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 70.07 (18117/25856)
Train | Batch (196/196) | Top-1: 70.07 (35035/50000)
Regular: 0.2077261507511139
Epoche: 158; regular: 0.2077261507511139: flops 68862592
#Filters: 281, #FLOPs: 16.72M | Top-1: 53.47
Epoch 159
Train | Batch (1/196) | Top-1: 69.92 (179/256)
Train | Batch (101/196) | Top-1: 69.89 (18072/25856)
Train | Batch (196/196) | Top-1: 70.11 (35054/50000)
Regular: 0.20211461186408997
Epoche: 159; regular: 0.20211461186408997: flops 68862592
#Filters: 246, #FLOPs: 14.19M | Top-1: 58.20
Epoch 160
Train | Batch (1/196) | Top-1: 70.70 (181/256)
Train | Batch (101/196) | Top-1: 70.09 (18122/25856)
Train | Batch (196/196) | Top-1: 70.32 (35159/50000)
Regular: 0.20562849938869476
Epoche: 160; regular: 0.20562849938869476: flops 68862592
#Filters: 262, #FLOPs: 15.29M | Top-1: 58.41
Epoch 161
Train | Batch (1/196) | Top-1: 72.66 (186/256)
Train | Batch (101/196) | Top-1: 69.79 (18045/25856)
Train | Batch (196/196) | Top-1: 69.91 (34957/50000)
Regular: 0.2035369873046875
Epoche: 161; regular: 0.2035369873046875: flops 68862592
#Filters: 261, #FLOPs: 15.29M | Top-1: 51.03
Epoch 162
Train | Batch (1/196) | Top-1: 71.88 (184/256)
Train | Batch (101/196) | Top-1: 70.51 (18232/25856)
Train | Batch (196/196) | Top-1: 70.52 (35259/50000)
Regular: 0.2012069821357727
Epoche: 162; regular: 0.2012069821357727: flops 68862592
#Filters: 277, #FLOPs: 16.64M | Top-1: 62.11
Epoch 163
Train | Batch (1/196) | Top-1: 72.27 (185/256)
Train | Batch (101/196) | Top-1: 70.48 (18224/25856)
Train | Batch (196/196) | Top-1: 70.27 (35136/50000)
Regular: 0.2037605345249176
Epoche: 163; regular: 0.2037605345249176: flops 68862592
#Filters: 290, #FLOPs: 18.22M | Top-1: 52.16
Epoch 164
Train | Batch (1/196) | Top-1: 70.31 (180/256)
Train | Batch (101/196) | Top-1: 70.77 (18299/25856)
Train | Batch (196/196) | Top-1: 70.65 (35325/50000)
Regular: 0.20395474135875702
Epoche: 164; regular: 0.20395474135875702: flops 68862592
#Filters: 275, #FLOPs: 16.72M | Top-1: 50.16
Epoch 165
Train | Batch (1/196) | Top-1: 65.62 (168/256)
Train | Batch (101/196) | Top-1: 70.21 (18153/25856)
Train | Batch (196/196) | Top-1: 70.16 (35081/50000)
Regular: 0.20568260550498962
Epoche: 165; regular: 0.20568260550498962: flops 68862592
#Filters: 279, #FLOPs: 16.87M | Top-1: 53.32
Epoch 166
Train | Batch (1/196) | Top-1: 68.75 (176/256)
Train | Batch (101/196) | Top-1: 70.22 (18156/25856)
Train | Batch (196/196) | Top-1: 70.27 (35135/50000)
Regular: 0.20387889444828033
Epoche: 166; regular: 0.20387889444828033: flops 68862592
#Filters: 273, #FLOPs: 16.75M | Top-1: 48.04
Epoch 167
Train | Batch (1/196) | Top-1: 64.06 (164/256)
Train | Batch (101/196) | Top-1: 69.77 (18039/25856)
Train | Batch (196/196) | Top-1: 70.13 (35064/50000)
Regular: 0.21000224351882935
Epoche: 167; regular: 0.21000224351882935: flops 68862592
#Filters: 292, #FLOPs: 17.96M | Top-1: 40.39
Epoch 168
Train | Batch (1/196) | Top-1: 74.61 (191/256)
Train | Batch (101/196) | Top-1: 70.92 (18336/25856)
Train | Batch (196/196) | Top-1: 70.40 (35200/50000)
Regular: 0.20272387564182281
Epoche: 168; regular: 0.20272387564182281: flops 68862592
#Filters: 276, #FLOPs: 17.05M | Top-1: 38.34
Epoch 169
Train | Batch (1/196) | Top-1: 66.02 (169/256)
Train | Batch (101/196) | Top-1: 70.62 (18260/25856)
Train | Batch (196/196) | Top-1: 70.48 (35239/50000)
Regular: 0.20306707918643951
Epoche: 169; regular: 0.20306707918643951: flops 68862592
#Filters: 278, #FLOPs: 17.16M | Top-1: 42.27
Epoch 170
Train | Batch (1/196) | Top-1: 61.72 (158/256)
Train | Batch (101/196) | Top-1: 70.37 (18194/25856)
Train | Batch (196/196) | Top-1: 70.37 (35185/50000)
Regular: 0.20307743549346924
Epoche: 170; regular: 0.20307743549346924: flops 68862592
#Filters: 279, #FLOPs: 17.09M | Top-1: 30.69
Epoch 171
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 70.59 (18253/25856)
Train | Batch (196/196) | Top-1: 70.41 (35204/50000)
Regular: 0.20121991634368896
Epoche: 171; regular: 0.20121991634368896: flops 68862592
#Filters: 275, #FLOPs: 17.09M | Top-1: 50.51
Epoch 172
Train | Batch (1/196) | Top-1: 66.80 (171/256)
Train | Batch (101/196) | Top-1: 70.45 (18215/25856)
Train | Batch (196/196) | Top-1: 70.26 (35130/50000)
Regular: 0.2117372304201126
Epoche: 172; regular: 0.2117372304201126: flops 68862592
#Filters: 268, #FLOPs: 15.92M | Top-1: 36.03
Epoch 173
Train | Batch (1/196) | Top-1: 69.53 (178/256)
Train | Batch (101/196) | Top-1: 70.91 (18334/25856)
Train | Batch (196/196) | Top-1: 70.67 (35335/50000)
Regular: 0.20530931651592255
Epoche: 173; regular: 0.20530931651592255: flops 68862592
#Filters: 274, #FLOPs: 16.68M | Top-1: 54.45
Epoch 174
Train | Batch (1/196) | Top-1: 69.92 (179/256)
Train | Batch (101/196) | Top-1: 70.12 (18130/25856)
Train | Batch (196/196) | Top-1: 70.07 (35034/50000)
Regular: 0.21036681532859802
Epoche: 174; regular: 0.21036681532859802: flops 68862592
#Filters: 281, #FLOPs: 16.87M | Top-1: 43.94
Epoch 175
Train | Batch (1/196) | Top-1: 69.92 (179/256)
Train | Batch (101/196) | Top-1: 70.48 (18223/25856)
Train | Batch (196/196) | Top-1: 70.24 (35119/50000)
Regular: 0.20793545246124268
Epoche: 175; regular: 0.20793545246124268: flops 68862592
#Filters: 296, #FLOPs: 18.33M | Top-1: 33.35
Epoch 176
Train | Batch (1/196) | Top-1: 71.48 (183/256)
Train | Batch (101/196) | Top-1: 70.63 (18263/25856)
Train | Batch (196/196) | Top-1: 70.27 (35135/50000)
Regular: 0.2049289047718048
Epoche: 176; regular: 0.2049289047718048: flops 68862592
#Filters: 295, #FLOPs: 18.15M | Top-1: 23.28
Epoch 177
Train | Batch (1/196) | Top-1: 64.84 (166/256)
Train | Batch (101/196) | Top-1: 70.63 (18262/25856)
Train | Batch (196/196) | Top-1: 70.45 (35224/50000)
Regular: 0.20817381143569946
Epoche: 177; regular: 0.20817381143569946: flops 68862592
#Filters: 281, #FLOPs: 17.01M | Top-1: 55.40
Epoch 178
Train | Batch (1/196) | Top-1: 67.58 (173/256)
Train | Batch (101/196) | Top-1: 70.75 (18294/25856)
Train | Batch (196/196) | Top-1: 70.56 (35278/50000)
Regular: 0.20355115830898285
Epoche: 178; regular: 0.20355115830898285: flops 68862592
#Filters: 275, #FLOPs: 16.83M | Top-1: 38.17
Epoch 179
Train | Batch (1/196) | Top-1: 74.22 (190/256)
Train | Batch (101/196) | Top-1: 70.34 (18188/25856)
Train | Batch (196/196) | Top-1: 70.47 (35236/50000)
Regular: 0.20389926433563232
Epoche: 179; regular: 0.20389926433563232: flops 68862592
#Filters: 275, #FLOPs: 16.87M | Top-1: 37.98
Drin!!
Layers that will be prunned: [(0, 10), (1, 15), (2, 10), (3, 14), (4, 10), (5, 14), (6, 10), (7, 14), (8, 10), (9, 15), (10, 10), (11, 29), (12, 22), (13, 30), (14, 22), (15, 30), (16, 22), (17, 30), (18, 22), (19, 30), (20, 22), (21, 38), (22, 38), (23, 51), (24, 38), (25, 60), (26, 38), (27, 63), (28, 38), (29, 43), (30, 38)]
Prunning filters..
Layer index: 0; Pruned filters: 1
Layer index: 0; Pruned filters: 4
Layer index: 0; Pruned filters: 4
Layer index: 0; Pruned filters: 1
Layer index: 2; Pruned filters: 1
Layer index: 2; Pruned filters: 4
Layer index: 2; Pruned filters: 4
Layer index: 2; Pruned filters: 1
Layer index: 4; Pruned filters: 1
Layer index: 4; Pruned filters: 4
Layer index: 4; Pruned filters: 4
Layer index: 4; Pruned filters: 1
Layer index: 6; Pruned filters: 1
Layer index: 6; Pruned filters: 4
Layer index: 6; Pruned filters: 4
Layer index: 6; Pruned filters: 1
Layer index: 8; Pruned filters: 1
Layer index: 8; Pruned filters: 4
Layer index: 8; Pruned filters: 4
Layer index: 8; Pruned filters: 1
Layer index: 10; Pruned filters: 1
Layer index: 10; Pruned filters: 4
Layer index: 10; Pruned filters: 4
Layer index: 10; Pruned filters: 1
Layer index: 12; Pruned filters: 1
Layer index: 12; Pruned filters: 4
Layer index: 12; Pruned filters: 4
Layer index: 12; Pruned filters: 1
Layer index: 12; Pruned filters: 4
Layer index: 12; Pruned filters: 4
Layer index: 12; Pruned filters: 1
Layer index: 12; Pruned filters: 3
Layer index: 14; Pruned filters: 1
Layer index: 14; Pruned filters: 4
Layer index: 14; Pruned filters: 4
Layer index: 14; Pruned filters: 1
Layer index: 14; Pruned filters: 4
Layer index: 14; Pruned filters: 4
Layer index: 14; Pruned filters: 1
Layer index: 14; Pruned filters: 3
Layer index: 16; Pruned filters: 1
Layer index: 16; Pruned filters: 4
Layer index: 16; Pruned filters: 4
Layer index: 16; Pruned filters: 1
Layer index: 16; Pruned filters: 4
Layer index: 16; Pruned filters: 4
Layer index: 16; Pruned filters: 1
Layer index: 16; Pruned filters: 3
Layer index: 18; Pruned filters: 1
Layer index: 18; Pruned filters: 4
Layer index: 18; Pruned filters: 4
Layer index: 18; Pruned filters: 1
Layer index: 18; Pruned filters: 4
Layer index: 18; Pruned filters: 4
Layer index: 18; Pruned filters: 1
Layer index: 18; Pruned filters: 3
Layer index: 20; Pruned filters: 1
Layer index: 20; Pruned filters: 4
Layer index: 20; Pruned filters: 4
Layer index: 20; Pruned filters: 1
Layer index: 20; Pruned filters: 4
Layer index: 20; Pruned filters: 4
Layer index: 20; Pruned filters: 1
Layer index: 20; Pruned filters: 3
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 4
Layer index: 22; Pruned filters: 4
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 4
Layer index: 22; Pruned filters: 4
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 5
Layer index: 22; Pruned filters: 2
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 2
Layer index: 22; Pruned filters: 3
Layer index: 22; Pruned filters: 2
Layer index: 22; Pruned filters: 3
Layer index: 22; Pruned filters: 1
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 4
Layer index: 24; Pruned filters: 4
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 4
Layer index: 24; Pruned filters: 4
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 5
Layer index: 24; Pruned filters: 2
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 2
Layer index: 24; Pruned filters: 3
Layer index: 24; Pruned filters: 2
Layer index: 24; Pruned filters: 3
Layer index: 24; Pruned filters: 1
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 4
Layer index: 26; Pruned filters: 4
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 4
Layer index: 26; Pruned filters: 4
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 5
Layer index: 26; Pruned filters: 2
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 2
Layer index: 26; Pruned filters: 3
Layer index: 26; Pruned filters: 2
Layer index: 26; Pruned filters: 3
Layer index: 26; Pruned filters: 1
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 4
Layer index: 28; Pruned filters: 4
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 4
Layer index: 28; Pruned filters: 4
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 5
Layer index: 28; Pruned filters: 2
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 2
Layer index: 28; Pruned filters: 3
Layer index: 28; Pruned filters: 2
Layer index: 28; Pruned filters: 3
Layer index: 28; Pruned filters: 1
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 4
Layer index: 30; Pruned filters: 4
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 4
Layer index: 30; Pruned filters: 4
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 5
Layer index: 30; Pruned filters: 2
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 2
Layer index: 30; Pruned filters: 3
Layer index: 30; Pruned filters: 2
Layer index: 30; Pruned filters: 3
Layer index: 30; Pruned filters: 1
Layer index: 1; Pruned filters: 3
Layer index: 1; Pruned filters: 12
Layer index: 3; Pruned filters: 14
Layer index: 5; Pruned filters: 14
Layer index: 7; Pruned filters: 14
Layer index: 9; Pruned filters: 5
Layer index: 9; Pruned filters: 10
Layer index: 11; Pruned filters: 6
Layer index: 11; Pruned filters: 9
Layer index: 11; Pruned filters: 12
Layer index: 11; Pruned filters: 2
Layer index: 13; Pruned filters: 10
Layer index: 13; Pruned filters: 10
Layer index: 13; Pruned filters: 10
Layer index: 15; Pruned filters: 30
Layer index: 17; Pruned filters: 30
Layer index: 19; Pruned filters: 22
Layer index: 19; Pruned filters: 8
Layer index: 21; Pruned filters: 3
Layer index: 21; Pruned filters: 4
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 5
Layer index: 21; Pruned filters: 5
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 2
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 1
Layer index: 21; Pruned filters: 5
Layer index: 23; Pruned filters: 8
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 14
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 5
Layer index: 23; Pruned filters: 12
Layer index: 23; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 4
Layer index: 25; Pruned filters: 33
Layer index: 25; Pruned filters: 10
Layer index: 25; Pruned filters: 11
Layer index: 27; Pruned filters: 25
Layer index: 27; Pruned filters: 38
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 5
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 3
Layer index: 29; Pruned filters: 5
Layer index: 29; Pruned filters: 6
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 3.237M | #Params: 0.030M
model: ResNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(6, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(6, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(6, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(6, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(6, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(6, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(10, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(10, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(10, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(10, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(10, 26, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(26, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(26, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(13, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(26, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(4, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(26, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(26, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(21, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (4): AvgPool2d(kernel_size=8, stride=8, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=26, out_features=10, bias=True)
  )
)
Test acc: 37.980000000000004
