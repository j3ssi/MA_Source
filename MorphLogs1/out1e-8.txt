no display found. Using non-interactive Agg backend
Namespace(batch_size=256, constraint='flops', datapath='/home/jessica.buehler/MA_Source/dataset/data/torch', dataset='torchvision.datasets.CIFAR10', epoch=180, inPlanes=16, large_input=False, lbda=1e-08, logger='MorphLogs1/logMorphNetFlops1e-8', lr=0.1, model='ft_mbnetv2', name='ft_mbnetv2', no_grow=True, prune_away=1, pruner='FilterPrunnerResNet')
I: 0
flops: 68862592
Before Pruning | FLOPs: 68.863M | #Params: 0.462M
Epoch 0
Train | Batch (1/196) | Top-1: 12.11 (31/256)
Train | Batch (101/196) | Top-1: 24.25 (6269/25856)
Train | Batch (196/196) | Top-1: 30.24 (15121/50000)
Regular: 1.7984788417816162
Epoche: 0; regular: 1.7984788417816162: flops 68862592
#Filters: 1099, #FLOPs: 63.41M | Top-1: 23.35
Epoch 1
Train | Batch (1/196) | Top-1: 37.50 (96/256)
Train | Batch (101/196) | Top-1: 47.80 (12359/25856)
Train | Batch (196/196) | Top-1: 52.13 (26065/50000)
Regular: 0.6281261444091797
Epoche: 1; regular: 0.6281261444091797: flops 68862592
#Filters: 1035, #FLOPs: 57.36M | Top-1: 35.78
Epoch 2
Train | Batch (1/196) | Top-1: 57.03 (146/256)
Train | Batch (101/196) | Top-1: 62.47 (16153/25856)
Train | Batch (196/196) | Top-1: 63.76 (31878/50000)
Regular: 0.29552125930786133
Epoche: 2; regular: 0.29552125930786133: flops 68862592
#Filters: 970, #FLOPs: 52.83M | Top-1: 30.51
Epoch 3
Train | Batch (1/196) | Top-1: 62.89 (161/256)
Train | Batch (101/196) | Top-1: 68.17 (17627/25856)
Train | Batch (196/196) | Top-1: 68.97 (34487/50000)
Regular: 0.2133946567773819
Epoche: 3; regular: 0.2133946567773819: flops 68862592
#Filters: 923, #FLOPs: 51.09M | Top-1: 44.55
Epoch 4
Train | Batch (1/196) | Top-1: 66.41 (170/256)
Train | Batch (101/196) | Top-1: 71.17 (18403/25856)
Train | Batch (196/196) | Top-1: 71.84 (35919/50000)
Regular: 0.2036571502685547
Epoche: 4; regular: 0.2036571502685547: flops 68862592
#Filters: 889, #FLOPs: 49.25M | Top-1: 44.38
Epoch 5
Train | Batch (1/196) | Top-1: 73.83 (189/256)
Train | Batch (101/196) | Top-1: 73.74 (19066/25856)
Train | Batch (196/196) | Top-1: 74.11 (37054/50000)
Regular: 0.20035330951213837
Epoche: 5; regular: 0.20035330951213837: flops 68862592
#Filters: 869, #FLOPs: 48.55M | Top-1: 47.80
Epoch 6
Train | Batch (1/196) | Top-1: 73.05 (187/256)
Train | Batch (101/196) | Top-1: 75.04 (19403/25856)
Train | Batch (196/196) | Top-1: 75.17 (37584/50000)
Regular: 0.19733117520809174
Epoche: 6; regular: 0.19733117520809174: flops 68862592
#Filters: 852, #FLOPs: 47.48M | Top-1: 58.33
Epoch 7
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 76.49 (19777/25856)
Train | Batch (196/196) | Top-1: 76.43 (38215/50000)
Regular: 0.19469405710697174
Epoche: 7; regular: 0.19469405710697174: flops 68862592
#Filters: 842, #FLOPs: 47.13M | Top-1: 39.15
Epoch 8
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 76.83 (19866/25856)
Train | Batch (196/196) | Top-1: 76.94 (38469/50000)
Regular: 0.1987588107585907
Epoche: 8; regular: 0.1987588107585907: flops 68862592
#Filters: 835, #FLOPs: 47.19M | Top-1: 48.58
Epoch 9
Train | Batch (1/196) | Top-1: 71.09 (182/256)
Train | Batch (101/196) | Top-1: 77.93 (20150/25856)
Train | Batch (196/196) | Top-1: 77.83 (38915/50000)
Regular: 0.19537925720214844
Epoche: 9; regular: 0.19537925720214844: flops 68862592
#Filters: 820, #FLOPs: 46.03M | Top-1: 57.39
Epoch 10
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 78.59 (20319/25856)
Train | Batch (196/196) | Top-1: 78.24 (39121/50000)
Regular: 0.1926480084657669
Epoche: 10; regular: 0.1926480084657669: flops 68862592
#Filters: 805, #FLOPs: 45.60M | Top-1: 59.94
Epoch 11
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 78.97 (20418/25856)
Train | Batch (196/196) | Top-1: 78.87 (39436/50000)
Regular: 0.1919611394405365
Epoche: 11; regular: 0.1919611394405365: flops 68862592
#Filters: 804, #FLOPs: 45.93M | Top-1: 58.38
Epoch 12
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 79.10 (20451/25856)
Train | Batch (196/196) | Top-1: 79.24 (39621/50000)
Regular: 0.1932666301727295
Epoche: 12; regular: 0.1932666301727295: flops 68862592
#Filters: 799, #FLOPs: 45.56M | Top-1: 61.38
Epoch 13
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 79.73 (20614/25856)
Train | Batch (196/196) | Top-1: 79.70 (39851/50000)
Regular: 0.19012591242790222
Epoche: 13; regular: 0.19012591242790222: flops 68862592
#Filters: 795, #FLOPs: 45.18M | Top-1: 35.88
Epoch 14
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 79.66 (20596/25856)
Train | Batch (196/196) | Top-1: 79.54 (39772/50000)
Regular: 0.19375953078269958
Epoche: 14; regular: 0.19375953078269958: flops 68862592
#Filters: 788, #FLOPs: 45.16M | Top-1: 53.41
Epoch 15
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 80.33 (20769/25856)
Train | Batch (196/196) | Top-1: 80.23 (40116/50000)
Regular: 0.19129407405853271
Epoche: 15; regular: 0.19129407405853271: flops 68862592
#Filters: 786, #FLOPs: 44.97M | Top-1: 65.13
Epoch 16
Train | Batch (1/196) | Top-1: 77.34 (198/256)
Train | Batch (101/196) | Top-1: 80.26 (20753/25856)
Train | Batch (196/196) | Top-1: 80.51 (40253/50000)
Regular: 0.19007840752601624
Epoche: 16; regular: 0.19007840752601624: flops 68862592
#Filters: 779, #FLOPs: 44.79M | Top-1: 55.74
Epoch 17
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 80.53 (20821/25856)
Train | Batch (196/196) | Top-1: 80.25 (40125/50000)
Regular: 0.19040103256702423
Epoche: 17; regular: 0.19040103256702423: flops 68862592
#Filters: 783, #FLOPs: 44.83M | Top-1: 57.82
Epoch 18
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 80.60 (20841/25856)
Train | Batch (196/196) | Top-1: 80.72 (40359/50000)
Regular: 0.19052016735076904
Epoche: 18; regular: 0.19052016735076904: flops 68862592
#Filters: 776, #FLOPs: 44.72M | Top-1: 68.66
Epoch 19
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 80.85 (20904/25856)
Train | Batch (196/196) | Top-1: 81.03 (40514/50000)
Regular: 0.18614548444747925
Epoche: 19; regular: 0.18614548444747925: flops 68862592
#Filters: 771, #FLOPs: 44.85M | Top-1: 67.51
Epoch 20
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 80.91 (20920/25856)
Train | Batch (196/196) | Top-1: 81.04 (40520/50000)
Regular: 0.19196027517318726
Epoche: 20; regular: 0.19196027517318726: flops 68862592
#Filters: 774, #FLOPs: 44.50M | Top-1: 75.36
Epoch 21
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 81.10 (20968/25856)
Train | Batch (196/196) | Top-1: 81.23 (40617/50000)
Regular: 0.18721801042556763
Epoche: 21; regular: 0.18721801042556763: flops 68862592
#Filters: 767, #FLOPs: 44.35M | Top-1: 63.69
Epoch 22
Train | Batch (1/196) | Top-1: 77.73 (199/256)
Train | Batch (101/196) | Top-1: 81.13 (20978/25856)
Train | Batch (196/196) | Top-1: 81.36 (40681/50000)
Regular: 0.18882833421230316
Epoche: 22; regular: 0.18882833421230316: flops 68862592
#Filters: 765, #FLOPs: 44.51M | Top-1: 67.79
Epoch 23
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 81.47 (21064/25856)
Train | Batch (196/196) | Top-1: 81.50 (40752/50000)
Regular: 0.18714040517807007
Epoche: 23; regular: 0.18714040517807007: flops 68862592
#Filters: 769, #FLOPs: 44.83M | Top-1: 70.00
Epoch 24
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 82.05 (21216/25856)
Train | Batch (196/196) | Top-1: 81.73 (40864/50000)
Regular: 0.18621952831745148
Epoche: 24; regular: 0.18621952831745148: flops 68862592
#Filters: 766, #FLOPs: 44.42M | Top-1: 66.53
Epoch 25
Train | Batch (1/196) | Top-1: 78.91 (202/256)
Train | Batch (101/196) | Top-1: 81.95 (21190/25856)
Train | Batch (196/196) | Top-1: 81.75 (40877/50000)
Regular: 0.1870897263288498
Epoche: 25; regular: 0.1870897263288498: flops 68862592
#Filters: 757, #FLOPs: 43.80M | Top-1: 65.18
Epoch 26
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 81.65 (21111/25856)
Train | Batch (196/196) | Top-1: 81.77 (40884/50000)
Regular: 0.18651573359966278
Epoche: 26; regular: 0.18651573359966278: flops 68862592
#Filters: 760, #FLOPs: 44.50M | Top-1: 44.76
Epoch 27
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 81.90 (21177/25856)
Train | Batch (196/196) | Top-1: 82.04 (41022/50000)
Regular: 0.18723532557487488
Epoche: 27; regular: 0.18723532557487488: flops 68862592
#Filters: 753, #FLOPs: 43.57M | Top-1: 74.68
Epoch 28
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 82.06 (21218/25856)
Train | Batch (196/196) | Top-1: 82.16 (41080/50000)
Regular: 0.18492509424686432
Epoche: 28; regular: 0.18492509424686432: flops 68862592
#Filters: 753, #FLOPs: 43.83M | Top-1: 59.48
Epoch 29
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 82.12 (21233/25856)
Train | Batch (196/196) | Top-1: 81.91 (40954/50000)
Regular: 0.18612325191497803
Epoche: 29; regular: 0.18612325191497803: flops 68862592
#Filters: 749, #FLOPs: 44.13M | Top-1: 52.75
Epoch 30
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 82.21 (21257/25856)
Train | Batch (196/196) | Top-1: 82.07 (41034/50000)
Regular: 0.1861904263496399
Epoche: 30; regular: 0.1861904263496399: flops 68862592
#Filters: 746, #FLOPs: 43.83M | Top-1: 67.37
Epoch 31
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 81.81 (21152/25856)
Train | Batch (196/196) | Top-1: 81.97 (40984/50000)
Regular: 0.18910591304302216
Epoche: 31; regular: 0.18910591304302216: flops 68862592
#Filters: 746, #FLOPs: 43.87M | Top-1: 63.35
Epoch 32
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 82.45 (21318/25856)
Train | Batch (196/196) | Top-1: 82.01 (41003/50000)
Regular: 0.1868325024843216
Epoche: 32; regular: 0.1868325024843216: flops 68862592
#Filters: 750, #FLOPs: 43.83M | Top-1: 64.88
Epoch 33
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 82.56 (21347/25856)
Train | Batch (196/196) | Top-1: 82.43 (41216/50000)
Regular: 0.190145805478096
Epoche: 33; regular: 0.190145805478096: flops 68862592
#Filters: 748, #FLOPs: 43.76M | Top-1: 67.94
Epoch 34
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 82.27 (21271/25856)
Train | Batch (196/196) | Top-1: 82.14 (41072/50000)
Regular: 0.186029314994812
Epoche: 34; regular: 0.186029314994812: flops 68862592
#Filters: 746, #FLOPs: 43.76M | Top-1: 66.21
Epoch 35
Train | Batch (1/196) | Top-1: 79.30 (203/256)
Train | Batch (101/196) | Top-1: 82.52 (21337/25856)
Train | Batch (196/196) | Top-1: 82.32 (41161/50000)
Regular: 0.18393750488758087
Epoche: 35; regular: 0.18393750488758087: flops 68862592
#Filters: 739, #FLOPs: 43.39M | Top-1: 60.72
Epoch 36
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 83.01 (21463/25856)
Train | Batch (196/196) | Top-1: 82.55 (41275/50000)
Regular: 0.18309064209461212
Epoche: 36; regular: 0.18309064209461212: flops 68862592
#Filters: 739, #FLOPs: 43.39M | Top-1: 58.21
Epoch 37
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 82.38 (21300/25856)
Train | Batch (196/196) | Top-1: 82.50 (41251/50000)
Regular: 0.18571795523166656
Epoche: 37; regular: 0.18571795523166656: flops 68862592
#Filters: 735, #FLOPs: 43.28M | Top-1: 74.27
Epoch 38
Train | Batch (1/196) | Top-1: 88.67 (227/256)
Train | Batch (101/196) | Top-1: 82.44 (21315/25856)
Train | Batch (196/196) | Top-1: 82.52 (41258/50000)
Regular: 0.18642786145210266
Epoche: 38; regular: 0.18642786145210266: flops 68862592
#Filters: 734, #FLOPs: 43.61M | Top-1: 60.76
Epoch 39
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 82.68 (21378/25856)
Train | Batch (196/196) | Top-1: 82.50 (41248/50000)
Regular: 0.18874098360538483
Epoche: 39; regular: 0.18874098360538483: flops 68862592
#Filters: 740, #FLOPs: 43.80M | Top-1: 71.60
Epoch 40
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 82.86 (21424/25856)
Train | Batch (196/196) | Top-1: 82.48 (41242/50000)
Regular: 0.188315287232399
Epoche: 40; regular: 0.188315287232399: flops 68862592
#Filters: 738, #FLOPs: 43.57M | Top-1: 66.12
Epoch 41
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 82.62 (21363/25856)
Train | Batch (196/196) | Top-1: 82.51 (41255/50000)
Regular: 0.1861250400543213
Epoche: 41; regular: 0.1861250400543213: flops 68862592
#Filters: 731, #FLOPs: 43.21M | Top-1: 58.22
Epoch 42
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 82.72 (21388/25856)
Train | Batch (196/196) | Top-1: 82.63 (41314/50000)
Regular: 0.18778526782989502
Epoche: 42; regular: 0.18778526782989502: flops 68862592
#Filters: 733, #FLOPs: 43.21M | Top-1: 60.29
Epoch 43
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 82.98 (21455/25856)
Train | Batch (196/196) | Top-1: 82.80 (41402/50000)
Regular: 0.18618445098400116
Epoche: 43; regular: 0.18618445098400116: flops 68862592
#Filters: 733, #FLOPs: 42.98M | Top-1: 69.90
Epoch 44
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.14 (21496/25856)
Train | Batch (196/196) | Top-1: 82.98 (41491/50000)
Regular: 0.18438223004341125
Epoche: 44; regular: 0.18438223004341125: flops 68862592
#Filters: 732, #FLOPs: 42.95M | Top-1: 57.26
Epoch 45
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 82.61 (21359/25856)
Train | Batch (196/196) | Top-1: 82.78 (41392/50000)
Regular: 0.18521761894226074
Epoche: 45; regular: 0.18521761894226074: flops 68862592
#Filters: 736, #FLOPs: 43.46M | Top-1: 63.47
Epoch 46
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 82.93 (21443/25856)
Train | Batch (196/196) | Top-1: 82.95 (41475/50000)
Regular: 0.18453755974769592
Epoche: 46; regular: 0.18453755974769592: flops 68862592
#Filters: 728, #FLOPs: 43.13M | Top-1: 75.73
Epoch 47
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 83.13 (21493/25856)
Train | Batch (196/196) | Top-1: 82.96 (41480/50000)
Regular: 0.18504633009433746
Epoche: 47; regular: 0.18504633009433746: flops 68862592
#Filters: 734, #FLOPs: 43.13M | Top-1: 60.66
Epoch 48
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 82.66 (21372/25856)
Train | Batch (196/196) | Top-1: 82.86 (41431/50000)
Regular: 0.18514615297317505
Epoche: 48; regular: 0.18514615297317505: flops 68862592
#Filters: 725, #FLOPs: 42.87M | Top-1: 53.81
Epoch 49
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.11 (21489/25856)
Train | Batch (196/196) | Top-1: 83.18 (41591/50000)
Regular: 0.1849883794784546
Epoche: 49; regular: 0.1849883794784546: flops 68862592
#Filters: 730, #FLOPs: 43.02M | Top-1: 63.45
Epoch 50
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 82.60 (21356/25856)
Train | Batch (196/196) | Top-1: 82.83 (41413/50000)
Regular: 0.18456263840198517
Epoche: 50; regular: 0.18456263840198517: flops 68862592
#Filters: 723, #FLOPs: 42.95M | Top-1: 56.37
Epoch 51
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 82.55 (21343/25856)
Train | Batch (196/196) | Top-1: 82.77 (41385/50000)
Regular: 0.18552938103675842
Epoche: 51; regular: 0.18552938103675842: flops 68862592
#Filters: 723, #FLOPs: 42.65M | Top-1: 62.16
Epoch 52
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 83.10 (21486/25856)
Train | Batch (196/196) | Top-1: 83.10 (41552/50000)
Regular: 0.18539296090602875
Epoche: 52; regular: 0.18539296090602875: flops 68862592
#Filters: 727, #FLOPs: 42.87M | Top-1: 55.03
Epoch 53
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 83.71 (21644/25856)
Train | Batch (196/196) | Top-1: 83.34 (41670/50000)
Regular: 0.18203194439411163
Epoche: 53; regular: 0.18203194439411163: flops 68862592
#Filters: 727, #FLOPs: 42.98M | Top-1: 67.03
Epoch 54
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 82.94 (21445/25856)
Train | Batch (196/196) | Top-1: 83.07 (41535/50000)
Regular: 0.18747776746749878
Epoche: 54; regular: 0.18747776746749878: flops 68862592
#Filters: 729, #FLOPs: 43.35M | Top-1: 63.09
Epoch 55
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 83.44 (21575/25856)
Train | Batch (196/196) | Top-1: 83.17 (41585/50000)
Regular: 0.18765881657600403
Epoche: 55; regular: 0.18765881657600403: flops 68862592
#Filters: 728, #FLOPs: 43.09M | Top-1: 72.06
Epoch 56
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 83.62 (21622/25856)
Train | Batch (196/196) | Top-1: 83.37 (41684/50000)
Regular: 0.1844586879014969
Epoche: 56; regular: 0.1844586879014969: flops 68862592
#Filters: 726, #FLOPs: 42.95M | Top-1: 58.25
Epoch 57
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 83.06 (21476/25856)
Train | Batch (196/196) | Top-1: 83.16 (41578/50000)
Regular: 0.18421445786952972
Epoche: 57; regular: 0.18421445786952972: flops 68862592
#Filters: 728, #FLOPs: 43.61M | Top-1: 63.30
Epoch 58
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 83.32 (21543/25856)
Train | Batch (196/196) | Top-1: 83.26 (41632/50000)
Regular: 0.18347226083278656
Epoche: 58; regular: 0.18347226083278656: flops 68862592
#Filters: 723, #FLOPs: 43.09M | Top-1: 63.16
Epoch 59
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 83.65 (21628/25856)
Train | Batch (196/196) | Top-1: 83.30 (41649/50000)
Regular: 0.1831112504005432
Epoche: 59; regular: 0.1831112504005432: flops 68862592
#Filters: 721, #FLOPs: 43.13M | Top-1: 56.04
Epoch 60
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 83.36 (21553/25856)
Train | Batch (196/196) | Top-1: 83.28 (41641/50000)
Regular: 0.18551157414913177
Epoche: 60; regular: 0.18551157414913177: flops 68862592
#Filters: 723, #FLOPs: 42.95M | Top-1: 62.49
Epoch 61
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 83.49 (21588/25856)
Train | Batch (196/196) | Top-1: 83.28 (41639/50000)
Regular: 0.18433387577533722
Epoche: 61; regular: 0.18433387577533722: flops 68862592
#Filters: 726, #FLOPs: 43.32M | Top-1: 62.32
Epoch 62
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.47 (21581/25856)
Train | Batch (196/196) | Top-1: 83.46 (41731/50000)
Regular: 0.1842191517353058
Epoche: 62; regular: 0.1842191517353058: flops 68862592
#Filters: 729, #FLOPs: 43.24M | Top-1: 72.89
Epoch 63
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 83.26 (21528/25856)
Train | Batch (196/196) | Top-1: 83.33 (41663/50000)
Regular: 0.18178527057170868
Epoche: 63; regular: 0.18178527057170868: flops 68862592
#Filters: 722, #FLOPs: 42.80M | Top-1: 65.07
Epoch 64
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 83.44 (21575/25856)
Train | Batch (196/196) | Top-1: 83.34 (41669/50000)
Regular: 0.18415836989879608
Epoche: 64; regular: 0.18415836989879608: flops 68862592
#Filters: 726, #FLOPs: 43.02M | Top-1: 63.37
Epoch 65
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 83.47 (21583/25856)
Train | Batch (196/196) | Top-1: 83.63 (41813/50000)
Regular: 0.18417587876319885
Epoche: 65; regular: 0.18417587876319885: flops 68862592
#Filters: 724, #FLOPs: 43.28M | Top-1: 72.66
Epoch 66
Train | Batch (1/196) | Top-1: 89.84 (230/256)
Train | Batch (101/196) | Top-1: 83.43 (21571/25856)
Train | Batch (196/196) | Top-1: 83.46 (41728/50000)
Regular: 0.1823979914188385
Epoche: 66; regular: 0.1823979914188385: flops 68862592
#Filters: 707, #FLOPs: 41.52M | Top-1: 36.86
Epoch 67
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 83.23 (21519/25856)
Train | Batch (196/196) | Top-1: 83.42 (41712/50000)
Regular: 0.18203653395175934
Epoche: 67; regular: 0.18203653395175934: flops 68862592
#Filters: 703, #FLOPs: 41.22M | Top-1: 73.76
Epoch 68
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 83.27 (21530/25856)
Train | Batch (196/196) | Top-1: 83.41 (41705/50000)
Regular: 0.18048186600208282
Epoche: 68; regular: 0.18048186600208282: flops 68862592
#Filters: 703, #FLOPs: 41.70M | Top-1: 64.03
Epoch 69
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.59 (21612/25856)
Train | Batch (196/196) | Top-1: 83.62 (41809/50000)
Regular: 0.18062394857406616
Epoche: 69; regular: 0.18062394857406616: flops 68862592
#Filters: 708, #FLOPs: 41.70M | Top-1: 48.84
Epoch 70
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 83.72 (21647/25856)
Train | Batch (196/196) | Top-1: 83.56 (41778/50000)
Regular: 0.17993277311325073
Epoche: 70; regular: 0.17993277311325073: flops 68862592
#Filters: 707, #FLOPs: 41.74M | Top-1: 67.58
Epoch 71
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 83.95 (21707/25856)
Train | Batch (196/196) | Top-1: 83.62 (41811/50000)
Regular: 0.17853742837905884
Epoche: 71; regular: 0.17853742837905884: flops 68862592
#Filters: 703, #FLOPs: 41.65M | Top-1: 59.13
Epoch 72
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 83.15 (21499/25856)
Train | Batch (196/196) | Top-1: 83.32 (41660/50000)
Regular: 0.17884856462478638
Epoche: 72; regular: 0.17884856462478638: flops 68862592
#Filters: 704, #FLOPs: 41.54M | Top-1: 60.42
Epoch 73
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 83.39 (21562/25856)
Train | Batch (196/196) | Top-1: 83.63 (41815/50000)
Regular: 0.18074654042720795
Epoche: 73; regular: 0.18074654042720795: flops 68862592
#Filters: 696, #FLOPs: 41.35M | Top-1: 73.13
Epoch 74
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 83.80 (21668/25856)
Train | Batch (196/196) | Top-1: 83.58 (41791/50000)
Regular: 0.1802382618188858
Epoche: 74; regular: 0.1802382618188858: flops 68862592
#Filters: 701, #FLOPs: 41.30M | Top-1: 53.73
Epoch 75
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 83.81 (21670/25856)
Train | Batch (196/196) | Top-1: 83.62 (41811/50000)
Regular: 0.1818021982908249
Epoche: 75; regular: 0.1818021982908249: flops 68862592
#Filters: 703, #FLOPs: 41.44M | Top-1: 59.59
Epoch 76
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.94 (21704/25856)
Train | Batch (196/196) | Top-1: 83.77 (41883/50000)
Regular: 0.18180477619171143
Epoche: 76; regular: 0.18180477619171143: flops 68862592
#Filters: 700, #FLOPs: 41.22M | Top-1: 65.57
Epoch 77
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.80 (21668/25856)
Train | Batch (196/196) | Top-1: 83.40 (41702/50000)
Regular: 0.18164633214473724
Epoche: 77; regular: 0.18164633214473724: flops 68862592
#Filters: 703, #FLOPs: 41.59M | Top-1: 64.80
Epoch 78
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 84.15 (21757/25856)
Train | Batch (196/196) | Top-1: 83.80 (41902/50000)
Regular: 0.17894160747528076
Epoche: 78; regular: 0.17894160747528076: flops 68862592
#Filters: 704, #FLOPs: 41.56M | Top-1: 60.07
Epoch 79
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 84.10 (21746/25856)
Train | Batch (196/196) | Top-1: 83.59 (41793/50000)
Regular: 0.18017643690109253
Epoche: 79; regular: 0.18017643690109253: flops 68862592
#Filters: 699, #FLOPs: 41.11M | Top-1: 70.47
Epoch 80
Train | Batch (1/196) | Top-1: 87.50 (224/256)
Train | Batch (101/196) | Top-1: 83.81 (21670/25856)
Train | Batch (196/196) | Top-1: 83.83 (41915/50000)
Regular: 0.1780105084180832
Epoche: 80; regular: 0.1780105084180832: flops 68862592
#Filters: 696, #FLOPs: 41.26M | Top-1: 72.88
Epoch 81
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 84.11 (21748/25856)
Train | Batch (196/196) | Top-1: 83.92 (41958/50000)
Regular: 0.18149776756763458
Epoche: 81; regular: 0.18149776756763458: flops 68862592
#Filters: 703, #FLOPs: 41.48M | Top-1: 50.84
Epoch 82
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 83.78 (21663/25856)
Train | Batch (196/196) | Top-1: 83.79 (41894/50000)
Regular: 0.18172310292720795
Epoche: 82; regular: 0.18172310292720795: flops 68862592
#Filters: 701, #FLOPs: 41.52M | Top-1: 71.56
Epoch 83
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 83.98 (21713/25856)
Train | Batch (196/196) | Top-1: 83.84 (41920/50000)
Regular: 0.18123018741607666
Epoche: 83; regular: 0.18123018741607666: flops 68862592
#Filters: 704, #FLOPs: 41.70M | Top-1: 72.13
Epoch 84
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 83.95 (21705/25856)
Train | Batch (196/196) | Top-1: 83.79 (41894/50000)
Regular: 0.17969311773777008
Epoche: 84; regular: 0.17969311773777008: flops 68862592
#Filters: 703, #FLOPs: 41.89M | Top-1: 57.72
Epoch 85
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 83.67 (21635/25856)
Train | Batch (196/196) | Top-1: 83.84 (41922/50000)
Regular: 0.17824412882328033
Epoche: 85; regular: 0.17824412882328033: flops 68862592
#Filters: 697, #FLOPs: 41.41M | Top-1: 33.15
Epoch 86
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 83.77 (21660/25856)
Train | Batch (196/196) | Top-1: 83.45 (41724/50000)
Regular: 0.17862741649150848
Epoche: 86; regular: 0.17862741649150848: flops 68862592
#Filters: 697, #FLOPs: 41.33M | Top-1: 70.36
Epoch 87
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 84.00 (21718/25856)
Train | Batch (196/196) | Top-1: 84.01 (42006/50000)
Regular: 0.17620934545993805
Epoche: 87; regular: 0.17620934545993805: flops 68862592
#Filters: 695, #FLOPs: 41.48M | Top-1: 63.36
Epoch 88
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 83.81 (21671/25856)
Train | Batch (196/196) | Top-1: 83.67 (41834/50000)
Regular: 0.17917080223560333
Epoche: 88; regular: 0.17917080223560333: flops 68862592
#Filters: 695, #FLOPs: 41.41M | Top-1: 74.34
Epoch 89
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 83.85 (21679/25856)
Train | Batch (196/196) | Top-1: 84.04 (42019/50000)
Regular: 0.1789475679397583
Epoche: 89; regular: 0.1789475679397583: flops 68862592
#Filters: 693, #FLOPs: 41.48M | Top-1: 47.47
Epoch 90
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 83.83 (21676/25856)
Train | Batch (196/196) | Top-1: 83.73 (41867/50000)
Regular: 0.17898103594779968
Epoche: 90; regular: 0.17898103594779968: flops 68862592
#Filters: 695, #FLOPs: 40.97M | Top-1: 54.14
Epoch 91
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 84.10 (21746/25856)
Train | Batch (196/196) | Top-1: 83.90 (41950/50000)
Regular: 0.17864151298999786
Epoche: 91; regular: 0.17864151298999786: flops 68862592
#Filters: 692, #FLOPs: 40.87M | Top-1: 65.96
Epoch 92
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 84.03 (21727/25856)
Train | Batch (196/196) | Top-1: 83.90 (41948/50000)
Regular: 0.17754694819450378
Epoche: 92; regular: 0.17754694819450378: flops 68862592
#Filters: 691, #FLOPs: 41.08M | Top-1: 59.28
Epoch 93
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 84.27 (21789/25856)
Train | Batch (196/196) | Top-1: 83.71 (41853/50000)
Regular: 0.17979225516319275
Epoche: 93; regular: 0.17979225516319275: flops 68862592
#Filters: 693, #FLOPs: 41.15M | Top-1: 61.45
Epoch 94
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 84.02 (21724/25856)
Train | Batch (196/196) | Top-1: 83.76 (41879/50000)
Regular: 0.17971833050251007
Epoche: 94; regular: 0.17971833050251007: flops 68862592
#Filters: 692, #FLOPs: 41.33M | Top-1: 57.10
Epoch 95
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 83.55 (21602/25856)
Train | Batch (196/196) | Top-1: 83.62 (41812/50000)
Regular: 0.1847003698348999
Epoche: 95; regular: 0.1847003698348999: flops 68862592
#Filters: 697, #FLOPs: 41.41M | Top-1: 70.07
Epoch 96
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.78 (21662/25856)
Train | Batch (196/196) | Top-1: 83.98 (41991/50000)
Regular: 0.18037432432174683
Epoche: 96; regular: 0.18037432432174683: flops 68862592
#Filters: 692, #FLOPs: 41.30M | Top-1: 67.73
Epoch 97
Train | Batch (1/196) | Top-1: 89.06 (228/256)
Train | Batch (101/196) | Top-1: 83.81 (21669/25856)
Train | Batch (196/196) | Top-1: 83.76 (41882/50000)
Regular: 0.18390300869941711
Epoche: 97; regular: 0.18390300869941711: flops 68862592
#Filters: 693, #FLOPs: 41.11M | Top-1: 73.66
Epoch 98
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 83.68 (21637/25856)
Train | Batch (196/196) | Top-1: 83.68 (41838/50000)
Regular: 0.18499350547790527
Epoche: 98; regular: 0.18499350547790527: flops 68862592
#Filters: 694, #FLOPs: 41.11M | Top-1: 71.81
Epoch 99
Train | Batch (1/196) | Top-1: 76.95 (197/256)
Train | Batch (101/196) | Top-1: 83.69 (21640/25856)
Train | Batch (196/196) | Top-1: 83.89 (41946/50000)
Regular: 0.18314462900161743
Epoche: 99; regular: 0.18314462900161743: flops 68862592
#Filters: 693, #FLOPs: 41.00M | Top-1: 61.47
Epoch 100
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 84.15 (21758/25856)
Train | Batch (196/196) | Top-1: 83.89 (41944/50000)
Regular: 0.18106549978256226
Epoche: 100; regular: 0.18106549978256226: flops 68862592
#Filters: 693, #FLOPs: 40.93M | Top-1: 70.27
Epoch 101
Train | Batch (1/196) | Top-1: 76.17 (195/256)
Train | Batch (101/196) | Top-1: 83.92 (21698/25856)
Train | Batch (196/196) | Top-1: 84.09 (42043/50000)
Regular: 0.17815935611724854
Epoche: 101; regular: 0.17815935611724854: flops 68862592
#Filters: 700, #FLOPs: 41.37M | Top-1: 69.98
Epoch 102
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.67 (21634/25856)
Train | Batch (196/196) | Top-1: 83.63 (41817/50000)
Regular: 0.18137024343013763
Epoche: 102; regular: 0.18137024343013763: flops 68862592
#Filters: 696, #FLOPs: 41.19M | Top-1: 60.19
Epoch 103
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 83.99 (21717/25856)
Train | Batch (196/196) | Top-1: 83.95 (41977/50000)
Regular: 0.17905128002166748
Epoche: 103; regular: 0.17905128002166748: flops 68862592
#Filters: 699, #FLOPs: 41.30M | Top-1: 53.33
Epoch 104
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 83.93 (21702/25856)
Train | Batch (196/196) | Top-1: 83.79 (41897/50000)
Regular: 0.18135060369968414
Epoche: 104; regular: 0.18135060369968414: flops 68862592
#Filters: 696, #FLOPs: 41.08M | Top-1: 62.41
Epoch 105
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 83.90 (21694/25856)
Train | Batch (196/196) | Top-1: 83.76 (41881/50000)
Regular: 0.17790286242961884
Epoche: 105; regular: 0.17790286242961884: flops 68862592
#Filters: 693, #FLOPs: 41.19M | Top-1: 60.02
Epoch 106
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 84.12 (21749/25856)
Train | Batch (196/196) | Top-1: 84.00 (42002/50000)
Regular: 0.1800222545862198
Epoche: 106; regular: 0.1800222545862198: flops 68862592
#Filters: 693, #FLOPs: 41.22M | Top-1: 66.91
Epoch 107
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 84.01 (21722/25856)
Train | Batch (196/196) | Top-1: 84.01 (42003/50000)
Regular: 0.17940674722194672
Epoche: 107; regular: 0.17940674722194672: flops 68862592
#Filters: 691, #FLOPs: 41.04M | Top-1: 66.69
Epoch 108
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.95 (21707/25856)
Train | Batch (196/196) | Top-1: 83.83 (41915/50000)
Regular: 0.17875029146671295
Epoche: 108; regular: 0.17875029146671295: flops 68862592
#Filters: 692, #FLOPs: 41.30M | Top-1: 59.94
Epoch 109
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.04 (21729/25856)
Train | Batch (196/196) | Top-1: 84.15 (42075/50000)
Regular: 0.17865651845932007
Epoche: 109; regular: 0.17865651845932007: flops 68862592
#Filters: 691, #FLOPs: 41.41M | Top-1: 60.09
Epoch 110
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 83.94 (21703/25856)
Train | Batch (196/196) | Top-1: 83.99 (41994/50000)
Regular: 0.17973192036151886
Epoche: 110; regular: 0.17973192036151886: flops 68862592
#Filters: 697, #FLOPs: 41.19M | Top-1: 63.17
Epoch 111
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 83.63 (21624/25856)
Train | Batch (196/196) | Top-1: 83.73 (41866/50000)
Regular: 0.18095925450325012
Epoche: 111; regular: 0.18095925450325012: flops 68862592
#Filters: 697, #FLOPs: 41.33M | Top-1: 62.14
Epoch 112
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 84.18 (21766/25856)
Train | Batch (196/196) | Top-1: 83.90 (41949/50000)
Regular: 0.17910656332969666
Epoche: 112; regular: 0.17910656332969666: flops 68862592
#Filters: 692, #FLOPs: 41.26M | Top-1: 45.71
Epoch 113
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 84.19 (21769/25856)
Train | Batch (196/196) | Top-1: 83.78 (41890/50000)
Regular: 0.17938914895057678
Epoche: 113; regular: 0.17938914895057678: flops 68862592
#Filters: 690, #FLOPs: 41.37M | Top-1: 68.37
Epoch 114
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 83.97 (21712/25856)
Train | Batch (196/196) | Top-1: 83.79 (41895/50000)
Regular: 0.17999953031539917
Epoche: 114; regular: 0.17999953031539917: flops 68862592
#Filters: 691, #FLOPs: 41.04M | Top-1: 66.36
Epoch 115
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 83.66 (21630/25856)
Train | Batch (196/196) | Top-1: 83.72 (41858/50000)
Regular: 0.17940662801265717
Epoche: 115; regular: 0.17940662801265717: flops 68862592
#Filters: 691, #FLOPs: 41.22M | Top-1: 69.96
Epoch 116
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 84.24 (21780/25856)
Train | Batch (196/196) | Top-1: 83.89 (41944/50000)
Regular: 0.1766466349363327
Epoche: 116; regular: 0.1766466349363327: flops 68862592
#Filters: 691, #FLOPs: 41.44M | Top-1: 70.31
Epoch 117
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 83.84 (21678/25856)
Train | Batch (196/196) | Top-1: 83.83 (41916/50000)
Regular: 0.1823655366897583
Epoche: 117; regular: 0.1823655366897583: flops 68862592
#Filters: 692, #FLOPs: 41.30M | Top-1: 61.66
Epoch 118
Train | Batch (1/196) | Top-1: 81.64 (209/256)
Train | Batch (101/196) | Top-1: 83.96 (21709/25856)
Train | Batch (196/196) | Top-1: 83.95 (41977/50000)
Regular: 0.18264389038085938
Epoche: 118; regular: 0.18264389038085938: flops 68862592
#Filters: 689, #FLOPs: 41.22M | Top-1: 78.48
Epoch 119
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 84.12 (21749/25856)
Train | Batch (196/196) | Top-1: 84.05 (42025/50000)
Regular: 0.18081018328666687
Epoche: 119; regular: 0.18081018328666687: flops 68862592
#Filters: 695, #FLOPs: 41.41M | Top-1: 66.48
Epoch 120
Train | Batch (1/196) | Top-1: 78.52 (201/256)
Train | Batch (101/196) | Top-1: 84.05 (21732/25856)
Train | Batch (196/196) | Top-1: 84.00 (41998/50000)
Regular: 0.1801648586988449
Epoche: 120; regular: 0.1801648586988449: flops 68862592
#Filters: 698, #FLOPs: 41.63M | Top-1: 66.46
Epoch 121
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.24 (21781/25856)
Train | Batch (196/196) | Top-1: 84.17 (42084/50000)
Regular: 0.1810334324836731
Epoche: 121; regular: 0.1810334324836731: flops 68862592
#Filters: 700, #FLOPs: 41.70M | Top-1: 67.96
Epoch 122
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 84.14 (21755/25856)
Train | Batch (196/196) | Top-1: 84.00 (41999/50000)
Regular: 0.1790551096200943
Epoche: 122; regular: 0.1790551096200943: flops 68862592
#Filters: 698, #FLOPs: 41.52M | Top-1: 61.87
Epoch 123
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.12 (21749/25856)
Train | Batch (196/196) | Top-1: 83.92 (41961/50000)
Regular: 0.17868609726428986
Epoche: 123; regular: 0.17868609726428986: flops 68862592
#Filters: 692, #FLOPs: 41.11M | Top-1: 61.05
Epoch 124
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 84.02 (21724/25856)
Train | Batch (196/196) | Top-1: 84.09 (42043/50000)
Regular: 0.18006670475006104
Epoche: 124; regular: 0.18006670475006104: flops 68862592
#Filters: 690, #FLOPs: 40.93M | Top-1: 67.33
Epoch 125
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 84.06 (21734/25856)
Train | Batch (196/196) | Top-1: 84.01 (42007/50000)
Regular: 0.1797284334897995
Epoche: 125; regular: 0.1797284334897995: flops 68862592
#Filters: 694, #FLOPs: 41.11M | Top-1: 76.10
Epoch 126
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 84.12 (21750/25856)
Train | Batch (196/196) | Top-1: 84.18 (42091/50000)
Regular: 0.1794491559267044
Epoche: 126; regular: 0.1794491559267044: flops 68862592
#Filters: 696, #FLOPs: 41.63M | Top-1: 51.87
Epoch 127
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 84.16 (21760/25856)
Train | Batch (196/196) | Top-1: 84.04 (42019/50000)
Regular: 0.17776916921138763
Epoche: 127; regular: 0.17776916921138763: flops 68862592
#Filters: 692, #FLOPs: 41.33M | Top-1: 66.85
Epoch 128
Train | Batch (1/196) | Top-1: 86.72 (222/256)
Train | Batch (101/196) | Top-1: 83.80 (21668/25856)
Train | Batch (196/196) | Top-1: 84.15 (42075/50000)
Regular: 0.1802525371313095
Epoche: 128; regular: 0.1802525371313095: flops 68862592
#Filters: 694, #FLOPs: 41.22M | Top-1: 71.32
Epoch 129
Train | Batch (1/196) | Top-1: 80.08 (205/256)
Train | Batch (101/196) | Top-1: 83.93 (21702/25856)
Train | Batch (196/196) | Top-1: 84.21 (42107/50000)
Regular: 0.17947697639465332
Epoche: 129; regular: 0.17947697639465332: flops 68862592
#Filters: 693, #FLOPs: 41.44M | Top-1: 59.06
Epoch 130
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 84.52 (21853/25856)
Train | Batch (196/196) | Top-1: 84.27 (42137/50000)
Regular: 0.1798788160085678
Epoche: 130; regular: 0.1798788160085678: flops 68862592
#Filters: 694, #FLOPs: 41.30M | Top-1: 60.94
Epoch 131
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 84.01 (21721/25856)
Train | Batch (196/196) | Top-1: 84.09 (42044/50000)
Regular: 0.18032310903072357
Epoche: 131; regular: 0.18032310903072357: flops 68862592
#Filters: 692, #FLOPs: 41.22M | Top-1: 61.68
Epoch 132
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 84.01 (21722/25856)
Train | Batch (196/196) | Top-1: 83.98 (41989/50000)
Regular: 0.18113328516483307
Epoche: 132; regular: 0.18113328516483307: flops 68862592
#Filters: 695, #FLOPs: 41.19M | Top-1: 65.11
Epoch 133
Train | Batch (1/196) | Top-1: 85.55 (219/256)
Train | Batch (101/196) | Top-1: 83.99 (21716/25856)
Train | Batch (196/196) | Top-1: 83.85 (41923/50000)
Regular: 0.17889618873596191
Epoche: 133; regular: 0.17889618873596191: flops 68862592
#Filters: 693, #FLOPs: 41.11M | Top-1: 60.47
Epoch 134
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 84.51 (21851/25856)
Train | Batch (196/196) | Top-1: 84.28 (42140/50000)
Regular: 0.17829932272434235
Epoche: 134; regular: 0.17829932272434235: flops 68862592
#Filters: 697, #FLOPs: 41.44M | Top-1: 56.66
Epoch 135
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.08 (21741/25856)
Train | Batch (196/196) | Top-1: 83.93 (41963/50000)
Regular: 0.17684994637966156
Epoche: 135; regular: 0.17684994637966156: flops 68862592
#Filters: 693, #FLOPs: 41.08M | Top-1: 53.81
Epoch 136
Train | Batch (1/196) | Top-1: 88.28 (226/256)
Train | Batch (101/196) | Top-1: 84.06 (21734/25856)
Train | Batch (196/196) | Top-1: 83.99 (41996/50000)
Regular: 0.17864175140857697
Epoche: 136; regular: 0.17864175140857697: flops 68862592
#Filters: 689, #FLOPs: 40.80M | Top-1: 59.61
Epoch 137
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 83.95 (21706/25856)
Train | Batch (196/196) | Top-1: 83.89 (41943/50000)
Regular: 0.17669564485549927
Epoche: 137; regular: 0.17669564485549927: flops 68862592
#Filters: 689, #FLOPs: 41.11M | Top-1: 54.02
Epoch 138
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 84.13 (21753/25856)
Train | Batch (196/196) | Top-1: 84.15 (42076/50000)
Regular: 0.18232910335063934
Epoche: 138; regular: 0.18232910335063934: flops 68862592
#Filters: 692, #FLOPs: 41.04M | Top-1: 72.03
Epoch 139
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 84.12 (21749/25856)
Train | Batch (196/196) | Top-1: 84.06 (42032/50000)
Regular: 0.18105050921440125
Epoche: 139; regular: 0.18105050921440125: flops 68862592
#Filters: 690, #FLOPs: 40.97M | Top-1: 70.05
Epoch 140
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 84.01 (21722/25856)
Train | Batch (196/196) | Top-1: 83.91 (41956/50000)
Regular: 0.17905573546886444
Epoche: 140; regular: 0.17905573546886444: flops 68862592
#Filters: 696, #FLOPs: 41.52M | Top-1: 60.05
Epoch 141
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 84.22 (21775/25856)
Train | Batch (196/196) | Top-1: 83.86 (41929/50000)
Regular: 0.1806764453649521
Epoche: 141; regular: 0.1806764453649521: flops 68862592
#Filters: 689, #FLOPs: 40.97M | Top-1: 72.39
Epoch 142
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 84.20 (21772/25856)
Train | Batch (196/196) | Top-1: 84.22 (42108/50000)
Regular: 0.17795619368553162
Epoche: 142; regular: 0.17795619368553162: flops 68862592
#Filters: 689, #FLOPs: 40.82M | Top-1: 52.51
Epoch 143
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 84.05 (21733/25856)
Train | Batch (196/196) | Top-1: 83.97 (41983/50000)
Regular: 0.1775679737329483
Epoche: 143; regular: 0.1775679737329483: flops 68862592
#Filters: 688, #FLOPs: 41.04M | Top-1: 72.67
Epoch 144
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 83.97 (21711/25856)
Train | Batch (196/196) | Top-1: 83.85 (41927/50000)
Regular: 0.17891253530979156
Epoche: 144; regular: 0.17891253530979156: flops 68862592
#Filters: 691, #FLOPs: 41.08M | Top-1: 66.56
Epoch 145
Train | Batch (1/196) | Top-1: 82.03 (210/256)
Train | Batch (101/196) | Top-1: 84.55 (21862/25856)
Train | Batch (196/196) | Top-1: 84.33 (42163/50000)
Regular: 0.17886675894260406
Epoche: 145; regular: 0.17886675894260406: flops 68862592
#Filters: 691, #FLOPs: 41.00M | Top-1: 56.85
Epoch 146
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 84.03 (21727/25856)
Train | Batch (196/196) | Top-1: 83.89 (41944/50000)
Regular: 0.1771511733531952
Epoche: 146; regular: 0.1771511733531952: flops 68862592
#Filters: 692, #FLOPs: 41.11M | Top-1: 59.87
Epoch 147
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 84.34 (21806/25856)
Train | Batch (196/196) | Top-1: 84.11 (42054/50000)
Regular: 0.17819154262542725
Epoche: 147; regular: 0.17819154262542725: flops 68862592
#Filters: 687, #FLOPs: 41.08M | Top-1: 55.38
Epoch 148
Train | Batch (1/196) | Top-1: 83.59 (214/256)
Train | Batch (101/196) | Top-1: 83.91 (21697/25856)
Train | Batch (196/196) | Top-1: 84.01 (42003/50000)
Regular: 0.1785302460193634
Epoche: 148; regular: 0.1785302460193634: flops 68862592
#Filters: 691, #FLOPs: 40.89M | Top-1: 63.82
Epoch 149
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 83.91 (21697/25856)
Train | Batch (196/196) | Top-1: 84.05 (42027/50000)
Regular: 0.18111900985240936
Epoche: 149; regular: 0.18111900985240936: flops 68862592
#Filters: 695, #FLOPs: 41.56M | Top-1: 76.49
Epoch 150
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 84.46 (21839/25856)
Train | Batch (196/196) | Top-1: 84.21 (42106/50000)
Regular: 0.17854173481464386
Epoche: 150; regular: 0.17854173481464386: flops 68862592
#Filters: 688, #FLOPs: 41.15M | Top-1: 53.48
Epoch 151
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 84.40 (21823/25856)
Train | Batch (196/196) | Top-1: 84.15 (42075/50000)
Regular: 0.17834129929542542
Epoche: 151; regular: 0.17834129929542542: flops 68862592
#Filters: 694, #FLOPs: 41.37M | Top-1: 56.48
Epoch 152
Train | Batch (1/196) | Top-1: 78.12 (200/256)
Train | Batch (101/196) | Top-1: 84.20 (21771/25856)
Train | Batch (196/196) | Top-1: 84.09 (42047/50000)
Regular: 0.17899447679519653
Epoche: 152; regular: 0.17899447679519653: flops 68862592
#Filters: 695, #FLOPs: 41.74M | Top-1: 70.22
Epoch 153
Train | Batch (1/196) | Top-1: 84.38 (216/256)
Train | Batch (101/196) | Top-1: 84.07 (21738/25856)
Train | Batch (196/196) | Top-1: 84.24 (42121/50000)
Regular: 0.17688891291618347
Epoche: 153; regular: 0.17688891291618347: flops 68862592
#Filters: 693, #FLOPs: 41.81M | Top-1: 60.21
Epoch 154
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 84.44 (21832/25856)
Train | Batch (196/196) | Top-1: 84.29 (42144/50000)
Regular: 0.17633216083049774
Epoche: 154; regular: 0.17633216083049774: flops 68862592
#Filters: 690, #FLOPs: 41.48M | Top-1: 63.39
Epoch 155
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 84.22 (21775/25856)
Train | Batch (196/196) | Top-1: 84.01 (42004/50000)
Regular: 0.17932139337062836
Epoche: 155; regular: 0.17932139337062836: flops 68862592
#Filters: 698, #FLOPs: 41.92M | Top-1: 74.04
Epoch 156
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 84.71 (21902/25856)
Train | Batch (196/196) | Top-1: 84.21 (42103/50000)
Regular: 0.1799948364496231
Epoche: 156; regular: 0.1799948364496231: flops 68862592
#Filters: 696, #FLOPs: 42.07M | Top-1: 68.79
Epoch 157
Train | Batch (1/196) | Top-1: 80.47 (206/256)
Train | Batch (101/196) | Top-1: 84.29 (21795/25856)
Train | Batch (196/196) | Top-1: 84.08 (42042/50000)
Regular: 0.18176937103271484
Epoche: 157; regular: 0.18176937103271484: flops 68862592
#Filters: 691, #FLOPs: 41.00M | Top-1: 63.44
Epoch 158
Train | Batch (1/196) | Top-1: 81.25 (208/256)
Train | Batch (101/196) | Top-1: 83.89 (21690/25856)
Train | Batch (196/196) | Top-1: 83.92 (41962/50000)
Regular: 0.18332241475582123
Epoche: 158; regular: 0.18332241475582123: flops 68862592
#Filters: 688, #FLOPs: 40.97M | Top-1: 66.83
Epoch 159
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 84.27 (21789/25856)
Train | Batch (196/196) | Top-1: 84.11 (42055/50000)
Regular: 0.1788257658481598
Epoche: 159; regular: 0.1788257658481598: flops 68862592
#Filters: 691, #FLOPs: 41.04M | Top-1: 64.73
Epoch 160
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 83.93 (21701/25856)
Train | Batch (196/196) | Top-1: 83.98 (41992/50000)
Regular: 0.18215230107307434
Epoche: 160; regular: 0.18215230107307434: flops 68862592
#Filters: 694, #FLOPs: 41.59M | Top-1: 62.62
Epoch 161
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 84.33 (21805/25856)
Train | Batch (196/196) | Top-1: 84.22 (42112/50000)
Regular: 0.17949871718883514
Epoche: 161; regular: 0.17949871718883514: flops 68862592
#Filters: 691, #FLOPs: 41.30M | Top-1: 67.51
Epoch 162
Train | Batch (1/196) | Top-1: 86.33 (221/256)
Train | Batch (101/196) | Top-1: 84.11 (21748/25856)
Train | Batch (196/196) | Top-1: 84.10 (42050/50000)
Regular: 0.17908409237861633
Epoche: 162; regular: 0.17908409237861633: flops 68862592
#Filters: 693, #FLOPs: 41.59M | Top-1: 72.23
Epoch 163
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 84.40 (21822/25856)
Train | Batch (196/196) | Top-1: 84.32 (42160/50000)
Regular: 0.17908070981502533
Epoche: 163; regular: 0.17908070981502533: flops 68862592
#Filters: 703, #FLOPs: 42.00M | Top-1: 49.44
Epoch 164
Train | Batch (1/196) | Top-1: 85.94 (220/256)
Train | Batch (101/196) | Top-1: 84.32 (21801/25856)
Train | Batch (196/196) | Top-1: 84.11 (42054/50000)
Regular: 0.1800551861524582
Epoche: 164; regular: 0.1800551861524582: flops 68862592
#Filters: 693, #FLOPs: 41.59M | Top-1: 69.29
Epoch 165
Train | Batch (1/196) | Top-1: 80.86 (207/256)
Train | Batch (101/196) | Top-1: 83.82 (21672/25856)
Train | Batch (196/196) | Top-1: 84.06 (42029/50000)
Regular: 0.17775189876556396
Epoche: 165; regular: 0.17775189876556396: flops 68862592
#Filters: 691, #FLOPs: 41.15M | Top-1: 64.06
Epoch 166
Train | Batch (1/196) | Top-1: 83.98 (215/256)
Train | Batch (101/196) | Top-1: 84.15 (21759/25856)
Train | Batch (196/196) | Top-1: 84.13 (42064/50000)
Regular: 0.17687556147575378
Epoche: 166; regular: 0.17687556147575378: flops 68862592
#Filters: 696, #FLOPs: 41.67M | Top-1: 73.18
Epoch 167
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 84.31 (21799/25856)
Train | Batch (196/196) | Top-1: 84.07 (42036/50000)
Regular: 0.1789296418428421
Epoche: 167; regular: 0.1789296418428421: flops 68862592
#Filters: 691, #FLOPs: 41.33M | Top-1: 52.35
Epoch 168
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 84.52 (21854/25856)
Train | Batch (196/196) | Top-1: 84.00 (41999/50000)
Regular: 0.17920778691768646
Epoche: 168; regular: 0.17920778691768646: flops 68862592
#Filters: 694, #FLOPs: 41.11M | Top-1: 67.56
Epoch 169
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.13 (21753/25856)
Train | Batch (196/196) | Top-1: 84.11 (42055/50000)
Regular: 0.1747131198644638
Epoche: 169; regular: 0.1747131198644638: flops 68862592
#Filters: 690, #FLOPs: 40.86M | Top-1: 57.88
Epoch 170
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 84.11 (21747/25856)
Train | Batch (196/196) | Top-1: 84.30 (42148/50000)
Regular: 0.17521508038043976
Epoche: 170; regular: 0.17521508038043976: flops 68862592
#Filters: 687, #FLOPs: 40.86M | Top-1: 75.06
Epoch 171
Train | Batch (1/196) | Top-1: 87.11 (223/256)
Train | Batch (101/196) | Top-1: 83.84 (21678/25856)
Train | Batch (196/196) | Top-1: 83.93 (41967/50000)
Regular: 0.1764453649520874
Epoche: 171; regular: 0.1764453649520874: flops 68862592
#Filters: 689, #FLOPs: 41.08M | Top-1: 63.06
Epoch 172
Train | Batch (1/196) | Top-1: 90.62 (232/256)
Train | Batch (101/196) | Top-1: 84.18 (21766/25856)
Train | Batch (196/196) | Top-1: 84.23 (42116/50000)
Regular: 0.17779120802879333
Epoche: 172; regular: 0.17779120802879333: flops 68862592
#Filters: 689, #FLOPs: 41.11M | Top-1: 51.59
Epoch 173
Train | Batch (1/196) | Top-1: 82.81 (212/256)
Train | Batch (101/196) | Top-1: 84.39 (21819/25856)
Train | Batch (196/196) | Top-1: 84.17 (42087/50000)
Regular: 0.1781507283449173
Epoche: 173; regular: 0.1781507283449173: flops 68862592
#Filters: 695, #FLOPs: 41.26M | Top-1: 67.71
Epoch 174
Train | Batch (1/196) | Top-1: 89.45 (229/256)
Train | Batch (101/196) | Top-1: 84.12 (21749/25856)
Train | Batch (196/196) | Top-1: 84.25 (42123/50000)
Regular: 0.1743582934141159
Epoche: 174; regular: 0.1743582934141159: flops 68862592
#Filters: 688, #FLOPs: 41.22M | Top-1: 65.47
Epoch 175
Train | Batch (1/196) | Top-1: 82.42 (211/256)
Train | Batch (101/196) | Top-1: 83.96 (21709/25856)
Train | Batch (196/196) | Top-1: 84.06 (42029/50000)
Regular: 0.17745406925678253
Epoche: 175; regular: 0.17745406925678253: flops 68862592
#Filters: 690, #FLOPs: 40.89M | Top-1: 67.73
Epoch 176
Train | Batch (1/196) | Top-1: 87.89 (225/256)
Train | Batch (101/196) | Top-1: 84.53 (21855/25856)
Train | Batch (196/196) | Top-1: 84.27 (42137/50000)
Regular: 0.17655664682388306
Epoche: 176; regular: 0.17655664682388306: flops 68862592
#Filters: 686, #FLOPs: 41.22M | Top-1: 57.81
Epoch 177
Train | Batch (1/196) | Top-1: 83.20 (213/256)
Train | Batch (101/196) | Top-1: 84.29 (21793/25856)
Train | Batch (196/196) | Top-1: 84.16 (42082/50000)
Regular: 0.17459242045879364
Epoche: 177; regular: 0.17459242045879364: flops 68862592
#Filters: 687, #FLOPs: 40.93M | Top-1: 71.56
Epoch 178
Train | Batch (1/196) | Top-1: 85.16 (218/256)
Train | Batch (101/196) | Top-1: 84.55 (21861/25856)
Train | Batch (196/196) | Top-1: 84.48 (42239/50000)
Regular: 0.176217719912529
Epoche: 178; regular: 0.176217719912529: flops 68862592
#Filters: 689, #FLOPs: 41.11M | Top-1: 64.06
Epoch 179
Train | Batch (1/196) | Top-1: 84.77 (217/256)
Train | Batch (101/196) | Top-1: 84.39 (21819/25856)
Train | Batch (196/196) | Top-1: 84.28 (42138/50000)
Regular: 0.17721813917160034
Epoche: 179; regular: 0.17721813917160034: flops 68862592
#Filters: 689, #FLOPs: 40.86M | Top-1: 60.25
Drin!!
Layers that will be prunned: [(0, 1), (1, 14), (2, 1), (3, 15), (4, 1), (5, 15), (6, 1), (7, 15), (8, 1), (9, 14), (10, 1), (11, 3), (12, 1), (13, 28), (14, 1), (15, 31), (16, 1), (17, 31), (18, 1), (19, 31), (20, 1), (22, 6), (23, 33), (24, 6), (25, 38), (26, 6), (27, 62), (28, 6), (29, 29), (30, 6)]
Prunning filters..
Layer index: 0; Pruned filters: 1
Layer index: 2; Pruned filters: 1
Layer index: 4; Pruned filters: 1
Layer index: 6; Pruned filters: 1
Layer index: 8; Pruned filters: 1
Layer index: 10; Pruned filters: 1
Layer index: 12; Pruned filters: 1
Layer index: 14; Pruned filters: 1
Layer index: 16; Pruned filters: 1
Layer index: 18; Pruned filters: 1
Layer index: 20; Pruned filters: 1
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 1
Layer index: 22; Pruned filters: 2
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 1
Layer index: 24; Pruned filters: 2
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 1
Layer index: 26; Pruned filters: 2
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 1
Layer index: 28; Pruned filters: 2
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 1
Layer index: 30; Pruned filters: 2
Layer index: 1; Pruned filters: 3
Layer index: 1; Pruned filters: 11
Layer index: 3; Pruned filters: 13
Layer index: 3; Pruned filters: 2
Layer index: 5; Pruned filters: 15
Layer index: 7; Pruned filters: 10
Layer index: 7; Pruned filters: 5
Layer index: 9; Pruned filters: 10
Layer index: 9; Pruned filters: 4
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 11; Pruned filters: 1
Layer index: 13; Pruned filters: 3
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 1
Layer index: 13; Pruned filters: 23
Layer index: 15; Pruned filters: 3
Layer index: 15; Pruned filters: 28
Layer index: 17; Pruned filters: 12
Layer index: 17; Pruned filters: 19
Layer index: 19; Pruned filters: 31
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 8
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 3
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 2
Layer index: 23; Pruned filters: 1
Layer index: 23; Pruned filters: 4
Layer index: 23; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 4
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 4
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 5
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 1
Layer index: 25; Pruned filters: 2
Layer index: 25; Pruned filters: 5
Layer index: 27; Pruned filters: 62
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 3
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 3
Layer index: 29; Pruned filters: 6
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 2
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 4
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Layer index: 29; Pruned filters: 1
Target (flops): 68.863M
After Pruning | FLOPs: 15.986M | #Params: 0.168M
model: ResNet(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(15, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(15, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(15, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(15, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(15, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(15, 29, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(29, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(31, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(4, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(31, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(31, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(31, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(1, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(31, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): DownsampleA(
          (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
        )
      )
      (1): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(58, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(31, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (2): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(58, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(26, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (3): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(58, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(2, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
      (4): BasicBlock(
        (conv): Sequential(
          (0): Conv2d(58, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(35, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (shortcut): Sequential()
      )
    )
    (4): AvgPool2d(kernel_size=8, stride=8, padding=0)
  )
  (classifier): Sequential(
    (0): Linear(in_features=58, out_features=10, bias=True)
  )
)
Test acc: 60.25
