no display found. Using non-interactive Agg backend
[3, 3, 3]
[4, 8, 16]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 120; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/room2x2/model.nn; checkpoint: ./output/experimente4/room222; saveModell: True; LR: 0.1
random number: 3428
Files already downloaded and verified
==> Resuming from checkpoint..
Start epoch: 121
First Lr: 0.1
Startepoche: 121
deeper epoch: 121
Epoche: [121/240]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.
  warnings.warn("Setting attributes on ParameterList is not supported.")
Epoch: [121][0/196]	Time 0.236 (0.236)	Data 0.554 (0.554)	Loss 0.7802 (0.7802)	Acc@1 71.094 (71.094)	Acc@5 97.266 (97.266)
Epoch: [121][64/196]	Time 0.093 (0.102)	Data 0.000 (0.009)	Loss 0.7721 (0.7726)	Acc@1 74.609 (72.812)	Acc@5 97.656 (97.837)
Epoch: [121][128/196]	Time 0.079 (0.098)	Data 0.000 (0.005)	Loss 0.8196 (0.7774)	Acc@1 72.656 (72.880)	Acc@5 97.266 (97.889)
Epoch: [121][192/196]	Time 0.077 (0.096)	Data 0.000 (0.003)	Loss 0.7762 (0.7828)	Acc@1 73.438 (72.674)	Acc@5 98.047 (97.847)
after train
test acc: 73.01


now deeper1
i: 3
j: 0; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 3; i0=: 4; i1=: 4
i: 4
j: 0; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 4; i0=: 4; i1=: 4
i: 5
j: 0; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 5; i0=: 4; i1=: 4
i: 6
j: 0; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 6; i0=: 4; i1=: 4
i: 7
j: 0; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))]
seq[j]: Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
j: 1; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 5; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
i: 7; i0=: 8; i1=: 4
skip: 8
i: 9
j: 0; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 9; i0=: 8; i1=: 8
i: 10
j: 0; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 10; i0=: 8; i1=: 8
i: 11
j: 0; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 11; i0=: 8; i1=: 8
i: 12
j: 0; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))]
seq[j]: Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
j: 1; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 5; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
i: 12; i0=: 16; i1=: 8
skip: 13
i: 14
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 14; i0=: 16; i1=: 16
i: 15
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 15; i0=: 16; i1=: 16
i: 16
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 16; i0=: 16; i1=: 16
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (8): Sequential(
      (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (13): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): AdaptiveAvgPool2d(output_size=(1, 1))
    (18): Linear(in_features=16, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
)
Nums: [[1, 1, 1, 1], [2, 1, 1, 1], [2, 1, 1, 1]]
num: 10; numofstages: 3, listofBlocks: [3, 3, 3], layers in blocj: 1
model.para: <generator object Module.named_parameters at 0x7f8a963706d0>
Epoche: [122/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [122][0/196]	Time 0.279 (0.279)	Data 0.596 (0.596)	Loss 2.7876 (2.7876)	Acc@1 8.203 (8.203)	Acc@5 50.000 (50.000)
Epoch: [122][64/196]	Time 0.120 (0.160)	Data 0.000 (0.010)	Loss 2.1402 (2.2675)	Acc@1 20.703 (14.988)	Acc@5 68.750 (62.716)
Epoch: [122][128/196]	Time 0.159 (0.160)	Data 0.000 (0.005)	Loss 2.1317 (2.2136)	Acc@1 15.234 (16.427)	Acc@5 69.922 (66.279)
Epoch: [122][192/196]	Time 0.156 (0.158)	Data 0.000 (0.003)	Loss 2.1145 (2.1851)	Acc@1 19.922 (17.270)	Acc@5 76.172 (68.313)
after train
test acc: 10.67
Epoche: [123/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [123][0/196]	Time 0.211 (0.211)	Data 0.514 (0.514)	Loss 2.1175 (2.1175)	Acc@1 19.922 (19.922)	Acc@5 75.000 (75.000)
Epoch: [123][64/196]	Time 0.129 (0.160)	Data 0.000 (0.009)	Loss 2.0617 (2.0744)	Acc@1 20.312 (20.024)	Acc@5 74.219 (74.303)
Epoch: [123][128/196]	Time 0.243 (0.160)	Data 0.000 (0.005)	Loss 1.9574 (2.0451)	Acc@1 24.609 (20.615)	Acc@5 79.297 (75.878)
Epoch: [123][192/196]	Time 0.316 (0.176)	Data 0.000 (0.003)	Loss 2.0107 (2.0206)	Acc@1 21.484 (21.268)	Acc@5 80.078 (77.334)
after train
test acc: 14.49
Epoche: [124/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [124][0/196]	Time 0.381 (0.381)	Data 0.754 (0.754)	Loss 1.9533 (1.9533)	Acc@1 18.359 (18.359)	Acc@5 82.031 (82.031)
Epoch: [124][64/196]	Time 0.202 (0.272)	Data 0.000 (0.012)	Loss 1.9804 (1.9333)	Acc@1 19.922 (22.596)	Acc@5 80.859 (82.085)
Epoch: [124][128/196]	Time 0.175 (0.268)	Data 0.000 (0.007)	Loss 1.8822 (1.9190)	Acc@1 21.094 (23.728)	Acc@5 82.812 (82.534)
Epoch: [124][192/196]	Time 0.348 (0.270)	Data 0.000 (0.005)	Loss 1.8287 (1.9048)	Acc@1 28.125 (24.201)	Acc@5 82.812 (82.893)
after train
test acc: 20.82
Epoche: [125/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [125][0/196]	Time 0.320 (0.320)	Data 0.642 (0.642)	Loss 1.8373 (1.8373)	Acc@1 25.781 (25.781)	Acc@5 86.719 (86.719)
Epoch: [125][64/196]	Time 0.359 (0.289)	Data 0.005 (0.011)	Loss 1.7800 (1.8311)	Acc@1 26.953 (27.933)	Acc@5 87.109 (84.531)
Epoch: [125][128/196]	Time 0.374 (0.284)	Data 0.004 (0.006)	Loss 1.7748 (1.8260)	Acc@1 29.688 (27.768)	Acc@5 85.938 (84.820)
Epoch: [125][192/196]	Time 0.223 (0.285)	Data 0.000 (0.004)	Loss 1.7775 (1.8167)	Acc@1 29.688 (27.973)	Acc@5 87.891 (85.085)
after train
test acc: 17.94
Epoche: [126/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [126][0/196]	Time 0.366 (0.366)	Data 0.630 (0.630)	Loss 1.7554 (1.7554)	Acc@1 29.297 (29.297)	Acc@5 85.156 (85.156)
Epoch: [126][64/196]	Time 0.243 (0.267)	Data 0.002 (0.011)	Loss 1.7892 (1.7798)	Acc@1 24.219 (28.582)	Acc@5 84.766 (85.709)
Epoch: [126][128/196]	Time 0.223 (0.268)	Data 0.000 (0.006)	Loss 1.6915 (1.7685)	Acc@1 33.984 (29.073)	Acc@5 86.328 (86.286)
Epoch: [126][192/196]	Time 0.299 (0.271)	Data 0.000 (0.004)	Loss 1.7750 (1.7619)	Acc@1 32.812 (29.283)	Acc@5 86.719 (86.488)
after train
test acc: 22.58
Epoche: [127/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [127][0/196]	Time 0.236 (0.236)	Data 0.700 (0.700)	Loss 1.7340 (1.7340)	Acc@1 32.031 (32.031)	Acc@5 85.547 (85.547)
Epoch: [127][64/196]	Time 0.214 (0.283)	Data 0.000 (0.012)	Loss 1.6968 (1.7141)	Acc@1 34.375 (31.827)	Acc@5 89.453 (87.873)
Epoch: [127][128/196]	Time 0.365 (0.283)	Data 0.000 (0.006)	Loss 1.6424 (1.7099)	Acc@1 34.375 (32.334)	Acc@5 87.500 (88.103)
Epoch: [127][192/196]	Time 0.220 (0.281)	Data 0.000 (0.004)	Loss 1.7382 (1.7021)	Acc@1 31.641 (32.661)	Acc@5 84.375 (88.162)
after train
test acc: 28.92
Epoche: [128/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [128][0/196]	Time 0.275 (0.275)	Data 0.687 (0.687)	Loss 1.6943 (1.6943)	Acc@1 32.812 (32.812)	Acc@5 87.500 (87.500)
Epoch: [128][64/196]	Time 0.321 (0.265)	Data 0.000 (0.011)	Loss 1.6715 (1.6643)	Acc@1 32.812 (34.772)	Acc@5 92.188 (89.345)
Epoch: [128][128/196]	Time 0.254 (0.276)	Data 0.000 (0.006)	Loss 1.6560 (1.6593)	Acc@1 40.625 (35.277)	Acc@5 91.797 (89.380)
Epoch: [128][192/196]	Time 0.240 (0.276)	Data 0.000 (0.005)	Loss 1.5477 (1.6483)	Acc@1 35.547 (35.757)	Acc@5 91.406 (89.542)
after train
test acc: 37.82
Epoche: [129/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [129][0/196]	Time 0.297 (0.297)	Data 0.701 (0.701)	Loss 1.6589 (1.6589)	Acc@1 34.375 (34.375)	Acc@5 88.672 (88.672)
Epoch: [129][64/196]	Time 0.210 (0.284)	Data 0.000 (0.012)	Loss 1.5883 (1.6142)	Acc@1 35.156 (37.608)	Acc@5 88.281 (89.970)
Epoch: [129][128/196]	Time 0.262 (0.273)	Data 0.000 (0.007)	Loss 1.5139 (1.6002)	Acc@1 42.188 (38.466)	Acc@5 92.188 (90.256)
Epoch: [129][192/196]	Time 0.261 (0.276)	Data 0.000 (0.005)	Loss 1.4490 (1.5906)	Acc@1 44.531 (38.805)	Acc@5 90.625 (90.372)
after train
test acc: 22.99
Epoche: [130/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [130][0/196]	Time 0.321 (0.321)	Data 0.723 (0.723)	Loss 1.6147 (1.6147)	Acc@1 37.109 (37.109)	Acc@5 87.500 (87.500)
Epoch: [130][64/196]	Time 0.199 (0.286)	Data 0.000 (0.013)	Loss 1.4876 (1.5555)	Acc@1 42.969 (40.944)	Acc@5 90.625 (91.046)
Epoch: [130][128/196]	Time 0.185 (0.295)	Data 0.000 (0.007)	Loss 1.3479 (1.5489)	Acc@1 43.750 (41.137)	Acc@5 95.312 (91.261)
Epoch: [130][192/196]	Time 0.284 (0.292)	Data 0.000 (0.005)	Loss 1.4843 (1.5396)	Acc@1 41.797 (41.433)	Acc@5 92.188 (91.341)
after train
test acc: 31.9
Epoche: [131/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [131][0/196]	Time 0.346 (0.346)	Data 0.610 (0.610)	Loss 1.5115 (1.5115)	Acc@1 42.578 (42.578)	Acc@5 91.406 (91.406)
Epoch: [131][64/196]	Time 0.160 (0.280)	Data 0.000 (0.010)	Loss 1.4865 (1.5230)	Acc@1 42.188 (42.608)	Acc@5 93.750 (91.358)
Epoch: [131][128/196]	Time 0.293 (0.273)	Data 0.000 (0.005)	Loss 1.4522 (1.5119)	Acc@1 46.875 (43.023)	Acc@5 92.188 (91.591)
Epoch: [131][192/196]	Time 0.364 (0.276)	Data 0.000 (0.004)	Loss 1.4004 (1.4902)	Acc@1 47.266 (43.734)	Acc@5 94.922 (91.910)
after train
test acc: 30.21
Epoche: [132/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [132][0/196]	Time 0.333 (0.333)	Data 0.713 (0.713)	Loss 1.4502 (1.4502)	Acc@1 42.188 (42.188)	Acc@5 93.359 (93.359)
Epoch: [132][64/196]	Time 0.388 (0.278)	Data 0.000 (0.012)	Loss 1.5640 (1.4291)	Acc@1 41.016 (46.064)	Acc@5 94.141 (93.281)
Epoch: [132][128/196]	Time 0.227 (0.273)	Data 0.000 (0.006)	Loss 1.4329 (1.4307)	Acc@1 46.875 (46.391)	Acc@5 92.578 (93.105)
Epoch: [132][192/196]	Time 0.503 (0.277)	Data 0.000 (0.004)	Loss 1.4069 (1.4189)	Acc@1 44.141 (46.780)	Acc@5 92.188 (93.280)
after train
test acc: 41.18
Epoche: [133/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [133][0/196]	Time 0.329 (0.329)	Data 0.664 (0.664)	Loss 1.3527 (1.3527)	Acc@1 48.828 (48.828)	Acc@5 96.094 (96.094)
Epoch: [133][64/196]	Time 0.319 (0.282)	Data 0.000 (0.011)	Loss 1.5000 (1.3844)	Acc@1 45.703 (49.026)	Acc@5 91.016 (93.522)
Epoch: [133][128/196]	Time 0.298 (0.282)	Data 0.000 (0.006)	Loss 1.3915 (1.3778)	Acc@1 50.391 (49.022)	Acc@5 92.578 (93.717)
Epoch: [133][192/196]	Time 0.255 (0.281)	Data 0.000 (0.004)	Loss 1.3208 (1.3830)	Acc@1 49.219 (48.861)	Acc@5 92.969 (93.651)
after train
test acc: 34.04
Epoche: [134/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [134][0/196]	Time 0.430 (0.430)	Data 0.742 (0.742)	Loss 1.3216 (1.3216)	Acc@1 50.000 (50.000)	Acc@5 95.312 (95.312)
Epoch: [134][64/196]	Time 0.300 (0.262)	Data 0.000 (0.013)	Loss 1.3829 (1.3448)	Acc@1 46.094 (50.451)	Acc@5 94.141 (93.786)
Epoch: [134][128/196]	Time 0.289 (0.272)	Data 0.000 (0.007)	Loss 1.3676 (1.3447)	Acc@1 50.000 (50.491)	Acc@5 95.312 (93.904)
Epoch: [134][192/196]	Time 0.278 (0.280)	Data 0.000 (0.005)	Loss 1.2794 (1.3421)	Acc@1 50.781 (50.694)	Acc@5 95.312 (93.950)
after train
test acc: 40.28
Epoche: [135/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [135][0/196]	Time 0.311 (0.311)	Data 0.698 (0.698)	Loss 1.3179 (1.3179)	Acc@1 53.906 (53.906)	Acc@5 94.922 (94.922)
Epoch: [135][64/196]	Time 0.271 (0.244)	Data 0.000 (0.011)	Loss 1.3466 (1.3202)	Acc@1 51.562 (51.550)	Acc@5 95.312 (94.243)
Epoch: [135][128/196]	Time 0.432 (0.265)	Data 0.000 (0.006)	Loss 1.3850 (1.3109)	Acc@1 51.562 (51.944)	Acc@5 90.625 (94.286)
Epoch: [135][192/196]	Time 0.146 (0.267)	Data 0.000 (0.004)	Loss 1.2812 (1.3115)	Acc@1 51.172 (51.888)	Acc@5 92.578 (94.230)
after train
test acc: 49.15
Epoche: [136/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [136][0/196]	Time 0.290 (0.290)	Data 0.624 (0.624)	Loss 1.4145 (1.4145)	Acc@1 48.828 (48.828)	Acc@5 94.922 (94.922)
Epoch: [136][64/196]	Time 0.138 (0.258)	Data 0.000 (0.011)	Loss 1.2706 (1.3036)	Acc@1 53.125 (52.482)	Acc@5 95.312 (94.411)
Epoch: [136][128/196]	Time 0.259 (0.273)	Data 0.000 (0.006)	Loss 1.2851 (1.3014)	Acc@1 52.344 (52.468)	Acc@5 94.922 (94.350)
Epoch: [136][192/196]	Time 0.372 (0.276)	Data 0.000 (0.004)	Loss 1.2199 (1.2944)	Acc@1 52.344 (52.682)	Acc@5 96.094 (94.359)
after train
test acc: 29.04
Epoche: [137/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [137][0/196]	Time 0.424 (0.424)	Data 0.694 (0.694)	Loss 1.2471 (1.2471)	Acc@1 52.734 (52.734)	Acc@5 95.312 (95.312)
Epoch: [137][64/196]	Time 0.132 (0.261)	Data 0.000 (0.011)	Loss 1.1820 (1.2689)	Acc@1 59.766 (53.750)	Acc@5 94.141 (94.603)
Epoch: [137][128/196]	Time 0.251 (0.265)	Data 0.000 (0.006)	Loss 1.2547 (1.2727)	Acc@1 52.734 (53.758)	Acc@5 94.531 (94.577)
Epoch: [137][192/196]	Time 0.314 (0.271)	Data 0.000 (0.004)	Loss 1.2022 (1.2682)	Acc@1 52.734 (53.756)	Acc@5 94.531 (94.675)
after train
test acc: 44.29
Epoche: [138/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [138][0/196]	Time 0.323 (0.323)	Data 0.733 (0.733)	Loss 1.3992 (1.3992)	Acc@1 51.562 (51.562)	Acc@5 90.625 (90.625)
Epoch: [138][64/196]	Time 0.199 (0.258)	Data 0.000 (0.013)	Loss 1.3107 (1.2552)	Acc@1 52.344 (54.026)	Acc@5 96.484 (94.718)
Epoch: [138][128/196]	Time 0.205 (0.260)	Data 0.000 (0.007)	Loss 1.1468 (1.2515)	Acc@1 57.031 (54.233)	Acc@5 96.094 (94.855)
Epoch: [138][192/196]	Time 0.265 (0.268)	Data 0.000 (0.005)	Loss 1.2391 (1.2536)	Acc@1 56.641 (54.149)	Acc@5 94.531 (94.798)
after train
test acc: 31.8
Epoche: [139/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [139][0/196]	Time 0.364 (0.364)	Data 0.591 (0.591)	Loss 1.3052 (1.3052)	Acc@1 50.781 (50.781)	Acc@5 95.312 (95.312)
Epoch: [139][64/196]	Time 0.225 (0.258)	Data 0.000 (0.010)	Loss 1.2029 (1.2484)	Acc@1 54.297 (54.044)	Acc@5 94.141 (94.832)
Epoch: [139][128/196]	Time 0.323 (0.265)	Data 0.000 (0.005)	Loss 1.3172 (1.2396)	Acc@1 54.297 (54.681)	Acc@5 94.531 (94.922)
Epoch: [139][192/196]	Time 0.248 (0.272)	Data 0.000 (0.003)	Loss 1.1641 (1.2428)	Acc@1 54.297 (54.702)	Acc@5 96.484 (94.817)
after train
test acc: 47.17
Epoche: [140/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [140][0/196]	Time 0.267 (0.267)	Data 0.520 (0.520)	Loss 1.1980 (1.1980)	Acc@1 54.297 (54.297)	Acc@5 95.312 (95.312)
Epoch: [140][64/196]	Time 0.119 (0.259)	Data 0.000 (0.008)	Loss 1.2642 (1.2290)	Acc@1 51.172 (54.898)	Acc@5 94.922 (94.892)
Epoch: [140][128/196]	Time 0.274 (0.273)	Data 0.000 (0.004)	Loss 1.1050 (1.2277)	Acc@1 56.641 (55.075)	Acc@5 96.094 (94.946)
Epoch: [140][192/196]	Time 0.314 (0.280)	Data 0.000 (0.003)	Loss 1.1046 (1.2272)	Acc@1 60.547 (54.979)	Acc@5 96.484 (95.043)
after train
test acc: 25.76
Epoche: [141/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [141][0/196]	Time 0.325 (0.325)	Data 0.482 (0.482)	Loss 1.2438 (1.2438)	Acc@1 53.125 (53.125)	Acc@5 94.531 (94.531)
Epoch: [141][64/196]	Time 0.274 (0.253)	Data 0.000 (0.008)	Loss 1.1544 (1.2137)	Acc@1 58.594 (55.583)	Acc@5 94.922 (95.144)
Epoch: [141][128/196]	Time 0.310 (0.272)	Data 0.000 (0.004)	Loss 1.2078 (1.2176)	Acc@1 55.469 (55.735)	Acc@5 93.750 (95.040)
Epoch: [141][192/196]	Time 0.309 (0.282)	Data 0.000 (0.003)	Loss 1.2015 (1.2157)	Acc@1 56.250 (55.922)	Acc@5 94.531 (95.076)
after train
test acc: 43.29
Epoche: [142/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [142][0/196]	Time 0.426 (0.426)	Data 0.502 (0.502)	Loss 1.0076 (1.0076)	Acc@1 61.719 (61.719)	Acc@5 98.047 (98.047)
Epoch: [142][64/196]	Time 0.293 (0.262)	Data 0.000 (0.008)	Loss 1.1679 (1.2081)	Acc@1 55.078 (55.769)	Acc@5 96.484 (95.325)
Epoch: [142][128/196]	Time 0.316 (0.277)	Data 0.000 (0.004)	Loss 1.2932 (1.2079)	Acc@1 52.344 (55.914)	Acc@5 94.531 (95.216)
Epoch: [142][192/196]	Time 0.283 (0.279)	Data 0.000 (0.003)	Loss 1.2822 (1.1982)	Acc@1 51.172 (56.246)	Acc@5 94.531 (95.329)
after train
test acc: 55.51
Epoche: [143/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [143][0/196]	Time 0.300 (0.300)	Data 0.540 (0.540)	Loss 1.2303 (1.2303)	Acc@1 54.688 (54.688)	Acc@5 96.094 (96.094)
Epoch: [143][64/196]	Time 0.320 (0.251)	Data 0.000 (0.009)	Loss 1.1154 (1.1824)	Acc@1 59.375 (56.959)	Acc@5 96.875 (95.601)
Epoch: [143][128/196]	Time 0.276 (0.270)	Data 0.000 (0.005)	Loss 1.0762 (1.1844)	Acc@1 60.156 (56.889)	Acc@5 96.875 (95.561)
Epoch: [143][192/196]	Time 0.261 (0.280)	Data 0.000 (0.003)	Loss 1.1195 (1.1841)	Acc@1 59.375 (56.938)	Acc@5 96.484 (95.470)
after train
test acc: 34.19
Epoche: [144/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [144][0/196]	Time 0.336 (0.336)	Data 0.462 (0.462)	Loss 1.0586 (1.0586)	Acc@1 60.547 (60.547)	Acc@5 97.266 (97.266)
Epoch: [144][64/196]	Time 0.261 (0.216)	Data 0.000 (0.007)	Loss 1.2253 (1.1830)	Acc@1 53.906 (56.472)	Acc@5 95.312 (95.559)
Epoch: [144][128/196]	Time 0.257 (0.253)	Data 0.000 (0.004)	Loss 1.2110 (1.1735)	Acc@1 50.781 (57.098)	Acc@5 94.531 (95.600)
Epoch: [144][192/196]	Time 0.299 (0.268)	Data 0.000 (0.003)	Loss 1.2061 (1.1763)	Acc@1 58.203 (56.934)	Acc@5 94.141 (95.515)
after train
test acc: 24.49
Epoche: [145/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [145][0/196]	Time 0.248 (0.248)	Data 0.462 (0.462)	Loss 1.2173 (1.2173)	Acc@1 55.469 (55.469)	Acc@5 95.703 (95.703)
Epoch: [145][64/196]	Time 0.319 (0.253)	Data 0.000 (0.007)	Loss 1.1364 (1.1668)	Acc@1 59.375 (57.506)	Acc@5 96.484 (95.631)
Epoch: [145][128/196]	Time 0.273 (0.272)	Data 0.000 (0.004)	Loss 1.0725 (1.1551)	Acc@1 60.547 (58.112)	Acc@5 96.484 (95.724)
Epoch: [145][192/196]	Time 0.309 (0.280)	Data 0.000 (0.003)	Loss 1.3007 (1.1609)	Acc@1 57.031 (57.946)	Acc@5 92.578 (95.681)
after train
test acc: 36.81
Epoche: [146/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [146][0/196]	Time 0.330 (0.330)	Data 0.505 (0.505)	Loss 1.2145 (1.2145)	Acc@1 57.812 (57.812)	Acc@5 94.141 (94.141)
Epoch: [146][64/196]	Time 0.308 (0.249)	Data 0.000 (0.008)	Loss 1.1740 (1.1666)	Acc@1 56.250 (57.981)	Acc@5 96.094 (95.469)
Epoch: [146][128/196]	Time 0.315 (0.274)	Data 0.000 (0.004)	Loss 0.9864 (1.1507)	Acc@1 65.625 (58.273)	Acc@5 97.266 (95.697)
Epoch: [146][192/196]	Time 0.309 (0.279)	Data 0.000 (0.003)	Loss 1.2096 (1.1500)	Acc@1 53.906 (58.316)	Acc@5 94.922 (95.731)
after train
test acc: 15.93
Epoche: [147/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [147][0/196]	Time 0.398 (0.398)	Data 0.596 (0.596)	Loss 1.1458 (1.1458)	Acc@1 58.984 (58.984)	Acc@5 94.531 (94.531)
Epoch: [147][64/196]	Time 0.282 (0.258)	Data 0.000 (0.009)	Loss 1.1091 (1.1504)	Acc@1 59.766 (58.245)	Acc@5 96.094 (95.505)
Epoch: [147][128/196]	Time 0.318 (0.273)	Data 0.000 (0.005)	Loss 1.1920 (1.1470)	Acc@1 56.250 (58.358)	Acc@5 94.922 (95.646)
Epoch: [147][192/196]	Time 0.294 (0.281)	Data 0.000 (0.003)	Loss 1.1342 (1.1379)	Acc@1 57.031 (58.691)	Acc@5 94.531 (95.752)
after train
test acc: 51.32
Epoche: [148/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [148][0/196]	Time 0.331 (0.331)	Data 0.455 (0.455)	Loss 1.0525 (1.0525)	Acc@1 63.672 (63.672)	Acc@5 98.047 (98.047)
Epoch: [148][64/196]	Time 0.274 (0.252)	Data 0.000 (0.007)	Loss 1.1000 (1.1289)	Acc@1 60.547 (59.285)	Acc@5 95.312 (95.956)
Epoch: [148][128/196]	Time 0.301 (0.270)	Data 0.000 (0.004)	Loss 0.9607 (1.1204)	Acc@1 64.062 (59.393)	Acc@5 96.094 (95.948)
Epoch: [148][192/196]	Time 0.305 (0.277)	Data 0.000 (0.003)	Loss 1.1597 (1.1194)	Acc@1 58.203 (59.519)	Acc@5 93.750 (95.879)
after train
test acc: 53.97
Epoche: [149/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [149][0/196]	Time 0.339 (0.339)	Data 0.660 (0.660)	Loss 1.2081 (1.2081)	Acc@1 55.859 (55.859)	Acc@5 96.094 (96.094)
Epoch: [149][64/196]	Time 0.293 (0.246)	Data 0.000 (0.010)	Loss 1.0978 (1.1068)	Acc@1 62.500 (60.024)	Acc@5 97.266 (96.070)
Epoch: [149][128/196]	Time 0.312 (0.272)	Data 0.000 (0.005)	Loss 1.0164 (1.1006)	Acc@1 65.234 (60.335)	Acc@5 98.047 (96.079)
Epoch: [149][192/196]	Time 0.330 (0.278)	Data 0.000 (0.004)	Loss 1.0262 (1.1051)	Acc@1 63.672 (60.237)	Acc@5 96.094 (96.063)
after train
test acc: 53.85
Epoche: [150/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [150][0/196]	Time 0.412 (0.412)	Data 0.522 (0.522)	Loss 1.0805 (1.0805)	Acc@1 60.547 (60.547)	Acc@5 96.875 (96.875)
Epoch: [150][64/196]	Time 0.312 (0.250)	Data 0.000 (0.008)	Loss 1.0220 (1.0936)	Acc@1 60.156 (60.493)	Acc@5 97.656 (96.238)
Epoch: [150][128/196]	Time 0.297 (0.271)	Data 0.000 (0.004)	Loss 1.2117 (1.0989)	Acc@1 58.984 (60.159)	Acc@5 94.922 (96.154)
Epoch: [150][192/196]	Time 0.227 (0.277)	Data 0.000 (0.003)	Loss 1.0263 (1.0985)	Acc@1 61.328 (60.276)	Acc@5 95.312 (96.098)
after train
test acc: 47.16
Epoche: [151/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [151][0/196]	Time 0.274 (0.274)	Data 0.529 (0.529)	Loss 1.1141 (1.1141)	Acc@1 59.766 (59.766)	Acc@5 95.312 (95.312)
Epoch: [151][64/196]	Time 0.309 (0.247)	Data 0.000 (0.008)	Loss 1.0740 (1.0880)	Acc@1 65.234 (60.403)	Acc@5 96.094 (96.310)
Epoch: [151][128/196]	Time 0.260 (0.275)	Data 0.000 (0.004)	Loss 1.0709 (1.0843)	Acc@1 56.641 (60.701)	Acc@5 96.094 (96.124)
Epoch: [151][192/196]	Time 0.314 (0.279)	Data 0.000 (0.003)	Loss 1.0503 (1.0876)	Acc@1 58.984 (60.618)	Acc@5 96.484 (96.112)
after train
test acc: 32.21
Epoche: [152/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [152][0/196]	Time 0.270 (0.270)	Data 0.460 (0.460)	Loss 1.2088 (1.2088)	Acc@1 57.812 (57.812)	Acc@5 96.484 (96.484)
Epoch: [152][64/196]	Time 0.261 (0.241)	Data 0.000 (0.007)	Loss 1.0125 (1.0819)	Acc@1 63.672 (60.661)	Acc@5 96.484 (96.154)
Epoch: [152][128/196]	Time 0.272 (0.264)	Data 0.000 (0.004)	Loss 0.8983 (1.0804)	Acc@1 65.234 (60.883)	Acc@5 98.047 (96.142)
Epoch: [152][192/196]	Time 0.301 (0.273)	Data 0.000 (0.003)	Loss 1.1600 (1.0785)	Acc@1 55.469 (61.014)	Acc@5 96.875 (96.231)
after train
test acc: 30.55
Epoche: [153/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [153][0/196]	Time 0.333 (0.333)	Data 0.529 (0.529)	Loss 1.1018 (1.1018)	Acc@1 60.938 (60.938)	Acc@5 93.750 (93.750)
Epoch: [153][64/196]	Time 0.289 (0.213)	Data 0.000 (0.009)	Loss 0.9901 (1.0772)	Acc@1 64.453 (61.226)	Acc@5 96.875 (96.022)
Epoch: [153][128/196]	Time 0.270 (0.245)	Data 0.000 (0.004)	Loss 1.0012 (1.0738)	Acc@1 65.625 (61.271)	Acc@5 96.875 (96.236)
Epoch: [153][192/196]	Time 0.272 (0.257)	Data 0.000 (0.003)	Loss 1.0098 (1.0718)	Acc@1 60.938 (61.379)	Acc@5 96.484 (96.304)
after train
test acc: 58.89
Epoche: [154/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [154][0/196]	Time 0.345 (0.345)	Data 0.544 (0.544)	Loss 0.9338 (0.9338)	Acc@1 66.406 (66.406)	Acc@5 96.875 (96.875)
Epoch: [154][64/196]	Time 0.310 (0.263)	Data 0.000 (0.009)	Loss 0.9091 (1.0540)	Acc@1 69.531 (61.683)	Acc@5 98.828 (96.593)
Epoch: [154][128/196]	Time 0.310 (0.282)	Data 0.000 (0.005)	Loss 1.0209 (1.0536)	Acc@1 65.625 (61.822)	Acc@5 95.703 (96.466)
Epoch: [154][192/196]	Time 0.257 (0.288)	Data 0.000 (0.003)	Loss 1.0166 (1.0543)	Acc@1 64.062 (61.866)	Acc@5 98.047 (96.387)
after train
test acc: 42.36
Epoche: [155/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [155][0/196]	Time 0.321 (0.321)	Data 0.510 (0.510)	Loss 1.0176 (1.0176)	Acc@1 64.062 (64.062)	Acc@5 97.656 (97.656)
Epoch: [155][64/196]	Time 0.288 (0.254)	Data 0.000 (0.008)	Loss 1.1191 (1.0653)	Acc@1 64.062 (61.665)	Acc@5 95.312 (96.292)
Epoch: [155][128/196]	Time 0.271 (0.270)	Data 0.000 (0.004)	Loss 1.0429 (1.0565)	Acc@1 58.984 (61.825)	Acc@5 97.656 (96.363)
Epoch: [155][192/196]	Time 0.269 (0.275)	Data 0.000 (0.003)	Loss 1.0280 (1.0455)	Acc@1 60.547 (62.138)	Acc@5 96.484 (96.432)
after train
test acc: 45.58
Epoche: [156/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [156][0/196]	Time 0.382 (0.382)	Data 0.535 (0.535)	Loss 0.9762 (0.9762)	Acc@1 61.328 (61.328)	Acc@5 97.266 (97.266)
Epoch: [156][64/196]	Time 0.296 (0.253)	Data 0.000 (0.009)	Loss 1.0489 (1.0429)	Acc@1 64.062 (62.260)	Acc@5 97.266 (96.599)
Epoch: [156][128/196]	Time 0.310 (0.273)	Data 0.000 (0.005)	Loss 0.9749 (1.0443)	Acc@1 61.328 (62.285)	Acc@5 97.656 (96.539)
Epoch: [156][192/196]	Time 0.291 (0.280)	Data 0.000 (0.003)	Loss 1.0039 (1.0391)	Acc@1 60.938 (62.453)	Acc@5 97.266 (96.584)
after train
test acc: 55.05
Epoche: [157/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [157][0/196]	Time 0.358 (0.358)	Data 0.437 (0.437)	Loss 0.9618 (0.9618)	Acc@1 65.625 (65.625)	Acc@5 98.438 (98.438)
Epoch: [157][64/196]	Time 0.283 (0.231)	Data 0.000 (0.007)	Loss 0.9387 (1.0376)	Acc@1 68.750 (62.548)	Acc@5 97.656 (96.502)
Epoch: [157][128/196]	Time 0.257 (0.262)	Data 0.000 (0.004)	Loss 1.0533 (1.0358)	Acc@1 66.406 (62.573)	Acc@5 96.094 (96.442)
Epoch: [157][192/196]	Time 0.296 (0.274)	Data 0.000 (0.003)	Loss 0.9477 (1.0357)	Acc@1 64.453 (62.591)	Acc@5 96.094 (96.484)
after train
test acc: 56.27
Epoche: [158/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [158][0/196]	Time 0.393 (0.393)	Data 0.616 (0.616)	Loss 0.9574 (0.9574)	Acc@1 61.328 (61.328)	Acc@5 98.438 (98.438)
Epoch: [158][64/196]	Time 0.319 (0.261)	Data 0.000 (0.010)	Loss 1.0909 (1.0278)	Acc@1 61.719 (62.692)	Acc@5 96.094 (96.611)
Epoch: [158][128/196]	Time 0.265 (0.272)	Data 0.000 (0.005)	Loss 1.1377 (1.0216)	Acc@1 58.984 (63.057)	Acc@5 94.141 (96.687)
Epoch: [158][192/196]	Time 0.306 (0.274)	Data 0.000 (0.003)	Loss 1.0623 (1.0260)	Acc@1 64.062 (62.830)	Acc@5 98.047 (96.697)
after train
test acc: 49.92
Epoche: [159/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [159][0/196]	Time 0.405 (0.405)	Data 0.486 (0.486)	Loss 1.0043 (1.0043)	Acc@1 65.234 (65.234)	Acc@5 96.875 (96.875)
Epoch: [159][64/196]	Time 0.291 (0.253)	Data 0.000 (0.008)	Loss 0.9807 (1.0192)	Acc@1 64.844 (63.179)	Acc@5 98.047 (96.719)
Epoch: [159][128/196]	Time 0.340 (0.276)	Data 0.000 (0.004)	Loss 1.0654 (1.0185)	Acc@1 61.328 (63.296)	Acc@5 96.484 (96.745)
Epoch: [159][192/196]	Time 0.315 (0.285)	Data 0.000 (0.003)	Loss 1.0867 (1.0139)	Acc@1 64.062 (63.484)	Acc@5 94.922 (96.695)
after train
test acc: 49.69
Epoche: [160/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [160][0/196]	Time 0.361 (0.361)	Data 0.401 (0.401)	Loss 0.9254 (0.9254)	Acc@1 67.969 (67.969)	Acc@5 96.094 (96.094)
Epoch: [160][64/196]	Time 0.305 (0.263)	Data 0.000 (0.007)	Loss 1.0350 (1.0098)	Acc@1 65.234 (63.227)	Acc@5 94.531 (96.755)
Epoch: [160][128/196]	Time 0.300 (0.274)	Data 0.000 (0.003)	Loss 0.9602 (1.0070)	Acc@1 62.109 (63.305)	Acc@5 96.094 (96.811)
Epoch: [160][192/196]	Time 0.285 (0.282)	Data 0.000 (0.002)	Loss 1.0090 (1.0083)	Acc@1 64.062 (63.401)	Acc@5 96.484 (96.739)
after train
test acc: 35.34
Epoche: [161/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [161][0/196]	Time 0.289 (0.289)	Data 0.398 (0.398)	Loss 1.0267 (1.0267)	Acc@1 62.500 (62.500)	Acc@5 97.656 (97.656)
Epoch: [161][64/196]	Time 0.292 (0.264)	Data 0.000 (0.006)	Loss 1.0303 (0.9894)	Acc@1 59.766 (64.093)	Acc@5 96.875 (96.977)
Epoch: [161][128/196]	Time 0.312 (0.280)	Data 0.000 (0.003)	Loss 0.9591 (0.9965)	Acc@1 65.234 (63.966)	Acc@5 98.438 (96.905)
Epoch: [161][192/196]	Time 0.296 (0.287)	Data 0.000 (0.002)	Loss 0.9957 (0.9947)	Acc@1 62.891 (64.152)	Acc@5 96.875 (96.936)
after train
test acc: 29.43
Epoche: [162/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [162][0/196]	Time 0.348 (0.348)	Data 0.386 (0.386)	Loss 1.0282 (1.0282)	Acc@1 62.109 (62.109)	Acc@5 95.703 (95.703)
Epoch: [162][64/196]	Time 0.295 (0.247)	Data 0.000 (0.006)	Loss 0.9433 (0.9977)	Acc@1 67.969 (64.357)	Acc@5 96.484 (96.599)
Epoch: [162][128/196]	Time 0.281 (0.270)	Data 0.000 (0.003)	Loss 0.9928 (0.9853)	Acc@1 65.625 (64.523)	Acc@5 96.094 (96.848)
Epoch: [162][192/196]	Time 0.290 (0.278)	Data 0.000 (0.002)	Loss 0.9467 (0.9887)	Acc@1 63.672 (64.382)	Acc@5 98.438 (96.871)
after train
test acc: 54.51
Epoche: [163/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [163][0/196]	Time 0.263 (0.263)	Data 0.515 (0.515)	Loss 1.0142 (1.0142)	Acc@1 63.281 (63.281)	Acc@5 96.484 (96.484)
Epoch: [163][64/196]	Time 0.322 (0.272)	Data 0.000 (0.008)	Loss 1.0140 (0.9769)	Acc@1 61.328 (65.126)	Acc@5 98.438 (96.959)
Epoch: [163][128/196]	Time 0.343 (0.286)	Data 0.000 (0.004)	Loss 0.9937 (0.9704)	Acc@1 62.500 (65.074)	Acc@5 98.828 (97.023)
Epoch: [163][192/196]	Time 0.297 (0.287)	Data 0.000 (0.003)	Loss 1.0206 (0.9787)	Acc@1 60.938 (64.757)	Acc@5 96.094 (96.952)
after train
test acc: 45.23
Epoche: [164/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [164][0/196]	Time 0.184 (0.184)	Data 0.529 (0.529)	Loss 0.8533 (0.8533)	Acc@1 68.750 (68.750)	Acc@5 97.266 (97.266)
Epoch: [164][64/196]	Time 0.308 (0.253)	Data 0.000 (0.009)	Loss 0.9684 (0.9703)	Acc@1 67.188 (65.306)	Acc@5 96.875 (96.905)
Epoch: [164][128/196]	Time 0.298 (0.280)	Data 0.000 (0.005)	Loss 1.0986 (0.9714)	Acc@1 60.938 (65.125)	Acc@5 94.922 (96.990)
Epoch: [164][192/196]	Time 0.311 (0.284)	Data 0.000 (0.003)	Loss 0.9392 (0.9733)	Acc@1 66.016 (65.030)	Acc@5 98.828 (96.990)
after train
test acc: 49.47
Epoche: [165/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [165][0/196]	Time 0.281 (0.281)	Data 0.469 (0.469)	Loss 0.9376 (0.9376)	Acc@1 64.062 (64.062)	Acc@5 96.484 (96.484)
Epoch: [165][64/196]	Time 0.310 (0.274)	Data 0.000 (0.008)	Loss 0.9139 (0.9775)	Acc@1 68.750 (64.790)	Acc@5 97.266 (97.073)
Epoch: [165][128/196]	Time 0.319 (0.291)	Data 0.000 (0.004)	Loss 0.9211 (0.9715)	Acc@1 67.969 (64.750)	Acc@5 97.266 (97.151)
Epoch: [165][192/196]	Time 0.284 (0.292)	Data 0.000 (0.003)	Loss 1.0106 (0.9729)	Acc@1 62.500 (64.864)	Acc@5 96.875 (97.079)
after train
test acc: 39.86
Epoche: [166/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [166][0/196]	Time 0.246 (0.246)	Data 0.512 (0.512)	Loss 1.1023 (1.1023)	Acc@1 62.500 (62.500)	Acc@5 94.922 (94.922)
Epoch: [166][64/196]	Time 0.308 (0.259)	Data 0.000 (0.008)	Loss 0.9620 (0.9603)	Acc@1 67.969 (65.439)	Acc@5 97.656 (97.314)
Epoch: [166][128/196]	Time 0.293 (0.274)	Data 0.000 (0.004)	Loss 0.9987 (0.9674)	Acc@1 66.016 (65.256)	Acc@5 96.484 (97.069)
Epoch: [166][192/196]	Time 0.330 (0.283)	Data 0.000 (0.003)	Loss 0.8505 (0.9637)	Acc@1 69.531 (65.455)	Acc@5 98.438 (97.051)
after train
test acc: 42.51
Epoche: [167/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [167][0/196]	Time 0.283 (0.283)	Data 0.498 (0.498)	Loss 0.9272 (0.9272)	Acc@1 69.922 (69.922)	Acc@5 97.266 (97.266)
Epoch: [167][64/196]	Time 0.347 (0.264)	Data 0.000 (0.008)	Loss 1.0510 (0.9472)	Acc@1 60.156 (65.925)	Acc@5 96.094 (96.989)
Epoch: [167][128/196]	Time 0.318 (0.280)	Data 0.000 (0.004)	Loss 1.0116 (0.9524)	Acc@1 63.281 (65.746)	Acc@5 97.266 (97.105)
Epoch: [167][192/196]	Time 0.255 (0.282)	Data 0.000 (0.003)	Loss 0.9718 (0.9539)	Acc@1 63.281 (65.870)	Acc@5 94.922 (97.031)
after train
test acc: 65.1
Epoche: [168/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [168][0/196]	Time 0.325 (0.325)	Data 0.448 (0.448)	Loss 0.9689 (0.9689)	Acc@1 64.844 (64.844)	Acc@5 98.438 (98.438)
Epoch: [168][64/196]	Time 0.284 (0.279)	Data 0.000 (0.007)	Loss 0.9342 (0.9313)	Acc@1 66.406 (66.304)	Acc@5 97.656 (97.392)
Epoch: [168][128/196]	Time 0.296 (0.286)	Data 0.000 (0.004)	Loss 1.0332 (0.9402)	Acc@1 61.719 (66.061)	Acc@5 98.047 (97.341)
Epoch: [168][192/196]	Time 0.304 (0.288)	Data 0.000 (0.003)	Loss 0.9117 (0.9429)	Acc@1 67.188 (65.941)	Acc@5 98.047 (97.217)
after train
test acc: 60.18
Epoche: [169/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [169][0/196]	Time 0.333 (0.333)	Data 0.472 (0.472)	Loss 0.8954 (0.8954)	Acc@1 66.016 (66.016)	Acc@5 97.266 (97.266)
Epoch: [169][64/196]	Time 0.311 (0.288)	Data 0.000 (0.008)	Loss 0.9234 (0.9462)	Acc@1 66.797 (66.130)	Acc@5 98.438 (97.163)
Epoch: [169][128/196]	Time 0.314 (0.294)	Data 0.000 (0.004)	Loss 0.9378 (0.9407)	Acc@1 66.016 (66.346)	Acc@5 95.312 (97.196)
Epoch: [169][192/196]	Time 0.273 (0.294)	Data 0.000 (0.003)	Loss 0.8175 (0.9377)	Acc@1 73.828 (66.366)	Acc@5 98.828 (97.233)
after train
test acc: 18.93
Epoche: [170/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [170][0/196]	Time 0.230 (0.230)	Data 0.522 (0.522)	Loss 0.9301 (0.9301)	Acc@1 67.578 (67.578)	Acc@5 96.875 (96.875)
Epoch: [170][64/196]	Time 0.266 (0.280)	Data 0.000 (0.008)	Loss 0.9145 (0.9308)	Acc@1 65.234 (66.701)	Acc@5 97.266 (97.278)
Epoch: [170][128/196]	Time 0.297 (0.285)	Data 0.000 (0.004)	Loss 0.9343 (0.9372)	Acc@1 64.844 (66.303)	Acc@5 97.266 (97.293)
Epoch: [170][192/196]	Time 0.297 (0.292)	Data 0.000 (0.003)	Loss 0.9575 (0.9327)	Acc@1 67.969 (66.637)	Acc@5 96.094 (97.272)
after train
test acc: 36.68
Epoche: [171/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [171][0/196]	Time 0.225 (0.225)	Data 0.570 (0.570)	Loss 0.9342 (0.9342)	Acc@1 67.969 (67.969)	Acc@5 97.656 (97.656)
Epoch: [171][64/196]	Time 0.270 (0.285)	Data 0.000 (0.009)	Loss 0.9538 (0.9283)	Acc@1 68.750 (66.971)	Acc@5 96.094 (97.175)
Epoch: [171][128/196]	Time 0.270 (0.285)	Data 0.000 (0.005)	Loss 1.0041 (0.9243)	Acc@1 63.281 (66.936)	Acc@5 98.047 (97.223)
Epoch: [171][192/196]	Time 0.153 (0.288)	Data 0.000 (0.003)	Loss 0.7919 (0.9254)	Acc@1 70.703 (66.870)	Acc@5 96.875 (97.221)
after train
test acc: 32.36
Epoche: [172/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [172][0/196]	Time 0.276 (0.276)	Data 0.381 (0.381)	Loss 0.8638 (0.8638)	Acc@1 67.578 (67.578)	Acc@5 97.266 (97.266)
Epoch: [172][64/196]	Time 0.300 (0.293)	Data 0.000 (0.006)	Loss 0.7664 (0.9124)	Acc@1 74.609 (67.626)	Acc@5 98.828 (97.320)
Epoch: [172][128/196]	Time 0.299 (0.291)	Data 0.000 (0.003)	Loss 0.9738 (0.9141)	Acc@1 63.672 (67.496)	Acc@5 94.141 (97.402)
Epoch: [172][192/196]	Time 0.299 (0.292)	Data 0.000 (0.002)	Loss 0.9831 (0.9178)	Acc@1 67.188 (67.388)	Acc@5 94.531 (97.324)
after train
test acc: 66.05
Epoche: [173/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [173][0/196]	Time 0.300 (0.300)	Data 0.458 (0.458)	Loss 0.9654 (0.9654)	Acc@1 63.672 (63.672)	Acc@5 98.047 (98.047)
Epoch: [173][64/196]	Time 0.257 (0.288)	Data 0.000 (0.007)	Loss 0.8385 (0.9163)	Acc@1 71.484 (67.025)	Acc@5 99.609 (97.392)
Epoch: [173][128/196]	Time 0.292 (0.286)	Data 0.000 (0.004)	Loss 0.8666 (0.9080)	Acc@1 67.188 (67.439)	Acc@5 97.656 (97.405)
Epoch: [173][192/196]	Time 0.110 (0.284)	Data 0.000 (0.003)	Loss 0.8420 (0.9069)	Acc@1 67.969 (67.552)	Acc@5 98.828 (97.409)
after train
test acc: 42.33
Epoche: [174/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [174][0/196]	Time 0.327 (0.327)	Data 0.448 (0.448)	Loss 0.9275 (0.9275)	Acc@1 69.531 (69.531)	Acc@5 97.656 (97.656)
Epoch: [174][64/196]	Time 0.299 (0.290)	Data 0.000 (0.007)	Loss 0.8844 (0.9138)	Acc@1 68.359 (67.212)	Acc@5 96.094 (97.446)
Epoch: [174][128/196]	Time 0.292 (0.291)	Data 0.000 (0.004)	Loss 0.9407 (0.9127)	Acc@1 67.969 (67.290)	Acc@5 98.047 (97.438)
Epoch: [174][192/196]	Time 0.109 (0.287)	Data 0.000 (0.003)	Loss 0.9048 (0.9141)	Acc@1 67.578 (67.335)	Acc@5 96.875 (97.371)
after train
test acc: 16.75
Epoche: [175/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [175][0/196]	Time 0.256 (0.256)	Data 0.611 (0.611)	Loss 1.0324 (1.0324)	Acc@1 61.719 (61.719)	Acc@5 95.703 (95.703)
Epoch: [175][64/196]	Time 0.303 (0.290)	Data 0.000 (0.010)	Loss 0.8489 (0.9107)	Acc@1 69.922 (67.734)	Acc@5 98.047 (97.290)
Epoch: [175][128/196]	Time 0.260 (0.291)	Data 0.000 (0.005)	Loss 0.9305 (0.9080)	Acc@1 66.016 (67.836)	Acc@5 98.828 (97.323)
Epoch: [175][192/196]	Time 0.205 (0.289)	Data 0.000 (0.004)	Loss 0.8921 (0.9056)	Acc@1 69.531 (68.005)	Acc@5 97.656 (97.334)
after train
test acc: 35.72
Epoche: [176/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [176][0/196]	Time 0.390 (0.390)	Data 0.557 (0.557)	Loss 0.8825 (0.8825)	Acc@1 67.969 (67.969)	Acc@5 96.094 (96.094)
Epoch: [176][64/196]	Time 0.204 (0.291)	Data 0.000 (0.009)	Loss 0.9116 (0.8897)	Acc@1 67.188 (68.516)	Acc@5 98.047 (97.578)
Epoch: [176][128/196]	Time 0.270 (0.293)	Data 0.000 (0.005)	Loss 0.8960 (0.8945)	Acc@1 65.234 (68.326)	Acc@5 96.875 (97.523)
Epoch: [176][192/196]	Time 0.208 (0.289)	Data 0.000 (0.003)	Loss 0.7298 (0.8897)	Acc@1 73.438 (68.327)	Acc@5 99.219 (97.602)
after train
test acc: 51.01
Epoche: [177/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [177][0/196]	Time 0.421 (0.421)	Data 0.532 (0.532)	Loss 0.8548 (0.8548)	Acc@1 67.969 (67.969)	Acc@5 98.828 (98.828)
Epoch: [177][64/196]	Time 0.297 (0.292)	Data 0.000 (0.008)	Loss 0.9548 (0.8834)	Acc@1 67.188 (68.678)	Acc@5 96.094 (97.428)
Epoch: [177][128/196]	Time 0.288 (0.295)	Data 0.000 (0.004)	Loss 0.9151 (0.8817)	Acc@1 67.969 (68.777)	Acc@5 96.094 (97.487)
Epoch: [177][192/196]	Time 0.314 (0.291)	Data 0.000 (0.003)	Loss 0.7988 (0.8841)	Acc@1 70.312 (68.618)	Acc@5 98.047 (97.482)
after train
test acc: 58.81
Epoche: [178/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [178][0/196]	Time 0.422 (0.422)	Data 0.541 (0.541)	Loss 0.9187 (0.9187)	Acc@1 67.969 (67.969)	Acc@5 97.656 (97.656)
Epoch: [178][64/196]	Time 0.310 (0.285)	Data 0.000 (0.009)	Loss 0.9371 (0.9031)	Acc@1 67.969 (67.855)	Acc@5 98.047 (97.308)
Epoch: [178][128/196]	Time 0.301 (0.291)	Data 0.000 (0.005)	Loss 0.7990 (0.8997)	Acc@1 73.438 (68.153)	Acc@5 99.219 (97.414)
Epoch: [178][192/196]	Time 0.181 (0.286)	Data 0.000 (0.003)	Loss 0.9180 (0.8916)	Acc@1 66.406 (68.412)	Acc@5 98.047 (97.494)
after train
test acc: 53.71
Epoche: [179/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [179][0/196]	Time 0.310 (0.310)	Data 0.589 (0.589)	Loss 0.7982 (0.7982)	Acc@1 74.219 (74.219)	Acc@5 96.484 (96.484)
Epoch: [179][64/196]	Time 0.292 (0.299)	Data 0.000 (0.009)	Loss 0.8878 (0.8713)	Acc@1 67.969 (68.972)	Acc@5 97.656 (97.566)
Epoch: [179][128/196]	Time 0.363 (0.295)	Data 0.000 (0.005)	Loss 0.9077 (0.8741)	Acc@1 66.797 (68.841)	Acc@5 94.531 (97.599)
Epoch: [179][192/196]	Time 0.204 (0.284)	Data 0.000 (0.003)	Loss 0.8481 (0.8743)	Acc@1 67.578 (68.938)	Acc@5 98.047 (97.581)
after train
test acc: 31.78
Epoche: [180/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [180][0/196]	Time 0.243 (0.243)	Data 0.489 (0.489)	Loss 0.9029 (0.9029)	Acc@1 68.359 (68.359)	Acc@5 98.438 (98.438)
Epoch: [180][64/196]	Time 0.354 (0.295)	Data 0.000 (0.008)	Loss 0.8566 (0.8826)	Acc@1 68.750 (68.353)	Acc@5 96.094 (97.548)
Epoch: [180][128/196]	Time 0.279 (0.291)	Data 0.000 (0.004)	Loss 0.7877 (0.8704)	Acc@1 69.531 (68.620)	Acc@5 99.609 (97.741)
Epoch: [180][192/196]	Time 0.104 (0.280)	Data 0.000 (0.003)	Loss 0.8495 (0.8757)	Acc@1 70.312 (68.693)	Acc@5 96.875 (97.636)
after train
test acc: 63.77
Epoche: [181/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [181][0/196]	Time 0.381 (0.381)	Data 0.470 (0.470)	Loss 0.8351 (0.8351)	Acc@1 69.922 (69.922)	Acc@5 98.828 (98.828)
Epoch: [181][64/196]	Time 0.329 (0.299)	Data 0.000 (0.008)	Loss 0.8253 (0.8636)	Acc@1 66.016 (69.351)	Acc@5 98.438 (97.656)
Epoch: [181][128/196]	Time 0.283 (0.296)	Data 0.000 (0.004)	Loss 0.7804 (0.8673)	Acc@1 69.531 (69.271)	Acc@5 99.219 (97.638)
Epoch: [181][192/196]	Time 0.288 (0.282)	Data 0.000 (0.003)	Loss 0.9511 (0.8705)	Acc@1 64.453 (69.108)	Acc@5 96.094 (97.670)
after train
test acc: 60.17
Epoche: [182/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [182][0/196]	Time 0.345 (0.345)	Data 0.539 (0.539)	Loss 0.8547 (0.8547)	Acc@1 68.359 (68.359)	Acc@5 97.656 (97.656)
Epoch: [182][64/196]	Time 0.281 (0.286)	Data 0.000 (0.009)	Loss 0.8110 (0.8508)	Acc@1 73.438 (69.375)	Acc@5 98.047 (97.855)
Epoch: [182][128/196]	Time 0.258 (0.291)	Data 0.000 (0.004)	Loss 0.9388 (0.8566)	Acc@1 63.281 (69.259)	Acc@5 96.484 (97.777)
Epoch: [182][192/196]	Time 0.168 (0.277)	Data 0.000 (0.003)	Loss 0.7898 (0.8594)	Acc@1 71.484 (69.294)	Acc@5 97.266 (97.717)
after train
test acc: 26.33
Epoche: [183/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [183][0/196]	Time 0.316 (0.316)	Data 0.426 (0.426)	Loss 0.7996 (0.7996)	Acc@1 71.484 (71.484)	Acc@5 97.266 (97.266)
Epoch: [183][64/196]	Time 0.275 (0.290)	Data 0.000 (0.007)	Loss 0.8745 (0.8502)	Acc@1 67.969 (69.351)	Acc@5 98.047 (97.837)
Epoch: [183][128/196]	Time 0.247 (0.293)	Data 0.000 (0.004)	Loss 0.8991 (0.8594)	Acc@1 66.016 (69.271)	Acc@5 96.484 (97.729)
Epoch: [183][192/196]	Time 0.286 (0.278)	Data 0.000 (0.003)	Loss 0.8540 (0.8582)	Acc@1 71.484 (69.464)	Acc@5 97.266 (97.687)
after train
test acc: 53.94
Epoche: [184/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [184][0/196]	Time 0.344 (0.344)	Data 0.462 (0.462)	Loss 0.9184 (0.9184)	Acc@1 67.578 (67.578)	Acc@5 97.266 (97.266)
Epoch: [184][64/196]	Time 0.349 (0.295)	Data 0.000 (0.007)	Loss 0.8420 (0.8465)	Acc@1 71.875 (69.946)	Acc@5 98.438 (97.861)
Epoch: [184][128/196]	Time 0.291 (0.288)	Data 0.000 (0.004)	Loss 0.8060 (0.8467)	Acc@1 71.484 (70.058)	Acc@5 98.047 (97.789)
Epoch: [184][192/196]	Time 0.293 (0.276)	Data 0.000 (0.003)	Loss 0.7635 (0.8484)	Acc@1 73.438 (70.001)	Acc@5 98.047 (97.774)
after train
test acc: 59.92
Epoche: [185/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [185][0/196]	Time 0.319 (0.319)	Data 0.537 (0.537)	Loss 0.9118 (0.9118)	Acc@1 67.578 (67.578)	Acc@5 98.828 (98.828)
Epoch: [185][64/196]	Time 0.256 (0.290)	Data 0.000 (0.009)	Loss 0.8538 (0.8484)	Acc@1 71.875 (70.192)	Acc@5 97.266 (97.698)
Epoch: [185][128/196]	Time 0.272 (0.292)	Data 0.000 (0.005)	Loss 0.8612 (0.8468)	Acc@1 71.484 (70.167)	Acc@5 95.703 (97.747)
Epoch: [185][192/196]	Time 0.269 (0.276)	Data 0.000 (0.003)	Loss 0.7306 (0.8444)	Acc@1 73.047 (70.339)	Acc@5 98.047 (97.755)
after train
test acc: 48.23
Epoche: [186/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [186][0/196]	Time 0.320 (0.320)	Data 0.500 (0.500)	Loss 0.7372 (0.7372)	Acc@1 69.922 (69.922)	Acc@5 99.609 (99.609)
Epoch: [186][64/196]	Time 0.299 (0.294)	Data 0.000 (0.008)	Loss 0.7731 (0.8423)	Acc@1 74.609 (69.958)	Acc@5 98.438 (97.909)
Epoch: [186][128/196]	Time 0.313 (0.290)	Data 0.000 (0.004)	Loss 0.8648 (0.8507)	Acc@1 70.312 (69.825)	Acc@5 97.266 (97.735)
Epoch: [186][192/196]	Time 0.272 (0.280)	Data 0.000 (0.003)	Loss 0.8193 (0.8452)	Acc@1 71.875 (69.948)	Acc@5 98.047 (97.812)
after train
test acc: 32.14
Epoche: [187/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [187][0/196]	Time 0.352 (0.352)	Data 0.473 (0.473)	Loss 0.7688 (0.7688)	Acc@1 73.047 (73.047)	Acc@5 96.484 (96.484)
Epoch: [187][64/196]	Time 0.258 (0.287)	Data 0.000 (0.008)	Loss 0.8251 (0.8444)	Acc@1 70.312 (69.988)	Acc@5 97.266 (97.620)
Epoch: [187][128/196]	Time 0.264 (0.285)	Data 0.000 (0.004)	Loss 0.8331 (0.8402)	Acc@1 70.312 (70.203)	Acc@5 98.047 (97.741)
Epoch: [187][192/196]	Time 0.272 (0.275)	Data 0.000 (0.003)	Loss 0.9019 (0.8437)	Acc@1 68.750 (70.110)	Acc@5 98.828 (97.774)
after train
test acc: 57.03
Epoche: [188/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [188][0/196]	Time 0.403 (0.403)	Data 0.514 (0.514)	Loss 0.8985 (0.8985)	Acc@1 66.406 (66.406)	Acc@5 98.828 (98.828)
Epoch: [188][64/196]	Time 0.314 (0.294)	Data 0.000 (0.008)	Loss 0.7583 (0.8361)	Acc@1 72.656 (70.613)	Acc@5 97.266 (97.885)
Epoch: [188][128/196]	Time 0.294 (0.290)	Data 0.000 (0.004)	Loss 0.9091 (0.8357)	Acc@1 67.578 (70.506)	Acc@5 98.828 (97.796)
Epoch: [188][192/196]	Time 0.266 (0.278)	Data 0.000 (0.003)	Loss 0.9503 (0.8373)	Acc@1 66.406 (70.634)	Acc@5 97.266 (97.806)
after train
test acc: 62.83
Epoche: [189/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [189][0/196]	Time 0.404 (0.404)	Data 0.496 (0.496)	Loss 0.8882 (0.8882)	Acc@1 66.016 (66.016)	Acc@5 98.438 (98.438)
Epoch: [189][64/196]	Time 0.267 (0.292)	Data 0.000 (0.008)	Loss 0.6576 (0.8303)	Acc@1 78.516 (70.553)	Acc@5 98.828 (97.812)
Epoch: [189][128/196]	Time 0.305 (0.284)	Data 0.000 (0.004)	Loss 0.8717 (0.8299)	Acc@1 68.750 (70.746)	Acc@5 98.828 (97.829)
Epoch: [189][192/196]	Time 0.317 (0.274)	Data 0.000 (0.003)	Loss 0.7986 (0.8269)	Acc@1 75.000 (70.897)	Acc@5 97.656 (97.849)
after train
test acc: 21.32
Epoche: [190/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [190][0/196]	Time 0.315 (0.315)	Data 0.523 (0.523)	Loss 0.8396 (0.8396)	Acc@1 73.047 (73.047)	Acc@5 97.266 (97.266)
Epoch: [190][64/196]	Time 0.291 (0.292)	Data 0.000 (0.009)	Loss 1.0210 (0.8260)	Acc@1 62.891 (71.112)	Acc@5 96.875 (97.993)
Epoch: [190][128/196]	Time 0.266 (0.292)	Data 0.000 (0.005)	Loss 0.8170 (0.8242)	Acc@1 69.531 (71.157)	Acc@5 99.609 (97.941)
Epoch: [190][192/196]	Time 0.290 (0.283)	Data 0.000 (0.003)	Loss 0.8924 (0.8273)	Acc@1 71.484 (71.021)	Acc@5 97.266 (97.919)
after train
test acc: 40.46
Epoche: [191/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [191][0/196]	Time 0.311 (0.311)	Data 0.537 (0.537)	Loss 0.9122 (0.9122)	Acc@1 70.312 (70.312)	Acc@5 96.484 (96.484)
Epoch: [191][64/196]	Time 0.300 (0.287)	Data 0.000 (0.009)	Loss 0.7252 (0.8286)	Acc@1 71.875 (70.655)	Acc@5 98.047 (97.758)
Epoch: [191][128/196]	Time 0.273 (0.286)	Data 0.000 (0.004)	Loss 0.7972 (0.8231)	Acc@1 68.359 (70.870)	Acc@5 96.875 (97.865)
Epoch: [191][192/196]	Time 0.311 (0.278)	Data 0.000 (0.003)	Loss 0.8843 (0.8229)	Acc@1 69.141 (70.926)	Acc@5 96.484 (97.838)
after train
test acc: 35.11
Epoche: [192/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [192][0/196]	Time 0.277 (0.277)	Data 0.610 (0.610)	Loss 0.8281 (0.8281)	Acc@1 70.312 (70.312)	Acc@5 97.656 (97.656)
Epoch: [192][64/196]	Time 0.279 (0.294)	Data 0.000 (0.010)	Loss 0.8157 (0.8165)	Acc@1 72.656 (71.316)	Acc@5 96.484 (97.837)
Epoch: [192][128/196]	Time 0.317 (0.293)	Data 0.000 (0.005)	Loss 0.7825 (0.8157)	Acc@1 73.047 (71.224)	Acc@5 98.047 (97.889)
Epoch: [192][192/196]	Time 0.313 (0.280)	Data 0.000 (0.003)	Loss 0.8317 (0.8159)	Acc@1 68.750 (71.248)	Acc@5 98.828 (97.869)
after train
test acc: 47.34
Epoche: [193/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [193][0/196]	Time 0.369 (0.369)	Data 0.479 (0.479)	Loss 0.6952 (0.6952)	Acc@1 77.734 (77.734)	Acc@5 98.828 (98.828)
Epoch: [193][64/196]	Time 0.287 (0.285)	Data 0.000 (0.008)	Loss 0.9054 (0.8157)	Acc@1 71.484 (71.124)	Acc@5 96.875 (98.053)
Epoch: [193][128/196]	Time 0.267 (0.286)	Data 0.000 (0.004)	Loss 0.8996 (0.8152)	Acc@1 69.141 (71.166)	Acc@5 97.266 (98.008)
Epoch: [193][192/196]	Time 0.310 (0.276)	Data 0.000 (0.003)	Loss 0.8277 (0.8189)	Acc@1 70.703 (70.982)	Acc@5 98.438 (98.019)
after train
test acc: 61.36
Epoche: [194/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [194][0/196]	Time 0.235 (0.235)	Data 0.462 (0.462)	Loss 0.6941 (0.6941)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [194][64/196]	Time 0.281 (0.297)	Data 0.000 (0.007)	Loss 0.7619 (0.8050)	Acc@1 71.094 (71.454)	Acc@5 100.000 (98.119)
Epoch: [194][128/196]	Time 0.286 (0.292)	Data 0.000 (0.004)	Loss 0.8717 (0.8073)	Acc@1 72.266 (71.527)	Acc@5 96.875 (98.053)
Epoch: [194][192/196]	Time 0.234 (0.279)	Data 0.000 (0.003)	Loss 0.9005 (0.8103)	Acc@1 73.438 (71.387)	Acc@5 97.266 (97.988)
after train
test acc: 49.23
Epoche: [195/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [195][0/196]	Time 0.310 (0.310)	Data 0.493 (0.493)	Loss 0.8438 (0.8438)	Acc@1 66.797 (66.797)	Acc@5 99.609 (99.609)
Epoch: [195][64/196]	Time 0.317 (0.296)	Data 0.000 (0.008)	Loss 0.8570 (0.7988)	Acc@1 70.312 (71.845)	Acc@5 96.875 (98.161)
Epoch: [195][128/196]	Time 0.318 (0.297)	Data 0.000 (0.004)	Loss 0.8761 (0.8058)	Acc@1 68.750 (71.654)	Acc@5 98.438 (98.053)
Epoch: [195][192/196]	Time 0.317 (0.283)	Data 0.000 (0.003)	Loss 0.7987 (0.8055)	Acc@1 76.562 (71.758)	Acc@5 98.438 (98.017)
after train
test acc: 61.7
Epoche: [196/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [196][0/196]	Time 0.359 (0.359)	Data 0.664 (0.664)	Loss 0.9651 (0.9651)	Acc@1 67.188 (67.188)	Acc@5 97.266 (97.266)
Epoch: [196][64/196]	Time 0.295 (0.292)	Data 0.000 (0.011)	Loss 0.8027 (0.8130)	Acc@1 69.141 (70.913)	Acc@5 97.656 (97.897)
Epoch: [196][128/196]	Time 0.247 (0.289)	Data 0.000 (0.005)	Loss 0.8744 (0.8112)	Acc@1 71.484 (71.257)	Acc@5 98.438 (97.898)
Epoch: [196][192/196]	Time 0.273 (0.276)	Data 0.000 (0.004)	Loss 0.7410 (0.8048)	Acc@1 73.047 (71.482)	Acc@5 98.438 (97.913)
after train
test acc: 64.31
Epoche: [197/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [197][0/196]	Time 0.351 (0.351)	Data 0.755 (0.755)	Loss 0.8417 (0.8417)	Acc@1 69.531 (69.531)	Acc@5 97.656 (97.656)
Epoch: [197][64/196]	Time 0.277 (0.292)	Data 0.000 (0.012)	Loss 0.8383 (0.7995)	Acc@1 71.094 (72.206)	Acc@5 97.266 (97.915)
Epoch: [197][128/196]	Time 0.159 (0.291)	Data 0.000 (0.006)	Loss 0.7854 (0.7965)	Acc@1 72.656 (72.196)	Acc@5 97.656 (97.950)
Epoch: [197][192/196]	Time 0.315 (0.279)	Data 0.000 (0.004)	Loss 0.7773 (0.8005)	Acc@1 72.266 (71.948)	Acc@5 98.438 (97.962)
after train
test acc: 58.99
Epoche: [198/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [198][0/196]	Time 0.303 (0.303)	Data 0.550 (0.550)	Loss 0.8350 (0.8350)	Acc@1 67.969 (67.969)	Acc@5 98.438 (98.438)
Epoch: [198][64/196]	Time 0.250 (0.294)	Data 0.000 (0.009)	Loss 0.7922 (0.7945)	Acc@1 73.438 (71.779)	Acc@5 98.047 (98.059)
Epoch: [198][128/196]	Time 0.183 (0.287)	Data 0.000 (0.005)	Loss 0.7332 (0.7983)	Acc@1 74.219 (71.848)	Acc@5 99.609 (98.050)
Epoch: [198][192/196]	Time 0.322 (0.282)	Data 0.000 (0.003)	Loss 0.8113 (0.7957)	Acc@1 74.219 (71.956)	Acc@5 98.438 (98.031)
after train
test acc: 70.84
Epoche: [199/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [199][0/196]	Time 0.293 (0.293)	Data 0.500 (0.500)	Loss 0.7740 (0.7740)	Acc@1 73.438 (73.438)	Acc@5 97.266 (97.266)
Epoch: [199][64/196]	Time 0.299 (0.299)	Data 0.000 (0.008)	Loss 0.8101 (0.7783)	Acc@1 72.266 (72.536)	Acc@5 97.656 (98.071)
Epoch: [199][128/196]	Time 0.224 (0.287)	Data 0.000 (0.004)	Loss 0.8418 (0.7844)	Acc@1 70.703 (72.514)	Acc@5 97.656 (98.095)
Epoch: [199][192/196]	Time 0.222 (0.285)	Data 0.000 (0.003)	Loss 0.7451 (0.7875)	Acc@1 76.953 (72.385)	Acc@5 96.484 (98.017)
after train
test acc: 31.97
Epoche: [200/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [200][0/196]	Time 0.392 (0.392)	Data 0.411 (0.411)	Loss 0.7241 (0.7241)	Acc@1 72.266 (72.266)	Acc@5 98.438 (98.438)
Epoch: [200][64/196]	Time 0.288 (0.280)	Data 0.000 (0.007)	Loss 0.7237 (0.7879)	Acc@1 74.219 (72.091)	Acc@5 98.438 (98.221)
Epoch: [200][128/196]	Time 0.149 (0.275)	Data 0.000 (0.004)	Loss 0.8063 (0.7887)	Acc@1 69.922 (72.160)	Acc@5 98.828 (98.171)
Epoch: [200][192/196]	Time 0.316 (0.271)	Data 0.000 (0.002)	Loss 0.7711 (0.7974)	Acc@1 71.875 (71.863)	Acc@5 98.047 (98.104)
after train
test acc: 55.6
Epoche: [201/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [201][0/196]	Time 0.344 (0.344)	Data 0.495 (0.495)	Loss 0.7666 (0.7666)	Acc@1 72.266 (72.266)	Acc@5 97.656 (97.656)
Epoch: [201][64/196]	Time 0.319 (0.302)	Data 0.000 (0.008)	Loss 0.8771 (0.7920)	Acc@1 67.578 (72.332)	Acc@5 97.656 (97.849)
Epoch: [201][128/196]	Time 0.187 (0.293)	Data 0.000 (0.004)	Loss 0.7473 (0.7842)	Acc@1 73.047 (72.644)	Acc@5 98.438 (97.932)
Epoch: [201][192/196]	Time 0.270 (0.289)	Data 0.000 (0.003)	Loss 0.7808 (0.7862)	Acc@1 71.094 (72.509)	Acc@5 98.047 (97.988)
after train
test acc: 56.32
Epoche: [202/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [202][0/196]	Time 0.282 (0.282)	Data 0.467 (0.467)	Loss 0.8370 (0.8370)	Acc@1 69.531 (69.531)	Acc@5 98.047 (98.047)
Epoch: [202][64/196]	Time 0.323 (0.294)	Data 0.000 (0.008)	Loss 0.8343 (0.7807)	Acc@1 72.656 (72.620)	Acc@5 97.266 (98.065)
Epoch: [202][128/196]	Time 0.193 (0.285)	Data 0.000 (0.004)	Loss 0.8166 (0.7853)	Acc@1 68.359 (72.314)	Acc@5 98.438 (98.204)
Epoch: [202][192/196]	Time 0.305 (0.281)	Data 0.000 (0.003)	Loss 0.8128 (0.7848)	Acc@1 73.438 (72.347)	Acc@5 96.875 (98.162)
after train
test acc: 58.7
Epoche: [203/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [203][0/196]	Time 0.353 (0.353)	Data 0.474 (0.474)	Loss 0.8386 (0.8386)	Acc@1 72.266 (72.266)	Acc@5 97.656 (97.656)
Epoch: [203][64/196]	Time 0.307 (0.290)	Data 0.000 (0.008)	Loss 0.8510 (0.7853)	Acc@1 68.359 (72.188)	Acc@5 97.266 (97.927)
Epoch: [203][128/196]	Time 0.182 (0.279)	Data 0.000 (0.004)	Loss 0.8771 (0.7729)	Acc@1 70.312 (72.811)	Acc@5 98.047 (98.098)
Epoch: [203][192/196]	Time 0.326 (0.275)	Data 0.000 (0.003)	Loss 0.7528 (0.7784)	Acc@1 72.656 (72.543)	Acc@5 97.266 (98.039)
after train
test acc: 70.08
Epoche: [204/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [204][0/196]	Time 0.377 (0.377)	Data 0.544 (0.544)	Loss 0.8238 (0.8238)	Acc@1 67.578 (67.578)	Acc@5 98.438 (98.438)
Epoch: [204][64/196]	Time 0.325 (0.303)	Data 0.000 (0.009)	Loss 0.6727 (0.7687)	Acc@1 74.219 (73.095)	Acc@5 99.219 (98.245)
Epoch: [204][128/196]	Time 0.234 (0.276)	Data 0.000 (0.005)	Loss 0.7649 (0.7769)	Acc@1 73.438 (72.620)	Acc@5 98.047 (98.144)
Epoch: [204][192/196]	Time 0.285 (0.276)	Data 0.000 (0.003)	Loss 0.8290 (0.7780)	Acc@1 69.531 (72.600)	Acc@5 96.484 (98.102)
after train
test acc: 64.86
Epoche: [205/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [205][0/196]	Time 0.331 (0.331)	Data 0.548 (0.548)	Loss 0.7239 (0.7239)	Acc@1 73.047 (73.047)	Acc@5 98.828 (98.828)
Epoch: [205][64/196]	Time 0.313 (0.305)	Data 0.000 (0.009)	Loss 0.7071 (0.7737)	Acc@1 73.828 (72.728)	Acc@5 98.047 (98.095)
Epoch: [205][128/196]	Time 0.297 (0.276)	Data 0.000 (0.005)	Loss 0.6775 (0.7746)	Acc@1 73.438 (72.653)	Acc@5 98.828 (98.113)
Epoch: [205][192/196]	Time 0.319 (0.283)	Data 0.000 (0.003)	Loss 0.7236 (0.7704)	Acc@1 72.656 (72.792)	Acc@5 99.219 (98.136)
after train
test acc: 68.34
Epoche: [206/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [206][0/196]	Time 0.289 (0.289)	Data 0.516 (0.516)	Loss 0.7521 (0.7521)	Acc@1 71.094 (71.094)	Acc@5 97.266 (97.266)
Epoch: [206][64/196]	Time 0.269 (0.287)	Data 0.000 (0.008)	Loss 0.8234 (0.7746)	Acc@1 69.141 (72.800)	Acc@5 97.656 (98.119)
Epoch: [206][128/196]	Time 0.273 (0.270)	Data 0.000 (0.004)	Loss 0.6887 (0.7742)	Acc@1 76.172 (72.774)	Acc@5 98.828 (98.071)
Epoch: [206][192/196]	Time 0.199 (0.271)	Data 0.000 (0.003)	Loss 0.7180 (0.7716)	Acc@1 75.391 (72.863)	Acc@5 98.828 (98.140)
after train
test acc: 35.02
Epoche: [207/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [207][0/196]	Time 0.321 (0.321)	Data 0.448 (0.448)	Loss 0.7264 (0.7264)	Acc@1 75.000 (75.000)	Acc@5 98.828 (98.828)
Epoch: [207][64/196]	Time 0.311 (0.291)	Data 0.000 (0.007)	Loss 0.6896 (0.7580)	Acc@1 75.000 (73.690)	Acc@5 98.828 (98.221)
Epoch: [207][128/196]	Time 0.310 (0.273)	Data 0.000 (0.004)	Loss 0.7989 (0.7621)	Acc@1 73.438 (73.353)	Acc@5 97.656 (98.174)
Epoch: [207][192/196]	Time 0.275 (0.278)	Data 0.000 (0.003)	Loss 0.7013 (0.7655)	Acc@1 72.266 (73.290)	Acc@5 98.438 (98.114)
after train
test acc: 45.0
Epoche: [208/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [208][0/196]	Time 0.310 (0.310)	Data 0.497 (0.497)	Loss 0.7470 (0.7470)	Acc@1 72.266 (72.266)	Acc@5 98.438 (98.438)
Epoch: [208][64/196]	Time 0.303 (0.301)	Data 0.000 (0.008)	Loss 0.7493 (0.7831)	Acc@1 69.922 (72.380)	Acc@5 98.047 (97.975)
Epoch: [208][128/196]	Time 0.317 (0.275)	Data 0.000 (0.004)	Loss 0.7208 (0.7709)	Acc@1 77.344 (72.593)	Acc@5 96.875 (98.083)
Epoch: [208][192/196]	Time 0.304 (0.281)	Data 0.000 (0.003)	Loss 0.6701 (0.7697)	Acc@1 74.219 (72.660)	Acc@5 99.609 (98.077)
after train
test acc: 54.8
Epoche: [209/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [209][0/196]	Time 0.336 (0.336)	Data 0.511 (0.511)	Loss 0.8368 (0.8368)	Acc@1 71.875 (71.875)	Acc@5 98.047 (98.047)
Epoch: [209][64/196]	Time 0.291 (0.276)	Data 0.000 (0.008)	Loss 0.8594 (0.7575)	Acc@1 74.609 (73.323)	Acc@5 96.094 (98.287)
Epoch: [209][128/196]	Time 0.315 (0.259)	Data 0.000 (0.004)	Loss 0.7551 (0.7680)	Acc@1 73.438 (72.998)	Acc@5 98.047 (98.207)
Epoch: [209][192/196]	Time 0.309 (0.274)	Data 0.000 (0.003)	Loss 0.7741 (0.7643)	Acc@1 73.828 (73.104)	Acc@5 97.656 (98.207)
after train
test acc: 63.46
Epoche: [210/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [210][0/196]	Time 0.321 (0.321)	Data 0.465 (0.465)	Loss 0.7543 (0.7543)	Acc@1 73.828 (73.828)	Acc@5 97.266 (97.266)
Epoch: [210][64/196]	Time 0.308 (0.296)	Data 0.000 (0.007)	Loss 0.7384 (0.7610)	Acc@1 70.703 (72.806)	Acc@5 98.438 (98.197)
Epoch: [210][128/196]	Time 0.289 (0.273)	Data 0.000 (0.004)	Loss 0.8571 (0.7611)	Acc@1 72.656 (72.953)	Acc@5 96.094 (98.253)
Epoch: [210][192/196]	Time 0.287 (0.282)	Data 0.000 (0.003)	Loss 0.7996 (0.7616)	Acc@1 73.438 (73.019)	Acc@5 98.828 (98.247)
after train
test acc: 53.41
Epoche: [211/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [211][0/196]	Time 0.292 (0.292)	Data 0.510 (0.510)	Loss 0.7663 (0.7663)	Acc@1 72.266 (72.266)	Acc@5 98.438 (98.438)
Epoch: [211][64/196]	Time 0.292 (0.305)	Data 0.000 (0.008)	Loss 0.6955 (0.7524)	Acc@1 75.781 (73.462)	Acc@5 97.656 (98.257)
Epoch: [211][128/196]	Time 0.286 (0.279)	Data 0.000 (0.004)	Loss 0.7833 (0.7574)	Acc@1 71.875 (73.253)	Acc@5 98.047 (98.180)
Epoch: [211][192/196]	Time 0.301 (0.280)	Data 0.000 (0.003)	Loss 0.8427 (0.7595)	Acc@1 69.531 (73.132)	Acc@5 98.438 (98.182)
after train
test acc: 61.13
Epoche: [212/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [212][0/196]	Time 0.315 (0.315)	Data 0.482 (0.482)	Loss 0.6565 (0.6565)	Acc@1 75.781 (75.781)	Acc@5 99.219 (99.219)
Epoch: [212][64/196]	Time 0.301 (0.299)	Data 0.000 (0.008)	Loss 0.8836 (0.7569)	Acc@1 70.312 (73.684)	Acc@5 96.484 (98.221)
Epoch: [212][128/196]	Time 0.288 (0.272)	Data 0.000 (0.004)	Loss 0.7848 (0.7552)	Acc@1 73.438 (73.768)	Acc@5 97.656 (98.186)
Epoch: [212][192/196]	Time 0.199 (0.281)	Data 0.000 (0.003)	Loss 0.7182 (0.7568)	Acc@1 72.266 (73.492)	Acc@5 98.438 (98.162)
after train
test acc: 33.61
Epoche: [213/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [213][0/196]	Time 0.378 (0.378)	Data 0.345 (0.345)	Loss 0.8675 (0.8675)	Acc@1 68.750 (68.750)	Acc@5 97.656 (97.656)
Epoch: [213][64/196]	Time 0.243 (0.302)	Data 0.000 (0.006)	Loss 0.7804 (0.7568)	Acc@1 72.266 (73.143)	Acc@5 99.219 (98.323)
Epoch: [213][128/196]	Time 0.255 (0.271)	Data 0.000 (0.003)	Loss 0.6527 (0.7508)	Acc@1 77.344 (73.389)	Acc@5 98.047 (98.232)
Epoch: [213][192/196]	Time 0.317 (0.280)	Data 0.000 (0.002)	Loss 0.7676 (0.7529)	Acc@1 75.000 (73.438)	Acc@5 98.828 (98.249)
after train
test acc: 50.83
Epoche: [214/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [214][0/196]	Time 0.307 (0.307)	Data 0.504 (0.504)	Loss 0.7517 (0.7517)	Acc@1 75.000 (75.000)	Acc@5 98.828 (98.828)
Epoch: [214][64/196]	Time 0.298 (0.280)	Data 0.000 (0.008)	Loss 0.7566 (0.7543)	Acc@1 74.609 (73.516)	Acc@5 98.438 (98.287)
Epoch: [214][128/196]	Time 0.271 (0.266)	Data 0.000 (0.004)	Loss 0.7151 (0.7597)	Acc@1 74.219 (73.307)	Acc@5 98.047 (98.162)
Epoch: [214][192/196]	Time 0.311 (0.276)	Data 0.000 (0.003)	Loss 0.7635 (0.7600)	Acc@1 71.094 (73.191)	Acc@5 99.219 (98.185)
after train
test acc: 66.8
Epoche: [215/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [215][0/196]	Time 0.312 (0.312)	Data 0.566 (0.566)	Loss 0.8213 (0.8213)	Acc@1 69.922 (69.922)	Acc@5 98.828 (98.828)
Epoch: [215][64/196]	Time 0.310 (0.295)	Data 0.000 (0.009)	Loss 0.8806 (0.7608)	Acc@1 68.359 (73.185)	Acc@5 97.656 (98.107)
Epoch: [215][128/196]	Time 0.300 (0.267)	Data 0.000 (0.005)	Loss 0.6856 (0.7516)	Acc@1 75.391 (73.628)	Acc@5 98.828 (98.241)
Epoch: [215][192/196]	Time 0.312 (0.274)	Data 0.000 (0.003)	Loss 0.7718 (0.7490)	Acc@1 69.922 (73.656)	Acc@5 98.828 (98.292)
after train
test acc: 58.66
Epoche: [216/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [216][0/196]	Time 0.342 (0.342)	Data 0.367 (0.367)	Loss 0.7821 (0.7821)	Acc@1 71.094 (71.094)	Acc@5 98.438 (98.438)
Epoch: [216][64/196]	Time 0.306 (0.291)	Data 0.000 (0.006)	Loss 0.9244 (0.7593)	Acc@1 69.922 (73.666)	Acc@5 98.047 (98.119)
Epoch: [216][128/196]	Time 0.287 (0.264)	Data 0.000 (0.003)	Loss 0.7458 (0.7484)	Acc@1 73.438 (73.771)	Acc@5 98.828 (98.235)
Epoch: [216][192/196]	Time 0.300 (0.272)	Data 0.000 (0.002)	Loss 0.8133 (0.7462)	Acc@1 66.797 (73.705)	Acc@5 98.047 (98.284)
after train
test acc: 50.51
Epoche: [217/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [217][0/196]	Time 0.286 (0.286)	Data 0.603 (0.603)	Loss 0.7774 (0.7774)	Acc@1 74.219 (74.219)	Acc@5 98.438 (98.438)
Epoch: [217][64/196]	Time 0.214 (0.291)	Data 0.000 (0.010)	Loss 0.6880 (0.7598)	Acc@1 77.344 (73.245)	Acc@5 96.875 (98.071)
Epoch: [217][128/196]	Time 0.298 (0.270)	Data 0.000 (0.005)	Loss 0.7914 (0.7475)	Acc@1 75.000 (73.819)	Acc@5 98.047 (98.156)
Epoch: [217][192/196]	Time 0.314 (0.280)	Data 0.000 (0.003)	Loss 0.6169 (0.7469)	Acc@1 77.734 (73.820)	Acc@5 99.609 (98.197)
after train
test acc: 65.75
Epoche: [218/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [218][0/196]	Time 0.362 (0.362)	Data 0.442 (0.442)	Loss 0.6918 (0.6918)	Acc@1 75.391 (75.391)	Acc@5 98.438 (98.438)
Epoch: [218][64/196]	Time 0.316 (0.300)	Data 0.000 (0.007)	Loss 0.7088 (0.7302)	Acc@1 75.391 (74.345)	Acc@5 98.828 (98.359)
Epoch: [218][128/196]	Time 0.278 (0.277)	Data 0.000 (0.004)	Loss 0.7448 (0.7401)	Acc@1 74.219 (74.146)	Acc@5 97.266 (98.259)
Epoch: [218][192/196]	Time 0.256 (0.287)	Data 0.000 (0.003)	Loss 0.7328 (0.7429)	Acc@1 75.391 (74.006)	Acc@5 98.828 (98.310)
after train
test acc: 26.0
Epoche: [219/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [219][0/196]	Time 0.355 (0.355)	Data 0.481 (0.481)	Loss 0.6647 (0.6647)	Acc@1 77.734 (77.734)	Acc@5 99.219 (99.219)
Epoch: [219][64/196]	Time 0.217 (0.287)	Data 0.000 (0.008)	Loss 0.7267 (0.7364)	Acc@1 74.609 (74.285)	Acc@5 98.828 (98.203)
Epoch: [219][128/196]	Time 0.317 (0.267)	Data 0.000 (0.004)	Loss 0.7580 (0.7327)	Acc@1 71.875 (74.391)	Acc@5 98.828 (98.244)
Epoch: [219][192/196]	Time 0.203 (0.279)	Data 0.000 (0.003)	Loss 0.7668 (0.7357)	Acc@1 72.656 (74.288)	Acc@5 98.438 (98.265)
after train
test acc: 60.59
Epoche: [220/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [220][0/196]	Time 0.314 (0.314)	Data 0.613 (0.613)	Loss 0.7526 (0.7526)	Acc@1 73.438 (73.438)	Acc@5 98.047 (98.047)
Epoch: [220][64/196]	Time 0.304 (0.296)	Data 0.000 (0.010)	Loss 0.7662 (0.7365)	Acc@1 73.828 (74.165)	Acc@5 96.875 (98.287)
Epoch: [220][128/196]	Time 0.305 (0.272)	Data 0.000 (0.005)	Loss 0.8240 (0.7390)	Acc@1 72.656 (73.973)	Acc@5 98.047 (98.271)
Epoch: [220][192/196]	Time 0.267 (0.276)	Data 0.000 (0.003)	Loss 0.7459 (0.7412)	Acc@1 73.828 (73.887)	Acc@5 98.438 (98.255)
after train
test acc: 24.68
Epoche: [221/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [221][0/196]	Time 0.370 (0.370)	Data 0.387 (0.387)	Loss 0.7437 (0.7437)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [221][64/196]	Time 0.295 (0.288)	Data 0.000 (0.006)	Loss 0.6735 (0.7323)	Acc@1 75.781 (74.231)	Acc@5 98.828 (98.215)
Epoch: [221][128/196]	Time 0.301 (0.265)	Data 0.000 (0.003)	Loss 0.6175 (0.7347)	Acc@1 80.469 (74.176)	Acc@5 98.438 (98.292)
Epoch: [221][192/196]	Time 0.312 (0.276)	Data 0.000 (0.002)	Loss 0.6904 (0.7345)	Acc@1 73.047 (74.205)	Acc@5 98.828 (98.265)
after train
test acc: 62.29
Epoche: [222/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [222][0/196]	Time 0.313 (0.313)	Data 0.606 (0.606)	Loss 0.7619 (0.7619)	Acc@1 75.391 (75.391)	Acc@5 98.047 (98.047)
Epoch: [222][64/196]	Time 0.310 (0.286)	Data 0.000 (0.010)	Loss 0.6651 (0.7315)	Acc@1 75.781 (74.285)	Acc@5 98.438 (98.143)
Epoch: [222][128/196]	Time 0.300 (0.262)	Data 0.000 (0.005)	Loss 0.7617 (0.7342)	Acc@1 71.484 (74.222)	Acc@5 98.828 (98.244)
Epoch: [222][192/196]	Time 0.303 (0.275)	Data 0.000 (0.003)	Loss 0.6634 (0.7337)	Acc@1 79.297 (74.223)	Acc@5 98.828 (98.314)
after train
test acc: 62.32
Epoche: [223/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [223][0/196]	Time 0.309 (0.309)	Data 0.410 (0.410)	Loss 0.9316 (0.9316)	Acc@1 68.750 (68.750)	Acc@5 98.438 (98.438)
Epoch: [223][64/196]	Time 0.268 (0.291)	Data 0.000 (0.007)	Loss 0.7286 (0.7387)	Acc@1 72.656 (74.309)	Acc@5 98.438 (98.323)
Epoch: [223][128/196]	Time 0.301 (0.272)	Data 0.000 (0.004)	Loss 0.7157 (0.7415)	Acc@1 75.000 (73.943)	Acc@5 98.438 (98.259)
Epoch: [223][192/196]	Time 0.277 (0.280)	Data 0.000 (0.002)	Loss 0.6573 (0.7383)	Acc@1 76.562 (74.028)	Acc@5 98.438 (98.243)
after train
test acc: 51.73
Epoche: [224/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [224][0/196]	Time 0.376 (0.376)	Data 0.657 (0.657)	Loss 0.7035 (0.7035)	Acc@1 72.266 (72.266)	Acc@5 98.438 (98.438)
Epoch: [224][64/196]	Time 0.108 (0.287)	Data 0.000 (0.010)	Loss 0.8362 (0.7277)	Acc@1 71.094 (74.231)	Acc@5 97.266 (98.311)
Epoch: [224][128/196]	Time 0.290 (0.275)	Data 0.000 (0.005)	Loss 0.6729 (0.7298)	Acc@1 76.562 (74.376)	Acc@5 98.438 (98.295)
Epoch: [224][192/196]	Time 0.336 (0.279)	Data 0.000 (0.004)	Loss 0.7241 (0.7359)	Acc@1 75.391 (74.172)	Acc@5 96.875 (98.241)
after train
test acc: 15.63
Epoche: [225/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [225][0/196]	Time 0.264 (0.264)	Data 0.531 (0.531)	Loss 0.6973 (0.6973)	Acc@1 74.609 (74.609)	Acc@5 98.828 (98.828)
Epoch: [225][64/196]	Time 0.181 (0.276)	Data 0.000 (0.009)	Loss 0.6811 (0.7185)	Acc@1 79.688 (75.084)	Acc@5 98.438 (98.311)
Epoch: [225][128/196]	Time 0.281 (0.265)	Data 0.000 (0.004)	Loss 0.7510 (0.7252)	Acc@1 73.047 (74.600)	Acc@5 98.438 (98.229)
Epoch: [225][192/196]	Time 0.232 (0.277)	Data 0.000 (0.003)	Loss 0.7043 (0.7264)	Acc@1 77.734 (74.563)	Acc@5 97.266 (98.227)
after train
test acc: 67.45
Epoche: [226/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [226][0/196]	Time 0.348 (0.348)	Data 0.503 (0.503)	Loss 0.7784 (0.7784)	Acc@1 72.656 (72.656)	Acc@5 96.875 (96.875)
Epoch: [226][64/196]	Time 0.223 (0.261)	Data 0.000 (0.008)	Loss 0.6529 (0.7344)	Acc@1 76.562 (74.081)	Acc@5 99.609 (98.311)
Epoch: [226][128/196]	Time 0.315 (0.267)	Data 0.000 (0.004)	Loss 0.6054 (0.7330)	Acc@1 80.078 (73.977)	Acc@5 99.219 (98.389)
Epoch: [226][192/196]	Time 0.293 (0.276)	Data 0.000 (0.003)	Loss 0.7677 (0.7302)	Acc@1 76.562 (74.215)	Acc@5 96.094 (98.346)
after train
test acc: 67.97
Epoche: [227/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [227][0/196]	Time 0.335 (0.335)	Data 0.544 (0.544)	Loss 0.7730 (0.7730)	Acc@1 75.391 (75.391)	Acc@5 97.266 (97.266)
Epoch: [227][64/196]	Time 0.212 (0.270)	Data 0.000 (0.009)	Loss 0.7049 (0.7328)	Acc@1 73.828 (74.489)	Acc@5 99.609 (98.299)
Epoch: [227][128/196]	Time 0.304 (0.266)	Data 0.000 (0.005)	Loss 0.6566 (0.7284)	Acc@1 78.516 (74.516)	Acc@5 97.656 (98.289)
Epoch: [227][192/196]	Time 0.202 (0.277)	Data 0.000 (0.003)	Loss 0.8167 (0.7263)	Acc@1 72.266 (74.553)	Acc@5 97.656 (98.342)
after train
test acc: 54.2
Epoche: [228/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [228][0/196]	Time 0.345 (0.345)	Data 0.616 (0.616)	Loss 0.6351 (0.6351)	Acc@1 78.125 (78.125)	Acc@5 97.266 (97.266)
Epoch: [228][64/196]	Time 0.269 (0.251)	Data 0.000 (0.010)	Loss 0.7154 (0.7082)	Acc@1 74.219 (74.964)	Acc@5 98.438 (98.305)
Epoch: [228][128/196]	Time 0.267 (0.269)	Data 0.000 (0.005)	Loss 0.7342 (0.7111)	Acc@1 73.438 (74.988)	Acc@5 98.047 (98.389)
Epoch: [228][192/196]	Time 0.308 (0.276)	Data 0.000 (0.004)	Loss 0.7663 (0.7137)	Acc@1 75.391 (74.854)	Acc@5 97.656 (98.440)
after train
test acc: 68.03
Epoche: [229/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [229][0/196]	Time 0.322 (0.322)	Data 0.500 (0.500)	Loss 0.7769 (0.7769)	Acc@1 72.266 (72.266)	Acc@5 98.438 (98.438)
Epoch: [229][64/196]	Time 0.320 (0.250)	Data 0.000 (0.008)	Loss 0.7136 (0.7114)	Acc@1 76.172 (75.240)	Acc@5 99.609 (98.377)
Epoch: [229][128/196]	Time 0.306 (0.272)	Data 0.000 (0.004)	Loss 0.7526 (0.7204)	Acc@1 73.438 (74.979)	Acc@5 99.219 (98.347)
Epoch: [229][192/196]	Time 0.312 (0.281)	Data 0.000 (0.003)	Loss 0.6145 (0.7212)	Acc@1 79.297 (74.838)	Acc@5 99.609 (98.302)
after train
test acc: 66.0
Epoche: [230/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [230][0/196]	Time 0.319 (0.319)	Data 0.467 (0.467)	Loss 0.7506 (0.7506)	Acc@1 70.703 (70.703)	Acc@5 98.438 (98.438)
Epoch: [230][64/196]	Time 0.260 (0.244)	Data 0.000 (0.007)	Loss 0.7423 (0.7231)	Acc@1 71.875 (74.393)	Acc@5 98.047 (98.353)
Epoch: [230][128/196]	Time 0.262 (0.270)	Data 0.000 (0.004)	Loss 0.7084 (0.7197)	Acc@1 72.656 (74.749)	Acc@5 99.219 (98.335)
Epoch: [230][192/196]	Time 0.287 (0.274)	Data 0.000 (0.003)	Loss 0.7419 (0.7174)	Acc@1 71.094 (74.711)	Acc@5 98.438 (98.314)
after train
test acc: 29.83
Epoche: [231/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [231][0/196]	Time 0.323 (0.323)	Data 0.525 (0.525)	Loss 0.6270 (0.6270)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [231][64/196]	Time 0.258 (0.250)	Data 0.000 (0.009)	Loss 0.7475 (0.7199)	Acc@1 73.047 (74.730)	Acc@5 98.438 (98.474)
Epoch: [231][128/196]	Time 0.262 (0.266)	Data 0.000 (0.004)	Loss 0.7328 (0.7185)	Acc@1 74.219 (74.955)	Acc@5 98.047 (98.389)
Epoch: [231][192/196]	Time 0.215 (0.270)	Data 0.000 (0.003)	Loss 0.6571 (0.7209)	Acc@1 79.688 (74.802)	Acc@5 98.047 (98.379)
after train
test acc: 67.56
Epoche: [232/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [232][0/196]	Time 0.335 (0.335)	Data 0.551 (0.551)	Loss 0.6151 (0.6151)	Acc@1 79.297 (79.297)	Acc@5 98.828 (98.828)
Epoch: [232][64/196]	Time 0.305 (0.253)	Data 0.000 (0.009)	Loss 0.7001 (0.7208)	Acc@1 75.000 (74.778)	Acc@5 98.438 (98.395)
Epoch: [232][128/196]	Time 0.278 (0.270)	Data 0.000 (0.005)	Loss 0.5888 (0.7131)	Acc@1 76.953 (74.961)	Acc@5 99.219 (98.441)
Epoch: [232][192/196]	Time 0.265 (0.274)	Data 0.000 (0.003)	Loss 0.6208 (0.7136)	Acc@1 78.906 (75.002)	Acc@5 98.828 (98.448)
after train
test acc: 70.32
Epoche: [233/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [233][0/196]	Time 0.348 (0.348)	Data 0.435 (0.435)	Loss 0.6291 (0.6291)	Acc@1 75.391 (75.391)	Acc@5 98.828 (98.828)
Epoch: [233][64/196]	Time 0.307 (0.254)	Data 0.000 (0.007)	Loss 0.7674 (0.7147)	Acc@1 71.875 (74.772)	Acc@5 98.438 (98.401)
Epoch: [233][128/196]	Time 0.291 (0.269)	Data 0.000 (0.004)	Loss 0.7328 (0.7154)	Acc@1 75.391 (74.930)	Acc@5 98.828 (98.307)
Epoch: [233][192/196]	Time 0.226 (0.278)	Data 0.000 (0.003)	Loss 0.7982 (0.7147)	Acc@1 71.094 (74.887)	Acc@5 99.219 (98.328)
after train
test acc: 63.81
Epoche: [234/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [234][0/196]	Time 0.343 (0.343)	Data 0.512 (0.512)	Loss 0.7320 (0.7320)	Acc@1 73.047 (73.047)	Acc@5 98.828 (98.828)
Epoch: [234][64/196]	Time 0.312 (0.258)	Data 0.000 (0.008)	Loss 0.6605 (0.7113)	Acc@1 75.781 (75.186)	Acc@5 98.438 (98.365)
Epoch: [234][128/196]	Time 0.307 (0.280)	Data 0.000 (0.004)	Loss 0.6082 (0.7138)	Acc@1 77.734 (75.154)	Acc@5 98.438 (98.341)
Epoch: [234][192/196]	Time 0.326 (0.289)	Data 0.000 (0.003)	Loss 0.7037 (0.7130)	Acc@1 76.953 (75.219)	Acc@5 98.438 (98.365)
after train
test acc: 46.26
Epoche: [235/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [235][0/196]	Time 0.317 (0.317)	Data 0.563 (0.563)	Loss 0.7149 (0.7149)	Acc@1 74.219 (74.219)	Acc@5 98.828 (98.828)
Epoch: [235][64/196]	Time 0.284 (0.238)	Data 0.000 (0.009)	Loss 0.7220 (0.6940)	Acc@1 78.906 (75.463)	Acc@5 97.656 (98.425)
Epoch: [235][128/196]	Time 0.274 (0.265)	Data 0.000 (0.005)	Loss 0.6490 (0.7104)	Acc@1 77.734 (75.121)	Acc@5 98.828 (98.353)
Epoch: [235][192/196]	Time 0.287 (0.273)	Data 0.000 (0.003)	Loss 0.6967 (0.7114)	Acc@1 72.266 (75.136)	Acc@5 98.438 (98.381)
after train
test acc: 70.27
Epoche: [236/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [236][0/196]	Time 0.346 (0.346)	Data 0.550 (0.550)	Loss 0.6697 (0.6697)	Acc@1 77.734 (77.734)	Acc@5 98.047 (98.047)
Epoch: [236][64/196]	Time 0.319 (0.255)	Data 0.000 (0.009)	Loss 0.7733 (0.7205)	Acc@1 73.828 (75.132)	Acc@5 96.875 (98.468)
Epoch: [236][128/196]	Time 0.309 (0.272)	Data 0.000 (0.005)	Loss 0.6785 (0.7140)	Acc@1 76.562 (75.079)	Acc@5 98.828 (98.459)
Epoch: [236][192/196]	Time 0.311 (0.277)	Data 0.000 (0.003)	Loss 0.6339 (0.7092)	Acc@1 77.344 (75.184)	Acc@5 98.828 (98.456)
after train
test acc: 65.63
Epoche: [237/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [237][0/196]	Time 0.402 (0.402)	Data 0.609 (0.609)	Loss 0.7169 (0.7169)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [237][64/196]	Time 0.278 (0.258)	Data 0.000 (0.010)	Loss 0.6349 (0.6937)	Acc@1 76.953 (75.805)	Acc@5 99.219 (98.516)
Epoch: [237][128/196]	Time 0.290 (0.276)	Data 0.000 (0.005)	Loss 0.7253 (0.6989)	Acc@1 73.438 (75.500)	Acc@5 99.609 (98.486)
Epoch: [237][192/196]	Time 0.305 (0.276)	Data 0.000 (0.003)	Loss 0.9021 (0.7037)	Acc@1 68.750 (75.447)	Acc@5 98.047 (98.438)
after train
test acc: 58.39
Epoche: [238/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [238][0/196]	Time 0.313 (0.313)	Data 0.395 (0.395)	Loss 0.6948 (0.6948)	Acc@1 78.516 (78.516)	Acc@5 97.656 (97.656)
Epoch: [238][64/196]	Time 0.305 (0.249)	Data 0.000 (0.006)	Loss 0.7800 (0.7192)	Acc@1 74.219 (74.838)	Acc@5 96.875 (98.293)
Epoch: [238][128/196]	Time 0.270 (0.274)	Data 0.000 (0.003)	Loss 0.8325 (0.7036)	Acc@1 74.609 (75.260)	Acc@5 96.875 (98.392)
Epoch: [238][192/196]	Time 0.286 (0.277)	Data 0.000 (0.002)	Loss 0.7407 (0.7040)	Acc@1 73.828 (75.350)	Acc@5 98.828 (98.385)
after train
test acc: 39.94
Epoche: [239/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [239][0/196]	Time 0.387 (0.387)	Data 0.391 (0.391)	Loss 0.6504 (0.6504)	Acc@1 77.344 (77.344)	Acc@5 98.047 (98.047)
Epoch: [239][64/196]	Time 0.275 (0.257)	Data 0.000 (0.006)	Loss 0.8696 (0.7160)	Acc@1 69.531 (74.982)	Acc@5 97.656 (98.383)
Epoch: [239][128/196]	Time 0.300 (0.275)	Data 0.000 (0.003)	Loss 0.7681 (0.7099)	Acc@1 75.781 (75.254)	Acc@5 98.828 (98.413)
Epoch: [239][192/196]	Time 0.289 (0.281)	Data 0.000 (0.002)	Loss 0.6967 (0.7046)	Acc@1 76.562 (75.409)	Acc@5 98.828 (98.421)
after train
test acc: 61.64
Epoche: [240/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [240][0/196]	Time 0.235 (0.235)	Data 0.405 (0.405)	Loss 0.7017 (0.7017)	Acc@1 76.953 (76.953)	Acc@5 98.828 (98.828)
Epoch: [240][64/196]	Time 0.302 (0.248)	Data 0.000 (0.007)	Loss 0.7712 (0.6978)	Acc@1 75.000 (75.571)	Acc@5 99.219 (98.480)
Epoch: [240][128/196]	Time 0.310 (0.277)	Data 0.000 (0.003)	Loss 0.7261 (0.6946)	Acc@1 75.781 (75.636)	Acc@5 98.438 (98.534)
Epoch: [240][192/196]	Time 0.309 (0.283)	Data 0.000 (0.002)	Loss 0.7033 (0.6954)	Acc@1 74.609 (75.532)	Acc@5 98.828 (98.525)
after train
test acc: 70.92
[INFO] Storing checkpoint...
Max memory: 26.1181952
