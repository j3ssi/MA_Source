no display found. Using non-interactive Agg backend
[5, 5, 5]
[16, 32, 64]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 180; reset: False; start epoche: 1; test: True pathtoModell: None; checkpoint: ./output/experimente4/deeperX; saveModell: False; LR: 0.1
random number: 954
Files already downloaded and verified

width: 16
module: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (3, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (4, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (5, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (6, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (7, 0
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=64, out_features=10, bias=True)
width: 32
module: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 16
conv gefunden
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (9, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (10, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (11, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (12, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (13, 0
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=64, out_features=10, bias=True)
width: 64
module: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 32
conv gefunden
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (15, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (16, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (17, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (18, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (19, 0
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=64, out_features=10, bias=True)
stagesI: {16: [(3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0)], 32: [(10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 0)], 64: [(16, 0), (17, 0), (18, 0), (19, 0), (21, None)]}
stagesO: {16: [(0, None), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0)], 32: [(8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0)], 64: [(14, 0), (15, 0), (16, 0), (17, 0), (18, 0), (19, 0)]}
Modell Erstellung
N2N(
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (15): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (16): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (15): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (16): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
deeper epoch: 90
Epoche: [1/180]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.
  warnings.warn("Setting attributes on ParameterList is not supported.")
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [1][0/196]	Time 0.114 (0.114)	Data 0.448 (0.448)	Loss 2.7244 (2.7244)	Acc@1 11.719 (11.719)	Acc@5 47.266 (47.266)
Epoch: [1][64/196]	Time 0.079 (0.081)	Data 0.000 (0.007)	Loss 1.7473 (2.0082)	Acc@1 33.984 (24.982)	Acc@5 89.453 (78.672)
Epoch: [1][128/196]	Time 0.080 (0.080)	Data 0.000 (0.004)	Loss 1.5338 (1.8290)	Acc@1 41.406 (31.238)	Acc@5 90.234 (83.881)
Epoch: [1][192/196]	Time 0.078 (0.080)	Data 0.000 (0.003)	Loss 1.3096 (1.7107)	Acc@1 49.609 (35.844)	Acc@5 94.531 (86.607)
after train
n1: 1 for:
wAcc: 32.86
test acc: 32.86
Epoche: [2/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [2][0/196]	Time 0.096 (0.096)	Data 0.364 (0.364)	Loss 1.4478 (1.4478)	Acc@1 46.875 (46.875)	Acc@5 92.188 (92.188)
Epoch: [2][64/196]	Time 0.080 (0.087)	Data 0.000 (0.006)	Loss 1.2430 (1.3242)	Acc@1 53.125 (51.046)	Acc@5 93.750 (93.654)
Epoch: [2][128/196]	Time 0.079 (0.083)	Data 0.000 (0.003)	Loss 1.1921 (1.2599)	Acc@1 57.812 (53.897)	Acc@5 93.750 (94.549)
Epoch: [2][192/196]	Time 0.097 (0.083)	Data 0.000 (0.002)	Loss 0.9972 (1.2121)	Acc@1 61.328 (55.912)	Acc@5 96.484 (94.914)
after train
n1: 2 for:
wAcc: 32.86
test acc: 48.96
Epoche: [3/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [3][0/196]	Time 0.112 (0.112)	Data 0.371 (0.371)	Loss 1.2028 (1.2028)	Acc@1 52.344 (52.344)	Acc@5 96.094 (96.094)
Epoch: [3][64/196]	Time 0.079 (0.087)	Data 0.000 (0.006)	Loss 1.0361 (1.0479)	Acc@1 64.844 (62.344)	Acc@5 95.703 (96.593)
Epoch: [3][128/196]	Time 0.080 (0.083)	Data 0.000 (0.003)	Loss 0.9827 (1.0301)	Acc@1 64.062 (63.000)	Acc@5 96.875 (96.690)
Epoch: [3][192/196]	Time 0.097 (0.088)	Data 0.000 (0.002)	Loss 0.8506 (1.0134)	Acc@1 70.312 (63.573)	Acc@5 99.609 (96.782)
after train
n1: 3 for:
wAcc: 40.91
test acc: 57.64
Epoche: [4/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [4][0/196]	Time 0.117 (0.117)	Data 0.356 (0.356)	Loss 0.8564 (0.8564)	Acc@1 66.406 (66.406)	Acc@5 97.656 (97.656)
Epoch: [4][64/196]	Time 0.079 (0.080)	Data 0.000 (0.006)	Loss 1.0162 (0.9201)	Acc@1 60.547 (67.338)	Acc@5 96.484 (97.115)
Epoch: [4][128/196]	Time 0.080 (0.080)	Data 0.000 (0.003)	Loss 0.9252 (0.9078)	Acc@1 63.672 (67.739)	Acc@5 98.438 (97.287)
Epoch: [4][192/196]	Time 0.079 (0.080)	Data 0.000 (0.002)	Loss 0.9919 (0.8943)	Acc@1 60.547 (68.110)	Acc@5 97.656 (97.460)
after train
n1: 4 for:
wAcc: 46.635999999999996
test acc: 58.2
Epoche: [5/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [5][0/196]	Time 0.122 (0.122)	Data 0.338 (0.338)	Loss 0.9117 (0.9117)	Acc@1 64.453 (64.453)	Acc@5 98.828 (98.828)
Epoch: [5][64/196]	Time 0.099 (0.086)	Data 0.000 (0.005)	Loss 0.7504 (0.8301)	Acc@1 73.047 (70.817)	Acc@5 98.047 (97.891)
Epoch: [5][128/196]	Time 0.079 (0.085)	Data 0.000 (0.003)	Loss 0.8581 (0.8191)	Acc@1 69.141 (71.115)	Acc@5 98.047 (98.023)
Epoch: [5][192/196]	Time 0.079 (0.083)	Data 0.000 (0.002)	Loss 0.6711 (0.8063)	Acc@1 78.516 (71.673)	Acc@5 98.438 (97.998)
after train
n1: 5 for:
wAcc: 49.198518518518526
test acc: 69.06
Epoche: [6/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [6][0/196]	Time 0.145 (0.145)	Data 0.340 (0.340)	Loss 0.7389 (0.7389)	Acc@1 75.391 (75.391)	Acc@5 98.438 (98.438)
Epoch: [6][64/196]	Time 0.099 (0.097)	Data 0.000 (0.006)	Loss 0.7294 (0.7549)	Acc@1 73.047 (73.389)	Acc@5 98.438 (98.221)
Epoch: [6][128/196]	Time 0.080 (0.091)	Data 0.000 (0.003)	Loss 0.8035 (0.7394)	Acc@1 69.531 (74.195)	Acc@5 98.828 (98.341)
Epoch: [6][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.7206 (0.7338)	Acc@1 78.125 (74.466)	Acc@5 98.438 (98.322)
after train
n1: 6 for:
wAcc: 53.662915451895046
test acc: 67.39
Epoche: [7/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [7][0/196]	Time 0.126 (0.126)	Data 0.389 (0.389)	Loss 0.6248 (0.6248)	Acc@1 77.344 (77.344)	Acc@5 98.438 (98.438)
Epoch: [7][64/196]	Time 0.079 (0.080)	Data 0.000 (0.006)	Loss 0.6511 (0.7142)	Acc@1 76.562 (75.012)	Acc@5 99.219 (98.407)
Epoch: [7][128/196]	Time 0.080 (0.080)	Data 0.000 (0.003)	Loss 0.6178 (0.6979)	Acc@1 76.953 (75.536)	Acc@5 98.438 (98.513)
Epoch: [7][192/196]	Time 0.079 (0.080)	Data 0.000 (0.002)	Loss 0.7306 (0.6927)	Acc@1 72.266 (75.781)	Acc@5 99.219 (98.523)
after train
n1: 7 for:
wAcc: 55.730488281250004
test acc: 73.39
Epoche: [8/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [8][0/196]	Time 0.095 (0.095)	Data 0.385 (0.385)	Loss 0.5806 (0.5806)	Acc@1 80.469 (80.469)	Acc@5 99.609 (99.609)
Epoch: [8][64/196]	Time 0.079 (0.080)	Data 0.000 (0.006)	Loss 0.6378 (0.6509)	Acc@1 81.250 (77.025)	Acc@5 96.875 (98.642)
Epoch: [8][128/196]	Time 0.080 (0.080)	Data 0.000 (0.003)	Loss 0.6348 (0.6492)	Acc@1 78.516 (77.238)	Acc@5 97.656 (98.634)
Epoch: [8][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.6611 (0.6501)	Acc@1 76.562 (77.241)	Acc@5 98.047 (98.616)
after train
n1: 8 for:
wAcc: 58.3841895901897
test acc: 70.84
Epoche: [9/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [9][0/196]	Time 0.127 (0.127)	Data 0.371 (0.371)	Loss 0.5966 (0.5966)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [9][64/196]	Time 0.099 (0.099)	Data 0.000 (0.006)	Loss 0.4737 (0.6203)	Acc@1 83.984 (78.732)	Acc@5 98.828 (98.852)
Epoch: [9][128/196]	Time 0.080 (0.094)	Data 0.000 (0.003)	Loss 0.6366 (0.6228)	Acc@1 78.125 (78.561)	Acc@5 99.609 (98.819)
Epoch: [9][192/196]	Time 0.098 (0.091)	Data 0.000 (0.002)	Loss 0.5813 (0.6241)	Acc@1 78.516 (78.518)	Acc@5 98.438 (98.763)
after train
n1: 9 for:
wAcc: 59.61145856
test acc: 74.88
Epoche: [10/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [10][0/196]	Time 0.111 (0.111)	Data 0.404 (0.404)	Loss 0.5622 (0.5622)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [10][64/196]	Time 0.080 (0.081)	Data 0.000 (0.006)	Loss 0.6221 (0.5934)	Acc@1 78.516 (79.351)	Acc@5 99.609 (98.972)
Epoch: [10][128/196]	Time 0.080 (0.080)	Data 0.000 (0.003)	Loss 0.5630 (0.6005)	Acc@1 80.859 (79.261)	Acc@5 98.828 (98.907)
Epoch: [10][192/196]	Time 0.079 (0.080)	Data 0.000 (0.002)	Loss 0.6078 (0.6004)	Acc@1 79.297 (79.208)	Acc@5 99.219 (98.925)
after train
n1: 10 for:
wAcc: 61.230325732200455
test acc: 69.94
Epoche: [11/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [11][0/196]	Time 0.115 (0.115)	Data 0.382 (0.382)	Loss 0.5611 (0.5611)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [11][64/196]	Time 0.081 (0.080)	Data 0.000 (0.006)	Loss 0.5185 (0.5761)	Acc@1 80.859 (79.880)	Acc@5 99.219 (98.840)
Epoch: [11][128/196]	Time 0.080 (0.082)	Data 0.000 (0.003)	Loss 0.6260 (0.5754)	Acc@1 77.734 (79.836)	Acc@5 98.047 (98.871)
Epoch: [11][192/196]	Time 0.079 (0.082)	Data 0.000 (0.002)	Loss 0.4818 (0.5755)	Acc@1 84.375 (79.975)	Acc@5 99.609 (98.887)
after train
n1: 11 for:
wAcc: 61.572166390016136
test acc: 77.72
Epoche: [12/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [12][0/196]	Time 0.126 (0.126)	Data 0.379 (0.379)	Loss 0.5672 (0.5672)	Acc@1 80.859 (80.859)	Acc@5 99.219 (99.219)
Epoch: [12][64/196]	Time 0.098 (0.088)	Data 0.000 (0.006)	Loss 0.5505 (0.5729)	Acc@1 82.422 (80.126)	Acc@5 100.000 (98.906)
Epoch: [12][128/196]	Time 0.098 (0.093)	Data 0.000 (0.003)	Loss 0.4778 (0.5604)	Acc@1 83.203 (80.481)	Acc@5 99.609 (99.025)
Epoch: [12][192/196]	Time 0.098 (0.095)	Data 0.000 (0.002)	Loss 0.5000 (0.5622)	Acc@1 80.859 (80.453)	Acc@5 99.219 (99.028)
after train
n1: 12 for:
wAcc: 63.06487896866749
test acc: 76.68
Epoche: [13/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [13][0/196]	Time 0.111 (0.111)	Data 0.377 (0.377)	Loss 0.5844 (0.5844)	Acc@1 78.516 (78.516)	Acc@5 99.609 (99.609)
Epoch: [13][64/196]	Time 0.091 (0.092)	Data 0.000 (0.006)	Loss 0.5824 (0.5420)	Acc@1 80.469 (81.046)	Acc@5 99.219 (99.044)
Epoch: [13][128/196]	Time 0.091 (0.091)	Data 0.000 (0.003)	Loss 0.5201 (0.5484)	Acc@1 82.812 (80.741)	Acc@5 99.609 (99.086)
Epoch: [13][192/196]	Time 0.090 (0.091)	Data 0.000 (0.002)	Loss 0.5789 (0.5486)	Acc@1 81.641 (80.764)	Acc@5 99.609 (99.055)
after train
n1: 13 for:
wAcc: 64.0377398240651
test acc: 73.56
Epoche: [14/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [14][0/196]	Time 0.092 (0.092)	Data 0.446 (0.446)	Loss 0.5491 (0.5491)	Acc@1 78.906 (78.906)	Acc@5 99.609 (99.609)
Epoch: [14][64/196]	Time 0.080 (0.081)	Data 0.000 (0.007)	Loss 0.5731 (0.5221)	Acc@1 80.859 (81.647)	Acc@5 99.609 (99.123)
Epoch: [14][128/196]	Time 0.080 (0.081)	Data 0.000 (0.004)	Loss 0.5846 (0.5311)	Acc@1 80.078 (81.426)	Acc@5 99.609 (99.149)
Epoch: [14][192/196]	Time 0.079 (0.080)	Data 0.000 (0.003)	Loss 0.5997 (0.5301)	Acc@1 78.906 (81.527)	Acc@5 98.828 (99.140)
after train
n1: 14 for:
wAcc: 64.3818786783523
test acc: 72.37
Epoche: [15/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [15][0/196]	Time 0.092 (0.092)	Data 0.337 (0.337)	Loss 0.4810 (0.4810)	Acc@1 80.859 (80.859)	Acc@5 98.828 (98.828)
Epoch: [15][64/196]	Time 0.099 (0.093)	Data 0.000 (0.005)	Loss 0.4662 (0.5250)	Acc@1 84.766 (81.641)	Acc@5 99.609 (99.177)
Epoch: [15][128/196]	Time 0.078 (0.094)	Data 0.000 (0.003)	Loss 0.4404 (0.5226)	Acc@1 83.594 (81.759)	Acc@5 100.000 (99.155)
Epoch: [15][192/196]	Time 0.080 (0.091)	Data 0.000 (0.002)	Loss 0.5299 (0.5241)	Acc@1 83.984 (81.827)	Acc@5 98.828 (99.152)
after train
n1: 15 for:
wAcc: 64.52700747276705
test acc: 74.76
Epoche: [16/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [16][0/196]	Time 0.086 (0.086)	Data 0.444 (0.444)	Loss 0.5120 (0.5120)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [16][64/196]	Time 0.080 (0.080)	Data 0.000 (0.007)	Loss 0.5569 (0.4997)	Acc@1 80.859 (82.770)	Acc@5 99.219 (99.165)
Epoch: [16][128/196]	Time 0.080 (0.080)	Data 0.000 (0.004)	Loss 0.3728 (0.5047)	Acc@1 88.281 (82.604)	Acc@5 99.609 (99.185)
Epoch: [16][192/196]	Time 0.079 (0.080)	Data 0.000 (0.003)	Loss 0.5395 (0.5125)	Acc@1 80.859 (82.286)	Acc@5 98.828 (99.146)
after train
n1: 16 for:
wAcc: 64.94631174231138
test acc: 76.8
Epoche: [17/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [17][0/196]	Time 0.097 (0.097)	Data 0.394 (0.394)	Loss 0.5351 (0.5351)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [17][64/196]	Time 0.080 (0.082)	Data 0.000 (0.006)	Loss 0.5705 (0.4939)	Acc@1 81.250 (82.933)	Acc@5 98.828 (99.237)
Epoch: [17][128/196]	Time 0.080 (0.081)	Data 0.000 (0.003)	Loss 0.5009 (0.4867)	Acc@1 81.641 (83.188)	Acc@5 99.609 (99.228)
Epoch: [17][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.4870 (0.4993)	Acc@1 82.812 (82.721)	Acc@5 99.219 (99.186)
after train
n1: 17 for:
wAcc: 65.52209369520564
test acc: 74.63
Epoche: [18/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [18][0/196]	Time 0.113 (0.113)	Data 0.356 (0.356)	Loss 0.4503 (0.4503)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [18][64/196]	Time 0.081 (0.084)	Data 0.000 (0.006)	Loss 0.5758 (0.4990)	Acc@1 80.859 (82.608)	Acc@5 98.828 (99.237)
Epoch: [18][128/196]	Time 0.079 (0.082)	Data 0.000 (0.003)	Loss 0.4149 (0.4918)	Acc@1 85.938 (82.791)	Acc@5 98.438 (99.240)
Epoch: [18][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.4992 (0.4941)	Acc@1 82.031 (82.839)	Acc@5 98.828 (99.227)
after train
n1: 18 for:
wAcc: 65.76899558508362
test acc: 76.45
Epoche: [19/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [19][0/196]	Time 0.129 (0.129)	Data 0.268 (0.268)	Loss 0.3662 (0.3662)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [19][64/196]	Time 0.080 (0.081)	Data 0.000 (0.004)	Loss 0.5532 (0.4892)	Acc@1 78.125 (83.239)	Acc@5 98.828 (99.279)
Epoch: [19][128/196]	Time 0.081 (0.081)	Data 0.000 (0.002)	Loss 0.4503 (0.4885)	Acc@1 85.547 (83.179)	Acc@5 99.219 (99.246)
Epoch: [19][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.5473 (0.4916)	Acc@1 82.031 (82.978)	Acc@5 99.609 (99.251)
after train
n1: 19 for:
wAcc: 66.16579514217896
test acc: 65.41
Epoche: [20/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [20][0/196]	Time 0.103 (0.103)	Data 0.289 (0.289)	Loss 0.5437 (0.5437)	Acc@1 81.250 (81.250)	Acc@5 98.828 (98.828)
Epoch: [20][64/196]	Time 0.099 (0.082)	Data 0.000 (0.005)	Loss 0.5382 (0.4898)	Acc@1 82.031 (83.365)	Acc@5 99.219 (99.243)
Epoch: [20][128/196]	Time 0.080 (0.081)	Data 0.000 (0.002)	Loss 0.4645 (0.4893)	Acc@1 83.984 (83.167)	Acc@5 99.609 (99.252)
Epoch: [20][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.4852 (0.4878)	Acc@1 82.031 (83.197)	Acc@5 99.219 (99.253)
after train
n1: 20 for:
wAcc: 65.45095977893796
test acc: 68.78
Epoche: [21/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [21][0/196]	Time 0.094 (0.094)	Data 0.379 (0.379)	Loss 0.5137 (0.5137)	Acc@1 82.422 (82.422)	Acc@5 98.828 (98.828)
Epoch: [21][64/196]	Time 0.079 (0.081)	Data 0.000 (0.006)	Loss 0.5579 (0.4639)	Acc@1 78.125 (84.056)	Acc@5 98.438 (99.297)
Epoch: [21][128/196]	Time 0.099 (0.081)	Data 0.000 (0.003)	Loss 0.4647 (0.4698)	Acc@1 83.594 (83.881)	Acc@5 99.609 (99.337)
Epoch: [21][192/196]	Time 0.098 (0.087)	Data 0.000 (0.002)	Loss 0.5460 (0.4725)	Acc@1 81.250 (83.806)	Acc@5 99.609 (99.310)
after train
n1: 21 for:
wAcc: 65.18171491228121
test acc: 75.4
Epoche: [22/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [22][0/196]	Time 0.116 (0.116)	Data 0.361 (0.361)	Loss 0.4251 (0.4251)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [22][64/196]	Time 0.080 (0.082)	Data 0.000 (0.006)	Loss 0.4380 (0.4614)	Acc@1 84.375 (84.105)	Acc@5 99.609 (99.297)
Epoch: [22][128/196]	Time 0.091 (0.087)	Data 0.000 (0.003)	Loss 0.4839 (0.4684)	Acc@1 84.375 (83.806)	Acc@5 98.828 (99.249)
Epoch: [22][192/196]	Time 0.090 (0.086)	Data 0.000 (0.002)	Loss 0.4148 (0.4719)	Acc@1 85.938 (83.654)	Acc@5 100.000 (99.255)
after train
n1: 22 for:
wAcc: 65.54156445738278
test acc: 77.58
Epoche: [23/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [23][0/196]	Time 0.118 (0.118)	Data 0.363 (0.363)	Loss 0.4104 (0.4104)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [23][64/196]	Time 0.080 (0.091)	Data 0.000 (0.006)	Loss 0.4314 (0.4742)	Acc@1 86.328 (83.780)	Acc@5 99.219 (99.177)
Epoch: [23][128/196]	Time 0.079 (0.086)	Data 0.000 (0.003)	Loss 0.4931 (0.4667)	Acc@1 84.375 (84.060)	Acc@5 99.609 (99.234)
Epoch: [23][192/196]	Time 0.079 (0.085)	Data 0.000 (0.002)	Loss 0.4956 (0.4673)	Acc@1 82.812 (84.001)	Acc@5 98.828 (99.245)
after train
n1: 23 for:
wAcc: 66.02994611501025
test acc: 77.28
Epoche: [24/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [24][0/196]	Time 0.089 (0.089)	Data 0.441 (0.441)	Loss 0.5988 (0.5988)	Acc@1 80.078 (80.078)	Acc@5 99.219 (99.219)
Epoch: [24][64/196]	Time 0.091 (0.089)	Data 0.000 (0.007)	Loss 0.3813 (0.4517)	Acc@1 87.109 (84.189)	Acc@5 99.219 (99.387)
Epoch: [24][128/196]	Time 0.080 (0.089)	Data 0.000 (0.004)	Loss 0.5086 (0.4495)	Acc@1 81.641 (84.151)	Acc@5 99.609 (99.379)
Epoch: [24][192/196]	Time 0.079 (0.089)	Data 0.000 (0.003)	Loss 0.4752 (0.4597)	Acc@1 83.594 (83.948)	Acc@5 99.219 (99.330)
after train
n1: 24 for:
wAcc: 66.42347783517094
test acc: 74.83
Epoche: [25/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [25][0/196]	Time 0.107 (0.107)	Data 0.356 (0.356)	Loss 0.3948 (0.3948)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [25][64/196]	Time 0.091 (0.088)	Data 0.000 (0.006)	Loss 0.3649 (0.4434)	Acc@1 86.328 (84.784)	Acc@5 100.000 (99.369)
Epoch: [25][128/196]	Time 0.080 (0.086)	Data 0.000 (0.003)	Loss 0.3856 (0.4556)	Acc@1 84.766 (84.363)	Acc@5 99.609 (99.291)
Epoch: [25][192/196]	Time 0.079 (0.085)	Data 0.000 (0.002)	Loss 0.4877 (0.4532)	Acc@1 84.375 (84.438)	Acc@5 97.656 (99.302)
after train
n1: 25 for:
wAcc: 66.5752379067469
test acc: 76.44
Epoche: [26/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [26][0/196]	Time 0.131 (0.131)	Data 0.268 (0.268)	Loss 0.3603 (0.3603)	Acc@1 87.891 (87.891)	Acc@5 100.000 (100.000)
Epoch: [26][64/196]	Time 0.079 (0.081)	Data 0.000 (0.004)	Loss 0.4329 (0.4595)	Acc@1 82.422 (84.105)	Acc@5 100.000 (99.417)
Epoch: [26][128/196]	Time 0.079 (0.083)	Data 0.000 (0.002)	Loss 0.4331 (0.4598)	Acc@1 85.938 (84.130)	Acc@5 99.219 (99.328)
Epoch: [26][192/196]	Time 0.098 (0.085)	Data 0.000 (0.002)	Loss 0.4743 (0.4556)	Acc@1 84.375 (84.227)	Acc@5 99.609 (99.338)
after train
n1: 26 for:
wAcc: 66.83021996590661
test acc: 77.56
Epoche: [27/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [27][0/196]	Time 0.125 (0.125)	Data 0.377 (0.377)	Loss 0.4349 (0.4349)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [27][64/196]	Time 0.080 (0.080)	Data 0.000 (0.006)	Loss 0.3541 (0.4320)	Acc@1 88.281 (85.078)	Acc@5 100.000 (99.423)
Epoch: [27][128/196]	Time 0.099 (0.080)	Data 0.000 (0.003)	Loss 0.4183 (0.4348)	Acc@1 87.109 (84.947)	Acc@5 99.609 (99.388)
Epoch: [27][192/196]	Time 0.079 (0.083)	Data 0.000 (0.002)	Loss 0.3924 (0.4438)	Acc@1 85.547 (84.695)	Acc@5 99.609 (99.362)
after train
n1: 27 for:
wAcc: 67.1348411618555
test acc: 63.69
Epoche: [28/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [28][0/196]	Time 0.095 (0.095)	Data 0.376 (0.376)	Loss 0.3688 (0.3688)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [28][64/196]	Time 0.080 (0.081)	Data 0.000 (0.006)	Loss 0.4004 (0.4267)	Acc@1 85.547 (85.186)	Acc@5 100.000 (99.459)
Epoch: [28][128/196]	Time 0.080 (0.081)	Data 0.000 (0.003)	Loss 0.4464 (0.4433)	Acc@1 84.766 (84.681)	Acc@5 99.219 (99.346)
Epoch: [28][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.5678 (0.4480)	Acc@1 78.906 (84.446)	Acc@5 98.047 (99.354)
after train
n1: 28 for:
wAcc: 66.446736929365
test acc: 78.93
Epoche: [29/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [29][0/196]	Time 0.098 (0.098)	Data 0.358 (0.358)	Loss 0.3717 (0.3717)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [29][64/196]	Time 0.080 (0.084)	Data 0.000 (0.006)	Loss 0.4911 (0.4370)	Acc@1 82.422 (84.844)	Acc@5 99.219 (99.411)
Epoch: [29][128/196]	Time 0.080 (0.082)	Data 0.000 (0.003)	Loss 0.4542 (0.4414)	Acc@1 84.766 (84.675)	Acc@5 99.609 (99.340)
Epoch: [29][192/196]	Time 0.079 (0.082)	Data 0.000 (0.002)	Loss 0.4679 (0.4463)	Acc@1 83.203 (84.606)	Acc@5 99.609 (99.326)
after train
n1: 29 for:
wAcc: 66.86979975332633
test acc: 81.33
Epoche: [30/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [30][0/196]	Time 0.125 (0.125)	Data 0.292 (0.292)	Loss 0.4724 (0.4724)	Acc@1 83.203 (83.203)	Acc@5 98.828 (98.828)
Epoch: [30][64/196]	Time 0.080 (0.082)	Data 0.000 (0.005)	Loss 0.4623 (0.4366)	Acc@1 84.766 (84.796)	Acc@5 99.219 (99.381)
Epoch: [30][128/196]	Time 0.080 (0.081)	Data 0.000 (0.003)	Loss 0.4227 (0.4398)	Acc@1 85.547 (84.711)	Acc@5 99.609 (99.391)
Epoch: [30][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.3771 (0.4385)	Acc@1 87.500 (84.885)	Acc@5 99.219 (99.369)
after train
n1: 30 for:
wAcc: 67.39697682366874
test acc: 76.68
Epoche: [31/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [31][0/196]	Time 0.114 (0.114)	Data 0.274 (0.274)	Loss 0.4407 (0.4407)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [31][64/196]	Time 0.079 (0.081)	Data 0.000 (0.004)	Loss 0.4318 (0.4445)	Acc@1 87.109 (84.796)	Acc@5 99.609 (99.393)
Epoch: [31][128/196]	Time 0.080 (0.081)	Data 0.000 (0.002)	Loss 0.4333 (0.4362)	Acc@1 85.547 (85.038)	Acc@5 99.609 (99.394)
Epoch: [31][192/196]	Time 0.098 (0.082)	Data 0.000 (0.002)	Loss 0.4078 (0.4381)	Acc@1 86.719 (84.934)	Acc@5 99.219 (99.393)
after train
n1: 30 for:
wAcc: 70.32332261633341
test acc: 79.85
Epoche: [32/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [32][0/196]	Time 0.094 (0.094)	Data 0.436 (0.436)	Loss 0.4521 (0.4521)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [32][64/196]	Time 0.099 (0.082)	Data 0.000 (0.007)	Loss 0.4746 (0.4274)	Acc@1 85.547 (85.066)	Acc@5 98.438 (99.465)
Epoch: [32][128/196]	Time 0.080 (0.085)	Data 0.000 (0.004)	Loss 0.5423 (0.4254)	Acc@1 79.688 (85.305)	Acc@5 98.828 (99.452)
Epoch: [32][192/196]	Time 0.079 (0.084)	Data 0.000 (0.003)	Loss 0.5314 (0.4328)	Acc@1 82.031 (85.029)	Acc@5 99.219 (99.395)
after train
n1: 30 for:
wAcc: 72.19274128053529
test acc: 80.17
Epoche: [33/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [33][0/196]	Time 0.111 (0.111)	Data 0.363 (0.363)	Loss 0.4683 (0.4683)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [33][64/196]	Time 0.080 (0.087)	Data 0.000 (0.006)	Loss 0.4377 (0.4304)	Acc@1 84.375 (85.198)	Acc@5 99.609 (99.321)
Epoch: [33][128/196]	Time 0.080 (0.087)	Data 0.000 (0.003)	Loss 0.4286 (0.4327)	Acc@1 83.984 (85.056)	Acc@5 100.000 (99.364)
Epoch: [33][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.4749 (0.4312)	Acc@1 84.766 (85.035)	Acc@5 99.219 (99.395)
after train
n1: 30 for:
wAcc: 72.78835760546004
test acc: 75.66
Epoche: [34/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [34][0/196]	Time 0.091 (0.091)	Data 0.444 (0.444)	Loss 0.4069 (0.4069)	Acc@1 86.328 (86.328)	Acc@5 99.609 (99.609)
Epoch: [34][64/196]	Time 0.080 (0.084)	Data 0.000 (0.007)	Loss 0.4444 (0.4282)	Acc@1 82.812 (85.150)	Acc@5 99.609 (99.393)
Epoch: [34][128/196]	Time 0.095 (0.082)	Data 0.000 (0.004)	Loss 0.4586 (0.4282)	Acc@1 83.203 (85.232)	Acc@5 99.219 (99.413)
Epoch: [34][192/196]	Time 0.069 (0.081)	Data 0.000 (0.003)	Loss 0.4296 (0.4290)	Acc@1 85.547 (85.197)	Acc@5 99.609 (99.393)
after train
n1: 30 for:
wAcc: 74.54356336837881
test acc: 73.12
Epoche: [35/180]; Lr: 0.1
batch Size 256
befor train
