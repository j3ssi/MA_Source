no display found. Using non-interactive Agg backend
[5, 5, 5]
[16, 32, 64]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 180; reset: False; start epoche: 1; test: True pathtoModell: None; checkpoint: ./output/experimente4/deeperX; saveModell: False; LR: 0.1
random number: 954
Files already downloaded and verified

width: 16
module: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (3, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (4, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (5, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (6, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (7, 0
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=64, out_features=10, bias=True)
width: 32
module: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 16
conv gefunden
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (9, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (10, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (11, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (12, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (13, 0
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=64, out_features=10, bias=True)
width: 64
module: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 32
conv gefunden
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (15, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (16, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (17, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (18, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (19, 0
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=64, out_features=10, bias=True)
stagesI: {16: [(3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0)], 32: [(10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 0)], 64: [(16, 0), (17, 0), (18, 0), (19, 0), (21, None)]}
stagesO: {16: [(0, None), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0)], 32: [(8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0)], 64: [(14, 0), (15, 0), (16, 0), (17, 0), (18, 0), (19, 0)]}
Modell Erstellung
N2N(
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (15): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (16): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (15): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (16): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
deeper epoch: 90
Epoche: [1/180]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.
  warnings.warn("Setting attributes on ParameterList is not supported.")
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [1][0/196]	Time 0.114 (0.114)	Data 0.448 (0.448)	Loss 2.7244 (2.7244)	Acc@1 11.719 (11.719)	Acc@5 47.266 (47.266)
Epoch: [1][64/196]	Time 0.079 (0.081)	Data 0.000 (0.007)	Loss 1.7473 (2.0082)	Acc@1 33.984 (24.982)	Acc@5 89.453 (78.672)
Epoch: [1][128/196]	Time 0.080 (0.080)	Data 0.000 (0.004)	Loss 1.5338 (1.8290)	Acc@1 41.406 (31.238)	Acc@5 90.234 (83.881)
Epoch: [1][192/196]	Time 0.078 (0.080)	Data 0.000 (0.003)	Loss 1.3096 (1.7107)	Acc@1 49.609 (35.844)	Acc@5 94.531 (86.607)
after train
n1: 1 for:
wAcc: 32.86
test acc: 32.86
Epoche: [2/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [2][0/196]	Time 0.096 (0.096)	Data 0.364 (0.364)	Loss 1.4478 (1.4478)	Acc@1 46.875 (46.875)	Acc@5 92.188 (92.188)
Epoch: [2][64/196]	Time 0.080 (0.087)	Data 0.000 (0.006)	Loss 1.2430 (1.3242)	Acc@1 53.125 (51.046)	Acc@5 93.750 (93.654)
Epoch: [2][128/196]	Time 0.079 (0.083)	Data 0.000 (0.003)	Loss 1.1921 (1.2599)	Acc@1 57.812 (53.897)	Acc@5 93.750 (94.549)
Epoch: [2][192/196]	Time 0.097 (0.083)	Data 0.000 (0.002)	Loss 0.9972 (1.2121)	Acc@1 61.328 (55.912)	Acc@5 96.484 (94.914)
after train
n1: 2 for:
wAcc: 32.86
test acc: 48.96
Epoche: [3/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [3][0/196]	Time 0.112 (0.112)	Data 0.371 (0.371)	Loss 1.2028 (1.2028)	Acc@1 52.344 (52.344)	Acc@5 96.094 (96.094)
Epoch: [3][64/196]	Time 0.079 (0.087)	Data 0.000 (0.006)	Loss 1.0361 (1.0479)	Acc@1 64.844 (62.344)	Acc@5 95.703 (96.593)
Epoch: [3][128/196]	Time 0.080 (0.083)	Data 0.000 (0.003)	Loss 0.9827 (1.0301)	Acc@1 64.062 (63.000)	Acc@5 96.875 (96.690)
Epoch: [3][192/196]	Time 0.097 (0.088)	Data 0.000 (0.002)	Loss 0.8506 (1.0134)	Acc@1 70.312 (63.573)	Acc@5 99.609 (96.782)
after train
n1: 3 for:
wAcc: 40.91
test acc: 57.64
Epoche: [4/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [4][0/196]	Time 0.117 (0.117)	Data 0.356 (0.356)	Loss 0.8564 (0.8564)	Acc@1 66.406 (66.406)	Acc@5 97.656 (97.656)
Epoch: [4][64/196]	Time 0.079 (0.080)	Data 0.000 (0.006)	Loss 1.0162 (0.9201)	Acc@1 60.547 (67.338)	Acc@5 96.484 (97.115)
Epoch: [4][128/196]	Time 0.080 (0.080)	Data 0.000 (0.003)	Loss 0.9252 (0.9078)	Acc@1 63.672 (67.739)	Acc@5 98.438 (97.287)
Epoch: [4][192/196]	Time 0.079 (0.080)	Data 0.000 (0.002)	Loss 0.9919 (0.8943)	Acc@1 60.547 (68.110)	Acc@5 97.656 (97.460)
after train
n1: 4 for:
wAcc: 46.635999999999996
test acc: 58.2
Epoche: [5/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [5][0/196]	Time 0.122 (0.122)	Data 0.338 (0.338)	Loss 0.9117 (0.9117)	Acc@1 64.453 (64.453)	Acc@5 98.828 (98.828)
Epoch: [5][64/196]	Time 0.099 (0.086)	Data 0.000 (0.005)	Loss 0.7504 (0.8301)	Acc@1 73.047 (70.817)	Acc@5 98.047 (97.891)
Epoch: [5][128/196]	Time 0.079 (0.085)	Data 0.000 (0.003)	Loss 0.8581 (0.8191)	Acc@1 69.141 (71.115)	Acc@5 98.047 (98.023)
Epoch: [5][192/196]	Time 0.079 (0.083)	Data 0.000 (0.002)	Loss 0.6711 (0.8063)	Acc@1 78.516 (71.673)	Acc@5 98.438 (97.998)
after train
n1: 5 for:
wAcc: 49.198518518518526
test acc: 69.06
Epoche: [6/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [6][0/196]	Time 0.145 (0.145)	Data 0.340 (0.340)	Loss 0.7389 (0.7389)	Acc@1 75.391 (75.391)	Acc@5 98.438 (98.438)
Epoch: [6][64/196]	Time 0.099 (0.097)	Data 0.000 (0.006)	Loss 0.7294 (0.7549)	Acc@1 73.047 (73.389)	Acc@5 98.438 (98.221)
Epoch: [6][128/196]	Time 0.080 (0.091)	Data 0.000 (0.003)	Loss 0.8035 (0.7394)	Acc@1 69.531 (74.195)	Acc@5 98.828 (98.341)
Epoch: [6][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.7206 (0.7338)	Acc@1 78.125 (74.466)	Acc@5 98.438 (98.322)
after train
n1: 6 for:
wAcc: 53.662915451895046
test acc: 67.39
Epoche: [7/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [7][0/196]	Time 0.126 (0.126)	Data 0.389 (0.389)	Loss 0.6248 (0.6248)	Acc@1 77.344 (77.344)	Acc@5 98.438 (98.438)
Epoch: [7][64/196]	Time 0.079 (0.080)	Data 0.000 (0.006)	Loss 0.6511 (0.7142)	Acc@1 76.562 (75.012)	Acc@5 99.219 (98.407)
Epoch: [7][128/196]	Time 0.080 (0.080)	Data 0.000 (0.003)	Loss 0.6178 (0.6979)	Acc@1 76.953 (75.536)	Acc@5 98.438 (98.513)
Epoch: [7][192/196]	Time 0.079 (0.080)	Data 0.000 (0.002)	Loss 0.7306 (0.6927)	Acc@1 72.266 (75.781)	Acc@5 99.219 (98.523)
after train
n1: 7 for:
wAcc: 55.730488281250004
test acc: 73.39
Epoche: [8/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [8][0/196]	Time 0.095 (0.095)	Data 0.385 (0.385)	Loss 0.5806 (0.5806)	Acc@1 80.469 (80.469)	Acc@5 99.609 (99.609)
Epoch: [8][64/196]	Time 0.079 (0.080)	Data 0.000 (0.006)	Loss 0.6378 (0.6509)	Acc@1 81.250 (77.025)	Acc@5 96.875 (98.642)
Epoch: [8][128/196]	Time 0.080 (0.080)	Data 0.000 (0.003)	Loss 0.6348 (0.6492)	Acc@1 78.516 (77.238)	Acc@5 97.656 (98.634)
Epoch: [8][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.6611 (0.6501)	Acc@1 76.562 (77.241)	Acc@5 98.047 (98.616)
after train
n1: 8 for:
wAcc: 58.3841895901897
test acc: 70.84
Epoche: [9/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [9][0/196]	Time 0.127 (0.127)	Data 0.371 (0.371)	Loss 0.5966 (0.5966)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [9][64/196]	Time 0.099 (0.099)	Data 0.000 (0.006)	Loss 0.4737 (0.6203)	Acc@1 83.984 (78.732)	Acc@5 98.828 (98.852)
Epoch: [9][128/196]	Time 0.080 (0.094)	Data 0.000 (0.003)	Loss 0.6366 (0.6228)	Acc@1 78.125 (78.561)	Acc@5 99.609 (98.819)
Epoch: [9][192/196]	Time 0.098 (0.091)	Data 0.000 (0.002)	Loss 0.5813 (0.6241)	Acc@1 78.516 (78.518)	Acc@5 98.438 (98.763)
after train
n1: 9 for:
wAcc: 59.61145856
test acc: 74.88
Epoche: [10/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [10][0/196]	Time 0.111 (0.111)	Data 0.404 (0.404)	Loss 0.5622 (0.5622)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [10][64/196]	Time 0.080 (0.081)	Data 0.000 (0.006)	Loss 0.6221 (0.5934)	Acc@1 78.516 (79.351)	Acc@5 99.609 (98.972)
Epoch: [10][128/196]	Time 0.080 (0.080)	Data 0.000 (0.003)	Loss 0.5630 (0.6005)	Acc@1 80.859 (79.261)	Acc@5 98.828 (98.907)
Epoch: [10][192/196]	Time 0.079 (0.080)	Data 0.000 (0.002)	Loss 0.6078 (0.6004)	Acc@1 79.297 (79.208)	Acc@5 99.219 (98.925)
after train
n1: 10 for:
wAcc: 61.230325732200455
test acc: 69.94
Epoche: [11/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [11][0/196]	Time 0.115 (0.115)	Data 0.382 (0.382)	Loss 0.5611 (0.5611)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [11][64/196]	Time 0.081 (0.080)	Data 0.000 (0.006)	Loss 0.5185 (0.5761)	Acc@1 80.859 (79.880)	Acc@5 99.219 (98.840)
Epoch: [11][128/196]	Time 0.080 (0.082)	Data 0.000 (0.003)	Loss 0.6260 (0.5754)	Acc@1 77.734 (79.836)	Acc@5 98.047 (98.871)
Epoch: [11][192/196]	Time 0.079 (0.082)	Data 0.000 (0.002)	Loss 0.4818 (0.5755)	Acc@1 84.375 (79.975)	Acc@5 99.609 (98.887)
after train
n1: 11 for:
wAcc: 61.572166390016136
test acc: 77.72
Epoche: [12/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [12][0/196]	Time 0.126 (0.126)	Data 0.379 (0.379)	Loss 0.5672 (0.5672)	Acc@1 80.859 (80.859)	Acc@5 99.219 (99.219)
Epoch: [12][64/196]	Time 0.098 (0.088)	Data 0.000 (0.006)	Loss 0.5505 (0.5729)	Acc@1 82.422 (80.126)	Acc@5 100.000 (98.906)
Epoch: [12][128/196]	Time 0.098 (0.093)	Data 0.000 (0.003)	Loss 0.4778 (0.5604)	Acc@1 83.203 (80.481)	Acc@5 99.609 (99.025)
Epoch: [12][192/196]	Time 0.098 (0.095)	Data 0.000 (0.002)	Loss 0.5000 (0.5622)	Acc@1 80.859 (80.453)	Acc@5 99.219 (99.028)
after train
n1: 12 for:
wAcc: 63.06487896866749
test acc: 76.68
Epoche: [13/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [13][0/196]	Time 0.111 (0.111)	Data 0.377 (0.377)	Loss 0.5844 (0.5844)	Acc@1 78.516 (78.516)	Acc@5 99.609 (99.609)
Epoch: [13][64/196]	Time 0.091 (0.092)	Data 0.000 (0.006)	Loss 0.5824 (0.5420)	Acc@1 80.469 (81.046)	Acc@5 99.219 (99.044)
Epoch: [13][128/196]	Time 0.091 (0.091)	Data 0.000 (0.003)	Loss 0.5201 (0.5484)	Acc@1 82.812 (80.741)	Acc@5 99.609 (99.086)
Epoch: [13][192/196]	Time 0.090 (0.091)	Data 0.000 (0.002)	Loss 0.5789 (0.5486)	Acc@1 81.641 (80.764)	Acc@5 99.609 (99.055)
after train
n1: 13 for:
wAcc: 64.0377398240651
test acc: 73.56
Epoche: [14/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [14][0/196]	Time 0.092 (0.092)	Data 0.446 (0.446)	Loss 0.5491 (0.5491)	Acc@1 78.906 (78.906)	Acc@5 99.609 (99.609)
Epoch: [14][64/196]	Time 0.080 (0.081)	Data 0.000 (0.007)	Loss 0.5731 (0.5221)	Acc@1 80.859 (81.647)	Acc@5 99.609 (99.123)
Epoch: [14][128/196]	Time 0.080 (0.081)	Data 0.000 (0.004)	Loss 0.5846 (0.5311)	Acc@1 80.078 (81.426)	Acc@5 99.609 (99.149)
Epoch: [14][192/196]	Time 0.079 (0.080)	Data 0.000 (0.003)	Loss 0.5997 (0.5301)	Acc@1 78.906 (81.527)	Acc@5 98.828 (99.140)
after train
n1: 14 for:
wAcc: 64.3818786783523
test acc: 72.37
Epoche: [15/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [15][0/196]	Time 0.092 (0.092)	Data 0.337 (0.337)	Loss 0.4810 (0.4810)	Acc@1 80.859 (80.859)	Acc@5 98.828 (98.828)
Epoch: [15][64/196]	Time 0.099 (0.093)	Data 0.000 (0.005)	Loss 0.4662 (0.5250)	Acc@1 84.766 (81.641)	Acc@5 99.609 (99.177)
Epoch: [15][128/196]	Time 0.078 (0.094)	Data 0.000 (0.003)	Loss 0.4404 (0.5226)	Acc@1 83.594 (81.759)	Acc@5 100.000 (99.155)
Epoch: [15][192/196]	Time 0.080 (0.091)	Data 0.000 (0.002)	Loss 0.5299 (0.5241)	Acc@1 83.984 (81.827)	Acc@5 98.828 (99.152)
after train
n1: 15 for:
wAcc: 64.52700747276705
test acc: 74.76
Epoche: [16/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [16][0/196]	Time 0.086 (0.086)	Data 0.444 (0.444)	Loss 0.5120 (0.5120)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [16][64/196]	Time 0.080 (0.080)	Data 0.000 (0.007)	Loss 0.5569 (0.4997)	Acc@1 80.859 (82.770)	Acc@5 99.219 (99.165)
Epoch: [16][128/196]	Time 0.080 (0.080)	Data 0.000 (0.004)	Loss 0.3728 (0.5047)	Acc@1 88.281 (82.604)	Acc@5 99.609 (99.185)
Epoch: [16][192/196]	Time 0.079 (0.080)	Data 0.000 (0.003)	Loss 0.5395 (0.5125)	Acc@1 80.859 (82.286)	Acc@5 98.828 (99.146)
after train
n1: 16 for:
wAcc: 64.94631174231138
test acc: 76.8
Epoche: [17/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [17][0/196]	Time 0.097 (0.097)	Data 0.394 (0.394)	Loss 0.5351 (0.5351)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [17][64/196]	Time 0.080 (0.082)	Data 0.000 (0.006)	Loss 0.5705 (0.4939)	Acc@1 81.250 (82.933)	Acc@5 98.828 (99.237)
Epoch: [17][128/196]	Time 0.080 (0.081)	Data 0.000 (0.003)	Loss 0.5009 (0.4867)	Acc@1 81.641 (83.188)	Acc@5 99.609 (99.228)
Epoch: [17][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.4870 (0.4993)	Acc@1 82.812 (82.721)	Acc@5 99.219 (99.186)
after train
n1: 17 for:
wAcc: 65.52209369520564
test acc: 74.63
Epoche: [18/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [18][0/196]	Time 0.113 (0.113)	Data 0.356 (0.356)	Loss 0.4503 (0.4503)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [18][64/196]	Time 0.081 (0.084)	Data 0.000 (0.006)	Loss 0.5758 (0.4990)	Acc@1 80.859 (82.608)	Acc@5 98.828 (99.237)
Epoch: [18][128/196]	Time 0.079 (0.082)	Data 0.000 (0.003)	Loss 0.4149 (0.4918)	Acc@1 85.938 (82.791)	Acc@5 98.438 (99.240)
Epoch: [18][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.4992 (0.4941)	Acc@1 82.031 (82.839)	Acc@5 98.828 (99.227)
after train
n1: 18 for:
wAcc: 65.76899558508362
test acc: 76.45
Epoche: [19/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [19][0/196]	Time 0.129 (0.129)	Data 0.268 (0.268)	Loss 0.3662 (0.3662)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [19][64/196]	Time 0.080 (0.081)	Data 0.000 (0.004)	Loss 0.5532 (0.4892)	Acc@1 78.125 (83.239)	Acc@5 98.828 (99.279)
Epoch: [19][128/196]	Time 0.081 (0.081)	Data 0.000 (0.002)	Loss 0.4503 (0.4885)	Acc@1 85.547 (83.179)	Acc@5 99.219 (99.246)
Epoch: [19][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.5473 (0.4916)	Acc@1 82.031 (82.978)	Acc@5 99.609 (99.251)
after train
n1: 19 for:
wAcc: 66.16579514217896
test acc: 65.41
Epoche: [20/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [20][0/196]	Time 0.103 (0.103)	Data 0.289 (0.289)	Loss 0.5437 (0.5437)	Acc@1 81.250 (81.250)	Acc@5 98.828 (98.828)
Epoch: [20][64/196]	Time 0.099 (0.082)	Data 0.000 (0.005)	Loss 0.5382 (0.4898)	Acc@1 82.031 (83.365)	Acc@5 99.219 (99.243)
Epoch: [20][128/196]	Time 0.080 (0.081)	Data 0.000 (0.002)	Loss 0.4645 (0.4893)	Acc@1 83.984 (83.167)	Acc@5 99.609 (99.252)
Epoch: [20][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.4852 (0.4878)	Acc@1 82.031 (83.197)	Acc@5 99.219 (99.253)
after train
n1: 20 for:
wAcc: 65.45095977893796
test acc: 68.78
Epoche: [21/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [21][0/196]	Time 0.094 (0.094)	Data 0.379 (0.379)	Loss 0.5137 (0.5137)	Acc@1 82.422 (82.422)	Acc@5 98.828 (98.828)
Epoch: [21][64/196]	Time 0.079 (0.081)	Data 0.000 (0.006)	Loss 0.5579 (0.4639)	Acc@1 78.125 (84.056)	Acc@5 98.438 (99.297)
Epoch: [21][128/196]	Time 0.099 (0.081)	Data 0.000 (0.003)	Loss 0.4647 (0.4698)	Acc@1 83.594 (83.881)	Acc@5 99.609 (99.337)
Epoch: [21][192/196]	Time 0.098 (0.087)	Data 0.000 (0.002)	Loss 0.5460 (0.4725)	Acc@1 81.250 (83.806)	Acc@5 99.609 (99.310)
after train
n1: 21 for:
wAcc: 65.18171491228121
test acc: 75.4
Epoche: [22/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [22][0/196]	Time 0.116 (0.116)	Data 0.361 (0.361)	Loss 0.4251 (0.4251)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [22][64/196]	Time 0.080 (0.082)	Data 0.000 (0.006)	Loss 0.4380 (0.4614)	Acc@1 84.375 (84.105)	Acc@5 99.609 (99.297)
Epoch: [22][128/196]	Time 0.091 (0.087)	Data 0.000 (0.003)	Loss 0.4839 (0.4684)	Acc@1 84.375 (83.806)	Acc@5 98.828 (99.249)
Epoch: [22][192/196]	Time 0.090 (0.086)	Data 0.000 (0.002)	Loss 0.4148 (0.4719)	Acc@1 85.938 (83.654)	Acc@5 100.000 (99.255)
after train
n1: 22 for:
wAcc: 65.54156445738278
test acc: 77.58
Epoche: [23/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [23][0/196]	Time 0.118 (0.118)	Data 0.363 (0.363)	Loss 0.4104 (0.4104)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [23][64/196]	Time 0.080 (0.091)	Data 0.000 (0.006)	Loss 0.4314 (0.4742)	Acc@1 86.328 (83.780)	Acc@5 99.219 (99.177)
Epoch: [23][128/196]	Time 0.079 (0.086)	Data 0.000 (0.003)	Loss 0.4931 (0.4667)	Acc@1 84.375 (84.060)	Acc@5 99.609 (99.234)
Epoch: [23][192/196]	Time 0.079 (0.085)	Data 0.000 (0.002)	Loss 0.4956 (0.4673)	Acc@1 82.812 (84.001)	Acc@5 98.828 (99.245)
after train
n1: 23 for:
wAcc: 66.02994611501025
test acc: 77.28
Epoche: [24/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [24][0/196]	Time 0.089 (0.089)	Data 0.441 (0.441)	Loss 0.5988 (0.5988)	Acc@1 80.078 (80.078)	Acc@5 99.219 (99.219)
Epoch: [24][64/196]	Time 0.091 (0.089)	Data 0.000 (0.007)	Loss 0.3813 (0.4517)	Acc@1 87.109 (84.189)	Acc@5 99.219 (99.387)
Epoch: [24][128/196]	Time 0.080 (0.089)	Data 0.000 (0.004)	Loss 0.5086 (0.4495)	Acc@1 81.641 (84.151)	Acc@5 99.609 (99.379)
Epoch: [24][192/196]	Time 0.079 (0.089)	Data 0.000 (0.003)	Loss 0.4752 (0.4597)	Acc@1 83.594 (83.948)	Acc@5 99.219 (99.330)
after train
n1: 24 for:
wAcc: 66.42347783517094
test acc: 74.83
Epoche: [25/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [25][0/196]	Time 0.107 (0.107)	Data 0.356 (0.356)	Loss 0.3948 (0.3948)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [25][64/196]	Time 0.091 (0.088)	Data 0.000 (0.006)	Loss 0.3649 (0.4434)	Acc@1 86.328 (84.784)	Acc@5 100.000 (99.369)
Epoch: [25][128/196]	Time 0.080 (0.086)	Data 0.000 (0.003)	Loss 0.3856 (0.4556)	Acc@1 84.766 (84.363)	Acc@5 99.609 (99.291)
Epoch: [25][192/196]	Time 0.079 (0.085)	Data 0.000 (0.002)	Loss 0.4877 (0.4532)	Acc@1 84.375 (84.438)	Acc@5 97.656 (99.302)
after train
n1: 25 for:
wAcc: 66.5752379067469
test acc: 76.44
Epoche: [26/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [26][0/196]	Time 0.131 (0.131)	Data 0.268 (0.268)	Loss 0.3603 (0.3603)	Acc@1 87.891 (87.891)	Acc@5 100.000 (100.000)
Epoch: [26][64/196]	Time 0.079 (0.081)	Data 0.000 (0.004)	Loss 0.4329 (0.4595)	Acc@1 82.422 (84.105)	Acc@5 100.000 (99.417)
Epoch: [26][128/196]	Time 0.079 (0.083)	Data 0.000 (0.002)	Loss 0.4331 (0.4598)	Acc@1 85.938 (84.130)	Acc@5 99.219 (99.328)
Epoch: [26][192/196]	Time 0.098 (0.085)	Data 0.000 (0.002)	Loss 0.4743 (0.4556)	Acc@1 84.375 (84.227)	Acc@5 99.609 (99.338)
after train
n1: 26 for:
wAcc: 66.83021996590661
test acc: 77.56
Epoche: [27/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [27][0/196]	Time 0.125 (0.125)	Data 0.377 (0.377)	Loss 0.4349 (0.4349)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [27][64/196]	Time 0.080 (0.080)	Data 0.000 (0.006)	Loss 0.3541 (0.4320)	Acc@1 88.281 (85.078)	Acc@5 100.000 (99.423)
Epoch: [27][128/196]	Time 0.099 (0.080)	Data 0.000 (0.003)	Loss 0.4183 (0.4348)	Acc@1 87.109 (84.947)	Acc@5 99.609 (99.388)
Epoch: [27][192/196]	Time 0.079 (0.083)	Data 0.000 (0.002)	Loss 0.3924 (0.4438)	Acc@1 85.547 (84.695)	Acc@5 99.609 (99.362)
after train
n1: 27 for:
wAcc: 67.1348411618555
test acc: 63.69
Epoche: [28/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [28][0/196]	Time 0.095 (0.095)	Data 0.376 (0.376)	Loss 0.3688 (0.3688)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [28][64/196]	Time 0.080 (0.081)	Data 0.000 (0.006)	Loss 0.4004 (0.4267)	Acc@1 85.547 (85.186)	Acc@5 100.000 (99.459)
Epoch: [28][128/196]	Time 0.080 (0.081)	Data 0.000 (0.003)	Loss 0.4464 (0.4433)	Acc@1 84.766 (84.681)	Acc@5 99.219 (99.346)
Epoch: [28][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.5678 (0.4480)	Acc@1 78.906 (84.446)	Acc@5 98.047 (99.354)
after train
n1: 28 for:
wAcc: 66.446736929365
test acc: 78.93
Epoche: [29/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [29][0/196]	Time 0.098 (0.098)	Data 0.358 (0.358)	Loss 0.3717 (0.3717)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [29][64/196]	Time 0.080 (0.084)	Data 0.000 (0.006)	Loss 0.4911 (0.4370)	Acc@1 82.422 (84.844)	Acc@5 99.219 (99.411)
Epoch: [29][128/196]	Time 0.080 (0.082)	Data 0.000 (0.003)	Loss 0.4542 (0.4414)	Acc@1 84.766 (84.675)	Acc@5 99.609 (99.340)
Epoch: [29][192/196]	Time 0.079 (0.082)	Data 0.000 (0.002)	Loss 0.4679 (0.4463)	Acc@1 83.203 (84.606)	Acc@5 99.609 (99.326)
after train
n1: 29 for:
wAcc: 66.86979975332633
test acc: 81.33
Epoche: [30/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [30][0/196]	Time 0.125 (0.125)	Data 0.292 (0.292)	Loss 0.4724 (0.4724)	Acc@1 83.203 (83.203)	Acc@5 98.828 (98.828)
Epoch: [30][64/196]	Time 0.080 (0.082)	Data 0.000 (0.005)	Loss 0.4623 (0.4366)	Acc@1 84.766 (84.796)	Acc@5 99.219 (99.381)
Epoch: [30][128/196]	Time 0.080 (0.081)	Data 0.000 (0.003)	Loss 0.4227 (0.4398)	Acc@1 85.547 (84.711)	Acc@5 99.609 (99.391)
Epoch: [30][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.3771 (0.4385)	Acc@1 87.500 (84.885)	Acc@5 99.219 (99.369)
after train
n1: 30 for:
wAcc: 67.39697682366874
test acc: 76.68
Epoche: [31/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [31][0/196]	Time 0.114 (0.114)	Data 0.274 (0.274)	Loss 0.4407 (0.4407)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [31][64/196]	Time 0.079 (0.081)	Data 0.000 (0.004)	Loss 0.4318 (0.4445)	Acc@1 87.109 (84.796)	Acc@5 99.609 (99.393)
Epoch: [31][128/196]	Time 0.080 (0.081)	Data 0.000 (0.002)	Loss 0.4333 (0.4362)	Acc@1 85.547 (85.038)	Acc@5 99.609 (99.394)
Epoch: [31][192/196]	Time 0.098 (0.082)	Data 0.000 (0.002)	Loss 0.4078 (0.4381)	Acc@1 86.719 (84.934)	Acc@5 99.219 (99.393)
after train
n1: 30 for:
wAcc: 70.32332261633341
test acc: 79.85
Epoche: [32/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [32][0/196]	Time 0.094 (0.094)	Data 0.436 (0.436)	Loss 0.4521 (0.4521)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [32][64/196]	Time 0.099 (0.082)	Data 0.000 (0.007)	Loss 0.4746 (0.4274)	Acc@1 85.547 (85.066)	Acc@5 98.438 (99.465)
Epoch: [32][128/196]	Time 0.080 (0.085)	Data 0.000 (0.004)	Loss 0.5423 (0.4254)	Acc@1 79.688 (85.305)	Acc@5 98.828 (99.452)
Epoch: [32][192/196]	Time 0.079 (0.084)	Data 0.000 (0.003)	Loss 0.5314 (0.4328)	Acc@1 82.031 (85.029)	Acc@5 99.219 (99.395)
after train
n1: 30 for:
wAcc: 72.19274128053529
test acc: 80.17
Epoche: [33/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [33][0/196]	Time 0.111 (0.111)	Data 0.363 (0.363)	Loss 0.4683 (0.4683)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [33][64/196]	Time 0.080 (0.087)	Data 0.000 (0.006)	Loss 0.4377 (0.4304)	Acc@1 84.375 (85.198)	Acc@5 99.609 (99.321)
Epoch: [33][128/196]	Time 0.080 (0.087)	Data 0.000 (0.003)	Loss 0.4286 (0.4327)	Acc@1 83.984 (85.056)	Acc@5 100.000 (99.364)
Epoch: [33][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.4749 (0.4312)	Acc@1 84.766 (85.035)	Acc@5 99.219 (99.395)
after train
n1: 30 for:
wAcc: 72.78835760546004
test acc: 75.66
Epoche: [34/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [34][0/196]	Time 0.091 (0.091)	Data 0.444 (0.444)	Loss 0.4069 (0.4069)	Acc@1 86.328 (86.328)	Acc@5 99.609 (99.609)
Epoch: [34][64/196]	Time 0.080 (0.084)	Data 0.000 (0.007)	Loss 0.4444 (0.4282)	Acc@1 82.812 (85.150)	Acc@5 99.609 (99.393)
Epoch: [34][128/196]	Time 0.095 (0.082)	Data 0.000 (0.004)	Loss 0.4586 (0.4282)	Acc@1 83.203 (85.232)	Acc@5 99.219 (99.413)
Epoch: [34][192/196]	Time 0.069 (0.081)	Data 0.000 (0.003)	Loss 0.4296 (0.4290)	Acc@1 85.547 (85.197)	Acc@5 99.609 (99.393)
after train
n1: 30 for:
wAcc: 74.54356336837881
test acc: 73.12
Epoche: [35/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [35][0/196]	Time 0.094 (0.094)	Data 0.296 (0.296)	Loss 0.4462 (0.4462)	Acc@1 85.156 (85.156)	Acc@5 98.047 (98.047)
Epoch: [35][64/196]	Time 0.081 (0.084)	Data 0.000 (0.005)	Loss 0.4533 (0.4163)	Acc@1 83.984 (85.499)	Acc@5 99.219 (99.423)
Epoch: [35][128/196]	Time 0.080 (0.082)	Data 0.000 (0.003)	Loss 0.3993 (0.4188)	Acc@1 86.719 (85.456)	Acc@5 98.828 (99.443)
Epoch: [35][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.4319 (0.4189)	Acc@1 84.766 (85.535)	Acc@5 99.609 (99.454)
after train
n1: 30 for:
wAcc: 74.2103027698232
test acc: 81.65
Epoche: [36/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [36][0/196]	Time 0.116 (0.116)	Data 0.344 (0.344)	Loss 0.3451 (0.3451)	Acc@1 88.672 (88.672)	Acc@5 99.609 (99.609)
Epoch: [36][64/196]	Time 0.080 (0.084)	Data 0.000 (0.006)	Loss 0.4676 (0.4277)	Acc@1 82.422 (85.373)	Acc@5 99.219 (99.429)
Epoch: [36][128/196]	Time 0.085 (0.083)	Data 0.000 (0.003)	Loss 0.3760 (0.4210)	Acc@1 85.547 (85.598)	Acc@5 99.609 (99.403)
Epoch: [36][192/196]	Time 0.098 (0.086)	Data 0.000 (0.002)	Loss 0.4267 (0.4204)	Acc@1 85.547 (85.727)	Acc@5 99.219 (99.395)
after train
n1: 30 for:
wAcc: 75.55765257974373
test acc: 80.71
Epoche: [37/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [37][0/196]	Time 0.105 (0.105)	Data 0.365 (0.365)	Loss 0.3585 (0.3585)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [37][64/196]	Time 0.080 (0.089)	Data 0.000 (0.006)	Loss 0.3344 (0.4170)	Acc@1 90.234 (85.841)	Acc@5 99.219 (99.345)
Epoch: [37][128/196]	Time 0.080 (0.085)	Data 0.000 (0.003)	Loss 0.4122 (0.4238)	Acc@1 85.938 (85.483)	Acc@5 100.000 (99.316)
Epoch: [37][192/196]	Time 0.079 (0.084)	Data 0.000 (0.002)	Loss 0.3999 (0.4202)	Acc@1 85.547 (85.514)	Acc@5 98.828 (99.360)
after train
n1: 30 for:
wAcc: 75.52143011975858
test acc: 75.78
Epoche: [38/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [38][0/196]	Time 0.099 (0.099)	Data 0.367 (0.367)	Loss 0.4504 (0.4504)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [38][64/196]	Time 0.081 (0.086)	Data 0.000 (0.006)	Loss 0.3801 (0.4138)	Acc@1 84.375 (85.745)	Acc@5 99.219 (99.459)
Epoch: [38][128/196]	Time 0.091 (0.087)	Data 0.000 (0.003)	Loss 0.4137 (0.4117)	Acc@1 86.328 (85.795)	Acc@5 99.219 (99.422)
Epoch: [38][192/196]	Time 0.090 (0.085)	Data 0.000 (0.002)	Loss 0.3057 (0.4146)	Acc@1 89.062 (85.707)	Acc@5 100.000 (99.411)
after train
n1: 30 for:
wAcc: 76.12214073877746
test acc: 79.74
Epoche: [39/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [39][0/196]	Time 0.139 (0.139)	Data 0.465 (0.465)	Loss 0.4267 (0.4267)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [39][64/196]	Time 0.080 (0.082)	Data 0.000 (0.007)	Loss 0.4604 (0.4094)	Acc@1 82.031 (85.841)	Acc@5 99.219 (99.471)
Epoch: [39][128/196]	Time 0.070 (0.081)	Data 0.000 (0.004)	Loss 0.3247 (0.4145)	Acc@1 88.281 (85.653)	Acc@5 100.000 (99.446)
Epoch: [39][192/196]	Time 0.079 (0.080)	Data 0.000 (0.003)	Loss 0.3548 (0.4142)	Acc@1 88.672 (85.687)	Acc@5 99.609 (99.433)
after train
n1: 30 for:
wAcc: 75.64141692091503
test acc: 79.69
Epoche: [40/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [40][0/196]	Time 0.112 (0.112)	Data 0.270 (0.270)	Loss 0.3517 (0.3517)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [40][64/196]	Time 0.080 (0.085)	Data 0.000 (0.004)	Loss 0.3844 (0.4021)	Acc@1 85.547 (86.100)	Acc@5 100.000 (99.459)
Epoch: [40][128/196]	Time 0.080 (0.083)	Data 0.000 (0.002)	Loss 0.4733 (0.4074)	Acc@1 81.250 (85.968)	Acc@5 99.609 (99.422)
Epoch: [40][192/196]	Time 0.079 (0.082)	Data 0.000 (0.002)	Loss 0.3294 (0.4074)	Acc@1 88.672 (85.940)	Acc@5 100.000 (99.431)
after train
n1: 30 for:
wAcc: 77.02730474459302
test acc: 81.3
Epoche: [41/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [41][0/196]	Time 0.121 (0.121)	Data 0.372 (0.372)	Loss 0.4173 (0.4173)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [41][64/196]	Time 0.079 (0.089)	Data 0.000 (0.006)	Loss 0.3682 (0.4090)	Acc@1 85.938 (85.787)	Acc@5 100.000 (99.447)
Epoch: [41][128/196]	Time 0.081 (0.085)	Data 0.000 (0.003)	Loss 0.3497 (0.4101)	Acc@1 87.891 (85.635)	Acc@5 99.219 (99.467)
Epoch: [41][192/196]	Time 0.079 (0.083)	Data 0.000 (0.002)	Loss 0.3733 (0.4144)	Acc@1 87.891 (85.616)	Acc@5 100.000 (99.421)
after train
n1: 30 for:
wAcc: 77.15261848347376
test acc: 67.54
Epoche: [42/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [42][0/196]	Time 0.098 (0.098)	Data 0.417 (0.417)	Loss 0.4361 (0.4361)	Acc@1 83.203 (83.203)	Acc@5 98.828 (98.828)
Epoch: [42][64/196]	Time 0.080 (0.083)	Data 0.000 (0.007)	Loss 0.3521 (0.4076)	Acc@1 85.938 (85.829)	Acc@5 99.609 (99.501)
Epoch: [42][128/196]	Time 0.081 (0.082)	Data 0.000 (0.003)	Loss 0.4177 (0.4139)	Acc@1 86.719 (85.689)	Acc@5 99.219 (99.458)
Epoch: [42][192/196]	Time 0.079 (0.081)	Data 0.000 (0.002)	Loss 0.3987 (0.4156)	Acc@1 86.328 (85.650)	Acc@5 99.609 (99.464)
after train
n1: 30 for:
wAcc: 76.08141749045816
test acc: 78.71
Epoche: [43/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [43][0/196]	Time 0.092 (0.092)	Data 0.446 (0.446)	Loss 0.3652 (0.3652)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [43][64/196]	Time 0.099 (0.098)	Data 0.000 (0.007)	Loss 0.4204 (0.4084)	Acc@1 83.984 (86.046)	Acc@5 99.609 (99.393)
Epoch: [43][128/196]	Time 0.099 (0.098)	Data 0.000 (0.004)	Loss 0.3614 (0.4037)	Acc@1 88.281 (86.174)	Acc@5 99.219 (99.431)
Epoch: [43][192/196]	Time 0.098 (0.098)	Data 0.000 (0.003)	Loss 0.3451 (0.4041)	Acc@1 86.719 (86.045)	Acc@5 100.000 (99.439)
after train
n1: 30 for:
wAcc: 76.07897520569665
test acc: 71.16
Epoche: [44/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [44][0/196]	Time 0.113 (0.113)	Data 0.361 (0.361)	Loss 0.4093 (0.4093)	Acc@1 83.594 (83.594)	Acc@5 99.609 (99.609)
Epoch: [44][64/196]	Time 0.080 (0.087)	Data 0.000 (0.006)	Loss 0.3672 (0.3978)	Acc@1 87.109 (86.382)	Acc@5 99.219 (99.375)
Epoch: [44][128/196]	Time 0.090 (0.085)	Data 0.000 (0.003)	Loss 0.3909 (0.4077)	Acc@1 85.156 (85.928)	Acc@5 99.609 (99.425)
Epoch: [44][192/196]	Time 0.090 (0.086)	Data 0.000 (0.002)	Loss 0.4706 (0.4105)	Acc@1 83.594 (85.844)	Acc@5 99.609 (99.439)
after train
n1: 30 for:
wAcc: 76.10712408843004
test acc: 76.58
Epoche: [45/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [45][0/196]	Time 0.102 (0.102)	Data 0.372 (0.372)	Loss 0.3902 (0.3902)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [45][64/196]	Time 0.080 (0.092)	Data 0.000 (0.006)	Loss 0.4461 (0.3971)	Acc@1 83.984 (86.460)	Acc@5 99.609 (99.489)
Epoch: [45][128/196]	Time 0.080 (0.086)	Data 0.000 (0.003)	Loss 0.3247 (0.4010)	Acc@1 86.719 (86.237)	Acc@5 100.000 (99.467)
Epoch: [45][192/196]	Time 0.079 (0.084)	Data 0.000 (0.002)	Loss 0.4064 (0.4043)	Acc@1 88.672 (86.004)	Acc@5 100.000 (99.496)
after train
n1: 30 for:
wAcc: 76.4325377885327
test acc: 73.0
Epoche: [46/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [46][0/196]	Time 0.111 (0.111)	Data 0.301 (0.301)	Loss 0.3881 (0.3881)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [46][64/196]	Time 0.099 (0.098)	Data 0.000 (0.005)	Loss 0.3516 (0.3976)	Acc@1 85.938 (86.196)	Acc@5 99.609 (99.453)
Epoch: [46][128/196]	Time 0.083 (0.091)	Data 0.000 (0.003)	Loss 0.3768 (0.4000)	Acc@1 87.500 (86.083)	Acc@5 100.000 (99.534)
Epoch: [46][192/196]	Time 0.070 (0.087)	Data 0.000 (0.002)	Loss 0.5089 (0.4010)	Acc@1 83.594 (86.097)	Acc@5 99.219 (99.498)
after train
n1: 30 for:
wAcc: 75.89738515844248
test acc: 71.31
Epoche: [47/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [47][0/196]	Time 0.100 (0.100)	Data 0.324 (0.324)	Loss 0.3786 (0.3786)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [47][64/196]	Time 0.099 (0.091)	Data 0.000 (0.005)	Loss 0.4824 (0.4015)	Acc@1 84.766 (86.130)	Acc@5 98.828 (99.507)
Epoch: [47][128/196]	Time 0.094 (0.093)	Data 0.000 (0.003)	Loss 0.4438 (0.3999)	Acc@1 85.156 (86.104)	Acc@5 99.609 (99.531)
Epoch: [47][192/196]	Time 0.079 (0.088)	Data 0.000 (0.002)	Loss 0.4019 (0.4039)	Acc@1 87.500 (85.960)	Acc@5 99.609 (99.482)
after train
n1: 30 for:
wAcc: 75.86452685982184
test acc: 80.57
Epoche: [48/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [48][0/196]	Time 0.101 (0.101)	Data 0.372 (0.372)	Loss 0.4647 (0.4647)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [48][64/196]	Time 0.080 (0.080)	Data 0.000 (0.006)	Loss 0.3836 (0.3893)	Acc@1 87.891 (86.460)	Acc@5 100.000 (99.555)
Epoch: [48][128/196]	Time 0.080 (0.080)	Data 0.000 (0.003)	Loss 0.4644 (0.3990)	Acc@1 84.375 (86.098)	Acc@5 100.000 (99.494)
Epoch: [48][192/196]	Time 0.082 (0.081)	Data 0.000 (0.002)	Loss 0.3483 (0.4011)	Acc@1 87.891 (86.049)	Acc@5 100.000 (99.484)
after train
n1: 30 for:
wAcc: 74.57214618012956
test acc: 82.22
Epoche: [49/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [49][0/196]	Time 0.113 (0.113)	Data 0.338 (0.338)	Loss 0.3380 (0.3380)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [49][64/196]	Time 0.080 (0.082)	Data 0.000 (0.005)	Loss 0.3827 (0.3769)	Acc@1 86.719 (86.995)	Acc@5 100.000 (99.543)
Epoch: [49][128/196]	Time 0.079 (0.082)	Data 0.000 (0.003)	Loss 0.3441 (0.3911)	Acc@1 89.453 (86.322)	Acc@5 100.000 (99.558)
Epoch: [49][192/196]	Time 0.074 (0.082)	Data 0.000 (0.002)	Loss 0.4807 (0.3978)	Acc@1 83.594 (86.097)	Acc@5 99.219 (99.524)
after train
n1: 30 for:
wAcc: 75.5527285519008
test acc: 79.2
Epoche: [50/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [50][0/196]	Time 0.102 (0.102)	Data 0.395 (0.395)	Loss 0.3678 (0.3678)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [50][64/196]	Time 0.098 (0.084)	Data 0.000 (0.006)	Loss 0.3410 (0.3867)	Acc@1 88.672 (86.785)	Acc@5 99.609 (99.513)
Epoch: [50][128/196]	Time 0.071 (0.083)	Data 0.000 (0.003)	Loss 0.3383 (0.3942)	Acc@1 88.672 (86.449)	Acc@5 100.000 (99.464)
Epoch: [50][192/196]	Time 0.078 (0.083)	Data 0.000 (0.002)	Loss 0.4317 (0.3951)	Acc@1 88.281 (86.391)	Acc@5 98.828 (99.492)
after train
n1: 30 for:
wAcc: 76.7450338962102
test acc: 79.4
Epoche: [51/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [51][0/196]	Time 0.096 (0.096)	Data 0.367 (0.367)	Loss 0.3108 (0.3108)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [51][64/196]	Time 0.080 (0.084)	Data 0.000 (0.006)	Loss 0.3669 (0.3950)	Acc@1 87.109 (86.310)	Acc@5 100.000 (99.489)
Epoch: [51][128/196]	Time 0.088 (0.084)	Data 0.000 (0.003)	Loss 0.4210 (0.4016)	Acc@1 84.766 (86.034)	Acc@5 99.219 (99.458)
Epoch: [51][192/196]	Time 0.078 (0.085)	Data 0.000 (0.002)	Loss 0.3502 (0.3974)	Acc@1 88.281 (86.195)	Acc@5 100.000 (99.456)
after train
n1: 30 for:
wAcc: 77.23146622672813
test acc: 84.02
Epoche: [52/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [52][0/196]	Time 0.103 (0.103)	Data 0.288 (0.288)	Loss 0.3044 (0.3044)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [52][64/196]	Time 0.089 (0.086)	Data 0.000 (0.005)	Loss 0.4219 (0.3846)	Acc@1 82.812 (86.641)	Acc@5 100.000 (99.609)
Epoch: [52][128/196]	Time 0.084 (0.086)	Data 0.000 (0.002)	Loss 0.4007 (0.3892)	Acc@1 87.109 (86.340)	Acc@5 99.219 (99.567)
Epoch: [52][192/196]	Time 0.078 (0.085)	Data 0.000 (0.002)	Loss 0.3074 (0.3952)	Acc@1 89.453 (86.229)	Acc@5 99.609 (99.516)
after train
n1: 30 for:
wAcc: 77.62606768041151
test acc: 79.16
Epoche: [53/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [53][0/196]	Time 0.091 (0.091)	Data 0.450 (0.450)	Loss 0.3218 (0.3218)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [53][64/196]	Time 0.082 (0.085)	Data 0.000 (0.007)	Loss 0.4394 (0.3845)	Acc@1 84.375 (86.893)	Acc@5 99.219 (99.435)
Epoch: [53][128/196]	Time 0.098 (0.086)	Data 0.000 (0.004)	Loss 0.3630 (0.3881)	Acc@1 88.672 (86.573)	Acc@5 99.219 (99.479)
Epoch: [53][192/196]	Time 0.097 (0.086)	Data 0.000 (0.003)	Loss 0.4719 (0.3925)	Acc@1 85.547 (86.492)	Acc@5 99.609 (99.460)
after train
n1: 30 for:
wAcc: 77.37085524062371
test acc: 76.41
Epoche: [54/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [54][0/196]	Time 0.118 (0.118)	Data 0.424 (0.424)	Loss 0.3840 (0.3840)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [54][64/196]	Time 0.080 (0.084)	Data 0.000 (0.007)	Loss 0.4002 (0.4025)	Acc@1 88.672 (86.352)	Acc@5 99.219 (99.417)
Epoch: [54][128/196]	Time 0.087 (0.085)	Data 0.000 (0.004)	Loss 0.4523 (0.3951)	Acc@1 83.984 (86.525)	Acc@5 99.219 (99.470)
Epoch: [54][192/196]	Time 0.089 (0.084)	Data 0.000 (0.002)	Loss 0.3275 (0.3947)	Acc@1 87.891 (86.429)	Acc@5 100.000 (99.488)
after train
n1: 30 for:
wAcc: 77.54160868709937
test acc: 79.7
Epoche: [55/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [55][0/196]	Time 0.128 (0.128)	Data 0.325 (0.325)	Loss 0.4174 (0.4174)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [55][64/196]	Time 0.083 (0.082)	Data 0.000 (0.005)	Loss 0.4207 (0.3895)	Acc@1 87.109 (86.358)	Acc@5 99.609 (99.429)
Epoch: [55][128/196]	Time 0.080 (0.083)	Data 0.000 (0.003)	Loss 0.3527 (0.3929)	Acc@1 88.281 (86.355)	Acc@5 99.609 (99.467)
Epoch: [55][192/196]	Time 0.103 (0.085)	Data 0.000 (0.002)	Loss 0.4496 (0.3947)	Acc@1 85.938 (86.304)	Acc@5 99.219 (99.488)
after train
n1: 30 for:
wAcc: 77.84276868365669
test acc: 80.11
Epoche: [56/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [56][0/196]	Time 0.106 (0.106)	Data 0.333 (0.333)	Loss 0.3428 (0.3428)	Acc@1 91.016 (91.016)	Acc@5 99.609 (99.609)
Epoch: [56][64/196]	Time 0.089 (0.082)	Data 0.000 (0.005)	Loss 0.3947 (0.3940)	Acc@1 86.719 (86.340)	Acc@5 99.219 (99.441)
Epoch: [56][128/196]	Time 0.086 (0.082)	Data 0.000 (0.003)	Loss 0.4705 (0.3919)	Acc@1 84.766 (86.413)	Acc@5 99.219 (99.485)
Epoch: [56][192/196]	Time 0.079 (0.082)	Data 0.000 (0.002)	Loss 0.3873 (0.3932)	Acc@1 86.328 (86.381)	Acc@5 99.609 (99.488)
after train
n1: 30 for:
wAcc: 75.98397287284857
test acc: 70.23
Epoche: [57/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [57][0/196]	Time 0.120 (0.120)	Data 0.399 (0.399)	Loss 0.3762 (0.3762)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [57][64/196]	Time 0.079 (0.085)	Data 0.000 (0.006)	Loss 0.3942 (0.3980)	Acc@1 86.328 (86.412)	Acc@5 99.219 (99.489)
Epoch: [57][128/196]	Time 0.079 (0.084)	Data 0.000 (0.003)	Loss 0.4381 (0.3978)	Acc@1 87.891 (86.274)	Acc@5 98.828 (99.509)
Epoch: [57][192/196]	Time 0.078 (0.085)	Data 0.000 (0.002)	Loss 0.4143 (0.3950)	Acc@1 84.766 (86.427)	Acc@5 99.609 (99.506)
after train
n1: 30 for:
wAcc: 77.81586694891786
test acc: 74.28
Epoche: [58/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [58][0/196]	Time 0.154 (0.154)	Data 0.390 (0.390)	Loss 0.3248 (0.3248)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [58][64/196]	Time 0.081 (0.082)	Data 0.000 (0.006)	Loss 0.3961 (0.3827)	Acc@1 86.719 (86.767)	Acc@5 99.219 (99.501)
Epoch: [58][128/196]	Time 0.078 (0.083)	Data 0.000 (0.003)	Loss 0.4081 (0.3839)	Acc@1 85.156 (86.900)	Acc@5 98.828 (99.497)
Epoch: [58][192/196]	Time 0.098 (0.084)	Data 0.000 (0.002)	Loss 0.3384 (0.3861)	Acc@1 87.500 (86.741)	Acc@5 100.000 (99.504)
after train
n1: 30 for:
wAcc: 77.93469423798358
test acc: 78.31
Epoche: [59/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [59][0/196]	Time 0.091 (0.091)	Data 0.350 (0.350)	Loss 0.3897 (0.3897)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [59][64/196]	Time 0.081 (0.082)	Data 0.000 (0.006)	Loss 0.3523 (0.3907)	Acc@1 85.938 (86.569)	Acc@5 100.000 (99.567)
Epoch: [59][128/196]	Time 0.079 (0.082)	Data 0.000 (0.003)	Loss 0.4005 (0.3885)	Acc@1 85.547 (86.613)	Acc@5 99.609 (99.519)
Epoch: [59][192/196]	Time 0.078 (0.083)	Data 0.000 (0.002)	Loss 0.4091 (0.3914)	Acc@1 85.547 (86.433)	Acc@5 99.609 (99.502)
after train
n1: 30 for:
wAcc: 77.28669627177284
test acc: 77.46
Epoche: [60/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [60][0/196]	Time 0.117 (0.117)	Data 0.360 (0.360)	Loss 0.3947 (0.3947)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [60][64/196]	Time 0.085 (0.086)	Data 0.000 (0.006)	Loss 0.4594 (0.3778)	Acc@1 83.594 (87.007)	Acc@5 100.000 (99.507)
Epoch: [60][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.3801 (0.3775)	Acc@1 85.156 (86.985)	Acc@5 100.000 (99.579)
Epoch: [60][192/196]	Time 0.078 (0.086)	Data 0.000 (0.002)	Loss 0.4107 (0.3835)	Acc@1 84.766 (86.788)	Acc@5 98.828 (99.534)
after train
n1: 30 for:
wAcc: 77.75613729392494
test acc: 80.63
Epoche: [61/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [61][0/196]	Time 0.135 (0.135)	Data 0.413 (0.413)	Loss 0.3404 (0.3404)	Acc@1 89.062 (89.062)	Acc@5 99.609 (99.609)
Epoch: [61][64/196]	Time 0.082 (0.088)	Data 0.000 (0.007)	Loss 0.3216 (0.3678)	Acc@1 89.844 (87.272)	Acc@5 99.609 (99.531)
Epoch: [61][128/196]	Time 0.088 (0.086)	Data 0.000 (0.003)	Loss 0.3385 (0.3795)	Acc@1 88.672 (86.831)	Acc@5 100.000 (99.506)
Epoch: [61][192/196]	Time 0.073 (0.086)	Data 0.000 (0.002)	Loss 0.4131 (0.3849)	Acc@1 86.719 (86.757)	Acc@5 100.000 (99.514)
after train
n1: 30 for:
wAcc: 77.98780748940884
test acc: 78.94
Epoche: [62/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [62][0/196]	Time 0.099 (0.099)	Data 0.360 (0.360)	Loss 0.3902 (0.3902)	Acc@1 86.328 (86.328)	Acc@5 98.438 (98.438)
Epoch: [62][64/196]	Time 0.078 (0.083)	Data 0.000 (0.006)	Loss 0.2842 (0.3794)	Acc@1 90.234 (86.743)	Acc@5 99.609 (99.585)
Epoch: [62][128/196]	Time 0.087 (0.085)	Data 0.000 (0.003)	Loss 0.3836 (0.3793)	Acc@1 88.672 (86.831)	Acc@5 99.219 (99.546)
Epoch: [62][192/196]	Time 0.074 (0.085)	Data 0.000 (0.002)	Loss 0.4147 (0.3811)	Acc@1 84.375 (86.792)	Acc@5 98.828 (99.534)
after train
n1: 30 for:
wAcc: 77.39726664112011
test acc: 72.76
Epoche: [63/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [63][0/196]	Time 0.115 (0.115)	Data 0.314 (0.314)	Loss 0.3761 (0.3761)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [63][64/196]	Time 0.080 (0.085)	Data 0.000 (0.005)	Loss 0.3013 (0.3837)	Acc@1 89.453 (86.424)	Acc@5 100.000 (99.567)
Epoch: [63][128/196]	Time 0.080 (0.084)	Data 0.000 (0.003)	Loss 0.4547 (0.3948)	Acc@1 85.156 (86.216)	Acc@5 100.000 (99.522)
Epoch: [63][192/196]	Time 0.086 (0.086)	Data 0.000 (0.002)	Loss 0.4540 (0.3931)	Acc@1 82.422 (86.385)	Acc@5 98.828 (99.500)
after train
n1: 30 for:
wAcc: 76.7309017927476
test acc: 80.39
Epoche: [64/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [64][0/196]	Time 0.109 (0.109)	Data 0.288 (0.288)	Loss 0.3703 (0.3703)	Acc@1 87.109 (87.109)	Acc@5 98.828 (98.828)
Epoch: [64][64/196]	Time 0.084 (0.086)	Data 0.000 (0.005)	Loss 0.3367 (0.3768)	Acc@1 88.281 (86.875)	Acc@5 99.219 (99.591)
Epoch: [64][128/196]	Time 0.078 (0.087)	Data 0.000 (0.002)	Loss 0.4503 (0.3780)	Acc@1 84.766 (86.910)	Acc@5 99.219 (99.558)
Epoch: [64][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.3542 (0.3844)	Acc@1 85.547 (86.652)	Acc@5 100.000 (99.506)
after train
n1: 30 for:
wAcc: 78.20008272811052
test acc: 79.56
Epoche: [65/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [65][0/196]	Time 0.105 (0.105)	Data 0.400 (0.400)	Loss 0.4409 (0.4409)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [65][64/196]	Time 0.081 (0.088)	Data 0.000 (0.006)	Loss 0.3821 (0.3786)	Acc@1 88.672 (87.055)	Acc@5 98.047 (99.549)
Epoch: [65][128/196]	Time 0.080 (0.085)	Data 0.000 (0.003)	Loss 0.4284 (0.3776)	Acc@1 85.547 (86.885)	Acc@5 99.609 (99.579)
Epoch: [65][192/196]	Time 0.089 (0.084)	Data 0.000 (0.002)	Loss 0.3625 (0.3837)	Acc@1 88.672 (86.737)	Acc@5 99.219 (99.545)
after train
n1: 30 for:
wAcc: 78.15193146248859
test acc: 80.98
Epoche: [66/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [66][0/196]	Time 0.094 (0.094)	Data 0.416 (0.416)	Loss 0.4118 (0.4118)	Acc@1 87.500 (87.500)	Acc@5 98.828 (98.828)
Epoch: [66][64/196]	Time 0.074 (0.081)	Data 0.000 (0.007)	Loss 0.4658 (0.3829)	Acc@1 84.375 (86.767)	Acc@5 99.609 (99.627)
Epoch: [66][128/196]	Time 0.086 (0.083)	Data 0.000 (0.003)	Loss 0.3544 (0.3866)	Acc@1 87.500 (86.555)	Acc@5 99.609 (99.573)
Epoch: [66][192/196]	Time 0.084 (0.082)	Data 0.000 (0.002)	Loss 0.3758 (0.3833)	Acc@1 88.281 (86.745)	Acc@5 100.000 (99.557)
after train
n1: 30 for:
wAcc: 77.62169901995915
test acc: 79.72
Epoche: [67/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [67][0/196]	Time 0.141 (0.141)	Data 0.408 (0.408)	Loss 0.3981 (0.3981)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [67][64/196]	Time 0.079 (0.087)	Data 0.000 (0.007)	Loss 0.3946 (0.3603)	Acc@1 86.719 (87.320)	Acc@5 99.609 (99.603)
Epoch: [67][128/196]	Time 0.090 (0.085)	Data 0.000 (0.003)	Loss 0.4302 (0.3801)	Acc@1 85.547 (86.897)	Acc@5 98.047 (99.503)
Epoch: [67][192/196]	Time 0.123 (0.086)	Data 0.000 (0.002)	Loss 0.5037 (0.3855)	Acc@1 81.641 (86.688)	Acc@5 99.219 (99.486)
after train
n1: 30 for:
wAcc: 78.32953704341797
test acc: 83.21
Epoche: [68/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [68][0/196]	Time 0.132 (0.132)	Data 0.313 (0.313)	Loss 0.3037 (0.3037)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [68][64/196]	Time 0.082 (0.086)	Data 0.000 (0.005)	Loss 0.3471 (0.3765)	Acc@1 87.500 (86.827)	Acc@5 99.609 (99.603)
Epoch: [68][128/196]	Time 0.085 (0.085)	Data 0.000 (0.003)	Loss 0.3437 (0.3791)	Acc@1 86.328 (86.791)	Acc@5 100.000 (99.561)
Epoch: [68][192/196]	Time 0.079 (0.085)	Data 0.000 (0.002)	Loss 0.3729 (0.3836)	Acc@1 88.281 (86.684)	Acc@5 98.828 (99.506)
after train
n1: 30 for:
wAcc: 78.63717754339979
test acc: 80.34
Epoche: [69/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [69][0/196]	Time 0.141 (0.141)	Data 0.344 (0.344)	Loss 0.3622 (0.3622)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [69][64/196]	Time 0.075 (0.088)	Data 0.000 (0.006)	Loss 0.4546 (0.3839)	Acc@1 84.375 (86.899)	Acc@5 99.219 (99.549)
Epoch: [69][128/196]	Time 0.079 (0.087)	Data 0.000 (0.003)	Loss 0.4065 (0.3815)	Acc@1 83.984 (87.000)	Acc@5 100.000 (99.567)
Epoch: [69][192/196]	Time 0.078 (0.087)	Data 0.000 (0.002)	Loss 0.4050 (0.3836)	Acc@1 86.328 (86.907)	Acc@5 100.000 (99.539)
after train
n1: 30 for:
wAcc: 78.97978116388995
test acc: 82.73
Epoche: [70/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [70][0/196]	Time 0.108 (0.108)	Data 0.430 (0.430)	Loss 0.3238 (0.3238)	Acc@1 89.453 (89.453)	Acc@5 100.000 (100.000)
Epoch: [70][64/196]	Time 0.083 (0.084)	Data 0.000 (0.007)	Loss 0.4785 (0.3700)	Acc@1 82.422 (87.374)	Acc@5 99.609 (99.537)
Epoch: [70][128/196]	Time 0.084 (0.085)	Data 0.000 (0.004)	Loss 0.3871 (0.3765)	Acc@1 88.281 (87.037)	Acc@5 99.219 (99.522)
Epoch: [70][192/196]	Time 0.079 (0.086)	Data 0.000 (0.002)	Loss 0.3787 (0.3827)	Acc@1 86.719 (86.852)	Acc@5 100.000 (99.504)
after train
n1: 30 for:
wAcc: 77.23256373855706
test acc: 80.76
Epoche: [71/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [71][0/196]	Time 0.106 (0.106)	Data 0.458 (0.458)	Loss 0.2990 (0.2990)	Acc@1 89.062 (89.062)	Acc@5 99.609 (99.609)
Epoch: [71][64/196]	Time 0.099 (0.093)	Data 0.000 (0.007)	Loss 0.4033 (0.3625)	Acc@1 88.672 (87.368)	Acc@5 99.609 (99.471)
Epoch: [71][128/196]	Time 0.080 (0.093)	Data 0.000 (0.004)	Loss 0.4262 (0.3669)	Acc@1 84.766 (87.164)	Acc@5 100.000 (99.528)
Epoch: [71][192/196]	Time 0.085 (0.091)	Data 0.000 (0.003)	Loss 0.4740 (0.3753)	Acc@1 82.812 (86.974)	Acc@5 99.219 (99.526)
after train
n1: 30 for:
wAcc: 79.0748928659568
test acc: 77.18
Epoche: [72/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [72][0/196]	Time 0.151 (0.151)	Data 0.362 (0.362)	Loss 0.3663 (0.3663)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [72][64/196]	Time 0.111 (0.093)	Data 0.000 (0.006)	Loss 0.3707 (0.3683)	Acc@1 87.500 (87.019)	Acc@5 100.000 (99.603)
Epoch: [72][128/196]	Time 0.107 (0.094)	Data 0.000 (0.003)	Loss 0.4720 (0.3713)	Acc@1 83.203 (87.012)	Acc@5 99.219 (99.552)
Epoch: [72][192/196]	Time 0.076 (0.092)	Data 0.000 (0.002)	Loss 0.3930 (0.3759)	Acc@1 85.547 (86.873)	Acc@5 98.828 (99.520)
after train
n1: 30 for:
wAcc: 77.86120195613042
test acc: 76.0
Epoche: [73/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [73][0/196]	Time 0.127 (0.127)	Data 0.535 (0.535)	Loss 0.3378 (0.3378)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [73][64/196]	Time 0.086 (0.095)	Data 0.000 (0.008)	Loss 0.3444 (0.3834)	Acc@1 89.453 (86.863)	Acc@5 99.609 (99.579)
Epoch: [73][128/196]	Time 0.081 (0.093)	Data 0.000 (0.004)	Loss 0.3684 (0.3779)	Acc@1 87.500 (87.000)	Acc@5 100.000 (99.588)
Epoch: [73][192/196]	Time 0.078 (0.093)	Data 0.000 (0.003)	Loss 0.4928 (0.3797)	Acc@1 82.031 (86.929)	Acc@5 99.219 (99.573)
after train
n1: 30 for:
wAcc: 78.52464805083025
test acc: 80.8
Epoche: [74/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [74][0/196]	Time 0.146 (0.146)	Data 0.476 (0.476)	Loss 0.3896 (0.3896)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [74][64/196]	Time 0.088 (0.092)	Data 0.000 (0.008)	Loss 0.4438 (0.3834)	Acc@1 85.547 (86.719)	Acc@5 99.609 (99.531)
Epoch: [74][128/196]	Time 0.083 (0.092)	Data 0.000 (0.004)	Loss 0.4030 (0.3796)	Acc@1 86.719 (86.816)	Acc@5 99.219 (99.561)
Epoch: [74][192/196]	Time 0.082 (0.091)	Data 0.000 (0.003)	Loss 0.3641 (0.3778)	Acc@1 87.500 (86.788)	Acc@5 99.609 (99.555)
after train
n1: 30 for:
wAcc: 78.15391457584701
test acc: 81.89
Epoche: [75/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [75][0/196]	Time 0.118 (0.118)	Data 0.439 (0.439)	Loss 0.2893 (0.2893)	Acc@1 91.016 (91.016)	Acc@5 99.609 (99.609)
Epoch: [75][64/196]	Time 0.083 (0.093)	Data 0.000 (0.007)	Loss 0.3860 (0.3775)	Acc@1 85.156 (86.929)	Acc@5 98.438 (99.489)
Epoch: [75][128/196]	Time 0.088 (0.091)	Data 0.000 (0.004)	Loss 0.3322 (0.3729)	Acc@1 89.844 (87.185)	Acc@5 100.000 (99.509)
Epoch: [75][192/196]	Time 0.079 (0.090)	Data 0.000 (0.003)	Loss 0.5013 (0.3846)	Acc@1 83.203 (86.699)	Acc@5 98.828 (99.492)
after train
n1: 30 for:
wAcc: 78.15064331340668
test acc: 83.31
Epoche: [76/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [76][0/196]	Time 0.217 (0.217)	Data 0.363 (0.363)	Loss 0.3982 (0.3982)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [76][64/196]	Time 0.094 (0.093)	Data 0.000 (0.006)	Loss 0.3600 (0.3675)	Acc@1 87.891 (87.151)	Acc@5 98.438 (99.495)
Epoch: [76][128/196]	Time 0.087 (0.093)	Data 0.000 (0.003)	Loss 0.3341 (0.3687)	Acc@1 88.281 (87.212)	Acc@5 99.609 (99.519)
Epoch: [76][192/196]	Time 0.092 (0.092)	Data 0.000 (0.002)	Loss 0.4052 (0.3710)	Acc@1 85.156 (87.158)	Acc@5 99.219 (99.486)
after train
n1: 30 for:
wAcc: 79.82214505519184
test acc: 76.77
Epoche: [77/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [77][0/196]	Time 0.152 (0.152)	Data 0.424 (0.424)	Loss 0.4212 (0.4212)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [77][64/196]	Time 0.087 (0.096)	Data 0.000 (0.007)	Loss 0.4177 (0.3687)	Acc@1 85.547 (87.278)	Acc@5 99.609 (99.585)
Epoch: [77][128/196]	Time 0.091 (0.096)	Data 0.000 (0.004)	Loss 0.3162 (0.3785)	Acc@1 88.672 (86.846)	Acc@5 99.219 (99.579)
Epoch: [77][192/196]	Time 0.078 (0.095)	Data 0.000 (0.002)	Loss 0.3831 (0.3778)	Acc@1 87.891 (86.885)	Acc@5 100.000 (99.565)
after train
n1: 30 for:
wAcc: 79.86375904043672
test acc: 79.89
Epoche: [78/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [78][0/196]	Time 0.091 (0.091)	Data 0.477 (0.477)	Loss 0.4312 (0.4312)	Acc@1 82.422 (82.422)	Acc@5 99.609 (99.609)
Epoch: [78][64/196]	Time 0.090 (0.090)	Data 0.000 (0.008)	Loss 0.3547 (0.3587)	Acc@1 88.672 (87.320)	Acc@5 98.828 (99.555)
Epoch: [78][128/196]	Time 0.088 (0.090)	Data 0.000 (0.004)	Loss 0.4018 (0.3737)	Acc@1 86.328 (86.919)	Acc@5 99.219 (99.509)
Epoch: [78][192/196]	Time 0.082 (0.091)	Data 0.000 (0.003)	Loss 0.3653 (0.3764)	Acc@1 85.938 (86.962)	Acc@5 99.219 (99.502)
after train
n1: 30 for:
wAcc: 79.42887610269622
test acc: 74.51
Epoche: [79/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [79][0/196]	Time 0.169 (0.169)	Data 0.412 (0.412)	Loss 0.2447 (0.2447)	Acc@1 92.969 (92.969)	Acc@5 99.609 (99.609)
Epoch: [79][64/196]	Time 0.107 (0.093)	Data 0.000 (0.007)	Loss 0.2956 (0.3546)	Acc@1 90.234 (87.650)	Acc@5 100.000 (99.561)
Epoch: [79][128/196]	Time 0.089 (0.091)	Data 0.000 (0.003)	Loss 0.3464 (0.3685)	Acc@1 87.891 (87.215)	Acc@5 99.609 (99.525)
Epoch: [79][192/196]	Time 0.082 (0.091)	Data 0.000 (0.002)	Loss 0.3669 (0.3711)	Acc@1 88.672 (87.168)	Acc@5 99.609 (99.506)
after train
n1: 30 for:
wAcc: 79.14044156880956
test acc: 77.86
Epoche: [80/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [80][0/196]	Time 0.155 (0.155)	Data 0.456 (0.456)	Loss 0.3856 (0.3856)	Acc@1 87.109 (87.109)	Acc@5 98.828 (98.828)
Epoch: [80][64/196]	Time 0.078 (0.095)	Data 0.000 (0.007)	Loss 0.2813 (0.3569)	Acc@1 90.625 (87.788)	Acc@5 100.000 (99.537)
Epoch: [80][128/196]	Time 0.087 (0.093)	Data 0.000 (0.004)	Loss 0.3345 (0.3630)	Acc@1 89.844 (87.654)	Acc@5 100.000 (99.522)
Epoch: [80][192/196]	Time 0.087 (0.092)	Data 0.000 (0.003)	Loss 0.4039 (0.3741)	Acc@1 84.766 (87.200)	Acc@5 99.609 (99.545)
after train
n1: 30 for:
wAcc: 79.72570682980029
test acc: 76.37
Epoche: [81/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [81][0/196]	Time 0.118 (0.118)	Data 0.399 (0.399)	Loss 0.3141 (0.3141)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [81][64/196]	Time 0.082 (0.090)	Data 0.000 (0.006)	Loss 0.3288 (0.3662)	Acc@1 88.672 (87.644)	Acc@5 99.609 (99.609)
Epoch: [81][128/196]	Time 0.087 (0.092)	Data 0.000 (0.003)	Loss 0.3326 (0.3670)	Acc@1 88.281 (87.570)	Acc@5 99.609 (99.522)
Epoch: [81][192/196]	Time 0.089 (0.090)	Data 0.000 (0.002)	Loss 0.3319 (0.3686)	Acc@1 88.281 (87.413)	Acc@5 100.000 (99.530)
after train
n1: 30 for:
wAcc: 78.80664044677388
test acc: 80.45
Epoche: [82/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [82][0/196]	Time 0.139 (0.139)	Data 0.401 (0.401)	Loss 0.4022 (0.4022)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [82][64/196]	Time 0.084 (0.092)	Data 0.000 (0.006)	Loss 0.4039 (0.3657)	Acc@1 85.547 (87.536)	Acc@5 99.609 (99.555)
Epoch: [82][128/196]	Time 0.089 (0.089)	Data 0.000 (0.003)	Loss 0.4521 (0.3712)	Acc@1 84.766 (87.300)	Acc@5 100.000 (99.558)
Epoch: [82][192/196]	Time 0.090 (0.091)	Data 0.000 (0.002)	Loss 0.3700 (0.3765)	Acc@1 86.328 (87.122)	Acc@5 99.219 (99.539)
after train
n1: 30 for:
wAcc: 78.51511936133815
test acc: 81.72
Epoche: [83/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [83][0/196]	Time 0.148 (0.148)	Data 0.362 (0.362)	Loss 0.3037 (0.3037)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [83][64/196]	Time 0.079 (0.088)	Data 0.000 (0.006)	Loss 0.3753 (0.3724)	Acc@1 85.547 (87.308)	Acc@5 100.000 (99.477)
Epoch: [83][128/196]	Time 0.085 (0.091)	Data 0.000 (0.003)	Loss 0.3348 (0.3716)	Acc@1 87.109 (87.230)	Acc@5 100.000 (99.516)
Epoch: [83][192/196]	Time 0.088 (0.090)	Data 0.000 (0.002)	Loss 0.3784 (0.3739)	Acc@1 87.500 (87.146)	Acc@5 100.000 (99.524)
after train
n1: 30 for:
wAcc: 79.19749337748428
test acc: 79.67
Epoche: [84/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [84][0/196]	Time 0.135 (0.135)	Data 0.338 (0.338)	Loss 0.3108 (0.3108)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [84][64/196]	Time 0.109 (0.091)	Data 0.000 (0.005)	Loss 0.3224 (0.3639)	Acc@1 88.281 (87.350)	Acc@5 100.000 (99.531)
Epoch: [84][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.3936 (0.3607)	Acc@1 88.281 (87.415)	Acc@5 99.609 (99.579)
Epoch: [84][192/196]	Time 0.130 (0.089)	Data 0.000 (0.002)	Loss 0.3600 (0.3639)	Acc@1 85.938 (87.350)	Acc@5 98.828 (99.553)
after train
n1: 30 for:
wAcc: 79.28724791418071
test acc: 77.43
Epoche: [85/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [85][0/196]	Time 0.159 (0.159)	Data 0.468 (0.468)	Loss 0.3720 (0.3720)	Acc@1 86.328 (86.328)	Acc@5 98.828 (98.828)
Epoch: [85][64/196]	Time 0.087 (0.091)	Data 0.000 (0.007)	Loss 0.4144 (0.3668)	Acc@1 85.938 (87.212)	Acc@5 100.000 (99.567)
Epoch: [85][128/196]	Time 0.078 (0.091)	Data 0.000 (0.004)	Loss 0.3230 (0.3687)	Acc@1 87.109 (87.212)	Acc@5 100.000 (99.546)
Epoch: [85][192/196]	Time 0.079 (0.091)	Data 0.000 (0.003)	Loss 0.3951 (0.3689)	Acc@1 87.109 (87.346)	Acc@5 100.000 (99.526)
after train
n1: 30 for:
wAcc: 77.73915728254447
test acc: 80.88
Epoche: [86/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [86][0/196]	Time 0.163 (0.163)	Data 0.391 (0.391)	Loss 0.4806 (0.4806)	Acc@1 82.812 (82.812)	Acc@5 98.828 (98.828)
Epoch: [86][64/196]	Time 0.086 (0.096)	Data 0.000 (0.006)	Loss 0.4124 (0.3668)	Acc@1 88.281 (87.386)	Acc@5 99.219 (99.537)
Epoch: [86][128/196]	Time 0.092 (0.094)	Data 0.000 (0.003)	Loss 0.3351 (0.3675)	Acc@1 89.453 (87.343)	Acc@5 99.609 (99.597)
Epoch: [86][192/196]	Time 0.085 (0.092)	Data 0.000 (0.002)	Loss 0.3052 (0.3638)	Acc@1 89.453 (87.559)	Acc@5 99.609 (99.595)
after train
n1: 30 for:
wAcc: 78.5272666034077
test acc: 80.83
Epoche: [87/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [87][0/196]	Time 0.117 (0.117)	Data 0.476 (0.476)	Loss 0.4086 (0.4086)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [87][64/196]	Time 0.107 (0.093)	Data 0.000 (0.008)	Loss 0.3530 (0.3726)	Acc@1 89.062 (87.440)	Acc@5 100.000 (99.495)
Epoch: [87][128/196]	Time 0.081 (0.089)	Data 0.000 (0.004)	Loss 0.4049 (0.3712)	Acc@1 87.500 (87.430)	Acc@5 99.609 (99.531)
Epoch: [87][192/196]	Time 0.081 (0.090)	Data 0.000 (0.003)	Loss 0.3661 (0.3779)	Acc@1 85.547 (87.122)	Acc@5 100.000 (99.543)
after train
n1: 30 for:
wAcc: 79.2584131240381
test acc: 78.98
Epoche: [88/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [88][0/196]	Time 0.094 (0.094)	Data 0.396 (0.396)	Loss 0.3559 (0.3559)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [88][64/196]	Time 0.091 (0.089)	Data 0.000 (0.007)	Loss 0.4402 (0.3592)	Acc@1 87.109 (87.494)	Acc@5 98.828 (99.597)
Epoch: [88][128/196]	Time 0.091 (0.093)	Data 0.000 (0.003)	Loss 0.3990 (0.3657)	Acc@1 84.766 (87.430)	Acc@5 99.609 (99.543)
Epoch: [88][192/196]	Time 0.087 (0.092)	Data 0.000 (0.002)	Loss 0.3546 (0.3691)	Acc@1 87.500 (87.273)	Acc@5 99.609 (99.534)
after train
n1: 30 for:
wAcc: 79.11757366334692
test acc: 82.41
Epoche: [89/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [89][0/196]	Time 0.139 (0.139)	Data 0.438 (0.438)	Loss 0.3537 (0.3537)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [89][64/196]	Time 0.086 (0.094)	Data 0.000 (0.007)	Loss 0.3464 (0.3646)	Acc@1 86.719 (87.212)	Acc@5 99.609 (99.543)
Epoch: [89][128/196]	Time 0.083 (0.092)	Data 0.000 (0.004)	Loss 0.3933 (0.3690)	Acc@1 84.766 (87.121)	Acc@5 99.609 (99.564)
Epoch: [89][192/196]	Time 0.081 (0.093)	Data 0.000 (0.003)	Loss 0.4639 (0.3703)	Acc@1 82.422 (87.111)	Acc@5 99.219 (99.557)
after train
n1: 30 for:
wAcc: 79.78824840217169
test acc: 82.9
Epoche: [90/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [90][0/196]	Time 0.119 (0.119)	Data 0.395 (0.395)	Loss 0.3050 (0.3050)	Acc@1 89.062 (89.062)	Acc@5 99.609 (99.609)
Epoch: [90][64/196]	Time 0.086 (0.090)	Data 0.000 (0.006)	Loss 0.3954 (0.3748)	Acc@1 84.766 (87.121)	Acc@5 99.609 (99.453)
Epoch: [90][128/196]	Time 0.084 (0.090)	Data 0.000 (0.003)	Loss 0.3928 (0.3721)	Acc@1 86.719 (87.255)	Acc@5 99.609 (99.537)
Epoch: [90][192/196]	Time 0.076 (0.089)	Data 0.000 (0.002)	Loss 0.5073 (0.3753)	Acc@1 82.422 (87.190)	Acc@5 98.047 (99.520)
after train
n1: 30 for:
wAcc: 79.74469753803297
test acc: 79.63


now deeper1
i: 3
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 3; i0=: 16; i1=: 16
i: 4
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 4; i0=: 16; i1=: 16
i: 5
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 5; i0=: 16; i1=: 16
i: 6
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 6; i0=: 16; i1=: 16
i: 7
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 7; i0=: 16; i1=: 16
i: 8
j: 0; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 5; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
i: 8; i0=: 32; i1=: 16
skip: 9
i: 10
j: 0; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 10; i0=: 32; i1=: 32
i: 11
j: 0; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 11; i0=: 32; i1=: 32
i: 12
j: 0; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 12; i0=: 32; i1=: 32
i: 13
j: 0; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 13; i0=: 32; i1=: 32
i: 14
j: 0; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)]
seq[j]: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 5; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
i: 14; i0=: 64; i1=: 32
skip: 15
i: 16
j: 0; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 16; i0=: 64; i1=: 64
i: 17
j: 0; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 17; i0=: 64; i1=: 64
i: 18
j: 0; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 18; i0=: 64; i1=: 64
i: 19
j: 0; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 19; i0=: 64; i1=: 64
N2N(
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (15): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (16): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (15): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (16): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Nums: [[1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [2, 1, 1, 1, 1]]
num: 10; numofstages: 3, listofBlocks: [5, 5, 5], layers in blocj: 1
model.para: <generator object Module.named_parameters at 0x7fe8afd1ba40>
Epoche: [91/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [91][0/196]	Time 0.239 (0.239)	Data 0.428 (0.428)	Loss 2.4653 (2.4653)	Acc@1 8.984 (8.984)	Acc@5 45.703 (45.703)
Epoch: [91][64/196]	Time 0.152 (0.158)	Data 0.000 (0.007)	Loss 2.3163 (2.3322)	Acc@1 10.156 (10.294)	Acc@5 53.906 (51.136)
Epoch: [91][128/196]	Time 0.156 (0.158)	Data 0.000 (0.004)	Loss 2.2819 (2.3136)	Acc@1 12.500 (10.941)	Acc@5 53.516 (52.429)
Epoch: [91][192/196]	Time 0.142 (0.158)	Data 0.000 (0.002)	Loss 2.2128 (2.2950)	Acc@1 16.797 (12.115)	Acc@5 67.578 (54.997)
after train
n1: 30 for:
wAcc: 78.84390727310831
test acc: 11.35
Epoche: [92/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [92][0/196]	Time 0.290 (0.290)	Data 0.379 (0.379)	Loss 2.1989 (2.1989)	Acc@1 17.188 (17.188)	Acc@5 67.188 (67.188)
Epoch: [92][64/196]	Time 0.187 (0.167)	Data 0.005 (0.006)	Loss 2.0000 (2.0896)	Acc@1 17.188 (19.483)	Acc@5 82.812 (73.846)
Epoch: [92][128/196]	Time 0.151 (0.161)	Data 0.000 (0.003)	Loss 1.9953 (2.0407)	Acc@1 26.562 (20.518)	Acc@5 78.125 (76.647)
Epoch: [92][192/196]	Time 0.214 (0.160)	Data 0.000 (0.002)	Loss 1.8365 (1.9978)	Acc@1 27.734 (21.669)	Acc@5 83.984 (78.678)
after train
n1: 30 for:
wAcc: 75.59246632434869
test acc: 25.76
Epoche: [93/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [93][0/196]	Time 0.225 (0.225)	Data 0.423 (0.423)	Loss 1.8873 (1.8873)	Acc@1 23.438 (23.438)	Acc@5 86.719 (86.719)
Epoch: [93][64/196]	Time 0.177 (0.163)	Data 0.000 (0.007)	Loss 1.8453 (1.8421)	Acc@1 29.297 (27.362)	Acc@5 83.984 (84.609)
Epoch: [93][128/196]	Time 0.160 (0.160)	Data 0.000 (0.004)	Loss 1.7191 (1.8183)	Acc@1 28.125 (28.122)	Acc@5 87.891 (85.468)
Epoch: [93][192/196]	Time 0.150 (0.161)	Data 0.000 (0.003)	Loss 1.6917 (1.7922)	Acc@1 33.984 (29.416)	Acc@5 87.109 (86.061)
after train
n1: 30 for:
wAcc: 72.25748240445972
test acc: 31.06
Epoche: [94/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [94][0/196]	Time 0.238 (0.238)	Data 0.370 (0.370)	Loss 1.7064 (1.7064)	Acc@1 33.984 (33.984)	Acc@5 87.891 (87.891)
Epoch: [94][64/196]	Time 0.159 (0.157)	Data 0.000 (0.006)	Loss 1.6725 (1.7253)	Acc@1 37.891 (32.067)	Acc@5 87.500 (87.320)
Epoch: [94][128/196]	Time 0.211 (0.157)	Data 0.000 (0.003)	Loss 1.6809 (1.6816)	Acc@1 31.250 (34.102)	Acc@5 89.844 (88.626)
Epoch: [94][192/196]	Time 0.148 (0.159)	Data 0.000 (0.002)	Loss 1.5732 (1.6513)	Acc@1 37.891 (35.514)	Acc@5 90.234 (89.362)
after train
n1: 30 for:
wAcc: 69.80485772513438
test acc: 21.76
Epoche: [95/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [95][0/196]	Time 0.187 (0.187)	Data 0.624 (0.624)	Loss 1.5583 (1.5583)	Acc@1 37.500 (37.500)	Acc@5 90.625 (90.625)
Epoch: [95][64/196]	Time 0.151 (0.160)	Data 0.000 (0.010)	Loss 1.5576 (1.5537)	Acc@1 39.844 (40.072)	Acc@5 90.625 (91.412)
Epoch: [95][128/196]	Time 0.153 (0.161)	Data 0.000 (0.005)	Loss 1.5606 (1.5333)	Acc@1 41.016 (41.297)	Acc@5 89.844 (91.876)
Epoch: [95][192/196]	Time 0.140 (0.159)	Data 0.000 (0.004)	Loss 1.4482 (1.5141)	Acc@1 46.094 (42.323)	Acc@5 93.359 (92.188)
after train
n1: 30 for:
wAcc: 66.52304192267705
test acc: 40.63
Epoche: [96/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [96][0/196]	Time 0.245 (0.245)	Data 0.472 (0.472)	Loss 1.4975 (1.4975)	Acc@1 41.406 (41.406)	Acc@5 91.797 (91.797)
Epoch: [96][64/196]	Time 0.145 (0.160)	Data 0.000 (0.008)	Loss 1.2640 (1.4315)	Acc@1 55.078 (46.695)	Acc@5 96.094 (93.353)
Epoch: [96][128/196]	Time 0.143 (0.158)	Data 0.000 (0.004)	Loss 1.3501 (1.4103)	Acc@1 50.000 (47.347)	Acc@5 93.359 (93.438)
Epoch: [96][192/196]	Time 0.139 (0.157)	Data 0.000 (0.003)	Loss 1.2728 (1.3903)	Acc@1 53.516 (48.106)	Acc@5 94.141 (93.564)
after train
n1: 30 for:
wAcc: 65.35704292373374
test acc: 46.77
Epoche: [97/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [97][0/196]	Time 0.217 (0.217)	Data 0.417 (0.417)	Loss 1.4257 (1.4257)	Acc@1 51.953 (51.953)	Acc@5 91.406 (91.406)
Epoch: [97][64/196]	Time 0.159 (0.163)	Data 0.000 (0.007)	Loss 1.4439 (1.3243)	Acc@1 42.969 (51.250)	Acc@5 93.750 (94.303)
Epoch: [97][128/196]	Time 0.152 (0.160)	Data 0.000 (0.004)	Loss 1.1257 (1.3080)	Acc@1 60.547 (51.535)	Acc@5 97.266 (94.443)
Epoch: [97][192/196]	Time 0.165 (0.161)	Data 0.000 (0.002)	Loss 1.2448 (1.2943)	Acc@1 55.469 (52.046)	Acc@5 95.703 (94.616)
after train
n1: 30 for:
wAcc: 63.742987194850826
test acc: 35.26
Epoche: [98/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [98][0/196]	Time 0.256 (0.256)	Data 0.345 (0.345)	Loss 1.2407 (1.2407)	Acc@1 55.859 (55.859)	Acc@5 94.141 (94.141)
Epoch: [98][64/196]	Time 0.154 (0.166)	Data 0.000 (0.006)	Loss 1.1019 (1.2338)	Acc@1 62.500 (54.483)	Acc@5 96.094 (95.409)
Epoch: [98][128/196]	Time 0.152 (0.161)	Data 0.000 (0.003)	Loss 1.1266 (1.2358)	Acc@1 57.812 (54.373)	Acc@5 97.656 (95.400)
Epoch: [98][192/196]	Time 0.160 (0.157)	Data 0.000 (0.002)	Loss 1.1205 (1.2221)	Acc@1 60.547 (54.839)	Acc@5 97.266 (95.478)
after train
n1: 30 for:
wAcc: 62.25087723957425
test acc: 47.93
Epoche: [99/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [99][0/196]	Time 0.212 (0.212)	Data 0.337 (0.337)	Loss 1.2200 (1.2200)	Acc@1 53.125 (53.125)	Acc@5 96.484 (96.484)
Epoch: [99][64/196]	Time 0.139 (0.158)	Data 0.000 (0.005)	Loss 1.1138 (1.1805)	Acc@1 61.719 (56.641)	Acc@5 94.922 (95.685)
Epoch: [99][128/196]	Time 0.160 (0.159)	Data 0.000 (0.003)	Loss 1.2072 (1.1734)	Acc@1 58.984 (57.095)	Acc@5 94.141 (95.761)
Epoch: [99][192/196]	Time 0.158 (0.158)	Data 0.000 (0.002)	Loss 1.0957 (1.1624)	Acc@1 62.109 (57.521)	Acc@5 97.656 (95.839)
after train
n1: 30 for:
wAcc: 61.04216340796222
test acc: 57.07
Epoche: [100/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [100][0/196]	Time 0.210 (0.210)	Data 0.341 (0.341)	Loss 1.1000 (1.1000)	Acc@1 58.203 (58.203)	Acc@5 96.875 (96.875)
Epoch: [100][64/196]	Time 0.154 (0.158)	Data 0.000 (0.006)	Loss 1.0823 (1.0919)	Acc@1 58.203 (60.319)	Acc@5 97.656 (96.484)
Epoch: [100][128/196]	Time 0.140 (0.155)	Data 0.000 (0.003)	Loss 1.0833 (1.0870)	Acc@1 61.328 (60.402)	Acc@5 97.656 (96.339)
Epoch: [100][192/196]	Time 0.146 (0.156)	Data 0.000 (0.002)	Loss 0.9980 (1.0839)	Acc@1 64.844 (60.567)	Acc@5 96.094 (96.312)
after train
n1: 30 for:
wAcc: 60.26836442606726
test acc: 52.89
Epoche: [101/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [101][0/196]	Time 0.314 (0.314)	Data 0.362 (0.362)	Loss 0.8916 (0.8916)	Acc@1 68.750 (68.750)	Acc@5 97.656 (97.656)
Epoch: [101][64/196]	Time 0.155 (0.158)	Data 0.000 (0.006)	Loss 1.1312 (1.0448)	Acc@1 55.078 (62.356)	Acc@5 98.828 (96.472)
Epoch: [101][128/196]	Time 0.151 (0.158)	Data 0.000 (0.003)	Loss 1.1886 (1.0405)	Acc@1 58.203 (62.542)	Acc@5 94.531 (96.481)
Epoch: [101][192/196]	Time 0.161 (0.160)	Data 0.000 (0.002)	Loss 1.0529 (1.0338)	Acc@1 62.891 (62.773)	Acc@5 95.703 (96.541)
after train
n1: 30 for:
wAcc: 59.62175827716143
test acc: 56.4
Epoche: [102/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [102][0/196]	Time 0.175 (0.175)	Data 0.495 (0.495)	Loss 0.9914 (0.9914)	Acc@1 67.578 (67.578)	Acc@5 96.484 (96.484)
Epoch: [102][64/196]	Time 0.173 (0.158)	Data 0.000 (0.008)	Loss 0.9263 (0.9898)	Acc@1 65.625 (64.339)	Acc@5 96.484 (96.887)
Epoch: [102][128/196]	Time 0.149 (0.160)	Data 0.000 (0.004)	Loss 1.0080 (0.9839)	Acc@1 61.328 (64.468)	Acc@5 97.266 (96.984)
Epoch: [102][192/196]	Time 0.150 (0.159)	Data 0.000 (0.003)	Loss 0.9512 (0.9723)	Acc@1 66.406 (64.913)	Acc@5 96.875 (96.980)
after train
n1: 30 for:
wAcc: 60.107798379207374
test acc: 60.24
Epoche: [103/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [103][0/196]	Time 0.237 (0.237)	Data 0.379 (0.379)	Loss 0.8819 (0.8819)	Acc@1 67.188 (67.188)	Acc@5 97.656 (97.656)
Epoch: [103][64/196]	Time 0.182 (0.164)	Data 0.000 (0.006)	Loss 0.9679 (0.9341)	Acc@1 65.625 (66.532)	Acc@5 95.703 (97.218)
Epoch: [103][128/196]	Time 0.152 (0.160)	Data 0.000 (0.003)	Loss 0.8413 (0.9317)	Acc@1 67.188 (66.652)	Acc@5 99.219 (97.287)
Epoch: [103][192/196]	Time 0.154 (0.158)	Data 0.000 (0.002)	Loss 0.8362 (0.9301)	Acc@1 71.094 (66.669)	Acc@5 96.875 (97.306)
after train
n1: 30 for:
wAcc: 60.27389961342749
test acc: 51.17
Epoche: [104/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [104][0/196]	Time 0.230 (0.230)	Data 0.414 (0.414)	Loss 0.8416 (0.8416)	Acc@1 69.141 (69.141)	Acc@5 97.656 (97.656)
Epoch: [104][64/196]	Time 0.144 (0.162)	Data 0.000 (0.007)	Loss 0.9340 (0.8911)	Acc@1 64.844 (68.071)	Acc@5 97.266 (97.578)
Epoch: [104][128/196]	Time 0.152 (0.158)	Data 0.000 (0.004)	Loss 0.7779 (0.8937)	Acc@1 72.656 (67.893)	Acc@5 97.656 (97.547)
Epoch: [104][192/196]	Time 0.147 (0.156)	Data 0.000 (0.002)	Loss 0.8750 (0.8912)	Acc@1 65.625 (67.967)	Acc@5 98.047 (97.525)
after train
n1: 30 for:
wAcc: 59.891828662555824
test acc: 62.19
Epoche: [105/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [105][0/196]	Time 0.246 (0.246)	Data 0.353 (0.353)	Loss 1.0387 (1.0387)	Acc@1 64.062 (64.062)	Acc@5 96.484 (96.484)
Epoch: [105][64/196]	Time 0.153 (0.161)	Data 0.000 (0.006)	Loss 0.8700 (0.8707)	Acc@1 67.969 (68.564)	Acc@5 97.266 (97.812)
Epoch: [105][128/196]	Time 0.156 (0.158)	Data 0.000 (0.003)	Loss 0.8279 (0.8650)	Acc@1 74.609 (68.917)	Acc@5 96.875 (97.720)
Epoch: [105][192/196]	Time 0.174 (0.157)	Data 0.000 (0.002)	Loss 0.8860 (0.8563)	Acc@1 67.969 (69.357)	Acc@5 98.047 (97.703)
after train
n1: 30 for:
wAcc: 59.09466519673191
test acc: 58.69
Epoche: [106/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [106][0/196]	Time 0.258 (0.258)	Data 0.501 (0.501)	Loss 0.7621 (0.7621)	Acc@1 75.000 (75.000)	Acc@5 98.047 (98.047)
Epoch: [106][64/196]	Time 0.159 (0.159)	Data 0.000 (0.008)	Loss 0.8672 (0.8302)	Acc@1 69.922 (70.006)	Acc@5 96.094 (98.011)
Epoch: [106][128/196]	Time 0.175 (0.157)	Data 0.000 (0.004)	Loss 0.8737 (0.8258)	Acc@1 68.750 (70.382)	Acc@5 98.828 (97.944)
Epoch: [106][192/196]	Time 0.163 (0.158)	Data 0.000 (0.003)	Loss 0.7964 (0.8203)	Acc@1 70.312 (70.630)	Acc@5 97.656 (98.021)
after train
n1: 30 for:
wAcc: 59.5195898232826
test acc: 63.78
Epoche: [107/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [107][0/196]	Time 0.251 (0.251)	Data 0.357 (0.357)	Loss 0.6951 (0.6951)	Acc@1 72.266 (72.266)	Acc@5 98.438 (98.438)
Epoch: [107][64/196]	Time 0.143 (0.156)	Data 0.000 (0.006)	Loss 0.7242 (0.7818)	Acc@1 72.266 (72.422)	Acc@5 98.438 (98.281)
Epoch: [107][128/196]	Time 0.149 (0.154)	Data 0.000 (0.003)	Loss 0.8772 (0.7810)	Acc@1 66.406 (72.241)	Acc@5 98.438 (98.229)
Epoch: [107][192/196]	Time 0.156 (0.158)	Data 0.000 (0.002)	Loss 0.8217 (0.7800)	Acc@1 69.531 (72.278)	Acc@5 98.047 (98.174)
after train
n1: 30 for:
wAcc: 59.01671381800713
test acc: 58.85
Epoche: [108/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [108][0/196]	Time 0.213 (0.213)	Data 0.355 (0.355)	Loss 0.6483 (0.6483)	Acc@1 76.953 (76.953)	Acc@5 98.438 (98.438)
Epoch: [108][64/196]	Time 0.153 (0.156)	Data 0.000 (0.006)	Loss 0.8006 (0.7744)	Acc@1 71.484 (72.049)	Acc@5 99.219 (98.317)
Epoch: [108][128/196]	Time 0.172 (0.159)	Data 0.000 (0.003)	Loss 0.6405 (0.7639)	Acc@1 76.953 (72.711)	Acc@5 99.609 (98.283)
Epoch: [108][192/196]	Time 0.169 (0.160)	Data 0.000 (0.002)	Loss 0.8198 (0.7628)	Acc@1 73.438 (72.948)	Acc@5 96.875 (98.201)
after train
n1: 30 for:
wAcc: 59.49023930457692
test acc: 67.37
Epoche: [109/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [109][0/196]	Time 0.216 (0.216)	Data 0.424 (0.424)	Loss 0.7221 (0.7221)	Acc@1 76.172 (76.172)	Acc@5 98.047 (98.047)
Epoch: [109][64/196]	Time 0.146 (0.153)	Data 0.000 (0.007)	Loss 0.6450 (0.7297)	Acc@1 79.297 (74.165)	Acc@5 98.828 (98.407)
Epoch: [109][128/196]	Time 0.160 (0.157)	Data 0.000 (0.004)	Loss 0.7547 (0.7411)	Acc@1 72.266 (73.916)	Acc@5 99.219 (98.383)
Epoch: [109][192/196]	Time 0.143 (0.155)	Data 0.000 (0.003)	Loss 0.7110 (0.7385)	Acc@1 75.391 (73.958)	Acc@5 98.828 (98.401)
after train
n1: 30 for:
wAcc: 59.78321424205419
test acc: 70.49
Epoche: [110/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [110][0/196]	Time 0.234 (0.234)	Data 0.397 (0.397)	Loss 0.6728 (0.6728)	Acc@1 74.609 (74.609)	Acc@5 99.219 (99.219)
Epoch: [110][64/196]	Time 0.176 (0.156)	Data 0.000 (0.006)	Loss 0.6259 (0.6905)	Acc@1 75.000 (75.589)	Acc@5 99.609 (98.624)
Epoch: [110][128/196]	Time 0.146 (0.158)	Data 0.000 (0.003)	Loss 0.6829 (0.7010)	Acc@1 79.688 (75.279)	Acc@5 98.828 (98.574)
Epoch: [110][192/196]	Time 0.152 (0.155)	Data 0.000 (0.002)	Loss 0.6576 (0.6989)	Acc@1 76.953 (75.318)	Acc@5 98.828 (98.593)
after train
n1: 30 for:
wAcc: 61.063785767085704
test acc: 63.29
Epoche: [111/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [111][0/196]	Time 0.273 (0.273)	Data 0.304 (0.304)	Loss 0.6142 (0.6142)	Acc@1 78.906 (78.906)	Acc@5 98.828 (98.828)
Epoch: [111][64/196]	Time 0.146 (0.164)	Data 0.000 (0.005)	Loss 0.5901 (0.6865)	Acc@1 79.688 (75.787)	Acc@5 99.609 (98.690)
Epoch: [111][128/196]	Time 0.157 (0.159)	Data 0.000 (0.003)	Loss 0.6470 (0.6863)	Acc@1 80.469 (75.908)	Acc@5 97.656 (98.631)
Epoch: [111][192/196]	Time 0.137 (0.157)	Data 0.000 (0.002)	Loss 0.7344 (0.6838)	Acc@1 74.609 (75.963)	Acc@5 98.438 (98.622)
after train
n1: 30 for:
wAcc: 61.39100566948837
test acc: 69.93
Epoche: [112/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [112][0/196]	Time 0.176 (0.176)	Data 0.385 (0.385)	Loss 0.6742 (0.6742)	Acc@1 75.781 (75.781)	Acc@5 98.047 (98.047)
Epoch: [112][64/196]	Time 0.143 (0.164)	Data 0.000 (0.006)	Loss 0.7627 (0.6552)	Acc@1 72.266 (76.737)	Acc@5 98.438 (98.924)
Epoch: [112][128/196]	Time 0.151 (0.157)	Data 0.000 (0.003)	Loss 0.6994 (0.6540)	Acc@1 76.172 (76.926)	Acc@5 98.828 (98.831)
Epoch: [112][192/196]	Time 0.178 (0.155)	Data 0.000 (0.002)	Loss 0.7248 (0.6575)	Acc@1 75.391 (76.777)	Acc@5 98.438 (98.800)
after train
n1: 30 for:
wAcc: 61.645557337173386
test acc: 69.24
Epoche: [113/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [113][0/196]	Time 0.187 (0.187)	Data 0.353 (0.353)	Loss 0.5656 (0.5656)	Acc@1 80.859 (80.859)	Acc@5 99.219 (99.219)
Epoch: [113][64/196]	Time 0.159 (0.159)	Data 0.000 (0.006)	Loss 0.6064 (0.6507)	Acc@1 77.734 (77.079)	Acc@5 99.609 (98.750)
Epoch: [113][128/196]	Time 0.150 (0.158)	Data 0.000 (0.003)	Loss 0.5321 (0.6419)	Acc@1 80.078 (77.362)	Acc@5 99.219 (98.801)
Epoch: [113][192/196]	Time 0.157 (0.159)	Data 0.000 (0.002)	Loss 0.5797 (0.6344)	Acc@1 80.469 (77.581)	Acc@5 98.828 (98.810)
after train
n1: 30 for:
wAcc: 61.81170349171226
test acc: 63.7
Epoche: [114/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [114][0/196]	Time 0.218 (0.218)	Data 0.335 (0.335)	Loss 0.5449 (0.5449)	Acc@1 80.078 (80.078)	Acc@5 100.000 (100.000)
Epoch: [114][64/196]	Time 0.160 (0.160)	Data 0.000 (0.005)	Loss 0.6344 (0.6264)	Acc@1 76.953 (78.095)	Acc@5 99.609 (98.948)
Epoch: [114][128/196]	Time 0.145 (0.158)	Data 0.000 (0.003)	Loss 0.6553 (0.6164)	Acc@1 75.781 (78.473)	Acc@5 98.438 (98.925)
Epoch: [114][192/196]	Time 0.141 (0.157)	Data 0.000 (0.002)	Loss 0.6192 (0.6171)	Acc@1 76.953 (78.481)	Acc@5 100.000 (98.891)
after train
n1: 30 for:
wAcc: 62.43226644538017
test acc: 72.28
Epoche: [115/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [115][0/196]	Time 0.226 (0.226)	Data 0.491 (0.491)	Loss 0.5732 (0.5732)	Acc@1 78.906 (78.906)	Acc@5 99.609 (99.609)
Epoch: [115][64/196]	Time 0.158 (0.157)	Data 0.000 (0.008)	Loss 0.5606 (0.6017)	Acc@1 80.469 (78.660)	Acc@5 99.609 (99.069)
Epoch: [115][128/196]	Time 0.143 (0.156)	Data 0.000 (0.004)	Loss 0.6059 (0.5978)	Acc@1 77.344 (78.767)	Acc@5 99.609 (99.061)
Epoch: [115][192/196]	Time 0.135 (0.155)	Data 0.000 (0.003)	Loss 0.5989 (0.5948)	Acc@1 78.125 (78.959)	Acc@5 99.219 (99.075)
after train
n1: 30 for:
wAcc: 63.0603760162032
test acc: 77.2
Epoche: [116/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [116][0/196]	Time 0.216 (0.216)	Data 0.364 (0.364)	Loss 0.5855 (0.5855)	Acc@1 79.688 (79.688)	Acc@5 99.609 (99.609)
Epoch: [116][64/196]	Time 0.152 (0.155)	Data 0.000 (0.006)	Loss 0.5802 (0.5878)	Acc@1 77.734 (79.165)	Acc@5 98.828 (99.044)
Epoch: [116][128/196]	Time 0.148 (0.156)	Data 0.003 (0.003)	Loss 0.5614 (0.5866)	Acc@1 80.859 (79.303)	Acc@5 99.219 (99.028)
Epoch: [116][192/196]	Time 0.168 (0.156)	Data 0.000 (0.002)	Loss 0.6257 (0.5831)	Acc@1 80.859 (79.572)	Acc@5 98.047 (98.986)
after train
n1: 30 for:
wAcc: 63.70517094071003
test acc: 75.93
Epoche: [117/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [117][0/196]	Time 0.280 (0.280)	Data 0.403 (0.403)	Loss 0.5320 (0.5320)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [117][64/196]	Time 0.170 (0.158)	Data 0.000 (0.006)	Loss 0.5987 (0.5625)	Acc@1 77.344 (80.319)	Acc@5 98.438 (99.062)
Epoch: [117][128/196]	Time 0.147 (0.159)	Data 0.000 (0.003)	Loss 0.6372 (0.5634)	Acc@1 78.516 (80.426)	Acc@5 99.219 (99.046)
Epoch: [117][192/196]	Time 0.145 (0.156)	Data 0.000 (0.002)	Loss 0.5363 (0.5624)	Acc@1 82.422 (80.394)	Acc@5 98.828 (99.061)
after train
n1: 30 for:
wAcc: 64.9897157310397
test acc: 74.04
Epoche: [118/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [118][0/196]	Time 0.202 (0.202)	Data 0.389 (0.389)	Loss 0.5999 (0.5999)	Acc@1 79.688 (79.688)	Acc@5 98.828 (98.828)
Epoch: [118][64/196]	Time 0.154 (0.154)	Data 0.000 (0.006)	Loss 0.5483 (0.5441)	Acc@1 82.422 (81.148)	Acc@5 98.047 (99.020)
Epoch: [118][128/196]	Time 0.143 (0.159)	Data 0.000 (0.003)	Loss 0.4323 (0.5433)	Acc@1 85.156 (81.029)	Acc@5 98.438 (99.082)
Epoch: [118][192/196]	Time 0.149 (0.157)	Data 0.000 (0.002)	Loss 0.4852 (0.5435)	Acc@1 79.688 (80.950)	Acc@5 99.609 (99.081)
after train
n1: 30 for:
wAcc: 65.64444020176362
test acc: 65.79
Epoche: [119/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [119][0/196]	Time 0.196 (0.196)	Data 0.397 (0.397)	Loss 0.4885 (0.4885)	Acc@1 82.422 (82.422)	Acc@5 100.000 (100.000)
Epoch: [119][64/196]	Time 0.152 (0.158)	Data 0.000 (0.006)	Loss 0.5814 (0.5331)	Acc@1 80.078 (81.472)	Acc@5 98.828 (99.105)
Epoch: [119][128/196]	Time 0.186 (0.156)	Data 0.000 (0.003)	Loss 0.5582 (0.5428)	Acc@1 82.031 (81.238)	Acc@5 98.438 (99.113)
Epoch: [119][192/196]	Time 0.155 (0.156)	Data 0.000 (0.002)	Loss 0.5696 (0.5377)	Acc@1 83.984 (81.501)	Acc@5 97.656 (99.148)
after train
n1: 30 for:
wAcc: 65.18111486430419
test acc: 71.86
Epoche: [120/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [120][0/196]	Time 0.184 (0.184)	Data 0.497 (0.497)	Loss 0.5927 (0.5927)	Acc@1 80.859 (80.859)	Acc@5 99.219 (99.219)
Epoch: [120][64/196]	Time 0.154 (0.156)	Data 0.000 (0.008)	Loss 0.5079 (0.5205)	Acc@1 81.250 (81.857)	Acc@5 99.609 (99.159)
Epoch: [120][128/196]	Time 0.145 (0.157)	Data 0.000 (0.004)	Loss 0.5529 (0.5279)	Acc@1 83.594 (81.565)	Acc@5 98.047 (99.161)
Epoch: [120][192/196]	Time 0.144 (0.157)	Data 0.000 (0.003)	Loss 0.6918 (0.5234)	Acc@1 75.781 (81.788)	Acc@5 97.656 (99.154)
after train
n1: 30 for:
wAcc: 55.74134755096374
test acc: 76.2
Epoche: [121/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [121][0/196]	Time 0.228 (0.228)	Data 0.475 (0.475)	Loss 0.3806 (0.3806)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [121][64/196]	Time 0.156 (0.158)	Data 0.000 (0.008)	Loss 0.4900 (0.5113)	Acc@1 84.766 (82.085)	Acc@5 99.219 (99.201)
Epoch: [121][128/196]	Time 0.161 (0.158)	Data 0.000 (0.004)	Loss 0.5969 (0.5120)	Acc@1 76.953 (82.016)	Acc@5 100.000 (99.207)
Epoch: [121][192/196]	Time 0.170 (0.158)	Data 0.000 (0.003)	Loss 0.5520 (0.5092)	Acc@1 80.469 (82.197)	Acc@5 99.219 (99.172)
after train
n1: 30 for:
wAcc: 59.144392652062365
test acc: 76.8
Epoche: [122/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [122][0/196]	Time 0.245 (0.245)	Data 0.332 (0.332)	Loss 0.5489 (0.5489)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [122][64/196]	Time 0.150 (0.151)	Data 0.000 (0.005)	Loss 0.5541 (0.5048)	Acc@1 82.422 (82.752)	Acc@5 98.438 (99.135)
Epoch: [122][128/196]	Time 0.145 (0.151)	Data 0.000 (0.003)	Loss 0.4111 (0.5049)	Acc@1 85.938 (82.579)	Acc@5 99.219 (99.176)
Epoch: [122][192/196]	Time 0.143 (0.155)	Data 0.000 (0.002)	Loss 0.5976 (0.4988)	Acc@1 79.297 (82.681)	Acc@5 98.438 (99.219)
after train
n1: 30 for:
wAcc: 61.049640347252286
test acc: 76.72
Epoche: [123/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [123][0/196]	Time 0.284 (0.284)	Data 0.342 (0.342)	Loss 0.4209 (0.4209)	Acc@1 86.328 (86.328)	Acc@5 99.609 (99.609)
Epoch: [123][64/196]	Time 0.143 (0.151)	Data 0.000 (0.006)	Loss 0.4308 (0.4935)	Acc@1 83.203 (82.909)	Acc@5 100.000 (99.195)
Epoch: [123][128/196]	Time 0.143 (0.151)	Data 0.000 (0.003)	Loss 0.5950 (0.4946)	Acc@1 80.078 (82.906)	Acc@5 99.219 (99.222)
Epoch: [123][192/196]	Time 0.145 (0.153)	Data 0.000 (0.002)	Loss 0.4728 (0.4910)	Acc@1 85.547 (83.059)	Acc@5 98.438 (99.257)
after train
n1: 30 for:
wAcc: 60.71620881023168
test acc: 77.81
Epoche: [124/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [124][0/196]	Time 0.244 (0.244)	Data 0.443 (0.443)	Loss 0.4732 (0.4732)	Acc@1 81.641 (81.641)	Acc@5 100.000 (100.000)
Epoch: [124][64/196]	Time 0.152 (0.155)	Data 0.000 (0.007)	Loss 0.5529 (0.4821)	Acc@1 82.422 (83.281)	Acc@5 98.828 (99.183)
Epoch: [124][128/196]	Time 0.153 (0.158)	Data 0.000 (0.004)	Loss 0.4189 (0.4798)	Acc@1 85.156 (83.430)	Acc@5 100.000 (99.261)
Epoch: [124][192/196]	Time 0.149 (0.155)	Data 0.000 (0.003)	Loss 0.3997 (0.4836)	Acc@1 86.328 (83.211)	Acc@5 100.000 (99.267)
after train
n1: 30 for:
wAcc: 64.54691063345517
test acc: 79.55
Epoche: [125/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [125][0/196]	Time 0.227 (0.227)	Data 0.523 (0.523)	Loss 0.4863 (0.4863)	Acc@1 85.938 (85.938)	Acc@5 98.047 (98.047)
Epoch: [125][64/196]	Time 0.178 (0.159)	Data 0.000 (0.008)	Loss 0.5400 (0.4666)	Acc@1 80.078 (83.792)	Acc@5 98.828 (99.273)
Epoch: [125][128/196]	Time 0.148 (0.158)	Data 0.000 (0.004)	Loss 0.4563 (0.4724)	Acc@1 83.594 (83.685)	Acc@5 98.438 (99.294)
Epoch: [125][192/196]	Time 0.144 (0.155)	Data 0.000 (0.003)	Loss 0.4883 (0.4731)	Acc@1 82.812 (83.606)	Acc@5 98.438 (99.318)
after train
n1: 30 for:
wAcc: 66.4024598443812
test acc: 72.22
Epoche: [126/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [126][0/196]	Time 0.173 (0.173)	Data 0.445 (0.445)	Loss 0.5438 (0.5438)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)
Epoch: [126][64/196]	Time 0.181 (0.156)	Data 0.000 (0.007)	Loss 0.4752 (0.4639)	Acc@1 83.984 (83.858)	Acc@5 99.219 (99.417)
Epoch: [126][128/196]	Time 0.142 (0.156)	Data 0.000 (0.004)	Loss 0.5008 (0.4623)	Acc@1 83.203 (83.912)	Acc@5 98.828 (99.379)
Epoch: [126][192/196]	Time 0.145 (0.155)	Data 0.000 (0.003)	Loss 0.4073 (0.4581)	Acc@1 86.328 (84.043)	Acc@5 98.828 (99.381)
after train
n1: 30 for:
wAcc: 65.11388149184545
test acc: 78.28
Epoche: [127/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [127][0/196]	Time 0.209 (0.209)	Data 0.314 (0.314)	Loss 0.3755 (0.3755)	Acc@1 90.234 (90.234)	Acc@5 99.609 (99.609)
Epoch: [127][64/196]	Time 0.149 (0.165)	Data 0.000 (0.005)	Loss 0.5680 (0.4639)	Acc@1 82.422 (83.990)	Acc@5 98.438 (99.315)
Epoch: [127][128/196]	Time 0.156 (0.162)	Data 0.000 (0.003)	Loss 0.3703 (0.4527)	Acc@1 86.328 (84.263)	Acc@5 99.609 (99.394)
Epoch: [127][192/196]	Time 0.143 (0.159)	Data 0.000 (0.002)	Loss 0.5753 (0.4478)	Acc@1 82.812 (84.389)	Acc@5 99.219 (99.391)
after train
n1: 30 for:
wAcc: 67.79490342263936
test acc: 79.02
Epoche: [128/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [128][0/196]	Time 0.199 (0.199)	Data 0.440 (0.440)	Loss 0.5042 (0.5042)	Acc@1 85.547 (85.547)	Acc@5 98.828 (98.828)
Epoch: [128][64/196]	Time 0.155 (0.165)	Data 0.000 (0.007)	Loss 0.4664 (0.4394)	Acc@1 84.766 (84.808)	Acc@5 99.609 (99.447)
Epoch: [128][128/196]	Time 0.158 (0.157)	Data 0.000 (0.004)	Loss 0.4036 (0.4401)	Acc@1 86.328 (84.741)	Acc@5 99.609 (99.437)
Epoch: [128][192/196]	Time 0.143 (0.156)	Data 0.000 (0.003)	Loss 0.4143 (0.4419)	Acc@1 83.984 (84.529)	Acc@5 100.000 (99.417)
after train
n1: 30 for:
wAcc: 69.84039583502421
test acc: 80.83
Epoche: [129/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [129][0/196]	Time 0.198 (0.198)	Data 0.527 (0.527)	Loss 0.4308 (0.4308)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [129][64/196]	Time 0.180 (0.154)	Data 0.000 (0.008)	Loss 0.3967 (0.4302)	Acc@1 87.109 (84.832)	Acc@5 99.609 (99.441)
Epoch: [129][128/196]	Time 0.176 (0.156)	Data 0.000 (0.004)	Loss 0.4659 (0.4357)	Acc@1 83.984 (84.678)	Acc@5 100.000 (99.410)
Epoch: [129][192/196]	Time 0.175 (0.157)	Data 0.000 (0.003)	Loss 0.4245 (0.4362)	Acc@1 82.812 (84.804)	Acc@5 100.000 (99.413)
after train
n1: 30 for:
wAcc: 69.94513524606984
test acc: 76.14
Epoche: [130/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [130][0/196]	Time 0.196 (0.196)	Data 0.420 (0.420)	Loss 0.5058 (0.5058)	Acc@1 83.594 (83.594)	Acc@5 99.609 (99.609)
Epoch: [130][64/196]	Time 0.142 (0.160)	Data 0.000 (0.007)	Loss 0.3986 (0.4262)	Acc@1 86.719 (84.820)	Acc@5 99.609 (99.501)
Epoch: [130][128/196]	Time 0.150 (0.158)	Data 0.000 (0.004)	Loss 0.4769 (0.4258)	Acc@1 85.547 (85.053)	Acc@5 97.656 (99.476)
Epoch: [130][192/196]	Time 0.184 (0.158)	Data 0.000 (0.002)	Loss 0.4644 (0.4232)	Acc@1 83.984 (85.219)	Acc@5 98.828 (99.454)
after train
n1: 30 for:
wAcc: 70.85221500579442
test acc: 77.83
Epoche: [131/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [131][0/196]	Time 0.256 (0.256)	Data 0.339 (0.339)	Loss 0.3307 (0.3307)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [131][64/196]	Time 0.155 (0.158)	Data 0.000 (0.006)	Loss 0.3598 (0.4096)	Acc@1 87.109 (85.691)	Acc@5 99.609 (99.537)
Epoch: [131][128/196]	Time 0.223 (0.157)	Data 0.000 (0.003)	Loss 0.4372 (0.4148)	Acc@1 83.984 (85.626)	Acc@5 99.609 (99.488)
Epoch: [131][192/196]	Time 0.132 (0.157)	Data 0.000 (0.002)	Loss 0.3294 (0.4174)	Acc@1 89.062 (85.519)	Acc@5 100.000 (99.484)
after train
n1: 30 for:
wAcc: 71.85751106265275
test acc: 76.32
Epoche: [132/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [132][0/196]	Time 0.203 (0.203)	Data 0.398 (0.398)	Loss 0.5237 (0.5237)	Acc@1 80.859 (80.859)	Acc@5 98.828 (98.828)
Epoch: [132][64/196]	Time 0.148 (0.162)	Data 0.000 (0.006)	Loss 0.4052 (0.4113)	Acc@1 85.156 (85.631)	Acc@5 99.609 (99.489)
Epoch: [132][128/196]	Time 0.152 (0.163)	Data 0.000 (0.003)	Loss 0.3515 (0.4087)	Acc@1 88.672 (85.716)	Acc@5 100.000 (99.488)
Epoch: [132][192/196]	Time 0.143 (0.160)	Data 0.000 (0.002)	Loss 0.4083 (0.4104)	Acc@1 88.281 (85.719)	Acc@5 98.828 (99.520)
after train
n1: 30 for:
wAcc: 70.8342402505464
test acc: 78.36
Epoche: [133/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [133][0/196]	Time 0.217 (0.217)	Data 0.454 (0.454)	Loss 0.3577 (0.3577)	Acc@1 87.891 (87.891)	Acc@5 99.219 (99.219)
Epoch: [133][64/196]	Time 0.153 (0.161)	Data 0.000 (0.007)	Loss 0.4134 (0.4074)	Acc@1 86.719 (85.841)	Acc@5 100.000 (99.423)
Epoch: [133][128/196]	Time 0.156 (0.159)	Data 0.000 (0.004)	Loss 0.3436 (0.4036)	Acc@1 89.062 (85.898)	Acc@5 100.000 (99.482)
Epoch: [133][192/196]	Time 0.143 (0.157)	Data 0.000 (0.003)	Loss 0.3628 (0.4042)	Acc@1 88.672 (85.921)	Acc@5 100.000 (99.468)
after train
n1: 30 for:
wAcc: 72.91284149842488
test acc: 80.51
Epoche: [134/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [134][0/196]	Time 0.242 (0.242)	Data 0.368 (0.368)	Loss 0.4402 (0.4402)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [134][64/196]	Time 0.153 (0.152)	Data 0.000 (0.006)	Loss 0.4488 (0.3917)	Acc@1 83.594 (86.088)	Acc@5 98.828 (99.459)
Epoch: [134][128/196]	Time 0.143 (0.152)	Data 0.000 (0.003)	Loss 0.4155 (0.3939)	Acc@1 86.328 (86.204)	Acc@5 100.000 (99.464)
Epoch: [134][192/196]	Time 0.153 (0.153)	Data 0.000 (0.002)	Loss 0.4751 (0.3958)	Acc@1 88.281 (86.170)	Acc@5 98.438 (99.504)
after train
n1: 30 for:
wAcc: 72.8970153062408
test acc: 75.25
Epoche: [135/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [135][0/196]	Time 0.243 (0.243)	Data 0.367 (0.367)	Loss 0.5019 (0.5019)	Acc@1 82.422 (82.422)	Acc@5 99.609 (99.609)
Epoch: [135][64/196]	Time 0.146 (0.157)	Data 0.000 (0.006)	Loss 0.4244 (0.3933)	Acc@1 84.375 (86.100)	Acc@5 98.828 (99.567)
Epoch: [135][128/196]	Time 0.144 (0.157)	Data 0.000 (0.003)	Loss 0.3732 (0.3927)	Acc@1 87.109 (86.192)	Acc@5 98.438 (99.597)
Epoch: [135][192/196]	Time 0.147 (0.157)	Data 0.000 (0.002)	Loss 0.3188 (0.3895)	Acc@1 88.281 (86.381)	Acc@5 100.000 (99.561)
after train
n1: 30 for:
wAcc: 73.78463909672075
test acc: 79.14
Epoche: [136/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [136][0/196]	Time 0.230 (0.230)	Data 0.392 (0.392)	Loss 0.2943 (0.2943)	Acc@1 91.016 (91.016)	Acc@5 99.219 (99.219)
Epoch: [136][64/196]	Time 0.157 (0.157)	Data 0.000 (0.006)	Loss 0.4930 (0.3984)	Acc@1 82.031 (85.986)	Acc@5 98.828 (99.507)
Epoch: [136][128/196]	Time 0.157 (0.154)	Data 0.000 (0.003)	Loss 0.3073 (0.3876)	Acc@1 89.062 (86.383)	Acc@5 99.219 (99.500)
Epoch: [136][192/196]	Time 0.144 (0.154)	Data 0.000 (0.002)	Loss 0.3002 (0.3855)	Acc@1 89.062 (86.575)	Acc@5 100.000 (99.526)
after train
n1: 30 for:
wAcc: 73.41745777456346
test acc: 82.21
Epoche: [137/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [137][0/196]	Time 0.184 (0.184)	Data 0.453 (0.453)	Loss 0.3681 (0.3681)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [137][64/196]	Time 0.162 (0.156)	Data 0.000 (0.007)	Loss 0.3598 (0.3758)	Acc@1 87.109 (87.169)	Acc@5 99.609 (99.483)
Epoch: [137][128/196]	Time 0.150 (0.156)	Data 0.000 (0.004)	Loss 0.3931 (0.3746)	Acc@1 84.766 (86.982)	Acc@5 99.609 (99.531)
Epoch: [137][192/196]	Time 0.143 (0.155)	Data 0.000 (0.003)	Loss 0.3954 (0.3752)	Acc@1 84.375 (86.873)	Acc@5 98.438 (99.573)
after train
n1: 30 for:
wAcc: 75.216383031011
test acc: 81.92
Epoche: [138/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [138][0/196]	Time 0.239 (0.239)	Data 0.401 (0.401)	Loss 0.3766 (0.3766)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [138][64/196]	Time 0.143 (0.158)	Data 0.000 (0.006)	Loss 0.3665 (0.3620)	Acc@1 87.109 (87.506)	Acc@5 100.000 (99.567)
Epoch: [138][128/196]	Time 0.151 (0.155)	Data 0.000 (0.003)	Loss 0.4340 (0.3664)	Acc@1 84.766 (87.309)	Acc@5 99.219 (99.585)
Epoch: [138][192/196]	Time 0.146 (0.155)	Data 0.000 (0.002)	Loss 0.4473 (0.3670)	Acc@1 83.594 (87.247)	Acc@5 100.000 (99.581)
after train
n1: 30 for:
wAcc: 76.09990650696304
test acc: 81.78
Epoche: [139/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [139][0/196]	Time 0.296 (0.296)	Data 0.379 (0.379)	Loss 0.3517 (0.3517)	Acc@1 90.234 (90.234)	Acc@5 99.609 (99.609)
Epoch: [139][64/196]	Time 0.162 (0.155)	Data 0.000 (0.006)	Loss 0.3898 (0.3533)	Acc@1 87.109 (87.861)	Acc@5 99.609 (99.700)
Epoch: [139][128/196]	Time 0.143 (0.154)	Data 0.000 (0.003)	Loss 0.3896 (0.3570)	Acc@1 86.719 (87.594)	Acc@5 100.000 (99.609)
Epoch: [139][192/196]	Time 0.149 (0.155)	Data 0.000 (0.002)	Loss 0.2669 (0.3618)	Acc@1 90.625 (87.324)	Acc@5 100.000 (99.583)
after train
n1: 30 for:
wAcc: 75.42552093952607
test acc: 80.88
Epoche: [140/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [140][0/196]	Time 0.208 (0.208)	Data 0.421 (0.421)	Loss 0.2972 (0.2972)	Acc@1 90.234 (90.234)	Acc@5 99.609 (99.609)
Epoch: [140][64/196]	Time 0.148 (0.157)	Data 0.000 (0.007)	Loss 0.4780 (0.3486)	Acc@1 82.422 (87.843)	Acc@5 99.609 (99.585)
Epoch: [140][128/196]	Time 0.145 (0.154)	Data 0.000 (0.004)	Loss 0.3315 (0.3555)	Acc@1 90.234 (87.685)	Acc@5 99.609 (99.570)
Epoch: [140][192/196]	Time 0.144 (0.155)	Data 0.000 (0.003)	Loss 0.3308 (0.3569)	Acc@1 90.625 (87.611)	Acc@5 100.000 (99.573)
after train
n1: 30 for:
wAcc: 76.73731155448834
test acc: 78.88
Epoche: [141/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [141][0/196]	Time 0.275 (0.275)	Data 0.334 (0.334)	Loss 0.2518 (0.2518)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [141][64/196]	Time 0.154 (0.158)	Data 0.000 (0.006)	Loss 0.3688 (0.3535)	Acc@1 85.547 (87.542)	Acc@5 99.219 (99.639)
Epoch: [141][128/196]	Time 0.145 (0.157)	Data 0.000 (0.003)	Loss 0.2998 (0.3562)	Acc@1 88.672 (87.512)	Acc@5 99.609 (99.600)
Epoch: [141][192/196]	Time 0.144 (0.155)	Data 0.000 (0.002)	Loss 0.3612 (0.3600)	Acc@1 86.719 (87.413)	Acc@5 99.609 (99.543)
after train
n1: 30 for:
wAcc: 76.77580204421731
test acc: 79.72
Epoche: [142/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [142][0/196]	Time 0.270 (0.270)	Data 0.391 (0.391)	Loss 0.3456 (0.3456)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [142][64/196]	Time 0.147 (0.157)	Data 0.000 (0.006)	Loss 0.2729 (0.3348)	Acc@1 90.625 (88.359)	Acc@5 99.609 (99.669)
Epoch: [142][128/196]	Time 0.142 (0.158)	Data 0.000 (0.003)	Loss 0.4724 (0.3395)	Acc@1 83.203 (88.212)	Acc@5 100.000 (99.649)
Epoch: [142][192/196]	Time 0.143 (0.156)	Data 0.000 (0.002)	Loss 0.3387 (0.3422)	Acc@1 87.500 (88.040)	Acc@5 99.609 (99.650)
after train
n1: 30 for:
wAcc: 76.16487927230331
test acc: 81.76
Epoche: [143/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [143][0/196]	Time 0.265 (0.265)	Data 0.419 (0.419)	Loss 0.2956 (0.2956)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [143][64/196]	Time 0.139 (0.157)	Data 0.000 (0.007)	Loss 0.3545 (0.3372)	Acc@1 88.672 (88.023)	Acc@5 99.609 (99.675)
Epoch: [143][128/196]	Time 0.148 (0.156)	Data 0.000 (0.004)	Loss 0.3391 (0.3370)	Acc@1 89.453 (88.148)	Acc@5 99.609 (99.655)
Epoch: [143][192/196]	Time 0.144 (0.154)	Data 0.000 (0.002)	Loss 0.3790 (0.3445)	Acc@1 86.328 (87.931)	Acc@5 99.219 (99.632)
after train
n1: 30 for:
wAcc: 77.76619296426672
test acc: 80.48
Epoche: [144/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [144][0/196]	Time 0.232 (0.232)	Data 0.452 (0.452)	Loss 0.3166 (0.3166)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [144][64/196]	Time 0.164 (0.165)	Data 0.000 (0.007)	Loss 0.3376 (0.3406)	Acc@1 88.281 (87.951)	Acc@5 99.219 (99.621)
Epoch: [144][128/196]	Time 0.156 (0.162)	Data 0.000 (0.004)	Loss 0.3335 (0.3481)	Acc@1 87.891 (87.748)	Acc@5 100.000 (99.664)
Epoch: [144][192/196]	Time 0.149 (0.159)	Data 0.000 (0.003)	Loss 0.3017 (0.3426)	Acc@1 87.891 (88.000)	Acc@5 99.609 (99.642)
after train
n1: 30 for:
wAcc: 78.65252015078792
test acc: 82.33
Epoche: [145/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [145][0/196]	Time 0.255 (0.255)	Data 0.427 (0.427)	Loss 0.2520 (0.2520)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [145][64/196]	Time 0.144 (0.159)	Data 0.000 (0.007)	Loss 0.3437 (0.3358)	Acc@1 86.719 (88.245)	Acc@5 99.609 (99.615)
Epoch: [145][128/196]	Time 0.148 (0.158)	Data 0.000 (0.004)	Loss 0.3983 (0.3429)	Acc@1 87.891 (88.078)	Acc@5 99.219 (99.615)
Epoch: [145][192/196]	Time 0.155 (0.158)	Data 0.000 (0.003)	Loss 0.3370 (0.3376)	Acc@1 88.281 (88.227)	Acc@5 100.000 (99.622)
after train
n1: 30 for:
wAcc: 78.7061837375547
test acc: 83.21
Epoche: [146/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [146][0/196]	Time 0.236 (0.236)	Data 0.368 (0.368)	Loss 0.2904 (0.2904)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [146][64/196]	Time 0.153 (0.160)	Data 0.000 (0.006)	Loss 0.3265 (0.3313)	Acc@1 88.672 (88.456)	Acc@5 99.219 (99.669)
Epoch: [146][128/196]	Time 0.142 (0.157)	Data 0.000 (0.003)	Loss 0.2486 (0.3302)	Acc@1 91.016 (88.472)	Acc@5 100.000 (99.673)
Epoch: [146][192/196]	Time 0.151 (0.154)	Data 0.000 (0.002)	Loss 0.3130 (0.3316)	Acc@1 89.062 (88.391)	Acc@5 99.609 (99.666)
after train
n1: 30 for:
wAcc: 78.7235311854911
test acc: 83.45
Epoche: [147/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [147][0/196]	Time 0.215 (0.215)	Data 0.476 (0.476)	Loss 0.3395 (0.3395)	Acc@1 90.234 (90.234)	Acc@5 99.219 (99.219)
Epoch: [147][64/196]	Time 0.143 (0.157)	Data 0.000 (0.008)	Loss 0.3396 (0.3111)	Acc@1 87.891 (89.159)	Acc@5 100.000 (99.681)
Epoch: [147][128/196]	Time 0.143 (0.157)	Data 0.000 (0.004)	Loss 0.3502 (0.3164)	Acc@1 87.891 (88.838)	Acc@5 99.609 (99.673)
Epoch: [147][192/196]	Time 0.145 (0.158)	Data 0.000 (0.003)	Loss 0.3101 (0.3215)	Acc@1 87.891 (88.696)	Acc@5 100.000 (99.674)
after train
n1: 30 for:
wAcc: 77.8358318101408
test acc: 80.75
Epoche: [148/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [148][0/196]	Time 0.220 (0.220)	Data 0.351 (0.351)	Loss 0.3204 (0.3204)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [148][64/196]	Time 0.156 (0.151)	Data 0.000 (0.006)	Loss 0.3788 (0.3116)	Acc@1 85.547 (88.918)	Acc@5 100.000 (99.706)
Epoch: [148][128/196]	Time 0.169 (0.152)	Data 0.000 (0.003)	Loss 0.2606 (0.3140)	Acc@1 92.578 (88.990)	Acc@5 100.000 (99.673)
Epoch: [148][192/196]	Time 0.146 (0.153)	Data 0.000 (0.002)	Loss 0.3125 (0.3179)	Acc@1 88.672 (88.884)	Acc@5 99.609 (99.660)
after train
n1: 30 for:
wAcc: 78.90133131356399
test acc: 80.48
Epoche: [149/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [149][0/196]	Time 0.272 (0.272)	Data 0.428 (0.428)	Loss 0.3463 (0.3463)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [149][64/196]	Time 0.159 (0.152)	Data 0.000 (0.007)	Loss 0.3285 (0.3153)	Acc@1 87.500 (89.183)	Acc@5 99.219 (99.694)
Epoch: [149][128/196]	Time 0.147 (0.153)	Data 0.000 (0.004)	Loss 0.2732 (0.3153)	Acc@1 90.234 (89.126)	Acc@5 100.000 (99.718)
Epoch: [149][192/196]	Time 0.216 (0.155)	Data 0.000 (0.003)	Loss 0.3740 (0.3170)	Acc@1 86.719 (88.971)	Acc@5 99.219 (99.694)
after train
n1: 30 for:
wAcc: 79.63057806467155
test acc: 79.41
Epoche: [150/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [150][0/196]	Time 0.292 (0.292)	Data 0.349 (0.349)	Loss 0.2810 (0.2810)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [150][64/196]	Time 0.152 (0.150)	Data 0.000 (0.006)	Loss 0.3615 (0.3246)	Acc@1 87.500 (88.846)	Acc@5 99.609 (99.706)
Epoch: [150][128/196]	Time 0.166 (0.153)	Data 0.000 (0.003)	Loss 0.3294 (0.3216)	Acc@1 87.891 (88.914)	Acc@5 100.000 (99.697)
Epoch: [150][192/196]	Time 0.174 (0.154)	Data 0.000 (0.002)	Loss 0.3317 (0.3187)	Acc@1 89.453 (88.996)	Acc@5 100.000 (99.676)
after train
n1: 30 for:
wAcc: 79.70308415613528
test acc: 79.29
Epoche: [151/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [151][0/196]	Time 0.293 (0.293)	Data 0.400 (0.400)	Loss 0.2950 (0.2950)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [151][64/196]	Time 0.149 (0.159)	Data 0.000 (0.006)	Loss 0.3347 (0.3004)	Acc@1 89.453 (89.303)	Acc@5 99.609 (99.748)
Epoch: [151][128/196]	Time 0.153 (0.158)	Data 0.000 (0.003)	Loss 0.3347 (0.3055)	Acc@1 89.453 (89.329)	Acc@5 99.219 (99.706)
Epoch: [151][192/196]	Time 0.153 (0.155)	Data 0.000 (0.002)	Loss 0.2908 (0.3084)	Acc@1 91.406 (89.180)	Acc@5 99.219 (99.678)
after train
n1: 30 for:
wAcc: 79.66486864083744
test acc: 84.0
Epoche: [152/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [152][0/196]	Time 0.210 (0.210)	Data 0.358 (0.358)	Loss 0.2870 (0.2870)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [152][64/196]	Time 0.172 (0.158)	Data 0.000 (0.006)	Loss 0.2745 (0.2907)	Acc@1 90.625 (89.970)	Acc@5 100.000 (99.730)
Epoch: [152][128/196]	Time 0.147 (0.155)	Data 0.000 (0.003)	Loss 0.2611 (0.2965)	Acc@1 90.625 (89.732)	Acc@5 100.000 (99.724)
Epoch: [152][192/196]	Time 0.169 (0.154)	Data 0.000 (0.002)	Loss 0.2638 (0.2998)	Acc@1 89.844 (89.619)	Acc@5 99.609 (99.713)
after train
n1: 30 for:
wAcc: 80.10212663237178
test acc: 80.3
Epoche: [153/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [153][0/196]	Time 0.249 (0.249)	Data 0.405 (0.405)	Loss 0.2952 (0.2952)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [153][64/196]	Time 0.168 (0.156)	Data 0.000 (0.007)	Loss 0.3065 (0.2972)	Acc@1 91.016 (89.279)	Acc@5 99.609 (99.748)
Epoch: [153][128/196]	Time 0.171 (0.158)	Data 0.000 (0.004)	Loss 0.2716 (0.2978)	Acc@1 91.016 (89.344)	Acc@5 100.000 (99.709)
Epoch: [153][192/196]	Time 0.133 (0.159)	Data 0.000 (0.003)	Loss 0.2638 (0.3001)	Acc@1 90.234 (89.342)	Acc@5 100.000 (99.709)
after train
n1: 30 for:
wAcc: 80.3664297656924
test acc: 82.98
Epoche: [154/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [154][0/196]	Time 0.221 (0.221)	Data 0.362 (0.362)	Loss 0.2759 (0.2759)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [154][64/196]	Time 0.143 (0.154)	Data 0.000 (0.006)	Loss 0.3129 (0.2974)	Acc@1 88.281 (89.477)	Acc@5 99.219 (99.778)
Epoch: [154][128/196]	Time 0.168 (0.155)	Data 0.000 (0.003)	Loss 0.2478 (0.2976)	Acc@1 89.453 (89.550)	Acc@5 100.000 (99.746)
Epoch: [154][192/196]	Time 0.143 (0.153)	Data 0.000 (0.002)	Loss 0.2470 (0.2975)	Acc@1 89.844 (89.583)	Acc@5 99.609 (99.711)
after train
n1: 30 for:
wAcc: 79.47541098557326
test acc: 83.83
Epoche: [155/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [155][0/196]	Time 0.217 (0.217)	Data 0.378 (0.378)	Loss 0.3097 (0.3097)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [155][64/196]	Time 0.143 (0.161)	Data 0.000 (0.006)	Loss 0.2369 (0.2886)	Acc@1 90.234 (89.681)	Acc@5 100.000 (99.790)
Epoch: [155][128/196]	Time 0.147 (0.158)	Data 0.000 (0.003)	Loss 0.2765 (0.2906)	Acc@1 91.016 (89.674)	Acc@5 100.000 (99.761)
Epoch: [155][192/196]	Time 0.140 (0.157)	Data 0.000 (0.002)	Loss 0.2496 (0.2914)	Acc@1 89.062 (89.647)	Acc@5 100.000 (99.755)
after train
n1: 30 for:
wAcc: 80.63239524920256
test acc: 79.56
Epoche: [156/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [156][0/196]	Time 0.271 (0.271)	Data 0.459 (0.459)	Loss 0.2576 (0.2576)	Acc@1 91.406 (91.406)	Acc@5 99.219 (99.219)
Epoch: [156][64/196]	Time 0.146 (0.152)	Data 0.000 (0.007)	Loss 0.2956 (0.2837)	Acc@1 89.453 (89.796)	Acc@5 99.609 (99.730)
Epoch: [156][128/196]	Time 0.138 (0.151)	Data 0.000 (0.004)	Loss 0.2607 (0.2848)	Acc@1 89.453 (89.826)	Acc@5 100.000 (99.743)
Epoch: [156][192/196]	Time 0.187 (0.151)	Data 0.000 (0.003)	Loss 0.2544 (0.2912)	Acc@1 93.359 (89.651)	Acc@5 99.609 (99.749)
after train
n1: 30 for:
wAcc: 80.67018401129118
test acc: 82.16
Epoche: [157/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [157][0/196]	Time 0.248 (0.248)	Data 0.426 (0.426)	Loss 0.3327 (0.3327)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [157][64/196]	Time 0.145 (0.166)	Data 0.000 (0.007)	Loss 0.2366 (0.2813)	Acc@1 89.844 (90.030)	Acc@5 100.000 (99.826)
Epoch: [157][128/196]	Time 0.160 (0.162)	Data 0.016 (0.004)	Loss 0.3875 (0.2852)	Acc@1 87.500 (90.022)	Acc@5 99.609 (99.794)
Epoch: [157][192/196]	Time 0.150 (0.161)	Data 0.000 (0.003)	Loss 0.2941 (0.2866)	Acc@1 89.453 (89.864)	Acc@5 99.609 (99.787)
after train
n1: 30 for:
wAcc: 81.02795759046273
test acc: 80.93
Epoche: [158/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [158][0/196]	Time 0.266 (0.266)	Data 0.400 (0.400)	Loss 0.2847 (0.2847)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [158][64/196]	Time 0.151 (0.159)	Data 0.000 (0.006)	Loss 0.2940 (0.2817)	Acc@1 91.016 (90.222)	Acc@5 100.000 (99.784)
Epoch: [158][128/196]	Time 0.141 (0.155)	Data 0.000 (0.003)	Loss 0.2195 (0.2815)	Acc@1 93.359 (90.074)	Acc@5 100.000 (99.806)
Epoch: [158][192/196]	Time 0.142 (0.155)	Data 0.000 (0.002)	Loss 0.2534 (0.2821)	Acc@1 92.188 (90.085)	Acc@5 100.000 (99.787)
after train
n1: 30 for:
wAcc: 80.34364404244747
test acc: 84.52
Epoche: [159/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [159][0/196]	Time 0.239 (0.239)	Data 0.322 (0.322)	Loss 0.2486 (0.2486)	Acc@1 90.234 (90.234)	Acc@5 100.000 (100.000)
Epoch: [159][64/196]	Time 0.154 (0.152)	Data 0.000 (0.005)	Loss 0.2888 (0.2767)	Acc@1 89.453 (90.216)	Acc@5 99.609 (99.784)
Epoch: [159][128/196]	Time 0.192 (0.153)	Data 0.000 (0.003)	Loss 0.2384 (0.2733)	Acc@1 91.797 (90.398)	Acc@5 100.000 (99.770)
Epoch: [159][192/196]	Time 0.174 (0.155)	Data 0.000 (0.002)	Loss 0.3635 (0.2743)	Acc@1 89.062 (90.344)	Acc@5 99.609 (99.751)
after train
n1: 30 for:
wAcc: 80.8573953940301
test acc: 84.23
Epoche: [160/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [160][0/196]	Time 0.202 (0.202)	Data 0.352 (0.352)	Loss 0.2368 (0.2368)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [160][64/196]	Time 0.169 (0.154)	Data 0.000 (0.006)	Loss 0.2875 (0.2603)	Acc@1 91.797 (91.022)	Acc@5 99.609 (99.868)
Epoch: [160][128/196]	Time 0.162 (0.155)	Data 0.000 (0.003)	Loss 0.2891 (0.2656)	Acc@1 89.062 (90.698)	Acc@5 100.000 (99.776)
Epoch: [160][192/196]	Time 0.150 (0.154)	Data 0.000 (0.002)	Loss 0.3150 (0.2711)	Acc@1 90.234 (90.504)	Acc@5 100.000 (99.763)
after train
n1: 30 for:
wAcc: 80.85669483652686
test acc: 82.52
Epoche: [161/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [161][0/196]	Time 0.261 (0.261)	Data 0.417 (0.417)	Loss 0.2402 (0.2402)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [161][64/196]	Time 0.164 (0.155)	Data 0.000 (0.007)	Loss 0.2424 (0.2718)	Acc@1 90.625 (90.240)	Acc@5 100.000 (99.802)
Epoch: [161][128/196]	Time 0.152 (0.156)	Data 0.000 (0.004)	Loss 0.2891 (0.2689)	Acc@1 89.844 (90.413)	Acc@5 99.609 (99.803)
Epoch: [161][192/196]	Time 0.141 (0.155)	Data 0.000 (0.002)	Loss 0.2234 (0.2726)	Acc@1 92.578 (90.364)	Acc@5 100.000 (99.800)
after train
n1: 30 for:
wAcc: 81.25891042384904
test acc: 85.65
Epoche: [162/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [162][0/196]	Time 0.192 (0.192)	Data 0.393 (0.393)	Loss 0.2307 (0.2307)	Acc@1 90.234 (90.234)	Acc@5 99.609 (99.609)
Epoch: [162][64/196]	Time 0.163 (0.152)	Data 0.000 (0.006)	Loss 0.2721 (0.2630)	Acc@1 90.625 (90.877)	Acc@5 100.000 (99.760)
Epoch: [162][128/196]	Time 0.159 (0.155)	Data 0.000 (0.003)	Loss 0.2430 (0.2597)	Acc@1 92.969 (90.913)	Acc@5 98.828 (99.779)
Epoch: [162][192/196]	Time 0.167 (0.154)	Data 0.000 (0.002)	Loss 0.2311 (0.2635)	Acc@1 90.625 (90.811)	Acc@5 100.000 (99.794)
after train
n1: 30 for:
wAcc: 81.85301387360849
test acc: 84.81
Epoche: [163/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [163][0/196]	Time 0.229 (0.229)	Data 0.403 (0.403)	Loss 0.2167 (0.2167)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [163][64/196]	Time 0.210 (0.161)	Data 0.000 (0.007)	Loss 0.3245 (0.2648)	Acc@1 88.672 (90.769)	Acc@5 99.219 (99.736)
Epoch: [163][128/196]	Time 0.153 (0.157)	Data 0.000 (0.003)	Loss 0.2541 (0.2720)	Acc@1 91.797 (90.495)	Acc@5 99.219 (99.752)
Epoch: [163][192/196]	Time 0.132 (0.155)	Data 0.000 (0.002)	Loss 0.1986 (0.2678)	Acc@1 94.141 (90.651)	Acc@5 100.000 (99.761)
after train
n1: 30 for:
wAcc: 81.28339338098762
test acc: 84.8
Epoche: [164/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [164][0/196]	Time 0.188 (0.188)	Data 0.424 (0.424)	Loss 0.2206 (0.2206)	Acc@1 91.016 (91.016)	Acc@5 99.609 (99.609)
Epoch: [164][64/196]	Time 0.156 (0.163)	Data 0.000 (0.007)	Loss 0.2273 (0.2572)	Acc@1 90.234 (90.721)	Acc@5 100.000 (99.880)
Epoch: [164][128/196]	Time 0.156 (0.158)	Data 0.000 (0.004)	Loss 0.2722 (0.2598)	Acc@1 92.578 (90.670)	Acc@5 98.828 (99.815)
Epoch: [164][192/196]	Time 0.156 (0.155)	Data 0.000 (0.002)	Loss 0.3330 (0.2617)	Acc@1 91.797 (90.639)	Acc@5 98.828 (99.796)
after train
n1: 30 for:
wAcc: 82.07261568505048
test acc: 83.63
Epoche: [165/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [165][0/196]	Time 0.252 (0.252)	Data 0.372 (0.372)	Loss 0.2477 (0.2477)	Acc@1 92.969 (92.969)	Acc@5 99.609 (99.609)
Epoch: [165][64/196]	Time 0.151 (0.163)	Data 0.000 (0.006)	Loss 0.2398 (0.2464)	Acc@1 90.625 (91.334)	Acc@5 99.609 (99.838)
Epoch: [165][128/196]	Time 0.153 (0.160)	Data 0.000 (0.003)	Loss 0.2342 (0.2485)	Acc@1 89.844 (91.219)	Acc@5 100.000 (99.809)
Epoch: [165][192/196]	Time 0.149 (0.157)	Data 0.000 (0.002)	Loss 0.2676 (0.2526)	Acc@1 91.016 (91.091)	Acc@5 99.609 (99.804)
after train
n1: 30 for:
wAcc: 82.61689607320235
test acc: 83.29
Epoche: [166/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [166][0/196]	Time 0.195 (0.195)	Data 0.417 (0.417)	Loss 0.2102 (0.2102)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [166][64/196]	Time 0.147 (0.151)	Data 0.000 (0.007)	Loss 0.1794 (0.2513)	Acc@1 94.531 (91.130)	Acc@5 100.000 (99.808)
Epoch: [166][128/196]	Time 0.154 (0.150)	Data 0.000 (0.004)	Loss 0.2087 (0.2559)	Acc@1 93.359 (90.901)	Acc@5 99.609 (99.809)
Epoch: [166][192/196]	Time 0.165 (0.153)	Data 0.000 (0.002)	Loss 0.2682 (0.2538)	Acc@1 91.406 (91.070)	Acc@5 99.609 (99.816)
after train
n1: 30 for:
wAcc: 82.6183992813953
test acc: 83.54
Epoche: [167/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [167][0/196]	Time 0.242 (0.242)	Data 0.354 (0.354)	Loss 0.2199 (0.2199)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [167][64/196]	Time 0.146 (0.156)	Data 0.000 (0.006)	Loss 0.2319 (0.2494)	Acc@1 91.406 (91.256)	Acc@5 99.609 (99.808)
Epoch: [167][128/196]	Time 0.168 (0.156)	Data 0.000 (0.003)	Loss 0.2316 (0.2524)	Acc@1 92.578 (91.161)	Acc@5 99.219 (99.812)
Epoch: [167][192/196]	Time 0.144 (0.156)	Data 0.000 (0.002)	Loss 0.2148 (0.2515)	Acc@1 91.797 (91.141)	Acc@5 100.000 (99.820)
after train
n1: 30 for:
wAcc: 82.65761877425902
test acc: 85.53
Epoche: [168/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [168][0/196]	Time 0.174 (0.174)	Data 0.540 (0.540)	Loss 0.2631 (0.2631)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [168][64/196]	Time 0.142 (0.154)	Data 0.000 (0.009)	Loss 0.3368 (0.2438)	Acc@1 88.281 (91.484)	Acc@5 100.000 (99.772)
Epoch: [168][128/196]	Time 0.181 (0.155)	Data 0.000 (0.004)	Loss 0.1953 (0.2486)	Acc@1 92.188 (91.212)	Acc@5 100.000 (99.767)
Epoch: [168][192/196]	Time 0.177 (0.155)	Data 0.000 (0.003)	Loss 0.2015 (0.2492)	Acc@1 92.578 (91.234)	Acc@5 100.000 (99.759)
after train
n1: 30 for:
wAcc: 82.71282829053013
test acc: 85.32
Epoche: [169/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [169][0/196]	Time 0.235 (0.235)	Data 0.408 (0.408)	Loss 0.2369 (0.2369)	Acc@1 92.969 (92.969)	Acc@5 99.609 (99.609)
Epoch: [169][64/196]	Time 0.153 (0.156)	Data 0.000 (0.007)	Loss 0.2087 (0.2398)	Acc@1 91.406 (91.683)	Acc@5 99.609 (99.832)
Epoch: [169][128/196]	Time 0.153 (0.154)	Data 0.000 (0.003)	Loss 0.3008 (0.2419)	Acc@1 89.844 (91.497)	Acc@5 99.609 (99.827)
Epoch: [169][192/196]	Time 0.154 (0.155)	Data 0.000 (0.002)	Loss 0.1948 (0.2428)	Acc@1 92.969 (91.435)	Acc@5 100.000 (99.816)
after train
n1: 30 for:
wAcc: 82.59190980246171
test acc: 85.98
Epoche: [170/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [170][0/196]	Time 0.269 (0.269)	Data 0.360 (0.360)	Loss 0.1844 (0.1844)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [170][64/196]	Time 0.145 (0.155)	Data 0.000 (0.006)	Loss 0.2491 (0.2474)	Acc@1 91.016 (91.220)	Acc@5 99.609 (99.820)
Epoch: [170][128/196]	Time 0.150 (0.156)	Data 0.000 (0.003)	Loss 0.2545 (0.2431)	Acc@1 90.625 (91.391)	Acc@5 99.609 (99.806)
Epoch: [170][192/196]	Time 0.145 (0.154)	Data 0.000 (0.002)	Loss 0.2598 (0.2448)	Acc@1 88.281 (91.358)	Acc@5 99.609 (99.804)
after train
n1: 30 for:
wAcc: 82.93192797490306
test acc: 83.89
Epoche: [171/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [171][0/196]	Time 0.264 (0.264)	Data 0.415 (0.415)	Loss 0.2036 (0.2036)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [171][64/196]	Time 0.169 (0.160)	Data 0.000 (0.007)	Loss 0.2160 (0.2384)	Acc@1 92.578 (91.286)	Acc@5 100.000 (99.874)
Epoch: [171][128/196]	Time 0.145 (0.163)	Data 0.000 (0.004)	Loss 0.2694 (0.2394)	Acc@1 90.234 (91.415)	Acc@5 100.000 (99.833)
Epoch: [171][192/196]	Time 0.157 (0.160)	Data 0.000 (0.002)	Loss 0.2368 (0.2424)	Acc@1 92.188 (91.337)	Acc@5 100.000 (99.838)
after train
n1: 30 for:
wAcc: 83.288644650072
test acc: 83.97
Epoche: [172/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [172][0/196]	Time 0.221 (0.221)	Data 0.384 (0.384)	Loss 0.1990 (0.1990)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [172][64/196]	Time 0.146 (0.156)	Data 0.000 (0.006)	Loss 0.1866 (0.2331)	Acc@1 92.188 (91.743)	Acc@5 100.000 (99.838)
Epoch: [172][128/196]	Time 0.160 (0.157)	Data 0.000 (0.003)	Loss 0.2793 (0.2305)	Acc@1 90.625 (91.873)	Acc@5 99.609 (99.833)
Epoch: [172][192/196]	Time 0.144 (0.155)	Data 0.000 (0.002)	Loss 0.3309 (0.2346)	Acc@1 88.281 (91.696)	Acc@5 99.609 (99.826)
after train
n1: 30 for:
wAcc: 83.14756426647382
test acc: 83.02
Epoche: [173/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [173][0/196]	Time 0.217 (0.217)	Data 0.492 (0.492)	Loss 0.2665 (0.2665)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [173][64/196]	Time 0.141 (0.158)	Data 0.000 (0.008)	Loss 0.2327 (0.2303)	Acc@1 91.406 (91.845)	Acc@5 100.000 (99.796)
Epoch: [173][128/196]	Time 0.144 (0.157)	Data 0.000 (0.004)	Loss 0.2714 (0.2328)	Acc@1 92.188 (91.776)	Acc@5 100.000 (99.818)
Epoch: [173][192/196]	Time 0.155 (0.157)	Data 0.000 (0.003)	Loss 0.2481 (0.2356)	Acc@1 91.797 (91.692)	Acc@5 99.609 (99.818)
after train
n1: 30 for:
wAcc: 83.40677319469748
test acc: 85.71
Epoche: [174/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [174][0/196]	Time 0.227 (0.227)	Data 0.448 (0.448)	Loss 0.2331 (0.2331)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [174][64/196]	Time 0.152 (0.159)	Data 0.000 (0.007)	Loss 0.1565 (0.2245)	Acc@1 94.141 (91.977)	Acc@5 99.609 (99.838)
Epoch: [174][128/196]	Time 0.147 (0.154)	Data 0.000 (0.004)	Loss 0.3182 (0.2289)	Acc@1 91.406 (91.973)	Acc@5 99.609 (99.836)
Epoch: [174][192/196]	Time 0.168 (0.154)	Data 0.000 (0.003)	Loss 0.1847 (0.2316)	Acc@1 94.141 (91.803)	Acc@5 100.000 (99.838)
after train
n1: 30 for:
wAcc: 83.6825826428327
test acc: 84.68
Epoche: [175/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [175][0/196]	Time 0.238 (0.238)	Data 0.323 (0.323)	Loss 0.2389 (0.2389)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [175][64/196]	Time 0.155 (0.159)	Data 0.000 (0.005)	Loss 0.2884 (0.2220)	Acc@1 89.453 (92.073)	Acc@5 99.609 (99.904)
Epoch: [175][128/196]	Time 0.173 (0.157)	Data 0.000 (0.003)	Loss 0.1968 (0.2249)	Acc@1 92.578 (92.060)	Acc@5 100.000 (99.858)
Epoch: [175][192/196]	Time 0.169 (0.157)	Data 0.000 (0.002)	Loss 0.1599 (0.2274)	Acc@1 95.703 (91.977)	Acc@5 100.000 (99.844)
after train
n1: 30 for:
wAcc: 83.78162692348508
test acc: 84.76
Epoche: [176/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [176][0/196]	Time 0.217 (0.217)	Data 0.401 (0.401)	Loss 0.2581 (0.2581)	Acc@1 91.016 (91.016)	Acc@5 99.219 (99.219)
Epoch: [176][64/196]	Time 0.143 (0.161)	Data 0.000 (0.006)	Loss 0.2714 (0.2341)	Acc@1 90.625 (91.797)	Acc@5 99.609 (99.826)
Epoch: [176][128/196]	Time 0.139 (0.157)	Data 0.000 (0.003)	Loss 0.2286 (0.2285)	Acc@1 92.188 (92.030)	Acc@5 99.609 (99.824)
Epoch: [176][192/196]	Time 0.158 (0.156)	Data 0.000 (0.002)	Loss 0.3012 (0.2281)	Acc@1 88.281 (92.048)	Acc@5 99.609 (99.826)
after train
n1: 30 for:
wAcc: 83.45443156257531
test acc: 86.71
Epoche: [177/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [177][0/196]	Time 0.238 (0.238)	Data 0.391 (0.391)	Loss 0.3043 (0.3043)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [177][64/196]	Time 0.142 (0.152)	Data 0.000 (0.006)	Loss 0.1509 (0.2335)	Acc@1 93.750 (91.671)	Acc@5 100.000 (99.856)
Epoch: [177][128/196]	Time 0.159 (0.152)	Data 0.000 (0.003)	Loss 0.1612 (0.2248)	Acc@1 94.141 (92.033)	Acc@5 100.000 (99.849)
Epoch: [177][192/196]	Time 0.142 (0.153)	Data 0.000 (0.002)	Loss 0.2668 (0.2262)	Acc@1 90.625 (91.985)	Acc@5 100.000 (99.848)
after train
n1: 30 for:
wAcc: 83.62543661550197
test acc: 85.95
Epoche: [178/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [178][0/196]	Time 0.196 (0.196)	Data 0.393 (0.393)	Loss 0.2377 (0.2377)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [178][64/196]	Time 0.151 (0.158)	Data 0.000 (0.006)	Loss 0.2003 (0.2111)	Acc@1 92.969 (92.440)	Acc@5 100.000 (99.844)
Epoch: [178][128/196]	Time 0.209 (0.160)	Data 0.000 (0.003)	Loss 0.1750 (0.2162)	Acc@1 92.969 (92.303)	Acc@5 99.609 (99.852)
Epoch: [178][192/196]	Time 0.175 (0.159)	Data 0.000 (0.002)	Loss 0.2449 (0.2163)	Acc@1 90.625 (92.305)	Acc@5 100.000 (99.856)
after train
n1: 30 for:
wAcc: 83.62072758050998
test acc: 85.31
Epoche: [179/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [179][0/196]	Time 0.255 (0.255)	Data 0.362 (0.362)	Loss 0.2178 (0.2178)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [179][64/196]	Time 0.142 (0.149)	Data 0.000 (0.006)	Loss 0.1959 (0.2180)	Acc@1 93.359 (92.320)	Acc@5 100.000 (99.874)
Epoch: [179][128/196]	Time 0.158 (0.150)	Data 0.000 (0.003)	Loss 0.2406 (0.2164)	Acc@1 90.234 (92.369)	Acc@5 100.000 (99.873)
Epoch: [179][192/196]	Time 0.169 (0.154)	Data 0.000 (0.002)	Loss 0.2074 (0.2186)	Acc@1 93.359 (92.325)	Acc@5 99.609 (99.864)
after train
n1: 30 for:
wAcc: 83.71236551102724
test acc: 84.49
Epoche: [180/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [180][0/196]	Time 0.182 (0.182)	Data 0.549 (0.549)	Loss 0.1781 (0.1781)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [180][64/196]	Time 0.160 (0.155)	Data 0.000 (0.009)	Loss 0.1731 (0.2140)	Acc@1 92.969 (92.596)	Acc@5 100.000 (99.880)
Epoch: [180][128/196]	Time 0.144 (0.155)	Data 0.000 (0.005)	Loss 0.1702 (0.2145)	Acc@1 95.312 (92.569)	Acc@5 100.000 (99.879)
Epoch: [180][192/196]	Time 0.136 (0.153)	Data 0.000 (0.003)	Loss 0.2128 (0.2146)	Acc@1 93.359 (92.534)	Acc@5 100.000 (99.887)
after train
n1: 30 for:
wAcc: 84.44342041267194
test acc: 82.74
Max memory: 114.2688768
Traceback (most recent call last):
  File "main.py", line 924, in <module>
    main()
  File "main.py", line 590, in main
    print(' {:5.3f}s'.format(ende - start), end='  ')
UnboundLocalError: local variable 'ende' referenced before assignment
