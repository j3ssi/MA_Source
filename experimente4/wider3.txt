no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 2;widthofFirstLayer: 16; Epochen: 180; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/wider3/model.nn; checkpoint: ./output/experimente4/wider3; saveModell: True; LR: 0.1
random number: 3802
Files already downloaded and verified
width: 8

Arch Num:  [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
conv0: Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0
bn1: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
Relu; i: 2
i : 2; block: 0
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 2; block: 0
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 2
i : 3; block: 1
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 3; block: 1
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 3
i : 4; block: 2
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 4; block: 2
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 4
i : 5; block: 3
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 5; block: 3
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 5
i : 6; block: 4
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 6; block: 4
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 6
i : 7; block: 0
Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 7; block: 0
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
i : 7; block: 0
Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 2 if 2
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 2
seq: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 7
seq1: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 8
i : 8; block: 1
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 8; block: 1
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 8
i : 9; block: 2
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 9; block: 2
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 9
i : 10; block: 3
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 10; block: 3
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 10
i : 11; block: 4
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 11; block: 4
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 11
i : 12; block: 0
Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 12; block: 0
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
i : 12; block: 0
Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 2 if 2
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 2
seq: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 12
seq1: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 13
i : 13; block: 1
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 13; block: 1
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 13
i : 14; block: 2
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 14; block: 2
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 14
i : 15; block: 3
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 15; block: 3
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 15
i : 16; block: 4
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 16; block: 4
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 16
avgpoll: AdaptiveAvgPool2d(output_size=(1, 1))
linear: Linear(in_features=32, out_features=10, bias=True)
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
deeper epoch: 0
Epoche: [1/180]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [1][0/196]	Time 0.453 (0.453)	Data 0.333 (0.333)	Loss 2.4967 (2.4967)	Acc@1 9.375 (9.375)	Acc@5 51.953 (51.953)
Epoch: [1][64/196]	Time 0.237 (0.424)	Data 0.000 (0.005)	Loss 1.9081 (2.0272)	Acc@1 28.516 (23.065)	Acc@5 82.422 (75.535)
Epoch: [1][128/196]	Time 0.355 (0.435)	Data 0.000 (0.003)	Loss 1.7580 (1.8824)	Acc@1 30.078 (28.609)	Acc@5 88.672 (81.395)
Epoch: [1][192/196]	Time 0.575 (0.480)	Data 0.000 (0.002)	Loss 1.5372 (1.7747)	Acc@1 46.875 (33.148)	Acc@5 91.406 (84.266)
after train
Epoche: [2/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [2][0/196]	Time 0.587 (0.587)	Data 0.431 (0.431)	Loss 1.5288 (1.5288)	Acc@1 44.141 (44.141)	Acc@5 89.062 (89.062)
Epoch: [2][64/196]	Time 0.565 (0.570)	Data 0.000 (0.007)	Loss 1.2850 (1.4366)	Acc@1 52.734 (47.290)	Acc@5 94.531 (92.188)
Epoch: [2][128/196]	Time 0.544 (0.556)	Data 0.000 (0.004)	Loss 1.2936 (1.3957)	Acc@1 54.297 (48.692)	Acc@5 93.750 (92.863)
Epoch: [2][192/196]	Time 0.633 (0.562)	Data 0.000 (0.003)	Loss 1.1885 (1.3440)	Acc@1 58.984 (50.808)	Acc@5 96.094 (93.355)
after train
Epoche: [3/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [3][0/196]	Time 0.930 (0.930)	Data 0.471 (0.471)	Loss 1.0977 (1.0977)	Acc@1 58.984 (58.984)	Acc@5 96.875 (96.875)
Epoch: [3][64/196]	Time 0.682 (0.720)	Data 0.000 (0.008)	Loss 1.1651 (1.1435)	Acc@1 54.297 (58.552)	Acc@5 96.094 (95.613)
Epoch: [3][128/196]	Time 0.678 (0.706)	Data 0.000 (0.004)	Loss 1.0754 (1.1358)	Acc@1 62.891 (58.612)	Acc@5 95.703 (95.658)
Epoch: [3][192/196]	Time 0.556 (0.706)	Data 0.000 (0.003)	Loss 1.0444 (1.1062)	Acc@1 62.500 (59.887)	Acc@5 96.484 (95.833)
after train
Epoche: [4/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [4][0/196]	Time 0.860 (0.860)	Data 0.402 (0.402)	Loss 1.0265 (1.0265)	Acc@1 63.281 (63.281)	Acc@5 96.875 (96.875)
Epoch: [4][64/196]	Time 0.709 (0.719)	Data 0.000 (0.006)	Loss 0.9576 (1.0075)	Acc@1 68.750 (64.309)	Acc@5 97.266 (96.917)
Epoch: [4][128/196]	Time 0.732 (0.709)	Data 0.000 (0.003)	Loss 0.8060 (0.9912)	Acc@1 69.531 (64.686)	Acc@5 98.828 (96.942)
Epoch: [4][192/196]	Time 0.713 (0.707)	Data 0.000 (0.002)	Loss 0.9047 (0.9715)	Acc@1 66.797 (65.202)	Acc@5 97.656 (97.003)
after train
Epoche: [5/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [5][0/196]	Time 0.848 (0.848)	Data 0.464 (0.464)	Loss 0.9112 (0.9112)	Acc@1 67.578 (67.578)	Acc@5 96.875 (96.875)
Epoch: [5][64/196]	Time 0.748 (0.714)	Data 0.000 (0.007)	Loss 0.8994 (0.8996)	Acc@1 68.359 (68.113)	Acc@5 96.875 (97.314)
Epoch: [5][128/196]	Time 0.862 (0.710)	Data 0.000 (0.004)	Loss 0.8560 (0.8838)	Acc@1 68.359 (68.635)	Acc@5 97.656 (97.369)
Epoch: [5][192/196]	Time 0.499 (0.707)	Data 0.000 (0.003)	Loss 0.7855 (0.8722)	Acc@1 74.609 (69.098)	Acc@5 96.875 (97.438)
after train
Epoche: [6/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [6][0/196]	Time 0.879 (0.879)	Data 0.319 (0.319)	Loss 0.9686 (0.9686)	Acc@1 66.016 (66.016)	Acc@5 97.656 (97.656)
Epoch: [6][64/196]	Time 0.676 (0.716)	Data 0.000 (0.005)	Loss 0.6970 (0.8368)	Acc@1 75.781 (70.601)	Acc@5 98.047 (97.806)
Epoch: [6][128/196]	Time 0.687 (0.711)	Data 0.000 (0.003)	Loss 0.8422 (0.8256)	Acc@1 71.875 (71.100)	Acc@5 97.656 (97.856)
Epoch: [6][192/196]	Time 0.582 (0.708)	Data 0.000 (0.002)	Loss 0.7312 (0.8068)	Acc@1 75.781 (71.796)	Acc@5 98.438 (98.010)
after train
Epoche: [7/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [7][0/196]	Time 0.859 (0.859)	Data 0.334 (0.334)	Loss 0.8224 (0.8224)	Acc@1 70.703 (70.703)	Acc@5 97.656 (97.656)
Epoch: [7][64/196]	Time 0.744 (0.711)	Data 0.000 (0.005)	Loss 0.7803 (0.7653)	Acc@1 73.438 (73.281)	Acc@5 97.266 (98.059)
Epoch: [7][128/196]	Time 0.664 (0.707)	Data 0.000 (0.003)	Loss 0.8012 (0.7569)	Acc@1 70.703 (73.637)	Acc@5 98.047 (98.047)
Epoch: [7][192/196]	Time 0.722 (0.706)	Data 0.000 (0.002)	Loss 0.7159 (0.7508)	Acc@1 74.219 (73.808)	Acc@5 99.609 (98.095)
after train
Epoche: [8/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [8][0/196]	Time 0.823 (0.823)	Data 0.352 (0.352)	Loss 0.7499 (0.7499)	Acc@1 71.484 (71.484)	Acc@5 99.219 (99.219)
Epoch: [8][64/196]	Time 0.716 (0.714)	Data 0.000 (0.006)	Loss 0.7424 (0.7246)	Acc@1 72.656 (74.982)	Acc@5 98.047 (98.281)
Epoch: [8][128/196]	Time 0.715 (0.706)	Data 0.000 (0.003)	Loss 0.7446 (0.7171)	Acc@1 76.172 (75.227)	Acc@5 96.875 (98.338)
Epoch: [8][192/196]	Time 0.708 (0.706)	Data 0.000 (0.002)	Loss 0.7450 (0.7103)	Acc@1 73.438 (75.472)	Acc@5 97.656 (98.423)
after train
Epoche: [9/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [9][0/196]	Time 0.741 (0.741)	Data 0.368 (0.368)	Loss 0.6316 (0.6316)	Acc@1 78.125 (78.125)	Acc@5 98.828 (98.828)
Epoch: [9][64/196]	Time 0.752 (0.717)	Data 0.000 (0.006)	Loss 0.6577 (0.6729)	Acc@1 77.734 (76.899)	Acc@5 98.047 (98.516)
Epoch: [9][128/196]	Time 0.709 (0.710)	Data 0.000 (0.003)	Loss 0.7526 (0.6867)	Acc@1 73.828 (76.226)	Acc@5 98.438 (98.434)
Epoch: [9][192/196]	Time 0.771 (0.710)	Data 0.000 (0.002)	Loss 0.6214 (0.6807)	Acc@1 78.516 (76.358)	Acc@5 99.609 (98.435)
after train
Epoche: [10/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [10][0/196]	Time 0.882 (0.882)	Data 0.394 (0.394)	Loss 0.7415 (0.7415)	Acc@1 72.656 (72.656)	Acc@5 98.828 (98.828)
Epoch: [10][64/196]	Time 0.689 (0.717)	Data 0.000 (0.006)	Loss 0.6116 (0.6543)	Acc@1 78.516 (77.290)	Acc@5 97.656 (98.540)
Epoch: [10][128/196]	Time 0.623 (0.707)	Data 0.000 (0.003)	Loss 0.5924 (0.6540)	Acc@1 81.250 (77.371)	Acc@5 98.047 (98.559)
Epoch: [10][192/196]	Time 0.645 (0.706)	Data 0.000 (0.002)	Loss 0.5485 (0.6580)	Acc@1 80.859 (77.117)	Acc@5 99.219 (98.559)
after train
Epoche: [11/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [11][0/196]	Time 0.803 (0.803)	Data 0.313 (0.313)	Loss 0.7608 (0.7608)	Acc@1 75.391 (75.391)	Acc@5 98.438 (98.438)
Epoch: [11][64/196]	Time 0.748 (0.715)	Data 0.000 (0.005)	Loss 0.6278 (0.6361)	Acc@1 80.078 (77.855)	Acc@5 98.438 (98.648)
Epoch: [11][128/196]	Time 0.652 (0.710)	Data 0.000 (0.003)	Loss 0.6153 (0.6337)	Acc@1 80.859 (78.040)	Acc@5 98.438 (98.713)
Epoch: [11][192/196]	Time 0.517 (0.707)	Data 0.000 (0.002)	Loss 0.6631 (0.6365)	Acc@1 74.219 (77.854)	Acc@5 98.828 (98.699)
after train
Epoche: [12/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [12][0/196]	Time 0.812 (0.812)	Data 0.379 (0.379)	Loss 0.7404 (0.7404)	Acc@1 73.828 (73.828)	Acc@5 99.219 (99.219)
Epoch: [12][64/196]	Time 0.785 (0.725)	Data 0.000 (0.006)	Loss 0.4745 (0.6151)	Acc@1 83.594 (78.798)	Acc@5 99.219 (98.678)
Epoch: [12][128/196]	Time 0.744 (0.712)	Data 0.000 (0.003)	Loss 0.5118 (0.6135)	Acc@1 80.859 (78.712)	Acc@5 99.609 (98.716)
Epoch: [12][192/196]	Time 0.659 (0.713)	Data 0.000 (0.002)	Loss 0.6364 (0.6163)	Acc@1 75.781 (78.659)	Acc@5 98.438 (98.705)
after train
Epoche: [13/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [13][0/196]	Time 0.805 (0.805)	Data 0.452 (0.452)	Loss 0.7241 (0.7241)	Acc@1 76.953 (76.953)	Acc@5 98.438 (98.438)
Epoch: [13][64/196]	Time 0.679 (0.713)	Data 0.000 (0.007)	Loss 0.7204 (0.5938)	Acc@1 76.562 (79.910)	Acc@5 97.656 (98.732)
Epoch: [13][128/196]	Time 0.712 (0.709)	Data 0.000 (0.004)	Loss 0.5115 (0.5968)	Acc@1 84.766 (79.624)	Acc@5 99.219 (98.768)
Epoch: [13][192/196]	Time 0.690 (0.710)	Data 0.000 (0.003)	Loss 0.6478 (0.6014)	Acc@1 77.734 (79.420)	Acc@5 100.000 (98.767)
after train
Epoche: [14/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [14][0/196]	Time 0.827 (0.827)	Data 0.352 (0.352)	Loss 0.5631 (0.5631)	Acc@1 81.250 (81.250)	Acc@5 98.047 (98.047)
Epoch: [14][64/196]	Time 0.677 (0.725)	Data 0.000 (0.006)	Loss 0.5757 (0.5924)	Acc@1 78.516 (80.012)	Acc@5 98.047 (98.924)
Epoch: [14][128/196]	Time 0.756 (0.716)	Data 0.000 (0.003)	Loss 0.6116 (0.5973)	Acc@1 80.469 (79.448)	Acc@5 97.266 (98.892)
Epoch: [14][192/196]	Time 0.670 (0.716)	Data 0.000 (0.002)	Loss 0.6359 (0.5943)	Acc@1 79.688 (79.493)	Acc@5 98.047 (98.869)
after train
Epoche: [15/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [15][0/196]	Time 0.794 (0.794)	Data 0.384 (0.384)	Loss 0.6581 (0.6581)	Acc@1 77.344 (77.344)	Acc@5 98.438 (98.438)
Epoch: [15][64/196]	Time 0.737 (0.718)	Data 0.000 (0.006)	Loss 0.7364 (0.5877)	Acc@1 78.125 (79.513)	Acc@5 97.656 (98.852)
Epoch: [15][128/196]	Time 0.747 (0.709)	Data 0.000 (0.003)	Loss 0.6491 (0.5856)	Acc@1 77.344 (79.681)	Acc@5 97.266 (98.822)
Epoch: [15][192/196]	Time 0.763 (0.711)	Data 0.000 (0.002)	Loss 0.6328 (0.5815)	Acc@1 78.516 (79.825)	Acc@5 98.438 (98.895)
after train
Epoche: [16/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [16][0/196]	Time 0.800 (0.800)	Data 0.391 (0.391)	Loss 0.5587 (0.5587)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [16][64/196]	Time 0.738 (0.713)	Data 0.000 (0.006)	Loss 0.6224 (0.5694)	Acc@1 79.297 (80.409)	Acc@5 98.047 (98.876)
Epoch: [16][128/196]	Time 0.665 (0.708)	Data 0.000 (0.003)	Loss 0.4940 (0.5717)	Acc@1 84.766 (80.329)	Acc@5 98.828 (98.867)
Epoch: [16][192/196]	Time 0.727 (0.708)	Data 0.000 (0.002)	Loss 0.5353 (0.5684)	Acc@1 80.078 (80.442)	Acc@5 99.219 (98.895)
after train
Epoche: [17/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [17][0/196]	Time 0.858 (0.858)	Data 0.372 (0.372)	Loss 0.6036 (0.6036)	Acc@1 79.297 (79.297)	Acc@5 99.219 (99.219)
Epoch: [17][64/196]	Time 0.814 (0.721)	Data 0.000 (0.006)	Loss 0.4854 (0.5582)	Acc@1 83.984 (80.415)	Acc@5 98.828 (98.978)
Epoch: [17][128/196]	Time 0.761 (0.708)	Data 0.000 (0.003)	Loss 0.4783 (0.5548)	Acc@1 83.594 (80.884)	Acc@5 100.000 (98.995)
Epoch: [17][192/196]	Time 0.675 (0.709)	Data 0.000 (0.002)	Loss 0.6196 (0.5572)	Acc@1 77.734 (80.780)	Acc@5 98.047 (98.994)
after train
Epoche: [18/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [18][0/196]	Time 0.831 (0.831)	Data 0.395 (0.395)	Loss 0.5478 (0.5478)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [18][64/196]	Time 0.690 (0.716)	Data 0.000 (0.006)	Loss 0.6307 (0.5547)	Acc@1 78.125 (80.871)	Acc@5 98.828 (98.870)
Epoch: [18][128/196]	Time 0.687 (0.704)	Data 0.000 (0.003)	Loss 0.4941 (0.5565)	Acc@1 84.375 (80.887)	Acc@5 99.609 (98.925)
Epoch: [18][192/196]	Time 0.699 (0.708)	Data 0.000 (0.002)	Loss 0.6030 (0.5567)	Acc@1 81.250 (80.876)	Acc@5 98.047 (98.978)
after train
Epoche: [19/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [19][0/196]	Time 0.783 (0.783)	Data 0.573 (0.573)	Loss 0.5834 (0.5834)	Acc@1 79.297 (79.297)	Acc@5 97.266 (97.266)
Epoch: [19][64/196]	Time 0.708 (0.714)	Data 0.000 (0.009)	Loss 0.5478 (0.5469)	Acc@1 78.516 (80.974)	Acc@5 98.828 (98.984)
Epoch: [19][128/196]	Time 0.645 (0.708)	Data 0.000 (0.005)	Loss 0.5395 (0.5484)	Acc@1 81.250 (80.878)	Acc@5 99.219 (98.946)
Epoch: [19][192/196]	Time 0.696 (0.709)	Data 0.000 (0.003)	Loss 0.6344 (0.5490)	Acc@1 78.516 (80.938)	Acc@5 98.047 (98.978)
after train
Epoche: [20/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [20][0/196]	Time 0.782 (0.782)	Data 0.424 (0.424)	Loss 0.4937 (0.4937)	Acc@1 82.812 (82.812)	Acc@5 99.609 (99.609)
Epoch: [20][64/196]	Time 0.739 (0.726)	Data 0.000 (0.007)	Loss 0.5508 (0.5365)	Acc@1 82.031 (81.136)	Acc@5 97.656 (98.954)
Epoch: [20][128/196]	Time 0.701 (0.712)	Data 0.000 (0.004)	Loss 0.5864 (0.5392)	Acc@1 78.516 (81.111)	Acc@5 99.219 (99.013)
Epoch: [20][192/196]	Time 0.884 (0.711)	Data 0.000 (0.003)	Loss 0.4924 (0.5408)	Acc@1 82.031 (81.125)	Acc@5 99.219 (99.035)
after train
Epoche: [21/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [21][0/196]	Time 0.743 (0.743)	Data 0.415 (0.415)	Loss 0.4648 (0.4648)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [21][64/196]	Time 0.707 (0.711)	Data 0.000 (0.007)	Loss 0.5785 (0.5409)	Acc@1 76.172 (81.310)	Acc@5 98.438 (99.129)
Epoch: [21][128/196]	Time 0.769 (0.702)	Data 0.000 (0.004)	Loss 0.5386 (0.5466)	Acc@1 82.031 (81.086)	Acc@5 98.828 (99.001)
Epoch: [21][192/196]	Time 0.707 (0.705)	Data 0.000 (0.002)	Loss 0.5916 (0.5371)	Acc@1 78.906 (81.369)	Acc@5 98.828 (99.039)
after train
Epoche: [22/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [22][0/196]	Time 0.772 (0.772)	Data 0.410 (0.410)	Loss 0.5788 (0.5788)	Acc@1 80.859 (80.859)	Acc@5 99.219 (99.219)
Epoch: [22][64/196]	Time 0.716 (0.709)	Data 0.000 (0.007)	Loss 0.5170 (0.5338)	Acc@1 83.594 (81.623)	Acc@5 99.609 (99.081)
Epoch: [22][128/196]	Time 0.689 (0.706)	Data 0.000 (0.003)	Loss 0.4943 (0.5327)	Acc@1 85.938 (81.647)	Acc@5 98.828 (99.070)
Epoch: [22][192/196]	Time 0.737 (0.706)	Data 0.000 (0.002)	Loss 0.5392 (0.5305)	Acc@1 81.641 (81.788)	Acc@5 99.609 (99.099)
after train
Epoche: [23/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [23][0/196]	Time 0.776 (0.776)	Data 0.419 (0.419)	Loss 0.4306 (0.4306)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [23][64/196]	Time 0.745 (0.718)	Data 0.000 (0.007)	Loss 0.4120 (0.5299)	Acc@1 84.375 (82.025)	Acc@5 99.219 (99.014)
Epoch: [23][128/196]	Time 0.581 (0.711)	Data 0.000 (0.004)	Loss 0.5326 (0.5297)	Acc@1 80.859 (82.086)	Acc@5 100.000 (99.046)
Epoch: [23][192/196]	Time 0.654 (0.710)	Data 0.000 (0.002)	Loss 0.5468 (0.5287)	Acc@1 82.031 (81.960)	Acc@5 98.828 (99.059)
after train
Epoche: [24/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [24][0/196]	Time 0.831 (0.831)	Data 0.366 (0.366)	Loss 0.5614 (0.5614)	Acc@1 80.859 (80.859)	Acc@5 98.828 (98.828)
Epoch: [24][64/196]	Time 0.715 (0.720)	Data 0.000 (0.006)	Loss 0.4163 (0.5165)	Acc@1 84.375 (81.905)	Acc@5 99.219 (99.038)
Epoch: [24][128/196]	Time 0.681 (0.707)	Data 0.000 (0.003)	Loss 0.6064 (0.5108)	Acc@1 79.297 (82.407)	Acc@5 98.047 (99.049)
Epoch: [24][192/196]	Time 0.645 (0.706)	Data 0.000 (0.002)	Loss 0.6257 (0.5157)	Acc@1 78.906 (82.159)	Acc@5 100.000 (99.047)
after train
Epoche: [25/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [25][0/196]	Time 0.866 (0.866)	Data 0.354 (0.354)	Loss 0.5209 (0.5209)	Acc@1 82.422 (82.422)	Acc@5 99.609 (99.609)
Epoch: [25][64/196]	Time 0.609 (0.718)	Data 0.000 (0.006)	Loss 0.4614 (0.5113)	Acc@1 83.594 (82.236)	Acc@5 100.000 (99.201)
Epoch: [25][128/196]	Time 0.697 (0.712)	Data 0.000 (0.003)	Loss 0.4920 (0.5121)	Acc@1 84.375 (82.461)	Acc@5 99.609 (99.173)
Epoch: [25][192/196]	Time 0.778 (0.711)	Data 0.000 (0.002)	Loss 0.5356 (0.5150)	Acc@1 80.078 (82.313)	Acc@5 99.609 (99.134)
after train
Epoche: [26/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [26][0/196]	Time 0.766 (0.766)	Data 0.348 (0.348)	Loss 0.5235 (0.5235)	Acc@1 78.516 (78.516)	Acc@5 98.828 (98.828)
Epoch: [26][64/196]	Time 0.738 (0.723)	Data 0.000 (0.006)	Loss 0.5256 (0.5112)	Acc@1 82.812 (82.236)	Acc@5 98.438 (98.972)
Epoch: [26][128/196]	Time 0.744 (0.711)	Data 0.000 (0.003)	Loss 0.4936 (0.5102)	Acc@1 83.594 (82.319)	Acc@5 98.828 (98.983)
Epoch: [26][192/196]	Time 0.674 (0.712)	Data 0.000 (0.002)	Loss 0.4435 (0.5083)	Acc@1 84.766 (82.387)	Acc@5 100.000 (99.039)
after train
Epoche: [27/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [27][0/196]	Time 0.717 (0.717)	Data 0.376 (0.376)	Loss 0.3940 (0.3940)	Acc@1 88.281 (88.281)	Acc@5 98.828 (98.828)
Epoch: [27][64/196]	Time 0.717 (0.714)	Data 0.000 (0.006)	Loss 0.4697 (0.5043)	Acc@1 85.156 (82.788)	Acc@5 98.828 (99.020)
Epoch: [27][128/196]	Time 0.673 (0.708)	Data 0.000 (0.003)	Loss 0.6450 (0.5084)	Acc@1 78.125 (82.546)	Acc@5 100.000 (99.092)
Epoch: [27][192/196]	Time 0.685 (0.711)	Data 0.000 (0.002)	Loss 0.5675 (0.5108)	Acc@1 82.031 (82.499)	Acc@5 98.828 (99.111)
after train
Epoche: [28/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [28][0/196]	Time 0.876 (0.876)	Data 0.345 (0.345)	Loss 0.6083 (0.6083)	Acc@1 78.516 (78.516)	Acc@5 98.828 (98.828)
Epoch: [28][64/196]	Time 0.719 (0.712)	Data 0.000 (0.006)	Loss 0.5357 (0.5012)	Acc@1 83.203 (82.548)	Acc@5 98.047 (99.129)
Epoch: [28][128/196]	Time 0.657 (0.705)	Data 0.000 (0.003)	Loss 0.4685 (0.5045)	Acc@1 83.203 (82.585)	Acc@5 99.609 (99.061)
Epoch: [28][192/196]	Time 0.728 (0.707)	Data 0.000 (0.002)	Loss 0.4031 (0.5070)	Acc@1 84.766 (82.489)	Acc@5 100.000 (99.111)
after train
Epoche: [29/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [29][0/196]	Time 0.742 (0.742)	Data 0.466 (0.466)	Loss 0.4852 (0.4852)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [29][64/196]	Time 0.709 (0.716)	Data 0.000 (0.008)	Loss 0.4867 (0.4977)	Acc@1 82.031 (83.119)	Acc@5 99.609 (99.201)
Epoch: [29][128/196]	Time 0.749 (0.710)	Data 0.000 (0.004)	Loss 0.6022 (0.4991)	Acc@1 80.078 (82.891)	Acc@5 98.828 (99.170)
Epoch: [29][192/196]	Time 0.658 (0.709)	Data 0.000 (0.003)	Loss 0.4472 (0.5039)	Acc@1 83.203 (82.715)	Acc@5 99.609 (99.130)
after train
Epoche: [30/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [30][0/196]	Time 0.702 (0.702)	Data 0.535 (0.535)	Loss 0.4951 (0.4951)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [30][64/196]	Time 0.682 (0.724)	Data 0.000 (0.009)	Loss 0.4013 (0.4953)	Acc@1 85.156 (82.897)	Acc@5 99.219 (99.201)
Epoch: [30][128/196]	Time 0.583 (0.712)	Data 0.000 (0.004)	Loss 0.4525 (0.4945)	Acc@1 84.375 (82.855)	Acc@5 98.828 (99.204)
Epoch: [30][192/196]	Time 0.685 (0.714)	Data 0.000 (0.003)	Loss 0.3902 (0.4965)	Acc@1 85.547 (82.754)	Acc@5 99.609 (99.217)
after train
Epoche: [31/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [31][0/196]	Time 0.793 (0.793)	Data 0.494 (0.494)	Loss 0.3777 (0.3777)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [31][64/196]	Time 0.792 (0.719)	Data 0.000 (0.008)	Loss 0.5406 (0.4784)	Acc@1 82.031 (83.612)	Acc@5 98.438 (99.297)
Epoch: [31][128/196]	Time 0.709 (0.702)	Data 0.000 (0.004)	Loss 0.4894 (0.4930)	Acc@1 84.375 (83.103)	Acc@5 99.219 (99.198)
Epoch: [31][192/196]	Time 0.663 (0.703)	Data 0.000 (0.003)	Loss 0.5642 (0.4921)	Acc@1 81.641 (83.001)	Acc@5 99.609 (99.196)
after train
Epoche: [32/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [32][0/196]	Time 0.623 (0.623)	Data 0.398 (0.398)	Loss 0.5231 (0.5231)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [32][64/196]	Time 0.766 (0.719)	Data 0.000 (0.007)	Loss 0.5019 (0.4844)	Acc@1 82.422 (83.618)	Acc@5 100.000 (99.213)
Epoch: [32][128/196]	Time 0.600 (0.705)	Data 0.000 (0.004)	Loss 0.3878 (0.4872)	Acc@1 89.453 (83.246)	Acc@5 99.609 (99.219)
Epoch: [32][192/196]	Time 0.751 (0.714)	Data 0.000 (0.003)	Loss 0.4365 (0.4868)	Acc@1 84.766 (83.242)	Acc@5 99.219 (99.251)
after train
Epoche: [33/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [33][0/196]	Time 0.626 (0.626)	Data 0.485 (0.485)	Loss 0.5028 (0.5028)	Acc@1 81.641 (81.641)	Acc@5 99.219 (99.219)
Epoch: [33][64/196]	Time 0.747 (0.698)	Data 0.000 (0.008)	Loss 0.5412 (0.4809)	Acc@1 81.641 (83.510)	Acc@5 99.219 (99.267)
Epoch: [33][128/196]	Time 0.636 (0.686)	Data 0.000 (0.005)	Loss 0.5163 (0.4866)	Acc@1 81.250 (83.264)	Acc@5 99.609 (99.249)
Epoch: [33][192/196]	Time 0.733 (0.698)	Data 0.000 (0.003)	Loss 0.5408 (0.4920)	Acc@1 81.250 (83.043)	Acc@5 99.609 (99.221)
after train
Epoche: [34/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [34][0/196]	Time 0.614 (0.614)	Data 0.437 (0.437)	Loss 0.4094 (0.4094)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [34][64/196]	Time 0.712 (0.706)	Data 0.000 (0.007)	Loss 0.4215 (0.4679)	Acc@1 86.328 (83.888)	Acc@5 98.828 (99.339)
Epoch: [34][128/196]	Time 0.653 (0.700)	Data 0.000 (0.004)	Loss 0.5164 (0.4854)	Acc@1 83.984 (83.285)	Acc@5 98.047 (99.267)
Epoch: [34][192/196]	Time 0.720 (0.709)	Data 0.000 (0.003)	Loss 0.5766 (0.4874)	Acc@1 80.078 (83.268)	Acc@5 99.219 (99.249)
after train
Epoche: [35/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [35][0/196]	Time 0.779 (0.779)	Data 0.611 (0.611)	Loss 0.5155 (0.5155)	Acc@1 81.641 (81.641)	Acc@5 100.000 (100.000)
Epoch: [35][64/196]	Time 0.708 (0.707)	Data 0.000 (0.010)	Loss 0.4097 (0.4811)	Acc@1 85.938 (83.546)	Acc@5 99.609 (99.327)
Epoch: [35][128/196]	Time 0.667 (0.707)	Data 0.000 (0.005)	Loss 0.4251 (0.4858)	Acc@1 82.422 (83.270)	Acc@5 99.219 (99.204)
Epoch: [35][192/196]	Time 0.730 (0.709)	Data 0.000 (0.004)	Loss 0.4657 (0.4857)	Acc@1 84.766 (83.296)	Acc@5 99.219 (99.166)
after train
Epoche: [36/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [36][0/196]	Time 0.722 (0.722)	Data 0.370 (0.370)	Loss 0.3456 (0.3456)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [36][64/196]	Time 0.723 (0.719)	Data 0.000 (0.006)	Loss 0.5050 (0.4881)	Acc@1 82.812 (83.245)	Acc@5 99.219 (99.171)
Epoch: [36][128/196]	Time 0.635 (0.708)	Data 0.000 (0.003)	Loss 0.4653 (0.4834)	Acc@1 84.766 (83.576)	Acc@5 98.828 (99.143)
Epoch: [36][192/196]	Time 0.681 (0.710)	Data 0.000 (0.002)	Loss 0.5533 (0.4804)	Acc@1 83.203 (83.559)	Acc@5 99.219 (99.168)
after train
Epoche: [37/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [37][0/196]	Time 0.686 (0.686)	Data 0.721 (0.721)	Loss 0.4061 (0.4061)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [37][64/196]	Time 0.669 (0.702)	Data 0.000 (0.012)	Loss 0.4637 (0.4713)	Acc@1 83.594 (83.876)	Acc@5 98.438 (99.327)
Epoch: [37][128/196]	Time 0.579 (0.699)	Data 0.000 (0.006)	Loss 0.5028 (0.4828)	Acc@1 82.031 (83.457)	Acc@5 99.609 (99.243)
Epoch: [37][192/196]	Time 0.674 (0.707)	Data 0.000 (0.004)	Loss 0.4341 (0.4805)	Acc@1 85.156 (83.442)	Acc@5 99.609 (99.239)
after train
Epoche: [38/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [38][0/196]	Time 0.961 (0.961)	Data 0.453 (0.453)	Loss 0.4680 (0.4680)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [38][64/196]	Time 0.707 (0.724)	Data 0.000 (0.007)	Loss 0.5461 (0.4816)	Acc@1 80.859 (83.618)	Acc@5 99.609 (99.231)
Epoch: [38][128/196]	Time 0.800 (0.713)	Data 0.000 (0.004)	Loss 0.5324 (0.4738)	Acc@1 82.812 (83.875)	Acc@5 99.609 (99.204)
Epoch: [38][192/196]	Time 0.719 (0.715)	Data 0.000 (0.003)	Loss 0.4252 (0.4786)	Acc@1 83.984 (83.715)	Acc@5 100.000 (99.201)
after train
Epoche: [39/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [39][0/196]	Time 0.620 (0.620)	Data 0.395 (0.395)	Loss 0.3630 (0.3630)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [39][64/196]	Time 0.677 (0.718)	Data 0.000 (0.008)	Loss 0.4062 (0.4633)	Acc@1 89.453 (83.924)	Acc@5 99.609 (99.333)
Epoch: [39][128/196]	Time 0.700 (0.704)	Data 0.000 (0.004)	Loss 0.5743 (0.4759)	Acc@1 81.250 (83.676)	Acc@5 98.438 (99.231)
Epoch: [39][192/196]	Time 0.733 (0.706)	Data 0.000 (0.003)	Loss 0.3998 (0.4727)	Acc@1 86.719 (83.833)	Acc@5 99.609 (99.247)
after train
Epoche: [40/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [40][0/196]	Time 0.687 (0.687)	Data 0.430 (0.430)	Loss 0.5067 (0.5067)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [40][64/196]	Time 0.645 (0.718)	Data 0.000 (0.007)	Loss 0.4276 (0.4660)	Acc@1 85.547 (84.123)	Acc@5 98.828 (99.213)
Epoch: [40][128/196]	Time 0.714 (0.702)	Data 0.000 (0.004)	Loss 0.4653 (0.4694)	Acc@1 84.375 (84.021)	Acc@5 99.219 (99.216)
Epoch: [40][192/196]	Time 0.637 (0.708)	Data 0.000 (0.003)	Loss 0.5062 (0.4735)	Acc@1 82.422 (83.857)	Acc@5 99.219 (99.245)
after train
Epoche: [41/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [41][0/196]	Time 0.490 (0.490)	Data 0.366 (0.366)	Loss 0.4694 (0.4694)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [41][64/196]	Time 0.488 (0.710)	Data 0.000 (0.006)	Loss 0.4551 (0.4722)	Acc@1 83.594 (83.780)	Acc@5 99.219 (99.171)
Epoch: [41][128/196]	Time 0.663 (0.705)	Data 0.000 (0.003)	Loss 0.5246 (0.4760)	Acc@1 83.984 (83.863)	Acc@5 99.219 (99.222)
Epoch: [41][192/196]	Time 0.659 (0.711)	Data 0.000 (0.002)	Loss 0.5539 (0.4744)	Acc@1 79.297 (83.721)	Acc@5 99.609 (99.259)
after train
Epoche: [42/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [42][0/196]	Time 0.635 (0.635)	Data 0.398 (0.398)	Loss 0.4027 (0.4027)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [42][64/196]	Time 0.720 (0.718)	Data 0.000 (0.007)	Loss 0.3860 (0.4544)	Acc@1 85.938 (84.351)	Acc@5 100.000 (99.339)
Epoch: [42][128/196]	Time 0.638 (0.701)	Data 0.000 (0.003)	Loss 0.6321 (0.4674)	Acc@1 75.781 (83.839)	Acc@5 98.047 (99.228)
Epoch: [42][192/196]	Time 0.698 (0.704)	Data 0.000 (0.002)	Loss 0.5333 (0.4694)	Acc@1 80.859 (83.786)	Acc@5 99.609 (99.223)
after train
Epoche: [43/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [43][0/196]	Time 0.739 (0.739)	Data 0.454 (0.454)	Loss 0.4650 (0.4650)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [43][64/196]	Time 0.593 (0.712)	Data 0.000 (0.008)	Loss 0.3663 (0.4677)	Acc@1 89.062 (83.990)	Acc@5 99.609 (99.159)
Epoch: [43][128/196]	Time 0.635 (0.704)	Data 0.000 (0.004)	Loss 0.4649 (0.4690)	Acc@1 86.719 (83.781)	Acc@5 99.609 (99.246)
Epoch: [43][192/196]	Time 0.741 (0.712)	Data 0.000 (0.003)	Loss 0.4579 (0.4702)	Acc@1 82.812 (83.754)	Acc@5 99.219 (99.227)
after train
Epoche: [44/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [44][0/196]	Time 0.948 (0.948)	Data 0.638 (0.638)	Loss 0.4423 (0.4423)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [44][64/196]	Time 0.901 (0.716)	Data 0.000 (0.011)	Loss 0.5768 (0.4731)	Acc@1 82.422 (83.828)	Acc@5 98.438 (99.177)
Epoch: [44][128/196]	Time 0.763 (0.705)	Data 0.000 (0.006)	Loss 0.4336 (0.4695)	Acc@1 84.375 (83.830)	Acc@5 99.609 (99.258)
Epoch: [44][192/196]	Time 0.664 (0.710)	Data 0.000 (0.004)	Loss 0.3771 (0.4687)	Acc@1 87.109 (83.827)	Acc@5 100.000 (99.267)
after train
Epoche: [45/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [45][0/196]	Time 0.654 (0.654)	Data 0.474 (0.474)	Loss 0.3918 (0.3918)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [45][64/196]	Time 0.732 (0.704)	Data 0.000 (0.008)	Loss 0.4686 (0.4675)	Acc@1 83.203 (83.870)	Acc@5 98.828 (99.213)
Epoch: [45][128/196]	Time 0.687 (0.691)	Data 0.000 (0.004)	Loss 0.4240 (0.4680)	Acc@1 85.156 (83.839)	Acc@5 100.000 (99.264)
Epoch: [45][192/196]	Time 0.726 (0.699)	Data 0.000 (0.003)	Loss 0.5126 (0.4705)	Acc@1 82.031 (83.725)	Acc@5 99.609 (99.275)
after train
Epoche: [46/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [46][0/196]	Time 0.635 (0.635)	Data 0.503 (0.503)	Loss 0.4896 (0.4896)	Acc@1 83.594 (83.594)	Acc@5 98.047 (98.047)
Epoch: [46][64/196]	Time 0.491 (0.706)	Data 0.021 (0.009)	Loss 0.6020 (0.4580)	Acc@1 79.297 (84.483)	Acc@5 98.047 (99.363)
Epoch: [46][128/196]	Time 0.640 (0.704)	Data 0.000 (0.005)	Loss 0.4252 (0.4592)	Acc@1 82.812 (84.254)	Acc@5 100.000 (99.301)
Epoch: [46][192/196]	Time 0.746 (0.707)	Data 0.000 (0.003)	Loss 0.4945 (0.4613)	Acc@1 82.031 (84.114)	Acc@5 99.219 (99.310)
after train
Epoche: [47/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [47][0/196]	Time 0.783 (0.783)	Data 0.360 (0.360)	Loss 0.4481 (0.4481)	Acc@1 85.547 (85.547)	Acc@5 98.828 (98.828)
Epoch: [47][64/196]	Time 0.691 (0.697)	Data 0.000 (0.006)	Loss 0.4951 (0.4688)	Acc@1 82.812 (83.696)	Acc@5 99.219 (99.261)
Epoch: [47][128/196]	Time 0.793 (0.693)	Data 0.000 (0.003)	Loss 0.4662 (0.4676)	Acc@1 83.203 (83.748)	Acc@5 98.438 (99.291)
Epoch: [47][192/196]	Time 0.651 (0.700)	Data 0.000 (0.002)	Loss 0.4934 (0.4642)	Acc@1 83.594 (84.011)	Acc@5 98.828 (99.286)
after train
Epoche: [48/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [48][0/196]	Time 0.878 (0.878)	Data 0.371 (0.371)	Loss 0.5128 (0.5128)	Acc@1 80.078 (80.078)	Acc@5 98.828 (98.828)
Epoch: [48][64/196]	Time 0.631 (0.717)	Data 0.000 (0.006)	Loss 0.4226 (0.4698)	Acc@1 83.984 (83.726)	Acc@5 100.000 (99.327)
Epoch: [48][128/196]	Time 0.684 (0.715)	Data 0.000 (0.003)	Loss 0.3751 (0.4636)	Acc@1 85.938 (83.969)	Acc@5 99.609 (99.310)
Epoch: [48][192/196]	Time 0.674 (0.710)	Data 0.000 (0.003)	Loss 0.5170 (0.4623)	Acc@1 85.156 (84.035)	Acc@5 98.828 (99.302)
after train
Epoche: [49/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [49][0/196]	Time 0.731 (0.731)	Data 0.443 (0.443)	Loss 0.4175 (0.4175)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [49][64/196]	Time 0.693 (0.708)	Data 0.000 (0.007)	Loss 0.5378 (0.4542)	Acc@1 82.031 (84.495)	Acc@5 97.656 (99.411)
Epoch: [49][128/196]	Time 0.701 (0.698)	Data 0.000 (0.004)	Loss 0.5027 (0.4589)	Acc@1 83.203 (84.502)	Acc@5 99.609 (99.310)
Epoch: [49][192/196]	Time 0.785 (0.703)	Data 0.000 (0.003)	Loss 0.3899 (0.4596)	Acc@1 86.719 (84.399)	Acc@5 100.000 (99.267)
after train
Epoche: [50/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [50][0/196]	Time 0.747 (0.747)	Data 0.362 (0.362)	Loss 0.4393 (0.4393)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [50][64/196]	Time 0.744 (0.720)	Data 0.000 (0.006)	Loss 0.4590 (0.4554)	Acc@1 83.984 (84.189)	Acc@5 99.609 (99.339)
Epoch: [50][128/196]	Time 0.655 (0.708)	Data 0.000 (0.003)	Loss 0.4142 (0.4533)	Acc@1 87.500 (84.308)	Acc@5 100.000 (99.331)
Epoch: [50][192/196]	Time 0.710 (0.708)	Data 0.000 (0.002)	Loss 0.4608 (0.4616)	Acc@1 85.938 (84.061)	Acc@5 99.609 (99.294)
after train
Epoche: [51/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [51][0/196]	Time 0.813 (0.813)	Data 0.357 (0.357)	Loss 0.4230 (0.4230)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [51][64/196]	Time 0.850 (0.707)	Data 0.000 (0.006)	Loss 0.4531 (0.4593)	Acc@1 82.031 (83.996)	Acc@5 99.609 (99.327)
Epoch: [51][128/196]	Time 0.695 (0.701)	Data 0.000 (0.003)	Loss 0.3664 (0.4525)	Acc@1 87.891 (84.275)	Acc@5 100.000 (99.334)
Epoch: [51][192/196]	Time 0.748 (0.706)	Data 0.000 (0.002)	Loss 0.3302 (0.4565)	Acc@1 89.062 (84.134)	Acc@5 100.000 (99.312)
after train
Epoche: [52/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [52][0/196]	Time 0.836 (0.836)	Data 0.447 (0.447)	Loss 0.3898 (0.3898)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [52][64/196]	Time 0.764 (0.709)	Data 0.000 (0.008)	Loss 0.4071 (0.4563)	Acc@1 83.984 (83.984)	Acc@5 98.828 (99.279)
Epoch: [52][128/196]	Time 0.734 (0.703)	Data 0.000 (0.004)	Loss 0.2886 (0.4501)	Acc@1 90.234 (84.433)	Acc@5 100.000 (99.319)
Epoch: [52][192/196]	Time 0.552 (0.707)	Data 0.000 (0.003)	Loss 0.4020 (0.4533)	Acc@1 87.500 (84.418)	Acc@5 99.219 (99.310)
after train
Epoche: [53/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [53][0/196]	Time 0.716 (0.716)	Data 0.377 (0.377)	Loss 0.3835 (0.3835)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [53][64/196]	Time 0.687 (0.702)	Data 0.000 (0.006)	Loss 0.5326 (0.4607)	Acc@1 83.984 (84.435)	Acc@5 99.609 (99.279)
Epoch: [53][128/196]	Time 0.693 (0.701)	Data 0.000 (0.003)	Loss 0.5412 (0.4542)	Acc@1 82.812 (84.490)	Acc@5 99.609 (99.322)
Epoch: [53][192/196]	Time 0.719 (0.706)	Data 0.000 (0.002)	Loss 0.3923 (0.4561)	Acc@1 85.938 (84.460)	Acc@5 99.609 (99.302)
after train
Epoche: [54/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [54][0/196]	Time 0.557 (0.557)	Data 0.372 (0.372)	Loss 0.4322 (0.4322)	Acc@1 85.547 (85.547)	Acc@5 100.000 (100.000)
Epoch: [54][64/196]	Time 0.797 (0.710)	Data 0.000 (0.006)	Loss 0.4714 (0.4361)	Acc@1 84.766 (84.700)	Acc@5 98.438 (99.351)
Epoch: [54][128/196]	Time 0.692 (0.699)	Data 0.000 (0.003)	Loss 0.4001 (0.4483)	Acc@1 88.672 (84.496)	Acc@5 98.828 (99.310)
Epoch: [54][192/196]	Time 0.680 (0.707)	Data 0.000 (0.002)	Loss 0.4128 (0.4505)	Acc@1 86.719 (84.468)	Acc@5 99.219 (99.290)
after train
Epoche: [55/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [55][0/196]	Time 0.784 (0.784)	Data 0.379 (0.379)	Loss 0.3238 (0.3238)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [55][64/196]	Time 0.679 (0.704)	Data 0.000 (0.006)	Loss 0.3859 (0.4509)	Acc@1 88.281 (84.718)	Acc@5 99.219 (99.351)
Epoch: [55][128/196]	Time 0.689 (0.698)	Data 0.000 (0.003)	Loss 0.5085 (0.4562)	Acc@1 81.250 (84.436)	Acc@5 98.828 (99.322)
Epoch: [55][192/196]	Time 0.617 (0.701)	Data 0.000 (0.003)	Loss 0.4978 (0.4601)	Acc@1 83.203 (84.254)	Acc@5 100.000 (99.322)
after train
Epoche: [56/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [56][0/196]	Time 0.757 (0.757)	Data 0.371 (0.371)	Loss 0.3944 (0.3944)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [56][64/196]	Time 0.663 (0.711)	Data 0.000 (0.006)	Loss 0.5206 (0.4337)	Acc@1 80.859 (85.030)	Acc@5 99.219 (99.375)
Epoch: [56][128/196]	Time 0.675 (0.708)	Data 0.000 (0.003)	Loss 0.3664 (0.4435)	Acc@1 85.547 (84.617)	Acc@5 99.609 (99.325)
Epoch: [56][192/196]	Time 0.753 (0.709)	Data 0.000 (0.002)	Loss 0.5029 (0.4480)	Acc@1 83.203 (84.474)	Acc@5 97.266 (99.308)
after train
Epoche: [57/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [57][0/196]	Time 0.710 (0.710)	Data 0.404 (0.404)	Loss 0.5202 (0.5202)	Acc@1 80.469 (80.469)	Acc@5 100.000 (100.000)
Epoch: [57][64/196]	Time 0.585 (0.693)	Data 0.000 (0.007)	Loss 0.4634 (0.4487)	Acc@1 85.547 (84.567)	Acc@5 99.609 (99.405)
Epoch: [57][128/196]	Time 0.659 (0.695)	Data 0.000 (0.004)	Loss 0.4595 (0.4487)	Acc@1 83.984 (84.602)	Acc@5 98.438 (99.358)
Epoch: [57][192/196]	Time 0.660 (0.700)	Data 0.000 (0.003)	Loss 0.4602 (0.4510)	Acc@1 84.375 (84.535)	Acc@5 99.609 (99.354)
after train
Epoche: [58/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [58][0/196]	Time 0.730 (0.730)	Data 0.422 (0.422)	Loss 0.4139 (0.4139)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [58][64/196]	Time 0.647 (0.709)	Data 0.000 (0.007)	Loss 0.4456 (0.4421)	Acc@1 82.812 (84.700)	Acc@5 98.047 (99.237)
Epoch: [58][128/196]	Time 0.659 (0.709)	Data 0.000 (0.004)	Loss 0.4638 (0.4497)	Acc@1 83.984 (84.387)	Acc@5 99.609 (99.246)
Epoch: [58][192/196]	Time 0.689 (0.711)	Data 0.000 (0.003)	Loss 0.3433 (0.4461)	Acc@1 87.109 (84.500)	Acc@5 100.000 (99.324)
after train
Epoche: [59/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [59][0/196]	Time 0.831 (0.831)	Data 0.292 (0.292)	Loss 0.4257 (0.4257)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [59][64/196]	Time 0.687 (0.710)	Data 0.000 (0.005)	Loss 0.4659 (0.4462)	Acc@1 83.594 (84.387)	Acc@5 100.000 (99.291)
Epoch: [59][128/196]	Time 0.694 (0.700)	Data 0.000 (0.003)	Loss 0.3435 (0.4447)	Acc@1 86.328 (84.560)	Acc@5 99.609 (99.322)
Epoch: [59][192/196]	Time 0.752 (0.705)	Data 0.000 (0.002)	Loss 0.4880 (0.4459)	Acc@1 82.031 (84.616)	Acc@5 100.000 (99.326)
after train
Epoche: [60/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [60][0/196]	Time 0.755 (0.755)	Data 0.530 (0.530)	Loss 0.4301 (0.4301)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [60][64/196]	Time 0.612 (0.695)	Data 0.000 (0.008)	Loss 0.3807 (0.4375)	Acc@1 86.719 (85.018)	Acc@5 99.609 (99.369)
Epoch: [60][128/196]	Time 0.788 (0.705)	Data 0.000 (0.004)	Loss 0.4443 (0.4431)	Acc@1 87.109 (84.811)	Acc@5 98.828 (99.358)
Epoch: [60][192/196]	Time 0.730 (0.709)	Data 0.000 (0.003)	Loss 0.4562 (0.4456)	Acc@1 82.422 (84.610)	Acc@5 99.609 (99.350)
after train
Epoche: [61/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [61][0/196]	Time 0.755 (0.755)	Data 0.480 (0.480)	Loss 0.3924 (0.3924)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [61][64/196]	Time 0.588 (0.710)	Data 0.000 (0.008)	Loss 0.5299 (0.4361)	Acc@1 82.031 (84.952)	Acc@5 98.438 (99.309)
Epoch: [61][128/196]	Time 0.596 (0.703)	Data 0.000 (0.004)	Loss 0.4317 (0.4432)	Acc@1 85.547 (84.687)	Acc@5 99.219 (99.337)
Epoch: [61][192/196]	Time 0.711 (0.709)	Data 0.000 (0.003)	Loss 0.5829 (0.4457)	Acc@1 81.250 (84.668)	Acc@5 99.609 (99.328)
after train
Epoche: [62/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [62][0/196]	Time 0.874 (0.874)	Data 0.495 (0.495)	Loss 0.4690 (0.4690)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [62][64/196]	Time 0.741 (0.712)	Data 0.000 (0.008)	Loss 0.4864 (0.4511)	Acc@1 82.812 (84.736)	Acc@5 100.000 (99.249)
Epoch: [62][128/196]	Time 0.667 (0.700)	Data 0.000 (0.004)	Loss 0.5007 (0.4464)	Acc@1 84.375 (84.784)	Acc@5 98.438 (99.294)
Epoch: [62][192/196]	Time 0.757 (0.706)	Data 0.000 (0.003)	Loss 0.3987 (0.4435)	Acc@1 85.156 (84.810)	Acc@5 99.219 (99.336)
after train
Epoche: [63/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [63][0/196]	Time 0.773 (0.773)	Data 0.377 (0.377)	Loss 0.4142 (0.4142)	Acc@1 87.500 (87.500)	Acc@5 98.438 (98.438)
Epoch: [63][64/196]	Time 0.613 (0.701)	Data 0.000 (0.006)	Loss 0.3934 (0.4456)	Acc@1 84.766 (84.537)	Acc@5 100.000 (99.297)
Epoch: [63][128/196]	Time 0.689 (0.694)	Data 0.000 (0.003)	Loss 0.4440 (0.4446)	Acc@1 85.156 (84.705)	Acc@5 99.609 (99.313)
Epoch: [63][192/196]	Time 0.590 (0.702)	Data 0.000 (0.002)	Loss 0.5353 (0.4450)	Acc@1 84.766 (84.753)	Acc@5 99.609 (99.292)
after train
Epoche: [64/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [64][0/196]	Time 0.657 (0.657)	Data 0.346 (0.346)	Loss 0.3518 (0.3518)	Acc@1 88.672 (88.672)	Acc@5 99.219 (99.219)
Epoch: [64][64/196]	Time 0.733 (0.701)	Data 0.000 (0.006)	Loss 0.3941 (0.4229)	Acc@1 87.500 (85.258)	Acc@5 98.828 (99.435)
Epoch: [64][128/196]	Time 0.644 (0.697)	Data 0.000 (0.003)	Loss 0.3408 (0.4363)	Acc@1 89.453 (84.932)	Acc@5 100.000 (99.364)
Epoch: [64][192/196]	Time 0.759 (0.701)	Data 0.000 (0.002)	Loss 0.5153 (0.4428)	Acc@1 83.203 (84.652)	Acc@5 99.609 (99.340)
after train
Epoche: [65/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [65][0/196]	Time 0.825 (0.825)	Data 0.412 (0.412)	Loss 0.4346 (0.4346)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [65][64/196]	Time 0.611 (0.705)	Data 0.000 (0.007)	Loss 0.4435 (0.4350)	Acc@1 83.203 (84.892)	Acc@5 98.828 (99.411)
Epoch: [65][128/196]	Time 0.775 (0.692)	Data 0.000 (0.004)	Loss 0.4906 (0.4411)	Acc@1 84.375 (84.775)	Acc@5 99.609 (99.379)
Epoch: [65][192/196]	Time 0.736 (0.701)	Data 0.000 (0.002)	Loss 0.4383 (0.4420)	Acc@1 84.375 (84.709)	Acc@5 99.609 (99.362)
after train
Epoche: [66/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [66][0/196]	Time 0.795 (0.795)	Data 0.357 (0.357)	Loss 0.4354 (0.4354)	Acc@1 85.156 (85.156)	Acc@5 98.047 (98.047)
Epoch: [66][64/196]	Time 0.716 (0.716)	Data 0.000 (0.006)	Loss 0.4544 (0.4454)	Acc@1 80.859 (84.537)	Acc@5 100.000 (99.267)
Epoch: [66][128/196]	Time 0.733 (0.709)	Data 0.000 (0.003)	Loss 0.3672 (0.4410)	Acc@1 87.500 (84.657)	Acc@5 99.219 (99.358)
Epoch: [66][192/196]	Time 0.722 (0.713)	Data 0.000 (0.002)	Loss 0.4862 (0.4448)	Acc@1 83.984 (84.557)	Acc@5 99.609 (99.350)
after train
Epoche: [67/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [67][0/196]	Time 0.757 (0.757)	Data 0.344 (0.344)	Loss 0.3467 (0.3467)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [67][64/196]	Time 0.704 (0.711)	Data 0.000 (0.006)	Loss 0.4447 (0.4401)	Acc@1 83.594 (84.988)	Acc@5 99.609 (99.339)
Epoch: [67][128/196]	Time 0.561 (0.700)	Data 0.000 (0.003)	Loss 0.4104 (0.4381)	Acc@1 85.547 (84.956)	Acc@5 99.609 (99.364)
Epoch: [67][192/196]	Time 0.698 (0.705)	Data 0.000 (0.002)	Loss 0.4278 (0.4357)	Acc@1 83.594 (85.015)	Acc@5 100.000 (99.364)
after train
Epoche: [68/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [68][0/196]	Time 0.775 (0.775)	Data 0.370 (0.370)	Loss 0.4489 (0.4489)	Acc@1 83.594 (83.594)	Acc@5 98.828 (98.828)
Epoch: [68][64/196]	Time 0.720 (0.713)	Data 0.000 (0.006)	Loss 0.3704 (0.4409)	Acc@1 87.109 (84.748)	Acc@5 99.219 (99.315)
Epoch: [68][128/196]	Time 0.647 (0.707)	Data 0.000 (0.003)	Loss 0.4458 (0.4459)	Acc@1 83.984 (84.645)	Acc@5 99.609 (99.322)
Epoch: [68][192/196]	Time 0.694 (0.708)	Data 0.000 (0.002)	Loss 0.4150 (0.4413)	Acc@1 85.547 (84.782)	Acc@5 99.609 (99.314)
after train
Epoche: [69/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [69][0/196]	Time 0.837 (0.837)	Data 0.343 (0.343)	Loss 0.3360 (0.3360)	Acc@1 89.453 (89.453)	Acc@5 99.219 (99.219)
Epoch: [69][64/196]	Time 0.704 (0.711)	Data 0.000 (0.006)	Loss 0.4713 (0.4315)	Acc@1 82.812 (84.928)	Acc@5 100.000 (99.357)
Epoch: [69][128/196]	Time 0.612 (0.705)	Data 0.000 (0.003)	Loss 0.4846 (0.4345)	Acc@1 85.938 (84.944)	Acc@5 98.438 (99.388)
Epoch: [69][192/196]	Time 0.679 (0.710)	Data 0.000 (0.002)	Loss 0.4483 (0.4347)	Acc@1 83.594 (85.037)	Acc@5 99.609 (99.369)
after train
Epoche: [70/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [70][0/196]	Time 0.837 (0.837)	Data 0.345 (0.345)	Loss 0.4466 (0.4466)	Acc@1 84.375 (84.375)	Acc@5 98.828 (98.828)
Epoch: [70][64/196]	Time 0.725 (0.713)	Data 0.000 (0.006)	Loss 0.5216 (0.4305)	Acc@1 82.031 (85.505)	Acc@5 99.219 (99.321)
Epoch: [70][128/196]	Time 0.660 (0.708)	Data 0.000 (0.003)	Loss 0.4831 (0.4374)	Acc@1 84.766 (85.117)	Acc@5 99.219 (99.288)
Epoch: [70][192/196]	Time 0.710 (0.712)	Data 0.000 (0.002)	Loss 0.4194 (0.4413)	Acc@1 86.328 (84.853)	Acc@5 98.438 (99.284)
after train
Epoche: [71/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [71][0/196]	Time 0.813 (0.813)	Data 0.510 (0.510)	Loss 0.4114 (0.4114)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [71][64/196]	Time 0.751 (0.711)	Data 0.000 (0.008)	Loss 0.3825 (0.4416)	Acc@1 86.328 (84.573)	Acc@5 99.219 (99.375)
Epoch: [71][128/196]	Time 0.549 (0.708)	Data 0.000 (0.004)	Loss 0.3571 (0.4342)	Acc@1 89.062 (84.853)	Acc@5 99.609 (99.394)
Epoch: [71][192/196]	Time 0.695 (0.713)	Data 0.000 (0.003)	Loss 0.4464 (0.4368)	Acc@1 85.938 (84.828)	Acc@5 98.438 (99.393)
after train
Epoche: [72/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [72][0/196]	Time 0.805 (0.805)	Data 0.410 (0.410)	Loss 0.4724 (0.4724)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [72][64/196]	Time 0.694 (0.713)	Data 0.000 (0.007)	Loss 0.5124 (0.4410)	Acc@1 83.984 (85.006)	Acc@5 99.609 (99.357)
Epoch: [72][128/196]	Time 0.723 (0.713)	Data 0.000 (0.003)	Loss 0.4256 (0.4365)	Acc@1 85.938 (85.171)	Acc@5 99.609 (99.319)
Epoch: [72][192/196]	Time 0.716 (0.715)	Data 0.000 (0.002)	Loss 0.3344 (0.4365)	Acc@1 90.234 (85.122)	Acc@5 100.000 (99.340)
after train
Epoche: [73/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [73][0/196]	Time 0.818 (0.818)	Data 0.357 (0.357)	Loss 0.5458 (0.5458)	Acc@1 78.906 (78.906)	Acc@5 99.609 (99.609)
Epoch: [73][64/196]	Time 0.719 (0.716)	Data 0.000 (0.006)	Loss 0.4079 (0.4447)	Acc@1 86.328 (84.567)	Acc@5 100.000 (99.393)
Epoch: [73][128/196]	Time 0.706 (0.712)	Data 0.000 (0.003)	Loss 0.3692 (0.4436)	Acc@1 87.109 (84.614)	Acc@5 98.828 (99.382)
Epoch: [73][192/196]	Time 0.717 (0.713)	Data 0.000 (0.002)	Loss 0.4100 (0.4456)	Acc@1 84.766 (84.628)	Acc@5 99.609 (99.358)
after train
Epoche: [74/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [74][0/196]	Time 0.756 (0.756)	Data 0.359 (0.359)	Loss 0.5312 (0.5312)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [74][64/196]	Time 0.743 (0.716)	Data 0.000 (0.006)	Loss 0.4050 (0.4283)	Acc@1 85.156 (85.042)	Acc@5 99.609 (99.387)
Epoch: [74][128/196]	Time 0.753 (0.705)	Data 0.000 (0.003)	Loss 0.4780 (0.4336)	Acc@1 82.812 (84.799)	Acc@5 99.219 (99.352)
Epoch: [74][192/196]	Time 0.714 (0.709)	Data 0.000 (0.002)	Loss 0.4690 (0.4339)	Acc@1 82.812 (84.883)	Acc@5 99.609 (99.364)
after train
Epoche: [75/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [75][0/196]	Time 0.809 (0.809)	Data 0.338 (0.338)	Loss 0.3720 (0.3720)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [75][64/196]	Time 0.704 (0.712)	Data 0.000 (0.005)	Loss 0.3948 (0.4176)	Acc@1 83.984 (85.589)	Acc@5 100.000 (99.393)
Epoch: [75][128/196]	Time 0.748 (0.704)	Data 0.000 (0.003)	Loss 0.4406 (0.4215)	Acc@1 87.109 (85.498)	Acc@5 98.828 (99.382)
Epoch: [75][192/196]	Time 0.670 (0.706)	Data 0.000 (0.002)	Loss 0.5832 (0.4277)	Acc@1 82.031 (85.191)	Acc@5 98.047 (99.371)
after train
Epoche: [76/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [76][0/196]	Time 0.826 (0.826)	Data 0.371 (0.371)	Loss 0.4951 (0.4951)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [76][64/196]	Time 0.713 (0.710)	Data 0.000 (0.006)	Loss 0.4473 (0.4459)	Acc@1 84.766 (84.814)	Acc@5 98.828 (99.273)
Epoch: [76][128/196]	Time 0.498 (0.704)	Data 0.000 (0.003)	Loss 0.4554 (0.4328)	Acc@1 86.719 (85.147)	Acc@5 98.828 (99.337)
Epoch: [76][192/196]	Time 0.717 (0.707)	Data 0.000 (0.002)	Loss 0.4195 (0.4335)	Acc@1 83.594 (85.114)	Acc@5 99.609 (99.330)
after train
Epoche: [77/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [77][0/196]	Time 0.777 (0.777)	Data 0.435 (0.435)	Loss 0.4617 (0.4617)	Acc@1 85.938 (85.938)	Acc@5 98.828 (98.828)
Epoch: [77][64/196]	Time 0.709 (0.708)	Data 0.000 (0.007)	Loss 0.4343 (0.4345)	Acc@1 86.328 (84.988)	Acc@5 99.219 (99.315)
Epoch: [77][128/196]	Time 0.711 (0.706)	Data 0.000 (0.004)	Loss 0.5282 (0.4345)	Acc@1 82.812 (85.056)	Acc@5 98.828 (99.346)
Epoch: [77][192/196]	Time 0.716 (0.711)	Data 0.000 (0.003)	Loss 0.3783 (0.4334)	Acc@1 88.281 (85.098)	Acc@5 99.219 (99.362)
after train
Epoche: [78/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [78][0/196]	Time 0.742 (0.742)	Data 0.424 (0.424)	Loss 0.3599 (0.3599)	Acc@1 89.062 (89.062)	Acc@5 98.438 (98.438)
Epoch: [78][64/196]	Time 0.713 (0.717)	Data 0.000 (0.007)	Loss 0.5118 (0.4156)	Acc@1 83.594 (85.781)	Acc@5 99.219 (99.465)
Epoch: [78][128/196]	Time 0.662 (0.715)	Data 0.000 (0.004)	Loss 0.4627 (0.4287)	Acc@1 85.547 (85.253)	Acc@5 99.219 (99.403)
Epoch: [78][192/196]	Time 0.730 (0.716)	Data 0.000 (0.003)	Loss 0.3888 (0.4274)	Acc@1 87.500 (85.349)	Acc@5 99.609 (99.415)
after train
Epoche: [79/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [79][0/196]	Time 0.816 (0.816)	Data 0.356 (0.356)	Loss 0.4053 (0.4053)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [79][64/196]	Time 0.711 (0.713)	Data 0.000 (0.006)	Loss 0.4752 (0.4304)	Acc@1 82.812 (85.288)	Acc@5 100.000 (99.309)
Epoch: [79][128/196]	Time 0.719 (0.709)	Data 0.000 (0.003)	Loss 0.3720 (0.4310)	Acc@1 88.672 (85.286)	Acc@5 100.000 (99.382)
Epoch: [79][192/196]	Time 0.714 (0.710)	Data 0.000 (0.002)	Loss 0.3510 (0.4335)	Acc@1 89.062 (85.176)	Acc@5 99.609 (99.417)
after train
Epoche: [80/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [80][0/196]	Time 0.881 (0.881)	Data 0.384 (0.384)	Loss 0.4704 (0.4704)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [80][64/196]	Time 0.712 (0.717)	Data 0.000 (0.006)	Loss 0.3560 (0.4097)	Acc@1 89.062 (85.944)	Acc@5 100.000 (99.483)
Epoch: [80][128/196]	Time 0.722 (0.709)	Data 0.000 (0.003)	Loss 0.4432 (0.4259)	Acc@1 83.594 (85.353)	Acc@5 99.609 (99.400)
Epoch: [80][192/196]	Time 0.707 (0.709)	Data 0.000 (0.002)	Loss 0.4309 (0.4324)	Acc@1 82.812 (85.185)	Acc@5 98.828 (99.397)
after train
Epoche: [81/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [81][0/196]	Time 0.775 (0.775)	Data 0.345 (0.345)	Loss 0.4454 (0.4454)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [81][64/196]	Time 0.726 (0.715)	Data 0.000 (0.006)	Loss 0.3944 (0.4026)	Acc@1 89.062 (86.130)	Acc@5 99.609 (99.507)
Epoch: [81][128/196]	Time 0.613 (0.710)	Data 0.000 (0.003)	Loss 0.4404 (0.4166)	Acc@1 85.156 (85.647)	Acc@5 98.828 (99.422)
Epoch: [81][192/196]	Time 0.762 (0.715)	Data 0.000 (0.002)	Loss 0.4027 (0.4241)	Acc@1 83.203 (85.363)	Acc@5 99.609 (99.366)
after train
Epoche: [82/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [82][0/196]	Time 0.824 (0.824)	Data 0.407 (0.407)	Loss 0.3906 (0.3906)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [82][64/196]	Time 0.732 (0.718)	Data 0.000 (0.007)	Loss 0.4153 (0.4254)	Acc@1 85.547 (85.391)	Acc@5 100.000 (99.309)
Epoch: [82][128/196]	Time 0.745 (0.712)	Data 0.000 (0.003)	Loss 0.4631 (0.4291)	Acc@1 85.156 (85.311)	Acc@5 98.828 (99.346)
Epoch: [82][192/196]	Time 0.729 (0.714)	Data 0.000 (0.002)	Loss 0.4712 (0.4360)	Acc@1 83.203 (85.065)	Acc@5 99.609 (99.369)
after train
Epoche: [83/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [83][0/196]	Time 0.723 (0.723)	Data 0.328 (0.328)	Loss 0.3935 (0.3935)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [83][64/196]	Time 0.735 (0.712)	Data 0.000 (0.006)	Loss 0.3302 (0.4237)	Acc@1 89.844 (85.415)	Acc@5 99.609 (99.297)
Epoch: [83][128/196]	Time 0.685 (0.713)	Data 0.000 (0.003)	Loss 0.4904 (0.4240)	Acc@1 83.594 (85.529)	Acc@5 99.609 (99.301)
Epoch: [83][192/196]	Time 0.715 (0.715)	Data 0.000 (0.002)	Loss 0.3984 (0.4313)	Acc@1 89.453 (85.290)	Acc@5 99.609 (99.300)
after train
Epoche: [84/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [84][0/196]	Time 0.765 (0.765)	Data 0.381 (0.381)	Loss 0.4221 (0.4221)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [84][64/196]	Time 0.708 (0.699)	Data 0.000 (0.006)	Loss 0.4939 (0.4173)	Acc@1 82.812 (85.769)	Acc@5 99.609 (99.483)
Epoch: [84][128/196]	Time 0.705 (0.699)	Data 0.000 (0.003)	Loss 0.4857 (0.4261)	Acc@1 83.203 (85.156)	Acc@5 99.219 (99.473)
Epoch: [84][192/196]	Time 0.696 (0.705)	Data 0.000 (0.002)	Loss 0.5718 (0.4277)	Acc@1 81.641 (85.185)	Acc@5 99.219 (99.431)
after train
Epoche: [85/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [85][0/196]	Time 0.653 (0.653)	Data 0.320 (0.320)	Loss 0.4207 (0.4207)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [85][64/196]	Time 0.675 (0.704)	Data 0.000 (0.005)	Loss 0.4377 (0.4357)	Acc@1 84.375 (85.018)	Acc@5 98.438 (99.297)
Epoch: [85][128/196]	Time 0.706 (0.702)	Data 0.000 (0.003)	Loss 0.4806 (0.4279)	Acc@1 87.500 (85.347)	Acc@5 97.266 (99.346)
Epoch: [85][192/196]	Time 0.700 (0.708)	Data 0.000 (0.002)	Loss 0.4264 (0.4269)	Acc@1 83.203 (85.373)	Acc@5 98.828 (99.377)
after train
Epoche: [86/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [86][0/196]	Time 0.719 (0.719)	Data 0.427 (0.427)	Loss 0.3330 (0.3330)	Acc@1 88.672 (88.672)	Acc@5 99.609 (99.609)
Epoch: [86][64/196]	Time 0.712 (0.718)	Data 0.000 (0.007)	Loss 0.4152 (0.4242)	Acc@1 82.031 (85.349)	Acc@5 99.609 (99.381)
Epoch: [86][128/196]	Time 0.670 (0.708)	Data 0.000 (0.004)	Loss 0.3939 (0.4270)	Acc@1 84.375 (85.211)	Acc@5 99.609 (99.403)
Epoch: [86][192/196]	Time 0.717 (0.712)	Data 0.000 (0.003)	Loss 0.4562 (0.4266)	Acc@1 82.812 (85.132)	Acc@5 99.219 (99.415)
after train
Epoche: [87/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [87][0/196]	Time 0.879 (0.879)	Data 0.405 (0.405)	Loss 0.2931 (0.2931)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [87][64/196]	Time 0.688 (0.715)	Data 0.000 (0.007)	Loss 0.4812 (0.4357)	Acc@1 82.812 (84.934)	Acc@5 99.609 (99.363)
Epoch: [87][128/196]	Time 0.578 (0.642)	Data 0.000 (0.003)	Loss 0.4671 (0.4273)	Acc@1 84.766 (85.426)	Acc@5 99.219 (99.379)
Epoch: [87][192/196]	Time 0.603 (0.619)	Data 0.000 (0.002)	Loss 0.4272 (0.4280)	Acc@1 86.328 (85.340)	Acc@5 100.000 (99.369)
after train
Epoche: [88/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [88][0/196]	Time 0.614 (0.614)	Data 0.358 (0.358)	Loss 0.4405 (0.4405)	Acc@1 82.031 (82.031)	Acc@5 100.000 (100.000)
Epoch: [88][64/196]	Time 0.566 (0.564)	Data 0.000 (0.006)	Loss 0.3811 (0.4220)	Acc@1 87.109 (85.493)	Acc@5 98.828 (99.495)
Epoch: [88][128/196]	Time 0.572 (0.560)	Data 0.000 (0.003)	Loss 0.3886 (0.4213)	Acc@1 83.594 (85.532)	Acc@5 100.000 (99.425)
Epoch: [88][192/196]	Time 0.563 (0.565)	Data 0.000 (0.002)	Loss 0.5018 (0.4255)	Acc@1 80.859 (85.320)	Acc@5 100.000 (99.413)
after train
Epoche: [89/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [89][0/196]	Time 0.754 (0.754)	Data 0.378 (0.378)	Loss 0.4204 (0.4204)	Acc@1 86.328 (86.328)	Acc@5 99.219 (99.219)
Epoch: [89][64/196]	Time 0.562 (0.566)	Data 0.000 (0.006)	Loss 0.4462 (0.4235)	Acc@1 84.375 (85.433)	Acc@5 99.609 (99.375)
Epoch: [89][128/196]	Time 0.435 (0.561)	Data 0.000 (0.003)	Loss 0.4127 (0.4310)	Acc@1 87.891 (85.102)	Acc@5 98.438 (99.388)
Epoch: [89][192/196]	Time 0.422 (0.518)	Data 0.000 (0.002)	Loss 0.3425 (0.4313)	Acc@1 88.672 (84.994)	Acc@5 99.219 (99.373)
after train
Epoche: [90/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [90][0/196]	Time 0.448 (0.448)	Data 0.344 (0.344)	Loss 0.4274 (0.4274)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [90][64/196]	Time 0.424 (0.412)	Data 0.000 (0.006)	Loss 0.4560 (0.4154)	Acc@1 83.594 (85.427)	Acc@5 99.219 (99.453)
Epoch: [90][128/196]	Time 0.428 (0.409)	Data 0.000 (0.003)	Loss 0.4551 (0.4179)	Acc@1 84.766 (85.489)	Acc@5 99.219 (99.358)
Epoch: [90][192/196]	Time 0.427 (0.416)	Data 0.000 (0.002)	Loss 0.4002 (0.4202)	Acc@1 86.719 (85.551)	Acc@5 100.000 (99.352)
after train
Traceback (most recent call last):
  File "main.py", line 884, in <module>
    main()
  File "main.py", line 453, in main
    model.wider(1.5, weight_norm=None, random_init=False, addNoise=True)
TypeError: wider() missing 1 required positional argument: 'delta_width'
