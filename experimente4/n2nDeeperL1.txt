Net2Net Deeper 1
j: 1 bis 5
no display found. Using non-interactive Agg backend
[5, 5, 5]
[16, 32, 64]
random number: 7928
Files already downloaded and verified
device count: 1
Startepoche: 1
Max memory: 0.101888
1
Epoche: [1/5]; Lr: 0.1
batch Size 256
Epoch: [1][0/196]	Time 1.679 (1.679)	Data 1.306 (1.306)	Loss 3.0637 (3.0637)	Acc@1 13.672 (13.672)	Acc@5 47.266 (47.266)
Epoch: [1][64/196]	Time 1.318 (1.145)	Data 0.000 (0.026)	Loss 2.3516 (2.9191)	Acc@1 18.359 (12.188)	Acc@5 60.156 (52.175)
Epoch: [1][128/196]	Time 1.539 (1.246)	Data 0.003 (0.017)	Loss 2.0445 (2.5419)	Acc@1 25.781 (15.722)	Acc@5 73.828 (61.025)
Epoch: [1][192/196]	Time 1.735 (1.336)	Data 0.000 (0.014)	Loss 1.8790 (2.3426)	Acc@1 25.391 (18.392)	Acc@5 87.500 (68.052)
1
Epoche: [2/5]; Lr: 0.1
batch Size 256
Epoch: [2][0/196]	Time 1.989 (1.989)	Data 0.958 (0.958)	Loss 1.8832 (1.8832)	Acc@1 25.391 (25.391)	Acc@5 82.812 (82.812)
Epoch: [2][64/196]	Time 1.654 (1.742)	Data 0.024 (0.020)	Loss 1.7558 (1.7991)	Acc@1 31.250 (29.958)	Acc@5 87.500 (85.379)
Epoch: [2][128/196]	Time 2.297 (1.835)	Data 0.013 (0.014)	Loss 1.5966 (1.7339)	Acc@1 40.234 (33.146)	Acc@5 89.062 (87.230)
Epoch: [2][192/196]	Time 2.222 (1.865)	Data 0.000 (0.011)	Loss 1.4195 (1.6679)	Acc@1 45.703 (36.318)	Acc@5 93.750 (88.469)
1
Epoche: [3/5]; Lr: 0.1
batch Size 256
Epoch: [3][0/196]	Time 2.440 (2.440)	Data 1.210 (1.210)	Loss 1.4482 (1.4482)	Acc@1 48.047 (48.047)	Acc@5 90.625 (90.625)
Epoch: [3][64/196]	Time 2.622 (2.432)	Data 0.000 (0.023)	Loss 1.2327 (1.3935)	Acc@1 54.688 (48.558)	Acc@5 96.875 (92.987)
Epoch: [3][128/196]	Time 2.762 (2.500)	Data 0.005 (0.015)	Loss 1.2477 (1.3519)	Acc@1 58.984 (50.790)	Acc@5 94.922 (93.290)
Epoch: [3][192/196]	Time 3.063 (2.604)	Data 0.000 (0.012)	Loss 1.1117 (1.2978)	Acc@1 59.375 (52.874)	Acc@5 96.484 (93.983)
1
Epoche: [4/5]; Lr: 0.1
batch Size 256
Epoch: [4][0/196]	Time 3.368 (3.368)	Data 1.183 (1.183)	Loss 1.1939 (1.1939)	Acc@1 57.031 (57.031)	Acc@5 94.922 (94.922)
Epoch: [4][64/196]	Time 2.017 (2.818)	Data 0.005 (0.022)	Loss 1.2465 (1.1745)	Acc@1 50.781 (58.233)	Acc@5 97.656 (95.090)
Epoch: [4][128/196]	Time 1.540 (2.416)	Data 0.000 (0.012)	Loss 1.1248 (1.1427)	Acc@1 62.891 (59.145)	Acc@5 94.922 (95.449)
Epoch: [4][192/196]	Time 1.856 (2.324)	Data 0.000 (0.009)	Loss 1.1027 (1.1281)	Acc@1 59.766 (59.660)	Acc@5 94.531 (95.549)
1
Epoche: [5/5]; Lr: 0.1
batch Size 256
Epoch: [5][0/196]	Time 2.223 (2.223)	Data 1.034 (1.034)	Loss 1.1370 (1.1370)	Acc@1 59.375 (59.375)	Acc@5 94.531 (94.531)
Epoch: [5][64/196]	Time 2.422 (3.130)	Data 0.000 (0.021)	Loss 1.0710 (1.0864)	Acc@1 62.109 (60.889)	Acc@5 95.312 (95.865)
Epoch: [5][128/196]	Time 4.200 (3.539)	Data 0.026 (0.013)	Loss 0.9870 (1.0559)	Acc@1 65.234 (62.300)	Acc@5 96.094 (96.227)
Epoch: [5][192/196]	Time 4.071 (3.760)	Data 0.000 (0.010)	Loss 1.1890 (1.0438)	Acc@1 56.250 (62.868)	Acc@5 94.141 (96.306)
Test acc1:  26.29
[INFO] Storing checkpoint...
Max memory: 81.321984
 741.356s  j: 6 bis 10
no display found. Using non-interactive Agg backend
[5, 5, 5]
[16, 32, 64]
random number: 9931
Files already downloaded and verified
Model: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01, inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): LeakyReLU(negative_slope=0.01, inplace=True)
)
==> Resuming from checkpoint..
Start epoch: 6
First Lr: 0.1
Startepoche: 6
Max memory: 0.3001344
1
Epoche: [6/10]; Lr: 0.1
batch Size 256
Epoch: [6][0/196]	Time 1.631 (1.631)	Data 1.225 (1.225)	Loss 1.0286 (1.0286)	Acc@1 62.891 (62.891)	Acc@5 97.266 (97.266)
Epoch: [6][64/196]	Time 1.389 (1.315)	Data 0.005 (0.024)	Loss 1.1188 (1.0345)	Acc@1 60.547 (63.936)	Acc@5 96.875 (96.424)
Epoch: [6][128/196]	Time 1.658 (1.521)	Data 0.000 (0.015)	Loss 1.0226 (1.0324)	Acc@1 59.766 (63.978)	Acc@5 98.047 (96.460)
Epoch: [6][192/196]	Time 2.352 (1.721)	Data 0.000 (0.012)	Loss 0.9314 (1.0327)	Acc@1 68.359 (63.996)	Acc@5 98.047 (96.345)
1
Epoche: [7/10]; Lr: 0.1
batch Size 256
Epoch: [7][0/196]	Time 2.094 (2.094)	Data 1.265 (1.265)	Loss 1.0278 (1.0278)	Acc@1 65.625 (65.625)	Acc@5 96.875 (96.875)
Epoch: [7][64/196]	Time 3.087 (2.554)	Data 0.005 (0.026)	Loss 1.0474 (1.0322)	Acc@1 61.719 (63.960)	Acc@5 96.875 (96.262)
Epoch: [7][128/196]	Time 1.838 (2.773)	Data 0.015 (0.016)	Loss 1.0492 (1.0387)	Acc@1 64.453 (63.717)	Acc@5 95.703 (96.327)
Epoch: [7][192/196]	Time 3.816 (2.995)	Data 0.024 (0.013)	Loss 0.9569 (1.0313)	Acc@1 67.969 (64.071)	Acc@5 97.656 (96.393)
1
Epoche: [8/10]; Lr: 0.1
batch Size 256
Epoch: [8][0/196]	Time 4.044 (4.044)	Data 1.301 (1.301)	Loss 1.0732 (1.0732)	Acc@1 63.281 (63.281)	Acc@5 94.922 (94.922)
Epoch: [8][64/196]	Time 4.252 (3.874)	Data 0.000 (0.024)	Loss 0.9726 (1.0281)	Acc@1 67.578 (64.069)	Acc@5 95.312 (96.196)
Epoch: [8][128/196]	Time 4.831 (4.096)	Data 0.000 (0.015)	Loss 0.9762 (1.0274)	Acc@1 66.797 (64.271)	Acc@5 96.875 (96.387)
Epoch: [8][192/196]	Time 4.864 (4.328)	Data 0.000 (0.012)	Loss 1.1549 (1.0293)	Acc@1 60.547 (64.097)	Acc@5 94.531 (96.399)
1
Epoche: [9/10]; Lr: 0.1
batch Size 256
Epoch: [9][0/196]	Time 4.000 (4.000)	Data 1.214 (1.214)	Loss 1.1099 (1.1099)	Acc@1 61.328 (61.328)	Acc@5 94.922 (94.922)
Epoch: [9][64/196]	Time 3.492 (3.196)	Data 0.000 (0.021)	Loss 0.9710 (1.0275)	Acc@1 64.453 (64.207)	Acc@5 97.656 (96.352)
Epoch: [9][128/196]	Time 4.897 (3.329)	Data 0.021 (0.011)	Loss 0.9460 (1.0277)	Acc@1 67.578 (64.277)	Acc@5 97.656 (96.351)
Epoch: [9][192/196]	Time 6.546 (4.207)	Data 0.000 (0.009)	Loss 1.0559 (1.0339)	Acc@1 62.109 (64.002)	Acc@5 97.266 (96.331)
1
Epoche: [10/10]; Lr: 0.1
batch Size 256
Epoch: [10][0/196]	Time 6.647 (6.647)	Data 1.312 (1.312)	Loss 1.0138 (1.0138)	Acc@1 65.234 (65.234)	Acc@5 96.094 (96.094)
Epoch: [10][64/196]	Time 7.162 (6.633)	Data 0.011 (0.026)	Loss 1.0769 (1.0298)	Acc@1 61.328 (64.549)	Acc@5 94.531 (96.334)
Epoch: [10][128/196]	Time 7.492 (6.783)	Data 0.010 (0.016)	Loss 0.9764 (1.0374)	Acc@1 66.406 (64.096)	Acc@5 98.828 (96.242)
Epoch: [10][192/196]	Time 7.842 (6.936)	Data 0.000 (0.012)	Loss 1.1315 (1.0339)	Acc@1 60.547 (64.192)	Acc@5 96.484 (96.294)
Test acc1:  64.01
[INFO] Storing checkpoint...
Max memory: 81.4211072
 1365.103s  j: 11 bis 15
no display found. Using non-interactive Agg backend
[5, 5, 5]
[16, 32, 64]
random number: 9694
Files already downloaded and verified
Model: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01, inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): LeakyReLU(negative_slope=0.01, inplace=True)
)
==> Resuming from checkpoint..
Start epoch: 11
First Lr: 0.1
Startepoche: 11
Max memory: 0.3001344
1
Epoche: [11/15]; Lr: 0.1
batch Size 256
Epoch: [11][0/196]	Time 1.695 (1.695)	Data 1.117 (1.117)	Loss 0.9742 (0.9742)	Acc@1 69.141 (69.141)	Acc@5 98.047 (98.047)
Epoch: [11][64/196]	Time 1.128 (1.248)	Data 0.005 (0.022)	Loss 1.1704 (1.0393)	Acc@1 57.422 (63.930)	Acc@5 96.094 (96.352)
Epoch: [11][128/196]	Time 2.056 (1.462)	Data 0.015 (0.015)	Loss 0.9698 (1.0354)	Acc@1 65.625 (64.153)	Acc@5 96.875 (96.360)
Epoch: [11][192/196]	Time 2.302 (1.697)	Data 0.000 (0.012)	Loss 1.0486 (1.0327)	Acc@1 63.672 (64.067)	Acc@5 96.484 (96.432)
1
Epoche: [12/15]; Lr: 0.1
batch Size 256
Epoch: [12][0/196]	Time 2.187 (2.187)	Data 1.497 (1.497)	Loss 0.8995 (0.8995)	Acc@1 67.578 (67.578)	Acc@5 98.828 (98.828)
Epoch: [12][64/196]	Time 2.792 (2.514)	Data 0.005 (0.029)	Loss 0.9727 (1.0407)	Acc@1 67.188 (63.762)	Acc@5 97.656 (96.508)
Epoch: [12][128/196]	Time 1.852 (2.500)	Data 0.000 (0.017)	Loss 1.0051 (1.0314)	Acc@1 65.625 (64.196)	Acc@5 96.484 (96.400)
Epoch: [12][192/196]	Time 2.276 (2.379)	Data 0.000 (0.013)	Loss 0.9521 (1.0358)	Acc@1 67.969 (63.955)	Acc@5 96.875 (96.387)
1
Epoche: [13/15]; Lr: 0.1
batch Size 256
Epoch: [13][0/196]	Time 2.714 (2.714)	Data 0.938 (0.938)	Loss 1.0132 (1.0132)	Acc@1 62.500 (62.500)	Acc@5 97.266 (97.266)
Epoch: [13][64/196]	Time 3.063 (2.506)	Data 0.000 (0.017)	Loss 0.9112 (1.0317)	Acc@1 65.625 (64.038)	Acc@5 97.656 (96.502)
Epoch: [13][128/196]	Time 4.735 (3.378)	Data 0.015 (0.012)	Loss 1.0622 (1.0292)	Acc@1 66.016 (64.011)	Acc@5 95.312 (96.460)
Epoch: [13][192/196]	Time 4.121 (3.846)	Data 0.000 (0.010)	Loss 1.0432 (1.0326)	Acc@1 65.625 (63.996)	Acc@5 95.312 (96.331)
1
Epoche: [14/15]; Lr: 0.1
batch Size 256
Epoch: [14][0/196]	Time 5.230 (5.230)	Data 1.321 (1.321)	Loss 0.9218 (0.9218)	Acc@1 69.141 (69.141)	Acc@5 98.828 (98.828)
Epoch: [14][64/196]	Time 5.795 (5.232)	Data 0.005 (0.027)	Loss 1.0136 (1.0389)	Acc@1 63.281 (63.594)	Acc@5 95.703 (96.310)
Epoch: [14][128/196]	Time 6.146 (5.433)	Data 0.000 (0.018)	Loss 1.0043 (1.0345)	Acc@1 66.406 (63.972)	Acc@5 96.484 (96.263)
Epoch: [14][192/196]	Time 6.506 (5.673)	Data 0.000 (0.013)	Loss 1.0579 (1.0332)	Acc@1 64.453 (64.097)	Acc@5 95.312 (96.318)
1
Epoche: [15/15]; Lr: 0.1
batch Size 256
Epoch: [15][0/196]	Time 6.266 (6.266)	Data 1.322 (1.322)	Loss 1.0617 (1.0617)	Acc@1 62.891 (62.891)	Acc@5 97.656 (97.656)
Epoch: [15][64/196]	Time 5.752 (6.510)	Data 0.026 (0.026)	Loss 1.0768 (1.0343)	Acc@1 58.203 (64.117)	Acc@5 95.703 (96.382)
Epoch: [15][128/196]	Time 3.975 (6.542)	Data 0.010 (0.016)	Loss 0.9437 (1.0385)	Acc@1 67.969 (63.944)	Acc@5 96.094 (96.251)
Epoch: [15][192/196]	Time 5.693 (5.857)	Data 0.000 (0.011)	Loss 1.0538 (1.0343)	Acc@1 62.500 (64.156)	Acc@5 94.922 (96.343)
Test acc1:  63.97
[INFO] Storing checkpoint...
Max memory: 81.4211072
 1150.803s  j: 16 bis 20
no display found. Using non-interactive Agg backend
[5, 5, 5]
[16, 32, 64]
random number: 9755
Files already downloaded and verified
Model: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01, inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): LeakyReLU(negative_slope=0.01, inplace=True)
)
==> Resuming from checkpoint..
Start epoch: 16
First Lr: 0.1
Startepoche: 16
Max memory: 0.3001344
1
Epoche: [16/20]; Lr: 0.1
batch Size 256
Epoch: [16][0/196]	Time 1.160 (1.160)	Data 0.923 (0.923)	Loss 0.9811 (0.9811)	Acc@1 67.578 (67.578)	Acc@5 96.094 (96.094)
Epoch: [16][64/196]	Time 1.622 (1.230)	Data 0.045 (0.021)	Loss 1.0624 (1.0285)	Acc@1 65.625 (64.465)	Acc@5 96.094 (96.220)
Epoch: [16][128/196]	Time 1.660 (1.407)	Data 0.000 (0.014)	Loss 1.0916 (1.0316)	Acc@1 59.766 (64.232)	Acc@5 94.922 (96.336)
Epoch: [16][192/196]	Time 2.224 (1.643)	Data 0.000 (0.011)	Loss 1.0360 (1.0339)	Acc@1 67.969 (64.162)	Acc@5 94.922 (96.318)
1
Epoche: [17/20]; Lr: 0.1
batch Size 256
Epoch: [17][0/196]	Time 2.357 (2.357)	Data 0.877 (0.877)	Loss 1.0148 (1.0148)	Acc@1 63.672 (63.672)	Acc@5 94.922 (94.922)
Epoch: [17][64/196]	Time 2.714 (2.559)	Data 0.005 (0.021)	Loss 1.0785 (1.0215)	Acc@1 60.156 (64.303)	Acc@5 96.094 (96.508)
Epoch: [17][128/196]	Time 3.293 (2.800)	Data 0.012 (0.014)	Loss 1.0145 (1.0294)	Acc@1 65.625 (64.123)	Acc@5 95.312 (96.297)
Epoch: [17][192/196]	Time 3.958 (3.018)	Data 0.000 (0.011)	Loss 1.0481 (1.0339)	Acc@1 62.500 (64.024)	Acc@5 97.656 (96.331)
1
Epoche: [18/20]; Lr: 0.1
batch Size 256
Epoch: [18][0/196]	Time 4.016 (4.016)	Data 1.022 (1.022)	Loss 1.1166 (1.1166)	Acc@1 60.156 (60.156)	Acc@5 94.531 (94.531)
Epoch: [18][64/196]	Time 4.226 (3.959)	Data 0.010 (0.021)	Loss 1.0246 (1.0396)	Acc@1 61.719 (63.840)	Acc@5 98.438 (96.220)
Epoch: [18][128/196]	Time 4.637 (4.184)	Data 0.005 (0.013)	Loss 1.0634 (1.0362)	Acc@1 66.406 (63.832)	Acc@5 94.141 (96.327)
Epoch: [18][192/196]	Time 4.547 (4.352)	Data 0.000 (0.010)	Loss 0.9993 (1.0337)	Acc@1 64.062 (63.941)	Acc@5 97.266 (96.434)
1
Epoche: [19/20]; Lr: 0.1
batch Size 256
Epoch: [19][0/196]	Time 5.228 (5.228)	Data 1.314 (1.314)	Loss 0.9927 (0.9927)	Acc@1 64.453 (64.453)	Acc@5 95.703 (95.703)
Epoch: [19][64/196]	Time 5.118 (5.194)	Data 0.000 (0.026)	Loss 1.0701 (1.0291)	Acc@1 59.766 (64.153)	Acc@5 96.094 (96.400)
Epoch: [19][128/196]	Time 3.880 (5.268)	Data 0.005 (0.016)	Loss 1.0807 (1.0296)	Acc@1 58.984 (63.975)	Acc@5 94.922 (96.412)
Epoch: [19][192/196]	Time 3.178 (4.719)	Data 0.000 (0.011)	Loss 1.0310 (1.0336)	Acc@1 64.844 (63.927)	Acc@5 95.703 (96.409)
1
Epoche: [20/20]; Lr: 0.1
batch Size 256
Epoch: [20][0/196]	Time 4.272 (4.272)	Data 0.987 (0.987)	Loss 1.0735 (1.0735)	Acc@1 61.328 (61.328)	Acc@5 95.312 (95.312)
Epoch: [20][64/196]	Time 7.436 (5.730)	Data 0.005 (0.023)	Loss 0.9908 (1.0368)	Acc@1 66.406 (63.786)	Acc@5 97.266 (96.274)
Epoch: [20][128/196]	Time 7.552 (6.339)	Data 0.005 (0.014)	Loss 1.0158 (1.0299)	Acc@1 64.453 (64.383)	Acc@5 96.875 (96.281)
Epoch: [20][192/196]	Time 7.101 (6.724)	Data 0.000 (0.011)	Loss 1.0785 (1.0341)	Acc@1 62.109 (64.113)	Acc@5 94.922 (96.290)
Test acc1:  63.99
[INFO] Storing checkpoint...
Max memory: 81.4211072
 1323.949s  j: 21 bis 25
no display found. Using non-interactive Agg backend
[5, 5, 5]
[16, 32, 64]
random number: 1070
Files already downloaded and verified
Model: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.01, inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): LeakyReLU(negative_slope=0.01, inplace=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): LeakyReLU(negative_slope=0.01, inplace=True)
)
==> Resuming from checkpoint..
Start epoch: 21
First Lr: 0.1
Startepoche: 21
Max memory: 0.3001344
1
Epoche: [21/25]; Lr: 0.1
batch Size 256
Epoch: [21][0/196]	Time 1.594 (1.594)	Data 1.323 (1.323)	Loss 1.0716 (1.0716)	Acc@1 60.938 (60.938)	Acc@5 96.484 (96.484)
Epoch: [21][64/196]	Time 1.711 (1.340)	Data 0.021 (0.027)	Loss 1.0047 (1.0256)	Acc@1 64.844 (64.639)	Acc@5 96.094 (96.496)
Epoch: [21][128/196]	Time 1.901 (1.526)	Data 0.000 (0.018)	Loss 0.9878 (1.0314)	Acc@1 60.547 (64.072)	Acc@5 96.875 (96.451)
Epoch: [21][192/196]	Time 2.285 (1.729)	Data 0.000 (0.014)	Loss 0.9424 (1.0307)	Acc@1 65.625 (64.107)	Acc@5 99.609 (96.448)
1
Epoche: [22/25]; Lr: 0.1
batch Size 256
Epoch: [22][0/196]	Time 2.651 (2.651)	Data 1.257 (1.257)	Loss 0.9322 (0.9322)	Acc@1 69.531 (69.531)	Acc@5 96.875 (96.875)
Epoch: [22][64/196]	Time 2.676 (2.650)	Data 0.000 (0.025)	Loss 1.1888 (1.0173)	Acc@1 57.031 (64.591)	Acc@5 94.922 (96.466)
Epoch: [22][128/196]	Time 3.618 (2.807)	Data 0.000 (0.016)	Loss 0.9807 (1.0307)	Acc@1 66.797 (63.914)	Acc@5 95.703 (96.363)
Epoch: [22][192/196]	Time 3.992 (3.029)	Data 0.000 (0.012)	Loss 1.0705 (1.0316)	Acc@1 60.156 (64.091)	Acc@5 96.875 (96.274)
1
Epoche: [23/25]; Lr: 0.1
batch Size 256
Epoch: [23][0/196]	Time 3.989 (3.989)	Data 1.192 (1.192)	Loss 0.9219 (0.9219)	Acc@1 66.406 (66.406)	Acc@5 98.047 (98.047)
Epoch: [23][64/196]	Time 3.451 (3.881)	Data 0.000 (0.026)	Loss 1.0045 (1.0393)	Acc@1 60.547 (63.852)	Acc@5 96.875 (96.274)
Epoch: [23][128/196]	Time 3.076 (3.424)	Data 0.000 (0.014)	Loss 1.0446 (1.0341)	Acc@1 60.938 (64.168)	Acc@5 97.266 (96.357)
Epoch: [23][192/196]	Time 3.758 (3.228)	Data 0.000 (0.010)	Loss 1.0194 (1.0338)	Acc@1 66.406 (64.222)	Acc@5 98.047 (96.258)
1
Epoche: [24/25]; Lr: 0.1
batch Size 256
