Net2Net Deeper 1
j: 1 bis 5
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8121
Files already downloaded and verified
width: 8

Arch Num:  [[1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [2, 1, 1, 1, 1]]
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (23): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): AdaptiveAvgPool2d(output_size=(1, 1))
    (37): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
Max memory: 0.0292352
lr: 0.1
lr: 0.1
1
Epoche:1/5; Lr: 0.1
batch Size 256
Epoch: [1][0/196]	Time 0.074 (0.074)	Data 0.285 (0.285)	Loss 2.4551 (2.4551)	Acc@1 8.594 (8.594)	Acc@5 48.047 (48.047)
Epoch: [1][64/196]	Time 0.033 (0.036)	Data 0.000 (0.005)	Loss 1.7947 (1.9856)	Acc@1 32.422 (25.054)	Acc@5 85.938 (77.867)
Epoch: [1][128/196]	Time 0.040 (0.037)	Data 0.000 (0.002)	Loss 1.7117 (1.8720)	Acc@1 34.375 (28.737)	Acc@5 87.109 (82.401)
Epoch: [1][192/196]	Time 0.031 (0.037)	Data 0.000 (0.002)	Loss 1.6761 (1.8138)	Acc@1 37.500 (31.005)	Acc@5 87.891 (84.363)
Max memory in training epoch: 5.2850176
lr: 0.1
lr: 0.1
1
Epoche:2/5; Lr: 0.1
batch Size 256
Epoch: [2][0/196]	Time 0.037 (0.037)	Data 0.302 (0.302)	Loss 1.6711 (1.6711)	Acc@1 33.594 (33.594)	Acc@5 89.062 (89.062)
Epoch: [2][64/196]	Time 0.036 (0.036)	Data 0.000 (0.005)	Loss 1.5942 (1.6421)	Acc@1 39.062 (38.029)	Acc@5 89.453 (89.237)
Epoch: [2][128/196]	Time 0.036 (0.035)	Data 0.000 (0.003)	Loss 1.5632 (1.6250)	Acc@1 39.453 (39.314)	Acc@5 92.188 (89.453)
Epoch: [2][192/196]	Time 0.032 (0.035)	Data 0.000 (0.002)	Loss 1.5285 (1.5981)	Acc@1 43.359 (40.382)	Acc@5 91.797 (90.048)
Max memory in training epoch: 5.2850176
lr: 0.1
lr: 0.1
1
Epoche:3/5; Lr: 0.1
batch Size 256
Epoch: [3][0/196]	Time 0.047 (0.047)	Data 0.279 (0.279)	Loss 1.4687 (1.4687)	Acc@1 46.094 (46.094)	Acc@5 91.797 (91.797)
Epoch: [3][64/196]	Time 0.036 (0.036)	Data 0.000 (0.004)	Loss 1.5342 (1.5192)	Acc@1 44.922 (43.750)	Acc@5 90.625 (91.520)
Epoch: [3][128/196]	Time 0.035 (0.036)	Data 0.000 (0.002)	Loss 1.4326 (1.4929)	Acc@1 46.094 (44.734)	Acc@5 91.797 (91.888)
Epoch: [3][192/196]	Time 0.039 (0.036)	Data 0.000 (0.002)	Loss 1.3843 (1.4811)	Acc@1 49.609 (45.282)	Acc@5 94.922 (92.121)
Max memory in training epoch: 5.2850176
lr: 0.1
lr: 0.1
1
Epoche:4/5; Lr: 0.1
batch Size 256
Epoch: [4][0/196]	Time 0.056 (0.056)	Data 0.290 (0.290)	Loss 1.3372 (1.3372)	Acc@1 52.344 (52.344)	Acc@5 92.578 (92.578)
Epoch: [4][64/196]	Time 0.038 (0.036)	Data 0.000 (0.005)	Loss 1.5297 (1.4302)	Acc@1 40.625 (47.776)	Acc@5 91.016 (92.734)
Epoch: [4][128/196]	Time 0.031 (0.036)	Data 0.000 (0.002)	Loss 1.4077 (1.4134)	Acc@1 44.141 (48.604)	Acc@5 93.750 (92.969)
Epoch: [4][192/196]	Time 0.036 (0.036)	Data 0.000 (0.002)	Loss 1.2832 (1.4019)	Acc@1 54.297 (49.041)	Acc@5 94.531 (93.114)
Max memory in training epoch: 5.2850176
lr: 0.1
lr: 0.1
1
Epoche:5/5; Lr: 0.1
batch Size 256
Epoch: [5][0/196]	Time 0.046 (0.046)	Data 0.272 (0.272)	Loss 1.2709 (1.2709)	Acc@1 52.734 (52.734)	Acc@5 95.312 (95.312)
Epoch: [5][64/196]	Time 0.040 (0.037)	Data 0.000 (0.004)	Loss 1.3383 (1.3488)	Acc@1 53.906 (50.986)	Acc@5 91.016 (93.786)
Epoch: [5][128/196]	Time 0.047 (0.036)	Data 0.000 (0.002)	Loss 1.3541 (1.3597)	Acc@1 47.266 (50.621)	Acc@5 93.750 (93.680)
Epoch: [5][192/196]	Time 0.029 (0.036)	Data 0.000 (0.002)	Loss 1.3334 (1.3546)	Acc@1 50.000 (50.909)	Acc@5 97.266 (93.744)
Max memory in training epoch: 5.2850176


now deeper1


Stage:  0


	Block:  0
size:8, 8, 3, 3; j: 2
conv: 5
layerin This Block: 2
bn: 7
Länge der ModuleListe: 40
j: 8; i: 3


	Block:  1
size:8, 8, 3, 3; j: 10
conv: 13
layerin This Block: 2
bn: 15
Länge der ModuleListe: 42
j: 16; i: 3


	Block:  2
size:16, 8, 3, 3; j: 18
conv: 21
layerin This Block: 2
bn: 23
Länge der ModuleListe: 44
j: 24; i: 3


	Block:  3
size:16, 16, 3, 3; j: 26
conv: 29
layerin This Block: 2
bn: 31
Länge der ModuleListe: 46
j: 32; i: 3


	Block:  4
size:32, 16, 3, 3; j: 34
conv: 37
layerin This Block: 2
bn: 39
Länge der ModuleListe: 48
j: 40; i: 3


Stage:  1


	Block:  0
size:32, 32, 3, 3; j: 42
conv: 45
layerin This Block: 3
bn: 47
Länge der ModuleListe: 50
j: 48; i: 3


	Block:  1
Traceback (most recent call last):
  File "main.py", line 965, in <module>
    main()
  File "main.py", line 545, in main
    model = model.deeper()
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 1027, in deeper
    module = self.module_list[j]
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/container.py", line 147, in __getitem__
    return self._modules[self._get_abs_string_index(idx)]
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/container.py", line 137, in _get_abs_string_index
    raise IndexError('index {} is out of range'.format(idx))
IndexError: index 50 is out of range
j: 6 bis 10
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4900
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 521
Max memory: 0.0346624
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:521/525; Lr: 0.0010000000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'src.n2n.N2N' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.
  warnings.warn(msg, SourceChangeWarning)
Epoch: [521][0/196]	Time 0.087 (0.087)	Data 0.225 (0.225)	Loss 0.9152 (0.9152)	Acc@1 67.188 (67.188)	Acc@5 95.312 (95.312)
Epoch: [521][64/196]	Time 0.040 (0.039)	Data 0.000 (0.004)	Loss 0.9319 (0.8891)	Acc@1 66.797 (69.020)	Acc@5 98.828 (97.440)
Epoch: [521][128/196]	Time 0.031 (0.037)	Data 0.000 (0.002)	Loss 0.9190 (0.8973)	Acc@1 70.312 (69.023)	Acc@5 96.875 (97.423)
Epoch: [521][192/196]	Time 0.033 (0.037)	Data 0.000 (0.001)	Loss 0.7751 (0.8987)	Acc@1 73.828 (68.772)	Acc@5 98.438 (97.500)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:522/525; Lr: 0.0010000000000000002
batch Size 256
Epoch: [522][0/196]	Time 0.050 (0.050)	Data 0.247 (0.247)	Loss 0.8970 (0.8970)	Acc@1 68.359 (68.359)	Acc@5 97.656 (97.656)
Epoch: [522][64/196]	Time 0.033 (0.037)	Data 0.000 (0.004)	Loss 0.8717 (0.8918)	Acc@1 69.922 (69.081)	Acc@5 98.047 (97.614)
Epoch: [522][128/196]	Time 0.036 (0.037)	Data 0.000 (0.002)	Loss 0.9563 (0.8950)	Acc@1 67.188 (68.995)	Acc@5 98.047 (97.668)
Epoch: [522][192/196]	Time 0.031 (0.037)	Data 0.000 (0.001)	Loss 0.8103 (0.8965)	Acc@1 75.781 (69.007)	Acc@5 97.266 (97.626)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:523/525; Lr: 0.0010000000000000002
batch Size 256
Epoch: [523][0/196]	Time 0.057 (0.057)	Data 0.269 (0.269)	Loss 0.8500 (0.8500)	Acc@1 71.875 (71.875)	Acc@5 98.047 (98.047)
Epoch: [523][64/196]	Time 0.032 (0.037)	Data 0.000 (0.004)	Loss 0.8178 (0.9042)	Acc@1 69.922 (68.431)	Acc@5 98.828 (97.488)
Epoch: [523][128/196]	Time 0.039 (0.037)	Data 0.000 (0.002)	Loss 0.9054 (0.9008)	Acc@1 70.312 (68.423)	Acc@5 97.656 (97.562)
Epoch: [523][192/196]	Time 0.034 (0.037)	Data 0.000 (0.002)	Loss 0.8702 (0.9016)	Acc@1 70.312 (68.384)	Acc@5 96.875 (97.543)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:524/525; Lr: 0.0010000000000000002
batch Size 256
Epoch: [524][0/196]	Time 0.061 (0.061)	Data 0.248 (0.248)	Loss 0.9348 (0.9348)	Acc@1 67.188 (67.188)	Acc@5 98.047 (98.047)
Epoch: [524][64/196]	Time 0.030 (0.034)	Data 0.000 (0.004)	Loss 0.9258 (0.9020)	Acc@1 65.625 (68.870)	Acc@5 98.438 (97.620)
Epoch: [524][128/196]	Time 0.030 (0.035)	Data 0.000 (0.002)	Loss 1.0535 (0.9041)	Acc@1 67.578 (68.768)	Acc@5 96.484 (97.599)
Epoch: [524][192/196]	Time 0.031 (0.035)	Data 0.000 (0.001)	Loss 0.8852 (0.9001)	Acc@1 67.969 (68.845)	Acc@5 98.047 (97.630)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:525/525; Lr: 0.0010000000000000002
batch Size 256
Epoch: [525][0/196]	Time 0.067 (0.067)	Data 0.272 (0.272)	Loss 0.8815 (0.8815)	Acc@1 73.438 (73.438)	Acc@5 98.047 (98.047)
Epoch: [525][64/196]	Time 0.037 (0.038)	Data 0.000 (0.004)	Loss 0.9096 (0.9027)	Acc@1 68.750 (68.834)	Acc@5 97.656 (97.506)
Epoch: [525][128/196]	Time 0.037 (0.037)	Data 0.000 (0.002)	Loss 0.8108 (0.8985)	Acc@1 74.219 (68.980)	Acc@5 98.438 (97.526)
Epoch: [525][192/196]	Time 0.046 (0.037)	Data 0.000 (0.002)	Loss 0.8929 (0.8986)	Acc@1 71.484 (69.048)	Acc@5 97.656 (97.549)
Max memory in training epoch: 5.2904448
[INFO] Storing checkpoint...
  67.95
Max memory: 7.7508608
 7.511s  j: 11 bis 15
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4427
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 526
Max memory: 0.0346624
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:526/530; Lr: 0.0010000000000000002
batch Size 256
Epoch: [526][0/196]	Time 0.070 (0.070)	Data 0.226 (0.226)	Loss 0.9227 (0.9227)	Acc@1 67.969 (67.969)	Acc@5 95.703 (95.703)
Epoch: [526][64/196]	Time 0.046 (0.038)	Data 0.000 (0.004)	Loss 0.8576 (0.8915)	Acc@1 71.875 (69.339)	Acc@5 98.828 (97.704)
Epoch: [526][128/196]	Time 0.031 (0.037)	Data 0.000 (0.002)	Loss 0.8786 (0.8956)	Acc@1 69.141 (68.889)	Acc@5 97.266 (97.611)
Epoch: [526][192/196]	Time 0.030 (0.036)	Data 0.000 (0.001)	Loss 0.9477 (0.8993)	Acc@1 67.969 (68.712)	Acc@5 96.484 (97.620)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:527/530; Lr: 0.0010000000000000002
batch Size 256
Epoch: [527][0/196]	Time 0.041 (0.041)	Data 0.263 (0.263)	Loss 0.8460 (0.8460)	Acc@1 68.750 (68.750)	Acc@5 98.828 (98.828)
Epoch: [527][64/196]	Time 0.030 (0.037)	Data 0.000 (0.004)	Loss 0.8943 (0.8996)	Acc@1 67.969 (68.528)	Acc@5 98.438 (97.668)
Epoch: [527][128/196]	Time 0.032 (0.037)	Data 0.000 (0.002)	Loss 0.9026 (0.9012)	Acc@1 67.578 (68.414)	Acc@5 97.266 (97.472)
Epoch: [527][192/196]	Time 0.055 (0.037)	Data 0.000 (0.002)	Loss 0.7929 (0.8997)	Acc@1 71.875 (68.663)	Acc@5 99.219 (97.527)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:528/530; Lr: 0.0010000000000000002
batch Size 256
Epoch: [528][0/196]	Time 0.044 (0.044)	Data 0.288 (0.288)	Loss 0.8913 (0.8913)	Acc@1 70.703 (70.703)	Acc@5 96.875 (96.875)
Epoch: [528][64/196]	Time 0.049 (0.036)	Data 0.000 (0.005)	Loss 0.8477 (0.8962)	Acc@1 71.094 (69.056)	Acc@5 96.484 (97.668)
Epoch: [528][128/196]	Time 0.038 (0.036)	Data 0.000 (0.002)	Loss 0.8650 (0.8959)	Acc@1 70.312 (69.253)	Acc@5 97.266 (97.599)
Epoch: [528][192/196]	Time 0.033 (0.036)	Data 0.000 (0.002)	Loss 0.8234 (0.8978)	Acc@1 73.438 (68.987)	Acc@5 98.828 (97.571)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:529/530; Lr: 0.0010000000000000002
batch Size 256
Epoch: [529][0/196]	Time 0.053 (0.053)	Data 0.322 (0.322)	Loss 0.9378 (0.9378)	Acc@1 66.016 (66.016)	Acc@5 98.438 (98.438)
Epoch: [529][64/196]	Time 0.043 (0.036)	Data 0.000 (0.005)	Loss 0.9628 (0.8991)	Acc@1 67.969 (68.840)	Acc@5 96.875 (97.644)
Epoch: [529][128/196]	Time 0.047 (0.036)	Data 0.000 (0.003)	Loss 0.8922 (0.8992)	Acc@1 68.750 (68.938)	Acc@5 96.875 (97.584)
Epoch: [529][192/196]	Time 0.037 (0.037)	Data 0.000 (0.002)	Loss 0.8971 (0.8977)	Acc@1 68.750 (68.977)	Acc@5 99.219 (97.561)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:530/530; Lr: 0.0010000000000000002
batch Size 256
Epoch: [530][0/196]	Time 0.053 (0.053)	Data 0.336 (0.336)	Loss 0.8837 (0.8837)	Acc@1 69.922 (69.922)	Acc@5 96.094 (96.094)
Epoch: [530][64/196]	Time 0.037 (0.037)	Data 0.000 (0.005)	Loss 0.8050 (0.9070)	Acc@1 69.922 (68.293)	Acc@5 97.656 (97.560)
Epoch: [530][128/196]	Time 0.031 (0.037)	Data 0.000 (0.003)	Loss 0.9689 (0.8977)	Acc@1 67.969 (68.856)	Acc@5 98.047 (97.578)
Epoch: [530][192/196]	Time 0.032 (0.037)	Data 0.000 (0.002)	Loss 1.0252 (0.8966)	Acc@1 66.016 (68.839)	Acc@5 96.484 (97.575)
Max memory in training epoch: 5.2904448
[INFO] Storing checkpoint...
  68.26
Max memory: 7.7508608
 7.611s  j: 16 bis 20
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1544
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 531
Max memory: 0.0346624
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:531/535; Lr: 0.0010000000000000002
batch Size 256
Epoch: [531][0/196]	Time 0.074 (0.074)	Data 0.244 (0.244)	Loss 0.9472 (0.9472)	Acc@1 67.188 (67.188)	Acc@5 96.094 (96.094)
Epoch: [531][64/196]	Time 0.036 (0.038)	Data 0.000 (0.004)	Loss 0.9316 (0.9087)	Acc@1 69.141 (68.113)	Acc@5 96.484 (97.638)
Epoch: [531][128/196]	Time 0.039 (0.037)	Data 0.000 (0.002)	Loss 0.8783 (0.9003)	Acc@1 70.312 (68.699)	Acc@5 97.656 (97.602)
Epoch: [531][192/196]	Time 0.033 (0.036)	Data 0.000 (0.002)	Loss 0.9224 (0.8998)	Acc@1 70.312 (68.730)	Acc@5 97.266 (97.589)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:532/535; Lr: 0.0010000000000000002
batch Size 256
Epoch: [532][0/196]	Time 0.054 (0.054)	Data 0.302 (0.302)	Loss 1.0136 (1.0136)	Acc@1 64.062 (64.062)	Acc@5 97.266 (97.266)
Epoch: [532][64/196]	Time 0.042 (0.037)	Data 0.000 (0.005)	Loss 0.7477 (0.9068)	Acc@1 73.047 (68.606)	Acc@5 98.828 (97.476)
Epoch: [532][128/196]	Time 0.037 (0.037)	Data 0.000 (0.003)	Loss 0.8270 (0.9020)	Acc@1 68.359 (68.847)	Acc@5 98.047 (97.502)
Epoch: [532][192/196]	Time 0.035 (0.036)	Data 0.000 (0.002)	Loss 0.8641 (0.8980)	Acc@1 70.703 (68.912)	Acc@5 97.656 (97.616)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:533/535; Lr: 0.0010000000000000002
batch Size 256
Epoch: [533][0/196]	Time 0.066 (0.066)	Data 0.274 (0.274)	Loss 0.8166 (0.8166)	Acc@1 67.578 (67.578)	Acc@5 98.438 (98.438)
Epoch: [533][64/196]	Time 0.034 (0.036)	Data 0.000 (0.004)	Loss 0.8522 (0.8964)	Acc@1 69.922 (68.858)	Acc@5 100.000 (97.590)
Epoch: [533][128/196]	Time 0.040 (0.036)	Data 0.000 (0.002)	Loss 0.9747 (0.8959)	Acc@1 67.188 (68.971)	Acc@5 98.438 (97.638)
Epoch: [533][192/196]	Time 0.031 (0.036)	Data 0.000 (0.002)	Loss 0.8965 (0.8975)	Acc@1 70.312 (68.894)	Acc@5 98.047 (97.614)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:534/535; Lr: 0.0010000000000000002
batch Size 256
Epoch: [534][0/196]	Time 0.043 (0.043)	Data 0.296 (0.296)	Loss 0.9104 (0.9104)	Acc@1 69.141 (69.141)	Acc@5 96.094 (96.094)
Epoch: [534][64/196]	Time 0.042 (0.037)	Data 0.000 (0.005)	Loss 0.9791 (0.9013)	Acc@1 68.750 (69.069)	Acc@5 97.266 (97.398)
Epoch: [534][128/196]	Time 0.038 (0.036)	Data 0.000 (0.002)	Loss 0.8769 (0.8975)	Acc@1 71.875 (68.895)	Acc@5 96.875 (97.559)
Epoch: [534][192/196]	Time 0.033 (0.036)	Data 0.000 (0.002)	Loss 1.0238 (0.8961)	Acc@1 66.406 (68.928)	Acc@5 96.094 (97.624)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:535/535; Lr: 0.0010000000000000002
batch Size 256
Epoch: [535][0/196]	Time 0.050 (0.050)	Data 0.297 (0.297)	Loss 0.8718 (0.8718)	Acc@1 71.094 (71.094)	Acc@5 98.047 (98.047)
Epoch: [535][64/196]	Time 0.033 (0.036)	Data 0.000 (0.005)	Loss 0.9182 (0.8914)	Acc@1 73.438 (68.996)	Acc@5 96.094 (97.740)
Epoch: [535][128/196]	Time 0.030 (0.036)	Data 0.000 (0.002)	Loss 0.8297 (0.9019)	Acc@1 68.359 (68.580)	Acc@5 99.219 (97.623)
Epoch: [535][192/196]	Time 0.032 (0.036)	Data 0.000 (0.002)	Loss 0.9481 (0.9018)	Acc@1 67.969 (68.588)	Acc@5 95.312 (97.577)
Max memory in training epoch: 5.2904448
[INFO] Storing checkpoint...
  67.77
Max memory: 7.7508608
 7.346s  j: 21 bis 25
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 357
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 536
Max memory: 0.0346624
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:536/540; Lr: 0.0010000000000000002
batch Size 256
Epoch: [536][0/196]	Time 0.061 (0.061)	Data 0.337 (0.337)	Loss 0.8528 (0.8528)	Acc@1 66.797 (66.797)	Acc@5 98.438 (98.438)
Epoch: [536][64/196]	Time 0.041 (0.036)	Data 0.000 (0.005)	Loss 0.9280 (0.8921)	Acc@1 67.188 (69.056)	Acc@5 98.828 (97.788)
Epoch: [536][128/196]	Time 0.040 (0.037)	Data 0.000 (0.003)	Loss 0.9244 (0.8936)	Acc@1 67.188 (69.016)	Acc@5 97.266 (97.705)
Epoch: [536][192/196]	Time 0.029 (0.037)	Data 0.000 (0.002)	Loss 0.8678 (0.8984)	Acc@1 68.359 (68.825)	Acc@5 98.828 (97.608)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:537/540; Lr: 0.0010000000000000002
batch Size 256
Epoch: [537][0/196]	Time 0.046 (0.046)	Data 0.283 (0.283)	Loss 0.9109 (0.9109)	Acc@1 67.969 (67.969)	Acc@5 97.656 (97.656)
Epoch: [537][64/196]	Time 0.029 (0.037)	Data 0.000 (0.005)	Loss 0.8905 (0.8952)	Acc@1 71.484 (69.111)	Acc@5 96.875 (97.512)
Epoch: [537][128/196]	Time 0.032 (0.036)	Data 0.000 (0.002)	Loss 0.9698 (0.9044)	Acc@1 67.188 (68.683)	Acc@5 96.875 (97.505)
Epoch: [537][192/196]	Time 0.031 (0.036)	Data 0.000 (0.002)	Loss 0.9039 (0.8995)	Acc@1 66.797 (68.898)	Acc@5 97.656 (97.587)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:538/540; Lr: 0.0010000000000000002
batch Size 256
Epoch: [538][0/196]	Time 0.033 (0.033)	Data 0.236 (0.236)	Loss 0.8899 (0.8899)	Acc@1 71.484 (71.484)	Acc@5 97.266 (97.266)
Epoch: [538][64/196]	Time 0.036 (0.036)	Data 0.000 (0.004)	Loss 0.9295 (0.8971)	Acc@1 66.797 (68.852)	Acc@5 96.875 (97.338)
Epoch: [538][128/196]	Time 0.029 (0.035)	Data 0.000 (0.002)	Loss 0.9435 (0.9025)	Acc@1 64.062 (68.844)	Acc@5 96.875 (97.475)
Epoch: [538][192/196]	Time 0.033 (0.035)	Data 0.000 (0.001)	Loss 0.8314 (0.8971)	Acc@1 73.438 (69.021)	Acc@5 97.266 (97.531)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:539/540; Lr: 0.0010000000000000002
batch Size 256
Epoch: [539][0/196]	Time 0.047 (0.047)	Data 0.305 (0.305)	Loss 0.9282 (0.9282)	Acc@1 65.625 (65.625)	Acc@5 98.047 (98.047)
Epoch: [539][64/196]	Time 0.038 (0.037)	Data 0.000 (0.005)	Loss 1.0190 (0.8982)	Acc@1 67.969 (69.147)	Acc@5 94.922 (97.692)
Epoch: [539][128/196]	Time 0.038 (0.037)	Data 0.000 (0.003)	Loss 0.9072 (0.8992)	Acc@1 68.750 (69.119)	Acc@5 98.047 (97.647)
Epoch: [539][192/196]	Time 0.031 (0.036)	Data 0.000 (0.002)	Loss 0.9514 (0.8999)	Acc@1 64.453 (68.963)	Acc@5 98.047 (97.557)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:540/540; Lr: 0.0010000000000000002
batch Size 256
Epoch: [540][0/196]	Time 0.061 (0.061)	Data 0.303 (0.303)	Loss 0.7858 (0.7858)	Acc@1 73.438 (73.438)	Acc@5 98.438 (98.438)
Epoch: [540][64/196]	Time 0.035 (0.036)	Data 0.000 (0.005)	Loss 0.9282 (0.8878)	Acc@1 64.453 (69.255)	Acc@5 97.266 (97.764)
Epoch: [540][128/196]	Time 0.055 (0.036)	Data 0.000 (0.003)	Loss 1.0208 (0.8972)	Acc@1 65.234 (68.723)	Acc@5 94.922 (97.665)
Epoch: [540][192/196]	Time 0.036 (0.036)	Data 0.000 (0.002)	Loss 0.8286 (0.8986)	Acc@1 72.266 (68.752)	Acc@5 97.266 (97.636)
Max memory in training epoch: 5.2904448
[INFO] Storing checkpoint...
  68.01
Max memory: 7.7508608
 7.401s  j: 26 bis 30
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1855
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 541
Max memory: 0.0346624
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:541/545; Lr: 0.0010000000000000002
batch Size 256
Epoch: [541][0/196]	Time 0.093 (0.093)	Data 0.291 (0.291)	Loss 0.9064 (0.9064)	Acc@1 69.141 (69.141)	Acc@5 98.828 (98.828)
Epoch: [541][64/196]	Time 0.035 (0.037)	Data 0.000 (0.005)	Loss 0.8321 (0.8964)	Acc@1 73.438 (69.267)	Acc@5 98.047 (97.566)
Epoch: [541][128/196]	Time 0.040 (0.037)	Data 0.000 (0.002)	Loss 0.8829 (0.8951)	Acc@1 69.922 (68.820)	Acc@5 96.875 (97.653)
Epoch: [541][192/196]	Time 0.035 (0.036)	Data 0.000 (0.002)	Loss 0.9512 (0.8967)	Acc@1 64.453 (68.774)	Acc@5 96.875 (97.624)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:542/545; Lr: 0.0010000000000000002
batch Size 256
Epoch: [542][0/196]	Time 0.048 (0.048)	Data 0.312 (0.312)	Loss 0.8359 (0.8359)	Acc@1 69.531 (69.531)	Acc@5 96.875 (96.875)
Epoch: [542][64/196]	Time 0.036 (0.037)	Data 0.000 (0.005)	Loss 0.9518 (0.9065)	Acc@1 69.531 (68.780)	Acc@5 96.484 (97.488)
Epoch: [542][128/196]	Time 0.037 (0.037)	Data 0.000 (0.003)	Loss 0.9379 (0.8966)	Acc@1 62.109 (69.110)	Acc@5 98.828 (97.562)
Epoch: [542][192/196]	Time 0.029 (0.037)	Data 0.000 (0.002)	Loss 0.9737 (0.9009)	Acc@1 66.406 (68.835)	Acc@5 96.484 (97.531)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:543/545; Lr: 0.0010000000000000002
batch Size 256
Epoch: [543][0/196]	Time 0.045 (0.045)	Data 0.206 (0.206)	Loss 0.8748 (0.8748)	Acc@1 67.969 (67.969)	Acc@5 97.656 (97.656)
Epoch: [543][64/196]	Time 0.038 (0.034)	Data 0.000 (0.003)	Loss 0.8545 (0.8954)	Acc@1 69.531 (69.020)	Acc@5 98.047 (97.698)
Epoch: [543][128/196]	Time 0.039 (0.035)	Data 0.000 (0.002)	Loss 1.0302 (0.8968)	Acc@1 62.500 (69.074)	Acc@5 94.922 (97.620)
Epoch: [543][192/196]	Time 0.038 (0.035)	Data 0.000 (0.001)	Loss 0.9430 (0.8985)	Acc@1 69.141 (68.934)	Acc@5 96.875 (97.559)
Max memory in training epoch: 5.2904448
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:544/545; Lr: 0.0010000000000000002
batch Size 256
