no display found. Using non-interactive Agg backend
[3, 3, 3]
[4, 8, 16]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 120; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/room2x1/model.nn; checkpoint: ./output/experimente4/room231; saveModell: True; LR: 0.1
random number: 1861
Files already downloaded and verified
==> Resuming from checkpoint..
Start epoch: 121
First Lr: 0.1
Startepoche: 121
deeper epoch: 0
Epoche: [121/240]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.
  warnings.warn("Setting attributes on ParameterList is not supported.")
Epoch: [121][0/196]	Time 0.299 (0.299)	Data 0.536 (0.536)	Loss 0.4603 (0.4603)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [121][64/196]	Time 0.123 (0.193)	Data 0.000 (0.008)	Loss 0.5116 (0.5379)	Acc@1 81.250 (81.460)	Acc@5 100.000 (99.026)
Epoch: [121][128/196]	Time 0.190 (0.181)	Data 0.000 (0.004)	Loss 0.5684 (0.5337)	Acc@1 78.125 (81.477)	Acc@5 98.438 (99.061)
Epoch: [121][192/196]	Time 0.116 (0.182)	Data 0.000 (0.003)	Loss 0.5494 (0.5343)	Acc@1 80.078 (81.501)	Acc@5 100.000 (99.091)
after train
test acc: 79.51


now deeper1
deep2: True
len param: 12
Block: 0
Block: 1
Block: 2
Block: 3
Block: 4
Block: 0
Block: 1
Block: 2
Block: 3
Block: 4
Block: 0
Block: 1
Block: 2
Block: 3
Block: 4
archNums: [[1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [2, 1, 1, 1, 1]]
len paramList: 15
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=16, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
)
args.layersInBlock: 1
module: Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Size of Weight: torch.Size([4, 3, 3, 3])
module: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Size of Weight: torch.Size([4])
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Size of Weight: torch.Size([8, 4, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Sequential: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 4, 1, 1])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Size of Weight: torch.Size([16, 8, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 8, 1, 1])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=16, out_features=10, bias=True)
Size of Weight: torch.Size([10, 16])
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=16, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
)
Epoche: [122/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [122][0/196]	Time 0.373 (0.373)	Data 0.504 (0.504)	Loss 2.5723 (2.5723)	Acc@1 27.734 (27.734)	Acc@5 74.609 (74.609)
Epoch: [122][64/196]	Time 0.193 (0.249)	Data 0.000 (0.009)	Loss 2.2915 (3.1412)	Acc@1 12.500 (13.089)	Acc@5 58.203 (54.099)
Epoch: [122][128/196]	Time 0.216 (0.242)	Data 0.000 (0.005)	Loss 2.2747 (2.7099)	Acc@1 11.328 (12.915)	Acc@5 57.031 (56.862)
Epoch: [122][192/196]	Time 0.241 (0.236)	Data 0.000 (0.003)	Loss 2.2131 (2.5527)	Acc@1 12.500 (13.399)	Acc@5 63.281 (58.964)
after train
test acc: 12.45
Epoche: [123/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [123][0/196]	Time 0.411 (0.411)	Data 0.478 (0.478)	Loss 2.2368 (2.2368)	Acc@1 15.234 (15.234)	Acc@5 62.891 (62.891)
Epoch: [123][64/196]	Time 0.176 (0.233)	Data 0.000 (0.008)	Loss 2.2175 (2.2299)	Acc@1 14.062 (14.898)	Acc@5 64.453 (63.966)
Epoch: [123][128/196]	Time 0.272 (0.243)	Data 0.000 (0.005)	Loss 2.2064 (2.2203)	Acc@1 16.797 (14.956)	Acc@5 58.203 (64.617)
Epoch: [123][192/196]	Time 0.211 (0.243)	Data 0.000 (0.003)	Loss 2.1926 (2.2158)	Acc@1 18.750 (15.002)	Acc@5 71.875 (64.935)
after train
test acc: 14.55
Epoche: [124/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [124][0/196]	Time 0.293 (0.293)	Data 0.729 (0.729)	Loss 2.2071 (2.2071)	Acc@1 16.797 (16.797)	Acc@5 67.969 (67.969)
Epoch: [124][64/196]	Time 0.182 (0.240)	Data 0.000 (0.012)	Loss 2.1960 (2.1962)	Acc@1 16.797 (15.457)	Acc@5 65.234 (66.430)
Epoch: [124][128/196]	Time 0.211 (0.234)	Data 0.002 (0.006)	Loss 2.1800 (2.1939)	Acc@1 23.438 (15.664)	Acc@5 69.531 (66.685)
Epoch: [124][192/196]	Time 0.214 (0.231)	Data 0.000 (0.005)	Loss 2.2083 (2.1928)	Acc@1 14.062 (15.678)	Acc@5 65.625 (66.704)
after train
test acc: 13.52
Epoche: [125/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [125][0/196]	Time 0.348 (0.348)	Data 0.578 (0.578)	Loss 2.2233 (2.2233)	Acc@1 20.312 (20.312)	Acc@5 67.969 (67.969)
Epoch: [125][64/196]	Time 0.266 (0.232)	Data 0.005 (0.010)	Loss 2.1547 (2.1891)	Acc@1 17.188 (15.276)	Acc@5 70.312 (67.464)
Epoch: [125][128/196]	Time 0.339 (0.231)	Data 0.006 (0.005)	Loss 2.2296 (2.1937)	Acc@1 15.234 (15.340)	Acc@5 61.719 (66.963)
Epoch: [125][192/196]	Time 0.224 (0.235)	Data 0.000 (0.004)	Loss 2.1957 (2.1910)	Acc@1 16.797 (15.473)	Acc@5 66.797 (67.086)
after train
test acc: 15.44
Epoche: [126/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [126][0/196]	Time 0.305 (0.305)	Data 0.432 (0.432)	Loss 2.1885 (2.1885)	Acc@1 17.188 (17.188)	Acc@5 68.750 (68.750)
Epoch: [126][64/196]	Time 0.190 (0.239)	Data 0.000 (0.007)	Loss 2.1620 (2.1841)	Acc@1 17.188 (15.685)	Acc@5 68.359 (67.542)
Epoch: [126][128/196]	Time 0.244 (0.236)	Data 0.000 (0.004)	Loss 2.1476 (2.1864)	Acc@1 14.453 (15.846)	Acc@5 71.094 (67.572)
Epoch: [126][192/196]	Time 0.349 (0.234)	Data 0.000 (0.003)	Loss 2.2031 (2.1833)	Acc@1 13.672 (15.809)	Acc@5 67.969 (67.791)
after train
test acc: 15.91
Epoche: [127/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [127][0/196]	Time 0.268 (0.268)	Data 0.478 (0.478)	Loss 2.2433 (2.2433)	Acc@1 11.719 (11.719)	Acc@5 61.719 (61.719)
Epoch: [127][64/196]	Time 0.338 (0.248)	Data 0.000 (0.008)	Loss 2.1656 (2.1857)	Acc@1 16.406 (15.457)	Acc@5 69.141 (67.885)
Epoch: [127][128/196]	Time 0.188 (0.239)	Data 0.000 (0.005)	Loss 2.1357 (2.1836)	Acc@1 17.578 (15.198)	Acc@5 71.875 (67.948)
Epoch: [127][192/196]	Time 0.252 (0.233)	Data 0.000 (0.003)	Loss 2.2123 (2.1842)	Acc@1 14.844 (15.338)	Acc@5 66.797 (67.710)
after train
test acc: 14.73
Epoche: [128/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [128][0/196]	Time 0.491 (0.491)	Data 0.437 (0.437)	Loss 2.1937 (2.1937)	Acc@1 12.891 (12.891)	Acc@5 66.406 (66.406)
Epoch: [128][64/196]	Time 0.202 (0.225)	Data 0.000 (0.008)	Loss 2.1438 (2.1950)	Acc@1 20.312 (15.306)	Acc@5 69.531 (67.109)
Epoch: [128][128/196]	Time 0.152 (0.222)	Data 0.000 (0.005)	Loss 2.1407 (2.1925)	Acc@1 16.797 (15.392)	Acc@5 69.531 (67.154)
Epoch: [128][192/196]	Time 0.228 (0.226)	Data 0.000 (0.003)	Loss 2.1684 (2.1924)	Acc@1 18.359 (15.425)	Acc@5 72.266 (67.277)
after train
test acc: 16.82
Epoche: [129/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [129][0/196]	Time 0.244 (0.244)	Data 0.682 (0.682)	Loss 2.1795 (2.1795)	Acc@1 12.500 (12.500)	Acc@5 68.750 (68.750)
Epoch: [129][64/196]	Time 0.261 (0.232)	Data 0.000 (0.012)	Loss 2.2098 (2.1901)	Acc@1 18.359 (15.174)	Acc@5 67.578 (67.031)
Epoch: [129][128/196]	Time 0.167 (0.228)	Data 0.000 (0.006)	Loss 2.1645 (2.1868)	Acc@1 19.922 (15.592)	Acc@5 69.531 (67.530)
Epoch: [129][192/196]	Time 0.199 (0.226)	Data 0.000 (0.005)	Loss 2.2061 (2.1853)	Acc@1 14.844 (15.587)	Acc@5 68.359 (67.793)
after train
test acc: 17.25
Epoche: [130/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [130][0/196]	Time 0.244 (0.244)	Data 0.522 (0.522)	Loss 2.1418 (2.1418)	Acc@1 21.875 (21.875)	Acc@5 73.047 (73.047)
Epoch: [130][64/196]	Time 0.234 (0.209)	Data 0.000 (0.010)	Loss 2.1857 (2.1808)	Acc@1 10.938 (15.553)	Acc@5 69.531 (68.528)
Epoch: [130][128/196]	Time 0.275 (0.216)	Data 0.004 (0.006)	Loss 2.1606 (2.1790)	Acc@1 14.844 (15.676)	Acc@5 70.703 (68.674)
Epoch: [130][192/196]	Time 0.264 (0.220)	Data 0.000 (0.004)	Loss 2.1172 (2.1786)	Acc@1 12.891 (15.769)	Acc@5 72.266 (68.507)
after train
test acc: 15.41
Epoche: [131/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [131][0/196]	Time 0.304 (0.304)	Data 0.518 (0.518)	Loss 2.1822 (2.1822)	Acc@1 11.328 (11.328)	Acc@5 67.188 (67.188)
Epoch: [131][64/196]	Time 0.290 (0.213)	Data 0.002 (0.009)	Loss 2.1981 (2.1797)	Acc@1 14.453 (15.397)	Acc@5 65.625 (68.299)
Epoch: [131][128/196]	Time 0.194 (0.212)	Data 0.000 (0.005)	Loss 2.1765 (2.1798)	Acc@1 15.234 (15.961)	Acc@5 67.969 (68.299)
Epoch: [131][192/196]	Time 0.202 (0.213)	Data 0.000 (0.004)	Loss 2.1141 (2.1785)	Acc@1 19.531 (15.898)	Acc@5 73.047 (68.317)
after train
test acc: 15.53
Epoche: [132/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [132][0/196]	Time 0.289 (0.289)	Data 0.488 (0.488)	Loss 2.1658 (2.1658)	Acc@1 18.750 (18.750)	Acc@5 69.922 (69.922)
Epoch: [132][64/196]	Time 0.214 (0.215)	Data 0.000 (0.008)	Loss 2.1941 (2.1732)	Acc@1 14.844 (16.334)	Acc@5 66.016 (68.468)
Epoch: [132][128/196]	Time 0.195 (0.206)	Data 0.000 (0.004)	Loss 2.1609 (2.1734)	Acc@1 18.750 (16.415)	Acc@5 69.922 (68.296)
Epoch: [132][192/196]	Time 0.216 (0.207)	Data 0.000 (0.003)	Loss 2.2213 (2.1745)	Acc@1 16.016 (16.311)	Acc@5 66.797 (68.199)
after train
test acc: 16.11
Epoche: [133/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [133][0/196]	Time 0.188 (0.188)	Data 0.559 (0.559)	Loss 2.1744 (2.1744)	Acc@1 14.453 (14.453)	Acc@5 68.359 (68.359)
Epoch: [133][64/196]	Time 0.298 (0.209)	Data 0.000 (0.009)	Loss 2.0979 (2.1700)	Acc@1 18.750 (16.821)	Acc@5 70.703 (68.954)
Epoch: [133][128/196]	Time 0.259 (0.209)	Data 0.000 (0.005)	Loss 2.1737 (2.1679)	Acc@1 15.234 (16.324)	Acc@5 73.828 (68.914)
Epoch: [133][192/196]	Time 0.265 (0.210)	Data 0.000 (0.004)	Loss 2.2082 (2.1683)	Acc@1 17.578 (16.396)	Acc@5 66.797 (68.859)
after train
test acc: 17.47
Epoche: [134/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [134][0/196]	Time 0.346 (0.346)	Data 0.568 (0.568)	Loss 2.0993 (2.0993)	Acc@1 20.703 (20.703)	Acc@5 71.875 (71.875)
Epoch: [134][64/196]	Time 0.235 (0.217)	Data 0.000 (0.010)	Loss 2.1974 (2.1756)	Acc@1 12.891 (15.853)	Acc@5 66.797 (67.560)
Epoch: [134][128/196]	Time 0.206 (0.220)	Data 0.000 (0.005)	Loss 2.1946 (2.1738)	Acc@1 14.453 (15.946)	Acc@5 67.578 (67.890)
Epoch: [134][192/196]	Time 0.283 (0.217)	Data 0.000 (0.004)	Loss 2.1832 (2.1744)	Acc@1 20.312 (15.951)	Acc@5 69.141 (68.250)
after train
test acc: 17.47
Epoche: [135/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [135][0/196]	Time 0.303 (0.303)	Data 0.790 (0.790)	Loss 2.1850 (2.1850)	Acc@1 17.188 (17.188)	Acc@5 70.312 (70.312)
Epoch: [135][64/196]	Time 0.234 (0.225)	Data 0.000 (0.013)	Loss 2.1414 (2.1703)	Acc@1 17.578 (16.034)	Acc@5 72.266 (68.870)
Epoch: [135][128/196]	Time 0.212 (0.218)	Data 0.000 (0.007)	Loss 2.1517 (2.1690)	Acc@1 20.703 (15.946)	Acc@5 71.094 (68.783)
Epoch: [135][192/196]	Time 0.391 (0.217)	Data 0.000 (0.005)	Loss 2.1877 (2.1723)	Acc@1 15.234 (15.773)	Acc@5 71.875 (68.568)
after train
test acc: 17.23
Epoche: [136/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [136][0/196]	Time 0.286 (0.286)	Data 0.405 (0.405)	Loss 2.1402 (2.1402)	Acc@1 17.188 (17.188)	Acc@5 71.484 (71.484)
Epoch: [136][64/196]	Time 0.170 (0.221)	Data 0.000 (0.007)	Loss 2.1825 (2.1691)	Acc@1 16.797 (16.286)	Acc@5 67.969 (68.804)
Epoch: [136][128/196]	Time 0.199 (0.214)	Data 0.000 (0.004)	Loss 2.2079 (2.1691)	Acc@1 16.797 (16.412)	Acc@5 66.797 (68.747)
Epoch: [136][192/196]	Time 0.208 (0.211)	Data 0.000 (0.003)	Loss 2.1806 (2.1677)	Acc@1 11.719 (16.309)	Acc@5 73.047 (68.722)
after train
test acc: 16.48
Epoche: [137/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [137][0/196]	Time 0.332 (0.332)	Data 0.449 (0.449)	Loss 2.1294 (2.1294)	Acc@1 22.266 (22.266)	Acc@5 76.562 (76.562)
Epoch: [137][64/196]	Time 0.224 (0.220)	Data 0.000 (0.008)	Loss 2.1476 (2.1593)	Acc@1 14.453 (16.208)	Acc@5 72.656 (68.912)
Epoch: [137][128/196]	Time 0.195 (0.217)	Data 0.010 (0.005)	Loss 2.1710 (2.1640)	Acc@1 14.844 (16.161)	Acc@5 71.875 (68.393)
Epoch: [137][192/196]	Time 0.156 (0.212)	Data 0.000 (0.003)	Loss 2.2098 (2.1672)	Acc@1 13.672 (16.190)	Acc@5 66.406 (68.153)
after train
test acc: 17.21
Epoche: [138/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [138][0/196]	Time 0.222 (0.222)	Data 0.553 (0.553)	Loss 2.1202 (2.1202)	Acc@1 17.578 (17.578)	Acc@5 74.609 (74.609)
Epoch: [138][64/196]	Time 0.160 (0.188)	Data 0.000 (0.009)	Loss 2.2073 (2.1637)	Acc@1 12.891 (16.238)	Acc@5 62.891 (68.341)
Epoch: [138][128/196]	Time 0.222 (0.178)	Data 0.000 (0.005)	Loss 2.2001 (2.1682)	Acc@1 14.062 (16.155)	Acc@5 68.750 (68.035)
Epoch: [138][192/196]	Time 0.175 (0.177)	Data 0.000 (0.003)	Loss 2.1714 (2.1677)	Acc@1 11.719 (16.008)	Acc@5 68.359 (68.074)
after train
test acc: 17.71
Epoche: [139/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [139][0/196]	Time 0.251 (0.251)	Data 0.376 (0.376)	Loss 2.0951 (2.0951)	Acc@1 17.578 (17.578)	Acc@5 72.656 (72.656)
Epoch: [139][64/196]	Time 0.165 (0.175)	Data 0.000 (0.006)	Loss 2.1751 (2.1463)	Acc@1 17.578 (17.374)	Acc@5 66.797 (69.495)
Epoch: [139][128/196]	Time 0.167 (0.174)	Data 0.000 (0.003)	Loss 2.1607 (2.1468)	Acc@1 16.797 (17.742)	Acc@5 69.531 (69.725)
Epoch: [139][192/196]	Time 0.153 (0.176)	Data 0.000 (0.002)	Loss 2.1456 (2.1425)	Acc@1 18.359 (17.946)	Acc@5 72.266 (70.039)
after train
test acc: 20.83
Epoche: [140/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [140][0/196]	Time 0.241 (0.241)	Data 0.374 (0.374)	Loss 2.1758 (2.1758)	Acc@1 17.578 (17.578)	Acc@5 70.703 (70.703)
Epoch: [140][64/196]	Time 0.160 (0.174)	Data 0.000 (0.006)	Loss 2.0940 (2.1178)	Acc@1 19.922 (19.856)	Acc@5 71.875 (71.929)
Epoch: [140][128/196]	Time 0.232 (0.172)	Data 0.000 (0.003)	Loss 2.1120 (2.1285)	Acc@1 19.141 (19.059)	Acc@5 74.219 (71.369)
Epoch: [140][192/196]	Time 0.153 (0.165)	Data 0.000 (0.002)	Loss 2.1395 (2.1338)	Acc@1 15.625 (18.639)	Acc@5 68.359 (71.041)
after train
test acc: 12.46
Epoche: [141/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [141][0/196]	Time 0.225 (0.225)	Data 0.379 (0.379)	Loss 2.0637 (2.0637)	Acc@1 20.312 (20.312)	Acc@5 78.516 (78.516)
Epoch: [141][64/196]	Time 0.142 (0.149)	Data 0.000 (0.006)	Loss 2.0878 (2.1295)	Acc@1 22.266 (18.528)	Acc@5 73.047 (71.220)
Epoch: [141][128/196]	Time 0.178 (0.152)	Data 0.000 (0.003)	Loss 2.1144 (2.1329)	Acc@1 17.578 (18.514)	Acc@5 67.578 (71.073)
Epoch: [141][192/196]	Time 0.197 (0.154)	Data 0.000 (0.002)	Loss 2.1569 (2.1315)	Acc@1 17.969 (18.529)	Acc@5 67.188 (70.942)
after train
test acc: 20.12
Epoche: [142/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [142][0/196]	Time 0.186 (0.186)	Data 0.365 (0.365)	Loss 2.1144 (2.1144)	Acc@1 17.188 (17.188)	Acc@5 69.922 (69.922)
Epoch: [142][64/196]	Time 0.147 (0.140)	Data 0.000 (0.006)	Loss 2.0909 (2.1122)	Acc@1 21.875 (20.042)	Acc@5 73.047 (71.599)
Epoch: [142][128/196]	Time 0.122 (0.139)	Data 0.000 (0.003)	Loss 2.1542 (2.1101)	Acc@1 18.359 (20.276)	Acc@5 69.922 (71.936)
Epoch: [142][192/196]	Time 0.134 (0.140)	Data 0.000 (0.002)	Loss 2.1367 (2.1117)	Acc@1 16.406 (20.037)	Acc@5 71.484 (72.098)
after train
test acc: 21.38
Epoche: [143/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [143][0/196]	Time 0.248 (0.248)	Data 0.330 (0.330)	Loss 2.1390 (2.1390)	Acc@1 21.484 (21.484)	Acc@5 72.266 (72.266)
Epoch: [143][64/196]	Time 0.176 (0.152)	Data 0.000 (0.005)	Loss 2.0902 (2.1113)	Acc@1 19.531 (19.333)	Acc@5 71.875 (72.578)
Epoch: [143][128/196]	Time 0.162 (0.152)	Data 0.000 (0.003)	Loss 2.0431 (2.1070)	Acc@1 22.266 (20.164)	Acc@5 76.953 (72.541)
Epoch: [143][192/196]	Time 0.118 (0.149)	Data 0.000 (0.002)	Loss 2.0074 (2.1053)	Acc@1 24.609 (20.561)	Acc@5 75.391 (72.842)
after train
test acc: 20.84
Epoche: [144/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [144][0/196]	Time 0.238 (0.238)	Data 0.333 (0.333)	Loss 2.0788 (2.0788)	Acc@1 25.000 (25.000)	Acc@5 73.047 (73.047)
Epoch: [144][64/196]	Time 0.144 (0.143)	Data 0.000 (0.005)	Loss 2.0740 (2.1008)	Acc@1 23.047 (21.082)	Acc@5 72.266 (72.764)
Epoch: [144][128/196]	Time 0.135 (0.144)	Data 0.000 (0.003)	Loss 2.0411 (2.0909)	Acc@1 25.000 (21.708)	Acc@5 78.906 (73.456)
Epoch: [144][192/196]	Time 0.137 (0.143)	Data 0.000 (0.002)	Loss 1.9780 (2.0870)	Acc@1 30.859 (22.106)	Acc@5 81.641 (73.792)
after train
test acc: 23.0
Epoche: [145/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [145][0/196]	Time 0.253 (0.253)	Data 0.324 (0.324)	Loss 2.1061 (2.1061)	Acc@1 25.391 (25.391)	Acc@5 69.531 (69.531)
Epoch: [145][64/196]	Time 0.121 (0.153)	Data 0.000 (0.005)	Loss 2.1245 (2.0989)	Acc@1 20.703 (22.458)	Acc@5 73.047 (73.011)
Epoch: [145][128/196]	Time 0.142 (0.154)	Data 0.000 (0.003)	Loss 2.2348 (2.1569)	Acc@1 16.406 (19.722)	Acc@5 61.719 (68.550)
Epoch: [145][192/196]	Time 0.162 (0.151)	Data 0.000 (0.002)	Loss 2.2008 (2.1739)	Acc@1 19.141 (18.956)	Acc@5 67.578 (67.501)
after train
test acc: 15.47
Epoche: [146/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [146][0/196]	Time 0.232 (0.232)	Data 0.380 (0.380)	Loss 2.2277 (2.2277)	Acc@1 15.234 (15.234)	Acc@5 66.797 (66.797)
Epoch: [146][64/196]	Time 0.141 (0.151)	Data 0.000 (0.006)	Loss 2.1063 (2.1834)	Acc@1 23.438 (19.020)	Acc@5 73.047 (67.536)
Epoch: [146][128/196]	Time 0.149 (0.149)	Data 0.000 (0.003)	Loss 2.1564 (2.1622)	Acc@1 22.656 (20.004)	Acc@5 67.188 (69.129)
Epoch: [146][192/196]	Time 0.156 (0.153)	Data 0.000 (0.002)	Loss 2.0369 (2.1374)	Acc@1 25.000 (20.908)	Acc@5 78.125 (70.578)
after train
test acc: 17.28
Epoche: [147/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [147][0/196]	Time 0.248 (0.248)	Data 0.316 (0.316)	Loss 2.0812 (2.0812)	Acc@1 24.219 (24.219)	Acc@5 71.875 (71.875)
Epoch: [147][64/196]	Time 0.139 (0.147)	Data 0.000 (0.005)	Loss 1.9903 (2.0624)	Acc@1 29.688 (23.930)	Acc@5 79.688 (74.603)
Epoch: [147][128/196]	Time 0.138 (0.152)	Data 0.000 (0.003)	Loss 1.9136 (2.0412)	Acc@1 32.031 (24.764)	Acc@5 80.859 (75.524)
Epoch: [147][192/196]	Time 0.139 (0.150)	Data 0.000 (0.002)	Loss 2.0433 (2.0264)	Acc@1 20.312 (24.980)	Acc@5 73.047 (76.235)
after train
test acc: 25.85
Epoche: [148/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [148][0/196]	Time 0.223 (0.223)	Data 0.369 (0.369)	Loss 1.9307 (1.9307)	Acc@1 31.250 (31.250)	Acc@5 81.641 (81.641)
Epoch: [148][64/196]	Time 0.136 (0.151)	Data 0.000 (0.006)	Loss 2.0132 (1.9723)	Acc@1 27.344 (26.556)	Acc@5 78.516 (78.672)
Epoch: [148][128/196]	Time 0.154 (0.152)	Data 0.000 (0.003)	Loss 1.8944 (1.9514)	Acc@1 27.344 (27.553)	Acc@5 82.812 (79.657)
Epoch: [148][192/196]	Time 0.151 (0.151)	Data 0.000 (0.002)	Loss 1.9798 (1.9482)	Acc@1 26.172 (27.775)	Acc@5 77.344 (79.795)
after train
test acc: 22.56
Epoche: [149/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [149][0/196]	Time 0.197 (0.197)	Data 0.362 (0.362)	Loss 1.9288 (1.9288)	Acc@1 31.250 (31.250)	Acc@5 78.125 (78.125)
Epoch: [149][64/196]	Time 0.137 (0.154)	Data 0.000 (0.006)	Loss 1.9524 (1.9278)	Acc@1 30.859 (28.714)	Acc@5 76.953 (80.120)
Epoch: [149][128/196]	Time 0.136 (0.150)	Data 0.000 (0.003)	Loss 1.8095 (1.9121)	Acc@1 27.734 (29.445)	Acc@5 85.156 (80.838)
Epoch: [149][192/196]	Time 0.160 (0.149)	Data 0.000 (0.002)	Loss 1.7740 (1.8956)	Acc@1 32.812 (29.989)	Acc@5 83.984 (81.537)
after train
test acc: 26.5
Epoche: [150/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [150][0/196]	Time 0.194 (0.194)	Data 0.351 (0.351)	Loss 1.8529 (1.8529)	Acc@1 32.422 (32.422)	Acc@5 84.375 (84.375)
Epoch: [150][64/196]	Time 0.145 (0.140)	Data 0.000 (0.006)	Loss 1.7607 (1.8386)	Acc@1 33.984 (32.049)	Acc@5 85.938 (83.450)
Epoch: [150][128/196]	Time 0.138 (0.144)	Data 0.000 (0.003)	Loss 1.8359 (1.8333)	Acc@1 32.422 (32.379)	Acc@5 85.156 (83.506)
Epoch: [150][192/196]	Time 0.146 (0.145)	Data 0.000 (0.002)	Loss 1.7395 (1.8260)	Acc@1 37.500 (32.675)	Acc@5 88.281 (83.624)
after train
test acc: 27.57
Epoche: [151/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [151][0/196]	Time 0.258 (0.258)	Data 0.369 (0.369)	Loss 1.8278 (1.8278)	Acc@1 37.500 (37.500)	Acc@5 83.203 (83.203)
Epoch: [151][64/196]	Time 0.148 (0.154)	Data 0.000 (0.006)	Loss 1.7438 (1.7871)	Acc@1 33.594 (34.267)	Acc@5 87.891 (84.922)
Epoch: [151][128/196]	Time 0.143 (0.147)	Data 0.000 (0.003)	Loss 1.7967 (1.7830)	Acc@1 32.031 (34.345)	Acc@5 84.375 (85.099)
Epoch: [151][192/196]	Time 0.139 (0.147)	Data 0.000 (0.002)	Loss 1.7921 (1.7722)	Acc@1 36.719 (34.936)	Acc@5 85.547 (85.359)
after train
test acc: 33.17
Epoche: [152/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [152][0/196]	Time 0.198 (0.198)	Data 0.438 (0.438)	Loss 1.7160 (1.7160)	Acc@1 33.594 (33.594)	Acc@5 87.891 (87.891)
Epoch: [152][64/196]	Time 0.169 (0.142)	Data 0.000 (0.007)	Loss 1.7040 (1.7283)	Acc@1 41.016 (36.532)	Acc@5 86.719 (86.388)
Epoch: [152][128/196]	Time 0.145 (0.146)	Data 0.000 (0.004)	Loss 1.7331 (1.7301)	Acc@1 37.109 (36.292)	Acc@5 84.375 (86.480)
Epoch: [152][192/196]	Time 0.148 (0.148)	Data 0.000 (0.003)	Loss 1.6566 (1.7191)	Acc@1 40.625 (36.664)	Acc@5 89.062 (86.842)
after train
test acc: 36.08
Epoche: [153/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [153][0/196]	Time 0.170 (0.170)	Data 0.452 (0.452)	Loss 1.7232 (1.7232)	Acc@1 33.203 (33.203)	Acc@5 87.109 (87.109)
Epoch: [153][64/196]	Time 0.166 (0.152)	Data 0.000 (0.007)	Loss 1.7001 (1.6832)	Acc@1 36.328 (38.143)	Acc@5 89.062 (87.704)
Epoch: [153][128/196]	Time 0.140 (0.151)	Data 0.000 (0.004)	Loss 1.6495 (1.6743)	Acc@1 39.062 (38.523)	Acc@5 87.500 (87.706)
Epoch: [153][192/196]	Time 0.182 (0.153)	Data 0.000 (0.003)	Loss 1.6098 (1.6637)	Acc@1 38.281 (38.911)	Acc@5 91.016 (88.142)
after train
test acc: 35.35
Epoche: [154/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [154][0/196]	Time 0.233 (0.233)	Data 0.308 (0.308)	Loss 1.6697 (1.6697)	Acc@1 37.500 (37.500)	Acc@5 89.453 (89.453)
Epoch: [154][64/196]	Time 0.153 (0.154)	Data 0.000 (0.005)	Loss 1.4886 (1.6332)	Acc@1 46.094 (39.712)	Acc@5 92.188 (88.912)
Epoch: [154][128/196]	Time 0.139 (0.154)	Data 0.000 (0.003)	Loss 1.6218 (1.6119)	Acc@1 39.453 (40.913)	Acc@5 90.234 (89.441)
Epoch: [154][192/196]	Time 0.147 (0.151)	Data 0.000 (0.002)	Loss 1.6166 (1.6032)	Acc@1 39.844 (41.157)	Acc@5 90.234 (89.633)
after train
test acc: 39.28
Epoche: [155/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [155][0/196]	Time 0.199 (0.199)	Data 0.402 (0.402)	Loss 1.6296 (1.6296)	Acc@1 44.141 (44.141)	Acc@5 87.891 (87.891)
Epoch: [155][64/196]	Time 0.140 (0.159)	Data 0.000 (0.007)	Loss 1.6040 (1.5563)	Acc@1 42.188 (42.368)	Acc@5 89.844 (90.625)
Epoch: [155][128/196]	Time 0.171 (0.158)	Data 0.000 (0.003)	Loss 1.4401 (1.5571)	Acc@1 48.828 (42.890)	Acc@5 91.406 (90.471)
Epoch: [155][192/196]	Time 0.154 (0.158)	Data 0.000 (0.002)	Loss 1.4207 (1.5505)	Acc@1 48.828 (43.345)	Acc@5 92.188 (90.542)
after train
test acc: 35.63
Epoche: [156/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [156][0/196]	Time 0.208 (0.208)	Data 0.394 (0.394)	Loss 1.5343 (1.5343)	Acc@1 42.969 (42.969)	Acc@5 89.453 (89.453)
Epoch: [156][64/196]	Time 0.150 (0.157)	Data 0.000 (0.006)	Loss 1.4609 (1.5149)	Acc@1 46.875 (44.621)	Acc@5 93.359 (91.436)
Epoch: [156][128/196]	Time 0.151 (0.153)	Data 0.000 (0.003)	Loss 1.4936 (1.5073)	Acc@1 44.922 (45.140)	Acc@5 91.406 (91.191)
Epoch: [156][192/196]	Time 0.129 (0.152)	Data 0.000 (0.002)	Loss 1.5173 (1.4980)	Acc@1 46.094 (45.319)	Acc@5 89.453 (91.445)
after train
test acc: 39.8
Epoche: [157/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [157][0/196]	Time 0.266 (0.266)	Data 0.267 (0.267)	Loss 1.3647 (1.3647)	Acc@1 48.828 (48.828)	Acc@5 95.703 (95.703)
Epoch: [157][64/196]	Time 0.139 (0.153)	Data 0.000 (0.004)	Loss 1.4935 (1.4569)	Acc@1 48.047 (46.052)	Acc@5 90.234 (92.224)
Epoch: [157][128/196]	Time 0.138 (0.152)	Data 0.000 (0.002)	Loss 1.3827 (1.4523)	Acc@1 53.125 (46.699)	Acc@5 92.969 (92.260)
Epoch: [157][192/196]	Time 0.139 (0.153)	Data 0.000 (0.002)	Loss 1.4375 (1.4459)	Acc@1 46.875 (46.988)	Acc@5 92.188 (92.307)
after train
test acc: 42.13
Epoche: [158/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [158][0/196]	Time 0.175 (0.175)	Data 0.325 (0.325)	Loss 1.3748 (1.3748)	Acc@1 51.953 (51.953)	Acc@5 93.359 (93.359)
Epoch: [158][64/196]	Time 0.151 (0.153)	Data 0.000 (0.005)	Loss 1.5039 (1.4266)	Acc@1 47.266 (47.999)	Acc@5 90.234 (92.332)
Epoch: [158][128/196]	Time 0.136 (0.150)	Data 0.000 (0.003)	Loss 1.3516 (1.4280)	Acc@1 49.609 (47.738)	Acc@5 91.406 (92.151)
Epoch: [158][192/196]	Time 0.150 (0.148)	Data 0.000 (0.002)	Loss 1.3763 (1.4173)	Acc@1 50.781 (48.132)	Acc@5 94.922 (92.428)
after train
test acc: 33.94
Epoche: [159/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [159][0/196]	Time 0.262 (0.262)	Data 0.360 (0.360)	Loss 1.5073 (1.5073)	Acc@1 48.047 (48.047)	Acc@5 92.188 (92.188)
Epoch: [159][64/196]	Time 0.163 (0.153)	Data 0.000 (0.006)	Loss 1.3998 (1.3920)	Acc@1 50.000 (49.976)	Acc@5 91.406 (92.692)
Epoch: [159][128/196]	Time 0.159 (0.153)	Data 0.000 (0.003)	Loss 1.2718 (1.3776)	Acc@1 55.859 (50.321)	Acc@5 94.141 (92.860)
Epoch: [159][192/196]	Time 0.147 (0.153)	Data 0.000 (0.002)	Loss 1.3961 (1.3754)	Acc@1 48.047 (50.170)	Acc@5 92.578 (92.908)
after train
test acc: 34.89
Epoche: [160/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [160][0/196]	Time 0.268 (0.268)	Data 0.365 (0.365)	Loss 1.3888 (1.3888)	Acc@1 47.266 (47.266)	Acc@5 95.312 (95.312)
Epoch: [160][64/196]	Time 0.136 (0.159)	Data 0.000 (0.006)	Loss 1.3182 (1.3697)	Acc@1 53.125 (50.571)	Acc@5 95.312 (93.083)
Epoch: [160][128/196]	Time 0.156 (0.155)	Data 0.000 (0.003)	Loss 1.3739 (1.3632)	Acc@1 51.562 (50.936)	Acc@5 91.016 (93.138)
Epoch: [160][192/196]	Time 0.140 (0.152)	Data 0.000 (0.002)	Loss 1.3843 (1.3500)	Acc@1 50.781 (51.405)	Acc@5 92.969 (93.347)
after train
test acc: 33.39
Epoche: [161/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [161][0/196]	Time 0.219 (0.219)	Data 0.375 (0.375)	Loss 1.3636 (1.3636)	Acc@1 51.172 (51.172)	Acc@5 92.578 (92.578)
Epoch: [161][64/196]	Time 0.179 (0.157)	Data 0.000 (0.006)	Loss 1.3313 (1.3269)	Acc@1 56.250 (51.947)	Acc@5 94.141 (93.510)
Epoch: [161][128/196]	Time 0.136 (0.158)	Data 0.000 (0.003)	Loss 1.2865 (1.3253)	Acc@1 53.906 (52.280)	Acc@5 94.922 (93.480)
Epoch: [161][192/196]	Time 0.172 (0.154)	Data 0.000 (0.002)	Loss 1.4427 (1.3141)	Acc@1 46.875 (52.532)	Acc@5 94.531 (93.703)
after train
test acc: 48.06
Epoche: [162/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [162][0/196]	Time 0.173 (0.173)	Data 0.342 (0.342)	Loss 1.2673 (1.2673)	Acc@1 57.422 (57.422)	Acc@5 93.750 (93.750)
Epoch: [162][64/196]	Time 0.142 (0.145)	Data 0.000 (0.006)	Loss 1.2092 (1.2950)	Acc@1 51.562 (53.029)	Acc@5 94.531 (93.978)
Epoch: [162][128/196]	Time 0.154 (0.145)	Data 0.000 (0.003)	Loss 1.3253 (1.2927)	Acc@1 52.344 (53.431)	Acc@5 93.750 (93.798)
Epoch: [162][192/196]	Time 0.145 (0.145)	Data 0.000 (0.002)	Loss 1.1510 (1.2876)	Acc@1 58.594 (53.576)	Acc@5 96.484 (93.803)
after train
test acc: 26.3
Epoche: [163/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [163][0/196]	Time 0.203 (0.203)	Data 0.353 (0.353)	Loss 1.3315 (1.3315)	Acc@1 51.562 (51.562)	Acc@5 92.969 (92.969)
Epoch: [163][64/196]	Time 0.148 (0.158)	Data 0.000 (0.006)	Loss 1.1876 (1.2718)	Acc@1 56.641 (53.918)	Acc@5 95.703 (94.279)
Epoch: [163][128/196]	Time 0.138 (0.152)	Data 0.000 (0.003)	Loss 1.2951 (1.2722)	Acc@1 54.297 (54.079)	Acc@5 94.531 (94.234)
Epoch: [163][192/196]	Time 0.133 (0.148)	Data 0.000 (0.002)	Loss 1.1626 (1.2613)	Acc@1 58.594 (54.540)	Acc@5 94.141 (94.256)
after train
test acc: 46.76
Epoche: [164/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [164][0/196]	Time 0.165 (0.165)	Data 0.407 (0.407)	Loss 1.2288 (1.2288)	Acc@1 56.641 (56.641)	Acc@5 94.922 (94.922)
Epoch: [164][64/196]	Time 0.163 (0.148)	Data 0.000 (0.007)	Loss 1.2031 (1.2474)	Acc@1 55.469 (54.856)	Acc@5 96.094 (94.189)
Epoch: [164][128/196]	Time 0.134 (0.145)	Data 0.000 (0.003)	Loss 1.2527 (1.2393)	Acc@1 53.516 (55.375)	Acc@5 95.312 (94.268)
Epoch: [164][192/196]	Time 0.158 (0.146)	Data 0.000 (0.002)	Loss 1.3709 (1.2371)	Acc@1 46.875 (55.641)	Acc@5 95.312 (94.363)
after train
test acc: 50.75
Epoche: [165/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [165][0/196]	Time 0.199 (0.199)	Data 0.348 (0.348)	Loss 1.2037 (1.2037)	Acc@1 57.031 (57.031)	Acc@5 95.703 (95.703)
Epoch: [165][64/196]	Time 0.143 (0.156)	Data 0.000 (0.006)	Loss 1.2596 (1.2203)	Acc@1 53.516 (56.671)	Acc@5 95.703 (94.165)
Epoch: [165][128/196]	Time 0.133 (0.151)	Data 0.000 (0.003)	Loss 1.1811 (1.2220)	Acc@1 55.859 (56.713)	Acc@5 94.531 (94.392)
Epoch: [165][192/196]	Time 0.132 (0.151)	Data 0.000 (0.002)	Loss 1.2693 (1.2222)	Acc@1 55.078 (56.552)	Acc@5 91.406 (94.454)
after train
test acc: 39.21
Epoche: [166/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [166][0/196]	Time 0.238 (0.238)	Data 0.361 (0.361)	Loss 1.3502 (1.3502)	Acc@1 56.250 (56.250)	Acc@5 92.578 (92.578)
Epoch: [166][64/196]	Time 0.115 (0.151)	Data 0.000 (0.006)	Loss 1.2732 (1.1961)	Acc@1 53.906 (57.296)	Acc@5 94.531 (94.573)
Epoch: [166][128/196]	Time 0.153 (0.151)	Data 0.000 (0.003)	Loss 1.1098 (1.1983)	Acc@1 59.375 (57.298)	Acc@5 95.703 (94.692)
Epoch: [166][192/196]	Time 0.154 (0.152)	Data 0.000 (0.002)	Loss 1.2612 (1.1988)	Acc@1 55.469 (57.173)	Acc@5 95.312 (94.701)
after train
test acc: 52.61
Epoche: [167/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [167][0/196]	Time 0.195 (0.195)	Data 0.328 (0.328)	Loss 1.0860 (1.0860)	Acc@1 58.984 (58.984)	Acc@5 96.094 (96.094)
Epoch: [167][64/196]	Time 0.138 (0.153)	Data 0.000 (0.005)	Loss 1.0789 (1.1901)	Acc@1 58.984 (57.987)	Acc@5 96.094 (94.645)
Epoch: [167][128/196]	Time 0.180 (0.153)	Data 0.000 (0.003)	Loss 1.1586 (1.1840)	Acc@1 58.203 (57.979)	Acc@5 96.484 (94.895)
Epoch: [167][192/196]	Time 0.138 (0.153)	Data 0.000 (0.002)	Loss 1.1673 (1.1832)	Acc@1 60.547 (57.819)	Acc@5 94.922 (94.940)
after train
test acc: 40.92
Epoche: [168/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [168][0/196]	Time 0.272 (0.272)	Data 0.451 (0.451)	Loss 1.2116 (1.2116)	Acc@1 53.516 (53.516)	Acc@5 95.312 (95.312)
Epoch: [168][64/196]	Time 0.144 (0.153)	Data 0.000 (0.007)	Loss 1.2378 (1.1884)	Acc@1 56.641 (57.608)	Acc@5 92.969 (94.874)
Epoch: [168][128/196]	Time 0.198 (0.155)	Data 0.000 (0.004)	Loss 1.1832 (1.1805)	Acc@1 58.203 (57.988)	Acc@5 94.531 (95.113)
Epoch: [168][192/196]	Time 0.177 (0.153)	Data 0.000 (0.003)	Loss 1.1475 (1.1707)	Acc@1 62.891 (58.314)	Acc@5 95.703 (95.181)
after train
test acc: 26.08
Epoche: [169/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [169][0/196]	Time 0.232 (0.232)	Data 0.358 (0.358)	Loss 1.1846 (1.1846)	Acc@1 57.422 (57.422)	Acc@5 96.484 (96.484)
Epoch: [169][64/196]	Time 0.139 (0.151)	Data 0.000 (0.006)	Loss 1.0047 (1.1701)	Acc@1 63.281 (58.558)	Acc@5 96.094 (95.102)
Epoch: [169][128/196]	Time 0.159 (0.150)	Data 0.000 (0.003)	Loss 1.1745 (1.1604)	Acc@1 58.984 (58.942)	Acc@5 94.922 (95.164)
Epoch: [169][192/196]	Time 0.140 (0.151)	Data 0.000 (0.002)	Loss 1.3407 (1.1525)	Acc@1 48.828 (59.116)	Acc@5 95.703 (95.221)
after train
test acc: 38.13
Epoche: [170/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [170][0/196]	Time 0.207 (0.207)	Data 0.321 (0.321)	Loss 1.0158 (1.0158)	Acc@1 63.281 (63.281)	Acc@5 95.312 (95.312)
Epoch: [170][64/196]	Time 0.139 (0.154)	Data 0.000 (0.005)	Loss 1.2296 (1.1287)	Acc@1 58.203 (60.186)	Acc@5 93.359 (95.361)
Epoch: [170][128/196]	Time 0.150 (0.152)	Data 0.000 (0.003)	Loss 1.0878 (1.1278)	Acc@1 58.984 (59.908)	Acc@5 96.875 (95.385)
Epoch: [170][192/196]	Time 0.153 (0.150)	Data 0.000 (0.002)	Loss 1.0727 (1.1360)	Acc@1 61.719 (59.618)	Acc@5 95.703 (95.432)
after train
test acc: 57.4
Epoche: [171/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [171][0/196]	Time 0.201 (0.201)	Data 0.352 (0.352)	Loss 1.2259 (1.2259)	Acc@1 54.688 (54.688)	Acc@5 92.188 (92.188)
Epoch: [171][64/196]	Time 0.148 (0.156)	Data 0.000 (0.006)	Loss 1.1226 (1.1152)	Acc@1 60.938 (60.799)	Acc@5 95.703 (95.583)
Epoch: [171][128/196]	Time 0.136 (0.153)	Data 0.000 (0.003)	Loss 1.2078 (1.1194)	Acc@1 57.812 (60.674)	Acc@5 95.703 (95.540)
Epoch: [171][192/196]	Time 0.131 (0.153)	Data 0.000 (0.002)	Loss 1.1409 (1.1189)	Acc@1 60.547 (60.591)	Acc@5 96.875 (95.642)
after train
test acc: 52.16
Epoche: [172/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [172][0/196]	Time 0.314 (0.314)	Data 0.293 (0.293)	Loss 1.0709 (1.0709)	Acc@1 63.281 (63.281)	Acc@5 95.312 (95.312)
Epoch: [172][64/196]	Time 0.158 (0.153)	Data 0.000 (0.005)	Loss 1.1996 (1.1246)	Acc@1 57.422 (60.132)	Acc@5 94.141 (95.661)
Epoch: [172][128/196]	Time 0.152 (0.154)	Data 0.000 (0.003)	Loss 1.0946 (1.1176)	Acc@1 60.156 (60.389)	Acc@5 96.094 (95.643)
Epoch: [172][192/196]	Time 0.143 (0.153)	Data 0.000 (0.002)	Loss 1.0999 (1.1135)	Acc@1 60.938 (60.506)	Acc@5 96.094 (95.675)
after train
test acc: 51.62
Epoche: [173/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [173][0/196]	Time 0.217 (0.217)	Data 0.417 (0.417)	Loss 1.1954 (1.1954)	Acc@1 57.812 (57.812)	Acc@5 94.922 (94.922)
Epoch: [173][64/196]	Time 0.152 (0.144)	Data 0.000 (0.007)	Loss 1.2935 (1.1126)	Acc@1 53.906 (60.938)	Acc@5 94.141 (95.661)
Epoch: [173][128/196]	Time 0.140 (0.143)	Data 0.000 (0.004)	Loss 1.1099 (1.0998)	Acc@1 62.109 (61.389)	Acc@5 96.094 (95.764)
Epoch: [173][192/196]	Time 0.134 (0.143)	Data 0.000 (0.002)	Loss 1.0576 (1.1040)	Acc@1 62.500 (61.140)	Acc@5 96.875 (95.804)
after train
test acc: 55.02
Epoche: [174/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [174][0/196]	Time 0.221 (0.221)	Data 0.420 (0.420)	Loss 1.0184 (1.0184)	Acc@1 64.844 (64.844)	Acc@5 96.094 (96.094)
Epoch: [174][64/196]	Time 0.171 (0.149)	Data 0.000 (0.007)	Loss 0.9968 (1.0897)	Acc@1 65.625 (61.779)	Acc@5 96.484 (96.016)
Epoch: [174][128/196]	Time 0.138 (0.146)	Data 0.000 (0.004)	Loss 1.1389 (1.0900)	Acc@1 62.891 (61.540)	Acc@5 92.188 (95.906)
Epoch: [174][192/196]	Time 0.180 (0.147)	Data 0.000 (0.002)	Loss 1.1614 (1.0897)	Acc@1 57.031 (61.595)	Acc@5 96.094 (95.887)
after train
test acc: 43.18
Epoche: [175/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [175][0/196]	Time 0.226 (0.226)	Data 0.338 (0.338)	Loss 1.1224 (1.1224)	Acc@1 56.250 (56.250)	Acc@5 96.875 (96.875)
Epoch: [175][64/196]	Time 0.139 (0.146)	Data 0.000 (0.006)	Loss 0.9672 (1.0891)	Acc@1 66.406 (61.743)	Acc@5 98.438 (95.962)
Epoch: [175][128/196]	Time 0.151 (0.153)	Data 0.000 (0.003)	Loss 1.1240 (1.0882)	Acc@1 58.984 (61.840)	Acc@5 96.484 (95.906)
Epoch: [175][192/196]	Time 0.133 (0.152)	Data 0.000 (0.002)	Loss 1.0396 (1.0869)	Acc@1 63.281 (61.674)	Acc@5 96.484 (95.865)
after train
test acc: 49.42
Epoche: [176/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [176][0/196]	Time 0.254 (0.254)	Data 0.385 (0.385)	Loss 1.0079 (1.0079)	Acc@1 62.109 (62.109)	Acc@5 97.266 (97.266)
Epoch: [176][64/196]	Time 0.147 (0.157)	Data 0.000 (0.006)	Loss 0.9756 (1.0896)	Acc@1 63.281 (61.526)	Acc@5 97.656 (95.781)
Epoch: [176][128/196]	Time 0.146 (0.157)	Data 0.000 (0.003)	Loss 1.0218 (1.0873)	Acc@1 64.453 (61.770)	Acc@5 96.094 (95.782)
Epoch: [176][192/196]	Time 0.145 (0.155)	Data 0.000 (0.002)	Loss 1.0889 (1.0864)	Acc@1 60.547 (61.644)	Acc@5 96.094 (95.855)
after train
test acc: 33.14
Epoche: [177/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [177][0/196]	Time 0.287 (0.287)	Data 0.301 (0.301)	Loss 1.0213 (1.0213)	Acc@1 65.625 (65.625)	Acc@5 98.047 (98.047)
Epoch: [177][64/196]	Time 0.167 (0.158)	Data 0.000 (0.005)	Loss 1.1836 (1.0983)	Acc@1 59.375 (61.280)	Acc@5 94.531 (95.703)
Epoch: [177][128/196]	Time 0.184 (0.156)	Data 0.000 (0.003)	Loss 1.0495 (1.0795)	Acc@1 62.109 (61.834)	Acc@5 95.312 (95.930)
Epoch: [177][192/196]	Time 0.143 (0.155)	Data 0.000 (0.002)	Loss 1.0600 (1.0833)	Acc@1 64.453 (61.699)	Acc@5 97.656 (95.867)
after train
test acc: 54.97
Epoche: [178/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [178][0/196]	Time 0.221 (0.221)	Data 0.392 (0.392)	Loss 1.0776 (1.0776)	Acc@1 61.328 (61.328)	Acc@5 96.094 (96.094)
Epoch: [178][64/196]	Time 0.149 (0.150)	Data 0.000 (0.006)	Loss 1.0898 (1.0648)	Acc@1 60.547 (62.145)	Acc@5 96.484 (96.226)
Epoch: [178][128/196]	Time 0.134 (0.148)	Data 0.000 (0.003)	Loss 0.9618 (1.0586)	Acc@1 67.969 (62.512)	Acc@5 98.047 (96.145)
Epoch: [178][192/196]	Time 0.139 (0.147)	Data 0.000 (0.002)	Loss 1.0395 (1.0661)	Acc@1 60.156 (62.290)	Acc@5 95.703 (96.080)
after train
test acc: 44.66
Epoche: [179/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [179][0/196]	Time 0.257 (0.257)	Data 0.383 (0.383)	Loss 1.1321 (1.1321)	Acc@1 63.672 (63.672)	Acc@5 94.141 (94.141)
Epoch: [179][64/196]	Time 0.155 (0.158)	Data 0.000 (0.006)	Loss 0.9547 (1.0700)	Acc@1 64.453 (62.482)	Acc@5 97.656 (95.901)
Epoch: [179][128/196]	Time 0.139 (0.157)	Data 0.000 (0.003)	Loss 1.1198 (1.0704)	Acc@1 61.719 (62.294)	Acc@5 94.531 (95.979)
Epoch: [179][192/196]	Time 0.143 (0.155)	Data 0.000 (0.002)	Loss 1.1210 (1.0611)	Acc@1 62.500 (62.441)	Acc@5 95.312 (96.059)
after train
test acc: 51.75
Epoche: [180/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [180][0/196]	Time 0.226 (0.226)	Data 0.338 (0.338)	Loss 1.0246 (1.0246)	Acc@1 61.328 (61.328)	Acc@5 94.922 (94.922)
Epoch: [180][64/196]	Time 0.143 (0.151)	Data 0.000 (0.006)	Loss 1.0730 (1.0505)	Acc@1 59.375 (62.987)	Acc@5 96.484 (96.112)
Epoch: [180][128/196]	Time 0.171 (0.149)	Data 0.000 (0.003)	Loss 0.9709 (1.0588)	Acc@1 67.578 (62.479)	Acc@5 95.703 (96.130)
Epoch: [180][192/196]	Time 0.132 (0.149)	Data 0.000 (0.002)	Loss 1.0437 (1.0647)	Acc@1 62.500 (62.326)	Acc@5 95.703 (96.078)
after train
test acc: 55.38
Epoche: [181/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [181][0/196]	Time 0.254 (0.254)	Data 0.355 (0.355)	Loss 0.9814 (0.9814)	Acc@1 64.453 (64.453)	Acc@5 96.484 (96.484)
Epoch: [181][64/196]	Time 0.136 (0.157)	Data 0.000 (0.006)	Loss 0.9534 (1.0645)	Acc@1 66.406 (62.139)	Acc@5 98.438 (96.208)
Epoch: [181][128/196]	Time 0.154 (0.152)	Data 0.000 (0.003)	Loss 1.0950 (1.0651)	Acc@1 62.109 (62.373)	Acc@5 94.922 (95.933)
Epoch: [181][192/196]	Time 0.137 (0.150)	Data 0.000 (0.002)	Loss 1.1251 (1.0643)	Acc@1 58.203 (62.545)	Acc@5 95.312 (95.938)
after train
test acc: 54.37
Epoche: [182/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [182][0/196]	Time 0.188 (0.188)	Data 0.422 (0.422)	Loss 1.1339 (1.1339)	Acc@1 60.547 (60.547)	Acc@5 94.141 (94.141)
Epoch: [182][64/196]	Time 0.133 (0.153)	Data 0.000 (0.007)	Loss 1.0936 (1.0621)	Acc@1 60.547 (62.410)	Acc@5 94.141 (96.052)
Epoch: [182][128/196]	Time 0.148 (0.156)	Data 0.000 (0.004)	Loss 1.1444 (1.0585)	Acc@1 58.984 (62.458)	Acc@5 97.266 (96.154)
Epoch: [182][192/196]	Time 0.139 (0.158)	Data 0.000 (0.003)	Loss 1.0774 (1.0540)	Acc@1 63.672 (62.589)	Acc@5 96.094 (96.134)
after train
test acc: 58.15
Epoche: [183/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [183][0/196]	Time 0.201 (0.201)	Data 0.466 (0.466)	Loss 1.0475 (1.0475)	Acc@1 62.500 (62.500)	Acc@5 97.266 (97.266)
Epoch: [183][64/196]	Time 0.187 (0.150)	Data 0.000 (0.008)	Loss 1.1034 (1.0515)	Acc@1 61.328 (63.023)	Acc@5 94.531 (95.895)
Epoch: [183][128/196]	Time 0.134 (0.152)	Data 0.000 (0.004)	Loss 1.0476 (1.0481)	Acc@1 63.672 (63.136)	Acc@5 96.484 (96.009)
Epoch: [183][192/196]	Time 0.126 (0.149)	Data 0.000 (0.003)	Loss 1.0466 (1.0492)	Acc@1 62.891 (63.083)	Acc@5 97.266 (96.011)
after train
test acc: 50.01
Epoche: [184/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [184][0/196]	Time 0.217 (0.217)	Data 0.404 (0.404)	Loss 1.1237 (1.1237)	Acc@1 55.469 (55.469)	Acc@5 96.484 (96.484)
Epoch: [184][64/196]	Time 0.138 (0.155)	Data 0.000 (0.007)	Loss 1.0637 (1.0447)	Acc@1 64.062 (63.161)	Acc@5 95.703 (96.310)
Epoch: [184][128/196]	Time 0.152 (0.153)	Data 0.000 (0.003)	Loss 1.0587 (1.0422)	Acc@1 63.672 (63.414)	Acc@5 95.703 (96.236)
Epoch: [184][192/196]	Time 0.167 (0.154)	Data 0.000 (0.002)	Loss 1.0681 (1.0423)	Acc@1 63.281 (63.273)	Acc@5 94.141 (96.231)
after train
test acc: 55.2
Epoche: [185/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [185][0/196]	Time 0.241 (0.241)	Data 0.344 (0.344)	Loss 1.0156 (1.0156)	Acc@1 61.719 (61.719)	Acc@5 95.703 (95.703)
Epoch: [185][64/196]	Time 0.129 (0.146)	Data 0.000 (0.006)	Loss 0.9428 (1.0523)	Acc@1 67.578 (62.524)	Acc@5 97.656 (96.304)
Epoch: [185][128/196]	Time 0.136 (0.145)	Data 0.000 (0.003)	Loss 1.2103 (1.0465)	Acc@1 58.594 (62.960)	Acc@5 94.141 (96.257)
Epoch: [185][192/196]	Time 0.139 (0.145)	Data 0.000 (0.002)	Loss 1.1461 (1.0493)	Acc@1 60.547 (63.071)	Acc@5 94.531 (96.191)
after train
test acc: 55.98
Epoche: [186/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [186][0/196]	Time 0.260 (0.260)	Data 0.335 (0.335)	Loss 1.0090 (1.0090)	Acc@1 64.062 (64.062)	Acc@5 97.266 (97.266)
Epoch: [186][64/196]	Time 0.124 (0.159)	Data 0.000 (0.005)	Loss 1.1204 (1.0386)	Acc@1 64.062 (63.534)	Acc@5 94.922 (96.322)
Epoch: [186][128/196]	Time 0.155 (0.156)	Data 0.000 (0.003)	Loss 1.1007 (1.0431)	Acc@1 59.375 (63.523)	Acc@5 96.875 (96.227)
Epoch: [186][192/196]	Time 0.159 (0.156)	Data 0.000 (0.002)	Loss 1.1824 (1.0412)	Acc@1 60.547 (63.528)	Acc@5 94.531 (96.211)
after train
test acc: 54.5
Epoche: [187/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [187][0/196]	Time 0.243 (0.243)	Data 0.402 (0.402)	Loss 1.0565 (1.0565)	Acc@1 64.453 (64.453)	Acc@5 96.484 (96.484)
Epoch: [187][64/196]	Time 0.169 (0.153)	Data 0.000 (0.007)	Loss 1.1147 (1.0318)	Acc@1 62.891 (63.756)	Acc@5 94.922 (96.256)
Epoch: [187][128/196]	Time 0.137 (0.150)	Data 0.000 (0.003)	Loss 1.0508 (1.0368)	Acc@1 61.719 (63.548)	Acc@5 95.703 (96.239)
Epoch: [187][192/196]	Time 0.136 (0.149)	Data 0.000 (0.002)	Loss 1.0119 (1.0310)	Acc@1 63.672 (63.684)	Acc@5 95.703 (96.304)
after train
test acc: 46.59
Epoche: [188/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [188][0/196]	Time 0.235 (0.235)	Data 0.368 (0.368)	Loss 1.0529 (1.0529)	Acc@1 62.891 (62.891)	Acc@5 95.312 (95.312)
Epoch: [188][64/196]	Time 0.164 (0.154)	Data 0.000 (0.006)	Loss 1.0279 (1.0247)	Acc@1 66.016 (63.972)	Acc@5 96.094 (96.436)
Epoch: [188][128/196]	Time 0.145 (0.159)	Data 0.000 (0.003)	Loss 1.1158 (1.0259)	Acc@1 60.156 (64.081)	Acc@5 94.531 (96.409)
Epoch: [188][192/196]	Time 0.171 (0.157)	Data 0.000 (0.002)	Loss 1.0747 (1.0285)	Acc@1 60.547 (63.828)	Acc@5 96.484 (96.314)
after train
test acc: 53.82
Epoche: [189/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [189][0/196]	Time 0.248 (0.248)	Data 0.329 (0.329)	Loss 1.0033 (1.0033)	Acc@1 67.969 (67.969)	Acc@5 96.484 (96.484)
Epoch: [189][64/196]	Time 0.153 (0.151)	Data 0.000 (0.005)	Loss 0.8552 (1.0361)	Acc@1 73.828 (63.768)	Acc@5 96.875 (96.232)
Epoch: [189][128/196]	Time 0.142 (0.153)	Data 0.000 (0.003)	Loss 1.1526 (1.0290)	Acc@1 63.672 (63.938)	Acc@5 96.484 (96.333)
Epoch: [189][192/196]	Time 0.147 (0.151)	Data 0.000 (0.002)	Loss 1.0629 (1.0289)	Acc@1 61.719 (63.895)	Acc@5 93.750 (96.306)
after train
test acc: 48.18
Epoche: [190/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [190][0/196]	Time 0.239 (0.239)	Data 0.315 (0.315)	Loss 1.0742 (1.0742)	Acc@1 59.766 (59.766)	Acc@5 96.484 (96.484)
Epoch: [190][64/196]	Time 0.150 (0.159)	Data 0.000 (0.005)	Loss 1.0419 (1.0403)	Acc@1 62.500 (63.419)	Acc@5 96.094 (96.196)
Epoch: [190][128/196]	Time 0.146 (0.155)	Data 0.000 (0.003)	Loss 1.1241 (1.0328)	Acc@1 60.156 (63.611)	Acc@5 95.703 (96.315)
Epoch: [190][192/196]	Time 0.140 (0.154)	Data 0.000 (0.002)	Loss 0.9693 (1.0331)	Acc@1 67.188 (63.559)	Acc@5 95.703 (96.353)
after train
test acc: 57.86
Epoche: [191/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [191][0/196]	Time 0.176 (0.176)	Data 0.443 (0.443)	Loss 1.0785 (1.0785)	Acc@1 67.969 (67.969)	Acc@5 95.703 (95.703)
Epoch: [191][64/196]	Time 0.148 (0.146)	Data 0.000 (0.007)	Loss 0.9942 (1.0169)	Acc@1 62.891 (64.147)	Acc@5 96.484 (96.671)
Epoch: [191][128/196]	Time 0.139 (0.145)	Data 0.000 (0.004)	Loss 1.0450 (1.0216)	Acc@1 64.453 (63.920)	Acc@5 96.094 (96.394)
Epoch: [191][192/196]	Time 0.164 (0.146)	Data 0.000 (0.003)	Loss 1.0250 (1.0244)	Acc@1 60.938 (63.755)	Acc@5 96.875 (96.367)
after train
test acc: 57.27
Epoche: [192/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [192][0/196]	Time 0.244 (0.244)	Data 0.485 (0.485)	Loss 0.9826 (0.9826)	Acc@1 63.281 (63.281)	Acc@5 97.266 (97.266)
Epoch: [192][64/196]	Time 0.148 (0.159)	Data 0.000 (0.008)	Loss 1.0256 (1.0326)	Acc@1 64.062 (63.738)	Acc@5 94.922 (96.520)
Epoch: [192][128/196]	Time 0.184 (0.158)	Data 0.000 (0.004)	Loss 1.1216 (1.0209)	Acc@1 62.891 (63.996)	Acc@5 96.875 (96.542)
Epoch: [192][192/196]	Time 0.144 (0.157)	Data 0.000 (0.003)	Loss 0.9391 (1.0239)	Acc@1 69.531 (63.890)	Acc@5 95.703 (96.395)
after train
test acc: 47.34
Epoche: [193/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [193][0/196]	Time 0.210 (0.210)	Data 0.429 (0.429)	Loss 0.9957 (0.9957)	Acc@1 62.500 (62.500)	Acc@5 97.266 (97.266)
Epoch: [193][64/196]	Time 0.179 (0.152)	Data 0.000 (0.007)	Loss 1.0706 (1.0324)	Acc@1 62.891 (63.480)	Acc@5 98.047 (96.448)
Epoch: [193][128/196]	Time 0.155 (0.152)	Data 0.000 (0.004)	Loss 1.0536 (1.0246)	Acc@1 57.031 (63.835)	Acc@5 98.047 (96.548)
Epoch: [193][192/196]	Time 0.176 (0.155)	Data 0.000 (0.003)	Loss 1.0830 (1.0213)	Acc@1 60.156 (63.927)	Acc@5 96.484 (96.525)
after train
test acc: 60.9
Epoche: [194/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [194][0/196]	Time 0.171 (0.171)	Data 0.368 (0.368)	Loss 0.8680 (0.8680)	Acc@1 71.094 (71.094)	Acc@5 97.656 (97.656)
Epoch: [194][64/196]	Time 0.178 (0.153)	Data 0.000 (0.006)	Loss 0.9727 (1.0095)	Acc@1 64.062 (65.036)	Acc@5 96.484 (96.472)
Epoch: [194][128/196]	Time 0.158 (0.153)	Data 0.000 (0.003)	Loss 0.9115 (1.0142)	Acc@1 65.625 (64.538)	Acc@5 97.656 (96.448)
Epoch: [194][192/196]	Time 0.131 (0.154)	Data 0.000 (0.002)	Loss 0.9412 (1.0128)	Acc@1 66.406 (64.477)	Acc@5 98.828 (96.444)
after train
test acc: 52.97
Epoche: [195/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [195][0/196]	Time 0.187 (0.187)	Data 0.359 (0.359)	Loss 1.1772 (1.1772)	Acc@1 57.031 (57.031)	Acc@5 96.875 (96.875)
Epoch: [195][64/196]	Time 0.175 (0.149)	Data 0.000 (0.006)	Loss 1.0149 (1.0207)	Acc@1 62.500 (63.438)	Acc@5 96.094 (96.785)
Epoch: [195][128/196]	Time 0.127 (0.149)	Data 0.000 (0.003)	Loss 1.0755 (1.0108)	Acc@1 65.625 (64.232)	Acc@5 95.703 (96.593)
Epoch: [195][192/196]	Time 0.144 (0.152)	Data 0.000 (0.002)	Loss 1.1143 (1.0115)	Acc@1 61.719 (64.338)	Acc@5 95.312 (96.549)
after train
test acc: 54.67
Epoche: [196/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [196][0/196]	Time 0.177 (0.177)	Data 0.341 (0.341)	Loss 1.0704 (1.0704)	Acc@1 60.938 (60.938)	Acc@5 94.922 (94.922)
Epoch: [196][64/196]	Time 0.160 (0.144)	Data 0.000 (0.006)	Loss 0.9763 (1.0148)	Acc@1 62.109 (64.099)	Acc@5 97.266 (96.388)
Epoch: [196][128/196]	Time 0.149 (0.144)	Data 0.000 (0.003)	Loss 0.9165 (1.0165)	Acc@1 70.312 (63.969)	Acc@5 96.094 (96.466)
Epoch: [196][192/196]	Time 0.149 (0.147)	Data 0.000 (0.002)	Loss 1.0198 (1.0125)	Acc@1 64.844 (64.103)	Acc@5 96.484 (96.543)
after train
test acc: 49.21
Epoche: [197/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [197][0/196]	Time 0.213 (0.213)	Data 0.363 (0.363)	Loss 0.9579 (0.9579)	Acc@1 62.109 (62.109)	Acc@5 98.047 (98.047)
Epoch: [197][64/196]	Time 0.182 (0.154)	Data 0.000 (0.006)	Loss 1.1110 (1.0120)	Acc@1 60.156 (64.123)	Acc@5 95.312 (96.508)
Epoch: [197][128/196]	Time 0.169 (0.154)	Data 0.000 (0.003)	Loss 0.9806 (1.0133)	Acc@1 62.109 (64.220)	Acc@5 96.875 (96.475)
Epoch: [197][192/196]	Time 0.152 (0.155)	Data 0.000 (0.002)	Loss 1.1121 (1.0087)	Acc@1 60.156 (64.405)	Acc@5 94.922 (96.590)
after train
test acc: 52.23
Epoche: [198/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [198][0/196]	Time 0.191 (0.191)	Data 0.339 (0.339)	Loss 0.8930 (0.8930)	Acc@1 68.359 (68.359)	Acc@5 98.047 (98.047)
Epoch: [198][64/196]	Time 0.132 (0.145)	Data 0.000 (0.006)	Loss 1.0516 (1.0047)	Acc@1 62.109 (64.465)	Acc@5 97.266 (96.520)
Epoch: [198][128/196]	Time 0.169 (0.144)	Data 0.000 (0.003)	Loss 1.0113 (1.0075)	Acc@1 65.234 (64.477)	Acc@5 94.922 (96.484)
Epoch: [198][192/196]	Time 0.148 (0.148)	Data 0.000 (0.002)	Loss 1.0142 (1.0094)	Acc@1 69.531 (64.512)	Acc@5 95.703 (96.430)
after train
test acc: 57.65
Epoche: [199/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [199][0/196]	Time 0.217 (0.217)	Data 0.566 (0.566)	Loss 0.9874 (0.9874)	Acc@1 63.672 (63.672)	Acc@5 96.875 (96.875)
Epoch: [199][64/196]	Time 0.140 (0.158)	Data 0.000 (0.009)	Loss 0.9140 (1.0061)	Acc@1 67.188 (64.369)	Acc@5 97.656 (96.767)
Epoch: [199][128/196]	Time 0.131 (0.162)	Data 0.000 (0.005)	Loss 0.9644 (1.0127)	Acc@1 68.359 (64.150)	Acc@5 96.094 (96.648)
Epoch: [199][192/196]	Time 0.161 (0.160)	Data 0.000 (0.003)	Loss 1.0056 (1.0109)	Acc@1 65.234 (64.332)	Acc@5 97.266 (96.687)
after train
test acc: 44.13
Epoche: [200/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [200][0/196]	Time 0.233 (0.233)	Data 0.372 (0.372)	Loss 0.9503 (0.9503)	Acc@1 64.844 (64.844)	Acc@5 98.438 (98.438)
Epoch: [200][64/196]	Time 0.151 (0.157)	Data 0.000 (0.006)	Loss 1.0352 (0.9975)	Acc@1 63.281 (65.174)	Acc@5 93.750 (96.575)
Epoch: [200][128/196]	Time 0.150 (0.157)	Data 0.000 (0.003)	Loss 0.9796 (1.0009)	Acc@1 64.062 (64.898)	Acc@5 94.922 (96.648)
Epoch: [200][192/196]	Time 0.180 (0.156)	Data 0.000 (0.002)	Loss 0.9915 (1.0010)	Acc@1 63.672 (64.872)	Acc@5 96.094 (96.594)
after train
test acc: 44.25
Epoche: [201/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [201][0/196]	Time 0.193 (0.193)	Data 0.445 (0.445)	Loss 0.9581 (0.9581)	Acc@1 68.359 (68.359)	Acc@5 94.141 (94.141)
Epoch: [201][64/196]	Time 0.166 (0.170)	Data 0.000 (0.007)	Loss 1.0770 (0.9995)	Acc@1 60.156 (64.982)	Acc@5 95.703 (96.550)
Epoch: [201][128/196]	Time 0.175 (0.165)	Data 0.000 (0.004)	Loss 0.9563 (1.0077)	Acc@1 66.016 (64.744)	Acc@5 96.484 (96.496)
Epoch: [201][192/196]	Time 0.160 (0.162)	Data 0.000 (0.003)	Loss 1.0875 (1.0022)	Acc@1 61.719 (64.949)	Acc@5 95.703 (96.539)
after train
test acc: 49.21
Epoche: [202/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [202][0/196]	Time 0.249 (0.249)	Data 0.453 (0.453)	Loss 1.0007 (1.0007)	Acc@1 64.453 (64.453)	Acc@5 98.047 (98.047)
Epoch: [202][64/196]	Time 0.157 (0.158)	Data 0.000 (0.007)	Loss 1.0822 (1.0132)	Acc@1 62.500 (64.303)	Acc@5 96.094 (96.412)
Epoch: [202][128/196]	Time 0.140 (0.156)	Data 0.000 (0.004)	Loss 1.0207 (1.0023)	Acc@1 66.797 (64.629)	Acc@5 97.266 (96.648)
Epoch: [202][192/196]	Time 0.176 (0.157)	Data 0.000 (0.003)	Loss 0.9641 (1.0037)	Acc@1 66.797 (64.589)	Acc@5 96.094 (96.616)
after train
test acc: 52.33
Epoche: [203/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [203][0/196]	Time 0.244 (0.244)	Data 0.349 (0.349)	Loss 0.9696 (0.9696)	Acc@1 64.062 (64.062)	Acc@5 96.875 (96.875)
Epoch: [203][64/196]	Time 0.192 (0.166)	Data 0.000 (0.006)	Loss 0.9701 (0.9959)	Acc@1 67.969 (64.946)	Acc@5 96.484 (96.532)
Epoch: [203][128/196]	Time 0.161 (0.162)	Data 0.000 (0.003)	Loss 0.9412 (0.9942)	Acc@1 64.453 (65.025)	Acc@5 96.875 (96.596)
Epoch: [203][192/196]	Time 0.175 (0.160)	Data 0.000 (0.002)	Loss 0.8576 (0.9940)	Acc@1 69.531 (65.066)	Acc@5 97.656 (96.610)
after train
test acc: 57.74
Epoche: [204/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [204][0/196]	Time 0.243 (0.243)	Data 0.393 (0.393)	Loss 0.9198 (0.9198)	Acc@1 67.969 (67.969)	Acc@5 98.047 (98.047)
Epoch: [204][64/196]	Time 0.141 (0.153)	Data 0.000 (0.006)	Loss 1.0895 (1.0087)	Acc@1 61.719 (64.189)	Acc@5 95.703 (96.749)
Epoch: [204][128/196]	Time 0.152 (0.157)	Data 0.000 (0.003)	Loss 1.0533 (1.0014)	Acc@1 61.719 (64.632)	Acc@5 98.047 (96.730)
Epoch: [204][192/196]	Time 0.150 (0.159)	Data 0.000 (0.002)	Loss 1.0208 (0.9967)	Acc@1 64.844 (64.929)	Acc@5 97.266 (96.644)
after train
test acc: 26.47
Epoche: [205/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [205][0/196]	Time 0.232 (0.232)	Data 0.383 (0.383)	Loss 1.0108 (1.0108)	Acc@1 64.453 (64.453)	Acc@5 95.312 (95.312)
Epoch: [205][64/196]	Time 0.149 (0.156)	Data 0.000 (0.006)	Loss 1.0104 (1.0175)	Acc@1 66.016 (64.621)	Acc@5 96.484 (96.569)
Epoch: [205][128/196]	Time 0.181 (0.156)	Data 0.000 (0.003)	Loss 1.0382 (0.9976)	Acc@1 60.156 (65.150)	Acc@5 96.484 (96.575)
Epoch: [205][192/196]	Time 0.187 (0.157)	Data 0.000 (0.002)	Loss 0.9549 (1.0015)	Acc@1 67.188 (64.824)	Acc@5 97.266 (96.535)
after train
test acc: 53.94
Epoche: [206/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [206][0/196]	Time 0.225 (0.225)	Data 0.549 (0.549)	Loss 0.9763 (0.9763)	Acc@1 62.500 (62.500)	Acc@5 97.266 (97.266)
Epoch: [206][64/196]	Time 0.150 (0.152)	Data 0.000 (0.009)	Loss 0.9683 (0.9841)	Acc@1 67.578 (65.337)	Acc@5 98.047 (96.761)
Epoch: [206][128/196]	Time 0.170 (0.160)	Data 0.000 (0.005)	Loss 1.0015 (0.9929)	Acc@1 63.281 (64.965)	Acc@5 97.266 (96.587)
Epoch: [206][192/196]	Time 0.145 (0.160)	Data 0.000 (0.003)	Loss 0.9592 (0.9898)	Acc@1 66.016 (65.186)	Acc@5 97.266 (96.689)
after train
test acc: 20.47
Epoche: [207/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [207][0/196]	Time 0.240 (0.240)	Data 0.440 (0.440)	Loss 0.9443 (0.9443)	Acc@1 68.750 (68.750)	Acc@5 93.750 (93.750)
Epoch: [207][64/196]	Time 0.145 (0.168)	Data 0.000 (0.007)	Loss 0.9749 (0.9875)	Acc@1 66.016 (65.691)	Acc@5 96.875 (96.623)
Epoch: [207][128/196]	Time 0.195 (0.168)	Data 0.000 (0.004)	Loss 1.0065 (0.9974)	Acc@1 64.844 (64.986)	Acc@5 96.094 (96.599)
Epoch: [207][192/196]	Time 0.143 (0.165)	Data 0.000 (0.003)	Loss 1.1101 (0.9957)	Acc@1 59.375 (65.038)	Acc@5 96.094 (96.652)
after train
test acc: 53.89
Epoche: [208/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [208][0/196]	Time 0.314 (0.314)	Data 0.308 (0.308)	Loss 1.0315 (1.0315)	Acc@1 62.500 (62.500)	Acc@5 96.094 (96.094)
Epoch: [208][64/196]	Time 0.151 (0.172)	Data 0.000 (0.005)	Loss 1.0346 (0.9973)	Acc@1 63.672 (64.898)	Acc@5 97.266 (96.358)
Epoch: [208][128/196]	Time 0.183 (0.165)	Data 0.000 (0.003)	Loss 0.9703 (0.9919)	Acc@1 65.234 (65.038)	Acc@5 95.703 (96.469)
Epoch: [208][192/196]	Time 0.163 (0.164)	Data 0.000 (0.002)	Loss 0.9385 (0.9896)	Acc@1 67.578 (65.087)	Acc@5 97.266 (96.602)
after train
test acc: 36.05
Epoche: [209/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [209][0/196]	Time 0.221 (0.221)	Data 0.521 (0.521)	Loss 0.9392 (0.9392)	Acc@1 69.922 (69.922)	Acc@5 97.266 (97.266)
Epoch: [209][64/196]	Time 0.130 (0.161)	Data 0.000 (0.008)	Loss 1.0267 (0.9799)	Acc@1 62.109 (65.751)	Acc@5 96.875 (96.731)
Epoch: [209][128/196]	Time 0.158 (0.160)	Data 0.000 (0.004)	Loss 1.0136 (0.9903)	Acc@1 67.969 (65.277)	Acc@5 96.484 (96.684)
Epoch: [209][192/196]	Time 0.121 (0.162)	Data 0.000 (0.003)	Loss 0.9830 (0.9894)	Acc@1 63.281 (65.386)	Acc@5 98.047 (96.695)
after train
test acc: 37.82
Epoche: [210/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [210][0/196]	Time 0.193 (0.193)	Data 0.409 (0.409)	Loss 1.0137 (1.0137)	Acc@1 66.797 (66.797)	Acc@5 96.875 (96.875)
Epoch: [210][64/196]	Time 0.145 (0.161)	Data 0.000 (0.007)	Loss 1.0954 (0.9879)	Acc@1 60.547 (65.325)	Acc@5 96.875 (96.677)
Epoch: [210][128/196]	Time 0.167 (0.162)	Data 0.000 (0.004)	Loss 0.9756 (0.9959)	Acc@1 67.578 (65.041)	Acc@5 95.703 (96.684)
Epoch: [210][192/196]	Time 0.149 (0.161)	Data 0.000 (0.002)	Loss 1.0409 (0.9921)	Acc@1 62.109 (65.125)	Acc@5 96.875 (96.699)
after train
test acc: 50.0
Epoche: [211/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [211][0/196]	Time 0.243 (0.243)	Data 0.365 (0.365)	Loss 0.9385 (0.9385)	Acc@1 66.406 (66.406)	Acc@5 97.656 (97.656)
Epoch: [211][64/196]	Time 0.187 (0.160)	Data 0.000 (0.006)	Loss 1.0013 (0.9851)	Acc@1 62.891 (65.264)	Acc@5 97.266 (96.707)
Epoch: [211][128/196]	Time 0.157 (0.162)	Data 0.000 (0.003)	Loss 1.1197 (0.9815)	Acc@1 60.938 (65.498)	Acc@5 96.875 (96.678)
Epoch: [211][192/196]	Time 0.156 (0.162)	Data 0.000 (0.002)	Loss 1.0711 (0.9824)	Acc@1 64.453 (65.530)	Acc@5 97.266 (96.747)
after train
test acc: 53.23
Epoche: [212/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [212][0/196]	Time 0.198 (0.198)	Data 0.514 (0.514)	Loss 1.0175 (1.0175)	Acc@1 66.016 (66.016)	Acc@5 96.875 (96.875)
Epoch: [212][64/196]	Time 0.173 (0.156)	Data 0.000 (0.008)	Loss 0.8860 (0.9758)	Acc@1 69.531 (65.403)	Acc@5 96.875 (96.851)
Epoch: [212][128/196]	Time 0.146 (0.157)	Data 0.000 (0.004)	Loss 0.9420 (0.9813)	Acc@1 66.406 (65.528)	Acc@5 97.266 (96.730)
Epoch: [212][192/196]	Time 0.151 (0.156)	Data 0.000 (0.003)	Loss 0.9347 (0.9811)	Acc@1 67.578 (65.542)	Acc@5 96.875 (96.723)
after train
test acc: 58.65
Epoche: [213/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [213][0/196]	Time 0.220 (0.220)	Data 0.460 (0.460)	Loss 1.0517 (1.0517)	Acc@1 62.109 (62.109)	Acc@5 96.875 (96.875)
Epoch: [213][64/196]	Time 0.179 (0.163)	Data 0.000 (0.007)	Loss 0.9859 (0.9668)	Acc@1 62.500 (65.968)	Acc@5 96.094 (96.671)
Epoch: [213][128/196]	Time 0.126 (0.160)	Data 0.000 (0.004)	Loss 1.1357 (0.9777)	Acc@1 61.328 (65.522)	Acc@5 94.922 (96.814)
Epoch: [213][192/196]	Time 0.156 (0.160)	Data 0.000 (0.003)	Loss 1.0007 (0.9827)	Acc@1 65.234 (65.412)	Acc@5 95.312 (96.662)
after train
test acc: 44.93
Epoche: [214/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [214][0/196]	Time 0.237 (0.237)	Data 0.391 (0.391)	Loss 0.9648 (0.9648)	Acc@1 67.188 (67.188)	Acc@5 94.922 (94.922)
Epoch: [214][64/196]	Time 0.194 (0.162)	Data 0.000 (0.006)	Loss 1.0038 (0.9833)	Acc@1 64.844 (65.655)	Acc@5 98.438 (96.899)
Epoch: [214][128/196]	Time 0.139 (0.160)	Data 0.000 (0.003)	Loss 1.0185 (0.9891)	Acc@1 64.453 (65.316)	Acc@5 97.266 (96.827)
Epoch: [214][192/196]	Time 0.209 (0.160)	Data 0.000 (0.002)	Loss 0.8759 (0.9843)	Acc@1 67.969 (65.449)	Acc@5 97.266 (96.830)
after train
test acc: 51.9
Epoche: [215/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [215][0/196]	Time 0.201 (0.201)	Data 0.403 (0.403)	Loss 0.9639 (0.9639)	Acc@1 64.062 (64.062)	Acc@5 95.703 (95.703)
Epoch: [215][64/196]	Time 0.161 (0.161)	Data 0.000 (0.007)	Loss 0.9950 (0.9709)	Acc@1 64.062 (65.757)	Acc@5 98.047 (96.995)
Epoch: [215][128/196]	Time 0.154 (0.159)	Data 0.000 (0.003)	Loss 1.0169 (0.9729)	Acc@1 62.891 (65.686)	Acc@5 98.047 (96.848)
Epoch: [215][192/196]	Time 0.139 (0.158)	Data 0.000 (0.002)	Loss 0.9852 (0.9742)	Acc@1 65.625 (65.714)	Acc@5 96.484 (96.845)
after train
test acc: 57.01
Epoche: [216/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [216][0/196]	Time 0.219 (0.219)	Data 0.417 (0.417)	Loss 1.0246 (1.0246)	Acc@1 65.625 (65.625)	Acc@5 95.312 (95.312)
Epoch: [216][64/196]	Time 0.140 (0.157)	Data 0.000 (0.007)	Loss 0.9794 (0.9861)	Acc@1 66.016 (65.535)	Acc@5 96.875 (96.496)
Epoch: [216][128/196]	Time 0.141 (0.159)	Data 0.000 (0.004)	Loss 0.9527 (0.9825)	Acc@1 67.188 (65.610)	Acc@5 97.266 (96.666)
Epoch: [216][192/196]	Time 0.131 (0.158)	Data 0.000 (0.002)	Loss 0.8912 (0.9825)	Acc@1 67.578 (65.542)	Acc@5 98.047 (96.622)
after train
test acc: 52.85
Epoche: [217/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [217][0/196]	Time 0.224 (0.224)	Data 0.350 (0.350)	Loss 0.9422 (0.9422)	Acc@1 63.672 (63.672)	Acc@5 94.922 (94.922)
Epoch: [217][64/196]	Time 0.164 (0.157)	Data 0.000 (0.006)	Loss 1.0141 (0.9772)	Acc@1 64.453 (65.859)	Acc@5 94.922 (96.707)
Epoch: [217][128/196]	Time 0.138 (0.160)	Data 0.000 (0.003)	Loss 0.9244 (0.9737)	Acc@1 66.016 (66.097)	Acc@5 98.438 (96.781)
Epoch: [217][192/196]	Time 0.157 (0.158)	Data 0.000 (0.002)	Loss 0.9866 (0.9735)	Acc@1 64.844 (66.052)	Acc@5 97.266 (96.794)
after train
test acc: 50.75
Epoche: [218/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [218][0/196]	Time 0.276 (0.276)	Data 0.362 (0.362)	Loss 0.8810 (0.8810)	Acc@1 70.703 (70.703)	Acc@5 98.047 (98.047)
Epoch: [218][64/196]	Time 0.163 (0.170)	Data 0.000 (0.006)	Loss 0.9027 (0.9750)	Acc@1 65.625 (65.859)	Acc@5 96.094 (96.863)
Epoch: [218][128/196]	Time 0.152 (0.163)	Data 0.000 (0.003)	Loss 0.9109 (0.9685)	Acc@1 68.359 (65.873)	Acc@5 98.047 (96.905)
Epoch: [218][192/196]	Time 0.147 (0.162)	Data 0.000 (0.002)	Loss 0.9871 (0.9674)	Acc@1 64.844 (65.973)	Acc@5 95.703 (96.843)
after train
test acc: 55.73
Epoche: [219/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [219][0/196]	Time 0.254 (0.254)	Data 0.367 (0.367)	Loss 1.0105 (1.0105)	Acc@1 63.672 (63.672)	Acc@5 97.266 (97.266)
Epoch: [219][64/196]	Time 0.153 (0.162)	Data 0.000 (0.006)	Loss 0.9303 (0.9827)	Acc@1 64.844 (65.150)	Acc@5 97.266 (96.695)
Epoch: [219][128/196]	Time 0.160 (0.159)	Data 0.000 (0.003)	Loss 0.9379 (0.9734)	Acc@1 66.797 (65.661)	Acc@5 96.875 (96.793)
Epoch: [219][192/196]	Time 0.150 (0.160)	Data 0.000 (0.002)	Loss 0.9092 (0.9695)	Acc@1 67.188 (65.821)	Acc@5 97.266 (96.794)
after train
test acc: 57.53
Epoche: [220/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [220][0/196]	Time 0.240 (0.240)	Data 0.364 (0.364)	Loss 1.0479 (1.0479)	Acc@1 62.891 (62.891)	Acc@5 96.484 (96.484)
Epoch: [220][64/196]	Time 0.148 (0.159)	Data 0.000 (0.006)	Loss 0.8625 (0.9726)	Acc@1 67.578 (65.721)	Acc@5 98.047 (96.761)
Epoch: [220][128/196]	Time 0.164 (0.159)	Data 0.000 (0.003)	Loss 0.9053 (0.9739)	Acc@1 66.406 (65.716)	Acc@5 97.266 (96.739)
Epoch: [220][192/196]	Time 0.154 (0.158)	Data 0.000 (0.002)	Loss 0.9209 (0.9671)	Acc@1 68.750 (66.066)	Acc@5 97.266 (96.861)
after train
test acc: 48.78
Epoche: [221/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [221][0/196]	Time 0.279 (0.279)	Data 0.409 (0.409)	Loss 1.1679 (1.1679)	Acc@1 64.062 (64.062)	Acc@5 94.141 (94.141)
Epoch: [221][64/196]	Time 0.170 (0.161)	Data 0.000 (0.007)	Loss 1.0880 (0.9688)	Acc@1 62.500 (66.430)	Acc@5 94.531 (96.683)
Epoch: [221][128/196]	Time 0.158 (0.160)	Data 0.000 (0.004)	Loss 1.0267 (0.9644)	Acc@1 63.672 (66.212)	Acc@5 96.875 (96.817)
Epoch: [221][192/196]	Time 0.141 (0.160)	Data 0.000 (0.002)	Loss 1.0342 (0.9700)	Acc@1 62.109 (65.888)	Acc@5 95.312 (96.772)
after train
test acc: 52.03
Epoche: [222/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [222][0/196]	Time 0.278 (0.278)	Data 0.288 (0.288)	Loss 1.0352 (1.0352)	Acc@1 63.672 (63.672)	Acc@5 95.703 (95.703)
Epoch: [222][64/196]	Time 0.154 (0.159)	Data 0.000 (0.005)	Loss 0.9177 (0.9623)	Acc@1 69.141 (66.220)	Acc@5 98.438 (96.725)
Epoch: [222][128/196]	Time 0.194 (0.159)	Data 0.000 (0.003)	Loss 1.0491 (0.9652)	Acc@1 62.500 (66.258)	Acc@5 97.266 (96.787)
Epoch: [222][192/196]	Time 0.165 (0.158)	Data 0.000 (0.002)	Loss 1.0283 (0.9699)	Acc@1 64.062 (66.131)	Acc@5 96.484 (96.713)
after train
test acc: 57.02
Epoche: [223/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [223][0/196]	Time 0.277 (0.277)	Data 0.335 (0.335)	Loss 1.0988 (1.0988)	Acc@1 58.594 (58.594)	Acc@5 95.312 (95.312)
Epoch: [223][64/196]	Time 0.160 (0.160)	Data 0.000 (0.005)	Loss 1.0206 (0.9518)	Acc@1 61.328 (66.490)	Acc@5 97.266 (96.815)
Epoch: [223][128/196]	Time 0.155 (0.160)	Data 0.000 (0.003)	Loss 0.8852 (0.9621)	Acc@1 70.703 (66.309)	Acc@5 98.047 (96.857)
Epoch: [223][192/196]	Time 0.154 (0.158)	Data 0.000 (0.002)	Loss 1.0032 (0.9667)	Acc@1 65.234 (66.042)	Acc@5 98.047 (96.788)
after train
test acc: 54.85
Epoche: [224/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [224][0/196]	Time 0.180 (0.180)	Data 0.383 (0.383)	Loss 0.9152 (0.9152)	Acc@1 69.922 (69.922)	Acc@5 96.484 (96.484)
Epoch: [224][64/196]	Time 0.152 (0.161)	Data 0.000 (0.006)	Loss 0.9996 (0.9765)	Acc@1 64.453 (66.244)	Acc@5 96.484 (96.767)
Epoch: [224][128/196]	Time 0.160 (0.161)	Data 0.000 (0.003)	Loss 1.0503 (0.9726)	Acc@1 61.719 (66.255)	Acc@5 95.703 (96.781)
Epoch: [224][192/196]	Time 0.162 (0.163)	Data 0.000 (0.002)	Loss 0.9383 (0.9745)	Acc@1 66.797 (65.935)	Acc@5 97.266 (96.768)
after train
test acc: 54.84
Epoche: [225/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [225][0/196]	Time 0.239 (0.239)	Data 0.482 (0.482)	Loss 0.9351 (0.9351)	Acc@1 69.531 (69.531)	Acc@5 97.266 (97.266)
Epoch: [225][64/196]	Time 0.154 (0.169)	Data 0.000 (0.008)	Loss 0.9856 (0.9600)	Acc@1 67.188 (66.142)	Acc@5 96.484 (96.965)
Epoch: [225][128/196]	Time 0.162 (0.163)	Data 0.000 (0.004)	Loss 1.0291 (0.9612)	Acc@1 65.234 (66.349)	Acc@5 95.312 (96.860)
Epoch: [225][192/196]	Time 0.139 (0.162)	Data 0.000 (0.003)	Loss 0.9144 (0.9607)	Acc@1 68.750 (66.469)	Acc@5 98.047 (96.812)
after train
test acc: 59.31
Epoche: [226/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [226][0/196]	Time 0.224 (0.224)	Data 0.359 (0.359)	Loss 0.9021 (0.9021)	Acc@1 66.797 (66.797)	Acc@5 96.094 (96.094)
Epoch: [226][64/196]	Time 0.153 (0.162)	Data 0.000 (0.006)	Loss 0.9641 (0.9638)	Acc@1 64.062 (65.811)	Acc@5 95.312 (96.983)
Epoch: [226][128/196]	Time 0.146 (0.162)	Data 0.000 (0.003)	Loss 0.9835 (0.9590)	Acc@1 64.453 (66.312)	Acc@5 96.875 (96.839)
Epoch: [226][192/196]	Time 0.152 (0.161)	Data 0.000 (0.002)	Loss 0.9134 (0.9635)	Acc@1 69.141 (66.184)	Acc@5 97.656 (96.810)
after train
test acc: 43.04
Epoche: [227/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [227][0/196]	Time 0.206 (0.206)	Data 0.474 (0.474)	Loss 1.0026 (1.0026)	Acc@1 67.188 (67.188)	Acc@5 95.312 (95.312)
Epoch: [227][64/196]	Time 0.151 (0.148)	Data 0.000 (0.008)	Loss 0.9155 (0.9643)	Acc@1 68.750 (66.106)	Acc@5 97.656 (96.809)
Epoch: [227][128/196]	Time 0.163 (0.149)	Data 0.000 (0.004)	Loss 0.9250 (0.9672)	Acc@1 69.531 (66.058)	Acc@5 97.266 (96.833)
Epoch: [227][192/196]	Time 0.153 (0.153)	Data 0.000 (0.003)	Loss 1.0208 (0.9654)	Acc@1 67.188 (66.210)	Acc@5 97.266 (96.883)
after train
test acc: 63.69
Epoche: [228/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [228][0/196]	Time 0.360 (0.360)	Data 0.317 (0.317)	Loss 1.0208 (1.0208)	Acc@1 67.188 (67.188)	Acc@5 96.094 (96.094)
Epoch: [228][64/196]	Time 0.154 (0.166)	Data 0.000 (0.005)	Loss 1.0261 (0.9758)	Acc@1 66.797 (65.751)	Acc@5 94.531 (96.713)
Epoch: [228][128/196]	Time 0.152 (0.166)	Data 0.000 (0.003)	Loss 0.9942 (0.9701)	Acc@1 64.062 (65.973)	Acc@5 96.484 (96.760)
Epoch: [228][192/196]	Time 0.149 (0.162)	Data 0.000 (0.002)	Loss 1.0337 (0.9630)	Acc@1 65.234 (66.230)	Acc@5 96.094 (96.766)
after train
test acc: 57.04
Epoche: [229/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [229][0/196]	Time 0.185 (0.185)	Data 0.359 (0.359)	Loss 1.0030 (1.0030)	Acc@1 65.234 (65.234)	Acc@5 96.875 (96.875)
Epoch: [229][64/196]	Time 0.160 (0.162)	Data 0.000 (0.006)	Loss 1.3044 (1.0712)	Acc@1 52.344 (62.344)	Acc@5 94.531 (95.986)
Epoch: [229][128/196]	Time 0.166 (0.159)	Data 0.000 (0.003)	Loss 1.1684 (1.1119)	Acc@1 58.594 (60.732)	Acc@5 95.703 (95.776)
Epoch: [229][192/196]	Time 0.146 (0.158)	Data 0.000 (0.002)	Loss 1.0506 (1.0997)	Acc@1 64.062 (61.110)	Acc@5 96.484 (95.887)
after train
test acc: 59.51
Epoche: [230/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [230][0/196]	Time 0.254 (0.254)	Data 0.421 (0.421)	Loss 1.0512 (1.0512)	Acc@1 60.156 (60.156)	Acc@5 94.922 (94.922)
Epoch: [230][64/196]	Time 0.146 (0.166)	Data 0.000 (0.007)	Loss 1.0466 (0.9956)	Acc@1 65.234 (64.868)	Acc@5 92.969 (96.472)
Epoch: [230][128/196]	Time 0.166 (0.163)	Data 0.000 (0.004)	Loss 1.0114 (0.9833)	Acc@1 65.234 (65.331)	Acc@5 96.094 (96.624)
Epoch: [230][192/196]	Time 0.151 (0.160)	Data 0.000 (0.003)	Loss 1.0294 (0.9801)	Acc@1 65.234 (65.580)	Acc@5 93.750 (96.588)
after train
test acc: 57.89
Epoche: [231/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [231][0/196]	Time 0.203 (0.203)	Data 0.433 (0.433)	Loss 1.1520 (1.1520)	Acc@1 62.500 (62.500)	Acc@5 95.703 (95.703)
Epoch: [231][64/196]	Time 0.139 (0.158)	Data 0.000 (0.007)	Loss 1.0329 (0.9682)	Acc@1 63.281 (65.998)	Acc@5 94.922 (96.803)
Epoch: [231][128/196]	Time 0.162 (0.163)	Data 0.000 (0.004)	Loss 0.9451 (0.9712)	Acc@1 66.797 (65.876)	Acc@5 96.875 (96.836)
Epoch: [231][192/196]	Time 0.178 (0.162)	Data 0.000 (0.003)	Loss 1.0367 (0.9663)	Acc@1 64.062 (66.135)	Acc@5 96.094 (96.855)
after train
test acc: 58.91
Epoche: [232/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [232][0/196]	Time 0.229 (0.229)	Data 0.381 (0.381)	Loss 0.9967 (0.9967)	Acc@1 66.797 (66.797)	Acc@5 94.922 (94.922)
Epoch: [232][64/196]	Time 0.154 (0.159)	Data 0.000 (0.006)	Loss 0.8488 (0.9590)	Acc@1 69.531 (66.352)	Acc@5 98.828 (96.929)
Epoch: [232][128/196]	Time 0.148 (0.158)	Data 0.000 (0.003)	Loss 0.9975 (0.9627)	Acc@1 62.500 (66.016)	Acc@5 96.484 (96.948)
Epoch: [232][192/196]	Time 0.140 (0.157)	Data 0.000 (0.002)	Loss 0.9252 (0.9680)	Acc@1 65.625 (65.779)	Acc@5 98.828 (96.857)
after train
test acc: 44.87
Epoche: [233/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [233][0/196]	Time 0.266 (0.266)	Data 0.351 (0.351)	Loss 0.9225 (0.9225)	Acc@1 65.234 (65.234)	Acc@5 96.875 (96.875)
Epoch: [233][64/196]	Time 0.183 (0.161)	Data 0.000 (0.006)	Loss 0.9593 (0.9627)	Acc@1 67.188 (66.094)	Acc@5 97.656 (96.971)
Epoch: [233][128/196]	Time 0.249 (0.158)	Data 0.000 (0.003)	Loss 0.9270 (0.9530)	Acc@1 65.234 (66.430)	Acc@5 96.875 (96.993)
Epoch: [233][192/196]	Time 0.139 (0.158)	Data 0.000 (0.002)	Loss 1.1075 (0.9652)	Acc@1 57.422 (65.929)	Acc@5 96.094 (96.830)
after train
test acc: 45.68
Epoche: [234/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [234][0/196]	Time 0.268 (0.268)	Data 0.346 (0.346)	Loss 1.0452 (1.0452)	Acc@1 62.891 (62.891)	Acc@5 96.484 (96.484)
Epoch: [234][64/196]	Time 0.146 (0.152)	Data 0.000 (0.006)	Loss 0.9941 (0.9724)	Acc@1 65.234 (65.907)	Acc@5 96.484 (96.809)
Epoch: [234][128/196]	Time 0.162 (0.154)	Data 0.000 (0.003)	Loss 1.1017 (0.9694)	Acc@1 64.062 (65.898)	Acc@5 96.484 (96.875)
Epoch: [234][192/196]	Time 0.158 (0.154)	Data 0.000 (0.002)	Loss 0.9221 (0.9727)	Acc@1 70.703 (65.880)	Acc@5 98.047 (96.770)
after train
test acc: 35.41
Epoche: [235/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [235][0/196]	Time 0.226 (0.226)	Data 0.445 (0.445)	Loss 1.0203 (1.0203)	Acc@1 62.891 (62.891)	Acc@5 97.266 (97.266)
Epoch: [235][64/196]	Time 0.164 (0.158)	Data 0.000 (0.007)	Loss 0.8806 (0.9675)	Acc@1 68.359 (65.913)	Acc@5 96.875 (96.743)
Epoch: [235][128/196]	Time 0.166 (0.162)	Data 0.000 (0.004)	Loss 0.9583 (0.9631)	Acc@1 63.672 (66.131)	Acc@5 96.484 (96.778)
Epoch: [235][192/196]	Time 0.159 (0.160)	Data 0.000 (0.003)	Loss 1.0005 (0.9584)	Acc@1 61.328 (66.408)	Acc@5 96.484 (96.820)
after train
test acc: 54.94
Epoche: [236/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [236][0/196]	Time 0.218 (0.218)	Data 0.334 (0.334)	Loss 0.9944 (0.9944)	Acc@1 66.797 (66.797)	Acc@5 94.922 (94.922)
Epoch: [236][64/196]	Time 0.135 (0.162)	Data 0.000 (0.005)	Loss 0.9384 (0.9385)	Acc@1 68.359 (67.085)	Acc@5 97.656 (97.007)
Epoch: [236][128/196]	Time 0.142 (0.157)	Data 0.000 (0.003)	Loss 1.0218 (0.9548)	Acc@1 64.844 (66.558)	Acc@5 95.312 (96.814)
Epoch: [236][192/196]	Time 0.154 (0.158)	Data 0.000 (0.002)	Loss 0.9608 (0.9608)	Acc@1 67.969 (66.552)	Acc@5 96.875 (96.764)
after train
test acc: 61.21
Epoche: [237/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [237][0/196]	Time 0.214 (0.214)	Data 0.349 (0.349)	Loss 0.9572 (0.9572)	Acc@1 66.016 (66.016)	Acc@5 97.656 (97.656)
Epoch: [237][64/196]	Time 0.152 (0.164)	Data 0.000 (0.006)	Loss 1.0814 (0.9630)	Acc@1 59.766 (66.376)	Acc@5 95.703 (97.019)
Epoch: [237][128/196]	Time 0.184 (0.164)	Data 0.000 (0.003)	Loss 1.0550 (0.9614)	Acc@1 61.719 (66.270)	Acc@5 96.484 (96.957)
Epoch: [237][192/196]	Time 0.150 (0.162)	Data 0.000 (0.002)	Loss 0.9778 (0.9628)	Acc@1 67.188 (66.169)	Acc@5 96.875 (96.932)
after train
test acc: 60.45
Epoche: [238/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [238][0/196]	Time 0.193 (0.193)	Data 0.443 (0.443)	Loss 0.9354 (0.9354)	Acc@1 66.016 (66.016)	Acc@5 97.656 (97.656)
Epoch: [238][64/196]	Time 0.167 (0.156)	Data 0.000 (0.007)	Loss 0.9605 (0.9651)	Acc@1 66.406 (66.526)	Acc@5 96.484 (96.743)
Epoch: [238][128/196]	Time 0.168 (0.161)	Data 0.000 (0.004)	Loss 0.9284 (0.9562)	Acc@1 64.453 (66.803)	Acc@5 98.047 (96.836)
Epoch: [238][192/196]	Time 0.146 (0.160)	Data 0.000 (0.003)	Loss 0.8752 (0.9527)	Acc@1 66.406 (66.756)	Acc@5 96.484 (96.897)
after train
test acc: 60.38
Epoche: [239/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [239][0/196]	Time 0.239 (0.239)	Data 0.360 (0.360)	Loss 0.8477 (0.8477)	Acc@1 70.703 (70.703)	Acc@5 96.875 (96.875)
Epoch: [239][64/196]	Time 0.144 (0.154)	Data 0.000 (0.006)	Loss 0.9234 (0.9619)	Acc@1 69.531 (66.569)	Acc@5 97.266 (96.785)
Epoch: [239][128/196]	Time 0.150 (0.156)	Data 0.000 (0.003)	Loss 0.9460 (0.9547)	Acc@1 64.453 (66.679)	Acc@5 96.094 (96.881)
Epoch: [239][192/196]	Time 0.151 (0.155)	Data 0.000 (0.002)	Loss 1.0558 (0.9563)	Acc@1 63.672 (66.469)	Acc@5 94.141 (96.818)
after train
test acc: 50.02
Epoche: [240/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [240][0/196]	Time 0.176 (0.176)	Data 0.472 (0.472)	Loss 0.8942 (0.8942)	Acc@1 70.312 (70.312)	Acc@5 96.875 (96.875)
Epoch: [240][64/196]	Time 0.157 (0.166)	Data 0.000 (0.008)	Loss 0.7979 (0.9501)	Acc@1 71.875 (66.605)	Acc@5 98.047 (96.923)
Epoch: [240][128/196]	Time 0.177 (0.164)	Data 0.000 (0.004)	Loss 0.8365 (0.9566)	Acc@1 69.922 (66.555)	Acc@5 97.656 (96.854)
Epoch: [240][192/196]	Time 0.155 (0.161)	Data 0.000 (0.003)	Loss 1.0612 (0.9560)	Acc@1 64.062 (66.580)	Acc@5 95.312 (96.881)
after train
test acc: 61.18
[INFO] Storing checkpoint...
Max memory: 29.5321088
