no display found. Using non-interactive Agg backend
[3, 3, 3]
[4, 8, 16]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 120; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/room2x2/model.nn; checkpoint: ./output/experimente4/room232; saveModell: True; LR: 0.1
random number: 6195
Files already downloaded and verified
==> Resuming from checkpoint..
Start epoch: 121
First Lr: 0.1
Startepoche: 121
deeper epoch: 0
Epoche: [121/240]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.
  warnings.warn("Setting attributes on ParameterList is not supported.")
Epoch: [121][0/196]	Time 0.169 (0.169)	Data 0.541 (0.541)	Loss 0.6659 (0.6659)	Acc@1 78.125 (78.125)	Acc@5 98.047 (98.047)
Epoch: [121][64/196]	Time 0.130 (0.110)	Data 0.000 (0.009)	Loss 0.6273 (0.6745)	Acc@1 77.344 (76.671)	Acc@5 99.219 (98.630)
Epoch: [121][128/196]	Time 0.106 (0.108)	Data 0.000 (0.004)	Loss 0.6974 (0.6762)	Acc@1 74.609 (76.457)	Acc@5 99.219 (98.559)
Epoch: [121][192/196]	Time 0.106 (0.108)	Data 0.000 (0.003)	Loss 0.6092 (0.6771)	Acc@1 77.734 (76.368)	Acc@5 97.656 (98.543)
after train
test acc: 75.35


now deeper1
deep2: True
len param: 12
Block: 0
Block: 1
Block: 2
Block: 3
Block: 4
Block: 0
Block: 1
Block: 2
Block: 3
Block: 4
Block: 0
Block: 1
Block: 2
Block: 3
Block: 4
archNums: [[1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [2, 1, 1, 1, 1]]
len paramList: 15
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=16, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
)
args.layersInBlock: 1
module: Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Size of Weight: torch.Size([4, 3, 3, 3])
module: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Size of Weight: torch.Size([4])
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Size of Weight: torch.Size([8, 4, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Sequential: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 4, 1, 1])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Size of Weight: torch.Size([16, 8, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 8, 1, 1])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=16, out_features=10, bias=True)
Size of Weight: torch.Size([10, 16])
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=16, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
)
Epoche: [122/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [122][0/196]	Time 0.168 (0.168)	Data 0.431 (0.431)	Loss 1.7569 (1.7569)	Acc@1 42.578 (42.578)	Acc@5 86.719 (86.719)
Epoch: [122][64/196]	Time 0.140 (0.156)	Data 0.000 (0.007)	Loss 1.8637 (1.9880)	Acc@1 28.906 (26.755)	Acc@5 81.250 (74.736)
Epoch: [122][128/196]	Time 0.167 (0.158)	Data 0.000 (0.004)	Loss 1.6625 (1.8845)	Acc@1 36.719 (28.861)	Acc@5 89.453 (80.493)
Epoch: [122][192/196]	Time 0.141 (0.154)	Data 0.000 (0.002)	Loss 1.5680 (1.8170)	Acc@1 41.016 (31.398)	Acc@5 92.188 (83.250)
after train
test acc: 27.64
Epoche: [123/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [123][0/196]	Time 0.159 (0.159)	Data 0.513 (0.513)	Loss 1.6533 (1.6533)	Acc@1 33.984 (33.984)	Acc@5 92.969 (92.969)
Epoch: [123][64/196]	Time 0.168 (0.148)	Data 0.000 (0.008)	Loss 1.4901 (1.5488)	Acc@1 48.438 (42.656)	Acc@5 90.625 (91.388)
Epoch: [123][128/196]	Time 0.239 (0.155)	Data 0.000 (0.004)	Loss 1.5136 (1.5154)	Acc@1 46.094 (44.404)	Acc@5 87.891 (91.748)
Epoch: [123][192/196]	Time 0.269 (0.183)	Data 0.000 (0.003)	Loss 1.4255 (1.4838)	Acc@1 48.438 (45.802)	Acc@5 92.969 (92.048)
after train
test acc: 20.89
Epoche: [124/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [124][0/196]	Time 0.319 (0.319)	Data 0.461 (0.461)	Loss 1.4296 (1.4296)	Acc@1 46.094 (46.094)	Acc@5 92.969 (92.969)
Epoch: [124][64/196]	Time 0.144 (0.224)	Data 0.000 (0.007)	Loss 1.2973 (1.3669)	Acc@1 47.656 (50.222)	Acc@5 95.312 (93.504)
Epoch: [124][128/196]	Time 0.214 (0.231)	Data 0.000 (0.004)	Loss 1.3885 (1.3417)	Acc@1 51.562 (51.460)	Acc@5 92.578 (93.820)
Epoch: [124][192/196]	Time 0.250 (0.235)	Data 0.000 (0.003)	Loss 1.2237 (1.3309)	Acc@1 56.250 (51.945)	Acc@5 95.312 (93.869)
after train
test acc: 37.35
Epoche: [125/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [125][0/196]	Time 0.460 (0.460)	Data 0.426 (0.426)	Loss 1.2246 (1.2246)	Acc@1 55.469 (55.469)	Acc@5 95.312 (95.312)
Epoch: [125][64/196]	Time 0.245 (0.350)	Data 0.000 (0.007)	Loss 1.1946 (1.2607)	Acc@1 56.250 (55.234)	Acc@5 95.312 (94.291)
Epoch: [125][128/196]	Time 0.394 (0.346)	Data 0.000 (0.004)	Loss 1.1291 (1.2416)	Acc@1 57.031 (55.605)	Acc@5 96.484 (94.683)
Epoch: [125][192/196]	Time 0.337 (0.350)	Data 0.000 (0.003)	Loss 1.2324 (1.2259)	Acc@1 50.781 (56.161)	Acc@5 96.094 (94.823)
after train
test acc: 49.56
Epoche: [126/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [126][0/196]	Time 0.490 (0.490)	Data 0.605 (0.605)	Loss 1.3005 (1.3005)	Acc@1 56.641 (56.641)	Acc@5 92.969 (92.969)
Epoch: [126][64/196]	Time 0.290 (0.474)	Data 0.000 (0.010)	Loss 1.0720 (1.1796)	Acc@1 61.328 (57.740)	Acc@5 96.875 (95.276)
Epoch: [126][128/196]	Time 0.370 (0.476)	Data 0.000 (0.005)	Loss 1.1837 (1.1745)	Acc@1 58.594 (57.879)	Acc@5 94.531 (95.370)
Epoch: [126][192/196]	Time 0.518 (0.476)	Data 0.000 (0.004)	Loss 1.0672 (1.1601)	Acc@1 62.500 (58.414)	Acc@5 98.438 (95.519)
after train
test acc: 32.55
Epoche: [127/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [127][0/196]	Time 0.459 (0.459)	Data 0.612 (0.612)	Loss 1.2269 (1.2269)	Acc@1 57.812 (57.812)	Acc@5 92.969 (92.969)
Epoch: [127][64/196]	Time 0.412 (0.469)	Data 0.000 (0.010)	Loss 1.0824 (1.1271)	Acc@1 58.984 (60.036)	Acc@5 96.484 (95.637)
Epoch: [127][128/196]	Time 0.614 (0.474)	Data 0.000 (0.005)	Loss 1.0397 (1.1221)	Acc@1 63.672 (59.941)	Acc@5 96.484 (95.927)
Epoch: [127][192/196]	Time 0.412 (0.473)	Data 0.000 (0.004)	Loss 1.0601 (1.1138)	Acc@1 62.891 (60.294)	Acc@5 96.094 (95.930)
after train
test acc: 15.33
Epoche: [128/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [128][0/196]	Time 0.542 (0.542)	Data 0.500 (0.500)	Loss 1.0648 (1.0648)	Acc@1 61.328 (61.328)	Acc@5 94.922 (94.922)
Epoch: [128][64/196]	Time 0.545 (0.485)	Data 0.000 (0.009)	Loss 0.9894 (1.0615)	Acc@1 67.188 (62.218)	Acc@5 96.484 (96.472)
Epoch: [128][128/196]	Time 0.339 (0.478)	Data 0.000 (0.005)	Loss 1.0476 (1.0736)	Acc@1 61.719 (62.012)	Acc@5 96.484 (96.285)
Epoch: [128][192/196]	Time 0.519 (0.480)	Data 0.000 (0.004)	Loss 1.1971 (1.0732)	Acc@1 58.984 (61.913)	Acc@5 95.312 (96.185)
after train
test acc: 22.85
Epoche: [129/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [129][0/196]	Time 0.458 (0.458)	Data 0.718 (0.718)	Loss 1.0174 (1.0174)	Acc@1 64.844 (64.844)	Acc@5 96.484 (96.484)
Epoch: [129][64/196]	Time 0.591 (0.485)	Data 0.000 (0.014)	Loss 1.1777 (1.0462)	Acc@1 54.688 (62.494)	Acc@5 94.922 (96.394)
Epoch: [129][128/196]	Time 0.582 (0.497)	Data 0.000 (0.008)	Loss 1.0092 (1.0540)	Acc@1 63.281 (62.500)	Acc@5 97.266 (96.445)
Epoch: [129][192/196]	Time 0.590 (0.512)	Data 0.000 (0.006)	Loss 1.0684 (1.0530)	Acc@1 65.234 (62.573)	Acc@5 94.922 (96.412)
after train
test acc: 21.7
Epoche: [130/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [130][0/196]	Time 0.705 (0.705)	Data 0.599 (0.599)	Loss 0.9477 (0.9477)	Acc@1 66.406 (66.406)	Acc@5 96.484 (96.484)
Epoch: [130][64/196]	Time 0.554 (0.522)	Data 0.000 (0.011)	Loss 1.0530 (1.0651)	Acc@1 59.766 (62.145)	Acc@5 96.484 (96.304)
Epoch: [130][128/196]	Time 0.621 (0.516)	Data 0.000 (0.006)	Loss 0.9789 (1.0504)	Acc@1 65.234 (62.836)	Acc@5 98.047 (96.391)
Epoch: [130][192/196]	Time 0.562 (0.505)	Data 0.000 (0.005)	Loss 0.9524 (1.0454)	Acc@1 67.188 (62.791)	Acc@5 98.438 (96.420)
after train
test acc: 38.64
Epoche: [131/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [131][0/196]	Time 0.500 (0.500)	Data 0.609 (0.609)	Loss 0.9576 (0.9576)	Acc@1 61.719 (61.719)	Acc@5 95.703 (95.703)
Epoch: [131][64/196]	Time 0.591 (0.498)	Data 0.000 (0.011)	Loss 0.9735 (1.0403)	Acc@1 65.625 (62.668)	Acc@5 96.484 (96.334)
Epoch: [131][128/196]	Time 0.561 (0.491)	Data 0.000 (0.006)	Loss 1.1312 (1.0339)	Acc@1 60.547 (63.042)	Acc@5 96.484 (96.493)
Epoch: [131][192/196]	Time 0.485 (0.489)	Data 0.000 (0.004)	Loss 0.9439 (1.0296)	Acc@1 66.406 (63.241)	Acc@5 96.875 (96.553)
after train
test acc: 49.95
Epoche: [132/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [132][0/196]	Time 0.489 (0.489)	Data 0.766 (0.766)	Loss 0.9873 (0.9873)	Acc@1 66.406 (66.406)	Acc@5 97.656 (97.656)
Epoch: [132][64/196]	Time 0.504 (0.497)	Data 0.000 (0.014)	Loss 1.0067 (1.0035)	Acc@1 61.719 (64.291)	Acc@5 98.047 (96.599)
Epoch: [132][128/196]	Time 0.502 (0.499)	Data 0.000 (0.007)	Loss 1.0139 (1.0015)	Acc@1 61.328 (64.347)	Acc@5 97.656 (96.672)
Epoch: [132][192/196]	Time 0.494 (0.495)	Data 0.000 (0.005)	Loss 0.9844 (1.0008)	Acc@1 63.672 (64.342)	Acc@5 97.266 (96.707)
after train
test acc: 46.76
Epoche: [133/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [133][0/196]	Time 0.810 (0.810)	Data 0.709 (0.709)	Loss 0.9409 (0.9409)	Acc@1 67.969 (67.969)	Acc@5 96.875 (96.875)
Epoch: [133][64/196]	Time 0.509 (0.477)	Data 0.000 (0.013)	Loss 1.0339 (1.0042)	Acc@1 64.844 (64.249)	Acc@5 96.484 (96.857)
Epoch: [133][128/196]	Time 0.419 (0.490)	Data 0.001 (0.007)	Loss 0.8971 (0.9875)	Acc@1 70.312 (65.016)	Acc@5 96.484 (96.848)
Epoch: [133][192/196]	Time 0.463 (0.490)	Data 0.000 (0.005)	Loss 0.9253 (0.9947)	Acc@1 68.359 (64.751)	Acc@5 97.266 (96.774)
after train
test acc: 19.87
Epoche: [134/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [134][0/196]	Time 0.628 (0.628)	Data 0.681 (0.681)	Loss 0.9119 (0.9119)	Acc@1 67.578 (67.578)	Acc@5 98.828 (98.828)
Epoch: [134][64/196]	Time 0.433 (0.521)	Data 0.000 (0.012)	Loss 1.0344 (1.0022)	Acc@1 64.453 (64.609)	Acc@5 97.266 (96.749)
Epoch: [134][128/196]	Time 0.492 (0.494)	Data 0.000 (0.006)	Loss 1.0171 (0.9912)	Acc@1 63.672 (64.901)	Acc@5 96.484 (96.730)
Epoch: [134][192/196]	Time 0.730 (0.503)	Data 0.000 (0.005)	Loss 0.9458 (0.9837)	Acc@1 66.797 (65.157)	Acc@5 96.875 (96.754)
after train
test acc: 58.42
Epoche: [135/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [135][0/196]	Time 0.589 (0.589)	Data 0.911 (0.911)	Loss 1.0109 (1.0109)	Acc@1 65.625 (65.625)	Acc@5 96.484 (96.484)
Epoch: [135][64/196]	Time 0.544 (0.503)	Data 0.000 (0.015)	Loss 0.9844 (0.9847)	Acc@1 65.625 (65.288)	Acc@5 96.484 (96.785)
Epoch: [135][128/196]	Time 0.455 (0.496)	Data 0.000 (0.008)	Loss 1.0151 (0.9859)	Acc@1 60.547 (65.395)	Acc@5 96.875 (96.787)
Epoch: [135][192/196]	Time 0.450 (0.488)	Data 0.000 (0.006)	Loss 0.9073 (0.9845)	Acc@1 69.141 (65.469)	Acc@5 98.438 (96.796)
after train
test acc: 35.0
Epoche: [136/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [136][0/196]	Time 1.024 (1.024)	Data 0.937 (0.937)	Loss 0.8469 (0.8469)	Acc@1 68.750 (68.750)	Acc@5 97.656 (97.656)
Epoch: [136][64/196]	Time 0.509 (0.521)	Data 0.000 (0.015)	Loss 0.9616 (0.9764)	Acc@1 69.141 (65.619)	Acc@5 97.656 (96.869)
Epoch: [136][128/196]	Time 0.398 (0.516)	Data 0.000 (0.008)	Loss 0.8855 (0.9728)	Acc@1 67.969 (65.913)	Acc@5 99.219 (96.814)
Epoch: [136][192/196]	Time 0.473 (0.509)	Data 0.000 (0.006)	Loss 0.9462 (0.9714)	Acc@1 67.969 (65.957)	Acc@5 98.047 (96.802)
after train
test acc: 49.93
Epoche: [137/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [137][0/196]	Time 0.418 (0.418)	Data 0.976 (0.976)	Loss 1.0062 (1.0062)	Acc@1 64.062 (64.062)	Acc@5 95.703 (95.703)
Epoch: [137][64/196]	Time 0.469 (0.448)	Data 0.000 (0.016)	Loss 0.9140 (0.9712)	Acc@1 68.359 (65.835)	Acc@5 98.047 (96.851)
Epoch: [137][128/196]	Time 0.415 (0.461)	Data 0.000 (0.009)	Loss 1.1017 (0.9617)	Acc@1 60.938 (66.273)	Acc@5 97.266 (96.902)
Epoch: [137][192/196]	Time 0.537 (0.469)	Data 0.000 (0.006)	Loss 0.8312 (0.9567)	Acc@1 65.625 (66.439)	Acc@5 98.438 (96.990)
after train
test acc: 47.98
Epoche: [138/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [138][0/196]	Time 0.728 (0.728)	Data 0.782 (0.782)	Loss 1.1245 (1.1245)	Acc@1 57.422 (57.422)	Acc@5 94.922 (94.922)
Epoch: [138][64/196]	Time 0.374 (0.503)	Data 0.000 (0.013)	Loss 0.9661 (0.9593)	Acc@1 62.500 (66.280)	Acc@5 97.656 (96.737)
Epoch: [138][128/196]	Time 0.414 (0.497)	Data 0.000 (0.007)	Loss 1.0765 (0.9600)	Acc@1 65.234 (66.403)	Acc@5 93.750 (96.814)
Epoch: [138][192/196]	Time 0.565 (0.494)	Data 0.000 (0.005)	Loss 1.0961 (0.9571)	Acc@1 62.500 (66.542)	Acc@5 95.703 (96.843)
after train
test acc: 39.21
Epoche: [139/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [139][0/196]	Time 0.509 (0.509)	Data 0.709 (0.709)	Loss 0.9762 (0.9762)	Acc@1 65.625 (65.625)	Acc@5 96.094 (96.094)
Epoch: [139][64/196]	Time 0.406 (0.469)	Data 0.000 (0.013)	Loss 0.8900 (0.9515)	Acc@1 70.312 (66.791)	Acc@5 98.047 (96.935)
Epoch: [139][128/196]	Time 0.384 (0.482)	Data 0.000 (0.007)	Loss 0.9456 (0.9412)	Acc@1 66.406 (67.312)	Acc@5 97.656 (96.990)
Epoch: [139][192/196]	Time 0.409 (0.486)	Data 0.000 (0.005)	Loss 0.9055 (0.9391)	Acc@1 67.188 (67.285)	Acc@5 98.047 (96.978)
after train
test acc: 41.3
Epoche: [140/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [140][0/196]	Time 0.532 (0.532)	Data 0.726 (0.726)	Loss 0.9579 (0.9579)	Acc@1 65.234 (65.234)	Acc@5 96.875 (96.875)
Epoch: [140][64/196]	Time 0.480 (0.490)	Data 0.000 (0.012)	Loss 0.9480 (0.9220)	Acc@1 67.188 (67.897)	Acc@5 97.656 (97.067)
Epoch: [140][128/196]	Time 0.494 (0.483)	Data 0.000 (0.007)	Loss 1.0229 (0.9273)	Acc@1 69.141 (67.645)	Acc@5 96.484 (97.063)
Epoch: [140][192/196]	Time 0.421 (0.491)	Data 0.000 (0.005)	Loss 0.9106 (0.9241)	Acc@1 69.531 (67.756)	Acc@5 97.656 (97.154)
after train
test acc: 40.44
Epoche: [141/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [141][0/196]	Time 0.599 (0.599)	Data 0.859 (0.859)	Loss 0.8580 (0.8580)	Acc@1 69.922 (69.922)	Acc@5 98.438 (98.438)
Epoch: [141][64/196]	Time 0.426 (0.487)	Data 0.000 (0.015)	Loss 0.8922 (0.9262)	Acc@1 69.531 (67.867)	Acc@5 97.656 (97.236)
Epoch: [141][128/196]	Time 0.391 (0.503)	Data 0.000 (0.008)	Loss 0.9190 (0.9189)	Acc@1 67.969 (67.954)	Acc@5 96.875 (97.154)
Epoch: [141][192/196]	Time 0.414 (0.504)	Data 0.000 (0.006)	Loss 0.9287 (0.9154)	Acc@1 64.844 (67.898)	Acc@5 96.875 (97.237)
after train
test acc: 51.35
Epoche: [142/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [142][0/196]	Time 0.564 (0.564)	Data 0.726 (0.726)	Loss 0.9043 (0.9043)	Acc@1 66.406 (66.406)	Acc@5 98.438 (98.438)
Epoch: [142][64/196]	Time 0.597 (0.471)	Data 0.000 (0.012)	Loss 0.8474 (0.9296)	Acc@1 69.922 (68.053)	Acc@5 98.047 (97.037)
Epoch: [142][128/196]	Time 0.558 (0.486)	Data 0.000 (0.007)	Loss 0.8611 (0.9148)	Acc@1 70.312 (68.502)	Acc@5 97.266 (97.199)
Epoch: [142][192/196]	Time 0.316 (0.490)	Data 0.000 (0.005)	Loss 0.9218 (0.9096)	Acc@1 67.969 (68.560)	Acc@5 97.656 (97.203)
after train
test acc: 53.72
Epoche: [143/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [143][0/196]	Time 0.719 (0.719)	Data 0.709 (0.709)	Loss 0.8406 (0.8406)	Acc@1 70.312 (70.312)	Acc@5 98.828 (98.828)
Epoch: [143][64/196]	Time 0.477 (0.493)	Data 0.000 (0.013)	Loss 0.9159 (0.8958)	Acc@1 67.578 (68.696)	Acc@5 98.047 (97.410)
Epoch: [143][128/196]	Time 0.306 (0.486)	Data 0.000 (0.007)	Loss 0.9465 (0.8988)	Acc@1 69.141 (68.583)	Acc@5 97.656 (97.287)
Epoch: [143][192/196]	Time 0.512 (0.492)	Data 0.000 (0.005)	Loss 0.9851 (0.8962)	Acc@1 66.016 (68.665)	Acc@5 96.875 (97.306)
after train
test acc: 48.47
Epoche: [144/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [144][0/196]	Time 0.685 (0.685)	Data 0.645 (0.645)	Loss 0.7650 (0.7650)	Acc@1 73.828 (73.828)	Acc@5 98.828 (98.828)
Epoch: [144][64/196]	Time 0.606 (0.489)	Data 0.000 (0.012)	Loss 0.9620 (0.9023)	Acc@1 64.844 (68.684)	Acc@5 96.484 (97.368)
Epoch: [144][128/196]	Time 0.541 (0.480)	Data 0.016 (0.007)	Loss 0.7594 (0.8927)	Acc@1 74.219 (68.980)	Acc@5 97.656 (97.353)
Epoch: [144][192/196]	Time 0.567 (0.484)	Data 0.000 (0.005)	Loss 0.9340 (0.8889)	Acc@1 67.578 (69.039)	Acc@5 96.875 (97.365)
after train
test acc: 34.0
Epoche: [145/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [145][0/196]	Time 0.462 (0.462)	Data 0.746 (0.746)	Loss 0.9325 (0.9325)	Acc@1 69.531 (69.531)	Acc@5 97.266 (97.266)
Epoch: [145][64/196]	Time 0.512 (0.475)	Data 0.000 (0.012)	Loss 0.9067 (0.8812)	Acc@1 66.797 (69.387)	Acc@5 96.484 (97.284)
Epoch: [145][128/196]	Time 0.364 (0.485)	Data 0.000 (0.007)	Loss 0.8623 (0.8849)	Acc@1 70.703 (69.138)	Acc@5 97.266 (97.387)
Epoch: [145][192/196]	Time 0.592 (0.499)	Data 0.000 (0.005)	Loss 0.9148 (0.8817)	Acc@1 71.094 (69.274)	Acc@5 96.875 (97.351)
after train
test acc: 36.78
Epoche: [146/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [146][0/196]	Time 0.514 (0.514)	Data 0.779 (0.779)	Loss 1.0816 (1.0816)	Acc@1 63.672 (63.672)	Acc@5 96.484 (96.484)
Epoch: [146][64/196]	Time 0.473 (0.459)	Data 0.000 (0.013)	Loss 0.8490 (0.8954)	Acc@1 74.219 (68.948)	Acc@5 97.266 (97.290)
Epoch: [146][128/196]	Time 0.241 (0.468)	Data 0.000 (0.008)	Loss 0.7884 (0.8787)	Acc@1 72.656 (69.465)	Acc@5 98.438 (97.481)
Epoch: [146][192/196]	Time 0.447 (0.483)	Data 0.000 (0.005)	Loss 0.7923 (0.8797)	Acc@1 72.656 (69.379)	Acc@5 98.438 (97.421)
after train
test acc: 46.79
Epoche: [147/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [147][0/196]	Time 0.673 (0.673)	Data 0.715 (0.715)	Loss 0.8549 (0.8549)	Acc@1 70.312 (70.312)	Acc@5 96.484 (96.484)
Epoch: [147][64/196]	Time 0.519 (0.486)	Data 0.000 (0.012)	Loss 0.8432 (0.8917)	Acc@1 71.094 (69.183)	Acc@5 97.656 (97.278)
Epoch: [147][128/196]	Time 0.513 (0.486)	Data 0.000 (0.007)	Loss 0.8799 (0.8729)	Acc@1 69.141 (69.834)	Acc@5 97.656 (97.450)
Epoch: [147][192/196]	Time 0.450 (0.479)	Data 0.000 (0.005)	Loss 0.8437 (0.8722)	Acc@1 71.875 (69.794)	Acc@5 97.266 (97.381)
after train
test acc: 53.81
Epoche: [148/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [148][0/196]	Time 0.490 (0.490)	Data 0.661 (0.661)	Loss 0.8657 (0.8657)	Acc@1 68.359 (68.359)	Acc@5 98.438 (98.438)
Epoch: [148][64/196]	Time 0.719 (0.504)	Data 0.006 (0.011)	Loss 0.8769 (0.8727)	Acc@1 67.969 (69.525)	Acc@5 96.484 (97.175)
Epoch: [148][128/196]	Time 0.297 (0.507)	Data 0.012 (0.007)	Loss 0.8365 (0.8717)	Acc@1 68.750 (69.574)	Acc@5 97.656 (97.320)
Epoch: [148][192/196]	Time 0.344 (0.505)	Data 0.000 (0.005)	Loss 0.8483 (0.8640)	Acc@1 71.484 (69.867)	Acc@5 98.438 (97.434)
after train
test acc: 29.97
Epoche: [149/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [149][0/196]	Time 0.586 (0.586)	Data 0.848 (0.848)	Loss 0.8873 (0.8873)	Acc@1 68.750 (68.750)	Acc@5 96.875 (96.875)
Epoch: [149][64/196]	Time 0.526 (0.488)	Data 0.000 (0.014)	Loss 0.8727 (0.8565)	Acc@1 68.750 (70.841)	Acc@5 96.484 (97.494)
Epoch: [149][128/196]	Time 0.504 (0.506)	Data 0.000 (0.009)	Loss 0.7550 (0.8551)	Acc@1 72.266 (70.803)	Acc@5 98.047 (97.475)
Epoch: [149][192/196]	Time 0.506 (0.502)	Data 0.000 (0.006)	Loss 0.9302 (0.8608)	Acc@1 66.016 (70.414)	Acc@5 96.875 (97.413)
after train
test acc: 18.19
Epoche: [150/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [150][0/196]	Time 0.550 (0.550)	Data 0.794 (0.794)	Loss 0.9099 (0.9099)	Acc@1 64.453 (64.453)	Acc@5 98.438 (98.438)
Epoch: [150][64/196]	Time 0.488 (0.479)	Data 0.000 (0.013)	Loss 0.8823 (0.8783)	Acc@1 69.922 (69.225)	Acc@5 97.266 (97.404)
Epoch: [150][128/196]	Time 0.469 (0.482)	Data 0.000 (0.008)	Loss 0.7722 (0.8676)	Acc@1 72.266 (69.525)	Acc@5 99.609 (97.423)
Epoch: [150][192/196]	Time 0.647 (0.488)	Data 0.000 (0.006)	Loss 0.8918 (0.8567)	Acc@1 70.312 (70.147)	Acc@5 98.047 (97.486)
after train
test acc: 53.43
Epoche: [151/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [151][0/196]	Time 0.582 (0.582)	Data 0.647 (0.647)	Loss 0.8034 (0.8034)	Acc@1 67.969 (67.969)	Acc@5 98.828 (98.828)
Epoch: [151][64/196]	Time 0.474 (0.482)	Data 0.028 (0.013)	Loss 0.8043 (0.8473)	Acc@1 75.000 (70.222)	Acc@5 95.703 (97.410)
Epoch: [151][128/196]	Time 0.479 (0.484)	Data 0.000 (0.007)	Loss 0.9158 (0.8592)	Acc@1 68.359 (69.916)	Acc@5 98.828 (97.469)
Epoch: [151][192/196]	Time 0.328 (0.485)	Data 0.000 (0.005)	Loss 0.8573 (0.8577)	Acc@1 73.828 (70.116)	Acc@5 96.875 (97.405)
after train
test acc: 45.72
Epoche: [152/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [152][0/196]	Time 0.745 (0.745)	Data 0.735 (0.735)	Loss 0.7045 (0.7045)	Acc@1 75.781 (75.781)	Acc@5 98.047 (98.047)
Epoch: [152][64/196]	Time 0.558 (0.476)	Data 0.000 (0.012)	Loss 0.7686 (0.8390)	Acc@1 70.703 (70.944)	Acc@5 98.828 (97.530)
Epoch: [152][128/196]	Time 0.470 (0.480)	Data 0.002 (0.007)	Loss 0.8131 (0.8480)	Acc@1 71.875 (70.385)	Acc@5 98.438 (97.526)
Epoch: [152][192/196]	Time 0.411 (0.483)	Data 0.000 (0.005)	Loss 0.7548 (0.8444)	Acc@1 76.953 (70.638)	Acc@5 97.656 (97.474)
after train
test acc: 52.18
Epoche: [153/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [153][0/196]	Time 0.556 (0.556)	Data 0.660 (0.660)	Loss 0.8129 (0.8129)	Acc@1 69.141 (69.141)	Acc@5 97.266 (97.266)
Epoch: [153][64/196]	Time 0.477 (0.455)	Data 0.000 (0.012)	Loss 0.8755 (0.8397)	Acc@1 68.359 (70.529)	Acc@5 98.047 (97.764)
Epoch: [153][128/196]	Time 0.426 (0.481)	Data 0.000 (0.007)	Loss 0.7661 (0.8384)	Acc@1 75.781 (70.706)	Acc@5 98.047 (97.671)
Epoch: [153][192/196]	Time 0.407 (0.493)	Data 0.000 (0.005)	Loss 0.8486 (0.8457)	Acc@1 68.359 (70.525)	Acc@5 98.047 (97.630)
after train
test acc: 30.73
Epoche: [154/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [154][0/196]	Time 0.721 (0.721)	Data 0.763 (0.763)	Loss 0.8658 (0.8658)	Acc@1 68.750 (68.750)	Acc@5 97.656 (97.656)
Epoch: [154][64/196]	Time 0.443 (0.513)	Data 0.000 (0.013)	Loss 0.7187 (0.8654)	Acc@1 76.953 (69.778)	Acc@5 98.438 (97.650)
Epoch: [154][128/196]	Time 0.340 (0.509)	Data 0.000 (0.007)	Loss 0.8515 (0.8600)	Acc@1 70.703 (70.043)	Acc@5 98.438 (97.562)
Epoch: [154][192/196]	Time 0.652 (0.502)	Data 0.000 (0.005)	Loss 0.8043 (0.8514)	Acc@1 69.922 (70.472)	Acc@5 99.219 (97.624)
after train
test acc: 67.72
Epoche: [155/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [155][0/196]	Time 0.332 (0.332)	Data 0.909 (0.909)	Loss 0.9414 (0.9414)	Acc@1 67.188 (67.188)	Acc@5 96.094 (96.094)
Epoch: [155][64/196]	Time 0.515 (0.473)	Data 0.007 (0.016)	Loss 0.8161 (0.8388)	Acc@1 73.438 (70.895)	Acc@5 97.656 (97.416)
Epoch: [155][128/196]	Time 0.401 (0.480)	Data 0.000 (0.009)	Loss 0.8656 (0.8369)	Acc@1 67.578 (70.758)	Acc@5 97.266 (97.465)
Epoch: [155][192/196]	Time 0.378 (0.486)	Data 0.000 (0.006)	Loss 0.7687 (0.8370)	Acc@1 71.094 (70.754)	Acc@5 98.828 (97.511)
after train
test acc: 46.75
Epoche: [156/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [156][0/196]	Time 0.802 (0.802)	Data 0.850 (0.850)	Loss 0.8653 (0.8653)	Acc@1 70.312 (70.312)	Acc@5 98.047 (98.047)
Epoch: [156][64/196]	Time 0.590 (0.481)	Data 0.000 (0.014)	Loss 0.8654 (0.8335)	Acc@1 66.016 (71.184)	Acc@5 97.656 (97.488)
Epoch: [156][128/196]	Time 0.511 (0.494)	Data 0.000 (0.008)	Loss 0.7634 (0.8346)	Acc@1 72.266 (70.906)	Acc@5 98.438 (97.578)
Epoch: [156][192/196]	Time 0.409 (0.494)	Data 0.000 (0.006)	Loss 0.6970 (0.8329)	Acc@1 76.562 (71.041)	Acc@5 98.438 (97.571)
after train
test acc: 51.33
Epoche: [157/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [157][0/196]	Time 0.637 (0.637)	Data 0.783 (0.783)	Loss 0.8266 (0.8266)	Acc@1 69.922 (69.922)	Acc@5 97.656 (97.656)
Epoch: [157][64/196]	Time 0.324 (0.486)	Data 0.000 (0.013)	Loss 0.8030 (0.8464)	Acc@1 69.531 (70.427)	Acc@5 97.656 (97.416)
Epoch: [157][128/196]	Time 0.426 (0.483)	Data 0.000 (0.007)	Loss 0.8134 (0.8373)	Acc@1 71.484 (70.830)	Acc@5 97.656 (97.526)
Epoch: [157][192/196]	Time 0.458 (0.488)	Data 0.000 (0.005)	Loss 0.6885 (0.8330)	Acc@1 77.734 (70.997)	Acc@5 99.609 (97.612)
after train
test acc: 46.23
Epoche: [158/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [158][0/196]	Time 0.763 (0.763)	Data 0.677 (0.677)	Loss 0.7509 (0.7509)	Acc@1 73.828 (73.828)	Acc@5 98.828 (98.828)
Epoch: [158][64/196]	Time 0.319 (0.492)	Data 0.000 (0.012)	Loss 0.7724 (0.8237)	Acc@1 71.875 (71.532)	Acc@5 98.438 (97.560)
Epoch: [158][128/196]	Time 0.525 (0.482)	Data 0.000 (0.007)	Loss 0.7701 (0.8198)	Acc@1 75.000 (71.739)	Acc@5 99.609 (97.638)
Epoch: [158][192/196]	Time 0.579 (0.488)	Data 0.000 (0.005)	Loss 0.7010 (0.8228)	Acc@1 75.391 (71.695)	Acc@5 98.047 (97.648)
after train
test acc: 52.08
Epoche: [159/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [159][0/196]	Time 0.868 (0.868)	Data 0.842 (0.842)	Loss 0.8675 (0.8675)	Acc@1 69.922 (69.922)	Acc@5 97.656 (97.656)
Epoch: [159][64/196]	Time 0.451 (0.524)	Data 0.005 (0.015)	Loss 0.9223 (0.8355)	Acc@1 67.578 (70.974)	Acc@5 95.703 (97.632)
Epoch: [159][128/196]	Time 0.468 (0.516)	Data 0.000 (0.008)	Loss 0.8656 (0.8240)	Acc@1 69.531 (71.342)	Acc@5 98.438 (97.708)
Epoch: [159][192/196]	Time 0.634 (0.504)	Data 0.000 (0.006)	Loss 0.7749 (0.8197)	Acc@1 72.656 (71.567)	Acc@5 97.656 (97.723)
after train
test acc: 58.29
Epoche: [160/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [160][0/196]	Time 0.685 (0.685)	Data 0.789 (0.789)	Loss 0.7306 (0.7306)	Acc@1 74.219 (74.219)	Acc@5 97.266 (97.266)
Epoch: [160][64/196]	Time 0.425 (0.475)	Data 0.000 (0.014)	Loss 0.8063 (0.8245)	Acc@1 72.266 (71.286)	Acc@5 98.438 (97.548)
Epoch: [160][128/196]	Time 0.419 (0.487)	Data 0.000 (0.007)	Loss 0.9120 (0.8145)	Acc@1 68.359 (71.439)	Acc@5 96.875 (97.662)
Epoch: [160][192/196]	Time 0.695 (0.490)	Data 0.000 (0.006)	Loss 0.9054 (0.8163)	Acc@1 71.875 (71.460)	Acc@5 95.703 (97.731)
after train
test acc: 33.54
Epoche: [161/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [161][0/196]	Time 0.664 (0.664)	Data 0.722 (0.722)	Loss 0.7168 (0.7168)	Acc@1 75.000 (75.000)	Acc@5 98.047 (98.047)
Epoch: [161][64/196]	Time 0.463 (0.507)	Data 0.000 (0.012)	Loss 0.8955 (0.8096)	Acc@1 71.875 (71.761)	Acc@5 96.094 (97.680)
Epoch: [161][128/196]	Time 0.329 (0.498)	Data 0.000 (0.007)	Loss 0.7561 (0.8148)	Acc@1 73.047 (71.451)	Acc@5 96.875 (97.708)
Epoch: [161][192/196]	Time 0.413 (0.494)	Data 0.000 (0.005)	Loss 0.7934 (0.8177)	Acc@1 72.266 (71.395)	Acc@5 96.875 (97.697)
after train
test acc: 58.17
Epoche: [162/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [162][0/196]	Time 0.510 (0.510)	Data 0.820 (0.820)	Loss 0.7593 (0.7593)	Acc@1 76.172 (76.172)	Acc@5 97.656 (97.656)
Epoch: [162][64/196]	Time 0.523 (0.500)	Data 0.000 (0.015)	Loss 0.8657 (0.8238)	Acc@1 66.406 (71.496)	Acc@5 99.609 (97.620)
Epoch: [162][128/196]	Time 0.375 (0.500)	Data 0.000 (0.008)	Loss 0.8454 (0.8117)	Acc@1 74.609 (71.854)	Acc@5 96.484 (97.799)
Epoch: [162][192/196]	Time 0.348 (0.494)	Data 0.000 (0.006)	Loss 0.8634 (0.8050)	Acc@1 67.969 (72.045)	Acc@5 98.047 (97.810)
after train
test acc: 43.11
Epoche: [163/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [163][0/196]	Time 0.606 (0.606)	Data 0.718 (0.718)	Loss 0.6904 (0.6904)	Acc@1 74.219 (74.219)	Acc@5 99.219 (99.219)
Epoch: [163][64/196]	Time 0.378 (0.486)	Data 0.000 (0.012)	Loss 0.7968 (0.8147)	Acc@1 73.828 (71.562)	Acc@5 98.047 (97.873)
Epoch: [163][128/196]	Time 0.481 (0.495)	Data 0.000 (0.006)	Loss 0.8225 (0.8080)	Acc@1 75.781 (71.960)	Acc@5 96.875 (97.741)
Epoch: [163][192/196]	Time 0.443 (0.495)	Data 0.000 (0.005)	Loss 0.7751 (0.8076)	Acc@1 73.828 (72.025)	Acc@5 98.438 (97.725)
after train
test acc: 49.26
Epoche: [164/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [164][0/196]	Time 0.472 (0.472)	Data 0.828 (0.828)	Loss 0.8285 (0.8285)	Acc@1 71.484 (71.484)	Acc@5 98.438 (98.438)
Epoch: [164][64/196]	Time 0.376 (0.430)	Data 0.000 (0.014)	Loss 0.7655 (0.8019)	Acc@1 72.266 (72.404)	Acc@5 98.828 (97.794)
Epoch: [164][128/196]	Time 0.622 (0.476)	Data 0.005 (0.008)	Loss 0.8147 (0.8050)	Acc@1 73.047 (72.205)	Acc@5 97.656 (97.817)
Epoch: [164][192/196]	Time 0.691 (0.479)	Data 0.000 (0.006)	Loss 0.7734 (0.8054)	Acc@1 73.828 (72.108)	Acc@5 98.047 (97.780)
after train
test acc: 28.74
Epoche: [165/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [165][0/196]	Time 0.695 (0.695)	Data 0.798 (0.798)	Loss 0.8308 (0.8308)	Acc@1 69.922 (69.922)	Acc@5 98.438 (98.438)
Epoch: [165][64/196]	Time 0.362 (0.500)	Data 0.000 (0.013)	Loss 0.9053 (0.7987)	Acc@1 67.578 (72.085)	Acc@5 96.875 (97.812)
Epoch: [165][128/196]	Time 0.593 (0.503)	Data 0.000 (0.007)	Loss 0.9120 (0.8077)	Acc@1 70.312 (71.784)	Acc@5 95.703 (97.786)
Epoch: [165][192/196]	Time 0.569 (0.493)	Data 0.000 (0.006)	Loss 0.7810 (0.8070)	Acc@1 75.000 (71.780)	Acc@5 96.484 (97.782)
after train
test acc: 50.81
Epoche: [166/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [166][0/196]	Time 0.614 (0.614)	Data 0.797 (0.797)	Loss 0.7741 (0.7741)	Acc@1 73.828 (73.828)	Acc@5 97.656 (97.656)
Epoch: [166][64/196]	Time 0.691 (0.489)	Data 0.000 (0.014)	Loss 0.8758 (0.7906)	Acc@1 67.969 (72.921)	Acc@5 97.656 (97.788)
Epoch: [166][128/196]	Time 0.492 (0.487)	Data 0.000 (0.008)	Loss 0.7407 (0.8076)	Acc@1 74.609 (71.990)	Acc@5 97.266 (97.762)
Epoch: [166][192/196]	Time 0.431 (0.492)	Data 0.000 (0.006)	Loss 0.8486 (0.8069)	Acc@1 67.969 (71.928)	Acc@5 97.266 (97.761)
after train
test acc: 30.69
Epoche: [167/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [167][0/196]	Time 0.668 (0.668)	Data 0.639 (0.639)	Loss 0.6580 (0.6580)	Acc@1 79.297 (79.297)	Acc@5 98.828 (98.828)
Epoch: [167][64/196]	Time 0.735 (0.490)	Data 0.001 (0.011)	Loss 0.8399 (0.8047)	Acc@1 68.750 (72.085)	Acc@5 98.438 (97.855)
Epoch: [167][128/196]	Time 0.374 (0.496)	Data 0.000 (0.006)	Loss 0.9393 (0.8075)	Acc@1 66.406 (71.890)	Acc@5 95.312 (97.786)
Epoch: [167][192/196]	Time 0.413 (0.494)	Data 0.000 (0.005)	Loss 0.6583 (0.8031)	Acc@1 78.125 (72.168)	Acc@5 100.000 (97.818)
after train
test acc: 51.22
Epoche: [168/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [168][0/196]	Time 0.555 (0.555)	Data 0.633 (0.633)	Loss 0.7760 (0.7760)	Acc@1 76.172 (76.172)	Acc@5 98.438 (98.438)
Epoch: [168][64/196]	Time 0.454 (0.484)	Data 0.000 (0.012)	Loss 0.9150 (0.8109)	Acc@1 69.922 (72.194)	Acc@5 97.266 (97.843)
Epoch: [168][128/196]	Time 0.540 (0.503)	Data 0.000 (0.007)	Loss 0.6631 (0.7977)	Acc@1 76.562 (72.571)	Acc@5 98.828 (97.847)
Epoch: [168][192/196]	Time 0.308 (0.491)	Data 0.000 (0.005)	Loss 0.7936 (0.7931)	Acc@1 71.875 (72.612)	Acc@5 97.266 (97.865)
after train
test acc: 55.31
Epoche: [169/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [169][0/196]	Time 0.572 (0.572)	Data 0.645 (0.645)	Loss 0.8570 (0.8570)	Acc@1 71.875 (71.875)	Acc@5 97.266 (97.266)
Epoch: [169][64/196]	Time 0.406 (0.461)	Data 0.000 (0.011)	Loss 0.8662 (0.8035)	Acc@1 68.359 (72.163)	Acc@5 98.438 (97.855)
Epoch: [169][128/196]	Time 0.610 (0.492)	Data 0.005 (0.007)	Loss 0.7195 (0.7924)	Acc@1 73.828 (72.465)	Acc@5 99.219 (97.941)
Epoch: [169][192/196]	Time 0.436 (0.490)	Data 0.000 (0.005)	Loss 0.7659 (0.7914)	Acc@1 69.922 (72.488)	Acc@5 99.609 (97.938)
after train
test acc: 55.82
Epoche: [170/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [170][0/196]	Time 0.429 (0.429)	Data 0.666 (0.666)	Loss 0.8324 (0.8324)	Acc@1 69.922 (69.922)	Acc@5 97.656 (97.656)
Epoch: [170][64/196]	Time 0.541 (0.473)	Data 0.000 (0.013)	Loss 0.8736 (0.7894)	Acc@1 68.359 (72.788)	Acc@5 97.266 (97.873)
Epoch: [170][128/196]	Time 0.638 (0.483)	Data 0.000 (0.007)	Loss 0.6433 (0.7869)	Acc@1 75.781 (72.786)	Acc@5 98.828 (97.883)
Epoch: [170][192/196]	Time 0.487 (0.486)	Data 0.000 (0.006)	Loss 0.7685 (0.7873)	Acc@1 72.266 (72.768)	Acc@5 98.828 (97.855)
after train
test acc: 33.23
Epoche: [171/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [171][0/196]	Time 0.699 (0.699)	Data 0.587 (0.587)	Loss 0.7204 (0.7204)	Acc@1 76.172 (76.172)	Acc@5 96.484 (96.484)
Epoch: [171][64/196]	Time 0.561 (0.493)	Data 0.000 (0.010)	Loss 0.7916 (0.8067)	Acc@1 73.828 (72.163)	Acc@5 98.438 (97.704)
Epoch: [171][128/196]	Time 0.564 (0.497)	Data 0.000 (0.006)	Loss 0.8505 (0.8009)	Acc@1 69.922 (72.284)	Acc@5 96.875 (97.844)
Epoch: [171][192/196]	Time 0.455 (0.492)	Data 0.000 (0.005)	Loss 0.7260 (0.8035)	Acc@1 75.000 (72.106)	Acc@5 99.219 (97.780)
after train
test acc: 51.07
Epoche: [172/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [172][0/196]	Time 0.504 (0.504)	Data 0.614 (0.614)	Loss 0.7706 (0.7706)	Acc@1 72.266 (72.266)	Acc@5 97.656 (97.656)
Epoch: [172][64/196]	Time 0.451 (0.466)	Data 0.000 (0.011)	Loss 0.8153 (0.7800)	Acc@1 72.656 (72.837)	Acc@5 98.047 (97.951)
Epoch: [172][128/196]	Time 0.552 (0.488)	Data 0.000 (0.006)	Loss 0.5942 (0.7791)	Acc@1 82.422 (72.868)	Acc@5 98.828 (97.953)
Epoch: [172][192/196]	Time 0.513 (0.493)	Data 0.000 (0.004)	Loss 0.7031 (0.7828)	Acc@1 76.953 (72.668)	Acc@5 98.438 (97.925)
after train
test acc: 53.17
Epoche: [173/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [173][0/196]	Time 0.828 (0.828)	Data 0.770 (0.770)	Loss 0.8270 (0.8270)	Acc@1 72.266 (72.266)	Acc@5 98.047 (98.047)
Epoch: [173][64/196]	Time 0.496 (0.490)	Data 0.000 (0.013)	Loss 0.7926 (0.7834)	Acc@1 76.562 (72.572)	Acc@5 97.266 (97.999)
Epoch: [173][128/196]	Time 0.555 (0.502)	Data 0.000 (0.007)	Loss 0.6651 (0.7859)	Acc@1 78.516 (72.671)	Acc@5 99.609 (97.944)
Epoch: [173][192/196]	Time 0.500 (0.494)	Data 0.000 (0.005)	Loss 0.7555 (0.7913)	Acc@1 71.094 (72.573)	Acc@5 98.438 (97.893)
after train
test acc: 50.96
Epoche: [174/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [174][0/196]	Time 0.533 (0.533)	Data 0.737 (0.737)	Loss 0.6836 (0.6836)	Acc@1 77.344 (77.344)	Acc@5 98.047 (98.047)
Epoch: [174][64/196]	Time 0.448 (0.504)	Data 0.000 (0.013)	Loss 0.6995 (0.7860)	Acc@1 78.125 (73.041)	Acc@5 98.047 (97.831)
Epoch: [174][128/196]	Time 0.373 (0.491)	Data 0.000 (0.007)	Loss 0.8431 (0.7917)	Acc@1 69.531 (72.777)	Acc@5 95.703 (97.856)
Epoch: [174][192/196]	Time 0.586 (0.488)	Data 0.000 (0.005)	Loss 0.6906 (0.7942)	Acc@1 74.609 (72.642)	Acc@5 98.438 (97.832)
after train
test acc: 67.91
Epoche: [175/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [175][0/196]	Time 0.563 (0.563)	Data 0.545 (0.545)	Loss 0.8282 (0.8282)	Acc@1 69.922 (69.922)	Acc@5 99.219 (99.219)
Epoch: [175][64/196]	Time 0.419 (0.491)	Data 0.000 (0.010)	Loss 0.7942 (0.7882)	Acc@1 70.312 (72.596)	Acc@5 99.609 (97.903)
Epoch: [175][128/196]	Time 0.328 (0.511)	Data 0.000 (0.006)	Loss 0.7105 (0.7867)	Acc@1 74.219 (72.668)	Acc@5 99.219 (97.889)
Epoch: [175][192/196]	Time 0.520 (0.503)	Data 0.000 (0.004)	Loss 0.7468 (0.7806)	Acc@1 74.609 (73.002)	Acc@5 98.047 (97.863)
after train
test acc: 22.06
Epoche: [176/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [176][0/196]	Time 0.427 (0.427)	Data 0.838 (0.838)	Loss 0.7467 (0.7467)	Acc@1 76.172 (76.172)	Acc@5 98.828 (98.828)
Epoch: [176][64/196]	Time 0.535 (0.514)	Data 0.000 (0.015)	Loss 0.8409 (0.7825)	Acc@1 72.656 (73.005)	Acc@5 98.047 (97.819)
Epoch: [176][128/196]	Time 0.258 (0.502)	Data 0.000 (0.008)	Loss 0.7865 (0.7851)	Acc@1 69.141 (72.965)	Acc@5 97.266 (97.799)
Epoch: [176][192/196]	Time 0.571 (0.502)	Data 0.000 (0.006)	Loss 0.8899 (0.7787)	Acc@1 69.922 (73.124)	Acc@5 97.266 (97.798)
after train
test acc: 58.57
Epoche: [177/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [177][0/196]	Time 0.437 (0.437)	Data 0.658 (0.658)	Loss 0.7489 (0.7489)	Acc@1 75.000 (75.000)	Acc@5 98.828 (98.828)
Epoch: [177][64/196]	Time 0.548 (0.452)	Data 0.000 (0.011)	Loss 0.8652 (0.7909)	Acc@1 69.531 (72.614)	Acc@5 97.266 (97.855)
Epoch: [177][128/196]	Time 0.662 (0.462)	Data 0.000 (0.006)	Loss 0.8105 (0.7816)	Acc@1 69.922 (72.911)	Acc@5 96.875 (97.889)
Epoch: [177][192/196]	Time 0.353 (0.471)	Data 0.000 (0.005)	Loss 0.8827 (0.7783)	Acc@1 68.750 (73.006)	Acc@5 95.703 (97.883)
after train
test acc: 25.86
Epoche: [178/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [178][0/196]	Time 0.637 (0.637)	Data 0.849 (0.849)	Loss 0.6574 (0.6574)	Acc@1 77.344 (77.344)	Acc@5 99.219 (99.219)
Epoch: [178][64/196]	Time 0.365 (0.471)	Data 0.000 (0.014)	Loss 0.7803 (0.7858)	Acc@1 73.047 (72.608)	Acc@5 98.828 (97.903)
Epoch: [178][128/196]	Time 0.338 (0.482)	Data 0.000 (0.008)	Loss 0.8130 (0.7813)	Acc@1 74.609 (72.965)	Acc@5 97.656 (97.880)
Epoch: [178][192/196]	Time 0.602 (0.480)	Data 0.000 (0.006)	Loss 0.8748 (0.7848)	Acc@1 69.531 (72.857)	Acc@5 98.047 (97.877)
after train
test acc: 28.97
Epoche: [179/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [179][0/196]	Time 0.714 (0.714)	Data 0.973 (0.973)	Loss 0.6713 (0.6713)	Acc@1 78.516 (78.516)	Acc@5 97.266 (97.266)
Epoch: [179][64/196]	Time 0.498 (0.495)	Data 0.000 (0.016)	Loss 0.7449 (0.7906)	Acc@1 75.000 (72.578)	Acc@5 98.047 (97.879)
Epoch: [179][128/196]	Time 0.430 (0.498)	Data 0.000 (0.008)	Loss 0.7435 (0.7885)	Acc@1 74.609 (72.696)	Acc@5 98.047 (97.902)
Epoch: [179][192/196]	Time 0.484 (0.488)	Data 0.000 (0.006)	Loss 0.6619 (0.7854)	Acc@1 81.250 (72.938)	Acc@5 99.219 (97.883)
after train
test acc: 46.72
Epoche: [180/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [180][0/196]	Time 0.582 (0.582)	Data 0.658 (0.658)	Loss 0.7314 (0.7314)	Acc@1 74.219 (74.219)	Acc@5 98.828 (98.828)
Epoch: [180][64/196]	Time 0.459 (0.469)	Data 0.000 (0.011)	Loss 0.7687 (0.7844)	Acc@1 74.219 (73.281)	Acc@5 96.875 (97.746)
Epoch: [180][128/196]	Time 0.484 (0.484)	Data 0.000 (0.006)	Loss 0.6814 (0.7924)	Acc@1 76.172 (72.726)	Acc@5 98.828 (97.759)
Epoch: [180][192/196]	Time 0.417 (0.485)	Data 0.000 (0.005)	Loss 0.7357 (0.7815)	Acc@1 74.609 (73.023)	Acc@5 98.828 (97.861)
after train
test acc: 22.86
Epoche: [181/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [181][0/196]	Time 0.560 (0.560)	Data 0.631 (0.631)	Loss 0.8080 (0.8080)	Acc@1 73.438 (73.438)	Acc@5 97.656 (97.656)
Epoch: [181][64/196]	Time 0.606 (0.513)	Data 0.000 (0.011)	Loss 0.8180 (0.7795)	Acc@1 68.359 (72.746)	Acc@5 97.266 (97.951)
Epoch: [181][128/196]	Time 0.424 (0.498)	Data 0.000 (0.006)	Loss 0.7835 (0.7753)	Acc@1 72.656 (72.938)	Acc@5 98.438 (97.920)
Epoch: [181][192/196]	Time 0.417 (0.489)	Data 0.000 (0.004)	Loss 0.7873 (0.7773)	Acc@1 73.047 (73.004)	Acc@5 98.438 (97.934)
after train
test acc: 46.83
Epoche: [182/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [182][0/196]	Time 0.619 (0.619)	Data 0.752 (0.752)	Loss 0.5539 (0.5539)	Acc@1 80.078 (80.078)	Acc@5 98.828 (98.828)
Epoch: [182][64/196]	Time 0.456 (0.520)	Data 0.000 (0.013)	Loss 0.7996 (0.7563)	Acc@1 74.609 (73.900)	Acc@5 98.438 (97.897)
Epoch: [182][128/196]	Time 0.632 (0.470)	Data 0.000 (0.007)	Loss 0.8135 (0.7648)	Acc@1 73.438 (73.713)	Acc@5 96.875 (97.892)
Epoch: [182][192/196]	Time 0.325 (0.483)	Data 0.000 (0.005)	Loss 0.7166 (0.7726)	Acc@1 73.828 (73.399)	Acc@5 98.047 (97.842)
after train
test acc: 22.02
Epoche: [183/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [183][0/196]	Time 0.422 (0.422)	Data 0.845 (0.845)	Loss 0.7002 (0.7002)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [183][64/196]	Time 0.404 (0.465)	Data 0.000 (0.014)	Loss 0.9387 (0.7770)	Acc@1 66.797 (72.855)	Acc@5 98.047 (97.891)
Epoch: [183][128/196]	Time 0.441 (0.490)	Data 0.000 (0.008)	Loss 0.7376 (0.7797)	Acc@1 77.734 (72.826)	Acc@5 99.219 (97.823)
Epoch: [183][192/196]	Time 0.657 (0.487)	Data 0.000 (0.005)	Loss 0.7072 (0.7795)	Acc@1 75.391 (72.970)	Acc@5 97.656 (97.818)
after train
test acc: 59.23
Epoche: [184/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [184][0/196]	Time 0.451 (0.451)	Data 0.857 (0.857)	Loss 0.6718 (0.6718)	Acc@1 73.047 (73.047)	Acc@5 98.828 (98.828)
Epoch: [184][64/196]	Time 0.450 (0.502)	Data 0.000 (0.015)	Loss 0.8815 (0.7924)	Acc@1 72.266 (72.680)	Acc@5 98.828 (97.686)
Epoch: [184][128/196]	Time 0.470 (0.483)	Data 0.000 (0.009)	Loss 0.7772 (0.7809)	Acc@1 71.875 (72.935)	Acc@5 97.266 (97.923)
Epoch: [184][192/196]	Time 0.487 (0.488)	Data 0.000 (0.006)	Loss 0.6662 (0.7771)	Acc@1 76.953 (73.158)	Acc@5 98.438 (97.877)
after train
test acc: 34.02
Epoche: [185/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [185][0/196]	Time 0.445 (0.445)	Data 0.760 (0.760)	Loss 0.8512 (0.8512)	Acc@1 73.828 (73.828)	Acc@5 94.922 (94.922)
Epoch: [185][64/196]	Time 0.440 (0.484)	Data 0.000 (0.013)	Loss 0.8296 (0.7666)	Acc@1 73.047 (73.510)	Acc@5 98.047 (97.873)
Epoch: [185][128/196]	Time 0.360 (0.464)	Data 0.000 (0.008)	Loss 0.6683 (0.7665)	Acc@1 74.609 (73.504)	Acc@5 97.266 (97.908)
Epoch: [185][192/196]	Time 0.440 (0.473)	Data 0.000 (0.006)	Loss 0.8131 (0.7677)	Acc@1 75.781 (73.332)	Acc@5 97.656 (97.994)
after train
test acc: 64.65
Epoche: [186/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [186][0/196]	Time 0.514 (0.514)	Data 0.698 (0.698)	Loss 0.8649 (0.8649)	Acc@1 69.141 (69.141)	Acc@5 99.219 (99.219)
Epoch: [186][64/196]	Time 0.458 (0.522)	Data 0.005 (0.012)	Loss 0.7533 (0.7823)	Acc@1 74.609 (72.620)	Acc@5 96.875 (97.776)
Epoch: [186][128/196]	Time 0.281 (0.516)	Data 0.000 (0.007)	Loss 0.7095 (0.7744)	Acc@1 77.344 (72.892)	Acc@5 97.266 (97.898)
Epoch: [186][192/196]	Time 0.692 (0.513)	Data 0.000 (0.005)	Loss 0.7600 (0.7760)	Acc@1 71.875 (72.917)	Acc@5 98.828 (97.881)
after train
test acc: 50.77
Epoche: [187/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [187][0/196]	Time 0.286 (0.286)	Data 0.834 (0.834)	Loss 0.7697 (0.7697)	Acc@1 73.438 (73.438)	Acc@5 96.875 (96.875)
Epoch: [187][64/196]	Time 0.463 (0.455)	Data 0.000 (0.014)	Loss 0.6450 (0.7645)	Acc@1 76.562 (73.413)	Acc@5 98.828 (97.969)
Epoch: [187][128/196]	Time 0.587 (0.467)	Data 0.000 (0.007)	Loss 0.6699 (0.7644)	Acc@1 75.000 (73.450)	Acc@5 98.438 (97.935)
Epoch: [187][192/196]	Time 0.487 (0.475)	Data 0.000 (0.005)	Loss 0.7315 (0.7668)	Acc@1 76.562 (73.361)	Acc@5 98.438 (97.976)
after train
test acc: 21.57
Epoche: [188/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [188][0/196]	Time 0.476 (0.476)	Data 0.751 (0.751)	Loss 0.7894 (0.7894)	Acc@1 73.438 (73.438)	Acc@5 98.438 (98.438)
Epoch: [188][64/196]	Time 0.777 (0.514)	Data 0.000 (0.013)	Loss 0.8530 (0.7717)	Acc@1 70.703 (73.528)	Acc@5 98.828 (97.879)
Epoch: [188][128/196]	Time 0.592 (0.505)	Data 0.000 (0.007)	Loss 0.7566 (0.7771)	Acc@1 73.828 (73.316)	Acc@5 98.047 (97.865)
Epoch: [188][192/196]	Time 0.645 (0.500)	Data 0.000 (0.005)	Loss 0.7517 (0.7736)	Acc@1 75.000 (73.373)	Acc@5 97.656 (97.903)
after train
test acc: 66.94
Epoche: [189/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [189][0/196]	Time 0.828 (0.828)	Data 1.026 (1.026)	Loss 0.6472 (0.6472)	Acc@1 79.297 (79.297)	Acc@5 98.047 (98.047)
Epoch: [189][64/196]	Time 0.586 (0.494)	Data 0.004 (0.017)	Loss 0.8060 (0.7634)	Acc@1 74.219 (73.504)	Acc@5 98.438 (98.053)
Epoch: [189][128/196]	Time 0.605 (0.503)	Data 0.000 (0.009)	Loss 0.7983 (0.7676)	Acc@1 73.047 (73.441)	Acc@5 97.266 (97.956)
Epoch: [189][192/196]	Time 0.570 (0.503)	Data 0.000 (0.007)	Loss 0.6738 (0.7659)	Acc@1 77.734 (73.561)	Acc@5 99.219 (97.962)
after train
test acc: 37.51
Epoche: [190/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [190][0/196]	Time 0.593 (0.593)	Data 0.700 (0.700)	Loss 0.8832 (0.8832)	Acc@1 68.750 (68.750)	Acc@5 94.922 (94.922)
Epoch: [190][64/196]	Time 0.708 (0.503)	Data 0.000 (0.012)	Loss 0.7600 (0.7591)	Acc@1 70.703 (73.600)	Acc@5 98.047 (98.119)
Epoch: [190][128/196]	Time 0.604 (0.496)	Data 0.000 (0.007)	Loss 0.8831 (0.7640)	Acc@1 70.703 (73.528)	Acc@5 98.438 (98.038)
Epoch: [190][192/196]	Time 0.405 (0.496)	Data 0.000 (0.005)	Loss 0.7799 (0.7621)	Acc@1 74.609 (73.664)	Acc@5 99.219 (98.063)
after train
test acc: 41.76
Epoche: [191/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [191][0/196]	Time 0.486 (0.486)	Data 0.630 (0.630)	Loss 0.7628 (0.7628)	Acc@1 76.562 (76.562)	Acc@5 97.656 (97.656)
Epoch: [191][64/196]	Time 0.565 (0.501)	Data 0.000 (0.011)	Loss 0.8156 (0.7700)	Acc@1 72.266 (73.696)	Acc@5 97.656 (98.011)
Epoch: [191][128/196]	Time 0.305 (0.504)	Data 0.000 (0.007)	Loss 0.6399 (0.7647)	Acc@1 77.734 (73.683)	Acc@5 98.047 (98.059)
Epoch: [191][192/196]	Time 0.424 (0.495)	Data 0.000 (0.005)	Loss 0.8293 (0.7691)	Acc@1 71.484 (73.545)	Acc@5 96.484 (97.974)
after train
test acc: 31.64
Epoche: [192/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [192][0/196]	Time 0.505 (0.505)	Data 0.795 (0.795)	Loss 0.7758 (0.7758)	Acc@1 71.484 (71.484)	Acc@5 98.438 (98.438)
Epoch: [192][64/196]	Time 0.549 (0.502)	Data 0.003 (0.014)	Loss 0.9606 (0.7749)	Acc@1 67.578 (73.383)	Acc@5 97.656 (97.891)
Epoch: [192][128/196]	Time 0.592 (0.496)	Data 0.000 (0.007)	Loss 0.7093 (0.7680)	Acc@1 73.438 (73.583)	Acc@5 98.438 (97.895)
Epoch: [192][192/196]	Time 0.413 (0.494)	Data 0.000 (0.005)	Loss 0.6862 (0.7657)	Acc@1 76.172 (73.650)	Acc@5 98.438 (97.968)
after train
test acc: 59.49
Epoche: [193/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [193][0/196]	Time 0.460 (0.460)	Data 0.871 (0.871)	Loss 0.8929 (0.8929)	Acc@1 73.438 (73.438)	Acc@5 96.484 (96.484)
Epoch: [193][64/196]	Time 0.551 (0.505)	Data 0.000 (0.015)	Loss 0.7660 (0.7656)	Acc@1 75.781 (73.558)	Acc@5 97.656 (97.891)
Epoch: [193][128/196]	Time 0.492 (0.504)	Data 0.000 (0.008)	Loss 0.6980 (0.7641)	Acc@1 75.781 (73.601)	Acc@5 97.656 (97.941)
Epoch: [193][192/196]	Time 0.431 (0.505)	Data 0.000 (0.006)	Loss 0.9050 (0.7620)	Acc@1 73.047 (73.688)	Acc@5 96.094 (97.917)
after train
test acc: 62.67
Epoche: [194/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [194][0/196]	Time 0.631 (0.631)	Data 0.897 (0.897)	Loss 0.6937 (0.6937)	Acc@1 76.562 (76.562)	Acc@5 98.047 (98.047)
Epoch: [194][64/196]	Time 0.442 (0.512)	Data 0.000 (0.015)	Loss 0.7210 (0.7669)	Acc@1 76.562 (73.612)	Acc@5 99.219 (97.987)
Epoch: [194][128/196]	Time 0.633 (0.504)	Data 0.000 (0.008)	Loss 0.8037 (0.7630)	Acc@1 70.312 (73.849)	Acc@5 97.656 (97.980)
Epoch: [194][192/196]	Time 0.390 (0.492)	Data 0.000 (0.006)	Loss 0.7968 (0.7639)	Acc@1 72.656 (73.733)	Acc@5 98.828 (98.014)
after train
test acc: 46.24
Epoche: [195/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [195][0/196]	Time 0.600 (0.600)	Data 0.797 (0.797)	Loss 0.7581 (0.7581)	Acc@1 72.656 (72.656)	Acc@5 98.828 (98.828)
Epoch: [195][64/196]	Time 0.507 (0.498)	Data 0.000 (0.014)	Loss 0.7740 (0.7804)	Acc@1 73.828 (73.071)	Acc@5 98.047 (97.897)
Epoch: [195][128/196]	Time 0.421 (0.485)	Data 0.000 (0.008)	Loss 0.6815 (0.7700)	Acc@1 78.516 (73.271)	Acc@5 98.047 (97.968)
Epoch: [195][192/196]	Time 0.369 (0.487)	Data 0.000 (0.005)	Loss 0.7738 (0.7634)	Acc@1 73.438 (73.452)	Acc@5 98.828 (97.948)
after train
test acc: 33.75
Epoche: [196/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [196][0/196]	Time 0.722 (0.722)	Data 0.577 (0.577)	Loss 0.7368 (0.7368)	Acc@1 74.219 (74.219)	Acc@5 97.266 (97.266)
Epoch: [196][64/196]	Time 0.432 (0.458)	Data 0.000 (0.010)	Loss 0.7847 (0.7638)	Acc@1 70.312 (73.654)	Acc@5 98.828 (97.849)
Epoch: [196][128/196]	Time 0.582 (0.463)	Data 0.004 (0.006)	Loss 0.8260 (0.7636)	Acc@1 74.219 (73.556)	Acc@5 97.656 (97.923)
Epoch: [196][192/196]	Time 0.479 (0.477)	Data 0.000 (0.004)	Loss 0.7037 (0.7655)	Acc@1 73.047 (73.476)	Acc@5 98.438 (97.950)
after train
test acc: 62.21
Epoche: [197/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [197][0/196]	Time 0.692 (0.692)	Data 0.769 (0.769)	Loss 0.8237 (0.8237)	Acc@1 73.828 (73.828)	Acc@5 96.094 (96.094)
Epoch: [197][64/196]	Time 0.559 (0.524)	Data 0.000 (0.013)	Loss 0.7733 (0.7631)	Acc@1 73.047 (73.930)	Acc@5 97.656 (97.999)
Epoch: [197][128/196]	Time 0.347 (0.488)	Data 0.000 (0.007)	Loss 0.7631 (0.7622)	Acc@1 69.922 (73.771)	Acc@5 98.047 (98.047)
Epoch: [197][192/196]	Time 0.736 (0.488)	Data 0.000 (0.005)	Loss 0.7705 (0.7574)	Acc@1 75.000 (73.899)	Acc@5 96.094 (98.041)
after train
test acc: 50.3
Epoche: [198/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [198][0/196]	Time 0.577 (0.577)	Data 0.746 (0.746)	Loss 0.8290 (0.8290)	Acc@1 70.703 (70.703)	Acc@5 98.438 (98.438)
Epoch: [198][64/196]	Time 0.358 (0.479)	Data 0.000 (0.013)	Loss 0.7847 (0.7648)	Acc@1 73.438 (73.570)	Acc@5 97.656 (97.782)
Epoch: [198][128/196]	Time 0.702 (0.483)	Data 0.000 (0.007)	Loss 0.7502 (0.7643)	Acc@1 74.219 (73.583)	Acc@5 98.438 (97.808)
Epoch: [198][192/196]	Time 0.390 (0.481)	Data 0.000 (0.005)	Loss 0.6996 (0.7641)	Acc@1 76.172 (73.549)	Acc@5 98.828 (97.875)
after train
test acc: 57.82
Epoche: [199/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [199][0/196]	Time 0.534 (0.534)	Data 1.144 (1.144)	Loss 0.7437 (0.7437)	Acc@1 74.609 (74.609)	Acc@5 98.047 (98.047)
Epoch: [199][64/196]	Time 0.422 (0.513)	Data 0.000 (0.018)	Loss 0.7510 (0.7705)	Acc@1 73.438 (73.185)	Acc@5 98.828 (97.855)
Epoch: [199][128/196]	Time 0.443 (0.498)	Data 0.000 (0.010)	Loss 0.9088 (0.7734)	Acc@1 68.750 (73.325)	Acc@5 98.047 (97.859)
Epoch: [199][192/196]	Time 0.359 (0.501)	Data 0.000 (0.007)	Loss 0.7547 (0.7697)	Acc@1 72.266 (73.330)	Acc@5 98.828 (97.944)
after train
test acc: 61.51
Epoche: [200/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [200][0/196]	Time 0.557 (0.557)	Data 0.908 (0.908)	Loss 0.7333 (0.7333)	Acc@1 71.094 (71.094)	Acc@5 98.828 (98.828)
Epoch: [200][64/196]	Time 0.908 (0.507)	Data 0.000 (0.015)	Loss 0.7070 (0.7567)	Acc@1 80.469 (73.612)	Acc@5 97.266 (98.101)
Epoch: [200][128/196]	Time 0.621 (0.518)	Data 0.000 (0.008)	Loss 0.7966 (0.7613)	Acc@1 72.266 (73.595)	Acc@5 98.047 (98.026)
Epoch: [200][192/196]	Time 0.431 (0.513)	Data 0.000 (0.006)	Loss 0.7466 (0.7576)	Acc@1 75.391 (73.905)	Acc@5 97.266 (98.010)
after train
test acc: 50.82
Epoche: [201/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [201][0/196]	Time 0.578 (0.578)	Data 0.621 (0.621)	Loss 0.8610 (0.8610)	Acc@1 71.875 (71.875)	Acc@5 96.484 (96.484)
Epoch: [201][64/196]	Time 0.404 (0.504)	Data 0.011 (0.011)	Loss 0.7773 (0.7446)	Acc@1 75.000 (74.501)	Acc@5 97.656 (98.131)
Epoch: [201][128/196]	Time 0.543 (0.504)	Data 0.000 (0.006)	Loss 0.6548 (0.7589)	Acc@1 76.953 (73.928)	Acc@5 98.438 (98.086)
Epoch: [201][192/196]	Time 0.470 (0.511)	Data 0.000 (0.005)	Loss 0.6857 (0.7585)	Acc@1 78.125 (73.921)	Acc@5 99.219 (97.988)
after train
test acc: 54.34
Epoche: [202/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [202][0/196]	Time 0.670 (0.670)	Data 0.835 (0.835)	Loss 0.9074 (0.9074)	Acc@1 69.531 (69.531)	Acc@5 97.266 (97.266)
Epoch: [202][64/196]	Time 0.588 (0.475)	Data 0.000 (0.014)	Loss 0.7958 (0.7580)	Acc@1 71.094 (73.912)	Acc@5 97.656 (98.017)
Epoch: [202][128/196]	Time 0.618 (0.486)	Data 0.000 (0.008)	Loss 0.7232 (0.7565)	Acc@1 76.953 (74.019)	Acc@5 98.047 (98.032)
Epoch: [202][192/196]	Time 0.405 (0.485)	Data 0.000 (0.005)	Loss 0.8362 (0.7568)	Acc@1 69.531 (73.996)	Acc@5 98.047 (98.061)
after train
test acc: 53.38
Epoche: [203/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [203][0/196]	Time 0.560 (0.560)	Data 0.861 (0.861)	Loss 0.6470 (0.6470)	Acc@1 78.516 (78.516)	Acc@5 97.656 (97.656)
Epoch: [203][64/196]	Time 0.510 (0.487)	Data 0.000 (0.015)	Loss 0.8078 (0.7689)	Acc@1 73.047 (73.239)	Acc@5 97.656 (97.855)
Epoch: [203][128/196]	Time 0.455 (0.464)	Data 0.000 (0.008)	Loss 0.7234 (0.7660)	Acc@1 71.875 (73.468)	Acc@5 98.828 (97.877)
Epoch: [203][192/196]	Time 0.359 (0.446)	Data 0.000 (0.006)	Loss 0.7766 (0.7639)	Acc@1 73.438 (73.561)	Acc@5 98.047 (97.956)
after train
test acc: 36.53
Epoche: [204/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [204][0/196]	Time 0.617 (0.617)	Data 0.681 (0.681)	Loss 0.7900 (0.7900)	Acc@1 73.438 (73.438)	Acc@5 96.875 (96.875)
Epoch: [204][64/196]	Time 0.311 (0.291)	Data 0.000 (0.012)	Loss 0.7730 (0.7505)	Acc@1 75.781 (74.225)	Acc@5 98.047 (97.969)
Epoch: [204][128/196]	Time 0.368 (0.290)	Data 0.006 (0.006)	Loss 0.7526 (0.7503)	Acc@1 75.781 (74.201)	Acc@5 97.656 (98.050)
Epoch: [204][192/196]	Time 0.290 (0.292)	Data 0.000 (0.004)	Loss 0.7522 (0.7485)	Acc@1 74.609 (74.188)	Acc@5 96.875 (98.041)
after train
test acc: 47.13
Epoche: [205/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [205][0/196]	Time 0.347 (0.347)	Data 0.554 (0.554)	Loss 0.7202 (0.7202)	Acc@1 74.609 (74.609)	Acc@5 98.047 (98.047)
Epoch: [205][64/196]	Time 0.311 (0.290)	Data 0.000 (0.010)	Loss 0.8241 (0.7596)	Acc@1 72.266 (73.768)	Acc@5 97.656 (97.975)
Epoch: [205][128/196]	Time 0.327 (0.278)	Data 0.005 (0.005)	Loss 0.7415 (0.7569)	Acc@1 73.438 (73.843)	Acc@5 97.656 (98.023)
Epoch: [205][192/196]	Time 0.258 (0.282)	Data 0.000 (0.004)	Loss 0.7820 (0.7521)	Acc@1 72.656 (74.012)	Acc@5 98.047 (98.037)
after train
test acc: 60.9
Epoche: [206/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [206][0/196]	Time 0.471 (0.471)	Data 0.655 (0.655)	Loss 0.6960 (0.6960)	Acc@1 72.656 (72.656)	Acc@5 99.219 (99.219)
Epoch: [206][64/196]	Time 0.324 (0.294)	Data 0.000 (0.012)	Loss 0.8638 (0.7686)	Acc@1 70.703 (73.143)	Acc@5 97.656 (98.023)
Epoch: [206][128/196]	Time 0.287 (0.288)	Data 0.000 (0.006)	Loss 0.8842 (0.7664)	Acc@1 69.141 (73.416)	Acc@5 96.875 (98.098)
Epoch: [206][192/196]	Time 0.267 (0.288)	Data 0.000 (0.004)	Loss 0.7896 (0.7676)	Acc@1 73.438 (73.415)	Acc@5 97.656 (98.019)
after train
test acc: 26.48
Epoche: [207/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [207][0/196]	Time 0.291 (0.291)	Data 0.692 (0.692)	Loss 0.7659 (0.7659)	Acc@1 71.484 (71.484)	Acc@5 98.438 (98.438)
Epoch: [207][64/196]	Time 0.298 (0.277)	Data 0.020 (0.012)	Loss 0.8596 (0.7558)	Acc@1 71.094 (74.050)	Acc@5 98.438 (98.011)
Epoch: [207][128/196]	Time 0.321 (0.278)	Data 0.000 (0.006)	Loss 0.6962 (0.7464)	Acc@1 75.000 (74.095)	Acc@5 96.875 (98.032)
Epoch: [207][192/196]	Time 0.190 (0.278)	Data 0.000 (0.005)	Loss 0.7707 (0.7504)	Acc@1 72.266 (74.055)	Acc@5 97.266 (98.004)
after train
test acc: 66.58
Epoche: [208/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [208][0/196]	Time 0.295 (0.295)	Data 0.575 (0.575)	Loss 0.6896 (0.6896)	Acc@1 73.828 (73.828)	Acc@5 98.047 (98.047)
Epoch: [208][64/196]	Time 0.351 (0.274)	Data 0.000 (0.010)	Loss 0.6864 (0.7466)	Acc@1 75.391 (74.135)	Acc@5 99.219 (98.173)
Epoch: [208][128/196]	Time 0.334 (0.281)	Data 0.011 (0.006)	Loss 0.6193 (0.7484)	Acc@1 79.297 (74.146)	Acc@5 98.047 (98.195)
Epoch: [208][192/196]	Time 0.319 (0.285)	Data 0.000 (0.004)	Loss 0.7360 (0.7484)	Acc@1 73.828 (74.152)	Acc@5 98.438 (98.146)
after train
test acc: 57.53
Epoche: [209/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [209][0/196]	Time 0.366 (0.366)	Data 0.634 (0.634)	Loss 0.7558 (0.7558)	Acc@1 71.875 (71.875)	Acc@5 98.047 (98.047)
Epoch: [209][64/196]	Time 0.267 (0.273)	Data 0.000 (0.011)	Loss 0.9233 (0.7470)	Acc@1 70.312 (74.207)	Acc@5 96.484 (98.041)
Epoch: [209][128/196]	Time 0.433 (0.278)	Data 0.000 (0.006)	Loss 0.6932 (0.7557)	Acc@1 76.172 (74.049)	Acc@5 98.438 (97.902)
Epoch: [209][192/196]	Time 0.267 (0.281)	Data 0.000 (0.004)	Loss 0.5538 (0.7544)	Acc@1 82.031 (74.035)	Acc@5 99.219 (97.948)
after train
test acc: 62.02
Epoche: [210/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [210][0/196]	Time 0.410 (0.410)	Data 0.778 (0.778)	Loss 0.6962 (0.6962)	Acc@1 75.391 (75.391)	Acc@5 99.609 (99.609)
Epoch: [210][64/196]	Time 0.350 (0.273)	Data 0.011 (0.013)	Loss 0.8155 (0.7523)	Acc@1 75.391 (74.261)	Acc@5 97.266 (98.197)
Epoch: [210][128/196]	Time 0.290 (0.276)	Data 0.000 (0.007)	Loss 0.6637 (0.7536)	Acc@1 78.516 (74.143)	Acc@5 98.828 (98.198)
Epoch: [210][192/196]	Time 0.265 (0.280)	Data 0.000 (0.005)	Loss 0.8038 (0.7553)	Acc@1 73.438 (74.061)	Acc@5 98.047 (98.124)
after train
test acc: 28.7
Epoche: [211/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [211][0/196]	Time 0.364 (0.364)	Data 0.730 (0.730)	Loss 0.7666 (0.7666)	Acc@1 73.438 (73.438)	Acc@5 98.828 (98.828)
Epoch: [211][64/196]	Time 0.297 (0.278)	Data 0.000 (0.012)	Loss 0.6719 (0.7566)	Acc@1 74.609 (73.750)	Acc@5 98.438 (98.053)
Epoch: [211][128/196]	Time 0.285 (0.284)	Data 0.000 (0.007)	Loss 0.8361 (0.7503)	Acc@1 69.922 (73.967)	Acc@5 98.828 (98.053)
Epoch: [211][192/196]	Time 0.286 (0.284)	Data 0.000 (0.005)	Loss 0.7229 (0.7513)	Acc@1 74.219 (73.984)	Acc@5 98.047 (98.039)
after train
test acc: 33.0
Epoche: [212/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [212][0/196]	Time 0.348 (0.348)	Data 0.596 (0.596)	Loss 0.8015 (0.8015)	Acc@1 70.312 (70.312)	Acc@5 97.656 (97.656)
Epoch: [212][64/196]	Time 0.273 (0.267)	Data 0.000 (0.010)	Loss 0.7102 (0.7588)	Acc@1 75.781 (73.924)	Acc@5 98.438 (98.047)
Epoch: [212][128/196]	Time 0.265 (0.268)	Data 0.000 (0.006)	Loss 0.7947 (0.7537)	Acc@1 73.828 (74.010)	Acc@5 96.484 (98.004)
Epoch: [212][192/196]	Time 0.348 (0.269)	Data 0.000 (0.004)	Loss 0.5996 (0.7578)	Acc@1 79.688 (73.810)	Acc@5 98.047 (98.002)
after train
test acc: 30.46
Epoche: [213/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [213][0/196]	Time 0.312 (0.312)	Data 0.677 (0.677)	Loss 0.7682 (0.7682)	Acc@1 70.703 (70.703)	Acc@5 97.656 (97.656)
Epoch: [213][64/196]	Time 0.262 (0.270)	Data 0.000 (0.011)	Loss 0.7202 (0.7682)	Acc@1 73.438 (73.498)	Acc@5 99.609 (97.867)
Epoch: [213][128/196]	Time 0.309 (0.276)	Data 0.011 (0.006)	Loss 0.7845 (0.7603)	Acc@1 71.094 (73.877)	Acc@5 98.438 (97.989)
Epoch: [213][192/196]	Time 0.347 (0.281)	Data 0.000 (0.004)	Loss 0.6857 (0.7569)	Acc@1 75.000 (73.913)	Acc@5 98.828 (98.039)
after train
test acc: 62.76
Epoche: [214/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [214][0/196]	Time 0.364 (0.364)	Data 0.779 (0.779)	Loss 0.5800 (0.5800)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [214][64/196]	Time 0.222 (0.267)	Data 0.000 (0.013)	Loss 0.6956 (0.7503)	Acc@1 75.781 (73.822)	Acc@5 98.047 (98.017)
Epoch: [214][128/196]	Time 0.335 (0.274)	Data 0.000 (0.007)	Loss 0.7478 (0.7463)	Acc@1 73.438 (74.207)	Acc@5 98.047 (98.017)
Epoch: [214][192/196]	Time 0.251 (0.277)	Data 0.000 (0.005)	Loss 0.8504 (0.7516)	Acc@1 69.531 (73.948)	Acc@5 96.875 (98.010)
after train
test acc: 60.75
Epoche: [215/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [215][0/196]	Time 0.335 (0.335)	Data 0.615 (0.615)	Loss 0.8564 (0.8564)	Acc@1 71.484 (71.484)	Acc@5 98.828 (98.828)
Epoch: [215][64/196]	Time 0.216 (0.264)	Data 0.000 (0.011)	Loss 0.8100 (0.7614)	Acc@1 71.094 (73.630)	Acc@5 97.656 (98.161)
Epoch: [215][128/196]	Time 0.328 (0.284)	Data 0.007 (0.006)	Loss 0.7510 (0.7532)	Acc@1 71.484 (73.970)	Acc@5 98.438 (98.138)
Epoch: [215][192/196]	Time 0.404 (0.289)	Data 0.000 (0.004)	Loss 0.8012 (0.7516)	Acc@1 72.656 (74.006)	Acc@5 98.438 (98.077)
after train
test acc: 45.72
Epoche: [216/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [216][0/196]	Time 0.394 (0.394)	Data 0.638 (0.638)	Loss 0.7756 (0.7756)	Acc@1 71.484 (71.484)	Acc@5 97.656 (97.656)
Epoch: [216][64/196]	Time 0.360 (0.262)	Data 0.000 (0.011)	Loss 0.7113 (0.7483)	Acc@1 75.781 (73.990)	Acc@5 98.828 (98.035)
Epoch: [216][128/196]	Time 0.192 (0.269)	Data 0.000 (0.006)	Loss 0.7526 (0.7522)	Acc@1 74.219 (73.840)	Acc@5 97.656 (98.083)
Epoch: [216][192/196]	Time 0.266 (0.274)	Data 0.000 (0.004)	Loss 0.8705 (0.7520)	Acc@1 71.484 (73.917)	Acc@5 97.266 (98.021)
after train
test acc: 57.09
Epoche: [217/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [217][0/196]	Time 0.427 (0.427)	Data 0.684 (0.684)	Loss 0.7369 (0.7369)	Acc@1 73.047 (73.047)	Acc@5 98.438 (98.438)
Epoch: [217][64/196]	Time 0.262 (0.267)	Data 0.000 (0.012)	Loss 0.6697 (0.7488)	Acc@1 74.609 (73.738)	Acc@5 97.656 (98.107)
Epoch: [217][128/196]	Time 0.187 (0.275)	Data 0.000 (0.006)	Loss 0.7384 (0.7458)	Acc@1 75.391 (73.973)	Acc@5 98.438 (98.150)
Epoch: [217][192/196]	Time 0.261 (0.276)	Data 0.000 (0.005)	Loss 0.7154 (0.7483)	Acc@1 74.609 (73.996)	Acc@5 97.656 (98.021)
after train
test acc: 58.04
Epoche: [218/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [218][0/196]	Time 0.317 (0.317)	Data 0.707 (0.707)	Loss 0.7356 (0.7356)	Acc@1 72.656 (72.656)	Acc@5 97.266 (97.266)
Epoch: [218][64/196]	Time 0.351 (0.296)	Data 0.000 (0.011)	Loss 0.6490 (0.7370)	Acc@1 77.344 (74.621)	Acc@5 98.828 (98.077)
Epoch: [218][128/196]	Time 0.359 (0.285)	Data 0.000 (0.006)	Loss 0.7147 (0.7486)	Acc@1 78.125 (74.122)	Acc@5 98.047 (98.077)
Epoch: [218][192/196]	Time 0.252 (0.279)	Data 0.000 (0.004)	Loss 0.8564 (0.7504)	Acc@1 71.875 (74.124)	Acc@5 96.484 (98.126)
after train
test acc: 32.09
Epoche: [219/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [219][0/196]	Time 0.277 (0.277)	Data 0.752 (0.752)	Loss 0.8181 (0.8181)	Acc@1 72.266 (72.266)	Acc@5 96.875 (96.875)
Epoch: [219][64/196]	Time 0.171 (0.274)	Data 0.000 (0.013)	Loss 0.7674 (0.7512)	Acc@1 72.266 (74.038)	Acc@5 98.047 (98.041)
Epoch: [219][128/196]	Time 0.250 (0.280)	Data 0.000 (0.007)	Loss 0.7656 (0.7569)	Acc@1 71.875 (73.883)	Acc@5 98.047 (97.932)
Epoch: [219][192/196]	Time 0.325 (0.287)	Data 0.000 (0.005)	Loss 0.6303 (0.7506)	Acc@1 75.781 (74.075)	Acc@5 98.828 (98.000)
after train
test acc: 64.02
Epoche: [220/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [220][0/196]	Time 0.404 (0.404)	Data 0.781 (0.781)	Loss 0.9016 (0.9016)	Acc@1 69.141 (69.141)	Acc@5 96.094 (96.094)
Epoch: [220][64/196]	Time 0.228 (0.284)	Data 0.000 (0.013)	Loss 0.6778 (0.7502)	Acc@1 78.125 (73.936)	Acc@5 99.219 (97.945)
Epoch: [220][128/196]	Time 0.249 (0.285)	Data 0.000 (0.007)	Loss 0.7275 (0.7491)	Acc@1 76.562 (73.743)	Acc@5 98.047 (98.080)
Epoch: [220][192/196]	Time 0.293 (0.290)	Data 0.000 (0.005)	Loss 0.7362 (0.7480)	Acc@1 72.266 (73.925)	Acc@5 98.438 (97.994)
after train
test acc: 64.73
Epoche: [221/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [221][0/196]	Time 0.443 (0.443)	Data 0.695 (0.695)	Loss 0.7064 (0.7064)	Acc@1 73.047 (73.047)	Acc@5 98.828 (98.828)
Epoch: [221][64/196]	Time 0.374 (0.287)	Data 0.000 (0.012)	Loss 0.7194 (0.7500)	Acc@1 73.438 (74.447)	Acc@5 98.438 (98.071)
Epoch: [221][128/196]	Time 0.300 (0.276)	Data 0.000 (0.006)	Loss 0.8381 (0.7376)	Acc@1 72.656 (74.570)	Acc@5 97.266 (98.132)
Epoch: [221][192/196]	Time 0.199 (0.276)	Data 0.000 (0.005)	Loss 0.6380 (0.7429)	Acc@1 74.219 (74.395)	Acc@5 99.219 (98.110)
after train
test acc: 54.97
Epoche: [222/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [222][0/196]	Time 0.380 (0.380)	Data 0.522 (0.522)	Loss 0.7257 (0.7257)	Acc@1 72.266 (72.266)	Acc@5 98.047 (98.047)
Epoch: [222][64/196]	Time 0.286 (0.281)	Data 0.000 (0.010)	Loss 0.6800 (0.7410)	Acc@1 76.172 (74.285)	Acc@5 98.828 (98.083)
Epoch: [222][128/196]	Time 0.319 (0.287)	Data 0.000 (0.005)	Loss 0.8394 (0.7445)	Acc@1 72.266 (74.113)	Acc@5 98.438 (98.129)
Epoch: [222][192/196]	Time 0.209 (0.289)	Data 0.000 (0.004)	Loss 0.6789 (0.7473)	Acc@1 76.172 (74.012)	Acc@5 98.438 (98.047)
after train
test acc: 59.23
Epoche: [223/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [223][0/196]	Time 0.351 (0.351)	Data 0.636 (0.636)	Loss 0.5457 (0.5457)	Acc@1 81.641 (81.641)	Acc@5 99.219 (99.219)
Epoch: [223][64/196]	Time 0.249 (0.274)	Data 0.000 (0.010)	Loss 0.6705 (0.7496)	Acc@1 77.344 (73.798)	Acc@5 97.656 (98.161)
Epoch: [223][128/196]	Time 0.402 (0.277)	Data 0.007 (0.006)	Loss 0.7630 (0.7556)	Acc@1 71.094 (73.731)	Acc@5 97.266 (98.113)
Epoch: [223][192/196]	Time 0.171 (0.277)	Data 0.000 (0.004)	Loss 0.6898 (0.7500)	Acc@1 77.734 (74.033)	Acc@5 97.266 (98.085)
after train
test acc: 59.31
Epoche: [224/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [224][0/196]	Time 0.398 (0.398)	Data 0.703 (0.703)	Loss 0.6061 (0.6061)	Acc@1 77.734 (77.734)	Acc@5 99.219 (99.219)
Epoch: [224][64/196]	Time 0.285 (0.286)	Data 0.000 (0.012)	Loss 0.7790 (0.7418)	Acc@1 73.438 (74.615)	Acc@5 98.828 (98.209)
Epoch: [224][128/196]	Time 0.276 (0.286)	Data 0.000 (0.007)	Loss 0.7921 (0.7427)	Acc@1 70.312 (74.394)	Acc@5 98.438 (98.204)
Epoch: [224][192/196]	Time 0.294 (0.282)	Data 0.000 (0.005)	Loss 0.6888 (0.7424)	Acc@1 75.781 (74.348)	Acc@5 98.828 (98.191)
after train
test acc: 68.77
Epoche: [225/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [225][0/196]	Time 0.377 (0.377)	Data 0.547 (0.547)	Loss 0.8220 (0.8220)	Acc@1 70.312 (70.312)	Acc@5 97.266 (97.266)
Epoch: [225][64/196]	Time 0.252 (0.272)	Data 0.000 (0.010)	Loss 0.8040 (0.7422)	Acc@1 71.484 (74.351)	Acc@5 98.047 (98.041)
Epoch: [225][128/196]	Time 0.304 (0.286)	Data 0.000 (0.006)	Loss 0.6898 (0.7435)	Acc@1 80.078 (74.382)	Acc@5 98.438 (98.101)
Epoch: [225][192/196]	Time 0.325 (0.288)	Data 0.000 (0.004)	Loss 0.6652 (0.7409)	Acc@1 79.297 (74.429)	Acc@5 98.047 (98.140)
after train
test acc: 57.22
Epoche: [226/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [226][0/196]	Time 0.426 (0.426)	Data 0.589 (0.589)	Loss 0.7425 (0.7425)	Acc@1 74.219 (74.219)	Acc@5 96.875 (96.875)
Epoch: [226][64/196]	Time 0.314 (0.279)	Data 0.000 (0.010)	Loss 0.7535 (0.7491)	Acc@1 73.828 (74.309)	Acc@5 97.656 (98.149)
Epoch: [226][128/196]	Time 0.385 (0.278)	Data 0.000 (0.005)	Loss 0.7420 (0.7439)	Acc@1 73.438 (74.213)	Acc@5 96.875 (98.113)
Epoch: [226][192/196]	Time 0.272 (0.272)	Data 0.000 (0.004)	Loss 0.8594 (0.7398)	Acc@1 69.922 (74.292)	Acc@5 97.656 (98.112)
after train
test acc: 52.25
Epoche: [227/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [227][0/196]	Time 0.375 (0.375)	Data 0.570 (0.570)	Loss 0.6996 (0.6996)	Acc@1 75.000 (75.000)	Acc@5 98.828 (98.828)
Epoch: [227][64/196]	Time 0.197 (0.283)	Data 0.000 (0.009)	Loss 0.6491 (0.7463)	Acc@1 76.953 (74.117)	Acc@5 98.828 (97.957)
Epoch: [227][128/196]	Time 0.241 (0.278)	Data 0.000 (0.006)	Loss 0.5907 (0.7422)	Acc@1 80.078 (74.343)	Acc@5 99.219 (98.056)
Epoch: [227][192/196]	Time 0.347 (0.284)	Data 0.000 (0.004)	Loss 0.7348 (0.7372)	Acc@1 71.875 (74.524)	Acc@5 98.047 (98.069)
after train
test acc: 30.21
Epoche: [228/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [228][0/196]	Time 0.320 (0.320)	Data 0.548 (0.548)	Loss 0.6409 (0.6409)	Acc@1 77.734 (77.734)	Acc@5 98.828 (98.828)
Epoch: [228][64/196]	Time 0.332 (0.280)	Data 0.000 (0.009)	Loss 0.8444 (0.7458)	Acc@1 70.703 (74.171)	Acc@5 98.828 (98.167)
Epoch: [228][128/196]	Time 0.260 (0.284)	Data 0.000 (0.005)	Loss 0.7026 (0.7416)	Acc@1 80.078 (74.712)	Acc@5 98.047 (98.068)
Epoch: [228][192/196]	Time 0.277 (0.284)	Data 0.000 (0.003)	Loss 0.6426 (0.7422)	Acc@1 76.562 (74.599)	Acc@5 98.438 (98.051)
after train
test acc: 52.32
Epoche: [229/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [229][0/196]	Time 0.291 (0.291)	Data 0.593 (0.593)	Loss 0.5961 (0.5961)	Acc@1 79.688 (79.688)	Acc@5 97.656 (97.656)
Epoch: [229][64/196]	Time 0.301 (0.270)	Data 0.000 (0.010)	Loss 0.7436 (0.7445)	Acc@1 77.344 (74.447)	Acc@5 96.875 (97.933)
Epoch: [229][128/196]	Time 0.289 (0.283)	Data 0.000 (0.006)	Loss 0.8470 (0.7389)	Acc@1 70.312 (74.307)	Acc@5 97.266 (98.107)
Epoch: [229][192/196]	Time 0.249 (0.285)	Data 0.000 (0.004)	Loss 0.7423 (0.7373)	Acc@1 71.875 (74.524)	Acc@5 98.828 (98.093)
after train
test acc: 40.02
Epoche: [230/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [230][0/196]	Time 0.371 (0.371)	Data 0.568 (0.568)	Loss 0.7091 (0.7091)	Acc@1 74.609 (74.609)	Acc@5 98.438 (98.438)
Epoch: [230][64/196]	Time 0.275 (0.274)	Data 0.000 (0.010)	Loss 0.7665 (0.7404)	Acc@1 70.703 (74.237)	Acc@5 98.047 (98.035)
Epoch: [230][128/196]	Time 0.343 (0.276)	Data 0.000 (0.005)	Loss 0.7614 (0.7364)	Acc@1 73.047 (74.440)	Acc@5 98.828 (98.144)
Epoch: [230][192/196]	Time 0.241 (0.280)	Data 0.000 (0.004)	Loss 0.7919 (0.7387)	Acc@1 72.266 (74.316)	Acc@5 97.266 (98.079)
after train
test acc: 38.88
Epoche: [231/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [231][0/196]	Time 0.285 (0.285)	Data 0.519 (0.519)	Loss 0.7103 (0.7103)	Acc@1 75.781 (75.781)	Acc@5 98.047 (98.047)
Epoch: [231][64/196]	Time 0.198 (0.274)	Data 0.000 (0.009)	Loss 0.8203 (0.7328)	Acc@1 73.047 (74.724)	Acc@5 98.828 (98.089)
Epoch: [231][128/196]	Time 0.396 (0.275)	Data 0.000 (0.005)	Loss 0.7844 (0.7269)	Acc@1 71.875 (74.918)	Acc@5 97.656 (98.107)
Epoch: [231][192/196]	Time 0.293 (0.281)	Data 0.000 (0.004)	Loss 0.6283 (0.7298)	Acc@1 77.734 (74.814)	Acc@5 99.219 (98.120)
after train
test acc: 49.51
Epoche: [232/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [232][0/196]	Time 0.483 (0.483)	Data 0.477 (0.477)	Loss 0.7853 (0.7853)	Acc@1 73.047 (73.047)	Acc@5 98.047 (98.047)
Epoch: [232][64/196]	Time 0.185 (0.287)	Data 0.000 (0.008)	Loss 0.6964 (0.7409)	Acc@1 76.953 (74.465)	Acc@5 98.828 (98.053)
Epoch: [232][128/196]	Time 0.224 (0.288)	Data 0.000 (0.004)	Loss 0.8620 (0.7405)	Acc@1 69.922 (74.503)	Acc@5 96.484 (98.035)
Epoch: [232][192/196]	Time 0.329 (0.284)	Data 0.000 (0.003)	Loss 0.7033 (0.7392)	Acc@1 75.000 (74.551)	Acc@5 97.266 (98.087)
after train
test acc: 19.25
Epoche: [233/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [233][0/196]	Time 0.324 (0.324)	Data 0.795 (0.795)	Loss 0.6327 (0.6327)	Acc@1 75.781 (75.781)	Acc@5 98.828 (98.828)
Epoch: [233][64/196]	Time 0.308 (0.275)	Data 0.000 (0.013)	Loss 0.7276 (0.7401)	Acc@1 75.000 (74.309)	Acc@5 97.656 (98.029)
Epoch: [233][128/196]	Time 0.339 (0.286)	Data 0.000 (0.007)	Loss 0.7813 (0.7383)	Acc@1 72.266 (74.470)	Acc@5 97.656 (97.947)
Epoch: [233][192/196]	Time 0.390 (0.285)	Data 0.000 (0.005)	Loss 0.6951 (0.7431)	Acc@1 77.344 (74.377)	Acc@5 98.828 (97.936)
after train
test acc: 52.15
Epoche: [234/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [234][0/196]	Time 0.350 (0.350)	Data 0.506 (0.506)	Loss 0.6415 (0.6415)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [234][64/196]	Time 0.284 (0.273)	Data 0.000 (0.009)	Loss 0.7696 (0.7264)	Acc@1 73.438 (74.910)	Acc@5 97.656 (98.329)
Epoch: [234][128/196]	Time 0.216 (0.279)	Data 0.000 (0.005)	Loss 0.7978 (0.7277)	Acc@1 73.047 (74.827)	Acc@5 98.438 (98.286)
Epoch: [234][192/196]	Time 0.271 (0.282)	Data 0.000 (0.004)	Loss 0.7850 (0.7346)	Acc@1 73.047 (74.591)	Acc@5 97.656 (98.257)
after train
test acc: 33.19
Epoche: [235/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [235][0/196]	Time 0.231 (0.231)	Data 0.540 (0.540)	Loss 0.7621 (0.7621)	Acc@1 73.047 (73.047)	Acc@5 97.656 (97.656)
Epoch: [235][64/196]	Time 0.290 (0.274)	Data 0.000 (0.009)	Loss 0.7521 (0.7318)	Acc@1 73.047 (74.874)	Acc@5 99.219 (98.161)
Epoch: [235][128/196]	Time 0.268 (0.284)	Data 0.000 (0.005)	Loss 0.7198 (0.7366)	Acc@1 75.000 (74.667)	Acc@5 97.656 (98.074)
Epoch: [235][192/196]	Time 0.287 (0.281)	Data 0.000 (0.003)	Loss 0.7868 (0.7371)	Acc@1 73.047 (74.468)	Acc@5 97.656 (98.126)
after train
test acc: 43.19
Epoche: [236/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [236][0/196]	Time 0.340 (0.340)	Data 0.773 (0.773)	Loss 0.8565 (0.8565)	Acc@1 72.266 (72.266)	Acc@5 97.266 (97.266)
Epoch: [236][64/196]	Time 0.270 (0.270)	Data 0.000 (0.013)	Loss 0.8312 (0.7465)	Acc@1 72.656 (74.249)	Acc@5 97.656 (98.101)
Epoch: [236][128/196]	Time 0.321 (0.279)	Data 0.000 (0.007)	Loss 0.6264 (0.7320)	Acc@1 79.297 (74.785)	Acc@5 96.484 (98.141)
Epoch: [236][192/196]	Time 0.280 (0.283)	Data 0.000 (0.005)	Loss 0.7442 (0.7288)	Acc@1 75.000 (74.858)	Acc@5 99.219 (98.209)
after train
test acc: 64.6
Epoche: [237/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [237][0/196]	Time 0.383 (0.383)	Data 0.622 (0.622)	Loss 0.6781 (0.6781)	Acc@1 75.391 (75.391)	Acc@5 97.266 (97.266)
Epoch: [237][64/196]	Time 0.378 (0.284)	Data 0.000 (0.010)	Loss 0.6447 (0.7319)	Acc@1 78.906 (74.537)	Acc@5 98.438 (98.233)
Epoch: [237][128/196]	Time 0.254 (0.285)	Data 0.000 (0.005)	Loss 0.7471 (0.7301)	Acc@1 73.438 (74.679)	Acc@5 97.656 (98.216)
Epoch: [237][192/196]	Time 0.311 (0.280)	Data 0.000 (0.004)	Loss 0.6745 (0.7379)	Acc@1 76.953 (74.413)	Acc@5 99.219 (98.148)
after train
test acc: 53.68
Epoche: [238/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [238][0/196]	Time 0.398 (0.398)	Data 0.612 (0.612)	Loss 0.7086 (0.7086)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [238][64/196]	Time 0.536 (0.286)	Data 0.000 (0.011)	Loss 0.7440 (0.7234)	Acc@1 73.828 (74.892)	Acc@5 98.438 (98.245)
Epoch: [238][128/196]	Time 0.275 (0.287)	Data 0.000 (0.006)	Loss 0.7820 (0.7313)	Acc@1 73.047 (74.755)	Acc@5 99.219 (98.210)
Epoch: [238][192/196]	Time 0.305 (0.285)	Data 0.000 (0.004)	Loss 0.7077 (0.7305)	Acc@1 77.344 (74.808)	Acc@5 97.266 (98.172)
after train
test acc: 67.8
Epoche: [239/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [239][0/196]	Time 0.381 (0.381)	Data 0.726 (0.726)	Loss 0.6801 (0.6801)	Acc@1 79.297 (79.297)	Acc@5 97.656 (97.656)
Epoch: [239][64/196]	Time 0.223 (0.276)	Data 0.000 (0.012)	Loss 0.7263 (0.7387)	Acc@1 75.391 (74.489)	Acc@5 98.047 (98.017)
Epoch: [239][128/196]	Time 0.313 (0.283)	Data 0.025 (0.007)	Loss 0.7878 (0.7332)	Acc@1 69.141 (74.737)	Acc@5 98.047 (98.053)
Epoch: [239][192/196]	Time 0.276 (0.283)	Data 0.000 (0.004)	Loss 0.7178 (0.7339)	Acc@1 75.391 (74.676)	Acc@5 97.656 (98.085)
after train
test acc: 54.98
Epoche: [240/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [240][0/196]	Time 0.343 (0.343)	Data 0.716 (0.716)	Loss 0.7189 (0.7189)	Acc@1 71.875 (71.875)	Acc@5 98.047 (98.047)
Epoch: [240][64/196]	Time 0.224 (0.268)	Data 0.000 (0.012)	Loss 0.7392 (0.7357)	Acc@1 75.000 (74.345)	Acc@5 97.656 (98.149)
Epoch: [240][128/196]	Time 0.166 (0.273)	Data 0.000 (0.007)	Loss 0.6805 (0.7361)	Acc@1 76.172 (74.467)	Acc@5 99.219 (98.132)
Epoch: [240][192/196]	Time 0.209 (0.283)	Data 0.000 (0.005)	Loss 0.7107 (0.7390)	Acc@1 76.953 (74.411)	Acc@5 96.875 (98.079)
after train
test acc: 59.71
[INFO] Storing checkpoint...
Max memory: 29.5321088
