no display found. Using non-interactive Agg backend
[3, 3, 3]
[4, 8, 16]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 120; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/room1x1/model.nn; checkpoint: ./output/experimente4/room111; saveModell: True; LR: 0.1
random number: 2469
Files already downloaded and verified
==> Resuming from checkpoint..
Start epoch: 121
First Lr: 0.1
Startepoche: 121
deeper epoch: 0
Epoche: [121/240]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [121][0/196]	Time 0.111 (0.111)	Data 0.426 (0.426)	Loss 0.5403 (0.5403)	Acc@1 82.031 (82.031)	Acc@5 98.438 (98.438)
Epoch: [121][64/196]	Time 0.060 (0.071)	Data 0.000 (0.007)	Loss 0.6068 (0.6181)	Acc@1 80.469 (78.389)	Acc@5 98.438 (98.816)
Epoch: [121][128/196]	Time 0.051 (0.069)	Data 0.000 (0.004)	Loss 0.6653 (0.6247)	Acc@1 76.172 (78.264)	Acc@5 97.266 (98.686)
Epoch: [121][192/196]	Time 0.062 (0.067)	Data 0.000 (0.002)	Loss 0.6810 (0.6248)	Acc@1 75.781 (78.257)	Acc@5 97.266 (98.660)
after train
test acc: 77.62
IndexL: 0
Module= Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); indexL: 1
indexConv: 1
modulelist[indexConv]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
indexConv: 2
modulelist[indexConv]: ReLU(inplace=True)
indexConv: 3
modulelist[indexConv]: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); indexConv: 3; index: 3
shape new w1: (16, 3, 3, 3)
shape new w2: (8, 16, 3, 3); old w2: (8, 8, 3, 3)
Batchnorm1
IndexL: 3
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 3
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 3
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 4
shape new w1: (16, 16, 3, 3)
shape new w2: (8, 16, 3, 3); old w2: (8, 8, 3, 3)
Batchnorm1
IndexL: 4
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 4
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 4
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 5
shape new w1: (16, 16, 3, 3)
shape new w2: (8, 16, 3, 3); old w2: (8, 8, 3, 3)
Batchnorm1
IndexL: 5
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 5
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 5
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 6
shape new w1: (16, 16, 3, 3)
shape new w2: (16, 16, 3, 3); old w2: (16, 8, 3, 3)
Batchnorm1
IndexL: 6
Module= Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)); i: 0 indexL: 6
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 6
Module= Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)); i: 0 indexL: 7
shape new w1: (32, 16, 3, 3)
module: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
Batchnorm2
1: Module: 6; 0; moduleBn: 6; 1; module1: 7; 0
shape new w2: (16, 16, 1, 1)
IndexL: 7
Module= Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2)); i: 0 indexL: 7
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 7
Module= Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2)); i: 0 indexL: 8
shape new w1: (32, 16, 1, 1)
shape new w2: (16, 32, 3, 3); old w2: (16, 16, 3, 3)
Batchnorm1
IndexL: 8
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 8
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 8
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 9
shape new w1: (32, 32, 3, 3)
shape new w2: (16, 32, 3, 3); old w2: (16, 16, 3, 3)
Batchnorm1
IndexL: 9
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 9
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 9
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 10
shape new w1: (32, 32, 3, 3)
shape new w2: (32, 32, 3, 3); old w2: (32, 16, 3, 3)
Batchnorm1
IndexL: 10
Module= Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)); i: 0 indexL: 10
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 10
Module= Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)); i: 0 indexL: 11
shape new w1: (64, 32, 3, 3)
module: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
Batchnorm2
1: Module: 10; 0; moduleBn: 10; 1; module1: 11; 0
shape new w2: (32, 32, 1, 1)
IndexL: 11
Module= Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2)); i: 0 indexL: 11
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 11
Module= Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2)); i: 0 indexL: 12
shape new w1: (64, 32, 1, 1)
shape new w2: (32, 64, 3, 3); old w2: (32, 32, 3, 3)
Batchnorm1
IndexL: 12
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 12
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 12
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 13
shape new w1: (64, 64, 3, 3)
shape new w2: (32, 64, 3, 3); old w2: (32, 32, 3, 3)
Batchnorm1
IndexL: 13
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0 indexL: 13
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 13
shape new w1: (64, 64, 3, 3)
shape new w2: (10, 64); old w2: (10, 32)
Batchnorm1
self: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): AdaptiveAvgPool2d(output_size=(1, 1))
    (15): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
module: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Size of Weight: torch.Size([16, 3, 3, 3])
module: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Size of Weight: torch.Size([16])
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
Size of Weight: torch.Size([32, 16, 3, 3])
Size of Weight: torch.Size([32])
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([32, 16, 1, 1])
Size of Weight: torch.Size([32])
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([32, 32, 3, 3])
Size of Weight: torch.Size([32])
Sequential: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([32, 32, 3, 3])
Size of Weight: torch.Size([32])
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
Size of Weight: torch.Size([64, 32, 3, 3])
Size of Weight: torch.Size([64])
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([64, 32, 1, 1])
Size of Weight: torch.Size([64])
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([64, 64, 3, 3])
Size of Weight: torch.Size([64])
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([64, 64, 3, 3])
Size of Weight: torch.Size([64])
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=64, out_features=10, bias=True)
Size of Weight: torch.Size([10, 64])
time for n2n: 0.04468798637390137
Epoche: [122/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [122][0/196]	Time 0.123 (0.123)	Data 0.422 (0.422)	Loss 1.5657 (1.5657)	Acc@1 49.219 (49.219)	Acc@5 91.406 (91.406)
Epoch: [122][64/196]	Time 0.100 (0.085)	Data 0.000 (0.007)	Loss 0.6138 (0.7615)	Acc@1 78.516 (73.419)	Acc@5 98.047 (98.197)
Epoch: [122][128/196]	Time 0.078 (0.082)	Data 0.000 (0.004)	Loss 0.6312 (0.7153)	Acc@1 78.125 (75.348)	Acc@5 98.828 (98.350)
Epoch: [122][192/196]	Time 0.064 (0.081)	Data 0.000 (0.002)	Loss 0.7790 (0.6965)	Acc@1 73.828 (75.955)	Acc@5 97.656 (98.421)
after train
test acc: 70.57
Epoche: [123/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [123][0/196]	Time 0.112 (0.112)	Data 0.401 (0.401)	Loss 0.6204 (0.6204)	Acc@1 81.250 (81.250)	Acc@5 98.047 (98.047)
Epoch: [123][64/196]	Time 0.067 (0.091)	Data 0.000 (0.006)	Loss 0.5898 (0.6257)	Acc@1 78.125 (78.498)	Acc@5 98.828 (98.828)
Epoch: [123][128/196]	Time 0.068 (0.085)	Data 0.000 (0.003)	Loss 0.5118 (0.6211)	Acc@1 81.641 (78.606)	Acc@5 99.219 (98.828)
Epoch: [123][192/196]	Time 0.072 (0.082)	Data 0.000 (0.002)	Loss 0.5965 (0.6183)	Acc@1 79.297 (78.669)	Acc@5 98.047 (98.826)
after train
test acc: 67.41
Epoche: [124/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [124][0/196]	Time 0.136 (0.136)	Data 0.350 (0.350)	Loss 0.6569 (0.6569)	Acc@1 75.391 (75.391)	Acc@5 99.219 (99.219)
Epoch: [124][64/196]	Time 0.073 (0.080)	Data 0.000 (0.006)	Loss 0.5479 (0.6207)	Acc@1 80.469 (78.395)	Acc@5 99.219 (98.744)
Epoch: [124][128/196]	Time 0.067 (0.080)	Data 0.000 (0.003)	Loss 0.5249 (0.6097)	Acc@1 79.688 (78.691)	Acc@5 100.000 (98.783)
Epoch: [124][192/196]	Time 0.062 (0.080)	Data 0.000 (0.002)	Loss 0.6018 (0.6058)	Acc@1 77.734 (78.906)	Acc@5 99.219 (98.796)
after train
test acc: 71.34
Epoche: [125/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [125][0/196]	Time 0.097 (0.097)	Data 0.315 (0.315)	Loss 0.5299 (0.5299)	Acc@1 81.641 (81.641)	Acc@5 99.219 (99.219)
Epoch: [125][64/196]	Time 0.127 (0.088)	Data 0.000 (0.005)	Loss 0.6045 (0.5733)	Acc@1 77.734 (80.240)	Acc@5 98.828 (99.032)
Epoch: [125][128/196]	Time 0.074 (0.084)	Data 0.000 (0.003)	Loss 0.4423 (0.5787)	Acc@1 84.766 (80.051)	Acc@5 99.609 (98.961)
Epoch: [125][192/196]	Time 0.073 (0.082)	Data 0.000 (0.002)	Loss 0.5991 (0.5867)	Acc@1 80.078 (79.692)	Acc@5 98.828 (98.901)
after train
test acc: 70.19
Epoche: [126/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [126][0/196]	Time 0.144 (0.144)	Data 0.356 (0.356)	Loss 0.5276 (0.5276)	Acc@1 80.859 (80.859)	Acc@5 99.609 (99.609)
Epoch: [126][64/196]	Time 0.072 (0.085)	Data 0.000 (0.006)	Loss 0.6101 (0.5784)	Acc@1 78.906 (79.970)	Acc@5 98.828 (99.008)
Epoch: [126][128/196]	Time 0.082 (0.084)	Data 0.000 (0.003)	Loss 0.5442 (0.5716)	Acc@1 80.469 (80.399)	Acc@5 98.828 (98.983)
Epoch: [126][192/196]	Time 0.062 (0.082)	Data 0.000 (0.002)	Loss 0.5679 (0.5759)	Acc@1 81.641 (80.096)	Acc@5 98.828 (98.948)
after train
test acc: 67.2
Epoche: [127/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [127][0/196]	Time 0.135 (0.135)	Data 0.303 (0.303)	Loss 0.5629 (0.5629)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [127][64/196]	Time 0.099 (0.090)	Data 0.000 (0.005)	Loss 0.7019 (0.5717)	Acc@1 74.609 (80.024)	Acc@5 99.219 (99.038)
Epoch: [127][128/196]	Time 0.096 (0.085)	Data 0.000 (0.003)	Loss 0.6370 (0.5749)	Acc@1 77.734 (80.114)	Acc@5 99.609 (98.973)
Epoch: [127][192/196]	Time 0.077 (0.084)	Data 0.000 (0.002)	Loss 0.5802 (0.5734)	Acc@1 76.562 (80.220)	Acc@5 99.609 (98.960)
after train
test acc: 75.42
Epoche: [128/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [128][0/196]	Time 0.113 (0.113)	Data 0.346 (0.346)	Loss 0.5764 (0.5764)	Acc@1 79.297 (79.297)	Acc@5 99.609 (99.609)
Epoch: [128][64/196]	Time 0.061 (0.083)	Data 0.000 (0.006)	Loss 0.5824 (0.5792)	Acc@1 78.125 (79.982)	Acc@5 99.609 (98.984)
Epoch: [128][128/196]	Time 0.091 (0.080)	Data 0.000 (0.003)	Loss 0.4987 (0.5645)	Acc@1 83.203 (80.405)	Acc@5 99.609 (98.980)
Epoch: [128][192/196]	Time 0.071 (0.079)	Data 0.000 (0.002)	Loss 0.4862 (0.5625)	Acc@1 83.594 (80.523)	Acc@5 99.219 (98.980)
after train
test acc: 69.29
Epoche: [129/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [129][0/196]	Time 0.097 (0.097)	Data 0.395 (0.395)	Loss 0.5003 (0.5003)	Acc@1 79.688 (79.688)	Acc@5 100.000 (100.000)
Epoch: [129][64/196]	Time 0.073 (0.082)	Data 0.000 (0.006)	Loss 0.5942 (0.5600)	Acc@1 79.297 (80.529)	Acc@5 98.438 (99.044)
Epoch: [129][128/196]	Time 0.072 (0.083)	Data 0.000 (0.003)	Loss 0.5259 (0.5585)	Acc@1 82.812 (80.738)	Acc@5 98.828 (99.019)
Epoch: [129][192/196]	Time 0.066 (0.081)	Data 0.000 (0.002)	Loss 0.6139 (0.5634)	Acc@1 79.688 (80.517)	Acc@5 98.828 (99.031)
after train
test acc: 68.65
Epoche: [130/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [130][0/196]	Time 0.139 (0.139)	Data 0.518 (0.518)	Loss 0.5433 (0.5433)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [130][64/196]	Time 0.070 (0.082)	Data 0.000 (0.008)	Loss 0.5563 (0.5628)	Acc@1 80.859 (80.895)	Acc@5 98.828 (99.032)
Epoch: [130][128/196]	Time 0.067 (0.081)	Data 0.000 (0.004)	Loss 0.5691 (0.5558)	Acc@1 81.641 (80.881)	Acc@5 99.219 (99.070)
Epoch: [130][192/196]	Time 0.065 (0.080)	Data 0.000 (0.003)	Loss 0.4892 (0.5548)	Acc@1 83.594 (80.872)	Acc@5 98.438 (99.083)
after train
test acc: 71.13
Epoche: [131/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [131][0/196]	Time 0.097 (0.097)	Data 0.440 (0.440)	Loss 0.5343 (0.5343)	Acc@1 79.688 (79.688)	Acc@5 99.609 (99.609)
Epoch: [131][64/196]	Time 0.136 (0.083)	Data 0.000 (0.007)	Loss 0.5164 (0.5396)	Acc@1 82.422 (81.298)	Acc@5 98.047 (99.050)
Epoch: [131][128/196]	Time 0.100 (0.083)	Data 0.000 (0.004)	Loss 0.6108 (0.5356)	Acc@1 78.516 (81.426)	Acc@5 99.219 (99.092)
Epoch: [131][192/196]	Time 0.071 (0.083)	Data 0.000 (0.003)	Loss 0.6018 (0.5427)	Acc@1 79.688 (81.305)	Acc@5 98.047 (99.037)
after train
test acc: 59.37
Epoche: [132/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [132][0/196]	Time 0.121 (0.121)	Data 0.418 (0.418)	Loss 0.4668 (0.4668)	Acc@1 81.641 (81.641)	Acc@5 100.000 (100.000)
Epoch: [132][64/196]	Time 0.090 (0.091)	Data 0.000 (0.007)	Loss 0.4914 (0.5314)	Acc@1 80.469 (81.412)	Acc@5 99.219 (99.153)
Epoch: [132][128/196]	Time 0.080 (0.088)	Data 0.000 (0.004)	Loss 0.5258 (0.5345)	Acc@1 81.641 (81.459)	Acc@5 98.047 (99.152)
Epoch: [132][192/196]	Time 0.068 (0.085)	Data 0.000 (0.002)	Loss 0.5697 (0.5403)	Acc@1 79.688 (81.428)	Acc@5 98.828 (99.116)
after train
test acc: 69.0
Epoche: [133/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [133][0/196]	Time 0.128 (0.128)	Data 0.383 (0.383)	Loss 0.5975 (0.5975)	Acc@1 83.203 (83.203)	Acc@5 98.438 (98.438)
Epoch: [133][64/196]	Time 0.068 (0.078)	Data 0.000 (0.007)	Loss 0.6053 (0.5412)	Acc@1 76.953 (81.244)	Acc@5 99.219 (99.111)
Epoch: [133][128/196]	Time 0.070 (0.081)	Data 0.000 (0.004)	Loss 0.4740 (0.5321)	Acc@1 83.203 (81.668)	Acc@5 99.219 (99.064)
Epoch: [133][192/196]	Time 0.065 (0.081)	Data 0.000 (0.002)	Loss 0.5780 (0.5324)	Acc@1 81.250 (81.699)	Acc@5 98.047 (99.051)
after train
test acc: 71.45
Epoche: [134/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [134][0/196]	Time 0.096 (0.096)	Data 0.358 (0.358)	Loss 0.5012 (0.5012)	Acc@1 82.812 (82.812)	Acc@5 98.828 (98.828)
Epoch: [134][64/196]	Time 0.087 (0.082)	Data 0.000 (0.006)	Loss 0.6947 (0.5348)	Acc@1 74.609 (81.472)	Acc@5 98.047 (99.069)
Epoch: [134][128/196]	Time 0.114 (0.082)	Data 0.000 (0.003)	Loss 0.5376 (0.5297)	Acc@1 82.422 (81.671)	Acc@5 98.828 (99.122)
Epoch: [134][192/196]	Time 0.130 (0.081)	Data 0.000 (0.002)	Loss 0.6025 (0.5328)	Acc@1 81.641 (81.550)	Acc@5 98.828 (99.122)
after train
test acc: 71.59
Epoche: [135/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [135][0/196]	Time 0.122 (0.122)	Data 0.542 (0.542)	Loss 0.5073 (0.5073)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [135][64/196]	Time 0.073 (0.087)	Data 0.000 (0.009)	Loss 0.4482 (0.5198)	Acc@1 86.328 (81.941)	Acc@5 99.219 (99.153)
Epoch: [135][128/196]	Time 0.073 (0.085)	Data 0.000 (0.005)	Loss 0.5659 (0.5184)	Acc@1 80.469 (82.146)	Acc@5 99.219 (99.152)
Epoch: [135][192/196]	Time 0.067 (0.083)	Data 0.000 (0.003)	Loss 0.5857 (0.5221)	Acc@1 80.469 (82.051)	Acc@5 98.438 (99.146)
after train
test acc: 79.2
Epoche: [136/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [136][0/196]	Time 0.127 (0.127)	Data 0.326 (0.326)	Loss 0.5364 (0.5364)	Acc@1 82.422 (82.422)	Acc@5 99.219 (99.219)
Epoch: [136][64/196]	Time 0.070 (0.088)	Data 0.000 (0.005)	Loss 0.6075 (0.5234)	Acc@1 79.688 (81.899)	Acc@5 98.438 (99.123)
Epoch: [136][128/196]	Time 0.068 (0.084)	Data 0.000 (0.003)	Loss 0.5329 (0.5197)	Acc@1 80.469 (81.937)	Acc@5 98.828 (99.188)
Epoch: [136][192/196]	Time 0.062 (0.083)	Data 0.000 (0.002)	Loss 0.5722 (0.5235)	Acc@1 80.469 (81.879)	Acc@5 98.438 (99.156)
after train
test acc: 66.66
Epoche: [137/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [137][0/196]	Time 0.105 (0.105)	Data 0.330 (0.330)	Loss 0.4829 (0.4829)	Acc@1 83.594 (83.594)	Acc@5 98.828 (98.828)
Epoch: [137][64/196]	Time 0.088 (0.081)	Data 0.000 (0.005)	Loss 0.4777 (0.5164)	Acc@1 83.594 (82.097)	Acc@5 99.219 (99.123)
Epoch: [137][128/196]	Time 0.076 (0.083)	Data 0.000 (0.003)	Loss 0.4629 (0.5114)	Acc@1 82.422 (82.407)	Acc@5 99.609 (99.140)
Epoch: [137][192/196]	Time 0.063 (0.081)	Data 0.000 (0.002)	Loss 0.3451 (0.5133)	Acc@1 88.672 (82.197)	Acc@5 99.609 (99.174)
after train
test acc: 61.8
Epoche: [138/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [138][0/196]	Time 0.119 (0.119)	Data 0.460 (0.460)	Loss 0.5568 (0.5568)	Acc@1 82.422 (82.422)	Acc@5 98.828 (98.828)
Epoch: [138][64/196]	Time 0.077 (0.081)	Data 0.000 (0.007)	Loss 0.5158 (0.4989)	Acc@1 83.203 (82.963)	Acc@5 98.438 (99.219)
Epoch: [138][128/196]	Time 0.084 (0.081)	Data 0.000 (0.004)	Loss 0.5169 (0.4995)	Acc@1 82.031 (82.879)	Acc@5 98.828 (99.243)
Epoch: [138][192/196]	Time 0.092 (0.080)	Data 0.000 (0.003)	Loss 0.4503 (0.5055)	Acc@1 84.375 (82.701)	Acc@5 100.000 (99.229)
after train
test acc: 67.95
Epoche: [139/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [139][0/196]	Time 0.128 (0.128)	Data 0.308 (0.308)	Loss 0.4666 (0.4666)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [139][64/196]	Time 0.088 (0.080)	Data 0.000 (0.005)	Loss 0.4872 (0.5171)	Acc@1 82.812 (81.989)	Acc@5 99.609 (99.117)
Epoch: [139][128/196]	Time 0.107 (0.079)	Data 0.000 (0.003)	Loss 0.6114 (0.5164)	Acc@1 76.172 (82.074)	Acc@5 98.828 (99.137)
Epoch: [139][192/196]	Time 0.063 (0.080)	Data 0.000 (0.002)	Loss 0.4788 (0.5115)	Acc@1 83.203 (82.355)	Acc@5 99.219 (99.164)
after train
test acc: 68.61
Epoche: [140/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [140][0/196]	Time 0.078 (0.078)	Data 0.333 (0.333)	Loss 0.4982 (0.4982)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [140][64/196]	Time 0.071 (0.081)	Data 0.000 (0.005)	Loss 0.5291 (0.4960)	Acc@1 80.469 (82.855)	Acc@5 98.828 (99.327)
Epoch: [140][128/196]	Time 0.075 (0.081)	Data 0.000 (0.003)	Loss 0.4689 (0.4931)	Acc@1 83.203 (82.970)	Acc@5 99.609 (99.301)
Epoch: [140][192/196]	Time 0.067 (0.080)	Data 0.000 (0.002)	Loss 0.5087 (0.5015)	Acc@1 83.203 (82.695)	Acc@5 99.219 (99.251)
after train
test acc: 75.58
Epoche: [141/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [141][0/196]	Time 0.151 (0.151)	Data 0.359 (0.359)	Loss 0.4258 (0.4258)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [141][64/196]	Time 0.090 (0.082)	Data 0.000 (0.006)	Loss 0.4876 (0.4949)	Acc@1 82.422 (83.089)	Acc@5 99.609 (99.267)
Epoch: [141][128/196]	Time 0.069 (0.080)	Data 0.000 (0.003)	Loss 0.3778 (0.4916)	Acc@1 86.328 (83.049)	Acc@5 98.828 (99.273)
Epoch: [141][192/196]	Time 0.073 (0.080)	Data 0.000 (0.002)	Loss 0.5010 (0.5036)	Acc@1 85.156 (82.655)	Acc@5 99.219 (99.207)
after train
test acc: 78.53
Epoche: [142/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [142][0/196]	Time 0.158 (0.158)	Data 0.532 (0.532)	Loss 0.4333 (0.4333)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [142][64/196]	Time 0.090 (0.084)	Data 0.000 (0.008)	Loss 0.5514 (0.4825)	Acc@1 81.641 (83.287)	Acc@5 99.219 (99.399)
Epoch: [142][128/196]	Time 0.112 (0.083)	Data 0.000 (0.004)	Loss 0.5334 (0.4882)	Acc@1 81.250 (83.146)	Acc@5 99.609 (99.316)
Epoch: [142][192/196]	Time 0.067 (0.081)	Data 0.000 (0.003)	Loss 0.5546 (0.4918)	Acc@1 83.203 (83.041)	Acc@5 98.047 (99.239)
after train
test acc: 65.29
Epoche: [143/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [143][0/196]	Time 0.133 (0.133)	Data 0.330 (0.330)	Loss 0.4414 (0.4414)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [143][64/196]	Time 0.077 (0.091)	Data 0.000 (0.007)	Loss 0.5215 (0.4884)	Acc@1 79.688 (83.323)	Acc@5 99.219 (99.225)
Epoch: [143][128/196]	Time 0.079 (0.087)	Data 0.000 (0.003)	Loss 0.4905 (0.4933)	Acc@1 83.984 (83.061)	Acc@5 99.609 (99.264)
Epoch: [143][192/196]	Time 0.066 (0.085)	Data 0.000 (0.002)	Loss 0.5358 (0.4915)	Acc@1 81.641 (83.100)	Acc@5 99.609 (99.251)
after train
test acc: 78.21
Epoche: [144/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [144][0/196]	Time 0.108 (0.108)	Data 0.396 (0.396)	Loss 0.6510 (0.6510)	Acc@1 77.344 (77.344)	Acc@5 99.609 (99.609)
Epoch: [144][64/196]	Time 0.147 (0.081)	Data 0.000 (0.006)	Loss 0.5554 (0.5014)	Acc@1 81.250 (82.740)	Acc@5 99.219 (99.165)
Epoch: [144][128/196]	Time 0.071 (0.084)	Data 0.000 (0.003)	Loss 0.5339 (0.4977)	Acc@1 82.031 (82.812)	Acc@5 99.609 (99.201)
Epoch: [144][192/196]	Time 0.065 (0.083)	Data 0.000 (0.002)	Loss 0.5065 (0.4981)	Acc@1 83.203 (82.885)	Acc@5 99.609 (99.243)
after train
test acc: 77.64
Epoche: [145/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [145][0/196]	Time 0.104 (0.104)	Data 0.405 (0.405)	Loss 0.5416 (0.5416)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [145][64/196]	Time 0.078 (0.082)	Data 0.000 (0.007)	Loss 0.4637 (0.4905)	Acc@1 85.156 (83.161)	Acc@5 100.000 (99.255)
Epoch: [145][128/196]	Time 0.073 (0.082)	Data 0.000 (0.003)	Loss 0.4779 (0.4881)	Acc@1 83.984 (83.261)	Acc@5 98.828 (99.231)
Epoch: [145][192/196]	Time 0.076 (0.082)	Data 0.000 (0.002)	Loss 0.4913 (0.4900)	Acc@1 80.859 (83.155)	Acc@5 100.000 (99.255)
after train
test acc: 74.18
Epoche: [146/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [146][0/196]	Time 0.122 (0.122)	Data 0.402 (0.402)	Loss 0.4429 (0.4429)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [146][64/196]	Time 0.067 (0.080)	Data 0.000 (0.007)	Loss 0.4208 (0.4761)	Acc@1 85.547 (83.558)	Acc@5 98.828 (99.261)
Epoch: [146][128/196]	Time 0.157 (0.082)	Data 0.000 (0.003)	Loss 0.4760 (0.4853)	Acc@1 83.984 (83.252)	Acc@5 98.438 (99.255)
Epoch: [146][192/196]	Time 0.079 (0.083)	Data 0.000 (0.002)	Loss 0.6451 (0.4903)	Acc@1 78.125 (83.219)	Acc@5 97.656 (99.223)
after train
test acc: 78.21
Epoche: [147/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [147][0/196]	Time 0.125 (0.125)	Data 0.459 (0.459)	Loss 0.3767 (0.3767)	Acc@1 86.328 (86.328)	Acc@5 99.609 (99.609)
Epoch: [147][64/196]	Time 0.069 (0.085)	Data 0.000 (0.007)	Loss 0.4325 (0.4848)	Acc@1 83.594 (83.510)	Acc@5 99.609 (99.363)
Epoch: [147][128/196]	Time 0.074 (0.082)	Data 0.000 (0.004)	Loss 0.4128 (0.4857)	Acc@1 83.594 (83.209)	Acc@5 100.000 (99.331)
Epoch: [147][192/196]	Time 0.067 (0.082)	Data 0.000 (0.003)	Loss 0.4968 (0.4858)	Acc@1 82.031 (83.298)	Acc@5 99.609 (99.304)
after train
test acc: 75.1
Epoche: [148/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [148][0/196]	Time 0.100 (0.100)	Data 0.410 (0.410)	Loss 0.3831 (0.3831)	Acc@1 85.156 (85.156)	Acc@5 98.828 (98.828)
Epoch: [148][64/196]	Time 0.063 (0.082)	Data 0.000 (0.007)	Loss 0.3912 (0.4822)	Acc@1 87.500 (83.107)	Acc@5 100.000 (99.273)
Epoch: [148][128/196]	Time 0.107 (0.083)	Data 0.000 (0.003)	Loss 0.4553 (0.4823)	Acc@1 83.594 (83.255)	Acc@5 99.609 (99.267)
Epoch: [148][192/196]	Time 0.075 (0.082)	Data 0.000 (0.002)	Loss 0.5760 (0.4883)	Acc@1 79.297 (83.106)	Acc@5 99.609 (99.237)
after train
test acc: 69.3
Epoche: [149/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [149][0/196]	Time 0.081 (0.081)	Data 0.506 (0.506)	Loss 0.5592 (0.5592)	Acc@1 79.297 (79.297)	Acc@5 100.000 (100.000)
Epoch: [149][64/196]	Time 0.070 (0.082)	Data 0.000 (0.008)	Loss 0.4830 (0.4766)	Acc@1 83.203 (83.546)	Acc@5 99.609 (99.321)
Epoch: [149][128/196]	Time 0.088 (0.082)	Data 0.000 (0.004)	Loss 0.5435 (0.4734)	Acc@1 81.250 (83.691)	Acc@5 98.828 (99.322)
Epoch: [149][192/196]	Time 0.105 (0.082)	Data 0.000 (0.003)	Loss 0.5821 (0.4798)	Acc@1 78.516 (83.412)	Acc@5 99.609 (99.249)
after train
test acc: 69.81
Epoche: [150/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [150][0/196]	Time 0.121 (0.121)	Data 0.357 (0.357)	Loss 0.4534 (0.4534)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [150][64/196]	Time 0.063 (0.082)	Data 0.000 (0.006)	Loss 0.4830 (0.4793)	Acc@1 83.594 (83.389)	Acc@5 99.609 (99.345)
Epoch: [150][128/196]	Time 0.084 (0.083)	Data 0.000 (0.003)	Loss 0.4882 (0.4792)	Acc@1 82.812 (83.476)	Acc@5 98.828 (99.288)
Epoch: [150][192/196]	Time 0.068 (0.081)	Data 0.000 (0.002)	Loss 0.4991 (0.4824)	Acc@1 82.812 (83.468)	Acc@5 99.219 (99.273)
after train
test acc: 78.78
Epoche: [151/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [151][0/196]	Time 0.125 (0.125)	Data 0.410 (0.410)	Loss 0.4171 (0.4171)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [151][64/196]	Time 0.075 (0.084)	Data 0.000 (0.007)	Loss 0.5551 (0.4626)	Acc@1 79.688 (83.774)	Acc@5 100.000 (99.345)
Epoch: [151][128/196]	Time 0.083 (0.083)	Data 0.000 (0.003)	Loss 0.4180 (0.4685)	Acc@1 84.766 (83.806)	Acc@5 99.219 (99.294)
Epoch: [151][192/196]	Time 0.069 (0.083)	Data 0.000 (0.002)	Loss 0.4482 (0.4699)	Acc@1 84.375 (83.748)	Acc@5 99.609 (99.296)
after train
test acc: 74.84
Epoche: [152/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [152][0/196]	Time 0.137 (0.137)	Data 0.490 (0.490)	Loss 0.4779 (0.4779)	Acc@1 81.641 (81.641)	Acc@5 99.609 (99.609)
Epoch: [152][64/196]	Time 0.063 (0.082)	Data 0.000 (0.008)	Loss 0.5581 (0.4769)	Acc@1 84.766 (83.726)	Acc@5 99.219 (99.345)
Epoch: [152][128/196]	Time 0.130 (0.082)	Data 0.000 (0.004)	Loss 0.4340 (0.4751)	Acc@1 87.109 (83.869)	Acc@5 99.219 (99.379)
Epoch: [152][192/196]	Time 0.077 (0.082)	Data 0.000 (0.003)	Loss 0.5295 (0.4782)	Acc@1 81.250 (83.786)	Acc@5 99.219 (99.320)
after train
test acc: 76.01
Epoche: [153/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [153][0/196]	Time 0.121 (0.121)	Data 0.377 (0.377)	Loss 0.5021 (0.5021)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [153][64/196]	Time 0.068 (0.083)	Data 0.000 (0.006)	Loss 0.4908 (0.4641)	Acc@1 80.859 (83.786)	Acc@5 98.828 (99.387)
Epoch: [153][128/196]	Time 0.080 (0.083)	Data 0.000 (0.003)	Loss 0.4310 (0.4659)	Acc@1 85.156 (83.697)	Acc@5 100.000 (99.367)
Epoch: [153][192/196]	Time 0.064 (0.080)	Data 0.000 (0.002)	Loss 0.4780 (0.4695)	Acc@1 80.469 (83.596)	Acc@5 99.609 (99.300)
after train
test acc: 70.17
Epoche: [154/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [154][0/196]	Time 0.092 (0.092)	Data 0.425 (0.425)	Loss 0.4248 (0.4248)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [154][64/196]	Time 0.089 (0.088)	Data 0.001 (0.007)	Loss 0.3757 (0.4729)	Acc@1 87.500 (83.810)	Acc@5 99.219 (99.273)
Epoch: [154][128/196]	Time 0.090 (0.088)	Data 0.000 (0.004)	Loss 0.4822 (0.4718)	Acc@1 83.203 (83.785)	Acc@5 98.047 (99.319)
Epoch: [154][192/196]	Time 0.063 (0.086)	Data 0.000 (0.002)	Loss 0.4196 (0.4698)	Acc@1 83.984 (83.946)	Acc@5 99.609 (99.330)
after train
test acc: 75.87
Epoche: [155/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [155][0/196]	Time 0.106 (0.106)	Data 0.331 (0.331)	Loss 0.4234 (0.4234)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [155][64/196]	Time 0.096 (0.082)	Data 0.000 (0.005)	Loss 0.4807 (0.4623)	Acc@1 80.469 (84.129)	Acc@5 99.609 (99.363)
Epoch: [155][128/196]	Time 0.097 (0.081)	Data 0.000 (0.003)	Loss 0.5116 (0.4688)	Acc@1 82.031 (83.715)	Acc@5 99.219 (99.358)
Epoch: [155][192/196]	Time 0.063 (0.080)	Data 0.000 (0.002)	Loss 0.5502 (0.4690)	Acc@1 79.688 (83.735)	Acc@5 98.438 (99.306)
after train
test acc: 80.03
Epoche: [156/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [156][0/196]	Time 0.125 (0.125)	Data 0.370 (0.370)	Loss 0.5197 (0.5197)	Acc@1 83.203 (83.203)	Acc@5 98.438 (98.438)
Epoch: [156][64/196]	Time 0.067 (0.083)	Data 0.000 (0.006)	Loss 0.4034 (0.4723)	Acc@1 87.500 (83.828)	Acc@5 99.219 (99.411)
Epoch: [156][128/196]	Time 0.068 (0.081)	Data 0.000 (0.003)	Loss 0.4688 (0.4668)	Acc@1 82.422 (83.763)	Acc@5 98.828 (99.403)
Epoch: [156][192/196]	Time 0.068 (0.081)	Data 0.000 (0.002)	Loss 0.4529 (0.4713)	Acc@1 83.594 (83.600)	Acc@5 99.219 (99.356)
after train
test acc: 76.16
Epoche: [157/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [157][0/196]	Time 0.153 (0.153)	Data 0.390 (0.390)	Loss 0.5044 (0.5044)	Acc@1 83.203 (83.203)	Acc@5 98.828 (98.828)
Epoch: [157][64/196]	Time 0.072 (0.077)	Data 0.000 (0.006)	Loss 0.4167 (0.4715)	Acc@1 84.766 (83.654)	Acc@5 99.609 (99.375)
Epoch: [157][128/196]	Time 0.074 (0.078)	Data 0.000 (0.003)	Loss 0.5003 (0.4671)	Acc@1 80.859 (83.966)	Acc@5 100.000 (99.319)
Epoch: [157][192/196]	Time 0.063 (0.078)	Data 0.000 (0.002)	Loss 0.5165 (0.4687)	Acc@1 80.859 (83.950)	Acc@5 99.219 (99.275)
after train
test acc: 67.44
Epoche: [158/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [158][0/196]	Time 0.195 (0.195)	Data 0.306 (0.306)	Loss 0.4682 (0.4682)	Acc@1 83.984 (83.984)	Acc@5 98.828 (98.828)
Epoch: [158][64/196]	Time 0.102 (0.088)	Data 0.000 (0.005)	Loss 0.5699 (0.4589)	Acc@1 79.688 (84.171)	Acc@5 98.438 (99.411)
Epoch: [158][128/196]	Time 0.066 (0.087)	Data 0.000 (0.003)	Loss 0.3867 (0.4554)	Acc@1 85.547 (84.257)	Acc@5 99.609 (99.373)
Epoch: [158][192/196]	Time 0.064 (0.085)	Data 0.000 (0.002)	Loss 0.4475 (0.4626)	Acc@1 85.156 (84.134)	Acc@5 99.609 (99.364)
after train
test acc: 78.61
Epoche: [159/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [159][0/196]	Time 0.123 (0.123)	Data 0.461 (0.461)	Loss 0.3250 (0.3250)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [159][64/196]	Time 0.085 (0.086)	Data 0.000 (0.007)	Loss 0.4228 (0.4644)	Acc@1 87.500 (84.177)	Acc@5 99.609 (99.453)
Epoch: [159][128/196]	Time 0.092 (0.084)	Data 0.000 (0.004)	Loss 0.4340 (0.4617)	Acc@1 85.547 (84.224)	Acc@5 100.000 (99.449)
Epoch: [159][192/196]	Time 0.084 (0.083)	Data 0.000 (0.003)	Loss 0.5541 (0.4629)	Acc@1 79.688 (84.148)	Acc@5 99.609 (99.427)
after train
test acc: 73.04
Epoche: [160/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [160][0/196]	Time 0.126 (0.126)	Data 0.466 (0.466)	Loss 0.3905 (0.3905)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [160][64/196]	Time 0.109 (0.081)	Data 0.000 (0.007)	Loss 0.4310 (0.4544)	Acc@1 85.938 (84.105)	Acc@5 98.828 (99.387)
Epoch: [160][128/196]	Time 0.080 (0.082)	Data 0.000 (0.004)	Loss 0.3757 (0.4571)	Acc@1 88.281 (84.157)	Acc@5 99.609 (99.385)
Epoch: [160][192/196]	Time 0.069 (0.081)	Data 0.000 (0.003)	Loss 0.4221 (0.4597)	Acc@1 85.547 (84.077)	Acc@5 100.000 (99.366)
after train
test acc: 78.1
Epoche: [161/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [161][0/196]	Time 0.126 (0.126)	Data 0.341 (0.341)	Loss 0.4765 (0.4765)	Acc@1 80.859 (80.859)	Acc@5 99.219 (99.219)
Epoch: [161][64/196]	Time 0.098 (0.083)	Data 0.000 (0.005)	Loss 0.4631 (0.4542)	Acc@1 84.766 (84.243)	Acc@5 99.609 (99.291)
Epoch: [161][128/196]	Time 0.070 (0.080)	Data 0.000 (0.003)	Loss 0.4221 (0.4598)	Acc@1 83.984 (84.224)	Acc@5 100.000 (99.337)
Epoch: [161][192/196]	Time 0.074 (0.081)	Data 0.000 (0.002)	Loss 0.4359 (0.4633)	Acc@1 84.375 (84.114)	Acc@5 99.219 (99.294)
after train
test acc: 77.89
Epoche: [162/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [162][0/196]	Time 0.118 (0.118)	Data 0.358 (0.358)	Loss 0.4444 (0.4444)	Acc@1 83.984 (83.984)	Acc@5 100.000 (100.000)
Epoch: [162][64/196]	Time 0.064 (0.086)	Data 0.000 (0.006)	Loss 0.5355 (0.4527)	Acc@1 80.859 (84.381)	Acc@5 98.828 (99.345)
Epoch: [162][128/196]	Time 0.076 (0.084)	Data 0.000 (0.003)	Loss 0.4539 (0.4553)	Acc@1 83.203 (84.414)	Acc@5 100.000 (99.334)
Epoch: [162][192/196]	Time 0.069 (0.082)	Data 0.000 (0.002)	Loss 0.5193 (0.4589)	Acc@1 82.812 (84.282)	Acc@5 98.828 (99.346)
after train
test acc: 77.21
Epoche: [163/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [163][0/196]	Time 0.137 (0.137)	Data 0.362 (0.362)	Loss 0.4276 (0.4276)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [163][64/196]	Time 0.072 (0.082)	Data 0.000 (0.006)	Loss 0.4701 (0.4589)	Acc@1 79.297 (84.008)	Acc@5 99.609 (99.465)
Epoch: [163][128/196]	Time 0.070 (0.081)	Data 0.000 (0.003)	Loss 0.4052 (0.4590)	Acc@1 84.375 (84.193)	Acc@5 100.000 (99.410)
Epoch: [163][192/196]	Time 0.065 (0.080)	Data 0.000 (0.002)	Loss 0.4643 (0.4631)	Acc@1 83.594 (84.063)	Acc@5 99.609 (99.366)
after train
test acc: 77.29
Epoche: [164/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [164][0/196]	Time 0.110 (0.110)	Data 0.373 (0.373)	Loss 0.5447 (0.5447)	Acc@1 80.078 (80.078)	Acc@5 98.438 (98.438)
Epoch: [164][64/196]	Time 0.099 (0.088)	Data 0.000 (0.006)	Loss 0.3806 (0.4665)	Acc@1 86.328 (83.720)	Acc@5 99.609 (99.321)
Epoch: [164][128/196]	Time 0.069 (0.083)	Data 0.000 (0.003)	Loss 0.5510 (0.4613)	Acc@1 81.641 (83.984)	Acc@5 99.219 (99.313)
Epoch: [164][192/196]	Time 0.060 (0.081)	Data 0.000 (0.002)	Loss 0.4582 (0.4598)	Acc@1 84.766 (84.106)	Acc@5 99.219 (99.288)
after train
test acc: 68.48
Epoche: [165/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [165][0/196]	Time 0.150 (0.150)	Data 0.324 (0.324)	Loss 0.3790 (0.3790)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [165][64/196]	Time 0.081 (0.080)	Data 0.000 (0.005)	Loss 0.3834 (0.4463)	Acc@1 87.891 (84.639)	Acc@5 99.219 (99.315)
Epoch: [165][128/196]	Time 0.077 (0.080)	Data 0.000 (0.003)	Loss 0.4309 (0.4508)	Acc@1 85.938 (84.602)	Acc@5 99.609 (99.352)
Epoch: [165][192/196]	Time 0.070 (0.081)	Data 0.000 (0.002)	Loss 0.4242 (0.4558)	Acc@1 85.938 (84.335)	Acc@5 99.219 (99.369)
after train
test acc: 75.63
Epoche: [166/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [166][0/196]	Time 0.083 (0.083)	Data 0.392 (0.392)	Loss 0.5137 (0.5137)	Acc@1 83.203 (83.203)	Acc@5 98.828 (98.828)
Epoch: [166][64/196]	Time 0.097 (0.089)	Data 0.000 (0.007)	Loss 0.4748 (0.4585)	Acc@1 82.812 (84.345)	Acc@5 99.609 (99.321)
Epoch: [166][128/196]	Time 0.068 (0.088)	Data 0.000 (0.004)	Loss 0.5283 (0.4635)	Acc@1 80.469 (84.045)	Acc@5 99.609 (99.307)
Epoch: [166][192/196]	Time 0.063 (0.085)	Data 0.000 (0.003)	Loss 0.3983 (0.4626)	Acc@1 86.719 (84.088)	Acc@5 99.609 (99.312)
after train
test acc: 75.5
Epoche: [167/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [167][0/196]	Time 0.095 (0.095)	Data 0.490 (0.490)	Loss 0.3647 (0.3647)	Acc@1 86.328 (86.328)	Acc@5 99.609 (99.609)
Epoch: [167][64/196]	Time 0.064 (0.079)	Data 0.000 (0.008)	Loss 0.4943 (0.4535)	Acc@1 83.984 (84.369)	Acc@5 99.609 (99.405)
Epoch: [167][128/196]	Time 0.075 (0.080)	Data 0.000 (0.004)	Loss 0.4798 (0.4530)	Acc@1 82.812 (84.423)	Acc@5 98.828 (99.382)
Epoch: [167][192/196]	Time 0.070 (0.082)	Data 0.000 (0.003)	Loss 0.5196 (0.4538)	Acc@1 80.078 (84.355)	Acc@5 99.219 (99.369)
after train
test acc: 74.96
Epoche: [168/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [168][0/196]	Time 0.110 (0.110)	Data 0.396 (0.396)	Loss 0.4817 (0.4817)	Acc@1 82.422 (82.422)	Acc@5 98.828 (98.828)
Epoch: [168][64/196]	Time 0.072 (0.086)	Data 0.000 (0.006)	Loss 0.3843 (0.4521)	Acc@1 86.328 (84.056)	Acc@5 100.000 (99.375)
Epoch: [168][128/196]	Time 0.067 (0.084)	Data 0.000 (0.003)	Loss 0.4141 (0.4520)	Acc@1 85.938 (84.320)	Acc@5 99.609 (99.397)
Epoch: [168][192/196]	Time 0.074 (0.081)	Data 0.000 (0.002)	Loss 0.4652 (0.4548)	Acc@1 83.203 (84.280)	Acc@5 98.828 (99.369)
after train
test acc: 69.93
Epoche: [169/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [169][0/196]	Time 0.124 (0.124)	Data 0.360 (0.360)	Loss 0.4567 (0.4567)	Acc@1 88.672 (88.672)	Acc@5 98.828 (98.828)
Epoch: [169][64/196]	Time 0.068 (0.088)	Data 0.000 (0.006)	Loss 0.3601 (0.4397)	Acc@1 86.719 (85.222)	Acc@5 100.000 (99.393)
Epoch: [169][128/196]	Time 0.071 (0.086)	Data 0.000 (0.003)	Loss 0.4474 (0.4420)	Acc@1 85.156 (84.887)	Acc@5 99.219 (99.346)
Epoch: [169][192/196]	Time 0.067 (0.084)	Data 0.000 (0.002)	Loss 0.4926 (0.4471)	Acc@1 82.812 (84.565)	Acc@5 100.000 (99.369)
after train
test acc: 80.05
Epoche: [170/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [170][0/196]	Time 0.168 (0.168)	Data 0.429 (0.429)	Loss 0.3873 (0.3873)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [170][64/196]	Time 0.093 (0.088)	Data 0.000 (0.007)	Loss 0.4276 (0.4435)	Acc@1 85.547 (84.730)	Acc@5 98.828 (99.321)
Epoch: [170][128/196]	Time 0.077 (0.087)	Data 0.000 (0.004)	Loss 0.4865 (0.4471)	Acc@1 84.375 (84.750)	Acc@5 99.609 (99.304)
Epoch: [170][192/196]	Time 0.069 (0.085)	Data 0.000 (0.003)	Loss 0.5289 (0.4510)	Acc@1 82.031 (84.594)	Acc@5 99.609 (99.350)
after train
test acc: 69.28
Epoche: [171/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [171][0/196]	Time 0.125 (0.125)	Data 0.448 (0.448)	Loss 0.4617 (0.4617)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [171][64/196]	Time 0.080 (0.081)	Data 0.000 (0.007)	Loss 0.4746 (0.4540)	Acc@1 84.375 (84.087)	Acc@5 99.609 (99.375)
Epoch: [171][128/196]	Time 0.063 (0.082)	Data 0.000 (0.004)	Loss 0.5045 (0.4554)	Acc@1 84.766 (84.218)	Acc@5 98.828 (99.388)
Epoch: [171][192/196]	Time 0.072 (0.082)	Data 0.000 (0.003)	Loss 0.4228 (0.4541)	Acc@1 84.766 (84.379)	Acc@5 99.609 (99.397)
after train
test acc: 80.3
Epoche: [172/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [172][0/196]	Time 0.147 (0.147)	Data 0.346 (0.346)	Loss 0.4151 (0.4151)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [172][64/196]	Time 0.084 (0.084)	Data 0.000 (0.006)	Loss 0.4427 (0.4572)	Acc@1 85.938 (84.453)	Acc@5 100.000 (99.405)
Epoch: [172][128/196]	Time 0.086 (0.083)	Data 0.000 (0.003)	Loss 0.4358 (0.4534)	Acc@1 85.156 (84.475)	Acc@5 99.609 (99.413)
Epoch: [172][192/196]	Time 0.061 (0.082)	Data 0.000 (0.002)	Loss 0.4258 (0.4515)	Acc@1 85.547 (84.488)	Acc@5 99.219 (99.405)
after train
test acc: 76.14
Epoche: [173/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [173][0/196]	Time 0.105 (0.105)	Data 0.380 (0.380)	Loss 0.5334 (0.5334)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [173][64/196]	Time 0.065 (0.088)	Data 0.000 (0.006)	Loss 0.5510 (0.4485)	Acc@1 83.203 (84.471)	Acc@5 98.828 (99.339)
Epoch: [173][128/196]	Time 0.076 (0.084)	Data 0.000 (0.003)	Loss 0.4245 (0.4435)	Acc@1 85.156 (84.705)	Acc@5 99.609 (99.331)
Epoch: [173][192/196]	Time 0.068 (0.084)	Data 0.000 (0.002)	Loss 0.4627 (0.4460)	Acc@1 86.328 (84.760)	Acc@5 98.828 (99.356)
after train
test acc: 76.51
Epoche: [174/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [174][0/196]	Time 0.111 (0.111)	Data 0.456 (0.456)	Loss 0.5400 (0.5400)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [174][64/196]	Time 0.069 (0.080)	Data 0.000 (0.008)	Loss 0.4534 (0.4412)	Acc@1 85.938 (84.784)	Acc@5 99.609 (99.363)
Epoch: [174][128/196]	Time 0.070 (0.082)	Data 0.000 (0.004)	Loss 0.4267 (0.4416)	Acc@1 85.156 (84.581)	Acc@5 100.000 (99.391)
Epoch: [174][192/196]	Time 0.068 (0.082)	Data 0.000 (0.003)	Loss 0.3925 (0.4460)	Acc@1 86.719 (84.555)	Acc@5 99.609 (99.379)
after train
test acc: 78.31
Epoche: [175/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [175][0/196]	Time 0.100 (0.100)	Data 0.490 (0.490)	Loss 0.4700 (0.4700)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [175][64/196]	Time 0.068 (0.081)	Data 0.000 (0.008)	Loss 0.4182 (0.4468)	Acc@1 85.547 (84.513)	Acc@5 99.219 (99.471)
Epoch: [175][128/196]	Time 0.064 (0.080)	Data 0.000 (0.004)	Loss 0.4377 (0.4504)	Acc@1 83.984 (84.369)	Acc@5 99.219 (99.428)
Epoch: [175][192/196]	Time 0.073 (0.080)	Data 0.000 (0.003)	Loss 0.4897 (0.4506)	Acc@1 83.594 (84.503)	Acc@5 98.828 (99.387)
after train
test acc: 78.21
Epoche: [176/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [176][0/196]	Time 0.115 (0.115)	Data 0.409 (0.409)	Loss 0.4220 (0.4220)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [176][64/196]	Time 0.091 (0.087)	Data 0.000 (0.007)	Loss 0.4092 (0.4437)	Acc@1 84.375 (84.796)	Acc@5 99.609 (99.387)
Epoch: [176][128/196]	Time 0.067 (0.082)	Data 0.000 (0.004)	Loss 0.4710 (0.4481)	Acc@1 84.375 (84.635)	Acc@5 99.609 (99.376)
Epoch: [176][192/196]	Time 0.069 (0.082)	Data 0.000 (0.002)	Loss 0.4824 (0.4512)	Acc@1 83.203 (84.565)	Acc@5 98.438 (99.360)
after train
test acc: 67.92
Epoche: [177/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [177][0/196]	Time 0.108 (0.108)	Data 0.424 (0.424)	Loss 0.4018 (0.4018)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [177][64/196]	Time 0.074 (0.086)	Data 0.000 (0.007)	Loss 0.4341 (0.4269)	Acc@1 83.203 (85.745)	Acc@5 100.000 (99.423)
Epoch: [177][128/196]	Time 0.118 (0.084)	Data 0.000 (0.004)	Loss 0.4445 (0.4500)	Acc@1 83.594 (84.859)	Acc@5 99.219 (99.361)
Epoch: [177][192/196]	Time 0.060 (0.084)	Data 0.000 (0.003)	Loss 0.4677 (0.4519)	Acc@1 83.203 (84.624)	Acc@5 99.219 (99.375)
after train
test acc: 79.61
Epoche: [178/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [178][0/196]	Time 0.104 (0.104)	Data 0.358 (0.358)	Loss 0.4017 (0.4017)	Acc@1 87.891 (87.891)	Acc@5 99.219 (99.219)
Epoch: [178][64/196]	Time 0.090 (0.082)	Data 0.000 (0.006)	Loss 0.4025 (0.4337)	Acc@1 85.547 (84.994)	Acc@5 100.000 (99.453)
Epoch: [178][128/196]	Time 0.066 (0.080)	Data 0.000 (0.003)	Loss 0.4252 (0.4454)	Acc@1 85.547 (84.605)	Acc@5 100.000 (99.425)
Epoch: [178][192/196]	Time 0.066 (0.079)	Data 0.000 (0.002)	Loss 0.3831 (0.4444)	Acc@1 86.719 (84.654)	Acc@5 100.000 (99.395)
after train
test acc: 80.5
Epoche: [179/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [179][0/196]	Time 0.114 (0.114)	Data 0.432 (0.432)	Loss 0.3379 (0.3379)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [179][64/196]	Time 0.071 (0.084)	Data 0.000 (0.007)	Loss 0.4528 (0.4255)	Acc@1 84.375 (85.373)	Acc@5 99.219 (99.489)
Epoch: [179][128/196]	Time 0.065 (0.085)	Data 0.000 (0.004)	Loss 0.4549 (0.4343)	Acc@1 85.156 (84.929)	Acc@5 98.828 (99.431)
Epoch: [179][192/196]	Time 0.068 (0.082)	Data 0.000 (0.003)	Loss 0.4563 (0.4372)	Acc@1 83.203 (84.798)	Acc@5 99.219 (99.429)
after train
test acc: 76.68
Epoche: [180/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [180][0/196]	Time 0.128 (0.128)	Data 0.340 (0.340)	Loss 0.4856 (0.4856)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [180][64/196]	Time 0.068 (0.083)	Data 0.000 (0.005)	Loss 0.4232 (0.4490)	Acc@1 88.281 (84.597)	Acc@5 98.828 (99.333)
Epoch: [180][128/196]	Time 0.071 (0.082)	Data 0.000 (0.003)	Loss 0.3585 (0.4456)	Acc@1 88.672 (84.702)	Acc@5 98.828 (99.307)
Epoch: [180][192/196]	Time 0.056 (0.082)	Data 0.000 (0.002)	Loss 0.4402 (0.4474)	Acc@1 83.984 (84.602)	Acc@5 99.219 (99.358)
after train
test acc: 80.08
Epoche: [181/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [181][0/196]	Time 0.083 (0.083)	Data 0.489 (0.489)	Loss 0.4600 (0.4600)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [181][64/196]	Time 0.064 (0.082)	Data 0.000 (0.008)	Loss 0.4393 (0.4360)	Acc@1 81.250 (85.036)	Acc@5 100.000 (99.405)
Epoch: [181][128/196]	Time 0.128 (0.083)	Data 0.000 (0.004)	Loss 0.4310 (0.4374)	Acc@1 83.594 (84.884)	Acc@5 99.609 (99.431)
Epoch: [181][192/196]	Time 0.070 (0.083)	Data 0.000 (0.003)	Loss 0.4692 (0.4398)	Acc@1 85.547 (84.816)	Acc@5 98.438 (99.431)
after train
test acc: 72.91
Epoche: [182/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [182][0/196]	Time 0.096 (0.096)	Data 0.437 (0.437)	Loss 0.4471 (0.4471)	Acc@1 84.375 (84.375)	Acc@5 98.828 (98.828)
Epoch: [182][64/196]	Time 0.074 (0.082)	Data 0.000 (0.007)	Loss 0.4365 (0.4287)	Acc@1 85.938 (85.186)	Acc@5 98.828 (99.537)
Epoch: [182][128/196]	Time 0.076 (0.082)	Data 0.000 (0.004)	Loss 0.5606 (0.4484)	Acc@1 81.250 (84.472)	Acc@5 98.047 (99.416)
Epoch: [182][192/196]	Time 0.065 (0.082)	Data 0.000 (0.003)	Loss 0.3756 (0.4507)	Acc@1 85.547 (84.397)	Acc@5 99.609 (99.435)
after train
test acc: 78.53
Epoche: [183/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [183][0/196]	Time 0.106 (0.106)	Data 0.514 (0.514)	Loss 0.4453 (0.4453)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [183][64/196]	Time 0.114 (0.082)	Data 0.000 (0.008)	Loss 0.3537 (0.4433)	Acc@1 89.844 (84.850)	Acc@5 98.828 (99.429)
Epoch: [183][128/196]	Time 0.112 (0.081)	Data 0.000 (0.004)	Loss 0.4306 (0.4420)	Acc@1 83.203 (84.747)	Acc@5 100.000 (99.406)
Epoch: [183][192/196]	Time 0.064 (0.080)	Data 0.000 (0.003)	Loss 0.4082 (0.4413)	Acc@1 86.328 (84.719)	Acc@5 99.609 (99.397)
after train
test acc: 79.95
Epoche: [184/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [184][0/196]	Time 0.121 (0.121)	Data 0.380 (0.380)	Loss 0.5431 (0.5431)	Acc@1 79.297 (79.297)	Acc@5 98.047 (98.047)
Epoch: [184][64/196]	Time 0.119 (0.087)	Data 0.000 (0.006)	Loss 0.5422 (0.4463)	Acc@1 79.688 (84.892)	Acc@5 98.438 (99.345)
Epoch: [184][128/196]	Time 0.062 (0.085)	Data 0.000 (0.003)	Loss 0.4176 (0.4452)	Acc@1 85.938 (84.747)	Acc@5 99.609 (99.367)
Epoch: [184][192/196]	Time 0.073 (0.083)	Data 0.000 (0.002)	Loss 0.4038 (0.4424)	Acc@1 83.984 (84.780)	Acc@5 100.000 (99.391)
after train
test acc: 75.51
Epoche: [185/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [185][0/196]	Time 0.123 (0.123)	Data 0.326 (0.326)	Loss 0.4738 (0.4738)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [185][64/196]	Time 0.083 (0.085)	Data 0.000 (0.006)	Loss 0.4409 (0.4562)	Acc@1 83.594 (84.501)	Acc@5 99.609 (99.327)
Epoch: [185][128/196]	Time 0.083 (0.086)	Data 0.000 (0.003)	Loss 0.4049 (0.4481)	Acc@1 85.938 (84.660)	Acc@5 99.609 (99.391)
Epoch: [185][192/196]	Time 0.069 (0.086)	Data 0.000 (0.002)	Loss 0.4119 (0.4431)	Acc@1 86.328 (84.802)	Acc@5 98.828 (99.411)
after train
test acc: 73.55
Epoche: [186/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [186][0/196]	Time 0.147 (0.147)	Data 0.314 (0.314)	Loss 0.3932 (0.3932)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [186][64/196]	Time 0.070 (0.082)	Data 0.000 (0.005)	Loss 0.4778 (0.4353)	Acc@1 83.984 (85.090)	Acc@5 100.000 (99.339)
Epoch: [186][128/196]	Time 0.090 (0.082)	Data 0.000 (0.003)	Loss 0.3495 (0.4344)	Acc@1 89.453 (85.162)	Acc@5 99.609 (99.385)
Epoch: [186][192/196]	Time 0.064 (0.082)	Data 0.000 (0.002)	Loss 0.4340 (0.4394)	Acc@1 84.766 (84.982)	Acc@5 99.219 (99.409)
after train
test acc: 82.3
Epoche: [187/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [187][0/196]	Time 0.089 (0.089)	Data 0.318 (0.318)	Loss 0.4552 (0.4552)	Acc@1 82.812 (82.812)	Acc@5 99.609 (99.609)
Epoch: [187][64/196]	Time 0.072 (0.078)	Data 0.000 (0.005)	Loss 0.3822 (0.4374)	Acc@1 86.719 (85.228)	Acc@5 99.219 (99.339)
Epoch: [187][128/196]	Time 0.075 (0.080)	Data 0.000 (0.003)	Loss 0.4835 (0.4382)	Acc@1 81.641 (85.026)	Acc@5 99.219 (99.379)
Epoch: [187][192/196]	Time 0.067 (0.080)	Data 0.000 (0.002)	Loss 0.4636 (0.4394)	Acc@1 83.984 (84.938)	Acc@5 99.219 (99.399)
after train
test acc: 79.11
Epoche: [188/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [188][0/196]	Time 0.081 (0.081)	Data 0.432 (0.432)	Loss 0.4156 (0.4156)	Acc@1 83.203 (83.203)	Acc@5 100.000 (100.000)
Epoch: [188][64/196]	Time 0.092 (0.092)	Data 0.000 (0.007)	Loss 0.4633 (0.4173)	Acc@1 85.156 (85.781)	Acc@5 99.609 (99.447)
Epoch: [188][128/196]	Time 0.073 (0.086)	Data 0.000 (0.004)	Loss 0.4652 (0.4308)	Acc@1 82.031 (85.126)	Acc@5 100.000 (99.416)
Epoch: [188][192/196]	Time 0.074 (0.084)	Data 0.000 (0.003)	Loss 0.4090 (0.4380)	Acc@1 87.109 (84.798)	Acc@5 100.000 (99.377)
after train
test acc: 75.51
Epoche: [189/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [189][0/196]	Time 0.118 (0.118)	Data 0.322 (0.322)	Loss 0.4061 (0.4061)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [189][64/196]	Time 0.074 (0.082)	Data 0.000 (0.005)	Loss 0.4180 (0.4378)	Acc@1 85.547 (84.934)	Acc@5 99.609 (99.363)
Epoch: [189][128/196]	Time 0.067 (0.084)	Data 0.000 (0.003)	Loss 0.4254 (0.4428)	Acc@1 84.375 (84.663)	Acc@5 99.219 (99.370)
Epoch: [189][192/196]	Time 0.067 (0.082)	Data 0.000 (0.002)	Loss 0.4908 (0.4426)	Acc@1 84.375 (84.618)	Acc@5 98.438 (99.389)
after train
test acc: 76.82
Epoche: [190/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [190][0/196]	Time 0.125 (0.125)	Data 0.373 (0.373)	Loss 0.4774 (0.4774)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [190][64/196]	Time 0.106 (0.084)	Data 0.000 (0.006)	Loss 0.4243 (0.4302)	Acc@1 86.328 (85.300)	Acc@5 100.000 (99.549)
Epoch: [190][128/196]	Time 0.087 (0.083)	Data 0.001 (0.003)	Loss 0.4420 (0.4315)	Acc@1 83.594 (85.150)	Acc@5 99.219 (99.431)
Epoch: [190][192/196]	Time 0.088 (0.082)	Data 0.000 (0.002)	Loss 0.4180 (0.4363)	Acc@1 85.156 (85.004)	Acc@5 100.000 (99.411)
after train
test acc: 79.3
Epoche: [191/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [191][0/196]	Time 0.127 (0.127)	Data 0.379 (0.379)	Loss 0.4527 (0.4527)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [191][64/196]	Time 0.085 (0.077)	Data 0.000 (0.006)	Loss 0.4690 (0.4336)	Acc@1 83.594 (84.748)	Acc@5 99.219 (99.345)
Epoch: [191][128/196]	Time 0.119 (0.079)	Data 0.000 (0.003)	Loss 0.4039 (0.4392)	Acc@1 88.281 (84.687)	Acc@5 99.609 (99.385)
Epoch: [191][192/196]	Time 0.100 (0.078)	Data 0.000 (0.002)	Loss 0.4154 (0.4441)	Acc@1 84.766 (84.494)	Acc@5 99.609 (99.391)
after train
test acc: 76.79
Epoche: [192/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [192][0/196]	Time 0.108 (0.108)	Data 0.343 (0.343)	Loss 0.4199 (0.4199)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [192][64/196]	Time 0.074 (0.081)	Data 0.000 (0.006)	Loss 0.5385 (0.4436)	Acc@1 82.812 (84.778)	Acc@5 99.219 (99.435)
Epoch: [192][128/196]	Time 0.078 (0.082)	Data 0.000 (0.003)	Loss 0.4818 (0.4402)	Acc@1 83.203 (85.029)	Acc@5 98.828 (99.413)
Epoch: [192][192/196]	Time 0.063 (0.081)	Data 0.000 (0.002)	Loss 0.4467 (0.4355)	Acc@1 83.984 (85.152)	Acc@5 100.000 (99.427)
after train
test acc: 77.9
Epoche: [193/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [193][0/196]	Time 0.136 (0.136)	Data 0.500 (0.500)	Loss 0.4360 (0.4360)	Acc@1 85.547 (85.547)	Acc@5 100.000 (100.000)
Epoch: [193][64/196]	Time 0.094 (0.083)	Data 0.000 (0.008)	Loss 0.4549 (0.4423)	Acc@1 85.547 (85.048)	Acc@5 99.219 (99.309)
Epoch: [193][128/196]	Time 0.064 (0.082)	Data 0.000 (0.004)	Loss 0.4785 (0.4384)	Acc@1 83.203 (85.011)	Acc@5 99.609 (99.376)
Epoch: [193][192/196]	Time 0.070 (0.082)	Data 0.000 (0.003)	Loss 0.3636 (0.4402)	Acc@1 88.281 (84.982)	Acc@5 100.000 (99.377)
after train
test acc: 70.9
Epoche: [194/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [194][0/196]	Time 0.088 (0.088)	Data 0.454 (0.454)	Loss 0.4371 (0.4371)	Acc@1 85.938 (85.938)	Acc@5 98.828 (98.828)
Epoch: [194][64/196]	Time 0.094 (0.082)	Data 0.000 (0.007)	Loss 0.4642 (0.4323)	Acc@1 83.984 (85.216)	Acc@5 98.828 (99.405)
Epoch: [194][128/196]	Time 0.081 (0.081)	Data 0.000 (0.004)	Loss 0.4345 (0.4332)	Acc@1 85.156 (85.065)	Acc@5 99.609 (99.382)
Epoch: [194][192/196]	Time 0.080 (0.080)	Data 0.000 (0.003)	Loss 0.5101 (0.4366)	Acc@1 82.031 (84.942)	Acc@5 99.609 (99.379)
after train
test acc: 78.25
Epoche: [195/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [195][0/196]	Time 0.094 (0.094)	Data 0.428 (0.428)	Loss 0.4716 (0.4716)	Acc@1 82.812 (82.812)	Acc@5 99.609 (99.609)
Epoch: [195][64/196]	Time 0.113 (0.084)	Data 0.000 (0.007)	Loss 0.4056 (0.4372)	Acc@1 87.891 (84.922)	Acc@5 100.000 (99.447)
Epoch: [195][128/196]	Time 0.065 (0.082)	Data 0.000 (0.004)	Loss 0.5244 (0.4304)	Acc@1 81.641 (85.084)	Acc@5 98.828 (99.437)
Epoch: [195][192/196]	Time 0.063 (0.083)	Data 0.000 (0.003)	Loss 0.4013 (0.4339)	Acc@1 83.984 (85.013)	Acc@5 99.609 (99.437)
after train
test acc: 80.96
Epoche: [196/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [196][0/196]	Time 0.127 (0.127)	Data 0.416 (0.416)	Loss 0.4523 (0.4523)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [196][64/196]	Time 0.090 (0.083)	Data 0.000 (0.007)	Loss 0.4562 (0.4368)	Acc@1 85.156 (85.030)	Acc@5 99.609 (99.549)
Epoch: [196][128/196]	Time 0.080 (0.084)	Data 0.000 (0.004)	Loss 0.4367 (0.4428)	Acc@1 84.766 (84.911)	Acc@5 100.000 (99.443)
Epoch: [196][192/196]	Time 0.068 (0.082)	Data 0.000 (0.003)	Loss 0.4575 (0.4386)	Acc@1 84.375 (84.982)	Acc@5 99.219 (99.443)
after train
test acc: 81.04
Epoche: [197/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [197][0/196]	Time 0.107 (0.107)	Data 0.478 (0.478)	Loss 0.4068 (0.4068)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [197][64/196]	Time 0.077 (0.091)	Data 0.000 (0.008)	Loss 0.5205 (0.4404)	Acc@1 82.422 (85.036)	Acc@5 99.609 (99.417)
Epoch: [197][128/196]	Time 0.083 (0.086)	Data 0.000 (0.004)	Loss 0.4452 (0.4386)	Acc@1 84.766 (85.074)	Acc@5 99.609 (99.385)
Epoch: [197][192/196]	Time 0.067 (0.085)	Data 0.000 (0.003)	Loss 0.3770 (0.4341)	Acc@1 87.109 (85.134)	Acc@5 100.000 (99.425)
after train
test acc: 78.0
Epoche: [198/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [198][0/196]	Time 0.082 (0.082)	Data 0.409 (0.409)	Loss 0.4654 (0.4654)	Acc@1 82.422 (82.422)	Acc@5 100.000 (100.000)
Epoch: [198][64/196]	Time 0.121 (0.082)	Data 0.000 (0.007)	Loss 0.3866 (0.4285)	Acc@1 87.500 (85.060)	Acc@5 99.219 (99.489)
Epoch: [198][128/196]	Time 0.088 (0.082)	Data 0.000 (0.003)	Loss 0.4261 (0.4337)	Acc@1 85.547 (84.944)	Acc@5 99.219 (99.464)
Epoch: [198][192/196]	Time 0.085 (0.082)	Data 0.000 (0.002)	Loss 0.4833 (0.4374)	Acc@1 82.812 (84.832)	Acc@5 99.219 (99.421)
after train
test acc: 68.67
Epoche: [199/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [199][0/196]	Time 0.126 (0.126)	Data 0.359 (0.359)	Loss 0.3865 (0.3865)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [199][64/196]	Time 0.085 (0.080)	Data 0.000 (0.006)	Loss 0.3962 (0.4190)	Acc@1 88.672 (85.312)	Acc@5 99.609 (99.549)
Epoch: [199][128/196]	Time 0.071 (0.081)	Data 0.000 (0.003)	Loss 0.3122 (0.4270)	Acc@1 90.234 (85.071)	Acc@5 99.609 (99.394)
Epoch: [199][192/196]	Time 0.068 (0.081)	Data 0.000 (0.002)	Loss 0.4817 (0.4330)	Acc@1 84.375 (84.962)	Acc@5 99.219 (99.393)
after train
test acc: 74.46
Epoche: [200/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [200][0/196]	Time 0.098 (0.098)	Data 0.461 (0.461)	Loss 0.5401 (0.5401)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [200][64/196]	Time 0.072 (0.085)	Data 0.000 (0.007)	Loss 0.4788 (0.4504)	Acc@1 80.859 (84.321)	Acc@5 98.438 (99.351)
Epoch: [200][128/196]	Time 0.087 (0.085)	Data 0.000 (0.004)	Loss 0.4507 (0.4400)	Acc@1 84.375 (84.808)	Acc@5 99.219 (99.413)
Epoch: [200][192/196]	Time 0.080 (0.084)	Data 0.000 (0.003)	Loss 0.4522 (0.4365)	Acc@1 83.203 (84.930)	Acc@5 99.219 (99.387)
after train
test acc: 80.16
Epoche: [201/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [201][0/196]	Time 0.092 (0.092)	Data 0.388 (0.388)	Loss 0.3890 (0.3890)	Acc@1 85.547 (85.547)	Acc@5 100.000 (100.000)
Epoch: [201][64/196]	Time 0.077 (0.085)	Data 0.000 (0.006)	Loss 0.4801 (0.4330)	Acc@1 84.766 (84.958)	Acc@5 97.656 (99.399)
Epoch: [201][128/196]	Time 0.081 (0.083)	Data 0.000 (0.003)	Loss 0.3684 (0.4358)	Acc@1 86.719 (84.899)	Acc@5 99.609 (99.419)
Epoch: [201][192/196]	Time 0.081 (0.083)	Data 0.000 (0.002)	Loss 0.4222 (0.4351)	Acc@1 86.719 (84.928)	Acc@5 99.219 (99.417)
after train
test acc: 76.78
Epoche: [202/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [202][0/196]	Time 0.112 (0.112)	Data 0.337 (0.337)	Loss 0.4512 (0.4512)	Acc@1 83.203 (83.203)	Acc@5 100.000 (100.000)
Epoch: [202][64/196]	Time 0.071 (0.085)	Data 0.000 (0.006)	Loss 0.3975 (0.4261)	Acc@1 87.109 (85.457)	Acc@5 99.609 (99.441)
Epoch: [202][128/196]	Time 0.068 (0.083)	Data 0.000 (0.003)	Loss 0.4947 (0.4222)	Acc@1 81.641 (85.553)	Acc@5 99.609 (99.437)
Epoch: [202][192/196]	Time 0.080 (0.082)	Data 0.000 (0.002)	Loss 0.4812 (0.4279)	Acc@1 80.078 (85.320)	Acc@5 100.000 (99.452)
after train
test acc: 72.6
Epoche: [203/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [203][0/196]	Time 0.129 (0.129)	Data 0.378 (0.378)	Loss 0.3912 (0.3912)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [203][64/196]	Time 0.086 (0.088)	Data 0.000 (0.006)	Loss 0.4319 (0.4364)	Acc@1 86.719 (84.958)	Acc@5 99.219 (99.465)
Epoch: [203][128/196]	Time 0.071 (0.087)	Data 0.000 (0.003)	Loss 0.5061 (0.4336)	Acc@1 84.375 (85.184)	Acc@5 98.828 (99.443)
Epoch: [203][192/196]	Time 0.064 (0.084)	Data 0.000 (0.002)	Loss 0.4123 (0.4317)	Acc@1 84.766 (85.274)	Acc@5 99.609 (99.421)
after train
test acc: 76.12
Epoche: [204/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [204][0/196]	Time 0.114 (0.114)	Data 0.466 (0.466)	Loss 0.3217 (0.3217)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [204][64/196]	Time 0.071 (0.085)	Data 0.000 (0.007)	Loss 0.4668 (0.4255)	Acc@1 83.203 (85.589)	Acc@5 99.609 (99.417)
Epoch: [204][128/196]	Time 0.093 (0.083)	Data 0.000 (0.004)	Loss 0.4501 (0.4281)	Acc@1 87.109 (85.314)	Acc@5 99.609 (99.425)
Epoch: [204][192/196]	Time 0.058 (0.084)	Data 0.000 (0.003)	Loss 0.4460 (0.4340)	Acc@1 84.375 (85.069)	Acc@5 98.438 (99.393)
after train
test acc: 69.61
Epoche: [205/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [205][0/196]	Time 0.100 (0.100)	Data 0.441 (0.441)	Loss 0.4366 (0.4366)	Acc@1 83.984 (83.984)	Acc@5 100.000 (100.000)
Epoch: [205][64/196]	Time 0.066 (0.082)	Data 0.000 (0.007)	Loss 0.4185 (0.4188)	Acc@1 87.109 (85.679)	Acc@5 99.609 (99.435)
Epoch: [205][128/196]	Time 0.098 (0.083)	Data 0.000 (0.004)	Loss 0.5153 (0.4221)	Acc@1 84.766 (85.474)	Acc@5 99.609 (99.449)
Epoch: [205][192/196]	Time 0.074 (0.082)	Data 0.000 (0.003)	Loss 0.3266 (0.4311)	Acc@1 89.844 (85.160)	Acc@5 99.609 (99.437)
after train
test acc: 77.16
Epoche: [206/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [206][0/196]	Time 0.147 (0.147)	Data 0.430 (0.430)	Loss 0.4492 (0.4492)	Acc@1 85.156 (85.156)	Acc@5 98.828 (98.828)
Epoch: [206][64/196]	Time 0.109 (0.085)	Data 0.000 (0.007)	Loss 0.4183 (0.4250)	Acc@1 87.500 (85.601)	Acc@5 98.828 (99.387)
Epoch: [206][128/196]	Time 0.074 (0.085)	Data 0.000 (0.004)	Loss 0.5223 (0.4330)	Acc@1 81.250 (85.096)	Acc@5 97.656 (99.370)
Epoch: [206][192/196]	Time 0.065 (0.083)	Data 0.000 (0.003)	Loss 0.4159 (0.4316)	Acc@1 85.156 (85.181)	Acc@5 99.609 (99.375)
after train
test acc: 77.32
Epoche: [207/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [207][0/196]	Time 0.104 (0.104)	Data 0.348 (0.348)	Loss 0.4513 (0.4513)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [207][64/196]	Time 0.061 (0.089)	Data 0.000 (0.006)	Loss 0.3955 (0.4265)	Acc@1 85.938 (85.240)	Acc@5 99.609 (99.423)
Epoch: [207][128/196]	Time 0.062 (0.085)	Data 0.000 (0.003)	Loss 0.4635 (0.4244)	Acc@1 83.594 (85.259)	Acc@5 98.828 (99.406)
Epoch: [207][192/196]	Time 0.064 (0.082)	Data 0.000 (0.002)	Loss 0.4407 (0.4313)	Acc@1 85.156 (85.027)	Acc@5 100.000 (99.369)
after train
test acc: 76.16
Epoche: [208/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [208][0/196]	Time 0.129 (0.129)	Data 0.338 (0.338)	Loss 0.4218 (0.4218)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [208][64/196]	Time 0.075 (0.079)	Data 0.000 (0.006)	Loss 0.3598 (0.4328)	Acc@1 83.984 (84.850)	Acc@5 99.609 (99.345)
Epoch: [208][128/196]	Time 0.071 (0.082)	Data 0.000 (0.003)	Loss 0.4498 (0.4294)	Acc@1 82.422 (84.965)	Acc@5 99.219 (99.437)
Epoch: [208][192/196]	Time 0.063 (0.081)	Data 0.000 (0.002)	Loss 0.4377 (0.4303)	Acc@1 84.766 (85.055)	Acc@5 99.609 (99.415)
after train
test acc: 72.98
Epoche: [209/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [209][0/196]	Time 0.175 (0.175)	Data 0.373 (0.373)	Loss 0.4550 (0.4550)	Acc@1 85.938 (85.938)	Acc@5 97.266 (97.266)
Epoch: [209][64/196]	Time 0.074 (0.086)	Data 0.000 (0.006)	Loss 0.4314 (0.4338)	Acc@1 87.500 (85.379)	Acc@5 99.609 (99.483)
Epoch: [209][128/196]	Time 0.087 (0.084)	Data 0.000 (0.003)	Loss 0.3378 (0.4309)	Acc@1 90.234 (85.323)	Acc@5 99.609 (99.449)
Epoch: [209][192/196]	Time 0.067 (0.081)	Data 0.000 (0.002)	Loss 0.4049 (0.4379)	Acc@1 87.891 (85.043)	Acc@5 99.219 (99.373)
after train
test acc: 77.94
Epoche: [210/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [210][0/196]	Time 0.128 (0.128)	Data 0.446 (0.446)	Loss 0.4459 (0.4459)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [210][64/196]	Time 0.111 (0.084)	Data 0.000 (0.007)	Loss 0.5005 (0.4089)	Acc@1 83.203 (85.775)	Acc@5 99.609 (99.393)
Epoch: [210][128/196]	Time 0.076 (0.082)	Data 0.000 (0.004)	Loss 0.4547 (0.4201)	Acc@1 84.766 (85.474)	Acc@5 98.828 (99.437)
Epoch: [210][192/196]	Time 0.069 (0.080)	Data 0.000 (0.003)	Loss 0.4291 (0.4264)	Acc@1 83.594 (85.290)	Acc@5 99.609 (99.441)
after train
test acc: 75.67
Epoche: [211/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [211][0/196]	Time 0.123 (0.123)	Data 0.419 (0.419)	Loss 0.4518 (0.4518)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [211][64/196]	Time 0.144 (0.086)	Data 0.000 (0.007)	Loss 0.5285 (0.4256)	Acc@1 81.250 (85.264)	Acc@5 99.609 (99.579)
Epoch: [211][128/196]	Time 0.126 (0.083)	Data 0.000 (0.004)	Loss 0.4207 (0.4293)	Acc@1 85.156 (85.150)	Acc@5 99.219 (99.464)
Epoch: [211][192/196]	Time 0.063 (0.081)	Data 0.000 (0.002)	Loss 0.4682 (0.4355)	Acc@1 84.375 (84.968)	Acc@5 99.219 (99.437)
after train
test acc: 74.12
Epoche: [212/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [212][0/196]	Time 0.129 (0.129)	Data 0.416 (0.416)	Loss 0.3714 (0.3714)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [212][64/196]	Time 0.132 (0.084)	Data 0.000 (0.007)	Loss 0.4254 (0.4300)	Acc@1 87.109 (85.300)	Acc@5 99.219 (99.405)
Epoch: [212][128/196]	Time 0.070 (0.084)	Data 0.000 (0.004)	Loss 0.4862 (0.4317)	Acc@1 84.375 (85.150)	Acc@5 99.219 (99.425)
Epoch: [212][192/196]	Time 0.074 (0.085)	Data 0.000 (0.002)	Loss 0.5060 (0.4365)	Acc@1 81.641 (84.974)	Acc@5 99.219 (99.419)
after train
test acc: 69.83
Epoche: [213/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [213][0/196]	Time 0.122 (0.122)	Data 0.314 (0.314)	Loss 0.4302 (0.4302)	Acc@1 86.328 (86.328)	Acc@5 99.219 (99.219)
Epoch: [213][64/196]	Time 0.073 (0.080)	Data 0.000 (0.006)	Loss 0.3919 (0.4264)	Acc@1 84.766 (85.403)	Acc@5 99.219 (99.429)
Epoch: [213][128/196]	Time 0.101 (0.080)	Data 0.000 (0.003)	Loss 0.4694 (0.4250)	Acc@1 83.203 (85.535)	Acc@5 98.828 (99.449)
Epoch: [213][192/196]	Time 0.064 (0.081)	Data 0.000 (0.002)	Loss 0.3950 (0.4292)	Acc@1 82.812 (85.231)	Acc@5 99.609 (99.456)
after train
test acc: 73.94
Epoche: [214/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [214][0/196]	Time 0.166 (0.166)	Data 0.418 (0.418)	Loss 0.3625 (0.3625)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [214][64/196]	Time 0.072 (0.082)	Data 0.000 (0.007)	Loss 0.3960 (0.4246)	Acc@1 87.891 (85.240)	Acc@5 99.219 (99.381)
Epoch: [214][128/196]	Time 0.089 (0.083)	Data 0.000 (0.004)	Loss 0.5056 (0.4314)	Acc@1 83.984 (85.087)	Acc@5 99.219 (99.340)
Epoch: [214][192/196]	Time 0.068 (0.080)	Data 0.000 (0.002)	Loss 0.4327 (0.4331)	Acc@1 86.328 (85.031)	Acc@5 98.828 (99.354)
after train
test acc: 73.31
Epoche: [215/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [215][0/196]	Time 0.096 (0.096)	Data 0.417 (0.417)	Loss 0.5315 (0.5315)	Acc@1 82.422 (82.422)	Acc@5 98.047 (98.047)
Epoch: [215][64/196]	Time 0.065 (0.077)	Data 0.000 (0.007)	Loss 0.4056 (0.4198)	Acc@1 85.938 (85.607)	Acc@5 98.828 (99.525)
Epoch: [215][128/196]	Time 0.070 (0.079)	Data 0.000 (0.004)	Loss 0.4681 (0.4295)	Acc@1 84.375 (85.065)	Acc@5 100.000 (99.473)
Epoch: [215][192/196]	Time 0.063 (0.080)	Data 0.000 (0.003)	Loss 0.3786 (0.4300)	Acc@1 86.719 (85.106)	Acc@5 100.000 (99.421)
after train
test acc: 81.38
Epoche: [216/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [216][0/196]	Time 0.102 (0.102)	Data 0.349 (0.349)	Loss 0.4126 (0.4126)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [216][64/196]	Time 0.067 (0.083)	Data 0.000 (0.006)	Loss 0.3162 (0.4138)	Acc@1 89.844 (85.751)	Acc@5 100.000 (99.429)
Epoch: [216][128/196]	Time 0.106 (0.084)	Data 0.000 (0.003)	Loss 0.5624 (0.4202)	Acc@1 81.641 (85.405)	Acc@5 99.219 (99.449)
Epoch: [216][192/196]	Time 0.072 (0.084)	Data 0.000 (0.002)	Loss 0.4676 (0.4277)	Acc@1 83.984 (85.174)	Acc@5 99.609 (99.429)
after train
test acc: 81.12
Epoche: [217/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [217][0/196]	Time 0.136 (0.136)	Data 0.381 (0.381)	Loss 0.4666 (0.4666)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [217][64/196]	Time 0.067 (0.082)	Data 0.000 (0.006)	Loss 0.4834 (0.4300)	Acc@1 84.375 (84.976)	Acc@5 98.047 (99.453)
Epoch: [217][128/196]	Time 0.066 (0.081)	Data 0.000 (0.003)	Loss 0.4457 (0.4356)	Acc@1 83.984 (84.956)	Acc@5 99.609 (99.461)
Epoch: [217][192/196]	Time 0.081 (0.082)	Data 0.000 (0.002)	Loss 0.4566 (0.4331)	Acc@1 85.156 (85.063)	Acc@5 99.219 (99.454)
after train
test acc: 78.59
Epoche: [218/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [218][0/196]	Time 0.104 (0.104)	Data 0.413 (0.413)	Loss 0.5881 (0.5881)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [218][64/196]	Time 0.102 (0.085)	Data 0.000 (0.007)	Loss 0.4343 (0.4243)	Acc@1 86.328 (85.499)	Acc@5 99.219 (99.411)
Epoch: [218][128/196]	Time 0.063 (0.081)	Data 0.000 (0.003)	Loss 0.4776 (0.4277)	Acc@1 84.766 (85.262)	Acc@5 99.609 (99.443)
Epoch: [218][192/196]	Time 0.095 (0.082)	Data 0.000 (0.002)	Loss 0.4239 (0.4262)	Acc@1 85.938 (85.257)	Acc@5 99.609 (99.435)
after train
test acc: 70.77
Epoche: [219/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [219][0/196]	Time 0.143 (0.143)	Data 0.347 (0.347)	Loss 0.5102 (0.5102)	Acc@1 82.812 (82.812)	Acc@5 98.047 (98.047)
Epoch: [219][64/196]	Time 0.076 (0.087)	Data 0.000 (0.006)	Loss 0.3538 (0.4269)	Acc@1 87.891 (85.541)	Acc@5 99.609 (99.357)
Epoch: [219][128/196]	Time 0.090 (0.086)	Data 0.000 (0.003)	Loss 0.3311 (0.4174)	Acc@1 89.062 (85.789)	Acc@5 99.219 (99.440)
Epoch: [219][192/196]	Time 0.069 (0.084)	Data 0.000 (0.002)	Loss 0.3896 (0.4204)	Acc@1 83.594 (85.486)	Acc@5 100.000 (99.433)
after train
test acc: 80.05
Epoche: [220/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [220][0/196]	Time 0.108 (0.108)	Data 0.386 (0.386)	Loss 0.4334 (0.4334)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [220][64/196]	Time 0.082 (0.085)	Data 0.000 (0.006)	Loss 0.3970 (0.4260)	Acc@1 84.375 (85.331)	Acc@5 100.000 (99.555)
Epoch: [220][128/196]	Time 0.091 (0.084)	Data 0.000 (0.003)	Loss 0.5006 (0.4247)	Acc@1 82.812 (85.389)	Acc@5 98.828 (99.488)
Epoch: [220][192/196]	Time 0.067 (0.083)	Data 0.000 (0.002)	Loss 0.5237 (0.4283)	Acc@1 81.250 (85.257)	Acc@5 98.828 (99.464)
after train
test acc: 72.58
Epoche: [221/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [221][0/196]	Time 0.127 (0.127)	Data 0.366 (0.366)	Loss 0.3821 (0.3821)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [221][64/196]	Time 0.103 (0.082)	Data 0.000 (0.006)	Loss 0.3819 (0.4197)	Acc@1 86.719 (85.258)	Acc@5 99.609 (99.489)
Epoch: [221][128/196]	Time 0.071 (0.081)	Data 0.000 (0.003)	Loss 0.4967 (0.4194)	Acc@1 80.078 (85.417)	Acc@5 99.609 (99.494)
Epoch: [221][192/196]	Time 0.077 (0.080)	Data 0.000 (0.002)	Loss 0.3651 (0.4271)	Acc@1 88.281 (85.239)	Acc@5 99.609 (99.454)
after train
test acc: 74.33
Epoche: [222/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [222][0/196]	Time 0.186 (0.186)	Data 0.343 (0.343)	Loss 0.3713 (0.3713)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [222][64/196]	Time 0.063 (0.085)	Data 0.000 (0.006)	Loss 0.3823 (0.4143)	Acc@1 87.500 (86.058)	Acc@5 99.219 (99.513)
Epoch: [222][128/196]	Time 0.126 (0.083)	Data 0.000 (0.003)	Loss 0.4604 (0.4288)	Acc@1 83.594 (85.486)	Acc@5 99.609 (99.446)
Epoch: [222][192/196]	Time 0.063 (0.083)	Data 0.000 (0.002)	Loss 0.5110 (0.4308)	Acc@1 82.031 (85.328)	Acc@5 100.000 (99.452)
after train
test acc: 76.76
Epoche: [223/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [223][0/196]	Time 0.098 (0.098)	Data 0.385 (0.385)	Loss 0.4151 (0.4151)	Acc@1 86.328 (86.328)	Acc@5 99.219 (99.219)
Epoch: [223][64/196]	Time 0.065 (0.078)	Data 0.000 (0.006)	Loss 0.4088 (0.4247)	Acc@1 86.328 (85.288)	Acc@5 99.219 (99.513)
Epoch: [223][128/196]	Time 0.063 (0.077)	Data 0.000 (0.003)	Loss 0.5064 (0.4264)	Acc@1 82.031 (85.244)	Acc@5 99.609 (99.473)
Epoch: [223][192/196]	Time 0.073 (0.080)	Data 0.000 (0.002)	Loss 0.4563 (0.4266)	Acc@1 83.984 (85.308)	Acc@5 98.828 (99.449)
after train
test acc: 78.62
Epoche: [224/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [224][0/196]	Time 0.085 (0.085)	Data 0.431 (0.431)	Loss 0.4083 (0.4083)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [224][64/196]	Time 0.064 (0.083)	Data 0.000 (0.007)	Loss 0.4143 (0.4240)	Acc@1 84.375 (85.385)	Acc@5 99.609 (99.465)
Epoch: [224][128/196]	Time 0.072 (0.080)	Data 0.000 (0.004)	Loss 0.4571 (0.4290)	Acc@1 85.156 (85.174)	Acc@5 99.219 (99.458)
Epoch: [224][192/196]	Time 0.069 (0.079)	Data 0.000 (0.002)	Loss 0.4231 (0.4279)	Acc@1 81.250 (85.166)	Acc@5 99.219 (99.456)
after train
test acc: 77.27
Epoche: [225/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [225][0/196]	Time 0.119 (0.119)	Data 0.415 (0.415)	Loss 0.4156 (0.4156)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [225][64/196]	Time 0.067 (0.085)	Data 0.000 (0.007)	Loss 0.4305 (0.4192)	Acc@1 84.375 (85.733)	Acc@5 99.219 (99.429)
Epoch: [225][128/196]	Time 0.067 (0.082)	Data 0.000 (0.003)	Loss 0.5322 (0.4231)	Acc@1 80.859 (85.386)	Acc@5 99.219 (99.467)
Epoch: [225][192/196]	Time 0.064 (0.082)	Data 0.000 (0.002)	Loss 0.4249 (0.4242)	Acc@1 85.156 (85.284)	Acc@5 99.219 (99.472)
after train
test acc: 78.13
Epoche: [226/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [226][0/196]	Time 0.103 (0.103)	Data 0.383 (0.383)	Loss 0.4552 (0.4552)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [226][64/196]	Time 0.071 (0.076)	Data 0.000 (0.006)	Loss 0.4956 (0.4265)	Acc@1 82.812 (85.469)	Acc@5 99.609 (99.471)
Epoch: [226][128/196]	Time 0.067 (0.083)	Data 0.000 (0.003)	Loss 0.4466 (0.4247)	Acc@1 84.766 (85.556)	Acc@5 99.219 (99.440)
Epoch: [226][192/196]	Time 0.070 (0.082)	Data 0.000 (0.002)	Loss 0.4994 (0.4269)	Acc@1 84.766 (85.429)	Acc@5 98.828 (99.456)
after train
test acc: 76.98
Epoche: [227/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [227][0/196]	Time 0.108 (0.108)	Data 0.420 (0.420)	Loss 0.4634 (0.4634)	Acc@1 84.375 (84.375)	Acc@5 98.828 (98.828)
Epoch: [227][64/196]	Time 0.069 (0.082)	Data 0.000 (0.007)	Loss 0.4183 (0.4308)	Acc@1 84.766 (85.282)	Acc@5 98.828 (99.375)
Epoch: [227][128/196]	Time 0.090 (0.081)	Data 0.000 (0.004)	Loss 0.3663 (0.4260)	Acc@1 86.719 (85.405)	Acc@5 99.609 (99.382)
Epoch: [227][192/196]	Time 0.066 (0.083)	Data 0.000 (0.002)	Loss 0.4976 (0.4252)	Acc@1 85.156 (85.440)	Acc@5 99.219 (99.425)
after train
test acc: 78.53
Epoche: [228/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [228][0/196]	Time 0.139 (0.139)	Data 0.385 (0.385)	Loss 0.3679 (0.3679)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [228][64/196]	Time 0.089 (0.080)	Data 0.000 (0.006)	Loss 0.4583 (0.4174)	Acc@1 83.984 (85.493)	Acc@5 99.219 (99.489)
Epoch: [228][128/196]	Time 0.113 (0.079)	Data 0.000 (0.003)	Loss 0.4343 (0.4247)	Acc@1 83.984 (85.383)	Acc@5 99.609 (99.425)
Epoch: [228][192/196]	Time 0.101 (0.080)	Data 0.000 (0.002)	Loss 0.3428 (0.4249)	Acc@1 88.281 (85.391)	Acc@5 100.000 (99.425)
after train
test acc: 75.32
Epoche: [229/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [229][0/196]	Time 0.150 (0.150)	Data 0.467 (0.467)	Loss 0.4544 (0.4544)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [229][64/196]	Time 0.086 (0.086)	Data 0.000 (0.007)	Loss 0.3537 (0.4245)	Acc@1 86.719 (85.487)	Acc@5 100.000 (99.423)
Epoch: [229][128/196]	Time 0.062 (0.083)	Data 0.000 (0.004)	Loss 0.3995 (0.4252)	Acc@1 86.719 (85.414)	Acc@5 100.000 (99.464)
Epoch: [229][192/196]	Time 0.064 (0.082)	Data 0.000 (0.003)	Loss 0.4280 (0.4271)	Acc@1 85.938 (85.272)	Acc@5 98.828 (99.443)
after train
test acc: 76.08
Epoche: [230/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [230][0/196]	Time 0.127 (0.127)	Data 0.345 (0.345)	Loss 0.3074 (0.3074)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [230][64/196]	Time 0.070 (0.086)	Data 0.000 (0.006)	Loss 0.3941 (0.4297)	Acc@1 86.719 (85.192)	Acc@5 99.219 (99.447)
Epoch: [230][128/196]	Time 0.066 (0.085)	Data 0.000 (0.003)	Loss 0.4315 (0.4250)	Acc@1 85.938 (85.471)	Acc@5 99.219 (99.440)
Epoch: [230][192/196]	Time 0.066 (0.083)	Data 0.000 (0.002)	Loss 0.4479 (0.4282)	Acc@1 83.984 (85.448)	Acc@5 99.219 (99.423)
after train
test acc: 76.81
Epoche: [231/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [231][0/196]	Time 0.111 (0.111)	Data 0.386 (0.386)	Loss 0.4144 (0.4144)	Acc@1 86.328 (86.328)	Acc@5 99.609 (99.609)
Epoch: [231][64/196]	Time 0.072 (0.084)	Data 0.000 (0.006)	Loss 0.4495 (0.4154)	Acc@1 83.984 (85.703)	Acc@5 98.438 (99.483)
Epoch: [231][128/196]	Time 0.088 (0.085)	Data 0.000 (0.003)	Loss 0.3617 (0.4170)	Acc@1 87.891 (85.595)	Acc@5 99.219 (99.449)
Epoch: [231][192/196]	Time 0.076 (0.085)	Data 0.000 (0.002)	Loss 0.4157 (0.4174)	Acc@1 87.109 (85.689)	Acc@5 99.219 (99.437)
after train
test acc: 74.67
Epoche: [232/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [232][0/196]	Time 0.107 (0.107)	Data 0.478 (0.478)	Loss 0.4211 (0.4211)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [232][64/196]	Time 0.071 (0.091)	Data 0.000 (0.008)	Loss 0.3255 (0.4186)	Acc@1 87.500 (85.445)	Acc@5 99.609 (99.519)
Epoch: [232][128/196]	Time 0.082 (0.088)	Data 0.000 (0.004)	Loss 0.5077 (0.4224)	Acc@1 82.812 (85.405)	Acc@5 99.219 (99.485)
Epoch: [232][192/196]	Time 0.109 (0.086)	Data 0.000 (0.003)	Loss 0.5608 (0.4244)	Acc@1 80.859 (85.415)	Acc@5 99.219 (99.472)
after train
test acc: 79.05
Epoche: [233/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [233][0/196]	Time 0.152 (0.152)	Data 0.296 (0.296)	Loss 0.4212 (0.4212)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [233][64/196]	Time 0.062 (0.082)	Data 0.000 (0.005)	Loss 0.4177 (0.4179)	Acc@1 87.109 (85.433)	Acc@5 99.219 (99.519)
Epoch: [233][128/196]	Time 0.075 (0.080)	Data 0.000 (0.003)	Loss 0.4549 (0.4202)	Acc@1 85.547 (85.411)	Acc@5 99.609 (99.491)
Epoch: [233][192/196]	Time 0.071 (0.080)	Data 0.000 (0.002)	Loss 0.4598 (0.4240)	Acc@1 86.719 (85.294)	Acc@5 98.828 (99.474)
after train
test acc: 78.42
Epoche: [234/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [234][0/196]	Time 0.128 (0.128)	Data 0.436 (0.436)	Loss 0.3928 (0.3928)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [234][64/196]	Time 0.074 (0.092)	Data 0.000 (0.007)	Loss 0.4341 (0.4119)	Acc@1 85.547 (85.956)	Acc@5 99.609 (99.381)
Epoch: [234][128/196]	Time 0.089 (0.088)	Data 0.000 (0.004)	Loss 0.4738 (0.4274)	Acc@1 83.594 (85.338)	Acc@5 100.000 (99.379)
Epoch: [234][192/196]	Time 0.072 (0.085)	Data 0.000 (0.003)	Loss 0.4082 (0.4245)	Acc@1 84.375 (85.355)	Acc@5 99.219 (99.419)
after train
test acc: 78.03
Epoche: [235/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [235][0/196]	Time 0.160 (0.160)	Data 0.360 (0.360)	Loss 0.4416 (0.4416)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [235][64/196]	Time 0.074 (0.087)	Data 0.000 (0.006)	Loss 0.4761 (0.4225)	Acc@1 83.594 (85.787)	Acc@5 99.609 (99.537)
Epoch: [235][128/196]	Time 0.072 (0.085)	Data 0.000 (0.003)	Loss 0.4542 (0.4274)	Acc@1 85.156 (85.435)	Acc@5 99.609 (99.458)
Epoch: [235][192/196]	Time 0.071 (0.084)	Data 0.000 (0.002)	Loss 0.3620 (0.4293)	Acc@1 86.328 (85.298)	Acc@5 100.000 (99.458)
after train
test acc: 74.76
Epoche: [236/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [236][0/196]	Time 0.151 (0.151)	Data 0.423 (0.423)	Loss 0.4051 (0.4051)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [236][64/196]	Time 0.093 (0.086)	Data 0.000 (0.007)	Loss 0.3202 (0.4144)	Acc@1 89.844 (85.901)	Acc@5 100.000 (99.471)
Epoch: [236][128/196]	Time 0.088 (0.084)	Data 0.000 (0.004)	Loss 0.3208 (0.4169)	Acc@1 89.062 (85.665)	Acc@5 99.609 (99.452)
Epoch: [236][192/196]	Time 0.067 (0.084)	Data 0.000 (0.002)	Loss 0.5292 (0.4259)	Acc@1 80.078 (85.340)	Acc@5 99.219 (99.409)
after train
test acc: 79.64
Epoche: [237/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [237][0/196]	Time 0.101 (0.101)	Data 0.463 (0.463)	Loss 0.4941 (0.4941)	Acc@1 82.422 (82.422)	Acc@5 98.438 (98.438)
Epoch: [237][64/196]	Time 0.070 (0.093)	Data 0.000 (0.008)	Loss 0.4229 (0.4255)	Acc@1 85.547 (85.024)	Acc@5 98.828 (99.411)
Epoch: [237][128/196]	Time 0.074 (0.087)	Data 0.000 (0.004)	Loss 0.4519 (0.4227)	Acc@1 87.109 (85.314)	Acc@5 99.609 (99.428)
Epoch: [237][192/196]	Time 0.079 (0.085)	Data 0.000 (0.003)	Loss 0.4461 (0.4239)	Acc@1 87.500 (85.334)	Acc@5 98.828 (99.395)
after train
test acc: 72.94
Epoche: [238/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [238][0/196]	Time 0.153 (0.153)	Data 0.321 (0.321)	Loss 0.4095 (0.4095)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [238][64/196]	Time 0.069 (0.087)	Data 0.000 (0.005)	Loss 0.4865 (0.4304)	Acc@1 82.812 (84.952)	Acc@5 98.438 (99.363)
Epoch: [238][128/196]	Time 0.077 (0.082)	Data 0.000 (0.003)	Loss 0.5261 (0.4309)	Acc@1 82.812 (85.029)	Acc@5 99.609 (99.367)
Epoch: [238][192/196]	Time 0.064 (0.080)	Data 0.000 (0.002)	Loss 0.3260 (0.4320)	Acc@1 88.281 (85.025)	Acc@5 99.609 (99.366)
after train
test acc: 80.66
Epoche: [239/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [239][0/196]	Time 0.114 (0.114)	Data 0.325 (0.325)	Loss 0.3905 (0.3905)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [239][64/196]	Time 0.070 (0.078)	Data 0.000 (0.005)	Loss 0.3876 (0.4055)	Acc@1 85.547 (85.865)	Acc@5 99.219 (99.453)
Epoch: [239][128/196]	Time 0.066 (0.080)	Data 0.000 (0.003)	Loss 0.4157 (0.4155)	Acc@1 85.938 (85.559)	Acc@5 98.438 (99.461)
Epoch: [239][192/196]	Time 0.071 (0.081)	Data 0.000 (0.002)	Loss 0.4301 (0.4184)	Acc@1 84.375 (85.506)	Acc@5 99.609 (99.437)
after train
test acc: 77.88
Epoche: [240/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [240][0/196]	Time 0.107 (0.107)	Data 0.444 (0.444)	Loss 0.3657 (0.3657)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [240][64/196]	Time 0.079 (0.085)	Data 0.000 (0.007)	Loss 0.3877 (0.4237)	Acc@1 87.891 (85.343)	Acc@5 99.609 (99.507)
Epoch: [240][128/196]	Time 0.102 (0.085)	Data 0.000 (0.004)	Loss 0.4437 (0.4187)	Acc@1 83.984 (85.617)	Acc@5 99.219 (99.506)
Epoch: [240][192/196]	Time 0.070 (0.084)	Data 0.000 (0.003)	Loss 0.4103 (0.4234)	Acc@1 87.891 (85.405)	Acc@5 99.219 (99.460)
after train
test acc: 76.56
[INFO] Storing checkpoint...
Max memory: 44.438016
 -1608542479.366s  