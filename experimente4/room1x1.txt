no display found. Using non-interactive Agg backend
[3, 3, 3]
[4, 8, 16]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 120; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/room1x1/model.nn; checkpoint: ./output/experimente4/room1x1; saveModell: True; LR: 0.1
random number: 7035
Files already downloaded and verified

width: 4
module: Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
conv gefunden
(i,j): (3, 0
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
conv gefunden
(i,j): (4, 0
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
conv gefunden
(i,j): (5, 0
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=16, out_features=10, bias=True)
width: 8
module: Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 4
conv gefunden
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
conv gefunden
(i,j): (7, 0
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
conv gefunden
(i,j): (8, 0
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
conv gefunden
(i,j): (9, 0
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=16, out_features=10, bias=True)
width: 16
module: Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 8
conv gefunden
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
conv gefunden
(i,j): (11, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (12, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (13, 0
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=16, out_features=10, bias=True)
stagesI: {4: [(3, 0), (4, 0), (5, 0), (6, 0), (7, 0)], 8: [(8, 0), (9, 0), (10, 0), (11, 0)], 16: [(12, 0), (13, 0), (15, None)]}
stagesO: {4: [(0, None), (3, 0), (4, 0), (5, 0)], 8: [(6, 0), (7, 0), (8, 0), (9, 0)], 16: [(10, 0), (11, 0), (12, 0), (13, 0)]}
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (7): Sequential(
      (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (11): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): AdaptiveAvgPool2d(output_size=(1, 1))
    (15): Linear(in_features=16, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
deeper epoch: 0
Epoche: [1/120]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [1][0/196]	Time 0.111 (0.111)	Data 0.516 (0.516)	Loss 2.4441 (2.4441)	Acc@1 9.766 (9.766)	Acc@5 50.391 (50.391)
Epoch: [1][64/196]	Time 0.057 (0.049)	Data 0.000 (0.008)	Loss 1.9306 (2.0208)	Acc@1 24.219 (21.869)	Acc@5 82.812 (75.463)
Epoch: [1][128/196]	Time 0.057 (0.053)	Data 0.000 (0.004)	Loss 1.7391 (1.9138)	Acc@1 31.641 (25.993)	Acc@5 85.156 (80.339)
Epoch: [1][192/196]	Time 0.058 (0.055)	Data 0.000 (0.003)	Loss 1.6621 (1.8486)	Acc@1 40.625 (28.765)	Acc@5 90.234 (82.912)
after train
test acc: 26.21
Epoche: [2/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [2][0/196]	Time 0.086 (0.086)	Data 0.296 (0.296)	Loss 1.7196 (1.7196)	Acc@1 32.422 (32.422)	Acc@5 89.062 (89.062)
Epoch: [2][64/196]	Time 0.057 (0.052)	Data 0.000 (0.005)	Loss 1.5452 (1.6440)	Acc@1 42.969 (37.752)	Acc@5 90.234 (89.393)
Epoch: [2][128/196]	Time 0.057 (0.055)	Data 0.000 (0.003)	Loss 1.3765 (1.6022)	Acc@1 46.484 (39.953)	Acc@5 93.359 (90.074)
Epoch: [2][192/196]	Time 0.058 (0.056)	Data 0.000 (0.002)	Loss 1.4189 (1.5751)	Acc@1 49.609 (41.398)	Acc@5 90.625 (90.487)
after train
test acc: 36.68
Epoche: [3/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [3][0/196]	Time 0.093 (0.093)	Data 0.445 (0.445)	Loss 1.4242 (1.4242)	Acc@1 47.656 (47.656)	Acc@5 93.750 (93.750)
Epoch: [3][64/196]	Time 0.049 (0.047)	Data 0.000 (0.007)	Loss 1.5337 (1.4688)	Acc@1 44.531 (45.403)	Acc@5 89.453 (92.320)
Epoch: [3][128/196]	Time 0.046 (0.046)	Data 0.000 (0.004)	Loss 1.3578 (1.4582)	Acc@1 52.344 (46.179)	Acc@5 93.750 (92.357)
Epoch: [3][192/196]	Time 0.045 (0.046)	Data 0.000 (0.003)	Loss 1.4352 (1.4421)	Acc@1 47.266 (46.948)	Acc@5 94.922 (92.613)
after train
test acc: 41.94
Epoche: [4/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [4][0/196]	Time 0.086 (0.086)	Data 0.307 (0.307)	Loss 1.3519 (1.3519)	Acc@1 51.172 (51.172)	Acc@5 92.578 (92.578)
Epoch: [4][64/196]	Time 0.058 (0.055)	Data 0.000 (0.005)	Loss 1.2970 (1.3693)	Acc@1 51.562 (50.595)	Acc@5 91.797 (93.107)
Epoch: [4][128/196]	Time 0.058 (0.056)	Data 0.000 (0.003)	Loss 1.2359 (1.3546)	Acc@1 56.641 (50.978)	Acc@5 93.359 (93.414)
Epoch: [4][192/196]	Time 0.058 (0.057)	Data 0.000 (0.002)	Loss 1.3956 (1.3404)	Acc@1 50.781 (51.423)	Acc@5 93.750 (93.699)
after train
test acc: 34.03
Epoche: [5/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [5][0/196]	Time 0.056 (0.056)	Data 0.446 (0.446)	Loss 1.1556 (1.1556)	Acc@1 55.859 (55.859)	Acc@5 96.094 (96.094)
Epoch: [5][64/196]	Time 0.045 (0.049)	Data 0.000 (0.007)	Loss 1.2007 (1.2737)	Acc@1 58.984 (53.864)	Acc@5 97.656 (94.411)
Epoch: [5][128/196]	Time 0.052 (0.048)	Data 0.000 (0.004)	Loss 1.0913 (1.2539)	Acc@1 59.766 (54.697)	Acc@5 96.094 (94.534)
Epoch: [5][192/196]	Time 0.052 (0.050)	Data 0.000 (0.003)	Loss 1.2883 (1.2527)	Acc@1 53.125 (54.860)	Acc@5 95.312 (94.645)
after train
test acc: 50.76
Epoche: [6/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [6][0/196]	Time 0.059 (0.059)	Data 0.515 (0.515)	Loss 1.2353 (1.2353)	Acc@1 55.859 (55.859)	Acc@5 94.531 (94.531)
Epoch: [6][64/196]	Time 0.048 (0.046)	Data 0.000 (0.008)	Loss 1.2790 (1.2099)	Acc@1 54.297 (56.851)	Acc@5 94.531 (94.898)
Epoch: [6][128/196]	Time 0.046 (0.047)	Data 0.000 (0.004)	Loss 1.0776 (1.2073)	Acc@1 58.594 (56.659)	Acc@5 98.047 (95.085)
Epoch: [6][192/196]	Time 0.045 (0.047)	Data 0.000 (0.003)	Loss 1.1512 (1.1998)	Acc@1 58.984 (56.863)	Acc@5 93.750 (95.136)
after train
test acc: 43.75
Epoche: [7/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [7][0/196]	Time 0.056 (0.056)	Data 0.405 (0.405)	Loss 1.1586 (1.1586)	Acc@1 62.891 (62.891)	Acc@5 95.312 (95.312)
Epoch: [7][64/196]	Time 0.052 (0.048)	Data 0.000 (0.006)	Loss 1.1889 (1.1802)	Acc@1 56.641 (57.722)	Acc@5 95.312 (95.144)
Epoch: [7][128/196]	Time 0.058 (0.052)	Data 0.000 (0.003)	Loss 1.2106 (1.1731)	Acc@1 54.688 (58.064)	Acc@5 94.531 (95.237)
Epoch: [7][192/196]	Time 0.058 (0.054)	Data 0.000 (0.002)	Loss 1.1296 (1.1605)	Acc@1 60.938 (58.343)	Acc@5 96.484 (95.359)
after train
test acc: 56.14
Epoche: [8/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [8][0/196]	Time 0.070 (0.070)	Data 0.440 (0.440)	Loss 1.1206 (1.1206)	Acc@1 57.031 (57.031)	Acc@5 96.875 (96.875)
Epoch: [8][64/196]	Time 0.059 (0.054)	Data 0.000 (0.007)	Loss 1.1904 (1.1402)	Acc@1 57.422 (59.561)	Acc@5 94.922 (95.469)
Epoch: [8][128/196]	Time 0.045 (0.051)	Data 0.000 (0.004)	Loss 1.0752 (1.1312)	Acc@1 60.938 (59.814)	Acc@5 97.266 (95.603)
Epoch: [8][192/196]	Time 0.045 (0.049)	Data 0.000 (0.003)	Loss 1.0973 (1.1263)	Acc@1 62.109 (59.934)	Acc@5 94.922 (95.606)
after train
test acc: 53.76
Epoche: [9/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [9][0/196]	Time 0.058 (0.058)	Data 0.314 (0.314)	Loss 1.1019 (1.1019)	Acc@1 60.547 (60.547)	Acc@5 96.094 (96.094)
Epoch: [9][64/196]	Time 0.058 (0.052)	Data 0.000 (0.005)	Loss 1.1705 (1.1185)	Acc@1 59.375 (60.072)	Acc@5 94.531 (95.835)
Epoch: [9][128/196]	Time 0.051 (0.055)	Data 0.000 (0.003)	Loss 1.1122 (1.1090)	Acc@1 60.156 (60.374)	Acc@5 93.359 (95.806)
Epoch: [9][192/196]	Time 0.045 (0.052)	Data 0.000 (0.002)	Loss 1.1751 (1.1057)	Acc@1 56.250 (60.523)	Acc@5 96.875 (95.839)
after train
test acc: 58.77
Epoche: [10/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [10][0/196]	Time 0.060 (0.060)	Data 0.355 (0.355)	Loss 1.1386 (1.1386)	Acc@1 56.250 (56.250)	Acc@5 96.094 (96.094)
Epoch: [10][64/196]	Time 0.045 (0.049)	Data 0.000 (0.006)	Loss 1.0753 (1.0882)	Acc@1 63.672 (61.286)	Acc@5 95.703 (96.040)
Epoch: [10][128/196]	Time 0.045 (0.048)	Data 0.000 (0.003)	Loss 1.1424 (1.0799)	Acc@1 62.891 (61.501)	Acc@5 94.922 (96.045)
Epoch: [10][192/196]	Time 0.041 (0.047)	Data 0.000 (0.002)	Loss 1.0621 (1.0787)	Acc@1 61.328 (61.482)	Acc@5 98.047 (96.065)
after train
test acc: 41.52
Epoche: [11/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [11][0/196]	Time 0.068 (0.068)	Data 0.299 (0.299)	Loss 1.0064 (1.0064)	Acc@1 60.547 (60.547)	Acc@5 98.438 (98.438)
Epoch: [11][64/196]	Time 0.045 (0.049)	Data 0.000 (0.005)	Loss 1.0015 (1.0773)	Acc@1 61.719 (61.448)	Acc@5 96.484 (96.238)
Epoch: [11][128/196]	Time 0.045 (0.047)	Data 0.000 (0.003)	Loss 1.1025 (1.0756)	Acc@1 60.156 (61.504)	Acc@5 95.312 (96.151)
Epoch: [11][192/196]	Time 0.036 (0.047)	Data 0.000 (0.002)	Loss 1.0123 (1.0685)	Acc@1 62.109 (61.974)	Acc@5 96.094 (96.181)
after train
test acc: 58.04
Epoche: [12/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [12][0/196]	Time 0.069 (0.069)	Data 0.292 (0.292)	Loss 0.9103 (0.9103)	Acc@1 64.453 (64.453)	Acc@5 99.219 (99.219)
Epoch: [12][64/196]	Time 0.040 (0.045)	Data 0.000 (0.005)	Loss 0.9823 (1.0640)	Acc@1 66.016 (62.085)	Acc@5 97.656 (96.046)
Epoch: [12][128/196]	Time 0.062 (0.051)	Data 0.000 (0.003)	Loss 1.0771 (1.0556)	Acc@1 60.547 (62.333)	Acc@5 94.922 (96.206)
Epoch: [12][192/196]	Time 0.058 (0.054)	Data 0.000 (0.002)	Loss 1.1169 (1.0526)	Acc@1 60.547 (62.464)	Acc@5 95.312 (96.254)
after train
test acc: 54.61
Epoche: [13/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [13][0/196]	Time 0.080 (0.080)	Data 0.297 (0.297)	Loss 1.0147 (1.0147)	Acc@1 64.062 (64.062)	Acc@5 96.875 (96.875)
Epoch: [13][64/196]	Time 0.058 (0.054)	Data 0.000 (0.005)	Loss 1.0413 (1.0549)	Acc@1 61.328 (62.374)	Acc@5 96.484 (95.901)
Epoch: [13][128/196]	Time 0.044 (0.053)	Data 0.000 (0.003)	Loss 0.8966 (1.0455)	Acc@1 67.188 (62.730)	Acc@5 97.656 (96.188)
Epoch: [13][192/196]	Time 0.045 (0.051)	Data 0.000 (0.002)	Loss 1.1285 (1.0444)	Acc@1 58.984 (62.717)	Acc@5 95.312 (96.187)
after train
test acc: 50.69
Epoche: [14/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [14][0/196]	Time 0.115 (0.115)	Data 0.418 (0.418)	Loss 0.9280 (0.9280)	Acc@1 66.016 (66.016)	Acc@5 97.656 (97.656)
Epoch: [14][64/196]	Time 0.045 (0.048)	Data 0.000 (0.007)	Loss 1.0198 (1.0341)	Acc@1 61.719 (62.981)	Acc@5 97.656 (96.514)
Epoch: [14][128/196]	Time 0.052 (0.049)	Data 0.000 (0.003)	Loss 1.0183 (1.0335)	Acc@1 60.547 (62.975)	Acc@5 98.047 (96.493)
Epoch: [14][192/196]	Time 0.045 (0.049)	Data 0.000 (0.002)	Loss 1.0402 (1.0316)	Acc@1 62.109 (63.301)	Acc@5 96.094 (96.383)
after train
test acc: 50.79
Epoche: [15/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [15][0/196]	Time 0.079 (0.079)	Data 0.429 (0.429)	Loss 1.1716 (1.1716)	Acc@1 57.031 (57.031)	Acc@5 95.312 (95.312)
Epoch: [15][64/196]	Time 0.058 (0.054)	Data 0.000 (0.007)	Loss 0.9271 (1.0327)	Acc@1 65.234 (63.089)	Acc@5 98.047 (96.364)
Epoch: [15][128/196]	Time 0.046 (0.050)	Data 0.000 (0.004)	Loss 0.9230 (1.0212)	Acc@1 67.188 (63.605)	Acc@5 94.922 (96.345)
Epoch: [15][192/196]	Time 0.058 (0.052)	Data 0.000 (0.002)	Loss 1.0173 (1.0249)	Acc@1 66.016 (63.540)	Acc@5 93.359 (96.296)
after train
test acc: 53.91
Epoche: [16/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [16][0/196]	Time 0.083 (0.083)	Data 0.338 (0.338)	Loss 1.0211 (1.0211)	Acc@1 64.453 (64.453)	Acc@5 95.312 (95.312)
Epoch: [16][64/196]	Time 0.046 (0.048)	Data 0.000 (0.005)	Loss 1.0224 (1.0162)	Acc@1 62.500 (63.978)	Acc@5 96.094 (96.514)
Epoch: [16][128/196]	Time 0.046 (0.048)	Data 0.000 (0.003)	Loss 0.9968 (1.0108)	Acc@1 65.625 (64.405)	Acc@5 96.875 (96.563)
Epoch: [16][192/196]	Time 0.045 (0.047)	Data 0.000 (0.002)	Loss 1.0388 (1.0112)	Acc@1 62.500 (64.139)	Acc@5 96.484 (96.590)
after train
test acc: 50.47
Epoche: [17/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [17][0/196]	Time 0.091 (0.091)	Data 0.443 (0.443)	Loss 0.9934 (0.9934)	Acc@1 62.500 (62.500)	Acc@5 96.094 (96.094)
Epoch: [17][64/196]	Time 0.048 (0.047)	Data 0.000 (0.007)	Loss 0.9872 (0.9954)	Acc@1 63.672 (64.537)	Acc@5 96.875 (96.641)
Epoch: [17][128/196]	Time 0.051 (0.048)	Data 0.000 (0.004)	Loss 0.9820 (0.9958)	Acc@1 61.719 (64.550)	Acc@5 97.656 (96.599)
Epoch: [17][192/196]	Time 0.052 (0.048)	Data 0.000 (0.003)	Loss 1.0195 (1.0019)	Acc@1 62.500 (64.384)	Acc@5 96.484 (96.573)
after train
test acc: 47.82
Epoche: [18/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [18][0/196]	Time 0.073 (0.073)	Data 0.325 (0.325)	Loss 0.9618 (0.9618)	Acc@1 64.453 (64.453)	Acc@5 97.266 (97.266)
Epoch: [18][64/196]	Time 0.057 (0.057)	Data 0.000 (0.005)	Loss 0.9693 (0.9907)	Acc@1 65.625 (64.784)	Acc@5 96.875 (96.833)
Epoch: [18][128/196]	Time 0.042 (0.056)	Data 0.000 (0.003)	Loss 0.9765 (0.9967)	Acc@1 62.500 (64.677)	Acc@5 97.656 (96.648)
Epoch: [18][192/196]	Time 0.045 (0.052)	Data 0.000 (0.002)	Loss 1.0034 (1.0011)	Acc@1 63.281 (64.494)	Acc@5 96.875 (96.551)
after train
test acc: 56.11
Epoche: [19/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [19][0/196]	Time 0.051 (0.051)	Data 0.511 (0.511)	Loss 0.8615 (0.8615)	Acc@1 67.578 (67.578)	Acc@5 97.266 (97.266)
Epoch: [19][64/196]	Time 0.058 (0.058)	Data 0.000 (0.008)	Loss 0.9886 (1.0006)	Acc@1 65.625 (64.014)	Acc@5 98.047 (96.599)
Epoch: [19][128/196]	Time 0.058 (0.058)	Data 0.000 (0.004)	Loss 0.9985 (0.9898)	Acc@1 65.625 (64.608)	Acc@5 96.875 (96.675)
Epoch: [19][192/196]	Time 0.058 (0.058)	Data 0.000 (0.003)	Loss 0.9954 (0.9927)	Acc@1 66.406 (64.568)	Acc@5 96.094 (96.707)
after train
test acc: 53.87
Epoche: [20/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [20][0/196]	Time 0.052 (0.052)	Data 0.300 (0.300)	Loss 1.0030 (1.0030)	Acc@1 64.844 (64.844)	Acc@5 96.875 (96.875)
Epoch: [20][64/196]	Time 0.045 (0.048)	Data 0.000 (0.005)	Loss 1.0063 (1.0005)	Acc@1 60.547 (64.345)	Acc@5 95.312 (96.544)
Epoch: [20][128/196]	Time 0.058 (0.048)	Data 0.000 (0.003)	Loss 1.1546 (0.9997)	Acc@1 60.156 (64.302)	Acc@5 95.703 (96.648)
Epoch: [20][192/196]	Time 0.052 (0.050)	Data 0.000 (0.002)	Loss 0.9255 (0.9939)	Acc@1 68.359 (64.643)	Acc@5 98.047 (96.685)
after train
test acc: 40.86
Epoche: [21/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [21][0/196]	Time 0.091 (0.091)	Data 0.411 (0.411)	Loss 1.0205 (1.0205)	Acc@1 61.719 (61.719)	Acc@5 96.094 (96.094)
Epoch: [21][64/196]	Time 0.044 (0.048)	Data 0.000 (0.007)	Loss 1.0437 (1.0126)	Acc@1 60.938 (64.219)	Acc@5 97.266 (96.514)
Epoch: [21][128/196]	Time 0.052 (0.050)	Data 0.000 (0.003)	Loss 0.8282 (0.9945)	Acc@1 71.094 (64.877)	Acc@5 97.656 (96.627)
Epoch: [21][192/196]	Time 0.052 (0.051)	Data 0.000 (0.002)	Loss 0.9776 (0.9865)	Acc@1 65.625 (64.963)	Acc@5 97.266 (96.687)
after train
test acc: 51.03
Epoche: [22/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [22][0/196]	Time 0.086 (0.086)	Data 0.304 (0.304)	Loss 0.9901 (0.9901)	Acc@1 64.844 (64.844)	Acc@5 97.656 (97.656)
Epoch: [22][64/196]	Time 0.047 (0.048)	Data 0.000 (0.005)	Loss 0.9265 (0.9779)	Acc@1 68.359 (65.012)	Acc@5 95.703 (96.893)
Epoch: [22][128/196]	Time 0.046 (0.047)	Data 0.000 (0.003)	Loss 0.9261 (0.9776)	Acc@1 67.578 (65.013)	Acc@5 96.875 (96.787)
Epoch: [22][192/196]	Time 0.045 (0.047)	Data 0.000 (0.002)	Loss 0.8677 (0.9792)	Acc@1 72.266 (65.119)	Acc@5 98.047 (96.794)
after train
test acc: 62.36
Epoche: [23/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [23][0/196]	Time 0.089 (0.089)	Data 0.307 (0.307)	Loss 0.8318 (0.8318)	Acc@1 68.750 (68.750)	Acc@5 98.047 (98.047)
Epoch: [23][64/196]	Time 0.045 (0.047)	Data 0.000 (0.005)	Loss 0.8695 (0.9620)	Acc@1 69.531 (65.799)	Acc@5 98.828 (97.181)
Epoch: [23][128/196]	Time 0.050 (0.048)	Data 0.000 (0.003)	Loss 0.9896 (0.9714)	Acc@1 66.016 (65.558)	Acc@5 96.484 (97.029)
Epoch: [23][192/196]	Time 0.045 (0.048)	Data 0.000 (0.002)	Loss 1.0769 (0.9746)	Acc@1 66.016 (65.435)	Acc@5 95.703 (96.903)
after train
test acc: 55.83
Epoche: [24/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [24][0/196]	Time 0.087 (0.087)	Data 0.425 (0.425)	Loss 0.9019 (0.9019)	Acc@1 67.578 (67.578)	Acc@5 97.266 (97.266)
Epoch: [24][64/196]	Time 0.045 (0.054)	Data 0.000 (0.007)	Loss 0.9309 (0.9711)	Acc@1 65.234 (65.463)	Acc@5 97.266 (96.839)
Epoch: [24][128/196]	Time 0.058 (0.055)	Data 0.000 (0.004)	Loss 1.0087 (0.9646)	Acc@1 66.797 (65.855)	Acc@5 94.531 (96.951)
Epoch: [24][192/196]	Time 0.058 (0.056)	Data 0.000 (0.002)	Loss 0.9512 (0.9623)	Acc@1 67.969 (65.963)	Acc@5 96.484 (96.972)
after train
test acc: 49.61
Epoche: [25/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [25][0/196]	Time 0.059 (0.059)	Data 0.379 (0.379)	Loss 0.9741 (0.9741)	Acc@1 67.969 (67.969)	Acc@5 95.703 (95.703)
Epoch: [25][64/196]	Time 0.052 (0.049)	Data 0.000 (0.006)	Loss 0.9349 (0.9888)	Acc@1 64.844 (64.928)	Acc@5 97.656 (96.893)
Epoch: [25][128/196]	Time 0.044 (0.049)	Data 0.000 (0.003)	Loss 0.9388 (0.9750)	Acc@1 71.484 (65.477)	Acc@5 96.484 (96.905)
Epoch: [25][192/196]	Time 0.058 (0.052)	Data 0.000 (0.002)	Loss 0.8885 (0.9699)	Acc@1 69.531 (65.732)	Acc@5 98.828 (96.885)
after train
test acc: 62.67
Epoche: [26/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [26][0/196]	Time 0.064 (0.064)	Data 0.415 (0.415)	Loss 0.8920 (0.8920)	Acc@1 70.312 (70.312)	Acc@5 97.266 (97.266)
Epoch: [26][64/196]	Time 0.044 (0.048)	Data 0.000 (0.007)	Loss 0.8986 (0.9596)	Acc@1 69.531 (66.028)	Acc@5 96.875 (97.019)
Epoch: [26][128/196]	Time 0.052 (0.049)	Data 0.000 (0.004)	Loss 1.0207 (0.9601)	Acc@1 66.797 (65.834)	Acc@5 95.312 (97.029)
Epoch: [26][192/196]	Time 0.045 (0.050)	Data 0.000 (0.003)	Loss 0.9479 (0.9589)	Acc@1 62.500 (66.182)	Acc@5 98.828 (96.982)
after train
test acc: 55.5
Epoche: [27/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [27][0/196]	Time 0.083 (0.083)	Data 0.296 (0.296)	Loss 0.8970 (0.8970)	Acc@1 67.578 (67.578)	Acc@5 98.438 (98.438)
Epoch: [27][64/196]	Time 0.045 (0.046)	Data 0.000 (0.005)	Loss 1.0045 (0.9657)	Acc@1 66.016 (66.076)	Acc@5 95.312 (96.863)
Epoch: [27][128/196]	Time 0.046 (0.046)	Data 0.000 (0.003)	Loss 0.9445 (0.9582)	Acc@1 66.016 (66.019)	Acc@5 97.266 (97.020)
Epoch: [27][192/196]	Time 0.045 (0.046)	Data 0.000 (0.002)	Loss 0.9750 (0.9540)	Acc@1 66.797 (66.220)	Acc@5 96.484 (97.069)
after train
test acc: 53.04
Epoche: [28/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [28][0/196]	Time 0.052 (0.052)	Data 0.323 (0.323)	Loss 0.8421 (0.8421)	Acc@1 70.312 (70.312)	Acc@5 97.656 (97.656)
Epoch: [28][64/196]	Time 0.046 (0.046)	Data 0.000 (0.005)	Loss 0.9291 (0.9418)	Acc@1 64.844 (66.857)	Acc@5 96.094 (97.188)
Epoch: [28][128/196]	Time 0.058 (0.052)	Data 0.000 (0.003)	Loss 1.0513 (0.9496)	Acc@1 57.812 (66.518)	Acc@5 97.266 (97.084)
Epoch: [28][192/196]	Time 0.058 (0.054)	Data 0.000 (0.002)	Loss 0.9705 (0.9513)	Acc@1 65.234 (66.673)	Acc@5 97.266 (97.047)
after train
test acc: 56.67
Epoche: [29/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [29][0/196]	Time 0.071 (0.071)	Data 0.414 (0.414)	Loss 1.0131 (1.0131)	Acc@1 63.281 (63.281)	Acc@5 96.094 (96.094)
Epoch: [29][64/196]	Time 0.058 (0.059)	Data 0.000 (0.007)	Loss 0.9809 (0.9211)	Acc@1 67.188 (67.374)	Acc@5 96.484 (97.326)
Epoch: [29][128/196]	Time 0.058 (0.058)	Data 0.000 (0.004)	Loss 0.9758 (0.9356)	Acc@1 64.062 (66.809)	Acc@5 98.438 (97.263)
Epoch: [29][192/196]	Time 0.045 (0.055)	Data 0.000 (0.002)	Loss 0.9598 (0.9406)	Acc@1 65.625 (66.744)	Acc@5 96.094 (97.160)
after train
test acc: 53.43
Epoche: [30/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [30][0/196]	Time 0.077 (0.077)	Data 0.338 (0.338)	Loss 0.8489 (0.8489)	Acc@1 67.969 (67.969)	Acc@5 99.219 (99.219)
Epoch: [30][64/196]	Time 0.046 (0.052)	Data 0.000 (0.005)	Loss 0.9208 (0.9173)	Acc@1 67.969 (67.446)	Acc@5 98.438 (97.452)
Epoch: [30][128/196]	Time 0.058 (0.054)	Data 0.000 (0.003)	Loss 0.8979 (0.9309)	Acc@1 67.969 (66.842)	Acc@5 96.875 (97.335)
Epoch: [30][192/196]	Time 0.058 (0.054)	Data 0.000 (0.002)	Loss 0.8635 (0.9386)	Acc@1 69.141 (66.835)	Acc@5 98.047 (97.243)
after train
test acc: 46.71
Epoche: [31/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [31][0/196]	Time 0.063 (0.063)	Data 0.407 (0.407)	Loss 0.8859 (0.8859)	Acc@1 68.359 (68.359)	Acc@5 97.266 (97.266)
Epoch: [31][64/196]	Time 0.058 (0.052)	Data 0.000 (0.007)	Loss 0.9519 (0.9418)	Acc@1 64.062 (67.127)	Acc@5 97.266 (97.151)
Epoch: [31][128/196]	Time 0.046 (0.053)	Data 0.000 (0.003)	Loss 0.7388 (0.9295)	Acc@1 75.000 (67.372)	Acc@5 99.609 (97.157)
Epoch: [31][192/196]	Time 0.058 (0.053)	Data 0.000 (0.002)	Loss 0.9860 (0.9317)	Acc@1 60.547 (67.289)	Acc@5 98.438 (97.146)
after train
test acc: 58.75
Epoche: [32/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [32][0/196]	Time 0.053 (0.053)	Data 0.424 (0.424)	Loss 0.8717 (0.8717)	Acc@1 69.531 (69.531)	Acc@5 96.875 (96.875)
Epoch: [32][64/196]	Time 0.045 (0.046)	Data 0.000 (0.007)	Loss 0.9027 (0.9214)	Acc@1 66.406 (67.572)	Acc@5 97.266 (97.284)
Epoch: [32][128/196]	Time 0.058 (0.050)	Data 0.000 (0.004)	Loss 0.9502 (0.9225)	Acc@1 68.359 (67.675)	Acc@5 97.656 (97.208)
Epoch: [32][192/196]	Time 0.045 (0.051)	Data 0.000 (0.002)	Loss 0.8976 (0.9215)	Acc@1 69.531 (67.720)	Acc@5 96.875 (97.166)
after train
test acc: 54.18
Epoche: [33/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [33][0/196]	Time 0.094 (0.094)	Data 0.417 (0.417)	Loss 0.9102 (0.9102)	Acc@1 69.922 (69.922)	Acc@5 97.266 (97.266)
Epoch: [33][64/196]	Time 0.058 (0.051)	Data 0.000 (0.007)	Loss 0.9853 (0.9212)	Acc@1 63.281 (67.644)	Acc@5 97.656 (97.079)
Epoch: [33][128/196]	Time 0.046 (0.051)	Data 0.000 (0.003)	Loss 1.0275 (0.9264)	Acc@1 61.719 (67.233)	Acc@5 96.484 (97.175)
Epoch: [33][192/196]	Time 0.058 (0.052)	Data 0.000 (0.002)	Loss 1.0832 (0.9233)	Acc@1 61.719 (67.424)	Acc@5 96.094 (97.268)
after train
test acc: 51.77
Epoche: [34/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [34][0/196]	Time 0.086 (0.086)	Data 0.424 (0.424)	Loss 0.9582 (0.9582)	Acc@1 66.406 (66.406)	Acc@5 97.266 (97.266)
Epoch: [34][64/196]	Time 0.044 (0.048)	Data 0.000 (0.007)	Loss 0.9267 (0.9175)	Acc@1 67.969 (67.584)	Acc@5 97.266 (97.073)
Epoch: [34][128/196]	Time 0.045 (0.048)	Data 0.000 (0.004)	Loss 0.9450 (0.9176)	Acc@1 68.750 (67.657)	Acc@5 96.484 (97.275)
Epoch: [34][192/196]	Time 0.052 (0.048)	Data 0.000 (0.002)	Loss 0.7736 (0.9185)	Acc@1 75.781 (67.714)	Acc@5 98.047 (97.288)
after train
test acc: 38.32
Epoche: [35/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [35][0/196]	Time 0.065 (0.065)	Data 0.478 (0.478)	Loss 0.8946 (0.8946)	Acc@1 66.797 (66.797)	Acc@5 97.656 (97.656)
Epoch: [35][64/196]	Time 0.046 (0.053)	Data 0.000 (0.008)	Loss 1.0288 (0.9007)	Acc@1 64.062 (68.293)	Acc@5 97.656 (97.572)
Epoch: [35][128/196]	Time 0.055 (0.054)	Data 0.000 (0.004)	Loss 0.8787 (0.9113)	Acc@1 67.969 (67.887)	Acc@5 97.266 (97.308)
Epoch: [35][192/196]	Time 0.045 (0.052)	Data 0.000 (0.003)	Loss 0.9183 (0.9125)	Acc@1 66.016 (68.013)	Acc@5 97.266 (97.274)
after train
test acc: 52.45
Epoche: [36/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [36][0/196]	Time 0.072 (0.072)	Data 0.340 (0.340)	Loss 0.9641 (0.9641)	Acc@1 69.141 (69.141)	Acc@5 96.094 (96.094)
Epoch: [36][64/196]	Time 0.046 (0.047)	Data 0.000 (0.005)	Loss 0.8938 (0.9175)	Acc@1 68.359 (67.825)	Acc@5 98.438 (97.332)
Epoch: [36][128/196]	Time 0.058 (0.050)	Data 0.000 (0.003)	Loss 0.8761 (0.9129)	Acc@1 71.484 (67.990)	Acc@5 98.828 (97.378)
Epoch: [36][192/196]	Time 0.058 (0.051)	Data 0.000 (0.002)	Loss 0.8930 (0.9072)	Acc@1 70.703 (68.218)	Acc@5 96.875 (97.357)
after train
test acc: 54.84
Epoche: [37/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [37][0/196]	Time 0.084 (0.084)	Data 0.298 (0.298)	Loss 0.9920 (0.9920)	Acc@1 65.234 (65.234)	Acc@5 97.656 (97.656)
Epoch: [37][64/196]	Time 0.049 (0.048)	Data 0.000 (0.005)	Loss 0.8666 (0.9027)	Acc@1 70.312 (68.155)	Acc@5 98.438 (97.302)
Epoch: [37][128/196]	Time 0.046 (0.048)	Data 0.000 (0.003)	Loss 0.9384 (0.9117)	Acc@1 68.750 (67.975)	Acc@5 96.875 (97.299)
Epoch: [37][192/196]	Time 0.037 (0.047)	Data 0.000 (0.002)	Loss 0.8907 (0.9069)	Acc@1 70.312 (68.131)	Acc@5 98.047 (97.405)
after train
test acc: 56.97
Epoche: [38/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [38][0/196]	Time 0.073 (0.073)	Data 0.438 (0.438)	Loss 0.8199 (0.8199)	Acc@1 70.312 (70.312)	Acc@5 98.828 (98.828)
Epoch: [38][64/196]	Time 0.052 (0.052)	Data 0.000 (0.007)	Loss 0.7669 (0.9117)	Acc@1 75.781 (68.347)	Acc@5 96.875 (97.308)
Epoch: [38][128/196]	Time 0.052 (0.052)	Data 0.000 (0.004)	Loss 0.8830 (0.8975)	Acc@1 69.141 (68.671)	Acc@5 98.047 (97.393)
Epoch: [38][192/196]	Time 0.052 (0.052)	Data 0.000 (0.003)	Loss 0.8898 (0.8983)	Acc@1 69.141 (68.606)	Acc@5 97.656 (97.415)
after train
test acc: 49.13
Epoche: [39/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [39][0/196]	Time 0.089 (0.089)	Data 0.424 (0.424)	Loss 0.8048 (0.8048)	Acc@1 73.828 (73.828)	Acc@5 96.484 (96.484)
Epoch: [39][64/196]	Time 0.058 (0.053)	Data 0.000 (0.007)	Loss 0.9697 (0.9019)	Acc@1 64.844 (68.377)	Acc@5 97.266 (97.338)
Epoch: [39][128/196]	Time 0.045 (0.053)	Data 0.000 (0.004)	Loss 0.8739 (0.8935)	Acc@1 68.750 (68.617)	Acc@5 96.484 (97.505)
Epoch: [39][192/196]	Time 0.058 (0.054)	Data 0.000 (0.002)	Loss 0.8054 (0.8897)	Acc@1 72.656 (68.871)	Acc@5 99.219 (97.509)
after train
test acc: 65.5
Epoche: [40/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [40][0/196]	Time 0.078 (0.078)	Data 0.451 (0.451)	Loss 0.8891 (0.8891)	Acc@1 69.531 (69.531)	Acc@5 98.047 (98.047)
Epoch: [40][64/196]	Time 0.058 (0.048)	Data 0.000 (0.007)	Loss 0.9409 (0.8870)	Acc@1 64.453 (68.990)	Acc@5 96.875 (97.362)
Epoch: [40][128/196]	Time 0.046 (0.053)	Data 0.000 (0.004)	Loss 0.8278 (0.8898)	Acc@1 73.047 (68.929)	Acc@5 98.047 (97.441)
Epoch: [40][192/196]	Time 0.039 (0.052)	Data 0.000 (0.003)	Loss 0.9735 (0.8896)	Acc@1 67.188 (68.930)	Acc@5 97.266 (97.387)
after train
test acc: 61.36
Epoche: [41/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [41][0/196]	Time 0.070 (0.070)	Data 0.323 (0.323)	Loss 0.9108 (0.9108)	Acc@1 69.922 (69.922)	Acc@5 96.484 (96.484)
Epoch: [41][64/196]	Time 0.048 (0.047)	Data 0.000 (0.005)	Loss 0.9240 (0.9031)	Acc@1 65.625 (68.510)	Acc@5 97.656 (97.254)
Epoch: [41][128/196]	Time 0.056 (0.049)	Data 0.000 (0.003)	Loss 0.9938 (0.8859)	Acc@1 63.281 (69.056)	Acc@5 98.047 (97.447)
Epoch: [41][192/196]	Time 0.056 (0.051)	Data 0.000 (0.002)	Loss 0.9579 (0.8908)	Acc@1 71.875 (68.928)	Acc@5 95.312 (97.371)
after train
test acc: 59.56
Epoche: [42/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [42][0/196]	Time 0.081 (0.081)	Data 0.404 (0.404)	Loss 0.8738 (0.8738)	Acc@1 70.312 (70.312)	Acc@5 97.266 (97.266)
Epoch: [42][64/196]	Time 0.058 (0.053)	Data 0.000 (0.006)	Loss 0.9215 (0.8818)	Acc@1 65.625 (68.876)	Acc@5 96.484 (97.722)
Epoch: [42][128/196]	Time 0.046 (0.050)	Data 0.000 (0.003)	Loss 0.8668 (0.8847)	Acc@1 68.359 (69.083)	Acc@5 96.875 (97.508)
Epoch: [42][192/196]	Time 0.058 (0.051)	Data 0.000 (0.002)	Loss 0.8569 (0.8790)	Acc@1 69.141 (69.294)	Acc@5 97.266 (97.589)
after train
test acc: 34.07
Epoche: [43/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [43][0/196]	Time 0.067 (0.067)	Data 0.438 (0.438)	Loss 0.8444 (0.8444)	Acc@1 72.656 (72.656)	Acc@5 98.438 (98.438)
Epoch: [43][64/196]	Time 0.058 (0.057)	Data 0.000 (0.007)	Loss 0.8554 (0.8817)	Acc@1 67.578 (69.213)	Acc@5 97.266 (97.452)
Epoch: [43][128/196]	Time 0.046 (0.052)	Data 0.000 (0.004)	Loss 1.0080 (0.8766)	Acc@1 65.625 (69.331)	Acc@5 95.703 (97.532)
Epoch: [43][192/196]	Time 0.045 (0.050)	Data 0.000 (0.003)	Loss 0.8558 (0.8806)	Acc@1 69.922 (69.220)	Acc@5 96.875 (97.504)
after train
test acc: 41.86
Epoche: [44/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [44][0/196]	Time 0.081 (0.081)	Data 0.307 (0.307)	Loss 0.8777 (0.8777)	Acc@1 69.922 (69.922)	Acc@5 97.656 (97.656)
Epoch: [44][64/196]	Time 0.047 (0.049)	Data 0.000 (0.005)	Loss 0.8939 (0.8739)	Acc@1 69.141 (69.261)	Acc@5 97.656 (97.506)
Epoch: [44][128/196]	Time 0.058 (0.049)	Data 0.000 (0.003)	Loss 0.8746 (0.8762)	Acc@1 71.875 (69.359)	Acc@5 96.875 (97.514)
Epoch: [44][192/196]	Time 0.045 (0.048)	Data 0.000 (0.002)	Loss 0.8127 (0.8818)	Acc@1 72.656 (69.031)	Acc@5 98.438 (97.496)
after train
test acc: 60.19
Epoche: [45/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [45][0/196]	Time 0.058 (0.058)	Data 0.421 (0.421)	Loss 0.7138 (0.7138)	Acc@1 74.219 (74.219)	Acc@5 98.438 (98.438)
Epoch: [45][64/196]	Time 0.046 (0.047)	Data 0.000 (0.007)	Loss 0.9470 (0.8665)	Acc@1 64.453 (69.886)	Acc@5 96.484 (97.650)
Epoch: [45][128/196]	Time 0.052 (0.050)	Data 0.000 (0.004)	Loss 0.7832 (0.8731)	Acc@1 74.219 (69.559)	Acc@5 98.828 (97.541)
Epoch: [45][192/196]	Time 0.045 (0.049)	Data 0.000 (0.002)	Loss 0.8668 (0.8746)	Acc@1 73.047 (69.604)	Acc@5 96.875 (97.591)
after train
test acc: 57.35
Epoche: [46/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [46][0/196]	Time 0.062 (0.062)	Data 0.454 (0.454)	Loss 0.9019 (0.9019)	Acc@1 70.703 (70.703)	Acc@5 96.094 (96.094)
Epoch: [46][64/196]	Time 0.058 (0.050)	Data 0.000 (0.007)	Loss 0.9870 (0.8784)	Acc@1 66.016 (69.008)	Acc@5 97.656 (97.728)
Epoch: [46][128/196]	Time 0.046 (0.053)	Data 0.000 (0.004)	Loss 0.9228 (0.8775)	Acc@1 70.312 (69.225)	Acc@5 96.484 (97.614)
Epoch: [46][192/196]	Time 0.045 (0.051)	Data 0.000 (0.003)	Loss 0.8811 (0.8783)	Acc@1 71.094 (69.270)	Acc@5 96.094 (97.585)
after train
test acc: 60.73
Epoche: [47/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [47][0/196]	Time 0.076 (0.076)	Data 0.426 (0.426)	Loss 0.9320 (0.9320)	Acc@1 67.969 (67.969)	Acc@5 97.656 (97.656)
Epoch: [47][64/196]	Time 0.040 (0.047)	Data 0.000 (0.007)	Loss 0.8074 (0.8648)	Acc@1 71.094 (69.736)	Acc@5 96.484 (97.638)
Epoch: [47][128/196]	Time 0.055 (0.047)	Data 0.000 (0.004)	Loss 0.8686 (0.8723)	Acc@1 67.188 (69.316)	Acc@5 97.656 (97.538)
Epoch: [47][192/196]	Time 0.058 (0.050)	Data 0.000 (0.002)	Loss 0.8230 (0.8728)	Acc@1 74.609 (69.379)	Acc@5 97.656 (97.517)
after train
test acc: 65.85
Epoche: [48/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [48][0/196]	Time 0.084 (0.084)	Data 0.314 (0.314)	Loss 0.8017 (0.8017)	Acc@1 70.312 (70.312)	Acc@5 98.828 (98.828)
Epoch: [48][64/196]	Time 0.058 (0.052)	Data 0.000 (0.005)	Loss 0.7913 (0.8709)	Acc@1 69.141 (69.195)	Acc@5 99.219 (97.740)
Epoch: [48][128/196]	Time 0.058 (0.055)	Data 0.000 (0.003)	Loss 0.8943 (0.8704)	Acc@1 64.453 (69.492)	Acc@5 98.047 (97.620)
Epoch: [48][192/196]	Time 0.058 (0.056)	Data 0.000 (0.002)	Loss 0.8477 (0.8701)	Acc@1 71.484 (69.517)	Acc@5 97.266 (97.610)
after train
test acc: 57.66
Epoche: [49/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [49][0/196]	Time 0.058 (0.058)	Data 0.293 (0.293)	Loss 0.8838 (0.8838)	Acc@1 69.141 (69.141)	Acc@5 97.266 (97.266)
Epoch: [49][64/196]	Time 0.044 (0.047)	Data 0.000 (0.005)	Loss 0.9654 (0.8740)	Acc@1 65.625 (69.399)	Acc@5 97.266 (97.440)
Epoch: [49][128/196]	Time 0.046 (0.048)	Data 0.000 (0.003)	Loss 0.8830 (0.8730)	Acc@1 70.312 (69.698)	Acc@5 98.438 (97.462)
Epoch: [49][192/196]	Time 0.045 (0.048)	Data 0.000 (0.002)	Loss 0.8468 (0.8720)	Acc@1 72.656 (69.847)	Acc@5 97.266 (97.486)
after train
test acc: 56.73
Epoche: [50/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [50][0/196]	Time 0.072 (0.072)	Data 0.438 (0.438)	Loss 0.8893 (0.8893)	Acc@1 67.188 (67.188)	Acc@5 97.266 (97.266)
Epoch: [50][64/196]	Time 0.050 (0.047)	Data 0.000 (0.007)	Loss 0.7686 (0.8583)	Acc@1 76.172 (70.210)	Acc@5 97.266 (97.476)
Epoch: [50][128/196]	Time 0.047 (0.047)	Data 0.000 (0.004)	Loss 0.7734 (0.8683)	Acc@1 71.484 (69.746)	Acc@5 98.828 (97.511)
Epoch: [50][192/196]	Time 0.058 (0.047)	Data 0.000 (0.002)	Loss 0.8387 (0.8627)	Acc@1 71.094 (69.916)	Acc@5 97.656 (97.567)
after train
test acc: 60.98
Epoche: [51/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [51][0/196]	Time 0.076 (0.076)	Data 0.319 (0.319)	Loss 0.9676 (0.9676)	Acc@1 63.672 (63.672)	Acc@5 97.656 (97.656)
Epoch: [51][64/196]	Time 0.049 (0.047)	Data 0.000 (0.005)	Loss 0.7725 (0.8802)	Acc@1 73.828 (69.008)	Acc@5 98.438 (97.578)
Epoch: [51][128/196]	Time 0.047 (0.047)	Data 0.000 (0.003)	Loss 0.8384 (0.8623)	Acc@1 71.875 (69.701)	Acc@5 96.484 (97.674)
Epoch: [51][192/196]	Time 0.052 (0.047)	Data 0.000 (0.002)	Loss 0.9051 (0.8656)	Acc@1 66.406 (69.829)	Acc@5 98.047 (97.596)
after train
test acc: 58.91
Epoche: [52/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [52][0/196]	Time 0.068 (0.068)	Data 0.299 (0.299)	Loss 1.0103 (1.0103)	Acc@1 65.625 (65.625)	Acc@5 96.484 (96.484)
Epoch: [52][64/196]	Time 0.058 (0.054)	Data 0.000 (0.005)	Loss 0.9550 (0.8640)	Acc@1 64.453 (69.748)	Acc@5 98.828 (97.722)
Epoch: [52][128/196]	Time 0.058 (0.056)	Data 0.000 (0.003)	Loss 0.8915 (0.8661)	Acc@1 68.750 (69.883)	Acc@5 97.656 (97.662)
Epoch: [52][192/196]	Time 0.052 (0.056)	Data 0.000 (0.002)	Loss 0.8937 (0.8674)	Acc@1 67.578 (69.928)	Acc@5 96.094 (97.610)
after train
test acc: 61.83
Epoche: [53/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [53][0/196]	Time 0.056 (0.056)	Data 0.460 (0.460)	Loss 0.8765 (0.8765)	Acc@1 69.922 (69.922)	Acc@5 99.219 (99.219)
Epoch: [53][64/196]	Time 0.052 (0.047)	Data 0.000 (0.007)	Loss 0.8946 (0.8660)	Acc@1 70.312 (70.072)	Acc@5 96.875 (97.548)
Epoch: [53][128/196]	Time 0.058 (0.047)	Data 0.000 (0.004)	Loss 0.8762 (0.8655)	Acc@1 67.578 (70.040)	Acc@5 98.828 (97.581)
Epoch: [53][192/196]	Time 0.058 (0.051)	Data 0.000 (0.003)	Loss 0.8181 (0.8646)	Acc@1 72.656 (70.114)	Acc@5 96.484 (97.616)
after train
test acc: 48.82
Epoche: [54/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [54][0/196]	Time 0.076 (0.076)	Data 0.314 (0.314)	Loss 0.9237 (0.9237)	Acc@1 69.531 (69.531)	Acc@5 97.656 (97.656)
Epoch: [54][64/196]	Time 0.058 (0.053)	Data 0.000 (0.005)	Loss 0.7906 (0.8524)	Acc@1 72.656 (70.337)	Acc@5 98.047 (97.837)
Epoch: [54][128/196]	Time 0.058 (0.055)	Data 0.000 (0.003)	Loss 0.9572 (0.8572)	Acc@1 69.141 (70.134)	Acc@5 96.094 (97.638)
Epoch: [54][192/196]	Time 0.045 (0.055)	Data 0.000 (0.002)	Loss 0.9611 (0.8594)	Acc@1 69.531 (70.043)	Acc@5 97.266 (97.666)
after train
test acc: 55.81
Epoche: [55/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [55][0/196]	Time 0.071 (0.071)	Data 0.437 (0.437)	Loss 0.7786 (0.7786)	Acc@1 72.656 (72.656)	Acc@5 98.828 (98.828)
Epoch: [55][64/196]	Time 0.058 (0.055)	Data 0.000 (0.007)	Loss 0.9185 (0.8623)	Acc@1 69.141 (70.120)	Acc@5 97.656 (97.584)
Epoch: [55][128/196]	Time 0.058 (0.056)	Data 0.000 (0.004)	Loss 0.8879 (0.8647)	Acc@1 69.531 (70.004)	Acc@5 96.484 (97.605)
Epoch: [55][192/196]	Time 0.045 (0.056)	Data 0.000 (0.003)	Loss 0.7665 (0.8617)	Acc@1 75.391 (70.084)	Acc@5 97.656 (97.666)
after train
test acc: 61.32
Epoche: [56/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [56][0/196]	Time 0.076 (0.076)	Data 0.422 (0.422)	Loss 0.8139 (0.8139)	Acc@1 71.875 (71.875)	Acc@5 98.047 (98.047)
Epoch: [56][64/196]	Time 0.058 (0.055)	Data 0.000 (0.007)	Loss 0.7980 (0.8554)	Acc@1 72.656 (70.583)	Acc@5 98.438 (97.728)
Epoch: [56][128/196]	Time 0.058 (0.056)	Data 0.000 (0.004)	Loss 0.9549 (0.8576)	Acc@1 66.797 (70.412)	Acc@5 98.828 (97.517)
Epoch: [56][192/196]	Time 0.058 (0.055)	Data 0.000 (0.003)	Loss 0.7335 (0.8623)	Acc@1 75.391 (70.195)	Acc@5 98.438 (97.502)
after train
test acc: 46.27
Epoche: [57/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [57][0/196]	Time 0.058 (0.058)	Data 0.430 (0.430)	Loss 0.8358 (0.8358)	Acc@1 71.484 (71.484)	Acc@5 97.266 (97.266)
Epoch: [57][64/196]	Time 0.058 (0.052)	Data 0.000 (0.007)	Loss 0.9176 (0.8775)	Acc@1 67.578 (69.387)	Acc@5 96.875 (97.506)
Epoch: [57][128/196]	Time 0.058 (0.051)	Data 0.000 (0.004)	Loss 0.9602 (0.8642)	Acc@1 67.188 (69.852)	Acc@5 97.656 (97.723)
Epoch: [57][192/196]	Time 0.045 (0.053)	Data 0.000 (0.002)	Loss 0.9329 (0.8676)	Acc@1 63.672 (69.572)	Acc@5 95.703 (97.679)
after train
test acc: 59.28
Epoche: [58/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [58][0/196]	Time 0.057 (0.057)	Data 0.373 (0.373)	Loss 0.8637 (0.8637)	Acc@1 70.312 (70.312)	Acc@5 97.266 (97.266)
Epoch: [58][64/196]	Time 0.044 (0.047)	Data 0.000 (0.006)	Loss 0.8194 (0.8540)	Acc@1 71.094 (70.349)	Acc@5 98.438 (97.590)
Epoch: [58][128/196]	Time 0.046 (0.046)	Data 0.000 (0.003)	Loss 0.9224 (0.8511)	Acc@1 69.531 (70.443)	Acc@5 96.094 (97.623)
Epoch: [58][192/196]	Time 0.058 (0.047)	Data 0.000 (0.002)	Loss 0.8230 (0.8494)	Acc@1 70.703 (70.551)	Acc@5 97.656 (97.618)
after train
test acc: 60.09
Epoche: [59/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [59][0/196]	Time 0.062 (0.062)	Data 0.404 (0.404)	Loss 0.9235 (0.9235)	Acc@1 64.453 (64.453)	Acc@5 96.875 (96.875)
Epoch: [59][64/196]	Time 0.046 (0.048)	Data 0.000 (0.006)	Loss 0.8896 (0.8479)	Acc@1 69.141 (70.192)	Acc@5 96.094 (97.728)
Epoch: [59][128/196]	Time 0.045 (0.049)	Data 0.000 (0.003)	Loss 0.8441 (0.8544)	Acc@1 72.656 (70.034)	Acc@5 97.656 (97.668)
Epoch: [59][192/196]	Time 0.045 (0.050)	Data 0.000 (0.002)	Loss 0.9981 (0.8576)	Acc@1 67.188 (70.060)	Acc@5 95.703 (97.594)
after train
test acc: 25.58
Epoche: [60/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [60][0/196]	Time 0.056 (0.056)	Data 0.380 (0.380)	Loss 0.9283 (0.9283)	Acc@1 67.969 (67.969)	Acc@5 94.922 (94.922)
Epoch: [60][64/196]	Time 0.058 (0.057)	Data 0.000 (0.006)	Loss 0.7721 (0.8516)	Acc@1 73.047 (70.409)	Acc@5 98.047 (97.584)
Epoch: [60][128/196]	Time 0.058 (0.054)	Data 0.000 (0.003)	Loss 0.8127 (0.8599)	Acc@1 71.484 (70.046)	Acc@5 98.047 (97.584)
Epoch: [60][192/196]	Time 0.058 (0.055)	Data 0.000 (0.002)	Loss 0.8010 (0.8545)	Acc@1 71.484 (70.199)	Acc@5 98.438 (97.604)
after train
test acc: 49.36
IndexL: 0
Module= Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); indexL: 1
indexConv: 1
modulelist[indexConv]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 moduleBn: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
indexConv: 2
modulelist[indexConv]: ReLU(inplace=True)
indexConv: 3
modulelist[indexConv]: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
module1: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); indexConv: 3; index: 3
shape new w1: (8, 3, 3, 3)
shape new w2: (4, 8, 3, 3); old w2: (4, 4, 3, 3)
Batchnorm1
IndexL: 3
Module= Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 3
 moduleBn: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 3
Module= Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 4
shape new w1: (8, 8, 3, 3)
shape new w2: (4, 8, 3, 3); old w2: (4, 4, 3, 3)
Batchnorm1
IndexL: 4
Module= Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 4
 moduleBn: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 4
Module= Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 5
shape new w1: (8, 8, 3, 3)
shape new w2: (4, 8, 3, 3); old w2: (4, 4, 3, 3)
Batchnorm1
IndexL: 5
Module= Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 5
 moduleBn: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 5
Module= Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 6
shape new w1: (8, 8, 3, 3)
shape new w2: (8, 8, 3, 3); old w2: (8, 4, 3, 3)
Batchnorm1
IndexL: 6
Module= Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i: 0 indexL: 6
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 6
Module= Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i: 0 indexL: 7
shape new w1: (16, 8, 3, 3)
module: Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Batchnorm2
1: Module: 6; 0; moduleBn: 6; 1; module1: 7; 0
shape new w2: (8, 8, 1, 1)
IndexL: 7
Module= Conv2d(8, 8, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 7
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 7
Module= Conv2d(8, 8, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 8
shape new w1: (16, 8, 1, 1)
shape new w2: (8, 16, 3, 3); old w2: (8, 8, 3, 3)
Batchnorm1
IndexL: 8
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 8
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 8
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 9
shape new w1: (16, 16, 3, 3)
shape new w2: (8, 16, 3, 3); old w2: (8, 8, 3, 3)
Batchnorm1
IndexL: 9
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 9
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 9
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 10
shape new w1: (16, 16, 3, 3)
shape new w2: (16, 16, 3, 3); old w2: (16, 8, 3, 3)
Batchnorm1
IndexL: 10
Module= Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i: 0 indexL: 10
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 10
Module= Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i: 0 indexL: 11
shape new w1: (32, 16, 3, 3)
module: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
Batchnorm2
1: Module: 10; 0; moduleBn: 10; 1; module1: 11; 0
shape new w2: (16, 16, 1, 1)
IndexL: 11
Module= Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 11
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 11
Module= Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 12
shape new w1: (32, 16, 1, 1)
shape new w2: (16, 32, 3, 3); old w2: (16, 16, 3, 3)
Batchnorm1
IndexL: 12
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 12
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 12
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 13
shape new w1: (32, 32, 3, 3)
shape new w2: (16, 32, 3, 3); old w2: (16, 16, 3, 3)
Batchnorm1
IndexL: 13
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 13
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 13
shape new w1: (32, 32, 3, 3)
shape new w2: (10, 32); old w2: (10, 16)
Batchnorm1
self: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (7): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (11): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): AdaptiveAvgPool2d(output_size=(1, 1))
    (15): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
module: Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
Size of Weight: torch.Size([8, 3, 3, 3])
module: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Size of Weight: torch.Size([8])
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
Size of Weight: torch.Size([16, 8, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 8, 1, 1])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
Sequential: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
Size of Weight: torch.Size([32, 16, 3, 3])
Size of Weight: torch.Size([32])
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([32, 16, 1, 1])
Size of Weight: torch.Size([32])
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([32, 32, 3, 3])
Size of Weight: torch.Size([32])
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([32, 32, 3, 3])
Size of Weight: torch.Size([32])
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=32, out_features=10, bias=True)
Size of Weight: torch.Size([10, 32])
time for n2n: 0.04080820083618164
Epoche: [61/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [61][0/196]	Time 0.072 (0.072)	Data 0.440 (0.440)	Loss 2.4856 (2.4856)	Acc@1 21.094 (21.094)	Acc@5 77.734 (77.734)
Epoch: [61][64/196]	Time 0.058 (0.059)	Data 0.000 (0.007)	Loss 1.0765 (1.0618)	Acc@1 66.797 (63.413)	Acc@5 93.359 (95.913)
Epoch: [61][128/196]	Time 0.057 (0.060)	Data 0.000 (0.004)	Loss 0.8358 (0.9706)	Acc@1 70.703 (66.397)	Acc@5 98.047 (96.784)
Epoch: [61][192/196]	Time 0.058 (0.060)	Data 0.000 (0.003)	Loss 0.7146 (0.9280)	Acc@1 76.172 (67.722)	Acc@5 99.219 (97.177)
after train
test acc: 50.26
Epoche: [62/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [62][0/196]	Time 0.080 (0.080)	Data 0.427 (0.427)	Loss 0.8211 (0.8211)	Acc@1 70.703 (70.703)	Acc@5 98.047 (98.047)
Epoch: [62][64/196]	Time 0.058 (0.058)	Data 0.000 (0.007)	Loss 0.9157 (0.8281)	Acc@1 68.750 (71.094)	Acc@5 97.656 (98.011)
Epoch: [62][128/196]	Time 0.058 (0.058)	Data 0.000 (0.004)	Loss 0.8734 (0.8333)	Acc@1 69.922 (70.988)	Acc@5 98.438 (97.838)
Epoch: [62][192/196]	Time 0.058 (0.058)	Data 0.000 (0.002)	Loss 0.8919 (0.8260)	Acc@1 71.875 (71.239)	Acc@5 95.312 (97.832)
after train
test acc: 69.43
Epoche: [63/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [63][0/196]	Time 0.072 (0.072)	Data 0.459 (0.459)	Loss 0.7664 (0.7664)	Acc@1 74.609 (74.609)	Acc@5 96.875 (96.875)
Epoch: [63][64/196]	Time 0.068 (0.061)	Data 0.000 (0.007)	Loss 0.6709 (0.7918)	Acc@1 78.516 (72.608)	Acc@5 98.828 (97.867)
Epoch: [63][128/196]	Time 0.068 (0.063)	Data 0.000 (0.004)	Loss 0.6781 (0.7928)	Acc@1 75.000 (72.587)	Acc@5 98.438 (97.953)
Epoch: [63][192/196]	Time 0.058 (0.062)	Data 0.000 (0.003)	Loss 0.6542 (0.7949)	Acc@1 79.688 (72.559)	Acc@5 98.828 (97.954)
after train
test acc: 68.57
Epoche: [64/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [64][0/196]	Time 0.084 (0.084)	Data 0.438 (0.438)	Loss 0.7301 (0.7301)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [64][64/196]	Time 0.067 (0.061)	Data 0.000 (0.007)	Loss 0.7613 (0.7863)	Acc@1 74.609 (72.650)	Acc@5 98.438 (98.023)
Epoch: [64][128/196]	Time 0.067 (0.063)	Data 0.000 (0.004)	Loss 0.7635 (0.7749)	Acc@1 74.609 (73.162)	Acc@5 98.438 (98.077)
Epoch: [64][192/196]	Time 0.068 (0.064)	Data 0.000 (0.003)	Loss 0.7972 (0.7790)	Acc@1 70.703 (73.073)	Acc@5 99.219 (98.029)
after train
test acc: 66.66
Epoche: [65/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [65][0/196]	Time 0.074 (0.074)	Data 0.486 (0.486)	Loss 0.7809 (0.7809)	Acc@1 74.609 (74.609)	Acc@5 98.438 (98.438)
Epoch: [65][64/196]	Time 0.068 (0.062)	Data 0.000 (0.008)	Loss 0.8877 (0.7645)	Acc@1 69.141 (73.474)	Acc@5 97.656 (98.071)
Epoch: [65][128/196]	Time 0.058 (0.063)	Data 0.000 (0.004)	Loss 0.6375 (0.7663)	Acc@1 80.469 (73.338)	Acc@5 99.219 (98.138)
Epoch: [65][192/196]	Time 0.058 (0.061)	Data 0.000 (0.003)	Loss 0.8210 (0.7664)	Acc@1 71.875 (73.381)	Acc@5 98.438 (98.146)
after train
test acc: 69.25
Epoche: [66/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [66][0/196]	Time 0.101 (0.101)	Data 0.439 (0.439)	Loss 0.7038 (0.7038)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [66][64/196]	Time 0.058 (0.061)	Data 0.000 (0.007)	Loss 0.7843 (0.7668)	Acc@1 70.312 (73.269)	Acc@5 98.047 (97.927)
Epoch: [66][128/196]	Time 0.058 (0.061)	Data 0.000 (0.004)	Loss 0.8483 (0.7584)	Acc@1 71.484 (73.601)	Acc@5 97.266 (98.071)
Epoch: [66][192/196]	Time 0.058 (0.061)	Data 0.000 (0.003)	Loss 0.7293 (0.7580)	Acc@1 75.781 (73.717)	Acc@5 97.656 (98.097)
after train
test acc: 49.63
Epoche: [67/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [67][0/196]	Time 0.070 (0.070)	Data 0.498 (0.498)	Loss 0.6684 (0.6684)	Acc@1 76.953 (76.953)	Acc@5 98.828 (98.828)
Epoch: [67][64/196]	Time 0.058 (0.060)	Data 0.000 (0.008)	Loss 0.7505 (0.7480)	Acc@1 73.828 (73.936)	Acc@5 98.047 (98.389)
Epoch: [67][128/196]	Time 0.068 (0.061)	Data 0.000 (0.004)	Loss 0.7953 (0.7381)	Acc@1 71.484 (74.325)	Acc@5 98.438 (98.425)
Epoch: [67][192/196]	Time 0.058 (0.062)	Data 0.000 (0.003)	Loss 0.8193 (0.7425)	Acc@1 69.922 (74.233)	Acc@5 98.047 (98.280)
after train
test acc: 67.26
Epoche: [68/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [68][0/196]	Time 0.076 (0.076)	Data 0.331 (0.331)	Loss 0.7224 (0.7224)	Acc@1 73.828 (73.828)	Acc@5 99.219 (99.219)
Epoch: [68][64/196]	Time 0.058 (0.060)	Data 0.000 (0.005)	Loss 0.7155 (0.7296)	Acc@1 76.562 (74.627)	Acc@5 98.438 (98.185)
Epoch: [68][128/196]	Time 0.057 (0.059)	Data 0.000 (0.003)	Loss 0.8181 (0.7367)	Acc@1 69.922 (74.452)	Acc@5 98.438 (98.232)
Epoch: [68][192/196]	Time 0.058 (0.058)	Data 0.000 (0.002)	Loss 0.7562 (0.7379)	Acc@1 72.656 (74.346)	Acc@5 98.047 (98.237)
after train
test acc: 66.43
Epoche: [69/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [69][0/196]	Time 0.092 (0.092)	Data 0.429 (0.429)	Loss 0.6895 (0.6895)	Acc@1 77.734 (77.734)	Acc@5 97.656 (97.656)
Epoch: [69][64/196]	Time 0.058 (0.063)	Data 0.000 (0.007)	Loss 0.8754 (0.7380)	Acc@1 70.703 (74.549)	Acc@5 95.312 (98.245)
Epoch: [69][128/196]	Time 0.057 (0.061)	Data 0.000 (0.004)	Loss 0.6105 (0.7300)	Acc@1 79.297 (74.833)	Acc@5 100.000 (98.250)
Epoch: [69][192/196]	Time 0.058 (0.060)	Data 0.000 (0.002)	Loss 0.6583 (0.7295)	Acc@1 77.344 (74.747)	Acc@5 98.828 (98.235)
after train
test acc: 58.15
Epoche: [70/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [70][0/196]	Time 0.098 (0.098)	Data 0.418 (0.418)	Loss 0.6943 (0.6943)	Acc@1 77.734 (77.734)	Acc@5 98.828 (98.828)
Epoch: [70][64/196]	Time 0.058 (0.059)	Data 0.000 (0.007)	Loss 0.6361 (0.7068)	Acc@1 76.953 (75.595)	Acc@5 98.828 (98.401)
Epoch: [70][128/196]	Time 0.058 (0.060)	Data 0.000 (0.003)	Loss 0.7767 (0.7120)	Acc@1 73.828 (75.194)	Acc@5 98.047 (98.347)
Epoch: [70][192/196]	Time 0.068 (0.062)	Data 0.000 (0.002)	Loss 0.6990 (0.7099)	Acc@1 76.953 (75.385)	Acc@5 96.875 (98.405)
after train
test acc: 70.56
Epoche: [71/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [71][0/196]	Time 0.109 (0.109)	Data 0.442 (0.442)	Loss 0.6623 (0.6623)	Acc@1 78.906 (78.906)	Acc@5 98.828 (98.828)
Epoch: [71][64/196]	Time 0.067 (0.062)	Data 0.000 (0.007)	Loss 0.6637 (0.7028)	Acc@1 75.391 (75.595)	Acc@5 99.219 (98.431)
Epoch: [71][128/196]	Time 0.068 (0.061)	Data 0.000 (0.004)	Loss 0.7899 (0.7126)	Acc@1 73.828 (75.509)	Acc@5 97.266 (98.344)
Epoch: [71][192/196]	Time 0.058 (0.060)	Data 0.000 (0.003)	Loss 0.6884 (0.7066)	Acc@1 76.562 (75.648)	Acc@5 97.266 (98.393)
after train
test acc: 64.56
Epoche: [72/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [72][0/196]	Time 0.071 (0.071)	Data 0.520 (0.520)	Loss 0.7318 (0.7318)	Acc@1 72.656 (72.656)	Acc@5 98.047 (98.047)
Epoch: [72][64/196]	Time 0.058 (0.058)	Data 0.000 (0.008)	Loss 0.7571 (0.6973)	Acc@1 71.875 (75.559)	Acc@5 98.438 (98.456)
Epoch: [72][128/196]	Time 0.058 (0.058)	Data 0.000 (0.004)	Loss 0.6304 (0.6997)	Acc@1 77.734 (75.512)	Acc@5 98.438 (98.489)
Epoch: [72][192/196]	Time 0.058 (0.058)	Data 0.000 (0.003)	Loss 0.6932 (0.7021)	Acc@1 73.438 (75.457)	Acc@5 100.000 (98.468)
after train
test acc: 63.65
Epoche: [73/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [73][0/196]	Time 0.081 (0.081)	Data 0.345 (0.345)	Loss 0.5647 (0.5647)	Acc@1 81.641 (81.641)	Acc@5 98.438 (98.438)
Epoch: [73][64/196]	Time 0.067 (0.067)	Data 0.000 (0.006)	Loss 0.7626 (0.6906)	Acc@1 69.141 (75.835)	Acc@5 99.219 (98.528)
Epoch: [73][128/196]	Time 0.057 (0.065)	Data 0.000 (0.003)	Loss 0.7593 (0.6999)	Acc@1 74.609 (75.678)	Acc@5 97.266 (98.516)
Epoch: [73][192/196]	Time 0.058 (0.062)	Data 0.000 (0.002)	Loss 0.6435 (0.7040)	Acc@1 76.953 (75.567)	Acc@5 99.219 (98.452)
after train
test acc: 53.0
Epoche: [74/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [74][0/196]	Time 0.097 (0.097)	Data 0.444 (0.444)	Loss 0.6532 (0.6532)	Acc@1 76.172 (76.172)	Acc@5 98.438 (98.438)
Epoch: [74][64/196]	Time 0.068 (0.062)	Data 0.000 (0.007)	Loss 0.7254 (0.6935)	Acc@1 71.484 (75.721)	Acc@5 99.609 (98.576)
Epoch: [74][128/196]	Time 0.058 (0.062)	Data 0.000 (0.004)	Loss 0.6550 (0.6874)	Acc@1 77.344 (75.942)	Acc@5 98.438 (98.534)
Epoch: [74][192/196]	Time 0.058 (0.062)	Data 0.000 (0.003)	Loss 0.7531 (0.6935)	Acc@1 75.000 (75.842)	Acc@5 97.656 (98.539)
after train
test acc: 65.55
Epoche: [75/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [75][0/196]	Time 0.101 (0.101)	Data 0.444 (0.444)	Loss 0.6910 (0.6910)	Acc@1 76.172 (76.172)	Acc@5 98.828 (98.828)
Epoch: [75][64/196]	Time 0.068 (0.062)	Data 0.000 (0.007)	Loss 0.6163 (0.6863)	Acc@1 75.781 (76.214)	Acc@5 99.219 (98.630)
Epoch: [75][128/196]	Time 0.068 (0.065)	Data 0.000 (0.004)	Loss 0.6582 (0.6781)	Acc@1 78.906 (76.614)	Acc@5 98.828 (98.662)
Epoch: [75][192/196]	Time 0.058 (0.064)	Data 0.000 (0.003)	Loss 0.6433 (0.6816)	Acc@1 77.344 (76.443)	Acc@5 99.609 (98.581)
after train
test acc: 68.64
Epoche: [76/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [76][0/196]	Time 0.070 (0.070)	Data 0.501 (0.501)	Loss 0.6246 (0.6246)	Acc@1 77.344 (77.344)	Acc@5 100.000 (100.000)
Epoch: [76][64/196]	Time 0.058 (0.058)	Data 0.000 (0.008)	Loss 0.6046 (0.6856)	Acc@1 76.562 (76.412)	Acc@5 99.219 (98.504)
Epoch: [76][128/196]	Time 0.058 (0.058)	Data 0.000 (0.004)	Loss 0.6633 (0.6790)	Acc@1 76.562 (76.547)	Acc@5 98.828 (98.459)
Epoch: [76][192/196]	Time 0.058 (0.058)	Data 0.000 (0.003)	Loss 0.6785 (0.6827)	Acc@1 75.000 (76.338)	Acc@5 98.828 (98.411)
after train
test acc: 70.06
Epoche: [77/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [77][0/196]	Time 0.086 (0.086)	Data 0.438 (0.438)	Loss 0.8112 (0.8112)	Acc@1 72.266 (72.266)	Acc@5 98.438 (98.438)
Epoch: [77][64/196]	Time 0.057 (0.059)	Data 0.000 (0.007)	Loss 0.7570 (0.6717)	Acc@1 73.438 (76.725)	Acc@5 98.047 (98.666)
Epoch: [77][128/196]	Time 0.058 (0.062)	Data 0.000 (0.004)	Loss 0.7096 (0.6762)	Acc@1 76.953 (76.656)	Acc@5 98.438 (98.607)
Epoch: [77][192/196]	Time 0.058 (0.062)	Data 0.000 (0.003)	Loss 0.6636 (0.6737)	Acc@1 77.344 (76.830)	Acc@5 98.438 (98.589)
after train
test acc: 60.42
Epoche: [78/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [78][0/196]	Time 0.093 (0.093)	Data 0.426 (0.426)	Loss 0.6091 (0.6091)	Acc@1 78.125 (78.125)	Acc@5 98.828 (98.828)
Epoch: [78][64/196]	Time 0.057 (0.059)	Data 0.000 (0.007)	Loss 0.6889 (0.6743)	Acc@1 77.734 (76.562)	Acc@5 98.047 (98.642)
Epoch: [78][128/196]	Time 0.057 (0.058)	Data 0.000 (0.004)	Loss 0.6413 (0.6732)	Acc@1 75.391 (76.617)	Acc@5 98.828 (98.559)
Epoch: [78][192/196]	Time 0.075 (0.060)	Data 0.000 (0.002)	Loss 0.6926 (0.6778)	Acc@1 74.609 (76.350)	Acc@5 98.438 (98.581)
after train
test acc: 62.47
Epoche: [79/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [79][0/196]	Time 0.078 (0.078)	Data 0.296 (0.296)	Loss 0.6550 (0.6550)	Acc@1 75.000 (75.000)	Acc@5 98.828 (98.828)
Epoch: [79][64/196]	Time 0.058 (0.060)	Data 0.000 (0.005)	Loss 0.6653 (0.6763)	Acc@1 75.781 (76.599)	Acc@5 99.609 (98.546)
Epoch: [79][128/196]	Time 0.058 (0.059)	Data 0.000 (0.003)	Loss 0.6477 (0.6713)	Acc@1 75.391 (77.056)	Acc@5 98.047 (98.501)
Epoch: [79][192/196]	Time 0.058 (0.058)	Data 0.000 (0.002)	Loss 0.6962 (0.6697)	Acc@1 73.438 (77.018)	Acc@5 99.219 (98.541)
after train
test acc: 67.62
Epoche: [80/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [80][0/196]	Time 0.100 (0.100)	Data 0.413 (0.413)	Loss 0.6356 (0.6356)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [80][64/196]	Time 0.058 (0.059)	Data 0.000 (0.007)	Loss 0.6623 (0.6683)	Acc@1 77.734 (76.671)	Acc@5 98.047 (98.636)
Epoch: [80][128/196]	Time 0.058 (0.058)	Data 0.000 (0.003)	Loss 0.6360 (0.6731)	Acc@1 78.516 (76.675)	Acc@5 98.828 (98.553)
Epoch: [80][192/196]	Time 0.058 (0.058)	Data 0.000 (0.002)	Loss 0.6359 (0.6690)	Acc@1 79.688 (76.767)	Acc@5 98.828 (98.614)
after train
test acc: 74.49
Epoche: [81/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [81][0/196]	Time 0.094 (0.094)	Data 0.435 (0.435)	Loss 0.5895 (0.5895)	Acc@1 78.516 (78.516)	Acc@5 99.219 (99.219)
Epoch: [81][64/196]	Time 0.068 (0.065)	Data 0.000 (0.007)	Loss 0.7684 (0.6416)	Acc@1 73.828 (77.740)	Acc@5 97.656 (98.840)
Epoch: [81][128/196]	Time 0.069 (0.063)	Data 0.000 (0.004)	Loss 0.6518 (0.6510)	Acc@1 79.688 (77.556)	Acc@5 99.219 (98.743)
Epoch: [81][192/196]	Time 0.058 (0.063)	Data 0.000 (0.002)	Loss 0.7254 (0.6606)	Acc@1 75.000 (77.206)	Acc@5 98.047 (98.654)
after train
test acc: 59.89
Epoche: [82/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [82][0/196]	Time 0.094 (0.094)	Data 0.517 (0.517)	Loss 0.6383 (0.6383)	Acc@1 80.469 (80.469)	Acc@5 98.047 (98.047)
Epoch: [82][64/196]	Time 0.058 (0.060)	Data 0.000 (0.008)	Loss 0.7351 (0.6546)	Acc@1 76.172 (77.320)	Acc@5 96.484 (98.546)
Epoch: [82][128/196]	Time 0.067 (0.061)	Data 0.000 (0.004)	Loss 0.6027 (0.6587)	Acc@1 81.641 (77.323)	Acc@5 99.219 (98.610)
Epoch: [82][192/196]	Time 0.067 (0.063)	Data 0.000 (0.003)	Loss 0.6997 (0.6574)	Acc@1 75.391 (77.309)	Acc@5 98.828 (98.612)
after train
test acc: 70.34
Epoche: [83/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [83][0/196]	Time 0.094 (0.094)	Data 0.444 (0.444)	Loss 0.6013 (0.6013)	Acc@1 77.344 (77.344)	Acc@5 98.828 (98.828)
Epoch: [83][64/196]	Time 0.058 (0.064)	Data 0.000 (0.007)	Loss 0.5809 (0.6420)	Acc@1 79.297 (78.101)	Acc@5 98.828 (98.660)
Epoch: [83][128/196]	Time 0.058 (0.061)	Data 0.000 (0.004)	Loss 0.7009 (0.6560)	Acc@1 76.172 (77.286)	Acc@5 98.047 (98.625)
Epoch: [83][192/196]	Time 0.058 (0.060)	Data 0.000 (0.003)	Loss 0.5882 (0.6545)	Acc@1 80.859 (77.287)	Acc@5 98.828 (98.640)
after train
test acc: 64.31
Epoche: [84/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [84][0/196]	Time 0.077 (0.077)	Data 0.429 (0.429)	Loss 0.5847 (0.5847)	Acc@1 78.516 (78.516)	Acc@5 99.609 (99.609)
Epoch: [84][64/196]	Time 0.058 (0.059)	Data 0.000 (0.007)	Loss 0.6163 (0.6462)	Acc@1 79.297 (77.151)	Acc@5 98.438 (98.804)
Epoch: [84][128/196]	Time 0.058 (0.058)	Data 0.000 (0.004)	Loss 0.6515 (0.6565)	Acc@1 79.688 (77.132)	Acc@5 98.438 (98.731)
Epoch: [84][192/196]	Time 0.058 (0.058)	Data 0.000 (0.002)	Loss 0.5769 (0.6521)	Acc@1 81.641 (77.285)	Acc@5 98.438 (98.693)
after train
test acc: 63.05
Epoche: [85/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [85][0/196]	Time 0.070 (0.070)	Data 0.503 (0.503)	Loss 0.6318 (0.6318)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [85][64/196]	Time 0.058 (0.061)	Data 0.000 (0.008)	Loss 0.7080 (0.6541)	Acc@1 73.828 (77.013)	Acc@5 97.656 (98.732)
Epoch: [85][128/196]	Time 0.076 (0.060)	Data 0.000 (0.004)	Loss 0.7067 (0.6529)	Acc@1 77.344 (77.286)	Acc@5 98.047 (98.665)
Epoch: [85][192/196]	Time 0.058 (0.061)	Data 0.000 (0.003)	Loss 0.6617 (0.6482)	Acc@1 77.734 (77.419)	Acc@5 99.219 (98.703)
after train
test acc: 65.7
Epoche: [86/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [86][0/196]	Time 0.088 (0.088)	Data 0.439 (0.439)	Loss 0.6530 (0.6530)	Acc@1 78.906 (78.906)	Acc@5 97.266 (97.266)
Epoch: [86][64/196]	Time 0.076 (0.076)	Data 0.000 (0.007)	Loss 0.6036 (0.6247)	Acc@1 81.250 (78.570)	Acc@5 99.219 (98.768)
Epoch: [86][128/196]	Time 0.057 (0.068)	Data 0.000 (0.004)	Loss 0.7199 (0.6446)	Acc@1 76.172 (77.971)	Acc@5 98.828 (98.634)
Epoch: [86][192/196]	Time 0.068 (0.067)	Data 0.000 (0.003)	Loss 0.6375 (0.6450)	Acc@1 76.172 (77.803)	Acc@5 97.656 (98.624)
after train
test acc: 67.9
Epoche: [87/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [87][0/196]	Time 0.092 (0.092)	Data 0.443 (0.443)	Loss 0.6175 (0.6175)	Acc@1 78.516 (78.516)	Acc@5 97.656 (97.656)
Epoch: [87][64/196]	Time 0.067 (0.062)	Data 0.000 (0.007)	Loss 0.6206 (0.6306)	Acc@1 76.562 (77.969)	Acc@5 98.828 (98.774)
Epoch: [87][128/196]	Time 0.067 (0.061)	Data 0.000 (0.004)	Loss 0.5894 (0.6419)	Acc@1 76.953 (77.722)	Acc@5 99.219 (98.734)
Epoch: [87][192/196]	Time 0.058 (0.061)	Data 0.000 (0.003)	Loss 0.7667 (0.6464)	Acc@1 73.828 (77.581)	Acc@5 97.266 (98.727)
after train
test acc: 62.67
Epoche: [88/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [88][0/196]	Time 0.111 (0.111)	Data 0.514 (0.514)	Loss 0.5918 (0.5918)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [88][64/196]	Time 0.058 (0.062)	Data 0.000 (0.008)	Loss 0.4957 (0.6436)	Acc@1 81.250 (77.879)	Acc@5 100.000 (98.750)
Epoch: [88][128/196]	Time 0.057 (0.060)	Data 0.000 (0.004)	Loss 0.7416 (0.6444)	Acc@1 75.391 (77.831)	Acc@5 97.266 (98.704)
Epoch: [88][192/196]	Time 0.058 (0.059)	Data 0.000 (0.003)	Loss 0.5657 (0.6380)	Acc@1 79.297 (77.989)	Acc@5 98.438 (98.715)
after train
test acc: 70.11
Epoche: [89/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [89][0/196]	Time 0.109 (0.109)	Data 0.427 (0.427)	Loss 0.5856 (0.5856)	Acc@1 80.859 (80.859)	Acc@5 98.828 (98.828)
Epoch: [89][64/196]	Time 0.058 (0.067)	Data 0.000 (0.007)	Loss 0.6643 (0.6381)	Acc@1 76.172 (78.035)	Acc@5 97.656 (98.630)
Epoch: [89][128/196]	Time 0.058 (0.062)	Data 0.000 (0.004)	Loss 0.6766 (0.6429)	Acc@1 77.344 (77.986)	Acc@5 98.438 (98.634)
Epoch: [89][192/196]	Time 0.068 (0.062)	Data 0.000 (0.002)	Loss 0.6838 (0.6449)	Acc@1 78.125 (77.860)	Acc@5 98.828 (98.680)
after train
test acc: 65.73
Epoche: [90/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [90][0/196]	Time 0.115 (0.115)	Data 0.444 (0.444)	Loss 0.5767 (0.5767)	Acc@1 81.641 (81.641)	Acc@5 99.219 (99.219)
Epoch: [90][64/196]	Time 0.057 (0.059)	Data 0.000 (0.007)	Loss 0.6726 (0.6486)	Acc@1 76.562 (77.632)	Acc@5 98.438 (98.696)
Epoch: [90][128/196]	Time 0.057 (0.058)	Data 0.000 (0.004)	Loss 0.6635 (0.6400)	Acc@1 75.781 (77.892)	Acc@5 99.219 (98.710)
Epoch: [90][192/196]	Time 0.058 (0.058)	Data 0.000 (0.003)	Loss 0.5832 (0.6392)	Acc@1 78.125 (77.963)	Acc@5 99.219 (98.666)
after train
test acc: 72.48
Epoche: [91/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [91][0/196]	Time 0.082 (0.082)	Data 0.476 (0.476)	Loss 0.6809 (0.6809)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [91][64/196]	Time 0.058 (0.061)	Data 0.000 (0.008)	Loss 0.5234 (0.6503)	Acc@1 84.766 (77.428)	Acc@5 98.047 (98.804)
Epoch: [91][128/196]	Time 0.058 (0.059)	Data 0.000 (0.004)	Loss 0.6014 (0.6378)	Acc@1 81.250 (77.904)	Acc@5 99.219 (98.849)
Epoch: [91][192/196]	Time 0.058 (0.059)	Data 0.000 (0.003)	Loss 0.6103 (0.6382)	Acc@1 76.953 (77.827)	Acc@5 99.609 (98.769)
after train
test acc: 68.78
Epoche: [92/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [92][0/196]	Time 0.094 (0.094)	Data 0.428 (0.428)	Loss 0.5616 (0.5616)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [92][64/196]	Time 0.057 (0.062)	Data 0.000 (0.007)	Loss 0.5852 (0.6205)	Acc@1 80.078 (78.786)	Acc@5 98.828 (98.684)
Epoch: [92][128/196]	Time 0.058 (0.060)	Data 0.000 (0.004)	Loss 0.7298 (0.6362)	Acc@1 76.953 (78.176)	Acc@5 97.656 (98.659)
Epoch: [92][192/196]	Time 0.068 (0.060)	Data 0.000 (0.002)	Loss 0.6647 (0.6408)	Acc@1 79.688 (77.898)	Acc@5 99.219 (98.688)
after train
test acc: 57.27
Epoche: [93/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [93][0/196]	Time 0.107 (0.107)	Data 0.417 (0.417)	Loss 0.6915 (0.6915)	Acc@1 76.953 (76.953)	Acc@5 96.875 (96.875)
Epoch: [93][64/196]	Time 0.057 (0.060)	Data 0.000 (0.007)	Loss 0.6633 (0.6487)	Acc@1 79.297 (77.716)	Acc@5 98.828 (98.678)
Epoch: [93][128/196]	Time 0.058 (0.059)	Data 0.000 (0.003)	Loss 0.6934 (0.6448)	Acc@1 77.344 (77.771)	Acc@5 99.219 (98.689)
Epoch: [93][192/196]	Time 0.058 (0.062)	Data 0.000 (0.002)	Loss 0.6309 (0.6420)	Acc@1 75.391 (77.844)	Acc@5 99.219 (98.727)
after train
test acc: 64.39
Epoche: [94/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [94][0/196]	Time 0.068 (0.068)	Data 0.511 (0.511)	Loss 0.7226 (0.7226)	Acc@1 77.734 (77.734)	Acc@5 98.047 (98.047)
Epoch: [94][64/196]	Time 0.057 (0.058)	Data 0.000 (0.008)	Loss 0.6773 (0.6423)	Acc@1 78.516 (77.776)	Acc@5 98.828 (98.744)
Epoch: [94][128/196]	Time 0.068 (0.063)	Data 0.000 (0.004)	Loss 0.7154 (0.6350)	Acc@1 77.734 (78.113)	Acc@5 98.828 (98.789)
Epoch: [94][192/196]	Time 0.058 (0.062)	Data 0.000 (0.003)	Loss 0.6244 (0.6355)	Acc@1 79.688 (78.210)	Acc@5 98.828 (98.739)
after train
test acc: 59.2
Epoche: [95/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [95][0/196]	Time 0.118 (0.118)	Data 0.403 (0.403)	Loss 0.5260 (0.5260)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [95][64/196]	Time 0.059 (0.067)	Data 0.000 (0.006)	Loss 0.5639 (0.6268)	Acc@1 82.422 (78.444)	Acc@5 99.609 (98.852)
Epoch: [95][128/196]	Time 0.063 (0.063)	Data 0.000 (0.003)	Loss 0.7251 (0.6273)	Acc@1 76.172 (78.334)	Acc@5 98.828 (98.858)
Epoch: [95][192/196]	Time 0.058 (0.062)	Data 0.000 (0.002)	Loss 0.6872 (0.6289)	Acc@1 76.562 (78.289)	Acc@5 99.219 (98.820)
after train
test acc: 67.46
Epoche: [96/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [96][0/196]	Time 0.099 (0.099)	Data 0.439 (0.439)	Loss 0.6102 (0.6102)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [96][64/196]	Time 0.067 (0.065)	Data 0.000 (0.007)	Loss 0.5470 (0.6313)	Acc@1 79.297 (78.035)	Acc@5 99.219 (98.774)
Epoch: [96][128/196]	Time 0.067 (0.066)	Data 0.000 (0.004)	Loss 0.5898 (0.6320)	Acc@1 80.078 (78.164)	Acc@5 100.000 (98.762)
Epoch: [96][192/196]	Time 0.068 (0.066)	Data 0.000 (0.003)	Loss 0.6524 (0.6304)	Acc@1 76.562 (78.267)	Acc@5 99.219 (98.763)
after train
test acc: 67.25
Epoche: [97/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [97][0/196]	Time 0.108 (0.108)	Data 0.422 (0.422)	Loss 0.5107 (0.5107)	Acc@1 85.156 (85.156)	Acc@5 98.438 (98.438)
Epoch: [97][64/196]	Time 0.058 (0.062)	Data 0.000 (0.007)	Loss 0.6169 (0.6415)	Acc@1 78.906 (77.566)	Acc@5 99.219 (98.666)
Epoch: [97][128/196]	Time 0.058 (0.061)	Data 0.000 (0.004)	Loss 0.6401 (0.6304)	Acc@1 78.906 (77.886)	Acc@5 99.219 (98.710)
Epoch: [97][192/196]	Time 0.058 (0.060)	Data 0.000 (0.002)	Loss 0.6476 (0.6262)	Acc@1 79.297 (78.137)	Acc@5 99.609 (98.711)
after train
test acc: 66.19
Epoche: [98/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [98][0/196]	Time 0.072 (0.072)	Data 0.444 (0.444)	Loss 0.5457 (0.5457)	Acc@1 80.469 (80.469)	Acc@5 100.000 (100.000)
Epoch: [98][64/196]	Time 0.076 (0.069)	Data 0.000 (0.007)	Loss 0.6704 (0.6295)	Acc@1 76.172 (78.257)	Acc@5 98.828 (98.846)
Epoch: [98][128/196]	Time 0.058 (0.066)	Data 0.000 (0.004)	Loss 0.6189 (0.6276)	Acc@1 78.516 (78.349)	Acc@5 98.047 (98.762)
Epoch: [98][192/196]	Time 0.058 (0.065)	Data 0.000 (0.003)	Loss 0.5378 (0.6286)	Acc@1 80.469 (78.350)	Acc@5 98.438 (98.723)
after train
test acc: 56.77
Epoche: [99/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [99][0/196]	Time 0.083 (0.083)	Data 0.445 (0.445)	Loss 0.7218 (0.7218)	Acc@1 75.391 (75.391)	Acc@5 98.047 (98.047)
Epoch: [99][64/196]	Time 0.057 (0.059)	Data 0.000 (0.007)	Loss 0.5729 (0.6188)	Acc@1 79.688 (78.660)	Acc@5 100.000 (98.846)
Epoch: [99][128/196]	Time 0.075 (0.060)	Data 0.000 (0.004)	Loss 0.6133 (0.6217)	Acc@1 80.078 (78.603)	Acc@5 100.000 (98.746)
Epoch: [99][192/196]	Time 0.058 (0.063)	Data 0.000 (0.003)	Loss 0.7398 (0.6259)	Acc@1 73.047 (78.524)	Acc@5 98.047 (98.719)
after train
test acc: 53.76
Epoche: [100/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [100][0/196]	Time 0.095 (0.095)	Data 0.438 (0.438)	Loss 0.6556 (0.6556)	Acc@1 78.516 (78.516)	Acc@5 98.047 (98.047)
Epoch: [100][64/196]	Time 0.058 (0.060)	Data 0.000 (0.007)	Loss 0.6237 (0.6269)	Acc@1 79.688 (78.149)	Acc@5 99.609 (98.738)
Epoch: [100][128/196]	Time 0.058 (0.061)	Data 0.000 (0.004)	Loss 0.5319 (0.6302)	Acc@1 81.250 (78.240)	Acc@5 99.219 (98.743)
Epoch: [100][192/196]	Time 0.058 (0.060)	Data 0.000 (0.003)	Loss 0.6647 (0.6309)	Acc@1 76.172 (78.141)	Acc@5 99.219 (98.751)
after train
test acc: 70.3
Epoche: [101/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [101][0/196]	Time 0.086 (0.086)	Data 0.441 (0.441)	Loss 0.6266 (0.6266)	Acc@1 77.344 (77.344)	Acc@5 98.828 (98.828)
Epoch: [101][64/196]	Time 0.058 (0.059)	Data 0.000 (0.007)	Loss 0.7224 (0.6248)	Acc@1 74.609 (78.317)	Acc@5 98.438 (98.804)
Epoch: [101][128/196]	Time 0.058 (0.058)	Data 0.000 (0.004)	Loss 0.7645 (0.6286)	Acc@1 70.312 (78.422)	Acc@5 97.656 (98.780)
Epoch: [101][192/196]	Time 0.058 (0.060)	Data 0.000 (0.003)	Loss 0.6434 (0.6265)	Acc@1 76.953 (78.447)	Acc@5 98.438 (98.780)
after train
test acc: 69.53
Epoche: [102/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [102][0/196]	Time 0.094 (0.094)	Data 0.434 (0.434)	Loss 0.5323 (0.5323)	Acc@1 82.422 (82.422)	Acc@5 99.609 (99.609)
Epoch: [102][64/196]	Time 0.058 (0.062)	Data 0.000 (0.007)	Loss 0.7409 (0.6292)	Acc@1 76.172 (77.951)	Acc@5 98.047 (98.792)
Epoch: [102][128/196]	Time 0.057 (0.061)	Data 0.000 (0.004)	Loss 0.6831 (0.6281)	Acc@1 77.344 (78.170)	Acc@5 97.656 (98.813)
Epoch: [102][192/196]	Time 0.058 (0.061)	Data 0.000 (0.002)	Loss 0.5656 (0.6266)	Acc@1 81.641 (78.303)	Acc@5 99.219 (98.812)
after train
test acc: 61.01
Epoche: [103/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [103][0/196]	Time 0.096 (0.096)	Data 0.455 (0.455)	Loss 0.5876 (0.5876)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [103][64/196]	Time 0.058 (0.059)	Data 0.000 (0.007)	Loss 0.6519 (0.6138)	Acc@1 76.172 (78.960)	Acc@5 99.609 (98.762)
Epoch: [103][128/196]	Time 0.068 (0.061)	Data 0.000 (0.004)	Loss 0.6080 (0.6285)	Acc@1 82.031 (78.379)	Acc@5 99.609 (98.758)
Epoch: [103][192/196]	Time 0.068 (0.063)	Data 0.000 (0.003)	Loss 0.5982 (0.6280)	Acc@1 82.422 (78.388)	Acc@5 98.047 (98.735)
after train
test acc: 62.54
Epoche: [104/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [104][0/196]	Time 0.099 (0.099)	Data 0.440 (0.440)	Loss 0.6296 (0.6296)	Acc@1 76.172 (76.172)	Acc@5 99.219 (99.219)
Epoch: [104][64/196]	Time 0.068 (0.062)	Data 0.000 (0.007)	Loss 0.6698 (0.6337)	Acc@1 76.562 (77.939)	Acc@5 99.609 (98.726)
Epoch: [104][128/196]	Time 0.058 (0.063)	Data 0.000 (0.004)	Loss 0.7069 (0.6270)	Acc@1 77.734 (78.198)	Acc@5 98.047 (98.792)
Epoch: [104][192/196]	Time 0.058 (0.062)	Data 0.000 (0.003)	Loss 0.5838 (0.6233)	Acc@1 81.250 (78.340)	Acc@5 98.047 (98.745)
after train
test acc: 69.47
Epoche: [105/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [105][0/196]	Time 0.090 (0.090)	Data 0.421 (0.421)	Loss 0.5697 (0.5697)	Acc@1 79.297 (79.297)	Acc@5 99.219 (99.219)
Epoch: [105][64/196]	Time 0.058 (0.060)	Data 0.000 (0.007)	Loss 0.5930 (0.6239)	Acc@1 78.906 (78.528)	Acc@5 98.828 (98.900)
Epoch: [105][128/196]	Time 0.058 (0.060)	Data 0.000 (0.004)	Loss 0.6192 (0.6259)	Acc@1 76.953 (78.431)	Acc@5 98.438 (98.871)
Epoch: [105][192/196]	Time 0.058 (0.060)	Data 0.000 (0.002)	Loss 0.6650 (0.6186)	Acc@1 76.953 (78.692)	Acc@5 98.828 (98.889)
after train
test acc: 67.65
Epoche: [106/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [106][0/196]	Time 0.085 (0.085)	Data 0.318 (0.318)	Loss 0.5080 (0.5080)	Acc@1 83.594 (83.594)	Acc@5 98.438 (98.438)
Epoch: [106][64/196]	Time 0.068 (0.061)	Data 0.000 (0.005)	Loss 0.5982 (0.6302)	Acc@1 79.688 (78.353)	Acc@5 97.656 (98.756)
Epoch: [106][128/196]	Time 0.058 (0.062)	Data 0.000 (0.003)	Loss 0.5943 (0.6220)	Acc@1 78.516 (78.603)	Acc@5 98.828 (98.774)
Epoch: [106][192/196]	Time 0.068 (0.061)	Data 0.000 (0.002)	Loss 0.6006 (0.6206)	Acc@1 78.516 (78.686)	Acc@5 98.438 (98.765)
after train
test acc: 64.94
Epoche: [107/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [107][0/196]	Time 0.095 (0.095)	Data 0.443 (0.443)	Loss 0.6256 (0.6256)	Acc@1 76.562 (76.562)	Acc@5 99.609 (99.609)
Epoch: [107][64/196]	Time 0.068 (0.059)	Data 0.000 (0.007)	Loss 0.5872 (0.6243)	Acc@1 80.469 (78.480)	Acc@5 98.047 (98.744)
Epoch: [107][128/196]	Time 0.058 (0.059)	Data 0.000 (0.004)	Loss 0.5265 (0.6212)	Acc@1 79.688 (78.597)	Acc@5 99.219 (98.719)
Epoch: [107][192/196]	Time 0.058 (0.059)	Data 0.000 (0.003)	Loss 0.6270 (0.6199)	Acc@1 78.516 (78.696)	Acc@5 99.219 (98.763)
after train
test acc: 66.18
Epoche: [108/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [108][0/196]	Time 0.095 (0.095)	Data 0.404 (0.404)	Loss 0.4760 (0.4760)	Acc@1 84.766 (84.766)	Acc@5 98.828 (98.828)
Epoch: [108][64/196]	Time 0.058 (0.059)	Data 0.000 (0.007)	Loss 0.7043 (0.6255)	Acc@1 73.438 (78.419)	Acc@5 98.828 (98.672)
Epoch: [108][128/196]	Time 0.057 (0.059)	Data 0.000 (0.003)	Loss 0.6046 (0.6259)	Acc@1 78.906 (78.385)	Acc@5 98.828 (98.713)
Epoch: [108][192/196]	Time 0.058 (0.058)	Data 0.000 (0.002)	Loss 0.6270 (0.6278)	Acc@1 78.516 (78.333)	Acc@5 99.219 (98.719)
after train
test acc: 65.92
Epoche: [109/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [109][0/196]	Time 0.069 (0.069)	Data 0.426 (0.426)	Loss 0.6380 (0.6380)	Acc@1 79.297 (79.297)	Acc@5 98.438 (98.438)
Epoch: [109][64/196]	Time 0.057 (0.059)	Data 0.000 (0.007)	Loss 0.6063 (0.6272)	Acc@1 77.734 (78.468)	Acc@5 100.000 (98.690)
Epoch: [109][128/196]	Time 0.057 (0.058)	Data 0.000 (0.004)	Loss 0.6567 (0.6215)	Acc@1 78.516 (78.582)	Acc@5 98.438 (98.771)
Epoch: [109][192/196]	Time 0.058 (0.058)	Data 0.000 (0.002)	Loss 0.7051 (0.6191)	Acc@1 77.734 (78.635)	Acc@5 98.438 (98.778)
after train
test acc: 72.57
Epoche: [110/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [110][0/196]	Time 0.084 (0.084)	Data 0.527 (0.527)	Loss 0.5964 (0.5964)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [110][64/196]	Time 0.057 (0.058)	Data 0.000 (0.008)	Loss 0.6553 (0.6193)	Acc@1 78.516 (78.864)	Acc@5 98.438 (98.828)
Epoch: [110][128/196]	Time 0.057 (0.058)	Data 0.000 (0.004)	Loss 0.5803 (0.6184)	Acc@1 81.250 (78.834)	Acc@5 98.438 (98.777)
Epoch: [110][192/196]	Time 0.058 (0.058)	Data 0.000 (0.003)	Loss 0.6110 (0.6195)	Acc@1 78.125 (78.617)	Acc@5 98.047 (98.776)
after train
test acc: 75.23
Epoche: [111/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [111][0/196]	Time 0.081 (0.081)	Data 0.357 (0.357)	Loss 0.6235 (0.6235)	Acc@1 81.641 (81.641)	Acc@5 97.656 (97.656)
Epoch: [111][64/196]	Time 0.058 (0.058)	Data 0.000 (0.006)	Loss 0.7170 (0.6197)	Acc@1 75.391 (78.984)	Acc@5 98.047 (98.774)
Epoch: [111][128/196]	Time 0.058 (0.059)	Data 0.000 (0.003)	Loss 0.5853 (0.6180)	Acc@1 78.516 (78.706)	Acc@5 99.219 (98.819)
Epoch: [111][192/196]	Time 0.058 (0.059)	Data 0.000 (0.002)	Loss 0.6804 (0.6189)	Acc@1 78.516 (78.673)	Acc@5 98.828 (98.776)
after train
test acc: 65.2
Epoche: [112/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [112][0/196]	Time 0.091 (0.091)	Data 0.421 (0.421)	Loss 0.6892 (0.6892)	Acc@1 75.391 (75.391)	Acc@5 99.219 (99.219)
Epoch: [112][64/196]	Time 0.068 (0.062)	Data 0.000 (0.007)	Loss 0.6180 (0.6129)	Acc@1 77.734 (78.702)	Acc@5 99.219 (98.870)
Epoch: [112][128/196]	Time 0.058 (0.061)	Data 0.000 (0.004)	Loss 0.6558 (0.6084)	Acc@1 79.297 (78.930)	Acc@5 98.438 (98.810)
Epoch: [112][192/196]	Time 0.058 (0.060)	Data 0.000 (0.002)	Loss 0.5446 (0.6083)	Acc@1 82.031 (78.971)	Acc@5 99.219 (98.842)
after train
test acc: 73.4
Epoche: [113/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [113][0/196]	Time 0.075 (0.075)	Data 0.500 (0.500)	Loss 0.6787 (0.6787)	Acc@1 76.172 (76.172)	Acc@5 98.438 (98.438)
Epoch: [113][64/196]	Time 0.057 (0.068)	Data 0.000 (0.008)	Loss 0.6674 (0.6009)	Acc@1 78.125 (79.531)	Acc@5 99.609 (98.864)
Epoch: [113][128/196]	Time 0.057 (0.064)	Data 0.000 (0.004)	Loss 0.6135 (0.6044)	Acc@1 81.250 (79.442)	Acc@5 96.875 (98.861)
Epoch: [113][192/196]	Time 0.058 (0.063)	Data 0.000 (0.003)	Loss 0.5211 (0.6128)	Acc@1 83.594 (79.099)	Acc@5 99.219 (98.816)
after train
test acc: 61.34
Epoche: [114/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [114][0/196]	Time 0.079 (0.079)	Data 0.435 (0.435)	Loss 0.6867 (0.6867)	Acc@1 76.953 (76.953)	Acc@5 98.047 (98.047)
Epoch: [114][64/196]	Time 0.058 (0.071)	Data 0.000 (0.007)	Loss 0.5366 (0.6237)	Acc@1 81.250 (78.486)	Acc@5 99.219 (98.822)
Epoch: [114][128/196]	Time 0.058 (0.066)	Data 0.000 (0.004)	Loss 0.6361 (0.6134)	Acc@1 77.344 (78.976)	Acc@5 98.047 (98.901)
Epoch: [114][192/196]	Time 0.058 (0.063)	Data 0.000 (0.003)	Loss 0.5918 (0.6116)	Acc@1 80.469 (79.001)	Acc@5 98.828 (98.856)
after train
test acc: 72.61
Epoche: [115/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [115][0/196]	Time 0.072 (0.072)	Data 0.489 (0.489)	Loss 0.5903 (0.5903)	Acc@1 77.344 (77.344)	Acc@5 99.219 (99.219)
Epoch: [115][64/196]	Time 0.076 (0.070)	Data 0.000 (0.008)	Loss 0.6115 (0.6249)	Acc@1 76.953 (78.047)	Acc@5 98.828 (98.828)
Epoch: [115][128/196]	Time 0.057 (0.067)	Data 0.000 (0.004)	Loss 0.6279 (0.6196)	Acc@1 77.344 (78.343)	Acc@5 98.828 (98.825)
Epoch: [115][192/196]	Time 0.058 (0.064)	Data 0.000 (0.003)	Loss 0.5787 (0.6162)	Acc@1 79.688 (78.576)	Acc@5 98.828 (98.800)
after train
test acc: 68.09
Epoche: [116/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [116][0/196]	Time 0.093 (0.093)	Data 0.415 (0.415)	Loss 0.5751 (0.5751)	Acc@1 80.469 (80.469)	Acc@5 98.828 (98.828)
Epoch: [116][64/196]	Time 0.076 (0.067)	Data 0.000 (0.007)	Loss 0.5861 (0.6306)	Acc@1 78.906 (78.504)	Acc@5 99.609 (98.792)
Epoch: [116][128/196]	Time 0.076 (0.066)	Data 0.000 (0.003)	Loss 0.5615 (0.6201)	Acc@1 82.812 (78.912)	Acc@5 99.219 (98.813)
Epoch: [116][192/196]	Time 0.055 (0.064)	Data 0.000 (0.002)	Loss 0.6906 (0.6153)	Acc@1 78.125 (78.959)	Acc@5 98.047 (98.810)
after train
test acc: 78.03
Epoche: [117/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [117][0/196]	Time 0.095 (0.095)	Data 0.348 (0.348)	Loss 0.5033 (0.5033)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [117][64/196]	Time 0.068 (0.064)	Data 0.000 (0.006)	Loss 0.5685 (0.6130)	Acc@1 80.859 (78.756)	Acc@5 99.609 (98.804)
Epoch: [117][128/196]	Time 0.068 (0.066)	Data 0.000 (0.003)	Loss 0.6531 (0.6120)	Acc@1 79.297 (78.640)	Acc@5 98.438 (98.852)
Epoch: [117][192/196]	Time 0.068 (0.066)	Data 0.000 (0.002)	Loss 0.5425 (0.6097)	Acc@1 82.031 (78.781)	Acc@5 98.438 (98.840)
after train
test acc: 69.2
Epoche: [118/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [118][0/196]	Time 0.084 (0.084)	Data 0.432 (0.432)	Loss 0.5759 (0.5759)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [118][64/196]	Time 0.058 (0.060)	Data 0.000 (0.007)	Loss 0.7332 (0.6212)	Acc@1 73.828 (78.492)	Acc@5 98.047 (98.858)
Epoch: [118][128/196]	Time 0.058 (0.063)	Data 0.000 (0.004)	Loss 0.6692 (0.6200)	Acc@1 74.219 (78.479)	Acc@5 98.047 (98.807)
Epoch: [118][192/196]	Time 0.067 (0.064)	Data 0.000 (0.002)	Loss 0.4775 (0.6110)	Acc@1 82.812 (78.754)	Acc@5 98.828 (98.812)
after train
test acc: 71.55
Epoche: [119/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [119][0/196]	Time 0.113 (0.113)	Data 0.460 (0.460)	Loss 0.5966 (0.5966)	Acc@1 79.297 (79.297)	Acc@5 99.219 (99.219)
Epoch: [119][64/196]	Time 0.058 (0.059)	Data 0.000 (0.007)	Loss 0.5773 (0.5993)	Acc@1 82.812 (79.441)	Acc@5 98.438 (98.756)
Epoch: [119][128/196]	Time 0.068 (0.060)	Data 0.000 (0.004)	Loss 0.6236 (0.6044)	Acc@1 78.125 (79.185)	Acc@5 98.438 (98.719)
Epoch: [119][192/196]	Time 0.068 (0.062)	Data 0.000 (0.003)	Loss 0.5597 (0.6111)	Acc@1 82.422 (78.862)	Acc@5 98.828 (98.769)
after train
test acc: 68.17
Epoche: [120/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [120][0/196]	Time 0.090 (0.090)	Data 0.408 (0.408)	Loss 0.6978 (0.6978)	Acc@1 74.219 (74.219)	Acc@5 98.438 (98.438)
Epoch: [120][64/196]	Time 0.067 (0.064)	Data 0.000 (0.007)	Loss 0.6647 (0.6196)	Acc@1 75.781 (78.311)	Acc@5 98.047 (98.888)
Epoch: [120][128/196]	Time 0.057 (0.063)	Data 0.000 (0.003)	Loss 0.5891 (0.6122)	Acc@1 80.469 (78.740)	Acc@5 99.609 (98.874)
Epoch: [120][192/196]	Time 0.058 (0.061)	Data 0.000 (0.002)	Loss 0.5757 (0.6084)	Acc@1 80.859 (78.933)	Acc@5 98.828 (98.832)
after train
test acc: 76.13
[INFO] Storing checkpoint...
Max memory: 22.4674304
 -1608550338.699s  