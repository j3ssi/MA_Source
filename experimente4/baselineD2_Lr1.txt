no display found. Using non-interactive Agg backend
[4, 4, 4]
[16, 32, 64]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 2;widthofFirstLayer: 16; Epochen: 180; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/baselineD2_Lr1/model.nn; checkpoint: ./output/experimente4/baselineD2_Lr1; saveModell: True; LR: 0.1
random number: 5705
Files already downloaded and verified
width: 16

Arch Num:  [[2, 2, 2, 2], [3, 2, 2, 2], [3, 2, 2, 2]]
conv0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0
bn1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
Relu; i: 2
i : 2; block: 0
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 2; block: 0
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 2
i : 3; block: 1
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 3; block: 1
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 3
i : 4; block: 2
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 4; block: 2
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 4
i : 5; block: 3
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 5; block: 3
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 5
i : 6; block: 0
Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 6; block: 0
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
i : 6; block: 0
Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 2 if 2
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 2
seq: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 6
seq1: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 7
i : 7; block: 1
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 7; block: 1
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 7
i : 8; block: 2
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 8; block: 2
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 8
i : 9; block: 3
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 9; block: 3
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 9
i : 10; block: 0
Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 10; block: 0
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
i : 10; block: 0
Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 2 if 2
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 2
seq: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 10
seq1: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 11
i : 11; block: 1
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 11; block: 1
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 11
i : 12; block: 2
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 12; block: 2
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 12
i : 13; block: 3
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 13; block: 3
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 13
avgpoll: AdaptiveAvgPool2d(output_size=(1, 1))
linear: Linear(in_features=64, out_features=10, bias=True)
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): AdaptiveAvgPool2d(output_size=(1, 1))
    (18): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 4
Startepoche: 1
deeper epoch: 0
Epoche: [1/180]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [1][0/196]	Time 0.162 (0.162)	Data 0.283 (0.283)	Loss 2.4598 (2.4598)	Acc@1 12.500 (12.500)	Acc@5 52.734 (52.734)
Epoch: [1][64/196]	Time 0.163 (0.155)	Data 0.000 (0.005)	Loss 1.7205 (1.9389)	Acc@1 38.281 (27.055)	Acc@5 85.938 (79.483)
Epoch: [1][128/196]	Time 0.151 (0.155)	Data 0.000 (0.002)	Loss 1.4983 (1.7953)	Acc@1 46.875 (32.558)	Acc@5 89.453 (83.993)
Epoch: [1][192/196]	Time 0.162 (0.152)	Data 0.000 (0.002)	Loss 1.4017 (1.6703)	Acc@1 51.172 (37.793)	Acc@5 91.797 (86.830)
after train
Epoche: [2/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [2][0/196]	Time 0.166 (0.166)	Data 0.270 (0.270)	Loss 1.3623 (1.3623)	Acc@1 56.250 (56.250)	Acc@5 92.578 (92.578)
Epoch: [2][64/196]	Time 0.143 (0.155)	Data 0.000 (0.004)	Loss 1.2510 (1.2774)	Acc@1 58.594 (53.954)	Acc@5 93.359 (94.399)
Epoch: [2][128/196]	Time 0.165 (0.153)	Data 0.000 (0.002)	Loss 1.1838 (1.2212)	Acc@1 53.516 (55.690)	Acc@5 98.438 (95.122)
Epoch: [2][192/196]	Time 0.163 (0.153)	Data 0.000 (0.002)	Loss 0.9953 (1.1700)	Acc@1 64.062 (57.651)	Acc@5 96.484 (95.499)
after train
Epoche: [3/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [3][0/196]	Time 0.138 (0.138)	Data 0.300 (0.300)	Loss 0.9305 (0.9305)	Acc@1 66.797 (66.797)	Acc@5 97.266 (97.266)
Epoch: [3][64/196]	Time 0.127 (0.150)	Data 0.000 (0.005)	Loss 0.8945 (0.9911)	Acc@1 69.922 (64.730)	Acc@5 97.266 (96.947)
Epoch: [3][128/196]	Time 0.154 (0.151)	Data 0.000 (0.003)	Loss 0.9053 (0.9504)	Acc@1 67.969 (66.285)	Acc@5 98.047 (97.199)
Epoch: [3][192/196]	Time 0.149 (0.153)	Data 0.000 (0.002)	Loss 0.8381 (0.9203)	Acc@1 69.531 (67.358)	Acc@5 97.266 (97.365)
after train
Epoche: [4/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [4][0/196]	Time 0.163 (0.163)	Data 0.280 (0.280)	Loss 0.8702 (0.8702)	Acc@1 71.875 (71.875)	Acc@5 97.266 (97.266)
Epoch: [4][64/196]	Time 0.159 (0.147)	Data 0.000 (0.005)	Loss 0.7986 (0.8197)	Acc@1 70.312 (71.244)	Acc@5 98.438 (97.915)
Epoch: [4][128/196]	Time 0.164 (0.150)	Data 0.000 (0.002)	Loss 0.7165 (0.7969)	Acc@1 74.219 (72.014)	Acc@5 98.438 (97.998)
Epoch: [4][192/196]	Time 0.158 (0.150)	Data 0.000 (0.002)	Loss 0.7577 (0.7793)	Acc@1 75.781 (72.737)	Acc@5 98.438 (98.104)
after train
Epoche: [5/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [5][0/196]	Time 0.191 (0.191)	Data 0.287 (0.287)	Loss 0.6596 (0.6596)	Acc@1 79.297 (79.297)	Acc@5 100.000 (100.000)
Epoch: [5][64/196]	Time 0.147 (0.155)	Data 0.000 (0.005)	Loss 0.6583 (0.7106)	Acc@1 77.344 (75.457)	Acc@5 98.438 (98.329)
Epoch: [5][128/196]	Time 0.142 (0.151)	Data 0.000 (0.002)	Loss 0.7218 (0.6945)	Acc@1 73.438 (75.890)	Acc@5 99.219 (98.383)
Epoch: [5][192/196]	Time 0.157 (0.151)	Data 0.000 (0.002)	Loss 0.6917 (0.6825)	Acc@1 75.391 (76.431)	Acc@5 98.828 (98.431)
after train
Epoche: [6/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [6][0/196]	Time 0.183 (0.183)	Data 0.277 (0.277)	Loss 0.6154 (0.6154)	Acc@1 76.953 (76.953)	Acc@5 99.219 (99.219)
Epoch: [6][64/196]	Time 0.154 (0.155)	Data 0.000 (0.004)	Loss 0.5112 (0.6384)	Acc@1 80.859 (77.806)	Acc@5 99.609 (98.804)
Epoch: [6][128/196]	Time 0.120 (0.151)	Data 0.000 (0.002)	Loss 0.6235 (0.6370)	Acc@1 79.688 (77.961)	Acc@5 99.219 (98.768)
Epoch: [6][192/196]	Time 0.152 (0.151)	Data 0.000 (0.002)	Loss 0.6345 (0.6362)	Acc@1 77.734 (77.925)	Acc@5 99.609 (98.767)
after train
Epoche: [7/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [7][0/196]	Time 0.171 (0.171)	Data 0.305 (0.305)	Loss 0.5961 (0.5961)	Acc@1 79.297 (79.297)	Acc@5 98.828 (98.828)
Epoch: [7][64/196]	Time 0.160 (0.150)	Data 0.000 (0.005)	Loss 0.6451 (0.6039)	Acc@1 79.297 (78.960)	Acc@5 97.266 (98.828)
Epoch: [7][128/196]	Time 0.139 (0.151)	Data 0.000 (0.003)	Loss 0.5870 (0.6002)	Acc@1 78.516 (79.218)	Acc@5 99.219 (98.858)
Epoch: [7][192/196]	Time 0.153 (0.150)	Data 0.000 (0.002)	Loss 0.5347 (0.5893)	Acc@1 80.859 (79.617)	Acc@5 99.609 (98.901)
after train
Epoche: [8/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [8][0/196]	Time 0.177 (0.177)	Data 0.326 (0.326)	Loss 0.6074 (0.6074)	Acc@1 77.734 (77.734)	Acc@5 98.828 (98.828)
Epoch: [8][64/196]	Time 0.157 (0.155)	Data 0.000 (0.005)	Loss 0.5350 (0.5785)	Acc@1 81.641 (80.024)	Acc@5 98.828 (98.888)
Epoch: [8][128/196]	Time 0.158 (0.155)	Data 0.000 (0.003)	Loss 0.5330 (0.5664)	Acc@1 80.078 (80.414)	Acc@5 98.438 (98.995)
Epoch: [8][192/196]	Time 0.147 (0.153)	Data 0.000 (0.002)	Loss 0.4545 (0.5610)	Acc@1 83.984 (80.669)	Acc@5 99.609 (98.990)
after train
Epoche: [9/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [9][0/196]	Time 0.113 (0.113)	Data 0.272 (0.272)	Loss 0.4752 (0.4752)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [9][64/196]	Time 0.159 (0.150)	Data 0.000 (0.004)	Loss 0.7192 (0.5400)	Acc@1 77.344 (81.286)	Acc@5 98.828 (99.050)
Epoch: [9][128/196]	Time 0.145 (0.150)	Data 0.000 (0.002)	Loss 0.5353 (0.5436)	Acc@1 80.859 (81.262)	Acc@5 98.438 (99.073)
Epoch: [9][192/196]	Time 0.164 (0.152)	Data 0.000 (0.002)	Loss 0.4711 (0.5395)	Acc@1 84.375 (81.382)	Acc@5 100.000 (99.073)
after train
Epoche: [10/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [10][0/196]	Time 0.176 (0.176)	Data 0.278 (0.278)	Loss 0.4973 (0.4973)	Acc@1 81.641 (81.641)	Acc@5 99.219 (99.219)
Epoch: [10][64/196]	Time 0.167 (0.145)	Data 0.000 (0.005)	Loss 0.4268 (0.5305)	Acc@1 84.766 (81.707)	Acc@5 99.609 (98.996)
Epoch: [10][128/196]	Time 0.159 (0.149)	Data 0.000 (0.002)	Loss 0.5821 (0.5228)	Acc@1 80.078 (81.949)	Acc@5 99.609 (99.095)
Epoch: [10][192/196]	Time 0.144 (0.151)	Data 0.000 (0.002)	Loss 0.4443 (0.5180)	Acc@1 83.594 (82.060)	Acc@5 98.828 (99.091)
after train
Epoche: [11/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [11][0/196]	Time 0.171 (0.171)	Data 0.307 (0.307)	Loss 0.4176 (0.4176)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [11][64/196]	Time 0.140 (0.151)	Data 0.000 (0.005)	Loss 0.5575 (0.4937)	Acc@1 80.859 (83.041)	Acc@5 98.828 (99.249)
Epoch: [11][128/196]	Time 0.149 (0.149)	Data 0.000 (0.003)	Loss 0.4084 (0.5018)	Acc@1 86.328 (82.625)	Acc@5 100.000 (99.222)
Epoch: [11][192/196]	Time 0.145 (0.150)	Data 0.000 (0.002)	Loss 0.4934 (0.5055)	Acc@1 85.938 (82.562)	Acc@5 99.219 (99.184)
after train
Epoche: [12/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [12][0/196]	Time 0.171 (0.171)	Data 0.283 (0.283)	Loss 0.4301 (0.4301)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [12][64/196]	Time 0.160 (0.154)	Data 0.000 (0.005)	Loss 0.5540 (0.4832)	Acc@1 81.641 (83.498)	Acc@5 98.047 (99.279)
Epoch: [12][128/196]	Time 0.111 (0.153)	Data 0.000 (0.002)	Loss 0.4490 (0.4879)	Acc@1 82.812 (83.261)	Acc@5 99.609 (99.255)
Epoch: [12][192/196]	Time 0.154 (0.150)	Data 0.000 (0.002)	Loss 0.5834 (0.4869)	Acc@1 78.125 (83.288)	Acc@5 98.438 (99.227)
after train
Epoche: [13/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [13][0/196]	Time 0.168 (0.168)	Data 0.271 (0.271)	Loss 0.5467 (0.5467)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [13][64/196]	Time 0.176 (0.154)	Data 0.000 (0.004)	Loss 0.4760 (0.4633)	Acc@1 83.984 (84.105)	Acc@5 98.828 (99.291)
Epoch: [13][128/196]	Time 0.143 (0.152)	Data 0.000 (0.002)	Loss 0.4773 (0.4729)	Acc@1 85.156 (83.669)	Acc@5 98.828 (99.273)
Epoch: [13][192/196]	Time 0.171 (0.151)	Data 0.000 (0.002)	Loss 0.4444 (0.4772)	Acc@1 85.156 (83.557)	Acc@5 99.219 (99.253)
after train
Epoche: [14/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [14][0/196]	Time 0.158 (0.158)	Data 0.326 (0.326)	Loss 0.3943 (0.3943)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [14][64/196]	Time 0.147 (0.150)	Data 0.000 (0.005)	Loss 0.3855 (0.4520)	Acc@1 87.500 (84.712)	Acc@5 100.000 (99.309)
Epoch: [14][128/196]	Time 0.159 (0.152)	Data 0.000 (0.003)	Loss 0.4352 (0.4590)	Acc@1 83.984 (84.369)	Acc@5 99.609 (99.261)
Epoch: [14][192/196]	Time 0.157 (0.153)	Data 0.000 (0.002)	Loss 0.4263 (0.4672)	Acc@1 83.984 (84.051)	Acc@5 99.609 (99.263)
after train
Epoche: [15/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [15][0/196]	Time 0.127 (0.127)	Data 0.276 (0.276)	Loss 0.5263 (0.5263)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [15][64/196]	Time 0.159 (0.148)	Data 0.000 (0.004)	Loss 0.4201 (0.4573)	Acc@1 85.547 (84.105)	Acc@5 100.000 (99.333)
Epoch: [15][128/196]	Time 0.156 (0.151)	Data 0.000 (0.002)	Loss 0.5154 (0.4649)	Acc@1 81.641 (83.821)	Acc@5 98.828 (99.328)
Epoch: [15][192/196]	Time 0.154 (0.151)	Data 0.000 (0.002)	Loss 0.4332 (0.4633)	Acc@1 86.328 (84.049)	Acc@5 99.219 (99.308)
after train
Epoche: [16/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [16][0/196]	Time 0.169 (0.169)	Data 0.288 (0.288)	Loss 0.3970 (0.3970)	Acc@1 88.281 (88.281)	Acc@5 98.828 (98.828)
Epoch: [16][64/196]	Time 0.157 (0.150)	Data 0.000 (0.005)	Loss 0.4631 (0.4547)	Acc@1 83.984 (84.147)	Acc@5 99.609 (99.435)
Epoch: [16][128/196]	Time 0.145 (0.149)	Data 0.000 (0.002)	Loss 0.4671 (0.4541)	Acc@1 80.859 (84.278)	Acc@5 99.609 (99.385)
Epoch: [16][192/196]	Time 0.162 (0.150)	Data 0.000 (0.002)	Loss 0.4247 (0.4505)	Acc@1 86.719 (84.496)	Acc@5 99.219 (99.375)
after train
Epoche: [17/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [17][0/196]	Time 0.187 (0.187)	Data 0.261 (0.261)	Loss 0.3796 (0.3796)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [17][64/196]	Time 0.155 (0.153)	Data 0.000 (0.004)	Loss 0.4856 (0.4497)	Acc@1 85.938 (84.814)	Acc@5 100.000 (99.285)
Epoch: [17][128/196]	Time 0.157 (0.150)	Data 0.000 (0.002)	Loss 0.4835 (0.4438)	Acc@1 80.469 (84.811)	Acc@5 100.000 (99.364)
Epoch: [17][192/196]	Time 0.159 (0.151)	Data 0.000 (0.002)	Loss 0.5216 (0.4443)	Acc@1 83.984 (84.768)	Acc@5 99.609 (99.348)
after train
Epoche: [18/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [18][0/196]	Time 0.179 (0.179)	Data 0.289 (0.289)	Loss 0.4066 (0.4066)	Acc@1 87.891 (87.891)	Acc@5 99.219 (99.219)
Epoch: [18][64/196]	Time 0.145 (0.151)	Data 0.000 (0.005)	Loss 0.4231 (0.4259)	Acc@1 84.375 (85.018)	Acc@5 99.609 (99.441)
Epoch: [18][128/196]	Time 0.157 (0.152)	Data 0.000 (0.002)	Loss 0.4027 (0.4298)	Acc@1 86.328 (84.941)	Acc@5 98.828 (99.422)
Epoch: [18][192/196]	Time 0.157 (0.149)	Data 0.000 (0.002)	Loss 0.5006 (0.4304)	Acc@1 82.812 (84.984)	Acc@5 100.000 (99.411)
after train
Epoche: [19/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [19][0/196]	Time 0.179 (0.179)	Data 0.291 (0.291)	Loss 0.4345 (0.4345)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [19][64/196]	Time 0.153 (0.154)	Data 0.000 (0.005)	Loss 0.4225 (0.4363)	Acc@1 85.547 (85.048)	Acc@5 98.828 (99.369)
Epoch: [19][128/196]	Time 0.156 (0.154)	Data 0.000 (0.002)	Loss 0.3424 (0.4311)	Acc@1 88.281 (85.165)	Acc@5 100.000 (99.355)
Epoch: [19][192/196]	Time 0.162 (0.151)	Data 0.000 (0.002)	Loss 0.5674 (0.4307)	Acc@1 82.812 (85.274)	Acc@5 100.000 (99.375)
after train
Epoche: [20/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [20][0/196]	Time 0.186 (0.186)	Data 0.266 (0.266)	Loss 0.3504 (0.3504)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [20][64/196]	Time 0.161 (0.154)	Data 0.000 (0.004)	Loss 0.5901 (0.4077)	Acc@1 78.906 (85.751)	Acc@5 98.047 (99.381)
Epoch: [20][128/196]	Time 0.156 (0.152)	Data 0.000 (0.002)	Loss 0.4868 (0.4175)	Acc@1 82.422 (85.541)	Acc@5 98.828 (99.422)
Epoch: [20][192/196]	Time 0.147 (0.152)	Data 0.000 (0.002)	Loss 0.4466 (0.4172)	Acc@1 83.594 (85.454)	Acc@5 98.828 (99.395)
after train
Epoche: [21/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [21][0/196]	Time 0.168 (0.168)	Data 0.287 (0.287)	Loss 0.3904 (0.3904)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [21][64/196]	Time 0.156 (0.147)	Data 0.000 (0.005)	Loss 0.3953 (0.4065)	Acc@1 86.328 (86.262)	Acc@5 99.219 (99.441)
Epoch: [21][128/196]	Time 0.158 (0.150)	Data 0.000 (0.002)	Loss 0.4765 (0.4144)	Acc@1 82.812 (85.698)	Acc@5 99.219 (99.391)
Epoch: [21][192/196]	Time 0.155 (0.151)	Data 0.000 (0.002)	Loss 0.3672 (0.4183)	Acc@1 87.500 (85.656)	Acc@5 99.219 (99.360)
after train
Epoche: [22/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [22][0/196]	Time 0.172 (0.172)	Data 0.361 (0.361)	Loss 0.3908 (0.3908)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [22][64/196]	Time 0.132 (0.150)	Data 0.000 (0.006)	Loss 0.3597 (0.4100)	Acc@1 87.500 (85.661)	Acc@5 100.000 (99.447)
Epoch: [22][128/196]	Time 0.150 (0.151)	Data 0.000 (0.003)	Loss 0.5103 (0.4171)	Acc@1 82.422 (85.586)	Acc@5 99.219 (99.461)
Epoch: [22][192/196]	Time 0.157 (0.151)	Data 0.000 (0.002)	Loss 0.4429 (0.4149)	Acc@1 86.328 (85.770)	Acc@5 98.828 (99.474)
after train
Epoche: [23/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [23][0/196]	Time 0.164 (0.164)	Data 0.278 (0.278)	Loss 0.4419 (0.4419)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [23][64/196]	Time 0.174 (0.154)	Data 0.000 (0.005)	Loss 0.4516 (0.4088)	Acc@1 84.375 (85.727)	Acc@5 99.609 (99.471)
Epoch: [23][128/196]	Time 0.158 (0.150)	Data 0.000 (0.002)	Loss 0.5126 (0.4089)	Acc@1 80.469 (85.768)	Acc@5 98.828 (99.410)
Epoch: [23][192/196]	Time 0.153 (0.151)	Data 0.000 (0.002)	Loss 0.4234 (0.4091)	Acc@1 87.109 (85.804)	Acc@5 100.000 (99.437)
after train
Epoche: [24/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [24][0/196]	Time 0.164 (0.164)	Data 0.249 (0.249)	Loss 0.4840 (0.4840)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [24][64/196]	Time 0.118 (0.152)	Data 0.000 (0.004)	Loss 0.5353 (0.4079)	Acc@1 83.203 (85.847)	Acc@5 99.219 (99.471)
Epoch: [24][128/196]	Time 0.158 (0.152)	Data 0.000 (0.002)	Loss 0.3647 (0.4129)	Acc@1 87.109 (85.671)	Acc@5 100.000 (99.455)
Epoch: [24][192/196]	Time 0.161 (0.150)	Data 0.000 (0.002)	Loss 0.3665 (0.4063)	Acc@1 86.719 (85.950)	Acc@5 100.000 (99.472)
after train
Epoche: [25/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [25][0/196]	Time 0.175 (0.175)	Data 0.283 (0.283)	Loss 0.2996 (0.2996)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [25][64/196]	Time 0.152 (0.150)	Data 0.000 (0.005)	Loss 0.4985 (0.3820)	Acc@1 84.375 (86.989)	Acc@5 99.219 (99.549)
Epoch: [25][128/196]	Time 0.168 (0.153)	Data 0.000 (0.002)	Loss 0.3831 (0.3927)	Acc@1 84.766 (86.567)	Acc@5 99.219 (99.452)
Epoch: [25][192/196]	Time 0.155 (0.150)	Data 0.000 (0.002)	Loss 0.3982 (0.3947)	Acc@1 84.375 (86.557)	Acc@5 99.609 (99.449)
after train
Epoche: [26/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [26][0/196]	Time 0.168 (0.168)	Data 0.304 (0.304)	Loss 0.3890 (0.3890)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [26][64/196]	Time 0.154 (0.154)	Data 0.000 (0.005)	Loss 0.3745 (0.3986)	Acc@1 86.719 (86.226)	Acc@5 99.219 (99.417)
Epoch: [26][128/196]	Time 0.151 (0.154)	Data 0.000 (0.003)	Loss 0.3079 (0.4007)	Acc@1 90.234 (86.231)	Acc@5 100.000 (99.425)
Epoch: [26][192/196]	Time 0.154 (0.153)	Data 0.000 (0.002)	Loss 0.4134 (0.3964)	Acc@1 83.984 (86.429)	Acc@5 100.000 (99.452)
after train
Epoche: [27/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [27][0/196]	Time 0.166 (0.166)	Data 0.289 (0.289)	Loss 0.3781 (0.3781)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [27][64/196]	Time 0.144 (0.150)	Data 0.000 (0.005)	Loss 0.3645 (0.3913)	Acc@1 87.500 (86.496)	Acc@5 99.609 (99.477)
Epoch: [27][128/196]	Time 0.148 (0.150)	Data 0.000 (0.002)	Loss 0.4442 (0.3873)	Acc@1 83.984 (86.589)	Acc@5 99.609 (99.479)
Epoch: [27][192/196]	Time 0.154 (0.152)	Data 0.000 (0.002)	Loss 0.4249 (0.3951)	Acc@1 85.156 (86.328)	Acc@5 99.609 (99.484)
after train
Epoche: [28/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [28][0/196]	Time 0.189 (0.189)	Data 0.251 (0.251)	Loss 0.4229 (0.4229)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [28][64/196]	Time 0.149 (0.151)	Data 0.000 (0.004)	Loss 0.3548 (0.3867)	Acc@1 88.281 (86.538)	Acc@5 99.219 (99.567)
Epoch: [28][128/196]	Time 0.142 (0.150)	Data 0.000 (0.002)	Loss 0.3087 (0.3808)	Acc@1 90.234 (86.900)	Acc@5 99.609 (99.558)
Epoch: [28][192/196]	Time 0.158 (0.151)	Data 0.000 (0.002)	Loss 0.4415 (0.3843)	Acc@1 85.547 (86.731)	Acc@5 98.828 (99.528)
after train
Epoche: [29/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [29][0/196]	Time 0.209 (0.209)	Data 0.307 (0.307)	Loss 0.3347 (0.3347)	Acc@1 88.672 (88.672)	Acc@5 99.609 (99.609)
Epoch: [29][64/196]	Time 0.158 (0.153)	Data 0.000 (0.005)	Loss 0.3515 (0.3704)	Acc@1 87.500 (87.181)	Acc@5 99.609 (99.453)
Epoch: [29][128/196]	Time 0.144 (0.150)	Data 0.000 (0.003)	Loss 0.4272 (0.3759)	Acc@1 86.328 (87.052)	Acc@5 99.219 (99.482)
Epoch: [29][192/196]	Time 0.156 (0.150)	Data 0.000 (0.002)	Loss 0.4474 (0.3777)	Acc@1 84.375 (86.907)	Acc@5 100.000 (99.494)
after train
Epoche: [30/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [30][0/196]	Time 0.180 (0.180)	Data 0.285 (0.285)	Loss 0.2823 (0.2823)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [30][64/196]	Time 0.138 (0.154)	Data 0.000 (0.005)	Loss 0.3674 (0.3735)	Acc@1 86.719 (87.314)	Acc@5 99.609 (99.531)
Epoch: [30][128/196]	Time 0.153 (0.152)	Data 0.000 (0.002)	Loss 0.4215 (0.3803)	Acc@1 84.766 (86.997)	Acc@5 99.609 (99.549)
Epoch: [30][192/196]	Time 0.162 (0.151)	Data 0.000 (0.002)	Loss 0.3774 (0.3867)	Acc@1 87.109 (86.705)	Acc@5 99.609 (99.526)
after train
Epoche: [31/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [31][0/196]	Time 0.167 (0.167)	Data 0.258 (0.258)	Loss 0.4305 (0.4305)	Acc@1 85.938 (85.938)	Acc@5 98.828 (98.828)
Epoch: [31][64/196]	Time 0.157 (0.153)	Data 0.000 (0.004)	Loss 0.3515 (0.3754)	Acc@1 87.891 (87.194)	Acc@5 99.609 (99.471)
Epoch: [31][128/196]	Time 0.167 (0.154)	Data 0.000 (0.002)	Loss 0.4239 (0.3779)	Acc@1 85.938 (87.091)	Acc@5 100.000 (99.497)
Epoch: [31][192/196]	Time 0.115 (0.153)	Data 0.000 (0.002)	Loss 0.4948 (0.3773)	Acc@1 81.250 (87.057)	Acc@5 98.438 (99.528)
after train
Epoche: [32/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [32][0/196]	Time 0.177 (0.177)	Data 0.279 (0.279)	Loss 0.3745 (0.3745)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [32][64/196]	Time 0.151 (0.151)	Data 0.000 (0.005)	Loss 0.3608 (0.3647)	Acc@1 86.328 (87.494)	Acc@5 99.609 (99.567)
Epoch: [32][128/196]	Time 0.148 (0.153)	Data 0.000 (0.002)	Loss 0.3718 (0.3719)	Acc@1 86.328 (87.237)	Acc@5 99.609 (99.543)
Epoch: [32][192/196]	Time 0.152 (0.152)	Data 0.000 (0.002)	Loss 0.3128 (0.3738)	Acc@1 89.844 (87.209)	Acc@5 99.609 (99.500)
after train
Epoche: [33/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [33][0/196]	Time 0.183 (0.183)	Data 0.303 (0.303)	Loss 0.3003 (0.3003)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [33][64/196]	Time 0.150 (0.150)	Data 0.000 (0.005)	Loss 0.3475 (0.3595)	Acc@1 90.234 (87.554)	Acc@5 99.609 (99.609)
Epoch: [33][128/196]	Time 0.096 (0.151)	Data 0.000 (0.003)	Loss 0.4006 (0.3700)	Acc@1 85.156 (87.267)	Acc@5 100.000 (99.543)
Epoch: [33][192/196]	Time 0.152 (0.151)	Data 0.000 (0.002)	Loss 0.5752 (0.3719)	Acc@1 80.859 (87.215)	Acc@5 99.219 (99.518)
after train
Epoche: [34/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [34][0/196]	Time 0.182 (0.182)	Data 0.250 (0.250)	Loss 0.3913 (0.3913)	Acc@1 86.328 (86.328)	Acc@5 99.219 (99.219)
Epoch: [34][64/196]	Time 0.158 (0.154)	Data 0.000 (0.004)	Loss 0.2939 (0.3675)	Acc@1 89.844 (87.344)	Acc@5 99.219 (99.531)
Epoch: [34][128/196]	Time 0.154 (0.150)	Data 0.000 (0.002)	Loss 0.3934 (0.3659)	Acc@1 87.500 (87.412)	Acc@5 100.000 (99.561)
Epoch: [34][192/196]	Time 0.153 (0.151)	Data 0.000 (0.002)	Loss 0.3787 (0.3674)	Acc@1 87.109 (87.407)	Acc@5 99.609 (99.532)
after train
Epoche: [35/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [35][0/196]	Time 0.139 (0.139)	Data 0.331 (0.331)	Loss 0.4090 (0.4090)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [35][64/196]	Time 0.152 (0.150)	Data 0.000 (0.005)	Loss 0.3308 (0.3684)	Acc@1 89.453 (87.188)	Acc@5 99.609 (99.639)
Epoch: [35][128/196]	Time 0.152 (0.149)	Data 0.000 (0.003)	Loss 0.3550 (0.3687)	Acc@1 87.500 (87.055)	Acc@5 100.000 (99.646)
Epoch: [35][192/196]	Time 0.154 (0.151)	Data 0.000 (0.002)	Loss 0.3251 (0.3700)	Acc@1 87.891 (87.095)	Acc@5 99.609 (99.575)
after train
Epoche: [36/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [36][0/196]	Time 0.139 (0.139)	Data 0.340 (0.340)	Loss 0.2844 (0.2844)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [36][64/196]	Time 0.157 (0.154)	Data 0.000 (0.005)	Loss 0.4962 (0.3512)	Acc@1 82.422 (87.734)	Acc@5 99.219 (99.694)
Epoch: [36][128/196]	Time 0.159 (0.155)	Data 0.000 (0.003)	Loss 0.3784 (0.3584)	Acc@1 84.375 (87.379)	Acc@5 100.000 (99.637)
Epoch: [36][192/196]	Time 0.156 (0.152)	Data 0.000 (0.002)	Loss 0.3667 (0.3650)	Acc@1 87.109 (87.231)	Acc@5 99.609 (99.591)
after train
Epoche: [37/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [37][0/196]	Time 0.166 (0.166)	Data 0.303 (0.303)	Loss 0.3646 (0.3646)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [37][64/196]	Time 0.157 (0.154)	Data 0.000 (0.005)	Loss 0.3072 (0.3586)	Acc@1 87.109 (87.800)	Acc@5 100.000 (99.609)
Epoch: [37][128/196]	Time 0.155 (0.151)	Data 0.000 (0.003)	Loss 0.3065 (0.3559)	Acc@1 89.453 (87.788)	Acc@5 100.000 (99.606)
Epoch: [37][192/196]	Time 0.118 (0.152)	Data 0.000 (0.002)	Loss 0.3822 (0.3631)	Acc@1 86.719 (87.508)	Acc@5 98.828 (99.599)
after train
Epoche: [38/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [38][0/196]	Time 0.173 (0.173)	Data 0.273 (0.273)	Loss 0.3677 (0.3677)	Acc@1 87.891 (87.891)	Acc@5 100.000 (100.000)
Epoch: [38][64/196]	Time 0.115 (0.151)	Data 0.000 (0.004)	Loss 0.3269 (0.3411)	Acc@1 90.234 (88.323)	Acc@5 98.047 (99.591)
Epoch: [38][128/196]	Time 0.158 (0.152)	Data 0.000 (0.002)	Loss 0.3885 (0.3538)	Acc@1 85.547 (87.994)	Acc@5 99.609 (99.573)
Epoch: [38][192/196]	Time 0.153 (0.152)	Data 0.000 (0.002)	Loss 0.3633 (0.3581)	Acc@1 85.938 (87.696)	Acc@5 99.219 (99.549)
after train
Epoche: [39/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [39][0/196]	Time 0.186 (0.186)	Data 0.285 (0.285)	Loss 0.4367 (0.4367)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [39][64/196]	Time 0.148 (0.146)	Data 0.000 (0.005)	Loss 0.4097 (0.3623)	Acc@1 83.594 (87.596)	Acc@5 99.609 (99.567)
Epoch: [39][128/196]	Time 0.157 (0.150)	Data 0.000 (0.002)	Loss 0.3782 (0.3615)	Acc@1 87.500 (87.766)	Acc@5 99.219 (99.543)
Epoch: [39][192/196]	Time 0.149 (0.150)	Data 0.000 (0.002)	Loss 0.3675 (0.3596)	Acc@1 87.109 (87.820)	Acc@5 99.609 (99.528)
after train
Epoche: [40/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [40][0/196]	Time 0.161 (0.161)	Data 0.266 (0.266)	Loss 0.4372 (0.4372)	Acc@1 87.109 (87.109)	Acc@5 98.438 (98.438)
Epoch: [40][64/196]	Time 0.149 (0.153)	Data 0.000 (0.004)	Loss 0.4100 (0.3482)	Acc@1 85.547 (88.137)	Acc@5 100.000 (99.567)
Epoch: [40][128/196]	Time 0.148 (0.150)	Data 0.000 (0.002)	Loss 0.3400 (0.3462)	Acc@1 87.891 (88.172)	Acc@5 99.219 (99.637)
Epoch: [40][192/196]	Time 0.150 (0.151)	Data 0.000 (0.002)	Loss 0.3297 (0.3523)	Acc@1 89.844 (87.901)	Acc@5 99.609 (99.595)
after train
Epoche: [41/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [41][0/196]	Time 0.184 (0.184)	Data 0.269 (0.269)	Loss 0.3521 (0.3521)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [41][64/196]	Time 0.157 (0.153)	Data 0.000 (0.004)	Loss 0.2702 (0.3409)	Acc@1 92.188 (88.221)	Acc@5 99.609 (99.567)
Epoch: [41][128/196]	Time 0.162 (0.149)	Data 0.000 (0.002)	Loss 0.2584 (0.3455)	Acc@1 91.406 (88.124)	Acc@5 99.609 (99.579)
Epoch: [41][192/196]	Time 0.151 (0.151)	Data 0.000 (0.002)	Loss 0.4118 (0.3519)	Acc@1 86.719 (87.818)	Acc@5 99.609 (99.573)
after train
Epoche: [42/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [42][0/196]	Time 0.184 (0.184)	Data 0.263 (0.263)	Loss 0.4010 (0.4010)	Acc@1 85.547 (85.547)	Acc@5 98.828 (98.828)
Epoch: [42][64/196]	Time 0.167 (0.152)	Data 0.000 (0.004)	Loss 0.4152 (0.3433)	Acc@1 83.203 (87.770)	Acc@5 100.000 (99.615)
Epoch: [42][128/196]	Time 0.144 (0.152)	Data 0.000 (0.002)	Loss 0.3000 (0.3477)	Acc@1 91.406 (87.939)	Acc@5 100.000 (99.615)
Epoch: [42][192/196]	Time 0.159 (0.151)	Data 0.000 (0.002)	Loss 0.3287 (0.3492)	Acc@1 89.844 (87.994)	Acc@5 99.219 (99.563)
after train
Epoche: [43/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [43][0/196]	Time 0.198 (0.198)	Data 0.291 (0.291)	Loss 0.3738 (0.3738)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [43][64/196]	Time 0.148 (0.154)	Data 0.000 (0.005)	Loss 0.3259 (0.3333)	Acc@1 84.375 (88.407)	Acc@5 99.609 (99.627)
Epoch: [43][128/196]	Time 0.154 (0.154)	Data 0.000 (0.002)	Loss 0.2753 (0.3399)	Acc@1 91.406 (88.275)	Acc@5 100.000 (99.603)
Epoch: [43][192/196]	Time 0.159 (0.153)	Data 0.000 (0.002)	Loss 0.3516 (0.3447)	Acc@1 87.109 (88.065)	Acc@5 99.219 (99.597)
after train
Epoche: [44/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [44][0/196]	Time 0.148 (0.148)	Data 0.289 (0.289)	Loss 0.3683 (0.3683)	Acc@1 90.234 (90.234)	Acc@5 100.000 (100.000)
Epoch: [44][64/196]	Time 0.157 (0.153)	Data 0.000 (0.005)	Loss 0.3366 (0.3403)	Acc@1 88.281 (88.209)	Acc@5 100.000 (99.597)
Epoch: [44][128/196]	Time 0.152 (0.151)	Data 0.000 (0.002)	Loss 0.3746 (0.3440)	Acc@1 85.938 (88.157)	Acc@5 99.609 (99.640)
Epoch: [44][192/196]	Time 0.150 (0.152)	Data 0.000 (0.002)	Loss 0.3571 (0.3502)	Acc@1 87.891 (87.878)	Acc@5 99.609 (99.626)
after train
Epoche: [45/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [45][0/196]	Time 0.167 (0.167)	Data 0.287 (0.287)	Loss 0.3307 (0.3307)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [45][64/196]	Time 0.157 (0.144)	Data 0.000 (0.005)	Loss 0.4952 (0.3406)	Acc@1 83.594 (88.191)	Acc@5 99.219 (99.681)
Epoch: [45][128/196]	Time 0.158 (0.149)	Data 0.000 (0.002)	Loss 0.4176 (0.3443)	Acc@1 88.672 (88.203)	Acc@5 99.609 (99.618)
Epoch: [45][192/196]	Time 0.138 (0.150)	Data 0.000 (0.002)	Loss 0.3202 (0.3466)	Acc@1 89.844 (88.123)	Acc@5 100.000 (99.601)
after train
Epoche: [46/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [46][0/196]	Time 0.180 (0.180)	Data 0.259 (0.259)	Loss 0.3532 (0.3532)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [46][64/196]	Time 0.147 (0.151)	Data 0.000 (0.004)	Loss 0.3798 (0.3395)	Acc@1 87.109 (88.203)	Acc@5 98.828 (99.567)
Epoch: [46][128/196]	Time 0.151 (0.150)	Data 0.000 (0.002)	Loss 0.2372 (0.3426)	Acc@1 91.797 (88.203)	Acc@5 100.000 (99.570)
Epoch: [46][192/196]	Time 0.158 (0.150)	Data 0.000 (0.002)	Loss 0.3768 (0.3456)	Acc@1 85.547 (88.010)	Acc@5 99.609 (99.599)
after train
Epoche: [47/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [47][0/196]	Time 0.184 (0.184)	Data 0.300 (0.300)	Loss 0.3705 (0.3705)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [47][64/196]	Time 0.160 (0.154)	Data 0.000 (0.005)	Loss 0.3955 (0.3477)	Acc@1 85.547 (88.215)	Acc@5 99.609 (99.591)
Epoch: [47][128/196]	Time 0.122 (0.150)	Data 0.000 (0.003)	Loss 0.3637 (0.3529)	Acc@1 87.891 (87.918)	Acc@5 99.609 (99.609)
Epoch: [47][192/196]	Time 0.152 (0.150)	Data 0.000 (0.002)	Loss 0.3250 (0.3463)	Acc@1 90.625 (88.053)	Acc@5 99.609 (99.615)
after train
Epoche: [48/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [48][0/196]	Time 0.163 (0.163)	Data 0.282 (0.282)	Loss 0.3920 (0.3920)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [48][64/196]	Time 0.159 (0.155)	Data 0.000 (0.005)	Loss 0.3338 (0.3483)	Acc@1 88.672 (88.047)	Acc@5 100.000 (99.609)
Epoch: [48][128/196]	Time 0.156 (0.153)	Data 0.000 (0.002)	Loss 0.3452 (0.3395)	Acc@1 85.938 (88.251)	Acc@5 100.000 (99.646)
Epoch: [48][192/196]	Time 0.143 (0.152)	Data 0.000 (0.002)	Loss 0.3068 (0.3393)	Acc@1 89.062 (88.291)	Acc@5 99.609 (99.615)
after train
Epoche: [49/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [49][0/196]	Time 0.158 (0.158)	Data 0.271 (0.271)	Loss 0.3139 (0.3139)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [49][64/196]	Time 0.157 (0.150)	Data 0.000 (0.004)	Loss 0.2256 (0.3310)	Acc@1 91.406 (88.696)	Acc@5 100.000 (99.597)
Epoch: [49][128/196]	Time 0.155 (0.152)	Data 0.000 (0.002)	Loss 0.3486 (0.3353)	Acc@1 87.500 (88.393)	Acc@5 99.609 (99.621)
Epoch: [49][192/196]	Time 0.107 (0.152)	Data 0.000 (0.002)	Loss 0.4141 (0.3374)	Acc@1 85.156 (88.283)	Acc@5 99.219 (99.591)
after train
Epoche: [50/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [50][0/196]	Time 0.181 (0.181)	Data 0.317 (0.317)	Loss 0.2585 (0.2585)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [50][64/196]	Time 0.162 (0.152)	Data 0.000 (0.005)	Loss 0.3436 (0.3387)	Acc@1 90.234 (88.353)	Acc@5 99.219 (99.597)
Epoch: [50][128/196]	Time 0.145 (0.152)	Data 0.000 (0.003)	Loss 0.2972 (0.3342)	Acc@1 88.672 (88.560)	Acc@5 100.000 (99.634)
Epoch: [50][192/196]	Time 0.145 (0.152)	Data 0.000 (0.002)	Loss 0.3601 (0.3325)	Acc@1 87.109 (88.544)	Acc@5 99.219 (99.636)
after train
Epoche: [51/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [51][0/196]	Time 0.166 (0.166)	Data 0.269 (0.269)	Loss 0.3741 (0.3741)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [51][64/196]	Time 0.171 (0.149)	Data 0.000 (0.004)	Loss 0.3105 (0.3356)	Acc@1 89.062 (88.690)	Acc@5 99.609 (99.597)
Epoch: [51][128/196]	Time 0.157 (0.150)	Data 0.000 (0.002)	Loss 0.3692 (0.3324)	Acc@1 88.672 (88.793)	Acc@5 99.609 (99.585)
Epoch: [51][192/196]	Time 0.145 (0.152)	Data 0.000 (0.002)	Loss 0.3170 (0.3344)	Acc@1 88.281 (88.684)	Acc@5 99.609 (99.585)
after train
Epoche: [52/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [52][0/196]	Time 0.166 (0.166)	Data 0.262 (0.262)	Loss 0.4143 (0.4143)	Acc@1 87.891 (87.891)	Acc@5 98.828 (98.828)
Epoch: [52][64/196]	Time 0.151 (0.149)	Data 0.000 (0.004)	Loss 0.3104 (0.3363)	Acc@1 87.500 (88.504)	Acc@5 99.609 (99.688)
Epoch: [52][128/196]	Time 0.157 (0.149)	Data 0.000 (0.002)	Loss 0.4412 (0.3305)	Acc@1 83.594 (88.614)	Acc@5 99.609 (99.661)
Epoch: [52][192/196]	Time 0.146 (0.151)	Data 0.000 (0.002)	Loss 0.3520 (0.3309)	Acc@1 88.281 (88.652)	Acc@5 100.000 (99.644)
after train
Epoche: [53/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [53][0/196]	Time 0.177 (0.177)	Data 0.268 (0.268)	Loss 0.2636 (0.2636)	Acc@1 89.453 (89.453)	Acc@5 100.000 (100.000)
Epoch: [53][64/196]	Time 0.149 (0.150)	Data 0.000 (0.004)	Loss 0.3757 (0.3238)	Acc@1 86.719 (88.642)	Acc@5 99.219 (99.639)
Epoch: [53][128/196]	Time 0.099 (0.150)	Data 0.000 (0.002)	Loss 0.4226 (0.3245)	Acc@1 85.547 (88.760)	Acc@5 99.219 (99.618)
Epoch: [53][192/196]	Time 0.160 (0.149)	Data 0.000 (0.002)	Loss 0.3702 (0.3353)	Acc@1 87.109 (88.484)	Acc@5 99.609 (99.597)
after train
Epoche: [54/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [54][0/196]	Time 0.180 (0.180)	Data 0.273 (0.273)	Loss 0.3967 (0.3967)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [54][64/196]	Time 0.143 (0.155)	Data 0.000 (0.004)	Loss 0.2550 (0.3214)	Acc@1 90.234 (88.972)	Acc@5 99.609 (99.669)
Epoch: [54][128/196]	Time 0.119 (0.154)	Data 0.000 (0.002)	Loss 0.4094 (0.3294)	Acc@1 83.594 (88.511)	Acc@5 100.000 (99.643)
Epoch: [54][192/196]	Time 0.144 (0.152)	Data 0.000 (0.002)	Loss 0.4809 (0.3312)	Acc@1 85.547 (88.484)	Acc@5 99.219 (99.644)
after train
Epoche: [55/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [55][0/196]	Time 0.183 (0.183)	Data 0.265 (0.265)	Loss 0.3218 (0.3218)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [55][64/196]	Time 0.154 (0.154)	Data 0.000 (0.004)	Loss 0.3689 (0.3381)	Acc@1 87.891 (88.095)	Acc@5 99.609 (99.657)
Epoch: [55][128/196]	Time 0.142 (0.153)	Data 0.000 (0.002)	Loss 0.2219 (0.3327)	Acc@1 92.188 (88.469)	Acc@5 100.000 (99.634)
Epoch: [55][192/196]	Time 0.143 (0.154)	Data 0.000 (0.002)	Loss 0.2652 (0.3337)	Acc@1 91.797 (88.597)	Acc@5 99.219 (99.624)
after train
Epoche: [56/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [56][0/196]	Time 0.130 (0.130)	Data 0.250 (0.250)	Loss 0.2851 (0.2851)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [56][64/196]	Time 0.166 (0.146)	Data 0.000 (0.004)	Loss 0.3485 (0.3184)	Acc@1 85.547 (88.990)	Acc@5 100.000 (99.627)
Epoch: [56][128/196]	Time 0.158 (0.149)	Data 0.000 (0.002)	Loss 0.2495 (0.3277)	Acc@1 92.969 (88.527)	Acc@5 99.609 (99.609)
Epoch: [56][192/196]	Time 0.162 (0.150)	Data 0.000 (0.002)	Loss 0.3471 (0.3323)	Acc@1 87.500 (88.429)	Acc@5 99.609 (99.613)
after train
Epoche: [57/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [57][0/196]	Time 0.164 (0.164)	Data 0.271 (0.271)	Loss 0.3581 (0.3581)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [57][64/196]	Time 0.151 (0.148)	Data 0.000 (0.004)	Loss 0.3168 (0.3182)	Acc@1 88.672 (88.828)	Acc@5 100.000 (99.627)
Epoch: [57][128/196]	Time 0.154 (0.151)	Data 0.000 (0.002)	Loss 0.2726 (0.3226)	Acc@1 89.844 (88.763)	Acc@5 99.609 (99.621)
Epoch: [57][192/196]	Time 0.157 (0.152)	Data 0.000 (0.002)	Loss 0.3241 (0.3237)	Acc@1 88.672 (88.753)	Acc@5 99.219 (99.650)
after train
Epoche: [58/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [58][0/196]	Time 0.181 (0.181)	Data 0.238 (0.238)	Loss 0.3397 (0.3397)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [58][64/196]	Time 0.154 (0.154)	Data 0.000 (0.004)	Loss 0.4017 (0.3211)	Acc@1 85.547 (89.014)	Acc@5 99.609 (99.657)
Epoch: [58][128/196]	Time 0.154 (0.148)	Data 0.000 (0.002)	Loss 0.3621 (0.3280)	Acc@1 88.672 (88.696)	Acc@5 98.828 (99.609)
Epoch: [58][192/196]	Time 0.158 (0.150)	Data 0.000 (0.001)	Loss 0.3716 (0.3308)	Acc@1 88.672 (88.619)	Acc@5 100.000 (99.597)
after train
Epoche: [59/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [59][0/196]	Time 0.175 (0.175)	Data 0.302 (0.302)	Loss 0.2942 (0.2942)	Acc@1 91.016 (91.016)	Acc@5 99.219 (99.219)
Epoch: [59][64/196]	Time 0.163 (0.150)	Data 0.000 (0.005)	Loss 0.2736 (0.3333)	Acc@1 89.844 (88.492)	Acc@5 99.219 (99.597)
Epoch: [59][128/196]	Time 0.152 (0.152)	Data 0.000 (0.003)	Loss 0.2883 (0.3229)	Acc@1 89.453 (88.872)	Acc@5 100.000 (99.621)
Epoch: [59][192/196]	Time 0.155 (0.151)	Data 0.000 (0.002)	Loss 0.4431 (0.3250)	Acc@1 85.938 (88.820)	Acc@5 100.000 (99.646)
after train
Epoche: [60/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [60][0/196]	Time 0.165 (0.165)	Data 0.278 (0.278)	Loss 0.3017 (0.3017)	Acc@1 91.016 (91.016)	Acc@5 99.609 (99.609)
Epoch: [60][64/196]	Time 0.150 (0.151)	Data 0.000 (0.005)	Loss 0.3560 (0.3194)	Acc@1 88.672 (89.147)	Acc@5 99.219 (99.609)
Epoch: [60][128/196]	Time 0.158 (0.152)	Data 0.000 (0.002)	Loss 0.3441 (0.3188)	Acc@1 89.062 (89.026)	Acc@5 99.609 (99.634)
Epoch: [60][192/196]	Time 0.157 (0.150)	Data 0.000 (0.002)	Loss 0.3544 (0.3275)	Acc@1 87.500 (88.678)	Acc@5 98.828 (99.611)
after train
Epoche: [61/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [61][0/196]	Time 0.187 (0.187)	Data 0.257 (0.257)	Loss 0.2583 (0.2583)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [61][64/196]	Time 0.159 (0.154)	Data 0.000 (0.004)	Loss 0.3552 (0.3159)	Acc@1 88.281 (89.062)	Acc@5 99.609 (99.730)
Epoch: [61][128/196]	Time 0.124 (0.152)	Data 0.000 (0.002)	Loss 0.3024 (0.3275)	Acc@1 88.281 (88.611)	Acc@5 99.609 (99.676)
Epoch: [61][192/196]	Time 0.145 (0.152)	Data 0.000 (0.002)	Loss 0.2792 (0.3261)	Acc@1 91.016 (88.698)	Acc@5 99.219 (99.664)
after train
Epoche: [62/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [62][0/196]	Time 0.188 (0.188)	Data 0.277 (0.277)	Loss 0.2638 (0.2638)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [62][64/196]	Time 0.158 (0.150)	Data 0.000 (0.004)	Loss 0.3226 (0.3252)	Acc@1 88.281 (88.816)	Acc@5 99.609 (99.663)
Epoch: [62][128/196]	Time 0.146 (0.150)	Data 0.000 (0.002)	Loss 0.3077 (0.3260)	Acc@1 91.016 (88.666)	Acc@5 99.609 (99.676)
Epoch: [62][192/196]	Time 0.170 (0.151)	Data 0.000 (0.002)	Loss 0.3714 (0.3216)	Acc@1 87.109 (88.834)	Acc@5 100.000 (99.682)
after train
Epoche: [63/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [63][0/196]	Time 0.183 (0.183)	Data 0.289 (0.289)	Loss 0.2998 (0.2998)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [63][64/196]	Time 0.120 (0.146)	Data 0.000 (0.005)	Loss 0.3751 (0.3084)	Acc@1 87.891 (89.177)	Acc@5 100.000 (99.736)
Epoch: [63][128/196]	Time 0.156 (0.151)	Data 0.000 (0.002)	Loss 0.2786 (0.3172)	Acc@1 91.797 (88.963)	Acc@5 99.609 (99.694)
Epoch: [63][192/196]	Time 0.161 (0.152)	Data 0.000 (0.002)	Loss 0.2567 (0.3208)	Acc@1 92.578 (88.836)	Acc@5 99.609 (99.670)
after train
Epoche: [64/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [64][0/196]	Time 0.165 (0.165)	Data 0.268 (0.268)	Loss 0.2909 (0.2909)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [64][64/196]	Time 0.152 (0.154)	Data 0.000 (0.004)	Loss 0.3208 (0.3079)	Acc@1 89.062 (89.243)	Acc@5 100.000 (99.742)
Epoch: [64][128/196]	Time 0.149 (0.152)	Data 0.000 (0.002)	Loss 0.4116 (0.3169)	Acc@1 86.328 (88.932)	Acc@5 100.000 (99.724)
Epoch: [64][192/196]	Time 0.144 (0.151)	Data 0.000 (0.002)	Loss 0.3297 (0.3240)	Acc@1 88.281 (88.747)	Acc@5 99.219 (99.719)
after train
Epoche: [65/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [65][0/196]	Time 0.182 (0.182)	Data 0.287 (0.287)	Loss 0.3525 (0.3525)	Acc@1 87.891 (87.891)	Acc@5 99.219 (99.219)
Epoch: [65][64/196]	Time 0.138 (0.153)	Data 0.000 (0.005)	Loss 0.3372 (0.3219)	Acc@1 89.453 (89.050)	Acc@5 99.609 (99.663)
Epoch: [65][128/196]	Time 0.145 (0.152)	Data 0.000 (0.002)	Loss 0.3198 (0.3250)	Acc@1 87.891 (88.766)	Acc@5 99.609 (99.673)
Epoch: [65][192/196]	Time 0.144 (0.151)	Data 0.000 (0.002)	Loss 0.4982 (0.3257)	Acc@1 84.375 (88.791)	Acc@5 98.828 (99.646)
after train
Epoche: [66/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [66][0/196]	Time 0.188 (0.188)	Data 0.270 (0.270)	Loss 0.2546 (0.2546)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [66][64/196]	Time 0.152 (0.150)	Data 0.000 (0.004)	Loss 0.2717 (0.3042)	Acc@1 89.844 (89.489)	Acc@5 99.609 (99.663)
Epoch: [66][128/196]	Time 0.150 (0.152)	Data 0.000 (0.002)	Loss 0.4311 (0.3174)	Acc@1 87.109 (89.111)	Acc@5 99.219 (99.664)
Epoch: [66][192/196]	Time 0.151 (0.151)	Data 0.000 (0.002)	Loss 0.3634 (0.3170)	Acc@1 89.062 (89.208)	Acc@5 99.609 (99.634)
after train
Epoche: [67/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [67][0/196]	Time 0.182 (0.182)	Data 0.278 (0.278)	Loss 0.1884 (0.1884)	Acc@1 93.750 (93.750)	Acc@5 99.609 (99.609)
Epoch: [67][64/196]	Time 0.147 (0.150)	Data 0.000 (0.005)	Loss 0.3200 (0.3018)	Acc@1 90.234 (89.663)	Acc@5 100.000 (99.663)
Epoch: [67][128/196]	Time 0.152 (0.151)	Data 0.000 (0.002)	Loss 0.3808 (0.3152)	Acc@1 87.109 (89.059)	Acc@5 100.000 (99.694)
Epoch: [67][192/196]	Time 0.160 (0.151)	Data 0.000 (0.002)	Loss 0.3905 (0.3197)	Acc@1 87.500 (88.965)	Acc@5 98.828 (99.694)
after train
Epoche: [68/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [68][0/196]	Time 0.174 (0.174)	Data 0.285 (0.285)	Loss 0.3743 (0.3743)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [68][64/196]	Time 0.166 (0.149)	Data 0.000 (0.005)	Loss 0.2550 (0.2985)	Acc@1 91.406 (89.639)	Acc@5 100.000 (99.657)
Epoch: [68][128/196]	Time 0.150 (0.150)	Data 0.000 (0.002)	Loss 0.2449 (0.3110)	Acc@1 92.578 (89.311)	Acc@5 100.000 (99.673)
Epoch: [68][192/196]	Time 0.155 (0.151)	Data 0.000 (0.002)	Loss 0.3523 (0.3139)	Acc@1 87.109 (89.301)	Acc@5 99.219 (99.684)
after train
Epoche: [69/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [69][0/196]	Time 0.168 (0.168)	Data 0.254 (0.254)	Loss 0.2458 (0.2458)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [69][64/196]	Time 0.114 (0.154)	Data 0.000 (0.004)	Loss 0.2645 (0.3155)	Acc@1 91.797 (89.297)	Acc@5 100.000 (99.724)
Epoch: [69][128/196]	Time 0.165 (0.149)	Data 0.000 (0.002)	Loss 0.2923 (0.3161)	Acc@1 90.625 (89.172)	Acc@5 99.219 (99.679)
Epoch: [69][192/196]	Time 0.155 (0.150)	Data 0.000 (0.002)	Loss 0.3314 (0.3178)	Acc@1 88.672 (89.105)	Acc@5 99.219 (99.662)
after train
Epoche: [70/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [70][0/196]	Time 0.165 (0.165)	Data 0.260 (0.260)	Loss 0.3279 (0.3279)	Acc@1 89.844 (89.844)	Acc@5 99.219 (99.219)
Epoch: [70][64/196]	Time 0.152 (0.150)	Data 0.000 (0.004)	Loss 0.3367 (0.3239)	Acc@1 86.328 (89.075)	Acc@5 99.609 (99.585)
Epoch: [70][128/196]	Time 0.165 (0.149)	Data 0.000 (0.002)	Loss 0.2868 (0.3183)	Acc@1 89.062 (89.235)	Acc@5 99.609 (99.649)
Epoch: [70][192/196]	Time 0.140 (0.151)	Data 0.000 (0.002)	Loss 0.2994 (0.3180)	Acc@1 90.625 (89.115)	Acc@5 100.000 (99.656)
after train
Epoche: [71/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [71][0/196]	Time 0.165 (0.165)	Data 0.277 (0.277)	Loss 0.3313 (0.3313)	Acc@1 91.016 (91.016)	Acc@5 99.609 (99.609)
Epoch: [71][64/196]	Time 0.138 (0.154)	Data 0.000 (0.004)	Loss 0.4325 (0.3127)	Acc@1 85.156 (89.279)	Acc@5 99.609 (99.700)
Epoch: [71][128/196]	Time 0.134 (0.153)	Data 0.000 (0.002)	Loss 0.2852 (0.3116)	Acc@1 89.453 (89.320)	Acc@5 100.000 (99.682)
Epoch: [71][192/196]	Time 0.145 (0.150)	Data 0.000 (0.002)	Loss 0.2715 (0.3137)	Acc@1 91.016 (89.257)	Acc@5 99.609 (99.658)
after train
Epoche: [72/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [72][0/196]	Time 0.164 (0.164)	Data 0.281 (0.281)	Loss 0.3613 (0.3613)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [72][64/196]	Time 0.153 (0.153)	Data 0.000 (0.005)	Loss 0.3664 (0.3222)	Acc@1 88.281 (89.117)	Acc@5 98.828 (99.603)
Epoch: [72][128/196]	Time 0.152 (0.152)	Data 0.000 (0.002)	Loss 0.2549 (0.3152)	Acc@1 91.797 (89.399)	Acc@5 100.000 (99.646)
Epoch: [72][192/196]	Time 0.149 (0.152)	Data 0.000 (0.002)	Loss 0.2996 (0.3183)	Acc@1 90.234 (89.160)	Acc@5 100.000 (99.658)
after train
Epoche: [73/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [73][0/196]	Time 0.162 (0.162)	Data 0.272 (0.272)	Loss 0.3033 (0.3033)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [73][64/196]	Time 0.155 (0.149)	Data 0.000 (0.004)	Loss 0.3007 (0.3057)	Acc@1 89.062 (89.477)	Acc@5 99.609 (99.724)
Epoch: [73][128/196]	Time 0.141 (0.150)	Data 0.000 (0.002)	Loss 0.3034 (0.3115)	Acc@1 90.234 (89.305)	Acc@5 99.609 (99.697)
Epoch: [73][192/196]	Time 0.151 (0.151)	Data 0.000 (0.002)	Loss 0.3690 (0.3142)	Acc@1 88.672 (89.239)	Acc@5 99.219 (99.698)
after train
Epoche: [74/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [74][0/196]	Time 0.186 (0.186)	Data 0.255 (0.255)	Loss 0.2981 (0.2981)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [74][64/196]	Time 0.157 (0.147)	Data 0.000 (0.004)	Loss 0.2223 (0.3126)	Acc@1 92.188 (89.081)	Acc@5 100.000 (99.706)
Epoch: [74][128/196]	Time 0.152 (0.150)	Data 0.000 (0.002)	Loss 0.3601 (0.3046)	Acc@1 87.891 (89.526)	Acc@5 100.000 (99.700)
Epoch: [74][192/196]	Time 0.147 (0.150)	Data 0.000 (0.002)	Loss 0.4263 (0.3087)	Acc@1 85.938 (89.376)	Acc@5 99.219 (99.676)
after train
Epoche: [75/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [75][0/196]	Time 0.172 (0.172)	Data 0.291 (0.291)	Loss 0.3945 (0.3945)	Acc@1 89.062 (89.062)	Acc@5 99.609 (99.609)
Epoch: [75][64/196]	Time 0.156 (0.153)	Data 0.000 (0.005)	Loss 0.3484 (0.3296)	Acc@1 85.547 (88.606)	Acc@5 100.000 (99.736)
Epoch: [75][128/196]	Time 0.168 (0.150)	Data 0.000 (0.002)	Loss 0.2792 (0.3245)	Acc@1 88.672 (88.811)	Acc@5 100.000 (99.676)
Epoch: [75][192/196]	Time 0.144 (0.150)	Data 0.000 (0.002)	Loss 0.3633 (0.3246)	Acc@1 87.891 (88.876)	Acc@5 100.000 (99.664)
after train
Epoche: [76/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [76][0/196]	Time 0.176 (0.176)	Data 0.305 (0.305)	Loss 0.2660 (0.2660)	Acc@1 91.406 (91.406)	Acc@5 99.609 (99.609)
Epoch: [76][64/196]	Time 0.159 (0.154)	Data 0.000 (0.005)	Loss 0.3360 (0.3082)	Acc@1 88.672 (89.141)	Acc@5 99.609 (99.724)
Epoch: [76][128/196]	Time 0.148 (0.151)	Data 0.000 (0.003)	Loss 0.3255 (0.3138)	Acc@1 91.016 (89.026)	Acc@5 100.000 (99.694)
Epoch: [76][192/196]	Time 0.164 (0.151)	Data 0.000 (0.002)	Loss 0.3194 (0.3189)	Acc@1 88.672 (88.876)	Acc@5 100.000 (99.656)
after train
Epoche: [77/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [77][0/196]	Time 0.197 (0.197)	Data 0.296 (0.296)	Loss 0.2454 (0.2454)	Acc@1 92.188 (92.188)	Acc@5 99.609 (99.609)
Epoch: [77][64/196]	Time 0.155 (0.152)	Data 0.000 (0.005)	Loss 0.2838 (0.3015)	Acc@1 90.234 (89.549)	Acc@5 100.000 (99.742)
Epoch: [77][128/196]	Time 0.152 (0.153)	Data 0.000 (0.003)	Loss 0.3309 (0.3056)	Acc@1 87.891 (89.577)	Acc@5 99.609 (99.703)
Epoch: [77][192/196]	Time 0.093 (0.151)	Data 0.000 (0.002)	Loss 0.2873 (0.3124)	Acc@1 90.234 (89.293)	Acc@5 99.609 (99.666)
after train
Epoche: [78/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [78][0/196]	Time 0.182 (0.182)	Data 0.294 (0.294)	Loss 0.2329 (0.2329)	Acc@1 93.359 (93.359)	Acc@5 99.609 (99.609)
Epoch: [78][64/196]	Time 0.144 (0.153)	Data 0.000 (0.005)	Loss 0.2982 (0.2981)	Acc@1 90.625 (89.820)	Acc@5 99.609 (99.621)
Epoch: [78][128/196]	Time 0.160 (0.154)	Data 0.000 (0.003)	Loss 0.2378 (0.3113)	Acc@1 91.406 (89.241)	Acc@5 100.000 (99.664)
Epoch: [78][192/196]	Time 0.154 (0.153)	Data 0.000 (0.002)	Loss 0.4093 (0.3143)	Acc@1 82.812 (89.148)	Acc@5 98.828 (99.646)
after train
Epoche: [79/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [79][0/196]	Time 0.173 (0.173)	Data 0.358 (0.358)	Loss 0.3208 (0.3208)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [79][64/196]	Time 0.151 (0.151)	Data 0.000 (0.006)	Loss 0.3196 (0.3030)	Acc@1 90.234 (89.549)	Acc@5 100.000 (99.736)
Epoch: [79][128/196]	Time 0.166 (0.151)	Data 0.000 (0.003)	Loss 0.2493 (0.3051)	Acc@1 90.234 (89.508)	Acc@5 100.000 (99.737)
Epoch: [79][192/196]	Time 0.157 (0.152)	Data 0.000 (0.002)	Loss 0.3724 (0.3104)	Acc@1 87.891 (89.350)	Acc@5 99.609 (99.684)
after train
Epoche: [80/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [80][0/196]	Time 0.187 (0.187)	Data 0.271 (0.271)	Loss 0.2887 (0.2887)	Acc@1 91.406 (91.406)	Acc@5 99.609 (99.609)
Epoch: [80][64/196]	Time 0.139 (0.146)	Data 0.000 (0.004)	Loss 0.2315 (0.3044)	Acc@1 92.969 (89.615)	Acc@5 99.609 (99.736)
Epoch: [80][128/196]	Time 0.164 (0.150)	Data 0.000 (0.002)	Loss 0.3319 (0.3081)	Acc@1 86.328 (89.350)	Acc@5 100.000 (99.724)
Epoch: [80][192/196]	Time 0.156 (0.151)	Data 0.000 (0.002)	Loss 0.3105 (0.3106)	Acc@1 90.625 (89.291)	Acc@5 99.219 (99.713)
after train
Epoche: [81/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [81][0/196]	Time 0.185 (0.185)	Data 0.274 (0.274)	Loss 0.3803 (0.3803)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [81][64/196]	Time 0.156 (0.152)	Data 0.000 (0.004)	Loss 0.2874 (0.3078)	Acc@1 89.453 (89.327)	Acc@5 100.000 (99.700)
Epoch: [81][128/196]	Time 0.169 (0.151)	Data 0.000 (0.002)	Loss 0.3749 (0.3121)	Acc@1 85.156 (89.256)	Acc@5 99.609 (99.673)
Epoch: [81][192/196]	Time 0.162 (0.151)	Data 0.000 (0.002)	Loss 0.3852 (0.3113)	Acc@1 91.016 (89.328)	Acc@5 98.828 (99.662)
after train
Epoche: [82/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [82][0/196]	Time 0.183 (0.183)	Data 0.294 (0.294)	Loss 0.3760 (0.3760)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [82][64/196]	Time 0.161 (0.155)	Data 0.000 (0.005)	Loss 0.2633 (0.3110)	Acc@1 91.406 (89.351)	Acc@5 100.000 (99.645)
Epoch: [82][128/196]	Time 0.110 (0.151)	Data 0.000 (0.003)	Loss 0.3377 (0.3154)	Acc@1 87.109 (89.220)	Acc@5 98.828 (99.634)
Epoch: [82][192/196]	Time 0.155 (0.151)	Data 0.000 (0.002)	Loss 0.2572 (0.3126)	Acc@1 90.234 (89.392)	Acc@5 99.609 (99.652)
after train
Epoche: [83/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [83][0/196]	Time 0.171 (0.171)	Data 0.258 (0.258)	Loss 0.2789 (0.2789)	Acc@1 91.406 (91.406)	Acc@5 99.609 (99.609)
Epoch: [83][64/196]	Time 0.145 (0.155)	Data 0.000 (0.004)	Loss 0.2473 (0.3072)	Acc@1 91.016 (89.441)	Acc@5 99.609 (99.742)
Epoch: [83][128/196]	Time 0.152 (0.153)	Data 0.000 (0.002)	Loss 0.3616 (0.3094)	Acc@1 89.453 (89.299)	Acc@5 99.219 (99.703)
Epoch: [83][192/196]	Time 0.160 (0.151)	Data 0.000 (0.002)	Loss 0.2967 (0.3116)	Acc@1 89.453 (89.216)	Acc@5 99.609 (99.698)
after train
Epoche: [84/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [84][0/196]	Time 0.170 (0.170)	Data 0.274 (0.274)	Loss 0.2920 (0.2920)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [84][64/196]	Time 0.156 (0.152)	Data 0.000 (0.004)	Loss 0.2531 (0.3018)	Acc@1 90.625 (89.579)	Acc@5 99.609 (99.681)
Epoch: [84][128/196]	Time 0.159 (0.153)	Data 0.000 (0.002)	Loss 0.2595 (0.2994)	Acc@1 91.406 (89.701)	Acc@5 99.219 (99.697)
Epoch: [84][192/196]	Time 0.159 (0.153)	Data 0.000 (0.002)	Loss 0.3659 (0.3056)	Acc@1 90.625 (89.421)	Acc@5 99.609 (99.684)
after train
Epoche: [85/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [85][0/196]	Time 0.142 (0.142)	Data 0.285 (0.285)	Loss 0.3807 (0.3807)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [85][64/196]	Time 0.164 (0.149)	Data 0.000 (0.005)	Loss 0.3472 (0.3094)	Acc@1 91.016 (89.513)	Acc@5 99.609 (99.597)
Epoch: [85][128/196]	Time 0.157 (0.152)	Data 0.000 (0.002)	Loss 0.1936 (0.3047)	Acc@1 92.578 (89.477)	Acc@5 100.000 (99.625)
Epoch: [85][192/196]	Time 0.167 (0.152)	Data 0.000 (0.002)	Loss 0.4375 (0.3097)	Acc@1 85.156 (89.291)	Acc@5 100.000 (99.648)
after train
Epoche: [86/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [86][0/196]	Time 0.189 (0.189)	Data 0.270 (0.270)	Loss 0.2664 (0.2664)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [86][64/196]	Time 0.155 (0.149)	Data 0.000 (0.004)	Loss 0.2589 (0.2945)	Acc@1 91.016 (89.627)	Acc@5 99.609 (99.712)
Epoch: [86][128/196]	Time 0.140 (0.149)	Data 0.000 (0.002)	Loss 0.3264 (0.2987)	Acc@1 89.453 (89.574)	Acc@5 99.609 (99.715)
Epoch: [86][192/196]	Time 0.154 (0.151)	Data 0.000 (0.002)	Loss 0.2916 (0.3058)	Acc@1 87.891 (89.350)	Acc@5 99.609 (99.702)
after train
Epoche: [87/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [87][0/196]	Time 0.178 (0.178)	Data 0.275 (0.275)	Loss 0.2710 (0.2710)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [87][64/196]	Time 0.160 (0.151)	Data 0.000 (0.004)	Loss 0.2745 (0.3058)	Acc@1 92.188 (89.657)	Acc@5 99.609 (99.621)
Epoch: [87][128/196]	Time 0.150 (0.150)	Data 0.000 (0.002)	Loss 0.2758 (0.3070)	Acc@1 91.406 (89.559)	Acc@5 100.000 (99.640)
Epoch: [87][192/196]	Time 0.157 (0.151)	Data 0.000 (0.002)	Loss 0.2637 (0.3106)	Acc@1 91.797 (89.425)	Acc@5 100.000 (99.658)
after train
Epoche: [88/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [88][0/196]	Time 0.155 (0.155)	Data 0.244 (0.244)	Loss 0.2844 (0.2844)	Acc@1 89.453 (89.453)	Acc@5 100.000 (100.000)
Epoch: [88][64/196]	Time 0.167 (0.151)	Data 0.000 (0.004)	Loss 0.2825 (0.2860)	Acc@1 91.016 (90.276)	Acc@5 100.000 (99.742)
Epoch: [88][128/196]	Time 0.118 (0.152)	Data 0.000 (0.002)	Loss 0.3004 (0.3040)	Acc@1 90.625 (89.668)	Acc@5 99.219 (99.721)
Epoch: [88][192/196]	Time 0.148 (0.150)	Data 0.000 (0.001)	Loss 0.3718 (0.3081)	Acc@1 86.328 (89.421)	Acc@5 99.609 (99.709)
after train
Epoche: [89/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [89][0/196]	Time 0.168 (0.168)	Data 0.304 (0.304)	Loss 0.2831 (0.2831)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [89][64/196]	Time 0.156 (0.154)	Data 0.000 (0.005)	Loss 0.3256 (0.3120)	Acc@1 87.891 (89.171)	Acc@5 99.609 (99.706)
Epoch: [89][128/196]	Time 0.152 (0.152)	Data 0.000 (0.003)	Loss 0.3266 (0.3079)	Acc@1 87.891 (89.480)	Acc@5 99.609 (99.703)
Epoch: [89][192/196]	Time 0.152 (0.150)	Data 0.000 (0.002)	Loss 0.2423 (0.3100)	Acc@1 90.625 (89.405)	Acc@5 100.000 (99.680)
after train
Epoche: [90/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [90][0/196]	Time 0.186 (0.186)	Data 0.303 (0.303)	Loss 0.2620 (0.2620)	Acc@1 91.016 (91.016)	Acc@5 99.609 (99.609)
Epoch: [90][64/196]	Time 0.162 (0.156)	Data 0.000 (0.005)	Loss 0.3703 (0.2932)	Acc@1 89.062 (89.880)	Acc@5 99.609 (99.718)
Epoch: [90][128/196]	Time 0.153 (0.154)	Data 0.000 (0.003)	Loss 0.2634 (0.2968)	Acc@1 92.578 (89.835)	Acc@5 100.000 (99.712)
Epoch: [90][192/196]	Time 0.147 (0.154)	Data 0.000 (0.002)	Loss 0.3183 (0.3031)	Acc@1 88.672 (89.601)	Acc@5 99.609 (99.715)
after train
Epoche: [91/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [91][0/196]	Time 0.162 (0.162)	Data 0.310 (0.310)	Loss 0.2701 (0.2701)	Acc@1 91.406 (91.406)	Acc@5 99.609 (99.609)
Epoch: [91][64/196]	Time 0.153 (0.145)	Data 0.000 (0.005)	Loss 0.2729 (0.3073)	Acc@1 90.234 (89.357)	Acc@5 100.000 (99.621)
Epoch: [91][128/196]	Time 0.156 (0.149)	Data 0.000 (0.003)	Loss 0.3339 (0.3079)	Acc@1 87.500 (89.284)	Acc@5 99.609 (99.628)
Epoch: [91][192/196]	Time 0.161 (0.150)	Data 0.000 (0.002)	Loss 0.2732 (0.3062)	Acc@1 91.406 (89.413)	Acc@5 100.000 (99.660)
after train
Epoche: [92/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [92][0/196]	Time 0.168 (0.168)	Data 0.279 (0.279)	Loss 0.2213 (0.2213)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [92][64/196]	Time 0.168 (0.150)	Data 0.000 (0.005)	Loss 0.2307 (0.2875)	Acc@1 92.188 (90.246)	Acc@5 100.000 (99.700)
Epoch: [92][128/196]	Time 0.155 (0.152)	Data 0.000 (0.002)	Loss 0.3197 (0.2996)	Acc@1 90.625 (89.692)	Acc@5 99.609 (99.700)
Epoch: [92][192/196]	Time 0.150 (0.152)	Data 0.000 (0.002)	Loss 0.4112 (0.3036)	Acc@1 86.328 (89.536)	Acc@5 99.219 (99.715)
after train
Epoche: [93/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [93][0/196]	Time 0.167 (0.167)	Data 0.272 (0.272)	Loss 0.3013 (0.3013)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [93][64/196]	Time 0.162 (0.154)	Data 0.000 (0.004)	Loss 0.2779 (0.2978)	Acc@1 90.234 (89.712)	Acc@5 99.609 (99.802)
Epoch: [93][128/196]	Time 0.163 (0.148)	Data 0.000 (0.002)	Loss 0.4349 (0.3021)	Acc@1 85.547 (89.574)	Acc@5 99.609 (99.779)
Epoch: [93][192/196]	Time 0.143 (0.149)	Data 0.000 (0.002)	Loss 0.2944 (0.3043)	Acc@1 90.234 (89.603)	Acc@5 100.000 (99.735)
after train
Epoche: [94/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [94][0/196]	Time 0.154 (0.154)	Data 0.295 (0.295)	Loss 0.2610 (0.2610)	Acc@1 90.625 (90.625)	Acc@5 98.828 (98.828)
Epoch: [94][64/196]	Time 0.167 (0.151)	Data 0.000 (0.005)	Loss 0.2933 (0.2964)	Acc@1 90.234 (89.802)	Acc@5 99.219 (99.657)
Epoch: [94][128/196]	Time 0.157 (0.152)	Data 0.000 (0.003)	Loss 0.3096 (0.3004)	Acc@1 87.891 (89.695)	Acc@5 100.000 (99.679)
Epoch: [94][192/196]	Time 0.158 (0.151)	Data 0.000 (0.002)	Loss 0.3111 (0.3019)	Acc@1 89.062 (89.662)	Acc@5 99.609 (99.674)
after train
Epoche: [95/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [95][0/196]	Time 0.183 (0.183)	Data 0.249 (0.249)	Loss 0.3404 (0.3404)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [95][64/196]	Time 0.153 (0.151)	Data 0.000 (0.004)	Loss 0.2892 (0.3000)	Acc@1 90.625 (89.712)	Acc@5 100.000 (99.748)
Epoch: [95][128/196]	Time 0.142 (0.152)	Data 0.000 (0.002)	Loss 0.3073 (0.3016)	Acc@1 89.844 (89.559)	Acc@5 100.000 (99.715)
Epoch: [95][192/196]	Time 0.146 (0.151)	Data 0.000 (0.002)	Loss 0.2831 (0.3050)	Acc@1 89.453 (89.467)	Acc@5 99.609 (99.684)
after train
Epoche: [96/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [96][0/196]	Time 0.185 (0.185)	Data 0.278 (0.278)	Loss 0.2549 (0.2549)	Acc@1 91.797 (91.797)	Acc@5 99.609 (99.609)
Epoch: [96][64/196]	Time 0.154 (0.154)	Data 0.000 (0.005)	Loss 0.3096 (0.2971)	Acc@1 87.109 (89.814)	Acc@5 99.609 (99.706)
Epoch: [96][128/196]	Time 0.113 (0.152)	Data 0.000 (0.002)	Loss 0.2438 (0.3031)	Acc@1 91.016 (89.492)	Acc@5 99.219 (99.697)
Epoch: [96][192/196]	Time 0.168 (0.153)	Data 0.000 (0.002)	Loss 0.3716 (0.3048)	Acc@1 85.938 (89.504)	Acc@5 100.000 (99.688)
after train
Epoche: [97/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [97][0/196]	Time 0.174 (0.174)	Data 0.283 (0.283)	Loss 0.2784 (0.2784)	Acc@1 91.797 (91.797)	Acc@5 99.609 (99.609)
Epoch: [97][64/196]	Time 0.162 (0.150)	Data 0.000 (0.005)	Loss 0.2662 (0.2845)	Acc@1 91.797 (90.294)	Acc@5 100.000 (99.681)
Epoch: [97][128/196]	Time 0.160 (0.151)	Data 0.000 (0.002)	Loss 0.2480 (0.2921)	Acc@1 91.016 (90.065)	Acc@5 100.000 (99.685)
Epoch: [97][192/196]	Time 0.157 (0.152)	Data 0.000 (0.002)	Loss 0.3090 (0.3034)	Acc@1 86.328 (89.651)	Acc@5 99.609 (99.670)
after train
Epoche: [98/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [98][0/196]	Time 0.178 (0.178)	Data 0.283 (0.283)	Loss 0.2795 (0.2795)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [98][64/196]	Time 0.136 (0.150)	Data 0.000 (0.005)	Loss 0.3058 (0.2947)	Acc@1 89.453 (89.904)	Acc@5 99.609 (99.694)
Epoch: [98][128/196]	Time 0.154 (0.149)	Data 0.000 (0.002)	Loss 0.2921 (0.3016)	Acc@1 90.625 (89.726)	Acc@5 99.609 (99.700)
Epoch: [98][192/196]	Time 0.150 (0.150)	Data 0.000 (0.002)	Loss 0.2241 (0.3003)	Acc@1 92.578 (89.805)	Acc@5 100.000 (99.700)
after train
Epoche: [99/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [99][0/196]	Time 0.171 (0.171)	Data 0.312 (0.312)	Loss 0.2923 (0.2923)	Acc@1 90.234 (90.234)	Acc@5 99.609 (99.609)
Epoch: [99][64/196]	Time 0.148 (0.154)	Data 0.000 (0.005)	Loss 0.3534 (0.3054)	Acc@1 89.844 (89.609)	Acc@5 99.609 (99.615)
Epoch: [99][128/196]	Time 0.167 (0.152)	Data 0.000 (0.003)	Loss 0.2880 (0.3037)	Acc@1 89.844 (89.568)	Acc@5 100.000 (99.646)
Epoch: [99][192/196]	Time 0.169 (0.151)	Data 0.000 (0.002)	Loss 0.3751 (0.3059)	Acc@1 87.891 (89.506)	Acc@5 100.000 (99.682)
after train
Epoche: [100/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [100][0/196]	Time 0.162 (0.162)	Data 0.275 (0.275)	Loss 0.3121 (0.3121)	Acc@1 90.234 (90.234)	Acc@5 99.609 (99.609)
Epoch: [100][64/196]	Time 0.144 (0.154)	Data 0.000 (0.004)	Loss 0.3266 (0.3069)	Acc@1 88.281 (89.597)	Acc@5 99.609 (99.688)
Epoch: [100][128/196]	Time 0.169 (0.152)	Data 0.000 (0.002)	Loss 0.3379 (0.3015)	Acc@1 88.672 (89.701)	Acc@5 100.000 (99.709)
Epoch: [100][192/196]	Time 0.145 (0.151)	Data 0.000 (0.002)	Loss 0.3185 (0.3042)	Acc@1 88.672 (89.546)	Acc@5 99.609 (99.711)
after train
Epoche: [101/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [101][0/196]	Time 0.176 (0.176)	Data 0.301 (0.301)	Loss 0.2466 (0.2466)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [101][64/196]	Time 0.148 (0.150)	Data 0.000 (0.005)	Loss 0.2933 (0.2929)	Acc@1 90.234 (89.814)	Acc@5 99.609 (99.736)
Epoch: [101][128/196]	Time 0.155 (0.152)	Data 0.000 (0.003)	Loss 0.3030 (0.2979)	Acc@1 89.453 (89.638)	Acc@5 99.609 (99.761)
Epoch: [101][192/196]	Time 0.136 (0.152)	Data 0.000 (0.002)	Loss 0.3070 (0.3015)	Acc@1 87.891 (89.599)	Acc@5 99.609 (99.723)
after train
Epoche: [102/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [102][0/196]	Time 0.137 (0.137)	Data 0.314 (0.314)	Loss 0.3099 (0.3099)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [102][64/196]	Time 0.146 (0.152)	Data 0.000 (0.005)	Loss 0.3667 (0.2870)	Acc@1 84.766 (90.144)	Acc@5 100.000 (99.700)
Epoch: [102][128/196]	Time 0.148 (0.152)	Data 0.000 (0.003)	Loss 0.3086 (0.2936)	Acc@1 89.844 (89.974)	Acc@5 100.000 (99.724)
Epoch: [102][192/196]	Time 0.163 (0.152)	Data 0.000 (0.002)	Loss 0.2914 (0.2961)	Acc@1 90.625 (89.832)	Acc@5 100.000 (99.727)
after train
Epoche: [103/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [103][0/196]	Time 0.158 (0.158)	Data 0.325 (0.325)	Loss 0.2362 (0.2362)	Acc@1 92.188 (92.188)	Acc@5 99.609 (99.609)
Epoch: [103][64/196]	Time 0.144 (0.148)	Data 0.000 (0.005)	Loss 0.3168 (0.2979)	Acc@1 89.453 (89.639)	Acc@5 99.609 (99.766)
Epoch: [103][128/196]	Time 0.156 (0.148)	Data 0.000 (0.003)	Loss 0.3115 (0.3009)	Acc@1 89.062 (89.644)	Acc@5 99.219 (99.709)
Epoch: [103][192/196]	Time 0.159 (0.150)	Data 0.000 (0.002)	Loss 0.3668 (0.2999)	Acc@1 86.719 (89.684)	Acc@5 99.609 (99.709)
after train
Epoche: [104/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [104][0/196]	Time 0.185 (0.185)	Data 0.242 (0.242)	Loss 0.2270 (0.2270)	Acc@1 92.188 (92.188)	Acc@5 99.609 (99.609)
Epoch: [104][64/196]	Time 0.154 (0.154)	Data 0.000 (0.004)	Loss 0.2346 (0.2962)	Acc@1 91.406 (89.838)	Acc@5 100.000 (99.724)
Epoch: [104][128/196]	Time 0.176 (0.149)	Data 0.000 (0.002)	Loss 0.2435 (0.2977)	Acc@1 91.797 (89.659)	Acc@5 100.000 (99.682)
Epoch: [104][192/196]	Time 0.159 (0.150)	Data 0.000 (0.001)	Loss 0.2229 (0.3027)	Acc@1 93.359 (89.481)	Acc@5 100.000 (99.686)
after train
Epoche: [105/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [105][0/196]	Time 0.178 (0.178)	Data 0.281 (0.281)	Loss 0.3299 (0.3299)	Acc@1 91.016 (91.016)	Acc@5 99.219 (99.219)
Epoch: [105][64/196]	Time 0.158 (0.151)	Data 0.000 (0.005)	Loss 0.3169 (0.2808)	Acc@1 89.453 (90.234)	Acc@5 99.609 (99.766)
Epoch: [105][128/196]	Time 0.136 (0.150)	Data 0.000 (0.002)	Loss 0.2959 (0.2893)	Acc@1 87.891 (89.983)	Acc@5 100.000 (99.730)
Epoch: [105][192/196]	Time 0.146 (0.151)	Data 0.000 (0.002)	Loss 0.2320 (0.2968)	Acc@1 92.188 (89.674)	Acc@5 99.219 (99.723)
after train
Epoche: [106/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [106][0/196]	Time 0.160 (0.160)	Data 0.269 (0.269)	Loss 0.4238 (0.4238)	Acc@1 83.984 (83.984)	Acc@5 100.000 (100.000)
Epoch: [106][64/196]	Time 0.145 (0.153)	Data 0.000 (0.004)	Loss 0.3056 (0.3018)	Acc@1 89.062 (89.447)	Acc@5 100.000 (99.754)
Epoch: [106][128/196]	Time 0.152 (0.153)	Data 0.000 (0.002)	Loss 0.3483 (0.2947)	Acc@1 85.938 (89.759)	Acc@5 99.609 (99.758)
Epoch: [106][192/196]	Time 0.156 (0.151)	Data 0.000 (0.002)	Loss 0.3299 (0.3004)	Acc@1 89.844 (89.560)	Acc@5 99.609 (99.735)
after train
Epoche: [107/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [107][0/196]	Time 0.196 (0.196)	Data 0.297 (0.297)	Loss 0.4045 (0.4045)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [107][64/196]	Time 0.152 (0.153)	Data 0.000 (0.005)	Loss 0.2950 (0.2984)	Acc@1 90.234 (89.976)	Acc@5 100.000 (99.754)
Epoch: [107][128/196]	Time 0.154 (0.151)	Data 0.000 (0.003)	Loss 0.3587 (0.3003)	Acc@1 86.719 (89.786)	Acc@5 99.609 (99.740)
Epoch: [107][192/196]	Time 0.168 (0.152)	Data 0.000 (0.002)	Loss 0.3725 (0.3031)	Acc@1 85.547 (89.591)	Acc@5 99.609 (99.733)
after train
Epoche: [108/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [108][0/196]	Time 0.161 (0.161)	Data 0.272 (0.272)	Loss 0.2236 (0.2236)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [108][64/196]	Time 0.141 (0.148)	Data 0.000 (0.004)	Loss 0.3533 (0.2886)	Acc@1 89.453 (90.138)	Acc@5 99.609 (99.712)
Epoch: [108][128/196]	Time 0.152 (0.151)	Data 0.000 (0.002)	Loss 0.2587 (0.2885)	Acc@1 91.797 (90.056)	Acc@5 100.000 (99.700)
Epoch: [108][192/196]	Time 0.154 (0.152)	Data 0.000 (0.002)	Loss 0.2284 (0.2931)	Acc@1 89.844 (89.929)	Acc@5 100.000 (99.709)
after train
Epoche: [109/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [109][0/196]	Time 0.150 (0.150)	Data 0.256 (0.256)	Loss 0.2450 (0.2450)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [109][64/196]	Time 0.162 (0.146)	Data 0.000 (0.004)	Loss 0.3149 (0.2917)	Acc@1 87.891 (90.126)	Acc@5 99.609 (99.778)
Epoch: [109][128/196]	Time 0.158 (0.150)	Data 0.000 (0.002)	Loss 0.3009 (0.2885)	Acc@1 89.844 (90.280)	Acc@5 99.609 (99.764)
Epoch: [109][192/196]	Time 0.151 (0.150)	Data 0.000 (0.002)	Loss 0.3852 (0.2925)	Acc@1 86.328 (90.101)	Acc@5 99.609 (99.735)
after train
Epoche: [110/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [110][0/196]	Time 0.177 (0.177)	Data 0.280 (0.280)	Loss 0.3340 (0.3340)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [110][64/196]	Time 0.152 (0.153)	Data 0.000 (0.005)	Loss 0.2860 (0.2951)	Acc@1 90.234 (90.006)	Acc@5 99.609 (99.730)
Epoch: [110][128/196]	Time 0.117 (0.150)	Data 0.000 (0.002)	Loss 0.2960 (0.2918)	Acc@1 89.844 (90.128)	Acc@5 99.609 (99.724)
Epoch: [110][192/196]	Time 0.144 (0.151)	Data 0.000 (0.002)	Loss 0.2782 (0.2998)	Acc@1 90.625 (89.797)	Acc@5 99.609 (99.698)
after train
Epoche: [111/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [111][0/196]	Time 0.175 (0.175)	Data 0.308 (0.308)	Loss 0.2765 (0.2765)	Acc@1 90.234 (90.234)	Acc@5 99.609 (99.609)
Epoch: [111][64/196]	Time 0.150 (0.154)	Data 0.000 (0.005)	Loss 0.3052 (0.2836)	Acc@1 90.625 (90.282)	Acc@5 100.000 (99.718)
Epoch: [111][128/196]	Time 0.138 (0.152)	Data 0.000 (0.003)	Loss 0.2510 (0.2952)	Acc@1 93.750 (89.698)	Acc@5 100.000 (99.737)
Epoch: [111][192/196]	Time 0.163 (0.151)	Data 0.000 (0.002)	Loss 0.3355 (0.3003)	Acc@1 87.500 (89.589)	Acc@5 99.609 (99.729)
after train
Epoche: [112/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [112][0/196]	Time 0.171 (0.171)	Data 0.284 (0.284)	Loss 0.2873 (0.2873)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [112][64/196]	Time 0.161 (0.150)	Data 0.000 (0.005)	Loss 0.2912 (0.2934)	Acc@1 88.281 (90.192)	Acc@5 100.000 (99.681)
Epoch: [112][128/196]	Time 0.166 (0.152)	Data 0.000 (0.002)	Loss 0.3162 (0.2952)	Acc@1 89.453 (89.898)	Acc@5 100.000 (99.706)
Epoch: [112][192/196]	Time 0.164 (0.150)	Data 0.000 (0.002)	Loss 0.3818 (0.2975)	Acc@1 87.109 (89.803)	Acc@5 100.000 (99.729)
after train
Epoche: [113/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [113][0/196]	Time 0.171 (0.171)	Data 0.275 (0.275)	Loss 0.2181 (0.2181)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [113][64/196]	Time 0.146 (0.155)	Data 0.000 (0.004)	Loss 0.3124 (0.2993)	Acc@1 88.281 (89.567)	Acc@5 99.609 (99.718)
Epoch: [113][128/196]	Time 0.159 (0.154)	Data 0.000 (0.002)	Loss 0.3493 (0.3013)	Acc@1 89.062 (89.605)	Acc@5 98.828 (99.694)
Epoch: [113][192/196]	Time 0.155 (0.153)	Data 0.000 (0.002)	Loss 0.2599 (0.3003)	Acc@1 91.016 (89.706)	Acc@5 100.000 (99.702)
after train
Epoche: [114/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [114][0/196]	Time 0.171 (0.171)	Data 0.243 (0.243)	Loss 0.2359 (0.2359)	Acc@1 93.750 (93.750)	Acc@5 99.609 (99.609)
Epoch: [114][64/196]	Time 0.152 (0.150)	Data 0.000 (0.004)	Loss 0.3264 (0.2913)	Acc@1 87.500 (89.880)	Acc@5 99.219 (99.706)
Epoch: [114][128/196]	Time 0.142 (0.150)	Data 0.000 (0.002)	Loss 0.2218 (0.2946)	Acc@1 92.188 (89.765)	Acc@5 99.609 (99.697)
Epoch: [114][192/196]	Time 0.164 (0.151)	Data 0.000 (0.001)	Loss 0.2579 (0.2979)	Acc@1 92.969 (89.722)	Acc@5 99.609 (99.698)
after train
Epoche: [115/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [115][0/196]	Time 0.189 (0.189)	Data 0.247 (0.247)	Loss 0.3792 (0.3792)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [115][64/196]	Time 0.149 (0.145)	Data 0.000 (0.004)	Loss 0.3508 (0.2984)	Acc@1 88.672 (89.441)	Acc@5 100.000 (99.688)
Epoch: [115][128/196]	Time 0.143 (0.149)	Data 0.000 (0.002)	Loss 0.2351 (0.2958)	Acc@1 92.578 (89.711)	Acc@5 100.000 (99.673)
Epoch: [115][192/196]	Time 0.151 (0.151)	Data 0.000 (0.002)	Loss 0.3430 (0.2975)	Acc@1 90.234 (89.682)	Acc@5 99.609 (99.656)
after train
Epoche: [116/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [116][0/196]	Time 0.170 (0.170)	Data 0.285 (0.285)	Loss 0.3349 (0.3349)	Acc@1 90.234 (90.234)	Acc@5 99.609 (99.609)
Epoch: [116][64/196]	Time 0.156 (0.153)	Data 0.000 (0.005)	Loss 0.3326 (0.2829)	Acc@1 88.281 (90.391)	Acc@5 100.000 (99.706)
Epoch: [116][128/196]	Time 0.145 (0.151)	Data 0.000 (0.002)	Loss 0.2446 (0.2901)	Acc@1 91.797 (90.031)	Acc@5 99.219 (99.673)
Epoch: [116][192/196]	Time 0.163 (0.151)	Data 0.000 (0.002)	Loss 0.3091 (0.2975)	Acc@1 88.672 (89.779)	Acc@5 99.609 (99.664)
after train
Epoche: [117/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [117][0/196]	Time 0.175 (0.175)	Data 0.255 (0.255)	Loss 0.3092 (0.3092)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [117][64/196]	Time 0.139 (0.153)	Data 0.000 (0.004)	Loss 0.2698 (0.2828)	Acc@1 91.016 (90.451)	Acc@5 100.000 (99.706)
Epoch: [117][128/196]	Time 0.118 (0.152)	Data 0.000 (0.002)	Loss 0.3485 (0.2921)	Acc@1 89.453 (90.101)	Acc@5 100.000 (99.676)
Epoch: [117][192/196]	Time 0.160 (0.151)	Data 0.000 (0.002)	Loss 0.4260 (0.2987)	Acc@1 83.203 (89.730)	Acc@5 98.438 (99.676)
after train
Epoche: [118/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [118][0/196]	Time 0.159 (0.159)	Data 0.299 (0.299)	Loss 0.3730 (0.3730)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [118][64/196]	Time 0.160 (0.155)	Data 0.000 (0.005)	Loss 0.2405 (0.2965)	Acc@1 92.969 (89.808)	Acc@5 100.000 (99.802)
Epoch: [118][128/196]	Time 0.157 (0.153)	Data 0.000 (0.003)	Loss 0.3793 (0.2926)	Acc@1 87.500 (89.950)	Acc@5 100.000 (99.746)
Epoch: [118][192/196]	Time 0.107 (0.152)	Data 0.000 (0.002)	Loss 0.2017 (0.2982)	Acc@1 93.750 (89.830)	Acc@5 100.000 (99.713)
after train
Epoche: [119/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [119][0/196]	Time 0.188 (0.188)	Data 0.258 (0.258)	Loss 0.2793 (0.2793)	Acc@1 89.453 (89.453)	Acc@5 100.000 (100.000)
Epoch: [119][64/196]	Time 0.160 (0.150)	Data 0.000 (0.004)	Loss 0.3187 (0.2725)	Acc@1 89.844 (90.535)	Acc@5 99.609 (99.808)
Epoch: [119][128/196]	Time 0.149 (0.152)	Data 0.000 (0.002)	Loss 0.3203 (0.2806)	Acc@1 89.453 (90.295)	Acc@5 100.000 (99.812)
Epoch: [119][192/196]	Time 0.138 (0.152)	Data 0.000 (0.002)	Loss 0.3389 (0.2859)	Acc@1 87.891 (90.113)	Acc@5 100.000 (99.765)
after train
Epoche: [120/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [120][0/196]	Time 0.187 (0.187)	Data 0.249 (0.249)	Loss 0.2479 (0.2479)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [120][64/196]	Time 0.163 (0.149)	Data 0.000 (0.004)	Loss 0.2254 (0.2731)	Acc@1 90.625 (90.691)	Acc@5 100.000 (99.706)
Epoch: [120][128/196]	Time 0.157 (0.152)	Data 0.000 (0.002)	Loss 0.3281 (0.2857)	Acc@1 88.672 (90.253)	Acc@5 99.609 (99.685)
Epoch: [120][192/196]	Time 0.150 (0.151)	Data 0.000 (0.002)	Loss 0.2990 (0.2896)	Acc@1 89.062 (90.093)	Acc@5 99.609 (99.696)
after train
Epoche: [121/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [121][0/196]	Time 0.180 (0.180)	Data 0.275 (0.275)	Loss 0.2044 (0.2044)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [121][64/196]	Time 0.153 (0.151)	Data 0.000 (0.004)	Loss 0.3686 (0.2802)	Acc@1 87.500 (90.294)	Acc@5 99.219 (99.778)
Epoch: [121][128/196]	Time 0.161 (0.149)	Data 0.000 (0.002)	Loss 0.2235 (0.2899)	Acc@1 92.188 (89.962)	Acc@5 100.000 (99.727)
Epoch: [121][192/196]	Time 0.144 (0.150)	Data 0.000 (0.002)	Loss 0.2080 (0.2916)	Acc@1 94.531 (89.943)	Acc@5 99.609 (99.717)
after train
Epoche: [122/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [122][0/196]	Time 0.184 (0.184)	Data 0.312 (0.312)	Loss 0.3522 (0.3522)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [122][64/196]	Time 0.153 (0.150)	Data 0.000 (0.005)	Loss 0.2636 (0.2802)	Acc@1 91.016 (90.258)	Acc@5 99.609 (99.778)
Epoch: [122][128/196]	Time 0.151 (0.149)	Data 0.000 (0.003)	Loss 0.3186 (0.2936)	Acc@1 91.797 (89.756)	Acc@5 100.000 (99.743)
Epoch: [122][192/196]	Time 0.137 (0.151)	Data 0.000 (0.002)	Loss 0.2780 (0.2983)	Acc@1 91.406 (89.643)	Acc@5 100.000 (99.719)
after train
Epoche: [123/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [123][0/196]	Time 0.147 (0.147)	Data 0.289 (0.289)	Loss 0.2201 (0.2201)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [123][64/196]	Time 0.161 (0.151)	Data 0.000 (0.005)	Loss 0.2203 (0.2852)	Acc@1 91.797 (90.240)	Acc@5 100.000 (99.784)
Epoch: [123][128/196]	Time 0.151 (0.153)	Data 0.000 (0.002)	Loss 0.2209 (0.2875)	Acc@1 92.969 (90.156)	Acc@5 100.000 (99.740)
Epoch: [123][192/196]	Time 0.156 (0.150)	Data 0.000 (0.002)	Loss 0.2454 (0.2902)	Acc@1 90.625 (90.038)	Acc@5 99.609 (99.731)
after train
Epoche: [124/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [124][0/196]	Time 0.179 (0.179)	Data 0.286 (0.286)	Loss 0.2271 (0.2271)	Acc@1 92.969 (92.969)	Acc@5 99.219 (99.219)
Epoch: [124][64/196]	Time 0.162 (0.155)	Data 0.000 (0.005)	Loss 0.2325 (0.2904)	Acc@1 91.406 (90.102)	Acc@5 99.609 (99.730)
Epoch: [124][128/196]	Time 0.155 (0.153)	Data 0.000 (0.002)	Loss 0.3354 (0.2867)	Acc@1 87.500 (90.201)	Acc@5 100.000 (99.724)
Epoch: [124][192/196]	Time 0.150 (0.153)	Data 0.000 (0.002)	Loss 0.3023 (0.2867)	Acc@1 89.844 (90.200)	Acc@5 100.000 (99.719)
after train
Epoche: [125/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [125][0/196]	Time 0.159 (0.159)	Data 0.281 (0.281)	Loss 0.2587 (0.2587)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [125][64/196]	Time 0.113 (0.154)	Data 0.000 (0.005)	Loss 0.3857 (0.3040)	Acc@1 87.500 (89.603)	Acc@5 100.000 (99.669)
Epoch: [125][128/196]	Time 0.157 (0.153)	Data 0.000 (0.002)	Loss 0.2970 (0.2999)	Acc@1 91.406 (89.650)	Acc@5 99.219 (99.691)
Epoch: [125][192/196]	Time 0.145 (0.153)	Data 0.000 (0.002)	Loss 0.3257 (0.2998)	Acc@1 87.891 (89.607)	Acc@5 100.000 (99.678)
after train
Epoche: [126/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [126][0/196]	Time 0.167 (0.167)	Data 0.317 (0.317)	Loss 0.2744 (0.2744)	Acc@1 91.406 (91.406)	Acc@5 99.609 (99.609)
Epoch: [126][64/196]	Time 0.158 (0.143)	Data 0.000 (0.005)	Loss 0.2853 (0.2824)	Acc@1 89.453 (90.349)	Acc@5 100.000 (99.742)
Epoch: [126][128/196]	Time 0.152 (0.148)	Data 0.000 (0.003)	Loss 0.2895 (0.2891)	Acc@1 92.188 (90.074)	Acc@5 99.609 (99.691)
Epoch: [126][192/196]	Time 0.112 (0.148)	Data 0.000 (0.002)	Loss 0.2115 (0.2887)	Acc@1 91.797 (90.180)	Acc@5 100.000 (99.717)
after train
Epoche: [127/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [127][0/196]	Time 0.144 (0.144)	Data 0.243 (0.243)	Loss 0.3181 (0.3181)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [127][64/196]	Time 0.100 (0.107)	Data 0.000 (0.004)	Loss 0.2816 (0.2859)	Acc@1 91.406 (90.457)	Acc@5 100.000 (99.742)
Epoch: [127][128/196]	Time 0.110 (0.106)	Data 0.000 (0.002)	Loss 0.3065 (0.2909)	Acc@1 89.062 (90.183)	Acc@5 99.609 (99.746)
Epoch: [127][192/196]	Time 0.112 (0.107)	Data 0.000 (0.001)	Loss 0.2911 (0.2973)	Acc@1 89.453 (89.906)	Acc@5 99.609 (99.733)
after train
Epoche: [128/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [128][0/196]	Time 0.137 (0.137)	Data 0.279 (0.279)	Loss 0.2462 (0.2462)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [128][64/196]	Time 0.114 (0.110)	Data 0.000 (0.005)	Loss 0.2613 (0.2849)	Acc@1 91.016 (90.192)	Acc@5 100.000 (99.700)
Epoch: [128][128/196]	Time 0.112 (0.106)	Data 0.000 (0.002)	Loss 0.2949 (0.2951)	Acc@1 89.844 (89.853)	Acc@5 99.609 (99.679)
Epoch: [128][192/196]	Time 0.110 (0.107)	Data 0.000 (0.002)	Loss 0.3164 (0.2991)	Acc@1 87.109 (89.759)	Acc@5 100.000 (99.688)
after train
Epoche: [129/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [129][0/196]	Time 0.126 (0.126)	Data 0.257 (0.257)	Loss 0.2807 (0.2807)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [129][64/196]	Time 0.114 (0.111)	Data 0.000 (0.004)	Loss 0.2882 (0.3006)	Acc@1 91.016 (89.784)	Acc@5 99.609 (99.633)
Epoch: [129][128/196]	Time 0.081 (0.107)	Data 0.000 (0.002)	Loss 0.2138 (0.2915)	Acc@1 93.359 (90.201)	Acc@5 100.000 (99.649)
Epoch: [129][192/196]	Time 0.110 (0.107)	Data 0.000 (0.002)	Loss 0.2502 (0.2909)	Acc@1 90.625 (90.123)	Acc@5 100.000 (99.676)
after train
Epoche: [130/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [130][0/196]	Time 0.139 (0.139)	Data 0.290 (0.290)	Loss 0.2839 (0.2839)	Acc@1 90.234 (90.234)	Acc@5 99.609 (99.609)
Epoch: [130][64/196]	Time 0.112 (0.109)	Data 0.000 (0.005)	Loss 0.3510 (0.2997)	Acc@1 88.672 (89.627)	Acc@5 100.000 (99.754)
Epoch: [130][128/196]	Time 0.097 (0.109)	Data 0.000 (0.002)	Loss 0.3627 (0.2939)	Acc@1 86.719 (90.028)	Acc@5 99.219 (99.740)
Epoch: [130][192/196]	Time 0.115 (0.107)	Data 0.000 (0.002)	Loss 0.2559 (0.2935)	Acc@1 91.016 (90.014)	Acc@5 100.000 (99.739)
after train
Epoche: [131/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [131][0/196]	Time 0.139 (0.139)	Data 0.241 (0.241)	Loss 0.2080 (0.2080)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [131][64/196]	Time 0.109 (0.111)	Data 0.000 (0.004)	Loss 0.4169 (0.2976)	Acc@1 85.156 (89.712)	Acc@5 99.609 (99.772)
Epoch: [131][128/196]	Time 0.101 (0.111)	Data 0.000 (0.002)	Loss 0.2890 (0.2933)	Acc@1 91.406 (89.919)	Acc@5 99.609 (99.743)
Epoch: [131][192/196]	Time 0.112 (0.108)	Data 0.000 (0.001)	Loss 0.3637 (0.2954)	Acc@1 86.328 (89.896)	Acc@5 100.000 (99.705)
after train
Epoche: [132/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [132][0/196]	Time 0.128 (0.128)	Data 0.242 (0.242)	Loss 0.2205 (0.2205)	Acc@1 92.969 (92.969)	Acc@5 99.609 (99.609)
Epoch: [132][64/196]	Time 0.115 (0.111)	Data 0.000 (0.004)	Loss 0.3375 (0.2761)	Acc@1 87.891 (90.601)	Acc@5 99.609 (99.778)
Epoch: [132][128/196]	Time 0.112 (0.110)	Data 0.000 (0.002)	Loss 0.3063 (0.2887)	Acc@1 89.062 (90.086)	Acc@5 100.000 (99.779)
Epoch: [132][192/196]	Time 0.069 (0.109)	Data 0.000 (0.001)	Loss 0.2436 (0.2948)	Acc@1 91.406 (89.826)	Acc@5 100.000 (99.761)
after train
Epoche: [133/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [133][0/196]	Time 0.133 (0.133)	Data 0.237 (0.237)	Loss 0.2402 (0.2402)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [133][64/196]	Time 0.100 (0.109)	Data 0.000 (0.004)	Loss 0.2635 (0.2750)	Acc@1 88.672 (90.589)	Acc@5 99.609 (99.706)
Epoch: [133][128/196]	Time 0.114 (0.109)	Data 0.000 (0.002)	Loss 0.3319 (0.2870)	Acc@1 89.844 (90.125)	Acc@5 100.000 (99.706)
Epoch: [133][192/196]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3493 (0.2910)	Acc@1 89.062 (89.959)	Acc@5 99.219 (99.709)
after train
Epoche: [134/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [134][0/196]	Time 0.138 (0.138)	Data 0.251 (0.251)	Loss 0.3333 (0.3333)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [134][64/196]	Time 0.111 (0.102)	Data 0.000 (0.004)	Loss 0.2872 (0.2917)	Acc@1 90.234 (89.916)	Acc@5 100.000 (99.790)
Epoch: [134][128/196]	Time 0.098 (0.107)	Data 0.000 (0.002)	Loss 0.1802 (0.2832)	Acc@1 94.922 (90.283)	Acc@5 99.609 (99.773)
Epoch: [134][192/196]	Time 0.117 (0.108)	Data 0.000 (0.002)	Loss 0.3643 (0.2877)	Acc@1 85.938 (90.180)	Acc@5 99.609 (99.743)
after train
Epoche: [135/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [135][0/196]	Time 0.121 (0.121)	Data 0.254 (0.254)	Loss 0.2604 (0.2604)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [135][64/196]	Time 0.113 (0.103)	Data 0.000 (0.004)	Loss 0.2932 (0.2946)	Acc@1 89.844 (90.024)	Acc@5 99.609 (99.681)
Epoch: [135][128/196]	Time 0.113 (0.107)	Data 0.000 (0.002)	Loss 0.2973 (0.2963)	Acc@1 91.797 (89.956)	Acc@5 99.609 (99.673)
Epoch: [135][192/196]	Time 0.115 (0.108)	Data 0.000 (0.002)	Loss 0.3951 (0.3009)	Acc@1 86.719 (89.678)	Acc@5 100.000 (99.705)
after train
Epoche: [136/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [136][0/196]	Time 0.138 (0.138)	Data 0.273 (0.273)	Loss 0.2353 (0.2353)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [136][64/196]	Time 0.072 (0.105)	Data 0.000 (0.004)	Loss 0.3370 (0.2793)	Acc@1 89.844 (90.361)	Acc@5 99.609 (99.724)
Epoch: [136][128/196]	Time 0.112 (0.105)	Data 0.000 (0.002)	Loss 0.2669 (0.2839)	Acc@1 92.188 (90.289)	Acc@5 100.000 (99.718)
Epoch: [136][192/196]	Time 0.110 (0.107)	Data 0.000 (0.002)	Loss 0.3286 (0.2865)	Acc@1 89.453 (90.194)	Acc@5 99.609 (99.729)
after train
Epoche: [137/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [137][0/196]	Time 0.121 (0.121)	Data 0.279 (0.279)	Loss 0.2988 (0.2988)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [137][64/196]	Time 0.110 (0.110)	Data 0.000 (0.005)	Loss 0.3712 (0.2866)	Acc@1 86.719 (90.066)	Acc@5 100.000 (99.748)
Epoch: [137][128/196]	Time 0.108 (0.106)	Data 0.000 (0.002)	Loss 0.2639 (0.2853)	Acc@1 90.234 (90.180)	Acc@5 100.000 (99.767)
Epoch: [137][192/196]	Time 0.116 (0.108)	Data 0.000 (0.002)	Loss 0.2593 (0.2881)	Acc@1 90.625 (90.036)	Acc@5 99.609 (99.751)
after train
Epoche: [138/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [138][0/196]	Time 0.116 (0.116)	Data 0.287 (0.287)	Loss 0.3086 (0.3086)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [138][64/196]	Time 0.091 (0.109)	Data 0.000 (0.005)	Loss 0.2255 (0.2837)	Acc@1 92.578 (90.288)	Acc@5 99.609 (99.742)
Epoch: [138][128/196]	Time 0.103 (0.105)	Data 0.000 (0.002)	Loss 0.2537 (0.2890)	Acc@1 89.844 (90.025)	Acc@5 100.000 (99.730)
Epoch: [138][192/196]	Time 0.113 (0.107)	Data 0.000 (0.002)	Loss 0.2077 (0.2879)	Acc@1 94.141 (90.099)	Acc@5 100.000 (99.715)
after train
Epoche: [139/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [139][0/196]	Time 0.118 (0.118)	Data 0.283 (0.283)	Loss 0.2419 (0.2419)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [139][64/196]	Time 0.113 (0.111)	Data 0.000 (0.005)	Loss 0.4026 (0.2826)	Acc@1 87.109 (90.174)	Acc@5 99.609 (99.784)
Epoch: [139][128/196]	Time 0.096 (0.110)	Data 0.000 (0.002)	Loss 0.3560 (0.2869)	Acc@1 89.453 (89.965)	Acc@5 98.828 (99.815)
Epoch: [139][192/196]	Time 0.120 (0.109)	Data 0.000 (0.002)	Loss 0.3565 (0.2922)	Acc@1 88.281 (89.858)	Acc@5 98.438 (99.751)
after train
Epoche: [140/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [140][0/196]	Time 0.123 (0.123)	Data 0.279 (0.279)	Loss 0.2720 (0.2720)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [140][64/196]	Time 0.111 (0.112)	Data 0.000 (0.005)	Loss 0.3915 (0.2935)	Acc@1 87.500 (89.982)	Acc@5 98.828 (99.748)
Epoch: [140][128/196]	Time 0.115 (0.110)	Data 0.000 (0.002)	Loss 0.3750 (0.2904)	Acc@1 86.328 (90.110)	Acc@5 100.000 (99.715)
Epoch: [140][192/196]	Time 0.111 (0.107)	Data 0.000 (0.002)	Loss 0.2924 (0.2907)	Acc@1 88.672 (90.077)	Acc@5 99.219 (99.715)
after train
Epoche: [141/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [141][0/196]	Time 0.115 (0.115)	Data 0.277 (0.277)	Loss 0.3553 (0.3553)	Acc@1 89.062 (89.062)	Acc@5 99.219 (99.219)
Epoch: [141][64/196]	Time 0.109 (0.111)	Data 0.000 (0.004)	Loss 0.2804 (0.2908)	Acc@1 91.016 (89.928)	Acc@5 100.000 (99.724)
Epoch: [141][128/196]	Time 0.122 (0.111)	Data 0.000 (0.002)	Loss 0.3254 (0.2933)	Acc@1 89.453 (89.901)	Acc@5 99.219 (99.715)
Epoch: [141][192/196]	Time 0.111 (0.108)	Data 0.000 (0.002)	Loss 0.2763 (0.2956)	Acc@1 92.578 (89.826)	Acc@5 100.000 (99.711)
after train
Epoche: [142/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [142][0/196]	Time 0.128 (0.128)	Data 0.274 (0.274)	Loss 0.2574 (0.2574)	Acc@1 92.578 (92.578)	Acc@5 99.219 (99.219)
Epoch: [142][64/196]	Time 0.111 (0.111)	Data 0.000 (0.004)	Loss 0.3933 (0.2772)	Acc@1 85.938 (90.385)	Acc@5 98.438 (99.730)
Epoch: [142][128/196]	Time 0.120 (0.110)	Data 0.000 (0.002)	Loss 0.3267 (0.2849)	Acc@1 88.672 (90.168)	Acc@5 100.000 (99.694)
Epoch: [142][192/196]	Time 0.107 (0.110)	Data 0.000 (0.002)	Loss 0.2424 (0.2846)	Acc@1 91.406 (90.151)	Acc@5 100.000 (99.711)
after train
Epoche: [143/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [143][0/196]	Time 0.091 (0.091)	Data 0.284 (0.284)	Loss 0.3169 (0.3169)	Acc@1 89.062 (89.062)	Acc@5 99.609 (99.609)
Epoch: [143][64/196]	Time 0.113 (0.108)	Data 0.000 (0.005)	Loss 0.3079 (0.2897)	Acc@1 87.500 (89.772)	Acc@5 99.609 (99.718)
Epoch: [143][128/196]	Time 0.114 (0.110)	Data 0.000 (0.002)	Loss 0.3580 (0.2874)	Acc@1 86.719 (89.941)	Acc@5 100.000 (99.700)
Epoch: [143][192/196]	Time 0.103 (0.111)	Data 0.000 (0.002)	Loss 0.2888 (0.2889)	Acc@1 91.016 (89.906)	Acc@5 100.000 (99.717)
after train
Epoche: [144/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [144][0/196]	Time 0.124 (0.124)	Data 0.248 (0.248)	Loss 0.2749 (0.2749)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [144][64/196]	Time 0.113 (0.102)	Data 0.000 (0.004)	Loss 0.1807 (0.2837)	Acc@1 93.750 (90.246)	Acc@5 99.609 (99.754)
Epoch: [144][128/196]	Time 0.110 (0.106)	Data 0.000 (0.002)	Loss 0.3278 (0.2875)	Acc@1 89.453 (90.134)	Acc@5 99.609 (99.721)
Epoch: [144][192/196]	Time 0.115 (0.107)	Data 0.000 (0.002)	Loss 0.2532 (0.2859)	Acc@1 91.797 (90.232)	Acc@5 100.000 (99.729)
after train
Epoche: [145/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [145][0/196]	Time 0.124 (0.124)	Data 0.252 (0.252)	Loss 0.2383 (0.2383)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [145][64/196]	Time 0.116 (0.103)	Data 0.000 (0.004)	Loss 0.3221 (0.2797)	Acc@1 90.625 (90.349)	Acc@5 99.609 (99.694)
Epoch: [145][128/196]	Time 0.101 (0.107)	Data 0.000 (0.002)	Loss 0.3638 (0.2779)	Acc@1 89.062 (90.419)	Acc@5 99.219 (99.761)
Epoch: [145][192/196]	Time 0.112 (0.107)	Data 0.000 (0.002)	Loss 0.2248 (0.2865)	Acc@1 93.359 (90.127)	Acc@5 100.000 (99.719)
after train
Epoche: [146/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [146][0/196]	Time 0.131 (0.131)	Data 0.263 (0.263)	Loss 0.3527 (0.3527)	Acc@1 87.891 (87.891)	Acc@5 100.000 (100.000)
Epoch: [146][64/196]	Time 0.111 (0.107)	Data 0.000 (0.004)	Loss 0.3217 (0.2932)	Acc@1 88.281 (90.024)	Acc@5 99.609 (99.639)
Epoch: [146][128/196]	Time 0.115 (0.106)	Data 0.000 (0.002)	Loss 0.3367 (0.2936)	Acc@1 87.500 (90.019)	Acc@5 100.000 (99.682)
Epoch: [146][192/196]	Time 0.110 (0.108)	Data 0.000 (0.002)	Loss 0.2368 (0.2885)	Acc@1 93.359 (90.162)	Acc@5 100.000 (99.676)
after train
Epoche: [147/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [147][0/196]	Time 0.122 (0.122)	Data 0.284 (0.284)	Loss 0.2439 (0.2439)	Acc@1 92.188 (92.188)	Acc@5 99.609 (99.609)
Epoch: [147][64/196]	Time 0.118 (0.110)	Data 0.000 (0.005)	Loss 0.2830 (0.2767)	Acc@1 88.281 (90.367)	Acc@5 100.000 (99.736)
Epoch: [147][128/196]	Time 0.114 (0.107)	Data 0.000 (0.002)	Loss 0.2451 (0.2841)	Acc@1 91.406 (90.177)	Acc@5 100.000 (99.755)
Epoch: [147][192/196]	Time 0.113 (0.108)	Data 0.000 (0.002)	Loss 0.3296 (0.2908)	Acc@1 87.109 (89.864)	Acc@5 99.219 (99.733)
after train
Epoche: [148/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [148][0/196]	Time 0.121 (0.121)	Data 0.280 (0.280)	Loss 0.2267 (0.2267)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [148][64/196]	Time 0.114 (0.112)	Data 0.000 (0.005)	Loss 0.2791 (0.2707)	Acc@1 90.234 (90.883)	Acc@5 99.219 (99.748)
Epoch: [148][128/196]	Time 0.107 (0.107)	Data 0.000 (0.002)	Loss 0.2687 (0.2822)	Acc@1 90.234 (90.346)	Acc@5 100.000 (99.767)
Epoch: [148][192/196]	Time 0.118 (0.108)	Data 0.000 (0.002)	Loss 0.2579 (0.2873)	Acc@1 91.797 (90.174)	Acc@5 99.219 (99.755)
after train
Epoche: [149/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [149][0/196]	Time 0.140 (0.140)	Data 0.244 (0.244)	Loss 0.2742 (0.2742)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [149][64/196]	Time 0.116 (0.110)	Data 0.000 (0.004)	Loss 0.2838 (0.2634)	Acc@1 90.625 (90.865)	Acc@5 100.000 (99.742)
Epoch: [149][128/196]	Time 0.116 (0.110)	Data 0.000 (0.002)	Loss 0.3223 (0.2739)	Acc@1 90.625 (90.528)	Acc@5 100.000 (99.749)
Epoch: [149][192/196]	Time 0.109 (0.107)	Data 0.000 (0.001)	Loss 0.2527 (0.2813)	Acc@1 89.453 (90.334)	Acc@5 100.000 (99.729)
after train
Epoche: [150/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [150][0/196]	Time 0.139 (0.139)	Data 0.241 (0.241)	Loss 0.2437 (0.2437)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [150][64/196]	Time 0.120 (0.111)	Data 0.000 (0.004)	Loss 0.2607 (0.2972)	Acc@1 90.234 (89.525)	Acc@5 100.000 (99.760)
Epoch: [150][128/196]	Time 0.109 (0.111)	Data 0.000 (0.002)	Loss 0.3036 (0.2861)	Acc@1 89.844 (89.910)	Acc@5 99.609 (99.797)
Epoch: [150][192/196]	Time 0.110 (0.108)	Data 0.000 (0.001)	Loss 0.2818 (0.2933)	Acc@1 89.453 (89.730)	Acc@5 100.000 (99.765)
after train
Epoche: [151/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [151][0/196]	Time 0.132 (0.132)	Data 0.239 (0.239)	Loss 0.2647 (0.2647)	Acc@1 91.797 (91.797)	Acc@5 99.609 (99.609)
Epoch: [151][64/196]	Time 0.110 (0.110)	Data 0.000 (0.004)	Loss 0.2905 (0.2707)	Acc@1 88.672 (90.817)	Acc@5 99.609 (99.778)
Epoch: [151][128/196]	Time 0.115 (0.110)	Data 0.000 (0.002)	Loss 0.2932 (0.2764)	Acc@1 90.234 (90.619)	Acc@5 99.609 (99.758)
Epoch: [151][192/196]	Time 0.108 (0.109)	Data 0.000 (0.001)	Loss 0.3487 (0.2789)	Acc@1 89.453 (90.473)	Acc@5 99.609 (99.769)
after train
Epoche: [152/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [152][0/196]	Time 0.125 (0.125)	Data 0.272 (0.272)	Loss 0.2946 (0.2946)	Acc@1 91.016 (91.016)	Acc@5 99.219 (99.219)
Epoch: [152][64/196]	Time 0.110 (0.112)	Data 0.000 (0.004)	Loss 0.2908 (0.2773)	Acc@1 89.453 (90.571)	Acc@5 100.000 (99.730)
Epoch: [152][128/196]	Time 0.098 (0.111)	Data 0.000 (0.002)	Loss 0.3517 (0.2764)	Acc@1 86.719 (90.480)	Acc@5 98.828 (99.737)
Epoch: [152][192/196]	Time 0.111 (0.111)	Data 0.000 (0.002)	Loss 0.2922 (0.2854)	Acc@1 90.234 (90.087)	Acc@5 100.000 (99.731)
after train
Epoche: [153/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [153][0/196]	Time 0.105 (0.105)	Data 0.246 (0.246)	Loss 0.2804 (0.2804)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [153][64/196]	Time 0.101 (0.103)	Data 0.000 (0.004)	Loss 0.2487 (0.2776)	Acc@1 92.188 (90.799)	Acc@5 100.000 (99.802)
Epoch: [153][128/196]	Time 0.117 (0.106)	Data 0.000 (0.002)	Loss 0.3320 (0.2814)	Acc@1 88.672 (90.543)	Acc@5 99.219 (99.761)
Epoch: [153][192/196]	Time 0.109 (0.107)	Data 0.000 (0.001)	Loss 0.2021 (0.2869)	Acc@1 93.359 (90.325)	Acc@5 100.000 (99.765)
after train
Epoche: [154/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [154][0/196]	Time 0.112 (0.112)	Data 0.276 (0.276)	Loss 0.3634 (0.3634)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [154][64/196]	Time 0.107 (0.102)	Data 0.000 (0.004)	Loss 0.2821 (0.2900)	Acc@1 90.234 (90.210)	Acc@5 98.828 (99.700)
Epoch: [154][128/196]	Time 0.101 (0.107)	Data 0.000 (0.002)	Loss 0.2939 (0.2819)	Acc@1 89.062 (90.440)	Acc@5 100.000 (99.734)
Epoch: [154][192/196]	Time 0.096 (0.108)	Data 0.000 (0.002)	Loss 0.2826 (0.2879)	Acc@1 91.016 (90.194)	Acc@5 99.219 (99.715)
after train
Epoche: [155/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [155][0/196]	Time 0.143 (0.143)	Data 0.278 (0.278)	Loss 0.2381 (0.2381)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [155][64/196]	Time 0.070 (0.106)	Data 0.000 (0.004)	Loss 0.2856 (0.2710)	Acc@1 89.453 (90.541)	Acc@5 99.609 (99.772)
Epoch: [155][128/196]	Time 0.099 (0.106)	Data 0.000 (0.002)	Loss 0.3065 (0.2804)	Acc@1 87.109 (90.283)	Acc@5 100.000 (99.737)
Epoch: [155][192/196]	Time 0.109 (0.107)	Data 0.000 (0.002)	Loss 0.3176 (0.2813)	Acc@1 90.625 (90.240)	Acc@5 99.219 (99.739)
after train
Epoche: [156/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [156][0/196]	Time 0.129 (0.129)	Data 0.271 (0.271)	Loss 0.2223 (0.2223)	Acc@1 94.531 (94.531)	Acc@5 99.609 (99.609)
Epoch: [156][64/196]	Time 0.108 (0.111)	Data 0.000 (0.004)	Loss 0.2601 (0.2720)	Acc@1 90.625 (90.451)	Acc@5 100.000 (99.742)
Epoch: [156][128/196]	Time 0.117 (0.107)	Data 0.000 (0.002)	Loss 0.4030 (0.2809)	Acc@1 86.328 (90.234)	Acc@5 99.219 (99.718)
Epoch: [156][192/196]	Time 0.111 (0.108)	Data 0.000 (0.002)	Loss 0.3256 (0.2855)	Acc@1 88.672 (90.155)	Acc@5 99.609 (99.711)
after train
Epoche: [157/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [157][0/196]	Time 0.130 (0.130)	Data 0.243 (0.243)	Loss 0.2673 (0.2673)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [157][64/196]	Time 0.096 (0.108)	Data 0.000 (0.004)	Loss 0.3486 (0.2763)	Acc@1 87.500 (90.619)	Acc@5 98.828 (99.784)
Epoch: [157][128/196]	Time 0.113 (0.105)	Data 0.000 (0.002)	Loss 0.2937 (0.2864)	Acc@1 90.234 (90.210)	Acc@5 99.609 (99.746)
Epoch: [157][192/196]	Time 0.112 (0.107)	Data 0.000 (0.001)	Loss 0.2703 (0.2840)	Acc@1 90.234 (90.194)	Acc@5 100.000 (99.755)
after train
Epoche: [158/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [158][0/196]	Time 0.120 (0.120)	Data 0.271 (0.271)	Loss 0.3784 (0.3784)	Acc@1 87.891 (87.891)	Acc@5 99.219 (99.219)
Epoch: [158][64/196]	Time 0.098 (0.111)	Data 0.000 (0.004)	Loss 0.3466 (0.2975)	Acc@1 89.453 (89.796)	Acc@5 99.219 (99.718)
Epoch: [158][128/196]	Time 0.092 (0.111)	Data 0.000 (0.002)	Loss 0.3622 (0.2888)	Acc@1 87.891 (90.125)	Acc@5 99.219 (99.703)
Epoch: [158][192/196]	Time 0.121 (0.108)	Data 0.000 (0.002)	Loss 0.3128 (0.2863)	Acc@1 89.844 (90.220)	Acc@5 99.219 (99.717)
after train
Epoche: [159/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [159][0/196]	Time 0.120 (0.120)	Data 0.279 (0.279)	Loss 0.3216 (0.3216)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [159][64/196]	Time 0.106 (0.111)	Data 0.000 (0.005)	Loss 0.2535 (0.2897)	Acc@1 91.797 (90.252)	Acc@5 99.609 (99.730)
Epoch: [159][128/196]	Time 0.102 (0.111)	Data 0.000 (0.002)	Loss 0.3434 (0.2859)	Acc@1 88.672 (90.283)	Acc@5 99.609 (99.752)
Epoch: [159][192/196]	Time 0.117 (0.108)	Data 0.000 (0.002)	Loss 0.3402 (0.2874)	Acc@1 86.719 (90.117)	Acc@5 99.609 (99.765)
after train
Epoche: [160/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [160][0/196]	Time 0.143 (0.143)	Data 0.245 (0.245)	Loss 0.1855 (0.1855)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [160][64/196]	Time 0.111 (0.110)	Data 0.000 (0.004)	Loss 0.2940 (0.2766)	Acc@1 89.453 (90.499)	Acc@5 98.828 (99.706)
Epoch: [160][128/196]	Time 0.110 (0.110)	Data 0.000 (0.002)	Loss 0.2505 (0.2815)	Acc@1 89.844 (90.316)	Acc@5 100.000 (99.721)
Epoch: [160][192/196]	Time 0.094 (0.108)	Data 0.000 (0.001)	Loss 0.3035 (0.2885)	Acc@1 91.797 (90.141)	Acc@5 100.000 (99.725)
after train
Epoche: [161/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [161][0/196]	Time 0.125 (0.125)	Data 0.294 (0.294)	Loss 0.2213 (0.2213)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [161][64/196]	Time 0.117 (0.112)	Data 0.000 (0.005)	Loss 0.2369 (0.2640)	Acc@1 91.797 (90.913)	Acc@5 100.000 (99.814)
Epoch: [161][128/196]	Time 0.108 (0.112)	Data 0.000 (0.003)	Loss 0.2993 (0.2823)	Acc@1 91.016 (90.228)	Acc@5 100.000 (99.782)
Epoch: [161][192/196]	Time 0.116 (0.112)	Data 0.000 (0.002)	Loss 0.2667 (0.2857)	Acc@1 89.453 (90.147)	Acc@5 99.609 (99.749)
after train
Epoche: [162/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [162][0/196]	Time 0.124 (0.124)	Data 0.258 (0.258)	Loss 0.3351 (0.3351)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [162][64/196]	Time 0.110 (0.106)	Data 0.000 (0.004)	Loss 0.3150 (0.2736)	Acc@1 87.891 (90.577)	Acc@5 100.000 (99.778)
Epoch: [162][128/196]	Time 0.106 (0.108)	Data 0.000 (0.002)	Loss 0.3023 (0.2781)	Acc@1 91.016 (90.407)	Acc@5 99.609 (99.770)
Epoch: [162][192/196]	Time 0.114 (0.108)	Data 0.000 (0.002)	Loss 0.2928 (0.2862)	Acc@1 89.453 (90.103)	Acc@5 100.000 (99.751)
after train
Epoche: [163/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [163][0/196]	Time 0.123 (0.123)	Data 0.249 (0.249)	Loss 0.2859 (0.2859)	Acc@1 90.234 (90.234)	Acc@5 100.000 (100.000)
Epoch: [163][64/196]	Time 0.113 (0.103)	Data 0.000 (0.004)	Loss 0.2319 (0.2886)	Acc@1 90.625 (89.862)	Acc@5 100.000 (99.784)
Epoch: [163][128/196]	Time 0.118 (0.107)	Data 0.000 (0.002)	Loss 0.2717 (0.2858)	Acc@1 90.234 (90.144)	Acc@5 99.609 (99.773)
Epoch: [163][192/196]	Time 0.115 (0.108)	Data 0.000 (0.002)	Loss 0.2936 (0.2811)	Acc@1 89.844 (90.267)	Acc@5 99.609 (99.767)
after train
Epoche: [164/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [164][0/196]	Time 0.121 (0.121)	Data 0.273 (0.273)	Loss 0.2997 (0.2997)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [164][64/196]	Time 0.110 (0.101)	Data 0.000 (0.004)	Loss 0.2686 (0.2876)	Acc@1 91.797 (90.096)	Acc@5 99.219 (99.754)
Epoch: [164][128/196]	Time 0.115 (0.106)	Data 0.000 (0.002)	Loss 0.2719 (0.2833)	Acc@1 90.625 (90.389)	Acc@5 99.609 (99.761)
Epoch: [164][192/196]	Time 0.115 (0.107)	Data 0.000 (0.002)	Loss 0.3387 (0.2849)	Acc@1 87.891 (90.263)	Acc@5 98.828 (99.767)
after train
Epoche: [165/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [165][0/196]	Time 0.113 (0.113)	Data 0.271 (0.271)	Loss 0.2759 (0.2759)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [165][64/196]	Time 0.111 (0.112)	Data 0.000 (0.004)	Loss 0.2755 (0.2840)	Acc@1 88.672 (90.439)	Acc@5 99.609 (99.742)
Epoch: [165][128/196]	Time 0.120 (0.108)	Data 0.000 (0.002)	Loss 0.2495 (0.2839)	Acc@1 93.359 (90.371)	Acc@5 99.219 (99.758)
Epoch: [165][192/196]	Time 0.100 (0.108)	Data 0.000 (0.002)	Loss 0.2845 (0.2875)	Acc@1 89.453 (90.287)	Acc@5 99.609 (99.749)
after train
Epoche: [166/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [166][0/196]	Time 0.123 (0.123)	Data 0.284 (0.284)	Loss 0.3639 (0.3639)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [166][64/196]	Time 0.114 (0.110)	Data 0.000 (0.005)	Loss 0.3307 (0.2809)	Acc@1 89.062 (90.264)	Acc@5 100.000 (99.748)
Epoch: [166][128/196]	Time 0.115 (0.106)	Data 0.000 (0.002)	Loss 0.2708 (0.2852)	Acc@1 89.062 (90.089)	Acc@5 100.000 (99.721)
Epoch: [166][192/196]	Time 0.116 (0.107)	Data 0.000 (0.002)	Loss 0.3164 (0.2844)	Acc@1 90.234 (90.174)	Acc@5 100.000 (99.727)
after train
Epoche: [167/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [167][0/196]	Time 0.133 (0.133)	Data 0.254 (0.254)	Loss 0.2164 (0.2164)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [167][64/196]	Time 0.101 (0.111)	Data 0.000 (0.004)	Loss 0.2686 (0.2776)	Acc@1 91.797 (90.355)	Acc@5 99.609 (99.772)
Epoch: [167][128/196]	Time 0.093 (0.108)	Data 0.000 (0.002)	Loss 0.2448 (0.2822)	Acc@1 92.188 (90.080)	Acc@5 100.000 (99.740)
Epoch: [167][192/196]	Time 0.101 (0.107)	Data 0.000 (0.002)	Loss 0.3473 (0.2896)	Acc@1 86.328 (89.860)	Acc@5 100.000 (99.765)
after train
Epoche: [168/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [168][0/196]	Time 0.131 (0.131)	Data 0.283 (0.283)	Loss 0.2744 (0.2744)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [168][64/196]	Time 0.120 (0.111)	Data 0.000 (0.005)	Loss 0.1786 (0.2750)	Acc@1 94.141 (90.763)	Acc@5 100.000 (99.772)
Epoch: [168][128/196]	Time 0.099 (0.110)	Data 0.000 (0.002)	Loss 0.3293 (0.2784)	Acc@1 87.109 (90.589)	Acc@5 100.000 (99.752)
Epoch: [168][192/196]	Time 0.118 (0.107)	Data 0.000 (0.002)	Loss 0.2631 (0.2830)	Acc@1 92.188 (90.449)	Acc@5 100.000 (99.737)
after train
Epoche: [169/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [169][0/196]	Time 0.123 (0.123)	Data 0.263 (0.263)	Loss 0.2264 (0.2264)	Acc@1 92.188 (92.188)	Acc@5 99.609 (99.609)
Epoch: [169][64/196]	Time 0.113 (0.110)	Data 0.000 (0.004)	Loss 0.2863 (0.2796)	Acc@1 90.234 (90.439)	Acc@5 100.000 (99.766)
Epoch: [169][128/196]	Time 0.116 (0.110)	Data 0.000 (0.002)	Loss 0.3341 (0.2834)	Acc@1 88.281 (90.216)	Acc@5 99.609 (99.730)
Epoch: [169][192/196]	Time 0.114 (0.108)	Data 0.000 (0.002)	Loss 0.2849 (0.2891)	Acc@1 91.797 (89.953)	Acc@5 99.609 (99.723)
after train
Epoche: [170/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [170][0/196]	Time 0.145 (0.145)	Data 0.282 (0.282)	Loss 0.2844 (0.2844)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [170][64/196]	Time 0.098 (0.110)	Data 0.000 (0.005)	Loss 0.2720 (0.2889)	Acc@1 90.234 (90.048)	Acc@5 99.219 (99.742)
Epoch: [170][128/196]	Time 0.115 (0.110)	Data 0.000 (0.002)	Loss 0.2746 (0.2850)	Acc@1 90.234 (90.271)	Acc@5 100.000 (99.712)
Epoch: [170][192/196]	Time 0.105 (0.110)	Data 0.000 (0.002)	Loss 0.3256 (0.2857)	Acc@1 86.719 (90.244)	Acc@5 99.219 (99.725)
after train
Epoche: [171/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [171][0/196]	Time 0.091 (0.091)	Data 0.295 (0.295)	Loss 0.2406 (0.2406)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [171][64/196]	Time 0.098 (0.107)	Data 0.000 (0.005)	Loss 0.2572 (0.2704)	Acc@1 91.016 (90.745)	Acc@5 100.000 (99.706)
Epoch: [171][128/196]	Time 0.108 (0.108)	Data 0.000 (0.003)	Loss 0.3509 (0.2756)	Acc@1 87.500 (90.592)	Acc@5 99.609 (99.688)
Epoch: [171][192/196]	Time 0.104 (0.109)	Data 0.000 (0.002)	Loss 0.3293 (0.2819)	Acc@1 87.891 (90.319)	Acc@5 99.609 (99.692)
after train
Epoche: [172/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [172][0/196]	Time 0.137 (0.137)	Data 0.279 (0.279)	Loss 0.2418 (0.2418)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [172][64/196]	Time 0.110 (0.101)	Data 0.000 (0.005)	Loss 0.2481 (0.2733)	Acc@1 91.406 (90.613)	Acc@5 99.609 (99.808)
Epoch: [172][128/196]	Time 0.120 (0.106)	Data 0.000 (0.002)	Loss 0.2518 (0.2827)	Acc@1 89.844 (90.298)	Acc@5 100.000 (99.758)
Epoch: [172][192/196]	Time 0.111 (0.108)	Data 0.000 (0.002)	Loss 0.2580 (0.2863)	Acc@1 90.234 (90.230)	Acc@5 99.219 (99.739)
after train
Epoche: [173/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [173][0/196]	Time 0.130 (0.130)	Data 0.270 (0.270)	Loss 0.3178 (0.3178)	Acc@1 89.453 (89.453)	Acc@5 100.000 (100.000)
Epoch: [173][64/196]	Time 0.113 (0.104)	Data 0.000 (0.004)	Loss 0.2608 (0.2849)	Acc@1 92.188 (90.246)	Acc@5 100.000 (99.724)
Epoch: [173][128/196]	Time 0.115 (0.107)	Data 0.000 (0.002)	Loss 0.3271 (0.2815)	Acc@1 85.938 (90.404)	Acc@5 100.000 (99.755)
Epoch: [173][192/196]	Time 0.106 (0.109)	Data 0.000 (0.002)	Loss 0.2955 (0.2798)	Acc@1 90.625 (90.459)	Acc@5 100.000 (99.753)
after train
Epoche: [174/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [174][0/196]	Time 0.124 (0.124)	Data 0.259 (0.259)	Loss 0.2422 (0.2422)	Acc@1 91.016 (91.016)	Acc@5 99.609 (99.609)
Epoch: [174][64/196]	Time 0.094 (0.107)	Data 0.000 (0.004)	Loss 0.2706 (0.2782)	Acc@1 90.234 (90.643)	Acc@5 100.000 (99.772)
Epoch: [174][128/196]	Time 0.098 (0.106)	Data 0.000 (0.002)	Loss 0.2757 (0.2844)	Acc@1 90.625 (90.268)	Acc@5 99.609 (99.755)
Epoch: [174][192/196]	Time 0.108 (0.108)	Data 0.000 (0.002)	Loss 0.2655 (0.2842)	Acc@1 91.406 (90.242)	Acc@5 99.609 (99.729)
after train
Epoche: [175/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [175][0/196]	Time 0.135 (0.135)	Data 0.255 (0.255)	Loss 0.1902 (0.1902)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [175][64/196]	Time 0.113 (0.111)	Data 0.000 (0.004)	Loss 0.2745 (0.2679)	Acc@1 89.844 (90.865)	Acc@5 100.000 (99.736)
Epoch: [175][128/196]	Time 0.117 (0.107)	Data 0.000 (0.002)	Loss 0.2756 (0.2747)	Acc@1 90.234 (90.495)	Acc@5 99.609 (99.746)
Epoch: [175][192/196]	Time 0.109 (0.108)	Data 0.000 (0.002)	Loss 0.2336 (0.2822)	Acc@1 91.016 (90.228)	Acc@5 100.000 (99.715)
after train
Epoche: [176/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [176][0/196]	Time 0.108 (0.108)	Data 0.263 (0.263)	Loss 0.3164 (0.3164)	Acc@1 88.672 (88.672)	Acc@5 99.609 (99.609)
Epoch: [176][64/196]	Time 0.108 (0.110)	Data 0.000 (0.004)	Loss 0.2289 (0.2687)	Acc@1 91.016 (90.811)	Acc@5 100.000 (99.766)
Epoch: [176][128/196]	Time 0.114 (0.106)	Data 0.000 (0.002)	Loss 0.1935 (0.2803)	Acc@1 94.922 (90.504)	Acc@5 100.000 (99.770)
Epoch: [176][192/196]	Time 0.113 (0.108)	Data 0.000 (0.002)	Loss 0.2835 (0.2837)	Acc@1 90.234 (90.325)	Acc@5 98.828 (99.741)
after train
Epoche: [177/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [177][0/196]	Time 0.131 (0.131)	Data 0.255 (0.255)	Loss 0.3255 (0.3255)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [177][64/196]	Time 0.113 (0.111)	Data 0.000 (0.004)	Loss 0.2571 (0.2768)	Acc@1 91.797 (90.294)	Acc@5 99.219 (99.694)
Epoch: [177][128/196]	Time 0.110 (0.110)	Data 0.000 (0.002)	Loss 0.2458 (0.2826)	Acc@1 90.234 (90.150)	Acc@5 100.000 (99.730)
Epoch: [177][192/196]	Time 0.109 (0.108)	Data 0.000 (0.002)	Loss 0.2428 (0.2857)	Acc@1 91.406 (90.079)	Acc@5 99.609 (99.729)
after train
Epoche: [178/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [178][0/196]	Time 0.101 (0.101)	Data 0.274 (0.274)	Loss 0.3349 (0.3349)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [178][64/196]	Time 0.101 (0.110)	Data 0.000 (0.004)	Loss 0.3050 (0.2764)	Acc@1 90.625 (90.559)	Acc@5 99.609 (99.754)
Epoch: [178][128/196]	Time 0.099 (0.110)	Data 0.000 (0.002)	Loss 0.3155 (0.2739)	Acc@1 89.062 (90.516)	Acc@5 99.609 (99.770)
Epoch: [178][192/196]	Time 0.115 (0.107)	Data 0.000 (0.002)	Loss 0.3181 (0.2820)	Acc@1 89.844 (90.289)	Acc@5 99.609 (99.747)
after train
Epoche: [179/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [179][0/196]	Time 0.128 (0.128)	Data 0.277 (0.277)	Loss 0.3229 (0.3229)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [179][64/196]	Time 0.109 (0.110)	Data 0.000 (0.004)	Loss 0.2952 (0.2794)	Acc@1 90.625 (90.325)	Acc@5 99.609 (99.730)
Epoch: [179][128/196]	Time 0.111 (0.109)	Data 0.000 (0.002)	Loss 0.2495 (0.2751)	Acc@1 91.406 (90.458)	Acc@5 99.609 (99.752)
Epoch: [179][192/196]	Time 0.100 (0.109)	Data 0.000 (0.002)	Loss 0.2524 (0.2833)	Acc@1 91.406 (90.247)	Acc@5 99.219 (99.741)
after train
Epoche: [180/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [180][0/196]	Time 0.131 (0.131)	Data 0.246 (0.246)	Loss 0.2216 (0.2216)	Acc@1 92.969 (92.969)	Acc@5 99.609 (99.609)
Epoch: [180][64/196]	Time 0.114 (0.109)	Data 0.000 (0.004)	Loss 0.1748 (0.2747)	Acc@1 94.531 (90.661)	Acc@5 100.000 (99.784)
Epoch: [180][128/196]	Time 0.114 (0.110)	Data 0.000 (0.002)	Loss 0.2839 (0.2868)	Acc@1 89.844 (90.237)	Acc@5 100.000 (99.746)
Epoch: [180][192/196]	Time 0.111 (0.110)	Data 0.000 (0.002)	Loss 0.3168 (0.2898)	Acc@1 89.844 (90.062)	Acc@5 100.000 (99.723)
after train
Max memory: 83.382528
 21.803s  