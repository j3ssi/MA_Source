no display found. Using non-interactive Agg backend
[3, 3, 3]
[4, 8, 16]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 120; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/room1x1/model.nn; checkpoint: ./output/experimente4/room121; saveModell: True; LR: 0.1
random number: 5794
Files already downloaded and verified
==> Resuming from checkpoint..
Start epoch: 121
First Lr: 0.1
Startepoche: 121
deeper epoch: 121
Epoche: [121/240]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [121][0/196]	Time 0.104 (0.104)	Data 0.452 (0.452)	Loss 0.7490 (0.7490)	Acc@1 71.875 (71.875)	Acc@5 97.266 (97.266)
Epoch: [121][64/196]	Time 0.078 (0.057)	Data 0.000 (0.007)	Loss 0.4921 (0.6329)	Acc@1 84.375 (77.770)	Acc@5 99.609 (98.750)
Epoch: [121][128/196]	Time 0.072 (0.058)	Data 0.000 (0.004)	Loss 0.6017 (0.6252)	Acc@1 78.906 (78.104)	Acc@5 100.000 (98.765)
Epoch: [121][192/196]	Time 0.060 (0.059)	Data 0.000 (0.003)	Loss 0.6691 (0.6265)	Acc@1 78.516 (78.161)	Acc@5 98.047 (98.755)
after train
test acc: 77.59


now deeper1
i: 3
j: 0; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 3; i0=: 8; i1=: 8
i: 4
j: 0; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 4; i0=: 8; i1=: 8
i: 5
j: 0; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 5; i0=: 8; i1=: 8
i: 6
j: 0; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))]
seq[j]: Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
j: 1; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 5; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
i: 6; i0=: 16; i1=: 8
skip: 7
i: 8
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 8; i0=: 16; i1=: 16
i: 9
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 9; i0=: 16; i1=: 16
i: 10
j: 0; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))]
seq[j]: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
j: 1; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 5; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
i: 10; i0=: 32; i1=: 16
skip: 11
i: 12
j: 0; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 12; i0=: 32; i1=: 32
i: 13
j: 0; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 1; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 13; i0=: 32; i1=: 32
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (7): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (11): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): AdaptiveAvgPool2d(output_size=(1, 1))
    (15): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Nums: [[1, 1, 1], [2, 1, 1], [2, 1, 1]]
num: 10; numofstages: 3, listofBlocks: [3, 3, 3], layers in blocj: 1
model.para: <generator object Module.named_parameters at 0x7fc46da90db0>
Epoche: [122/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [122][0/196]	Time 0.236 (0.236)	Data 0.567 (0.567)	Loss 2.4741 (2.4741)	Acc@1 9.375 (9.375)	Acc@5 49.219 (49.219)
Epoch: [122][64/196]	Time 0.110 (0.109)	Data 0.000 (0.009)	Loss 2.3087 (2.3378)	Acc@1 13.672 (10.637)	Acc@5 58.984 (52.626)
Epoch: [122][128/196]	Time 0.090 (0.107)	Data 0.000 (0.005)	Loss 2.1327 (2.3012)	Acc@1 19.141 (12.088)	Acc@5 69.531 (56.962)
Epoch: [122][192/196]	Time 0.094 (0.102)	Data 0.000 (0.003)	Loss 2.0963 (2.2586)	Acc@1 23.828 (13.917)	Acc@5 78.906 (60.891)
after train
test acc: 20.0
Epoche: [123/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [123][0/196]	Time 0.118 (0.118)	Data 0.494 (0.494)	Loss 2.0577 (2.0577)	Acc@1 22.266 (22.266)	Acc@5 78.516 (78.516)
Epoch: [123][64/196]	Time 0.079 (0.097)	Data 0.000 (0.008)	Loss 2.0266 (2.0313)	Acc@1 21.094 (22.218)	Acc@5 77.344 (77.596)
Epoch: [123][128/196]	Time 0.096 (0.100)	Data 0.000 (0.004)	Loss 1.8649 (1.9826)	Acc@1 27.344 (23.628)	Acc@5 82.812 (79.588)
Epoch: [123][192/196]	Time 0.083 (0.099)	Data 0.000 (0.003)	Loss 1.7622 (1.9276)	Acc@1 31.641 (25.213)	Acc@5 89.062 (81.307)
after train
test acc: 28.16
Epoche: [124/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [124][0/196]	Time 0.161 (0.161)	Data 0.460 (0.460)	Loss 1.7215 (1.7215)	Acc@1 32.031 (32.031)	Acc@5 85.938 (85.938)
Epoch: [124][64/196]	Time 0.102 (0.097)	Data 0.000 (0.007)	Loss 1.7501 (1.7596)	Acc@1 32.031 (30.619)	Acc@5 86.328 (86.550)
Epoch: [124][128/196]	Time 0.087 (0.096)	Data 0.000 (0.004)	Loss 1.7087 (1.7237)	Acc@1 39.062 (32.510)	Acc@5 90.234 (87.436)
Epoch: [124][192/196]	Time 0.101 (0.095)	Data 0.000 (0.003)	Loss 1.5461 (1.6937)	Acc@1 42.578 (34.203)	Acc@5 91.016 (88.103)
after train
test acc: 39.72
Epoche: [125/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [125][0/196]	Time 0.158 (0.158)	Data 0.431 (0.431)	Loss 1.6078 (1.6078)	Acc@1 37.109 (37.109)	Acc@5 88.281 (88.281)
Epoch: [125][64/196]	Time 0.093 (0.096)	Data 0.000 (0.007)	Loss 1.6358 (1.5858)	Acc@1 36.328 (39.387)	Acc@5 90.625 (90.457)
Epoch: [125][128/196]	Time 0.112 (0.095)	Data 0.000 (0.004)	Loss 1.4134 (1.5516)	Acc@1 47.656 (41.503)	Acc@5 92.578 (90.773)
Epoch: [125][192/196]	Time 0.101 (0.094)	Data 0.000 (0.003)	Loss 1.3519 (1.5229)	Acc@1 48.438 (42.831)	Acc@5 92.969 (91.248)
after train
test acc: 35.92
Epoche: [126/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [126][0/196]	Time 0.174 (0.174)	Data 0.382 (0.382)	Loss 1.4374 (1.4374)	Acc@1 46.875 (46.875)	Acc@5 90.625 (90.625)
Epoch: [126][64/196]	Time 0.090 (0.095)	Data 0.000 (0.006)	Loss 1.3866 (1.3922)	Acc@1 49.219 (49.014)	Acc@5 91.016 (92.837)
Epoch: [126][128/196]	Time 0.111 (0.098)	Data 0.000 (0.003)	Loss 1.3554 (1.3731)	Acc@1 53.516 (49.858)	Acc@5 90.625 (93.223)
Epoch: [126][192/196]	Time 0.098 (0.098)	Data 0.000 (0.002)	Loss 1.4025 (1.3546)	Acc@1 50.781 (50.650)	Acc@5 92.578 (93.517)
after train
test acc: 35.74
Epoche: [127/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [127][0/196]	Time 0.253 (0.253)	Data 0.483 (0.483)	Loss 1.3519 (1.3519)	Acc@1 48.828 (48.828)	Acc@5 94.531 (94.531)
Epoch: [127][64/196]	Time 0.095 (0.119)	Data 0.001 (0.008)	Loss 1.3096 (1.2757)	Acc@1 51.562 (53.233)	Acc@5 94.922 (94.712)
Epoch: [127][128/196]	Time 0.127 (0.116)	Data 0.000 (0.004)	Loss 1.2213 (1.2635)	Acc@1 53.906 (53.979)	Acc@5 96.094 (94.628)
Epoch: [127][192/196]	Time 0.092 (0.110)	Data 0.000 (0.003)	Loss 1.2924 (1.2577)	Acc@1 51.562 (54.447)	Acc@5 93.750 (94.632)
after train
test acc: 40.88
Epoche: [128/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [128][0/196]	Time 0.190 (0.190)	Data 0.599 (0.599)	Loss 1.1976 (1.1976)	Acc@1 53.906 (53.906)	Acc@5 93.359 (93.359)
Epoch: [128][64/196]	Time 0.097 (0.098)	Data 0.000 (0.010)	Loss 1.2875 (1.2043)	Acc@1 55.469 (56.743)	Acc@5 95.312 (95.150)
Epoch: [128][128/196]	Time 0.085 (0.098)	Data 0.000 (0.005)	Loss 1.0838 (1.1848)	Acc@1 61.719 (57.437)	Acc@5 95.312 (95.325)
Epoch: [128][192/196]	Time 0.101 (0.097)	Data 0.000 (0.003)	Loss 1.1339 (1.1674)	Acc@1 60.938 (58.003)	Acc@5 95.703 (95.448)
after train
test acc: 43.9
Epoche: [129/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [129][0/196]	Time 0.126 (0.126)	Data 0.478 (0.478)	Loss 1.1005 (1.1005)	Acc@1 63.672 (63.672)	Acc@5 94.922 (94.922)
Epoch: [129][64/196]	Time 0.095 (0.091)	Data 0.000 (0.008)	Loss 1.1054 (1.1243)	Acc@1 60.938 (59.477)	Acc@5 94.922 (95.889)
Epoch: [129][128/196]	Time 0.093 (0.093)	Data 0.000 (0.004)	Loss 1.0265 (1.1094)	Acc@1 63.281 (60.056)	Acc@5 96.484 (95.861)
Epoch: [129][192/196]	Time 0.106 (0.094)	Data 0.000 (0.003)	Loss 0.9846 (1.0979)	Acc@1 63.672 (60.652)	Acc@5 98.047 (95.889)
after train
test acc: 50.1
Epoche: [130/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [130][0/196]	Time 0.151 (0.151)	Data 0.416 (0.416)	Loss 1.0175 (1.0175)	Acc@1 66.797 (66.797)	Acc@5 96.094 (96.094)
Epoch: [130][64/196]	Time 0.095 (0.093)	Data 0.000 (0.007)	Loss 0.9570 (1.0719)	Acc@1 64.062 (61.340)	Acc@5 97.656 (96.346)
Epoch: [130][128/196]	Time 0.097 (0.098)	Data 0.000 (0.004)	Loss 1.0020 (1.0606)	Acc@1 65.234 (61.979)	Acc@5 97.266 (96.266)
Epoch: [130][192/196]	Time 0.086 (0.096)	Data 0.000 (0.002)	Loss 0.9994 (1.0492)	Acc@1 65.625 (62.352)	Acc@5 95.703 (96.399)
after train
test acc: 61.23
Epoche: [131/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [131][0/196]	Time 0.131 (0.131)	Data 0.407 (0.407)	Loss 1.0511 (1.0511)	Acc@1 61.719 (61.719)	Acc@5 96.094 (96.094)
Epoch: [131][64/196]	Time 0.072 (0.096)	Data 0.000 (0.007)	Loss 0.9745 (1.0178)	Acc@1 62.891 (63.780)	Acc@5 98.047 (96.550)
Epoch: [131][128/196]	Time 0.087 (0.095)	Data 0.000 (0.003)	Loss 1.1253 (1.0055)	Acc@1 58.203 (64.132)	Acc@5 95.703 (96.590)
Epoch: [131][192/196]	Time 0.086 (0.093)	Data 0.000 (0.002)	Loss 0.8738 (0.9961)	Acc@1 72.266 (64.407)	Acc@5 96.875 (96.768)
after train
test acc: 52.65
Epoche: [132/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [132][0/196]	Time 0.117 (0.117)	Data 0.484 (0.484)	Loss 1.0427 (1.0427)	Acc@1 58.984 (58.984)	Acc@5 97.266 (97.266)
Epoch: [132][64/196]	Time 0.094 (0.096)	Data 0.000 (0.008)	Loss 0.8625 (0.9800)	Acc@1 67.969 (64.796)	Acc@5 98.438 (96.881)
Epoch: [132][128/196]	Time 0.096 (0.096)	Data 0.000 (0.004)	Loss 0.9077 (0.9717)	Acc@1 67.969 (65.325)	Acc@5 95.703 (96.954)
Epoch: [132][192/196]	Time 0.087 (0.096)	Data 0.000 (0.003)	Loss 1.0013 (0.9689)	Acc@1 65.234 (65.425)	Acc@5 97.656 (96.982)
after train
test acc: 57.95
Epoche: [133/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [133][0/196]	Time 0.138 (0.138)	Data 0.433 (0.433)	Loss 1.0519 (1.0519)	Acc@1 63.281 (63.281)	Acc@5 96.875 (96.875)
Epoch: [133][64/196]	Time 0.103 (0.099)	Data 0.000 (0.007)	Loss 0.9941 (0.9326)	Acc@1 64.062 (67.085)	Acc@5 96.875 (97.230)
Epoch: [133][128/196]	Time 0.086 (0.100)	Data 0.000 (0.004)	Loss 0.9809 (0.9325)	Acc@1 64.453 (67.103)	Acc@5 97.266 (97.208)
Epoch: [133][192/196]	Time 0.103 (0.098)	Data 0.000 (0.003)	Loss 0.9317 (0.9235)	Acc@1 69.922 (67.240)	Acc@5 94.531 (97.343)
after train
test acc: 61.97
Epoche: [134/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [134][0/196]	Time 0.209 (0.209)	Data 0.386 (0.386)	Loss 0.7558 (0.7558)	Acc@1 71.875 (71.875)	Acc@5 99.609 (99.609)
Epoch: [134][64/196]	Time 0.087 (0.097)	Data 0.000 (0.006)	Loss 0.9153 (0.8984)	Acc@1 67.969 (68.113)	Acc@5 96.875 (97.494)
Epoch: [134][128/196]	Time 0.099 (0.094)	Data 0.000 (0.003)	Loss 0.8613 (0.8977)	Acc@1 69.141 (68.026)	Acc@5 97.656 (97.523)
Epoch: [134][192/196]	Time 0.071 (0.093)	Data 0.000 (0.002)	Loss 0.7903 (0.8951)	Acc@1 73.828 (68.204)	Acc@5 97.656 (97.482)
after train
test acc: 54.4
Epoche: [135/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [135][0/196]	Time 0.156 (0.156)	Data 0.450 (0.450)	Loss 0.8001 (0.8001)	Acc@1 70.703 (70.703)	Acc@5 98.047 (98.047)
Epoch: [135][64/196]	Time 0.107 (0.092)	Data 0.000 (0.007)	Loss 0.7841 (0.8593)	Acc@1 69.922 (69.579)	Acc@5 98.438 (97.740)
Epoch: [135][128/196]	Time 0.101 (0.093)	Data 0.000 (0.004)	Loss 0.9338 (0.8684)	Acc@1 69.531 (69.350)	Acc@5 95.312 (97.635)
Epoch: [135][192/196]	Time 0.112 (0.094)	Data 0.000 (0.003)	Loss 0.7929 (0.8592)	Acc@1 70.703 (69.683)	Acc@5 98.438 (97.695)
after train
test acc: 67.08
Epoche: [136/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [136][0/196]	Time 0.108 (0.108)	Data 0.411 (0.411)	Loss 0.7884 (0.7884)	Acc@1 70.312 (70.312)	Acc@5 98.438 (98.438)
Epoch: [136][64/196]	Time 0.102 (0.098)	Data 0.000 (0.007)	Loss 0.8193 (0.8206)	Acc@1 72.266 (70.919)	Acc@5 98.047 (97.975)
Epoch: [136][128/196]	Time 0.095 (0.096)	Data 0.000 (0.003)	Loss 0.8371 (0.8298)	Acc@1 69.531 (70.476)	Acc@5 98.438 (97.965)
Epoch: [136][192/196]	Time 0.091 (0.094)	Data 0.000 (0.002)	Loss 0.8767 (0.8255)	Acc@1 69.531 (70.695)	Acc@5 96.484 (97.952)
after train
test acc: 64.16
Epoche: [137/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [137][0/196]	Time 0.143 (0.143)	Data 0.397 (0.397)	Loss 0.7838 (0.7838)	Acc@1 73.047 (73.047)	Acc@5 96.484 (96.484)
Epoch: [137][64/196]	Time 0.083 (0.096)	Data 0.000 (0.006)	Loss 0.6944 (0.7954)	Acc@1 75.391 (72.019)	Acc@5 99.219 (97.969)
Epoch: [137][128/196]	Time 0.093 (0.100)	Data 0.000 (0.003)	Loss 0.8797 (0.8013)	Acc@1 68.359 (71.787)	Acc@5 98.828 (97.914)
Epoch: [137][192/196]	Time 0.085 (0.100)	Data 0.000 (0.002)	Loss 0.8457 (0.8024)	Acc@1 72.266 (71.877)	Acc@5 98.438 (97.915)
after train
test acc: 60.3
Epoche: [138/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [138][0/196]	Time 0.147 (0.147)	Data 0.460 (0.460)	Loss 0.7412 (0.7412)	Acc@1 73.047 (73.047)	Acc@5 98.438 (98.438)
Epoch: [138][64/196]	Time 0.093 (0.093)	Data 0.000 (0.007)	Loss 0.8062 (0.7677)	Acc@1 69.922 (73.083)	Acc@5 99.219 (98.317)
Epoch: [138][128/196]	Time 0.118 (0.094)	Data 0.000 (0.004)	Loss 0.6725 (0.7651)	Acc@1 76.172 (73.204)	Acc@5 98.828 (98.304)
Epoch: [138][192/196]	Time 0.095 (0.097)	Data 0.000 (0.003)	Loss 0.7196 (0.7728)	Acc@1 72.266 (72.968)	Acc@5 98.828 (98.195)
after train
test acc: 65.69
Epoche: [139/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [139][0/196]	Time 0.197 (0.197)	Data 0.441 (0.441)	Loss 0.8284 (0.8284)	Acc@1 71.875 (71.875)	Acc@5 99.219 (99.219)
Epoch: [139][64/196]	Time 0.151 (0.117)	Data 0.000 (0.007)	Loss 0.7781 (0.7598)	Acc@1 69.922 (73.618)	Acc@5 98.047 (98.143)
Epoch: [139][128/196]	Time 0.092 (0.109)	Data 0.000 (0.004)	Loss 0.7445 (0.7540)	Acc@1 72.656 (73.577)	Acc@5 98.828 (98.177)
Epoch: [139][192/196]	Time 0.101 (0.105)	Data 0.000 (0.003)	Loss 0.8825 (0.7558)	Acc@1 71.094 (73.520)	Acc@5 96.875 (98.154)
after train
test acc: 65.35
Epoche: [140/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [140][0/196]	Time 0.240 (0.240)	Data 0.383 (0.383)	Loss 0.7218 (0.7218)	Acc@1 74.219 (74.219)	Acc@5 97.656 (97.656)
Epoch: [140][64/196]	Time 0.116 (0.099)	Data 0.000 (0.006)	Loss 0.6736 (0.7400)	Acc@1 76.953 (74.081)	Acc@5 99.219 (98.251)
Epoch: [140][128/196]	Time 0.110 (0.098)	Data 0.000 (0.003)	Loss 0.6864 (0.7329)	Acc@1 76.562 (74.391)	Acc@5 98.438 (98.283)
Epoch: [140][192/196]	Time 0.093 (0.099)	Data 0.000 (0.002)	Loss 0.8016 (0.7297)	Acc@1 72.656 (74.470)	Acc@5 98.828 (98.328)
after train
test acc: 70.22
Epoche: [141/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [141][0/196]	Time 0.132 (0.132)	Data 0.571 (0.571)	Loss 0.6808 (0.6808)	Acc@1 75.000 (75.000)	Acc@5 99.219 (99.219)
Epoch: [141][64/196]	Time 0.080 (0.095)	Data 0.000 (0.009)	Loss 0.8569 (0.7306)	Acc@1 69.531 (74.633)	Acc@5 98.438 (98.323)
Epoch: [141][128/196]	Time 0.118 (0.094)	Data 0.000 (0.005)	Loss 0.7031 (0.7217)	Acc@1 75.781 (74.603)	Acc@5 98.828 (98.401)
Epoch: [141][192/196]	Time 0.111 (0.095)	Data 0.000 (0.003)	Loss 0.8687 (0.7178)	Acc@1 73.438 (74.769)	Acc@5 96.875 (98.474)
after train
test acc: 68.76
Epoche: [142/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [142][0/196]	Time 0.189 (0.189)	Data 0.395 (0.395)	Loss 0.5823 (0.5823)	Acc@1 78.125 (78.125)	Acc@5 99.219 (99.219)
Epoch: [142][64/196]	Time 0.095 (0.098)	Data 0.000 (0.006)	Loss 0.7119 (0.7058)	Acc@1 73.828 (75.397)	Acc@5 99.219 (98.486)
Epoch: [142][128/196]	Time 0.089 (0.095)	Data 0.000 (0.003)	Loss 0.6665 (0.6964)	Acc@1 78.125 (75.787)	Acc@5 97.266 (98.453)
Epoch: [142][192/196]	Time 0.104 (0.096)	Data 0.000 (0.002)	Loss 0.7146 (0.6975)	Acc@1 74.609 (75.777)	Acc@5 98.438 (98.425)
after train
test acc: 66.38
Epoche: [143/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [143][0/196]	Time 0.120 (0.120)	Data 0.446 (0.446)	Loss 0.7449 (0.7449)	Acc@1 75.391 (75.391)	Acc@5 99.219 (99.219)
Epoch: [143][64/196]	Time 0.097 (0.091)	Data 0.000 (0.007)	Loss 0.6903 (0.6949)	Acc@1 74.609 (75.883)	Acc@5 99.609 (98.431)
Epoch: [143][128/196]	Time 0.086 (0.093)	Data 0.000 (0.004)	Loss 0.6102 (0.6903)	Acc@1 75.781 (75.972)	Acc@5 98.828 (98.495)
Epoch: [143][192/196]	Time 0.090 (0.093)	Data 0.000 (0.003)	Loss 0.6372 (0.6830)	Acc@1 78.125 (76.224)	Acc@5 98.438 (98.508)
after train
test acc: 70.92
Epoche: [144/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [144][0/196]	Time 0.161 (0.161)	Data 0.541 (0.541)	Loss 0.5402 (0.5402)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [144][64/196]	Time 0.108 (0.098)	Data 0.000 (0.009)	Loss 0.6719 (0.6718)	Acc@1 76.562 (76.460)	Acc@5 99.609 (98.672)
Epoch: [144][128/196]	Time 0.162 (0.104)	Data 0.000 (0.005)	Loss 0.6993 (0.6747)	Acc@1 75.391 (76.499)	Acc@5 99.219 (98.571)
Epoch: [144][192/196]	Time 0.167 (0.110)	Data 0.000 (0.003)	Loss 0.7247 (0.6761)	Acc@1 73.047 (76.471)	Acc@5 98.828 (98.573)
after train
test acc: 70.28
Epoche: [145/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [145][0/196]	Time 0.148 (0.148)	Data 0.492 (0.492)	Loss 0.7651 (0.7651)	Acc@1 75.391 (75.391)	Acc@5 98.047 (98.047)
Epoch: [145][64/196]	Time 0.091 (0.099)	Data 0.000 (0.008)	Loss 0.6972 (0.6545)	Acc@1 75.000 (77.127)	Acc@5 97.266 (98.636)
Epoch: [145][128/196]	Time 0.094 (0.097)	Data 0.000 (0.004)	Loss 0.7981 (0.6563)	Acc@1 74.609 (77.089)	Acc@5 98.047 (98.680)
Epoch: [145][192/196]	Time 0.088 (0.096)	Data 0.000 (0.003)	Loss 0.7258 (0.6545)	Acc@1 75.000 (77.117)	Acc@5 98.047 (98.642)
after train
test acc: 75.03
Epoche: [146/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [146][0/196]	Time 0.221 (0.221)	Data 0.368 (0.368)	Loss 0.6789 (0.6789)	Acc@1 74.219 (74.219)	Acc@5 98.828 (98.828)
Epoch: [146][64/196]	Time 0.097 (0.097)	Data 0.000 (0.006)	Loss 0.6671 (0.6461)	Acc@1 76.562 (77.500)	Acc@5 99.219 (98.726)
Epoch: [146][128/196]	Time 0.093 (0.095)	Data 0.000 (0.003)	Loss 0.5583 (0.6439)	Acc@1 80.469 (77.589)	Acc@5 98.438 (98.765)
Epoch: [146][192/196]	Time 0.091 (0.093)	Data 0.000 (0.002)	Loss 0.6553 (0.6444)	Acc@1 77.734 (77.621)	Acc@5 98.828 (98.725)
after train
test acc: 71.54
Epoche: [147/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [147][0/196]	Time 0.161 (0.161)	Data 0.366 (0.366)	Loss 0.4716 (0.4716)	Acc@1 82.422 (82.422)	Acc@5 100.000 (100.000)
Epoch: [147][64/196]	Time 0.086 (0.096)	Data 0.000 (0.006)	Loss 0.5875 (0.6421)	Acc@1 78.906 (77.746)	Acc@5 99.609 (98.768)
Epoch: [147][128/196]	Time 0.090 (0.096)	Data 0.000 (0.003)	Loss 0.6227 (0.6377)	Acc@1 78.516 (77.734)	Acc@5 98.438 (98.810)
Epoch: [147][192/196]	Time 0.091 (0.097)	Data 0.000 (0.002)	Loss 0.5692 (0.6333)	Acc@1 82.031 (77.971)	Acc@5 99.609 (98.784)
after train
test acc: 73.75
Epoche: [148/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [148][0/196]	Time 0.125 (0.125)	Data 0.440 (0.440)	Loss 0.5400 (0.5400)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [148][64/196]	Time 0.092 (0.093)	Data 0.000 (0.007)	Loss 0.5596 (0.6210)	Acc@1 80.469 (78.558)	Acc@5 98.828 (98.870)
Epoch: [148][128/196]	Time 0.086 (0.093)	Data 0.000 (0.004)	Loss 0.6999 (0.6255)	Acc@1 75.391 (78.310)	Acc@5 98.047 (98.807)
Epoch: [148][192/196]	Time 0.083 (0.093)	Data 0.000 (0.003)	Loss 0.6425 (0.6209)	Acc@1 76.172 (78.437)	Acc@5 98.047 (98.816)
after train
test acc: 74.55
Epoche: [149/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [149][0/196]	Time 0.123 (0.123)	Data 0.444 (0.444)	Loss 0.6092 (0.6092)	Acc@1 76.562 (76.562)	Acc@5 99.219 (99.219)
Epoch: [149][64/196]	Time 0.108 (0.094)	Data 0.000 (0.007)	Loss 0.5451 (0.6197)	Acc@1 78.906 (78.696)	Acc@5 99.609 (98.756)
Epoch: [149][128/196]	Time 0.112 (0.093)	Data 0.000 (0.004)	Loss 0.6364 (0.6182)	Acc@1 76.562 (78.461)	Acc@5 98.828 (98.777)
Epoch: [149][192/196]	Time 0.085 (0.092)	Data 0.000 (0.003)	Loss 0.6473 (0.6086)	Acc@1 76.562 (78.738)	Acc@5 98.828 (98.826)
after train
test acc: 76.79
Epoche: [150/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [150][0/196]	Time 0.173 (0.173)	Data 0.422 (0.422)	Loss 0.5983 (0.5983)	Acc@1 79.297 (79.297)	Acc@5 99.219 (99.219)
Epoch: [150][64/196]	Time 0.098 (0.093)	Data 0.000 (0.007)	Loss 0.5789 (0.5889)	Acc@1 80.078 (79.177)	Acc@5 98.438 (98.990)
Epoch: [150][128/196]	Time 0.080 (0.092)	Data 0.000 (0.004)	Loss 0.5016 (0.5955)	Acc@1 84.375 (79.148)	Acc@5 99.609 (98.961)
Epoch: [150][192/196]	Time 0.085 (0.092)	Data 0.000 (0.002)	Loss 0.5540 (0.5995)	Acc@1 81.250 (79.145)	Acc@5 98.828 (98.899)
after train
test acc: 76.87
Epoche: [151/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [151][0/196]	Time 0.152 (0.152)	Data 0.391 (0.391)	Loss 0.4740 (0.4740)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [151][64/196]	Time 0.101 (0.096)	Data 0.000 (0.006)	Loss 0.6167 (0.5769)	Acc@1 80.078 (79.838)	Acc@5 98.828 (98.912)
Epoch: [151][128/196]	Time 0.080 (0.098)	Data 0.000 (0.003)	Loss 0.5224 (0.5828)	Acc@1 82.812 (79.751)	Acc@5 98.828 (98.925)
Epoch: [151][192/196]	Time 0.085 (0.097)	Data 0.000 (0.002)	Loss 0.4872 (0.5873)	Acc@1 82.812 (79.414)	Acc@5 99.609 (98.966)
after train
test acc: 68.41
Epoche: [152/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [152][0/196]	Time 0.142 (0.142)	Data 0.424 (0.424)	Loss 0.5801 (0.5801)	Acc@1 80.469 (80.469)	Acc@5 99.609 (99.609)
Epoch: [152][64/196]	Time 0.108 (0.101)	Data 0.000 (0.007)	Loss 0.6049 (0.5817)	Acc@1 79.688 (79.802)	Acc@5 98.828 (98.972)
Epoch: [152][128/196]	Time 0.151 (0.106)	Data 0.000 (0.004)	Loss 0.6239 (0.5872)	Acc@1 78.125 (79.572)	Acc@5 99.609 (98.973)
Epoch: [152][192/196]	Time 0.088 (0.108)	Data 0.000 (0.002)	Loss 0.5378 (0.5826)	Acc@1 83.203 (79.728)	Acc@5 99.609 (98.980)
after train
test acc: 75.43
Epoche: [153/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [153][0/196]	Time 0.157 (0.157)	Data 0.537 (0.537)	Loss 0.6352 (0.6352)	Acc@1 77.734 (77.734)	Acc@5 99.609 (99.609)
Epoch: [153][64/196]	Time 0.100 (0.108)	Data 0.000 (0.009)	Loss 0.5576 (0.5736)	Acc@1 82.031 (80.012)	Acc@5 99.609 (99.008)
Epoch: [153][128/196]	Time 0.097 (0.100)	Data 0.000 (0.004)	Loss 0.3876 (0.5728)	Acc@1 87.109 (80.148)	Acc@5 99.219 (98.964)
Epoch: [153][192/196]	Time 0.118 (0.099)	Data 0.000 (0.003)	Loss 0.6816 (0.5744)	Acc@1 76.172 (79.963)	Acc@5 98.047 (98.978)
after train
test acc: 72.22
Epoche: [154/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [154][0/196]	Time 0.135 (0.135)	Data 0.490 (0.490)	Loss 0.5533 (0.5533)	Acc@1 76.953 (76.953)	Acc@5 99.609 (99.609)
Epoch: [154][64/196]	Time 0.096 (0.095)	Data 0.000 (0.008)	Loss 0.5473 (0.5735)	Acc@1 81.250 (80.270)	Acc@5 98.828 (98.858)
Epoch: [154][128/196]	Time 0.099 (0.096)	Data 0.000 (0.004)	Loss 0.4601 (0.5705)	Acc@1 83.594 (80.490)	Acc@5 99.219 (98.961)
Epoch: [154][192/196]	Time 0.112 (0.098)	Data 0.000 (0.003)	Loss 0.5632 (0.5713)	Acc@1 82.812 (80.422)	Acc@5 99.219 (98.968)
after train
test acc: 76.15
Epoche: [155/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [155][0/196]	Time 0.170 (0.170)	Data 0.544 (0.544)	Loss 0.6146 (0.6146)	Acc@1 79.688 (79.688)	Acc@5 98.828 (98.828)
Epoch: [155][64/196]	Time 0.112 (0.098)	Data 0.000 (0.009)	Loss 0.5953 (0.5548)	Acc@1 79.688 (80.493)	Acc@5 98.828 (99.087)
Epoch: [155][128/196]	Time 0.081 (0.097)	Data 0.000 (0.005)	Loss 0.4371 (0.5554)	Acc@1 83.594 (80.551)	Acc@5 99.609 (99.089)
Epoch: [155][192/196]	Time 0.158 (0.098)	Data 0.000 (0.003)	Loss 0.5114 (0.5600)	Acc@1 80.859 (80.467)	Acc@5 99.219 (99.028)
after train
test acc: 72.84
Epoche: [156/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [156][0/196]	Time 0.161 (0.161)	Data 0.450 (0.450)	Loss 0.5323 (0.5323)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [156][64/196]	Time 0.134 (0.112)	Data 0.000 (0.007)	Loss 0.5454 (0.5680)	Acc@1 81.641 (80.288)	Acc@5 98.828 (98.948)
Epoch: [156][128/196]	Time 0.113 (0.113)	Data 0.000 (0.004)	Loss 0.5244 (0.5633)	Acc@1 80.078 (80.532)	Acc@5 99.609 (98.998)
Epoch: [156][192/196]	Time 0.104 (0.113)	Data 0.000 (0.003)	Loss 0.5169 (0.5591)	Acc@1 82.031 (80.590)	Acc@5 100.000 (99.035)
after train
test acc: 75.84
Epoche: [157/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [157][0/196]	Time 0.195 (0.195)	Data 0.527 (0.527)	Loss 0.6140 (0.6140)	Acc@1 78.516 (78.516)	Acc@5 99.219 (99.219)
Epoch: [157][64/196]	Time 0.092 (0.110)	Data 0.000 (0.008)	Loss 0.5532 (0.5380)	Acc@1 78.516 (81.298)	Acc@5 100.000 (99.135)
Epoch: [157][128/196]	Time 0.164 (0.115)	Data 0.000 (0.004)	Loss 0.5183 (0.5441)	Acc@1 82.422 (81.138)	Acc@5 100.000 (99.086)
Epoch: [157][192/196]	Time 0.105 (0.115)	Data 0.000 (0.003)	Loss 0.5837 (0.5431)	Acc@1 76.172 (81.102)	Acc@5 98.438 (99.095)
after train
test acc: 77.4
Epoche: [158/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [158][0/196]	Time 0.173 (0.173)	Data 0.522 (0.522)	Loss 0.5900 (0.5900)	Acc@1 79.297 (79.297)	Acc@5 99.609 (99.609)
Epoch: [158][64/196]	Time 0.114 (0.118)	Data 0.000 (0.008)	Loss 0.4557 (0.5391)	Acc@1 85.156 (80.925)	Acc@5 98.828 (99.201)
Epoch: [158][128/196]	Time 0.112 (0.115)	Data 0.000 (0.004)	Loss 0.5047 (0.5460)	Acc@1 84.375 (81.023)	Acc@5 98.828 (99.152)
Epoch: [158][192/196]	Time 0.098 (0.114)	Data 0.000 (0.003)	Loss 0.5334 (0.5475)	Acc@1 81.250 (81.017)	Acc@5 98.828 (99.130)
after train
test acc: 72.31
Epoche: [159/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [159][0/196]	Time 0.165 (0.165)	Data 0.539 (0.539)	Loss 0.5041 (0.5041)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [159][64/196]	Time 0.139 (0.116)	Data 0.000 (0.009)	Loss 0.5703 (0.5252)	Acc@1 81.641 (81.797)	Acc@5 99.219 (99.147)
Epoch: [159][128/196]	Time 0.090 (0.115)	Data 0.000 (0.005)	Loss 0.6064 (0.5319)	Acc@1 81.250 (81.616)	Acc@5 99.609 (99.098)
Epoch: [159][192/196]	Time 0.093 (0.115)	Data 0.000 (0.003)	Loss 0.5691 (0.5333)	Acc@1 78.906 (81.525)	Acc@5 98.828 (99.134)
after train
test acc: 72.55
Epoche: [160/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [160][0/196]	Time 0.169 (0.169)	Data 0.508 (0.508)	Loss 0.5265 (0.5265)	Acc@1 80.859 (80.859)	Acc@5 99.219 (99.219)
Epoch: [160][64/196]	Time 0.150 (0.113)	Data 0.000 (0.008)	Loss 0.5218 (0.5286)	Acc@1 82.812 (81.935)	Acc@5 98.828 (99.153)
Epoch: [160][128/196]	Time 0.115 (0.117)	Data 0.000 (0.004)	Loss 0.5359 (0.5272)	Acc@1 78.906 (81.722)	Acc@5 100.000 (99.146)
Epoch: [160][192/196]	Time 0.092 (0.115)	Data 0.000 (0.003)	Loss 0.5135 (0.5270)	Acc@1 83.203 (81.790)	Acc@5 98.047 (99.140)
after train
test acc: 75.68
Epoche: [161/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [161][0/196]	Time 0.158 (0.158)	Data 0.507 (0.507)	Loss 0.4960 (0.4960)	Acc@1 82.422 (82.422)	Acc@5 99.219 (99.219)
Epoch: [161][64/196]	Time 0.098 (0.117)	Data 0.000 (0.008)	Loss 0.6381 (0.5331)	Acc@1 80.469 (81.647)	Acc@5 98.047 (99.020)
Epoch: [161][128/196]	Time 0.143 (0.115)	Data 0.000 (0.004)	Loss 0.4781 (0.5239)	Acc@1 83.594 (81.756)	Acc@5 100.000 (99.137)
Epoch: [161][192/196]	Time 0.086 (0.114)	Data 0.000 (0.003)	Loss 0.4792 (0.5220)	Acc@1 82.812 (81.807)	Acc@5 99.219 (99.156)
after train
test acc: 70.06
Epoche: [162/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [162][0/196]	Time 0.197 (0.197)	Data 0.473 (0.473)	Loss 0.5186 (0.5186)	Acc@1 81.641 (81.641)	Acc@5 99.219 (99.219)
Epoch: [162][64/196]	Time 0.118 (0.114)	Data 0.000 (0.008)	Loss 0.4932 (0.5218)	Acc@1 82.031 (81.755)	Acc@5 99.219 (99.177)
Epoch: [162][128/196]	Time 0.094 (0.115)	Data 0.000 (0.004)	Loss 0.4194 (0.5131)	Acc@1 85.938 (81.943)	Acc@5 99.219 (99.191)
Epoch: [162][192/196]	Time 0.100 (0.114)	Data 0.000 (0.003)	Loss 0.5110 (0.5180)	Acc@1 84.375 (81.987)	Acc@5 98.047 (99.132)
after train
test acc: 77.83
Epoche: [163/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [163][0/196]	Time 0.248 (0.248)	Data 0.451 (0.451)	Loss 0.5546 (0.5546)	Acc@1 80.469 (80.469)	Acc@5 99.609 (99.609)
Epoch: [163][64/196]	Time 0.150 (0.118)	Data 0.000 (0.007)	Loss 0.5208 (0.5023)	Acc@1 79.297 (82.278)	Acc@5 100.000 (99.333)
Epoch: [163][128/196]	Time 0.089 (0.117)	Data 0.000 (0.004)	Loss 0.6421 (0.5116)	Acc@1 76.953 (82.158)	Acc@5 98.047 (99.252)
Epoch: [163][192/196]	Time 0.078 (0.115)	Data 0.000 (0.003)	Loss 0.4604 (0.5096)	Acc@1 84.375 (82.302)	Acc@5 99.219 (99.203)
after train
test acc: 76.99
Epoche: [164/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [164][0/196]	Time 0.140 (0.140)	Data 0.472 (0.472)	Loss 0.4829 (0.4829)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [164][64/196]	Time 0.085 (0.113)	Data 0.000 (0.008)	Loss 0.5774 (0.5100)	Acc@1 80.078 (82.488)	Acc@5 99.219 (99.099)
Epoch: [164][128/196]	Time 0.129 (0.111)	Data 0.000 (0.004)	Loss 0.4771 (0.5102)	Acc@1 83.594 (82.258)	Acc@5 99.219 (99.185)
Epoch: [164][192/196]	Time 0.118 (0.113)	Data 0.000 (0.003)	Loss 0.4899 (0.5101)	Acc@1 80.469 (82.282)	Acc@5 99.609 (99.156)
after train
test acc: 78.95
Epoche: [165/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [165][0/196]	Time 0.213 (0.213)	Data 0.515 (0.515)	Loss 0.5115 (0.5115)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [165][64/196]	Time 0.105 (0.112)	Data 0.000 (0.008)	Loss 0.5350 (0.5003)	Acc@1 82.031 (82.524)	Acc@5 99.219 (99.249)
Epoch: [165][128/196]	Time 0.102 (0.116)	Data 0.000 (0.004)	Loss 0.5222 (0.5013)	Acc@1 82.031 (82.455)	Acc@5 99.219 (99.273)
Epoch: [165][192/196]	Time 0.105 (0.114)	Data 0.000 (0.003)	Loss 0.5660 (0.5012)	Acc@1 80.469 (82.483)	Acc@5 98.828 (99.227)
after train
test acc: 79.04
Epoche: [166/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [166][0/196]	Time 0.151 (0.151)	Data 0.564 (0.564)	Loss 0.5436 (0.5436)	Acc@1 79.688 (79.688)	Acc@5 99.609 (99.609)
Epoch: [166][64/196]	Time 0.094 (0.122)	Data 0.000 (0.009)	Loss 0.5503 (0.4905)	Acc@1 80.859 (82.885)	Acc@5 98.047 (99.135)
Epoch: [166][128/196]	Time 0.124 (0.116)	Data 0.000 (0.005)	Loss 0.4787 (0.4977)	Acc@1 83.984 (82.703)	Acc@5 98.828 (99.188)
Epoch: [166][192/196]	Time 0.092 (0.115)	Data 0.000 (0.003)	Loss 0.4863 (0.4948)	Acc@1 80.078 (82.835)	Acc@5 99.219 (99.186)
after train
test acc: 77.44
Epoche: [167/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [167][0/196]	Time 0.187 (0.187)	Data 0.453 (0.453)	Loss 0.3900 (0.3900)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [167][64/196]	Time 0.145 (0.114)	Data 0.000 (0.007)	Loss 0.4965 (0.4862)	Acc@1 81.641 (83.191)	Acc@5 100.000 (99.237)
Epoch: [167][128/196]	Time 0.140 (0.113)	Data 0.000 (0.004)	Loss 0.4513 (0.4883)	Acc@1 83.984 (83.161)	Acc@5 99.609 (99.234)
Epoch: [167][192/196]	Time 0.106 (0.113)	Data 0.000 (0.003)	Loss 0.4776 (0.4920)	Acc@1 86.328 (83.035)	Acc@5 98.047 (99.213)
after train
test acc: 75.91
Epoche: [168/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [168][0/196]	Time 0.179 (0.179)	Data 0.459 (0.459)	Loss 0.4957 (0.4957)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [168][64/196]	Time 0.108 (0.116)	Data 0.000 (0.007)	Loss 0.4801 (0.4716)	Acc@1 84.375 (83.798)	Acc@5 99.219 (99.351)
Epoch: [168][128/196]	Time 0.102 (0.114)	Data 0.000 (0.004)	Loss 0.4952 (0.4794)	Acc@1 83.984 (83.560)	Acc@5 99.609 (99.313)
Epoch: [168][192/196]	Time 0.111 (0.113)	Data 0.000 (0.003)	Loss 0.4481 (0.4866)	Acc@1 84.375 (83.276)	Acc@5 99.219 (99.255)
after train
test acc: 77.83
Epoche: [169/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [169][0/196]	Time 0.172 (0.172)	Data 0.704 (0.704)	Loss 0.4905 (0.4905)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [169][64/196]	Time 0.123 (0.120)	Data 0.000 (0.011)	Loss 0.4382 (0.4857)	Acc@1 85.547 (83.179)	Acc@5 99.219 (99.279)
Epoch: [169][128/196]	Time 0.102 (0.115)	Data 0.000 (0.006)	Loss 0.4598 (0.4789)	Acc@1 82.422 (83.358)	Acc@5 99.219 (99.340)
Epoch: [169][192/196]	Time 0.096 (0.113)	Data 0.000 (0.004)	Loss 0.4729 (0.4817)	Acc@1 82.031 (83.296)	Acc@5 99.609 (99.308)
after train
test acc: 79.18
Epoche: [170/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [170][0/196]	Time 0.168 (0.168)	Data 0.574 (0.574)	Loss 0.4621 (0.4621)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [170][64/196]	Time 0.141 (0.111)	Data 0.000 (0.009)	Loss 0.4403 (0.4730)	Acc@1 85.547 (83.528)	Acc@5 99.219 (99.339)
Epoch: [170][128/196]	Time 0.100 (0.113)	Data 0.000 (0.005)	Loss 0.4587 (0.4792)	Acc@1 82.812 (83.285)	Acc@5 99.609 (99.307)
Epoch: [170][192/196]	Time 0.091 (0.111)	Data 0.000 (0.003)	Loss 0.4919 (0.4800)	Acc@1 84.375 (83.213)	Acc@5 98.828 (99.279)
after train
test acc: 76.76
Epoche: [171/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [171][0/196]	Time 0.215 (0.215)	Data 0.526 (0.526)	Loss 0.5186 (0.5186)	Acc@1 82.812 (82.812)	Acc@5 99.609 (99.609)
Epoch: [171][64/196]	Time 0.101 (0.114)	Data 0.000 (0.008)	Loss 0.4727 (0.4719)	Acc@1 82.422 (83.672)	Acc@5 99.219 (99.279)
Epoch: [171][128/196]	Time 0.155 (0.113)	Data 0.000 (0.004)	Loss 0.5318 (0.4765)	Acc@1 81.250 (83.418)	Acc@5 99.219 (99.252)
Epoch: [171][192/196]	Time 0.174 (0.115)	Data 0.000 (0.003)	Loss 0.3853 (0.4731)	Acc@1 85.547 (83.606)	Acc@5 99.219 (99.300)
after train
test acc: 77.03
Epoche: [172/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [172][0/196]	Time 0.181 (0.181)	Data 0.530 (0.530)	Loss 0.3891 (0.3891)	Acc@1 87.891 (87.891)	Acc@5 100.000 (100.000)
Epoch: [172][64/196]	Time 0.090 (0.108)	Data 0.000 (0.009)	Loss 0.4219 (0.4664)	Acc@1 86.328 (83.768)	Acc@5 99.609 (99.285)
Epoch: [172][128/196]	Time 0.085 (0.108)	Data 0.000 (0.005)	Loss 0.4728 (0.4688)	Acc@1 83.984 (83.615)	Acc@5 98.047 (99.316)
Epoch: [172][192/196]	Time 0.097 (0.109)	Data 0.000 (0.003)	Loss 0.5147 (0.4706)	Acc@1 83.594 (83.563)	Acc@5 98.828 (99.302)
after train
test acc: 76.35
Epoche: [173/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [173][0/196]	Time 0.160 (0.160)	Data 0.606 (0.606)	Loss 0.5649 (0.5649)	Acc@1 78.906 (78.906)	Acc@5 99.609 (99.609)
Epoch: [173][64/196]	Time 0.138 (0.112)	Data 0.000 (0.010)	Loss 0.5202 (0.4631)	Acc@1 81.250 (83.912)	Acc@5 98.438 (99.351)
Epoch: [173][128/196]	Time 0.135 (0.112)	Data 0.000 (0.005)	Loss 0.4840 (0.4768)	Acc@1 83.984 (83.612)	Acc@5 98.828 (99.252)
Epoch: [173][192/196]	Time 0.110 (0.110)	Data 0.000 (0.003)	Loss 0.5223 (0.4743)	Acc@1 80.859 (83.571)	Acc@5 99.609 (99.294)
after train
test acc: 79.47
Epoche: [174/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [174][0/196]	Time 0.165 (0.165)	Data 0.497 (0.497)	Loss 0.4940 (0.4940)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [174][64/196]	Time 0.129 (0.113)	Data 0.000 (0.008)	Loss 0.4840 (0.4543)	Acc@1 82.422 (84.405)	Acc@5 99.609 (99.375)
Epoch: [174][128/196]	Time 0.202 (0.114)	Data 0.000 (0.004)	Loss 0.4721 (0.4561)	Acc@1 82.422 (84.066)	Acc@5 98.828 (99.334)
Epoch: [174][192/196]	Time 0.089 (0.116)	Data 0.000 (0.003)	Loss 0.4939 (0.4624)	Acc@1 85.547 (83.934)	Acc@5 99.219 (99.294)
after train
test acc: 80.9
Epoche: [175/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [175][0/196]	Time 0.189 (0.189)	Data 0.549 (0.549)	Loss 0.4360 (0.4360)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [175][64/196]	Time 0.113 (0.115)	Data 0.000 (0.009)	Loss 0.3995 (0.4620)	Acc@1 84.766 (83.888)	Acc@5 99.609 (99.375)
Epoch: [175][128/196]	Time 0.137 (0.112)	Data 0.000 (0.005)	Loss 0.5142 (0.4585)	Acc@1 83.203 (83.915)	Acc@5 100.000 (99.413)
Epoch: [175][192/196]	Time 0.093 (0.111)	Data 0.000 (0.003)	Loss 0.4584 (0.4615)	Acc@1 83.594 (83.881)	Acc@5 99.219 (99.393)
after train
test acc: 78.83
Epoche: [176/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [176][0/196]	Time 0.170 (0.170)	Data 0.504 (0.504)	Loss 0.3936 (0.3936)	Acc@1 83.203 (83.203)	Acc@5 100.000 (100.000)
Epoch: [176][64/196]	Time 0.198 (0.113)	Data 0.000 (0.008)	Loss 0.4783 (0.4531)	Acc@1 80.469 (84.014)	Acc@5 98.438 (99.393)
Epoch: [176][128/196]	Time 0.116 (0.114)	Data 0.000 (0.004)	Loss 0.4914 (0.4533)	Acc@1 82.422 (84.063)	Acc@5 98.828 (99.319)
Epoch: [176][192/196]	Time 0.093 (0.112)	Data 0.000 (0.003)	Loss 0.5217 (0.4545)	Acc@1 82.812 (83.984)	Acc@5 98.438 (99.316)
after train
test acc: 79.91
Epoche: [177/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [177][0/196]	Time 0.159 (0.159)	Data 0.400 (0.400)	Loss 0.4566 (0.4566)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [177][64/196]	Time 0.086 (0.118)	Data 0.000 (0.006)	Loss 0.4357 (0.4483)	Acc@1 86.328 (84.531)	Acc@5 99.219 (99.363)
Epoch: [177][128/196]	Time 0.102 (0.119)	Data 0.000 (0.003)	Loss 0.5224 (0.4519)	Acc@1 84.375 (84.269)	Acc@5 98.047 (99.434)
Epoch: [177][192/196]	Time 0.096 (0.117)	Data 0.000 (0.002)	Loss 0.4246 (0.4538)	Acc@1 83.594 (84.235)	Acc@5 100.000 (99.387)
after train
test acc: 80.93
Epoche: [178/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [178][0/196]	Time 0.145 (0.145)	Data 0.476 (0.476)	Loss 0.4054 (0.4054)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [178][64/196]	Time 0.120 (0.117)	Data 0.000 (0.008)	Loss 0.4623 (0.4550)	Acc@1 84.766 (84.297)	Acc@5 99.219 (99.267)
Epoch: [178][128/196]	Time 0.124 (0.116)	Data 0.000 (0.004)	Loss 0.4770 (0.4514)	Acc@1 83.594 (84.317)	Acc@5 100.000 (99.334)
Epoch: [178][192/196]	Time 0.114 (0.115)	Data 0.000 (0.003)	Loss 0.4119 (0.4507)	Acc@1 85.156 (84.385)	Acc@5 99.219 (99.348)
after train
test acc: 80.22
Epoche: [179/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [179][0/196]	Time 0.213 (0.213)	Data 0.458 (0.458)	Loss 0.3999 (0.3999)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [179][64/196]	Time 0.099 (0.116)	Data 0.000 (0.007)	Loss 0.4123 (0.4417)	Acc@1 87.109 (84.808)	Acc@5 98.828 (99.411)
Epoch: [179][128/196]	Time 0.125 (0.116)	Data 0.000 (0.004)	Loss 0.5025 (0.4455)	Acc@1 83.594 (84.681)	Acc@5 98.828 (99.391)
Epoch: [179][192/196]	Time 0.086 (0.113)	Data 0.000 (0.003)	Loss 0.3972 (0.4474)	Acc@1 87.109 (84.527)	Acc@5 99.219 (99.371)
after train
test acc: 79.93
Epoche: [180/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [180][0/196]	Time 0.181 (0.181)	Data 0.719 (0.719)	Loss 0.4655 (0.4655)	Acc@1 83.594 (83.594)	Acc@5 99.609 (99.609)
Epoch: [180][64/196]	Time 0.098 (0.119)	Data 0.000 (0.012)	Loss 0.3899 (0.4497)	Acc@1 85.938 (84.435)	Acc@5 99.219 (99.309)
Epoch: [180][128/196]	Time 0.104 (0.116)	Data 0.000 (0.006)	Loss 0.4219 (0.4445)	Acc@1 81.641 (84.584)	Acc@5 100.000 (99.355)
Epoch: [180][192/196]	Time 0.093 (0.115)	Data 0.000 (0.004)	Loss 0.4682 (0.4435)	Acc@1 83.984 (84.583)	Acc@5 98.047 (99.360)
after train
test acc: 73.96
Epoche: [181/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [181][0/196]	Time 0.171 (0.171)	Data 0.544 (0.544)	Loss 0.3791 (0.3791)	Acc@1 87.109 (87.109)	Acc@5 98.828 (98.828)
Epoch: [181][64/196]	Time 0.092 (0.113)	Data 0.000 (0.009)	Loss 0.4789 (0.4337)	Acc@1 82.812 (84.591)	Acc@5 98.828 (99.411)
Epoch: [181][128/196]	Time 0.094 (0.112)	Data 0.000 (0.005)	Loss 0.4438 (0.4344)	Acc@1 85.156 (84.702)	Acc@5 99.609 (99.428)
Epoch: [181][192/196]	Time 0.106 (0.111)	Data 0.000 (0.003)	Loss 0.4120 (0.4365)	Acc@1 86.719 (84.735)	Acc@5 100.000 (99.415)
after train
test acc: 80.45
Epoche: [182/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [182][0/196]	Time 0.143 (0.143)	Data 0.482 (0.482)	Loss 0.3658 (0.3658)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [182][64/196]	Time 0.116 (0.119)	Data 0.000 (0.008)	Loss 0.5145 (0.4411)	Acc@1 82.422 (84.525)	Acc@5 98.828 (99.453)
Epoch: [182][128/196]	Time 0.101 (0.115)	Data 0.000 (0.004)	Loss 0.4390 (0.4420)	Acc@1 86.328 (84.463)	Acc@5 100.000 (99.425)
Epoch: [182][192/196]	Time 0.093 (0.115)	Data 0.000 (0.003)	Loss 0.4504 (0.4457)	Acc@1 85.938 (84.500)	Acc@5 98.047 (99.397)
after train
test acc: 81.59
Epoche: [183/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [183][0/196]	Time 0.132 (0.132)	Data 0.524 (0.524)	Loss 0.3501 (0.3501)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [183][64/196]	Time 0.111 (0.119)	Data 0.000 (0.009)	Loss 0.4826 (0.4224)	Acc@1 84.766 (85.403)	Acc@5 99.219 (99.525)
Epoch: [183][128/196]	Time 0.103 (0.112)	Data 0.000 (0.004)	Loss 0.3949 (0.4255)	Acc@1 86.719 (85.247)	Acc@5 99.609 (99.464)
Epoch: [183][192/196]	Time 0.090 (0.111)	Data 0.000 (0.003)	Loss 0.5154 (0.4345)	Acc@1 82.422 (84.841)	Acc@5 100.000 (99.413)
after train
test acc: 81.2
Epoche: [184/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [184][0/196]	Time 0.172 (0.172)	Data 0.506 (0.506)	Loss 0.4541 (0.4541)	Acc@1 82.031 (82.031)	Acc@5 99.609 (99.609)
Epoch: [184][64/196]	Time 0.142 (0.108)	Data 0.000 (0.008)	Loss 0.3744 (0.4344)	Acc@1 87.500 (85.174)	Acc@5 99.609 (99.483)
Epoch: [184][128/196]	Time 0.104 (0.112)	Data 0.000 (0.004)	Loss 0.4188 (0.4331)	Acc@1 83.594 (85.050)	Acc@5 100.000 (99.431)
Epoch: [184][192/196]	Time 0.095 (0.111)	Data 0.000 (0.003)	Loss 0.4162 (0.4292)	Acc@1 85.547 (85.009)	Acc@5 99.219 (99.435)
after train
test acc: 81.1
Epoche: [185/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [185][0/196]	Time 0.133 (0.133)	Data 0.486 (0.486)	Loss 0.4388 (0.4388)	Acc@1 82.422 (82.422)	Acc@5 99.609 (99.609)
Epoch: [185][64/196]	Time 0.099 (0.119)	Data 0.000 (0.008)	Loss 0.4737 (0.4303)	Acc@1 86.328 (84.934)	Acc@5 99.609 (99.435)
Epoch: [185][128/196]	Time 0.118 (0.115)	Data 0.000 (0.004)	Loss 0.3724 (0.4294)	Acc@1 87.109 (85.150)	Acc@5 100.000 (99.425)
Epoch: [185][192/196]	Time 0.101 (0.117)	Data 0.000 (0.003)	Loss 0.3828 (0.4294)	Acc@1 86.328 (85.089)	Acc@5 99.609 (99.452)
after train
test acc: 80.38
Epoche: [186/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [186][0/196]	Time 0.135 (0.135)	Data 0.471 (0.471)	Loss 0.3757 (0.3757)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [186][64/196]	Time 0.091 (0.110)	Data 0.000 (0.008)	Loss 0.3776 (0.4212)	Acc@1 87.891 (85.325)	Acc@5 99.609 (99.465)
Epoch: [186][128/196]	Time 0.105 (0.112)	Data 0.000 (0.004)	Loss 0.4666 (0.4244)	Acc@1 85.547 (85.283)	Acc@5 100.000 (99.470)
Epoch: [186][192/196]	Time 0.091 (0.111)	Data 0.000 (0.003)	Loss 0.4366 (0.4252)	Acc@1 86.328 (85.144)	Acc@5 99.219 (99.482)
after train
test acc: 81.12
Epoche: [187/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [187][0/196]	Time 0.226 (0.226)	Data 0.394 (0.394)	Loss 0.3568 (0.3568)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [187][64/196]	Time 0.103 (0.118)	Data 0.000 (0.007)	Loss 0.4623 (0.4134)	Acc@1 82.812 (85.715)	Acc@5 100.000 (99.417)
Epoch: [187][128/196]	Time 0.142 (0.115)	Data 0.000 (0.003)	Loss 0.4019 (0.4257)	Acc@1 85.547 (85.317)	Acc@5 99.219 (99.361)
Epoch: [187][192/196]	Time 0.088 (0.112)	Data 0.000 (0.002)	Loss 0.3754 (0.4250)	Acc@1 87.500 (85.142)	Acc@5 98.828 (99.393)
after train
test acc: 80.06
Epoche: [188/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [188][0/196]	Time 0.202 (0.202)	Data 0.555 (0.555)	Loss 0.4108 (0.4108)	Acc@1 87.891 (87.891)	Acc@5 98.828 (98.828)
Epoch: [188][64/196]	Time 0.142 (0.114)	Data 0.000 (0.009)	Loss 0.4387 (0.4231)	Acc@1 84.766 (85.373)	Acc@5 99.219 (99.429)
Epoch: [188][128/196]	Time 0.082 (0.115)	Data 0.000 (0.005)	Loss 0.4290 (0.4196)	Acc@1 85.547 (85.447)	Acc@5 99.219 (99.440)
Epoch: [188][192/196]	Time 0.102 (0.114)	Data 0.000 (0.003)	Loss 0.4437 (0.4203)	Acc@1 86.719 (85.377)	Acc@5 99.609 (99.466)
after train
test acc: 81.4
Epoche: [189/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [189][0/196]	Time 0.192 (0.192)	Data 0.498 (0.498)	Loss 0.5596 (0.5596)	Acc@1 79.297 (79.297)	Acc@5 99.219 (99.219)
Epoch: [189][64/196]	Time 0.106 (0.117)	Data 0.000 (0.008)	Loss 0.4170 (0.4170)	Acc@1 83.594 (85.168)	Acc@5 99.609 (99.399)
Epoch: [189][128/196]	Time 0.098 (0.116)	Data 0.000 (0.004)	Loss 0.4530 (0.4244)	Acc@1 83.203 (84.938)	Acc@5 99.609 (99.446)
Epoch: [189][192/196]	Time 0.108 (0.114)	Data 0.000 (0.003)	Loss 0.3410 (0.4239)	Acc@1 87.500 (85.087)	Acc@5 100.000 (99.431)
after train
test acc: 80.91
Epoche: [190/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [190][0/196]	Time 0.173 (0.173)	Data 0.527 (0.527)	Loss 0.3329 (0.3329)	Acc@1 89.453 (89.453)	Acc@5 99.219 (99.219)
Epoch: [190][64/196]	Time 0.085 (0.109)	Data 0.000 (0.008)	Loss 0.3653 (0.4132)	Acc@1 87.891 (85.487)	Acc@5 99.219 (99.441)
Epoch: [190][128/196]	Time 0.090 (0.109)	Data 0.000 (0.004)	Loss 0.4223 (0.4143)	Acc@1 85.547 (85.489)	Acc@5 99.219 (99.440)
Epoch: [190][192/196]	Time 0.090 (0.109)	Data 0.000 (0.003)	Loss 0.3756 (0.4153)	Acc@1 87.109 (85.472)	Acc@5 100.000 (99.427)
after train
test acc: 79.65
Epoche: [191/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [191][0/196]	Time 0.171 (0.171)	Data 0.425 (0.425)	Loss 0.3528 (0.3528)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [191][64/196]	Time 0.092 (0.119)	Data 0.000 (0.007)	Loss 0.5538 (0.4168)	Acc@1 81.250 (85.343)	Acc@5 98.828 (99.513)
Epoch: [191][128/196]	Time 0.098 (0.114)	Data 0.000 (0.004)	Loss 0.3850 (0.4128)	Acc@1 84.375 (85.423)	Acc@5 100.000 (99.485)
Epoch: [191][192/196]	Time 0.108 (0.115)	Data 0.000 (0.003)	Loss 0.5080 (0.4161)	Acc@1 81.250 (85.393)	Acc@5 100.000 (99.500)
after train
test acc: 78.14
Epoche: [192/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [192][0/196]	Time 0.211 (0.211)	Data 0.430 (0.430)	Loss 0.3976 (0.3976)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [192][64/196]	Time 0.085 (0.112)	Data 0.000 (0.007)	Loss 0.4427 (0.4063)	Acc@1 83.203 (85.944)	Acc@5 99.219 (99.561)
Epoch: [192][128/196]	Time 0.176 (0.112)	Data 0.000 (0.004)	Loss 0.4328 (0.4120)	Acc@1 84.375 (85.813)	Acc@5 99.609 (99.443)
Epoch: [192][192/196]	Time 0.120 (0.112)	Data 0.000 (0.003)	Loss 0.4723 (0.4127)	Acc@1 85.547 (85.816)	Acc@5 98.828 (99.462)
after train
test acc: 81.51
Epoche: [193/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [193][0/196]	Time 0.219 (0.219)	Data 0.455 (0.455)	Loss 0.2875 (0.2875)	Acc@1 89.062 (89.062)	Acc@5 99.609 (99.609)
Epoch: [193][64/196]	Time 0.109 (0.117)	Data 0.000 (0.007)	Loss 0.4405 (0.4013)	Acc@1 84.766 (86.058)	Acc@5 100.000 (99.531)
Epoch: [193][128/196]	Time 0.093 (0.115)	Data 0.000 (0.004)	Loss 0.3185 (0.4022)	Acc@1 88.672 (86.001)	Acc@5 99.609 (99.512)
Epoch: [193][192/196]	Time 0.093 (0.112)	Data 0.000 (0.003)	Loss 0.5577 (0.4049)	Acc@1 82.812 (85.828)	Acc@5 98.828 (99.486)
after train
test acc: 81.42
Epoche: [194/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [194][0/196]	Time 0.187 (0.187)	Data 0.493 (0.493)	Loss 0.3667 (0.3667)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [194][64/196]	Time 0.127 (0.116)	Data 0.000 (0.008)	Loss 0.3337 (0.4049)	Acc@1 87.500 (85.871)	Acc@5 99.609 (99.381)
Epoch: [194][128/196]	Time 0.104 (0.118)	Data 0.000 (0.004)	Loss 0.4516 (0.4031)	Acc@1 83.594 (85.956)	Acc@5 99.609 (99.479)
Epoch: [194][192/196]	Time 0.090 (0.116)	Data 0.000 (0.003)	Loss 0.4076 (0.4056)	Acc@1 89.453 (85.917)	Acc@5 99.609 (99.484)
after train
test acc: 82.54
Epoche: [195/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [195][0/196]	Time 0.185 (0.185)	Data 0.523 (0.523)	Loss 0.4152 (0.4152)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [195][64/196]	Time 0.174 (0.111)	Data 0.000 (0.008)	Loss 0.4013 (0.4001)	Acc@1 86.719 (85.950)	Acc@5 100.000 (99.495)
Epoch: [195][128/196]	Time 0.101 (0.111)	Data 0.000 (0.004)	Loss 0.3828 (0.4036)	Acc@1 84.375 (85.913)	Acc@5 100.000 (99.491)
Epoch: [195][192/196]	Time 0.093 (0.111)	Data 0.000 (0.003)	Loss 0.4655 (0.4051)	Acc@1 84.766 (85.887)	Acc@5 99.609 (99.441)
after train
test acc: 80.24
Epoche: [196/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [196][0/196]	Time 0.150 (0.150)	Data 0.396 (0.396)	Loss 0.2831 (0.2831)	Acc@1 91.406 (91.406)	Acc@5 99.609 (99.609)
Epoch: [196][64/196]	Time 0.106 (0.116)	Data 0.000 (0.006)	Loss 0.4604 (0.3912)	Acc@1 81.250 (86.082)	Acc@5 100.000 (99.627)
Epoch: [196][128/196]	Time 0.131 (0.115)	Data 0.000 (0.003)	Loss 0.3762 (0.3997)	Acc@1 87.891 (86.028)	Acc@5 99.609 (99.522)
Epoch: [196][192/196]	Time 0.089 (0.114)	Data 0.000 (0.002)	Loss 0.4054 (0.3986)	Acc@1 85.547 (86.035)	Acc@5 99.609 (99.520)
after train
test acc: 82.21
Epoche: [197/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [197][0/196]	Time 0.183 (0.183)	Data 0.614 (0.614)	Loss 0.3955 (0.3955)	Acc@1 83.984 (83.984)	Acc@5 100.000 (100.000)
Epoch: [197][64/196]	Time 0.138 (0.109)	Data 0.000 (0.010)	Loss 0.4518 (0.3976)	Acc@1 82.812 (86.124)	Acc@5 100.000 (99.525)
Epoch: [197][128/196]	Time 0.153 (0.113)	Data 0.000 (0.005)	Loss 0.3119 (0.3941)	Acc@1 89.453 (86.322)	Acc@5 99.609 (99.549)
Epoch: [197][192/196]	Time 0.093 (0.113)	Data 0.000 (0.003)	Loss 0.4316 (0.4005)	Acc@1 84.766 (86.004)	Acc@5 98.438 (99.520)
after train
test acc: 80.44
Epoche: [198/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [198][0/196]	Time 0.188 (0.188)	Data 0.393 (0.393)	Loss 0.4601 (0.4601)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [198][64/196]	Time 0.117 (0.116)	Data 0.000 (0.006)	Loss 0.3838 (0.4085)	Acc@1 83.203 (85.667)	Acc@5 100.000 (99.441)
Epoch: [198][128/196]	Time 0.103 (0.112)	Data 0.000 (0.003)	Loss 0.3044 (0.3971)	Acc@1 87.109 (86.010)	Acc@5 100.000 (99.479)
Epoch: [198][192/196]	Time 0.098 (0.112)	Data 0.000 (0.002)	Loss 0.3782 (0.3965)	Acc@1 89.453 (86.168)	Acc@5 98.828 (99.490)
after train
test acc: 81.12
Epoche: [199/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [199][0/196]	Time 0.182 (0.182)	Data 0.474 (0.474)	Loss 0.4551 (0.4551)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [199][64/196]	Time 0.091 (0.113)	Data 0.000 (0.008)	Loss 0.4254 (0.3862)	Acc@1 85.156 (86.617)	Acc@5 99.219 (99.513)
Epoch: [199][128/196]	Time 0.102 (0.114)	Data 0.000 (0.004)	Loss 0.3973 (0.3893)	Acc@1 86.328 (86.498)	Acc@5 99.609 (99.546)
Epoch: [199][192/196]	Time 0.105 (0.114)	Data 0.000 (0.003)	Loss 0.5070 (0.3937)	Acc@1 83.594 (86.415)	Acc@5 99.609 (99.500)
after train
test acc: 79.38
Epoche: [200/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [200][0/196]	Time 0.165 (0.165)	Data 0.563 (0.563)	Loss 0.3687 (0.3687)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [200][64/196]	Time 0.140 (0.115)	Data 0.000 (0.009)	Loss 0.2925 (0.3847)	Acc@1 89.453 (86.430)	Acc@5 99.609 (99.561)
Epoch: [200][128/196]	Time 0.113 (0.117)	Data 0.000 (0.005)	Loss 0.3799 (0.3899)	Acc@1 86.719 (86.289)	Acc@5 100.000 (99.570)
Epoch: [200][192/196]	Time 0.085 (0.114)	Data 0.000 (0.003)	Loss 0.3786 (0.3914)	Acc@1 87.891 (86.231)	Acc@5 99.609 (99.528)
after train
test acc: 80.38
Epoche: [201/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [201][0/196]	Time 0.212 (0.212)	Data 0.507 (0.507)	Loss 0.4036 (0.4036)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [201][64/196]	Time 0.147 (0.120)	Data 0.000 (0.008)	Loss 0.3378 (0.3798)	Acc@1 88.672 (86.953)	Acc@5 99.609 (99.543)
Epoch: [201][128/196]	Time 0.113 (0.117)	Data 0.000 (0.004)	Loss 0.2741 (0.3857)	Acc@1 89.844 (86.719)	Acc@5 100.000 (99.519)
Epoch: [201][192/196]	Time 0.113 (0.115)	Data 0.000 (0.003)	Loss 0.4056 (0.3898)	Acc@1 85.156 (86.508)	Acc@5 100.000 (99.504)
after train
test acc: 82.17
Epoche: [202/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [202][0/196]	Time 0.103 (0.103)	Data 0.439 (0.439)	Loss 0.2459 (0.2459)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [202][64/196]	Time 0.094 (0.121)	Data 0.000 (0.007)	Loss 0.4197 (0.3898)	Acc@1 85.156 (86.250)	Acc@5 99.609 (99.441)
Epoch: [202][128/196]	Time 0.171 (0.115)	Data 0.005 (0.004)	Loss 0.2955 (0.3897)	Acc@1 91.016 (86.343)	Acc@5 99.609 (99.497)
Epoch: [202][192/196]	Time 0.094 (0.114)	Data 0.000 (0.003)	Loss 0.3468 (0.3883)	Acc@1 87.891 (86.401)	Acc@5 99.609 (99.484)
after train
test acc: 82.73
Epoche: [203/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [203][0/196]	Time 0.175 (0.175)	Data 0.481 (0.481)	Loss 0.3397 (0.3397)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [203][64/196]	Time 0.173 (0.118)	Data 0.000 (0.008)	Loss 0.3475 (0.3836)	Acc@1 89.844 (86.683)	Acc@5 99.609 (99.567)
Epoch: [203][128/196]	Time 0.103 (0.116)	Data 0.000 (0.004)	Loss 0.3640 (0.3813)	Acc@1 88.672 (86.779)	Acc@5 99.609 (99.558)
Epoch: [203][192/196]	Time 0.128 (0.113)	Data 0.000 (0.003)	Loss 0.4683 (0.3827)	Acc@1 83.984 (86.699)	Acc@5 99.219 (99.518)
after train
test acc: 80.53
Epoche: [204/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [204][0/196]	Time 0.192 (0.192)	Data 0.497 (0.497)	Loss 0.3741 (0.3741)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [204][64/196]	Time 0.102 (0.110)	Data 0.000 (0.008)	Loss 0.3303 (0.3758)	Acc@1 89.453 (86.869)	Acc@5 99.609 (99.567)
Epoch: [204][128/196]	Time 0.115 (0.110)	Data 0.000 (0.004)	Loss 0.4611 (0.3829)	Acc@1 83.594 (86.643)	Acc@5 99.609 (99.561)
Epoch: [204][192/196]	Time 0.096 (0.110)	Data 0.000 (0.003)	Loss 0.3393 (0.3842)	Acc@1 87.891 (86.587)	Acc@5 99.609 (99.530)
after train
test acc: 82.18
Epoche: [205/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [205][0/196]	Time 0.146 (0.146)	Data 0.474 (0.474)	Loss 0.4055 (0.4055)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [205][64/196]	Time 0.127 (0.115)	Data 0.000 (0.008)	Loss 0.3336 (0.3756)	Acc@1 87.891 (86.809)	Acc@5 99.609 (99.597)
Epoch: [205][128/196]	Time 0.103 (0.114)	Data 0.000 (0.004)	Loss 0.4114 (0.3790)	Acc@1 85.156 (86.819)	Acc@5 99.219 (99.522)
Epoch: [205][192/196]	Time 0.100 (0.113)	Data 0.000 (0.003)	Loss 0.3161 (0.3819)	Acc@1 89.453 (86.666)	Acc@5 100.000 (99.528)
after train
test acc: 80.96
Epoche: [206/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [206][0/196]	Time 0.135 (0.135)	Data 0.484 (0.484)	Loss 0.4138 (0.4138)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [206][64/196]	Time 0.101 (0.113)	Data 0.000 (0.008)	Loss 0.3788 (0.3722)	Acc@1 87.109 (87.314)	Acc@5 99.219 (99.483)
Epoch: [206][128/196]	Time 0.119 (0.116)	Data 0.000 (0.004)	Loss 0.3307 (0.3794)	Acc@1 87.500 (86.791)	Acc@5 100.000 (99.500)
Epoch: [206][192/196]	Time 0.098 (0.114)	Data 0.000 (0.003)	Loss 0.3750 (0.3788)	Acc@1 87.500 (86.796)	Acc@5 99.609 (99.522)
after train
test acc: 79.98
Epoche: [207/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [207][0/196]	Time 0.159 (0.159)	Data 0.660 (0.660)	Loss 0.4145 (0.4145)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [207][64/196]	Time 0.103 (0.115)	Data 0.000 (0.010)	Loss 0.3716 (0.3824)	Acc@1 88.672 (86.623)	Acc@5 99.609 (99.501)
Epoch: [207][128/196]	Time 0.116 (0.112)	Data 0.000 (0.005)	Loss 0.3656 (0.3747)	Acc@1 89.844 (86.846)	Acc@5 99.609 (99.540)
Epoch: [207][192/196]	Time 0.099 (0.113)	Data 0.000 (0.004)	Loss 0.3989 (0.3758)	Acc@1 85.547 (86.889)	Acc@5 100.000 (99.553)
after train
test acc: 80.59
Epoche: [208/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [208][0/196]	Time 0.185 (0.185)	Data 0.400 (0.400)	Loss 0.2748 (0.2748)	Acc@1 89.453 (89.453)	Acc@5 99.219 (99.219)
Epoch: [208][64/196]	Time 0.118 (0.109)	Data 0.000 (0.007)	Loss 0.3600 (0.3666)	Acc@1 87.891 (87.464)	Acc@5 99.609 (99.531)
Epoch: [208][128/196]	Time 0.136 (0.111)	Data 0.000 (0.003)	Loss 0.3956 (0.3727)	Acc@1 85.938 (87.009)	Acc@5 100.000 (99.555)
Epoch: [208][192/196]	Time 0.102 (0.113)	Data 0.000 (0.002)	Loss 0.3504 (0.3755)	Acc@1 85.938 (86.864)	Acc@5 100.000 (99.537)
after train
test acc: 82.34
Epoche: [209/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [209][0/196]	Time 0.174 (0.174)	Data 0.487 (0.487)	Loss 0.3816 (0.3816)	Acc@1 88.672 (88.672)	Acc@5 99.219 (99.219)
Epoch: [209][64/196]	Time 0.136 (0.111)	Data 0.000 (0.008)	Loss 0.3624 (0.3745)	Acc@1 87.500 (86.851)	Acc@5 99.609 (99.603)
Epoch: [209][128/196]	Time 0.096 (0.113)	Data 0.000 (0.004)	Loss 0.3341 (0.3763)	Acc@1 86.719 (86.858)	Acc@5 99.609 (99.600)
Epoch: [209][192/196]	Time 0.095 (0.113)	Data 0.000 (0.003)	Loss 0.3550 (0.3776)	Acc@1 86.328 (86.763)	Acc@5 99.609 (99.585)
after train
test acc: 82.03
Epoche: [210/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [210][0/196]	Time 0.147 (0.147)	Data 0.473 (0.473)	Loss 0.3882 (0.3882)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [210][64/196]	Time 0.122 (0.112)	Data 0.003 (0.008)	Loss 0.4359 (0.3634)	Acc@1 83.594 (87.188)	Acc@5 99.219 (99.555)
Epoch: [210][128/196]	Time 0.099 (0.112)	Data 0.000 (0.004)	Loss 0.3297 (0.3712)	Acc@1 89.062 (86.931)	Acc@5 100.000 (99.567)
Epoch: [210][192/196]	Time 0.094 (0.112)	Data 0.000 (0.003)	Loss 0.4043 (0.3760)	Acc@1 86.328 (86.830)	Acc@5 100.000 (99.561)
after train
test acc: 81.37
Epoche: [211/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [211][0/196]	Time 0.158 (0.158)	Data 0.463 (0.463)	Loss 0.3690 (0.3690)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [211][64/196]	Time 0.094 (0.117)	Data 0.000 (0.008)	Loss 0.4165 (0.3631)	Acc@1 87.500 (87.668)	Acc@5 99.219 (99.579)
Epoch: [211][128/196]	Time 0.093 (0.114)	Data 0.000 (0.004)	Loss 0.3571 (0.3581)	Acc@1 87.891 (87.654)	Acc@5 98.828 (99.570)
Epoch: [211][192/196]	Time 0.089 (0.112)	Data 0.000 (0.003)	Loss 0.4582 (0.3624)	Acc@1 83.203 (87.449)	Acc@5 98.438 (99.539)
after train
test acc: 79.36
Epoche: [212/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [212][0/196]	Time 0.173 (0.173)	Data 0.432 (0.432)	Loss 0.3178 (0.3178)	Acc@1 89.844 (89.844)	Acc@5 98.828 (98.828)
Epoch: [212][64/196]	Time 0.116 (0.114)	Data 0.000 (0.007)	Loss 0.3472 (0.3616)	Acc@1 86.719 (87.374)	Acc@5 99.609 (99.561)
Epoch: [212][128/196]	Time 0.088 (0.109)	Data 0.000 (0.004)	Loss 0.4570 (0.3641)	Acc@1 83.203 (87.364)	Acc@5 99.219 (99.552)
Epoch: [212][192/196]	Time 0.081 (0.110)	Data 0.000 (0.003)	Loss 0.4344 (0.3673)	Acc@1 84.375 (87.202)	Acc@5 99.219 (99.545)
after train
test acc: 81.48
Epoche: [213/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [213][0/196]	Time 0.227 (0.227)	Data 0.450 (0.450)	Loss 0.3208 (0.3208)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [213][64/196]	Time 0.104 (0.114)	Data 0.000 (0.008)	Loss 0.3740 (0.3648)	Acc@1 85.547 (87.145)	Acc@5 100.000 (99.567)
Epoch: [213][128/196]	Time 0.096 (0.114)	Data 0.000 (0.004)	Loss 0.3272 (0.3662)	Acc@1 90.234 (87.227)	Acc@5 99.609 (99.528)
Epoch: [213][192/196]	Time 0.086 (0.114)	Data 0.000 (0.003)	Loss 0.4248 (0.3670)	Acc@1 86.328 (87.251)	Acc@5 99.609 (99.526)
after train
test acc: 80.62
Epoche: [214/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [214][0/196]	Time 0.204 (0.204)	Data 0.391 (0.391)	Loss 0.3582 (0.3582)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [214][64/196]	Time 0.096 (0.107)	Data 0.000 (0.006)	Loss 0.4094 (0.3561)	Acc@1 89.062 (87.530)	Acc@5 99.219 (99.603)
Epoch: [214][128/196]	Time 0.129 (0.116)	Data 0.000 (0.003)	Loss 0.3306 (0.3624)	Acc@1 89.062 (87.309)	Acc@5 99.609 (99.600)
Epoch: [214][192/196]	Time 0.086 (0.115)	Data 0.000 (0.002)	Loss 0.3666 (0.3668)	Acc@1 88.672 (87.166)	Acc@5 99.609 (99.599)
after train
test acc: 82.01
Epoche: [215/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [215][0/196]	Time 0.142 (0.142)	Data 0.614 (0.614)	Loss 0.3974 (0.3974)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [215][64/196]	Time 0.112 (0.114)	Data 0.000 (0.010)	Loss 0.3742 (0.3560)	Acc@1 86.719 (87.566)	Acc@5 99.609 (99.567)
Epoch: [215][128/196]	Time 0.126 (0.113)	Data 0.000 (0.005)	Loss 0.2992 (0.3615)	Acc@1 92.188 (87.333)	Acc@5 99.609 (99.573)
Epoch: [215][192/196]	Time 0.114 (0.114)	Data 0.000 (0.003)	Loss 0.3411 (0.3601)	Acc@1 88.672 (87.411)	Acc@5 100.000 (99.569)
after train
test acc: 81.47
Epoche: [216/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [216][0/196]	Time 0.179 (0.179)	Data 0.409 (0.409)	Loss 0.3624 (0.3624)	Acc@1 87.891 (87.891)	Acc@5 100.000 (100.000)
Epoch: [216][64/196]	Time 0.099 (0.116)	Data 0.000 (0.007)	Loss 0.3793 (0.3579)	Acc@1 87.500 (87.578)	Acc@5 98.828 (99.573)
Epoch: [216][128/196]	Time 0.095 (0.116)	Data 0.000 (0.003)	Loss 0.3395 (0.3587)	Acc@1 87.500 (87.536)	Acc@5 100.000 (99.579)
Epoch: [216][192/196]	Time 0.093 (0.114)	Data 0.000 (0.002)	Loss 0.4465 (0.3626)	Acc@1 84.375 (87.395)	Acc@5 99.219 (99.585)
after train
test acc: 82.37
Epoche: [217/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [217][0/196]	Time 0.239 (0.239)	Data 0.584 (0.584)	Loss 0.3488 (0.3488)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [217][64/196]	Time 0.103 (0.120)	Data 0.000 (0.009)	Loss 0.3109 (0.3711)	Acc@1 87.891 (86.887)	Acc@5 100.000 (99.489)
Epoch: [217][128/196]	Time 0.106 (0.118)	Data 0.000 (0.005)	Loss 0.3202 (0.3641)	Acc@1 89.062 (87.173)	Acc@5 99.609 (99.561)
Epoch: [217][192/196]	Time 0.126 (0.115)	Data 0.000 (0.003)	Loss 0.3964 (0.3656)	Acc@1 86.719 (87.247)	Acc@5 99.609 (99.561)
after train
test acc: 81.94
Epoche: [218/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [218][0/196]	Time 0.142 (0.142)	Data 0.501 (0.501)	Loss 0.3321 (0.3321)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [218][64/196]	Time 0.109 (0.118)	Data 0.000 (0.008)	Loss 0.3785 (0.3625)	Acc@1 86.719 (87.212)	Acc@5 99.609 (99.525)
Epoch: [218][128/196]	Time 0.105 (0.114)	Data 0.000 (0.005)	Loss 0.3181 (0.3546)	Acc@1 87.891 (87.742)	Acc@5 99.609 (99.564)
Epoch: [218][192/196]	Time 0.090 (0.111)	Data 0.000 (0.003)	Loss 0.3526 (0.3586)	Acc@1 87.500 (87.607)	Acc@5 100.000 (99.581)
after train
test acc: 80.88
Epoche: [219/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [219][0/196]	Time 0.165 (0.165)	Data 0.542 (0.542)	Loss 0.2725 (0.2725)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [219][64/196]	Time 0.097 (0.115)	Data 0.000 (0.009)	Loss 0.3446 (0.3536)	Acc@1 89.844 (87.554)	Acc@5 99.609 (99.609)
Epoch: [219][128/196]	Time 0.169 (0.116)	Data 0.000 (0.004)	Loss 0.3829 (0.3499)	Acc@1 83.984 (87.633)	Acc@5 99.219 (99.637)
Epoch: [219][192/196]	Time 0.147 (0.115)	Data 0.000 (0.003)	Loss 0.3875 (0.3560)	Acc@1 87.109 (87.532)	Acc@5 99.609 (99.613)
after train
test acc: 81.4
Epoche: [220/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [220][0/196]	Time 0.138 (0.138)	Data 0.695 (0.695)	Loss 0.3896 (0.3896)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [220][64/196]	Time 0.102 (0.108)	Data 0.000 (0.011)	Loss 0.3543 (0.3481)	Acc@1 87.891 (87.764)	Acc@5 99.609 (99.609)
Epoch: [220][128/196]	Time 0.099 (0.111)	Data 0.000 (0.006)	Loss 0.3330 (0.3520)	Acc@1 87.891 (87.667)	Acc@5 100.000 (99.546)
Epoch: [220][192/196]	Time 0.093 (0.112)	Data 0.000 (0.004)	Loss 0.3348 (0.3535)	Acc@1 89.844 (87.595)	Acc@5 99.609 (99.593)
after train
test acc: 83.59
Epoche: [221/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [221][0/196]	Time 0.175 (0.175)	Data 0.571 (0.571)	Loss 0.3179 (0.3179)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [221][64/196]	Time 0.132 (0.116)	Data 0.000 (0.009)	Loss 0.3102 (0.3470)	Acc@1 88.672 (87.861)	Acc@5 100.000 (99.597)
Epoch: [221][128/196]	Time 0.105 (0.115)	Data 0.000 (0.005)	Loss 0.4056 (0.3533)	Acc@1 85.938 (87.718)	Acc@5 100.000 (99.570)
Epoch: [221][192/196]	Time 0.085 (0.114)	Data 0.000 (0.003)	Loss 0.3012 (0.3551)	Acc@1 88.281 (87.656)	Acc@5 100.000 (99.575)
after train
test acc: 82.86
Epoche: [222/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [222][0/196]	Time 0.204 (0.204)	Data 0.443 (0.443)	Loss 0.3244 (0.3244)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [222][64/196]	Time 0.138 (0.112)	Data 0.000 (0.007)	Loss 0.2705 (0.3441)	Acc@1 89.844 (87.981)	Acc@5 100.000 (99.603)
Epoch: [222][128/196]	Time 0.108 (0.113)	Data 0.000 (0.004)	Loss 0.3447 (0.3471)	Acc@1 87.891 (87.827)	Acc@5 100.000 (99.615)
Epoch: [222][192/196]	Time 0.103 (0.113)	Data 0.000 (0.003)	Loss 0.3215 (0.3503)	Acc@1 88.281 (87.773)	Acc@5 99.219 (99.603)
after train
test acc: 81.65
Epoche: [223/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [223][0/196]	Time 0.115 (0.115)	Data 0.496 (0.496)	Loss 0.3163 (0.3163)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [223][64/196]	Time 0.124 (0.114)	Data 0.000 (0.008)	Loss 0.3314 (0.3509)	Acc@1 90.625 (87.945)	Acc@5 99.219 (99.591)
Epoch: [223][128/196]	Time 0.097 (0.112)	Data 0.000 (0.004)	Loss 0.3118 (0.3546)	Acc@1 86.719 (87.688)	Acc@5 100.000 (99.570)
Epoch: [223][192/196]	Time 0.095 (0.112)	Data 0.000 (0.003)	Loss 0.2809 (0.3530)	Acc@1 88.672 (87.700)	Acc@5 100.000 (99.587)
after train
test acc: 81.39
Epoche: [224/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [224][0/196]	Time 0.198 (0.198)	Data 0.504 (0.504)	Loss 0.3028 (0.3028)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [224][64/196]	Time 0.119 (0.109)	Data 0.000 (0.008)	Loss 0.2813 (0.3409)	Acc@1 89.844 (87.843)	Acc@5 99.609 (99.615)
Epoch: [224][128/196]	Time 0.114 (0.111)	Data 0.000 (0.004)	Loss 0.3463 (0.3455)	Acc@1 87.891 (87.685)	Acc@5 99.609 (99.573)
Epoch: [224][192/196]	Time 0.089 (0.114)	Data 0.000 (0.003)	Loss 0.3545 (0.3464)	Acc@1 85.547 (87.745)	Acc@5 99.609 (99.593)
after train
test acc: 82.05
Epoche: [225/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [225][0/196]	Time 0.200 (0.200)	Data 0.404 (0.404)	Loss 0.4468 (0.4468)	Acc@1 83.984 (83.984)	Acc@5 98.438 (98.438)
Epoch: [225][64/196]	Time 0.086 (0.111)	Data 0.000 (0.006)	Loss 0.3280 (0.3476)	Acc@1 87.500 (87.662)	Acc@5 99.609 (99.609)
Epoch: [225][128/196]	Time 0.151 (0.111)	Data 0.000 (0.003)	Loss 0.3908 (0.3477)	Acc@1 84.375 (87.812)	Acc@5 100.000 (99.582)
Epoch: [225][192/196]	Time 0.112 (0.112)	Data 0.000 (0.002)	Loss 0.3462 (0.3479)	Acc@1 89.453 (87.741)	Acc@5 99.609 (99.585)
after train
test acc: 77.67
Epoche: [226/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [226][0/196]	Time 0.170 (0.170)	Data 0.579 (0.579)	Loss 0.2945 (0.2945)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [226][64/196]	Time 0.114 (0.115)	Data 0.000 (0.009)	Loss 0.3384 (0.3435)	Acc@1 89.844 (88.095)	Acc@5 100.000 (99.609)
Epoch: [226][128/196]	Time 0.127 (0.112)	Data 0.000 (0.005)	Loss 0.3432 (0.3426)	Acc@1 85.156 (88.057)	Acc@5 100.000 (99.615)
Epoch: [226][192/196]	Time 0.095 (0.112)	Data 0.000 (0.003)	Loss 0.3934 (0.3438)	Acc@1 86.719 (87.899)	Acc@5 100.000 (99.632)
after train
test acc: 81.19
Epoche: [227/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [227][0/196]	Time 0.198 (0.198)	Data 0.593 (0.593)	Loss 0.3767 (0.3767)	Acc@1 86.328 (86.328)	Acc@5 99.609 (99.609)
Epoch: [227][64/196]	Time 0.120 (0.110)	Data 0.000 (0.009)	Loss 0.3523 (0.3482)	Acc@1 87.109 (87.740)	Acc@5 99.609 (99.573)
Epoch: [227][128/196]	Time 0.101 (0.112)	Data 0.000 (0.005)	Loss 0.3639 (0.3453)	Acc@1 88.281 (87.882)	Acc@5 100.000 (99.615)
Epoch: [227][192/196]	Time 0.095 (0.111)	Data 0.000 (0.003)	Loss 0.3945 (0.3470)	Acc@1 86.719 (87.814)	Acc@5 99.609 (99.630)
after train
test acc: 81.77
Epoche: [228/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [228][0/196]	Time 0.122 (0.122)	Data 0.588 (0.588)	Loss 0.3516 (0.3516)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [228][64/196]	Time 0.119 (0.112)	Data 0.000 (0.009)	Loss 0.3336 (0.3269)	Acc@1 87.891 (88.389)	Acc@5 100.000 (99.700)
Epoch: [228][128/196]	Time 0.117 (0.119)	Data 0.000 (0.005)	Loss 0.3262 (0.3393)	Acc@1 88.281 (88.066)	Acc@5 100.000 (99.649)
Epoch: [228][192/196]	Time 0.093 (0.115)	Data 0.000 (0.003)	Loss 0.3265 (0.3428)	Acc@1 89.062 (87.984)	Acc@5 98.828 (99.626)
after train
test acc: 77.77
Epoche: [229/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [229][0/196]	Time 0.136 (0.136)	Data 0.421 (0.421)	Loss 0.2768 (0.2768)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [229][64/196]	Time 0.139 (0.116)	Data 0.001 (0.007)	Loss 0.4043 (0.3301)	Acc@1 84.375 (88.269)	Acc@5 99.219 (99.778)
Epoch: [229][128/196]	Time 0.089 (0.115)	Data 0.000 (0.004)	Loss 0.3877 (0.3446)	Acc@1 86.328 (87.860)	Acc@5 99.219 (99.688)
Epoch: [229][192/196]	Time 0.118 (0.113)	Data 0.000 (0.002)	Loss 0.3414 (0.3480)	Acc@1 88.672 (87.791)	Acc@5 99.219 (99.656)
after train
test acc: 81.85
Epoche: [230/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [230][0/196]	Time 0.219 (0.219)	Data 0.437 (0.437)	Loss 0.2636 (0.2636)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [230][64/196]	Time 0.105 (0.115)	Data 0.000 (0.007)	Loss 0.3791 (0.3316)	Acc@1 87.891 (88.365)	Acc@5 99.219 (99.669)
Epoch: [230][128/196]	Time 0.111 (0.116)	Data 0.000 (0.004)	Loss 0.2576 (0.3336)	Acc@1 92.188 (88.290)	Acc@5 100.000 (99.667)
Epoch: [230][192/196]	Time 0.098 (0.115)	Data 0.000 (0.003)	Loss 0.3690 (0.3381)	Acc@1 87.891 (88.111)	Acc@5 99.219 (99.646)
after train
test acc: 82.99
Epoche: [231/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [231][0/196]	Time 0.159 (0.159)	Data 0.528 (0.528)	Loss 0.3831 (0.3831)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [231][64/196]	Time 0.085 (0.122)	Data 0.000 (0.008)	Loss 0.3665 (0.3314)	Acc@1 85.938 (88.239)	Acc@5 100.000 (99.706)
Epoch: [231][128/196]	Time 0.099 (0.115)	Data 0.000 (0.004)	Loss 0.3258 (0.3332)	Acc@1 88.672 (88.221)	Acc@5 99.219 (99.676)
Epoch: [231][192/196]	Time 0.084 (0.113)	Data 0.000 (0.003)	Loss 0.3820 (0.3388)	Acc@1 88.672 (88.138)	Acc@5 99.219 (99.652)
after train
test acc: 83.39
Epoche: [232/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [232][0/196]	Time 0.153 (0.153)	Data 0.460 (0.460)	Loss 0.3503 (0.3503)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [232][64/196]	Time 0.109 (0.114)	Data 0.000 (0.007)	Loss 0.3052 (0.3360)	Acc@1 89.453 (88.227)	Acc@5 100.000 (99.573)
Epoch: [232][128/196]	Time 0.107 (0.112)	Data 0.000 (0.004)	Loss 0.3636 (0.3357)	Acc@1 87.109 (88.260)	Acc@5 98.828 (99.634)
Epoch: [232][192/196]	Time 0.100 (0.114)	Data 0.000 (0.003)	Loss 0.3930 (0.3393)	Acc@1 87.109 (88.121)	Acc@5 99.219 (99.626)
after train
test acc: 82.56
Epoche: [233/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [233][0/196]	Time 0.156 (0.156)	Data 0.513 (0.513)	Loss 0.3216 (0.3216)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [233][64/196]	Time 0.092 (0.116)	Data 0.000 (0.008)	Loss 0.3020 (0.3304)	Acc@1 86.719 (88.666)	Acc@5 100.000 (99.591)
Epoch: [233][128/196]	Time 0.093 (0.114)	Data 0.000 (0.004)	Loss 0.3010 (0.3369)	Acc@1 87.891 (88.263)	Acc@5 100.000 (99.628)
Epoch: [233][192/196]	Time 0.097 (0.113)	Data 0.000 (0.003)	Loss 0.3227 (0.3374)	Acc@1 89.844 (88.214)	Acc@5 99.609 (99.622)
after train
test acc: 83.48
Epoche: [234/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [234][0/196]	Time 0.216 (0.216)	Data 0.652 (0.652)	Loss 0.3546 (0.3546)	Acc@1 88.672 (88.672)	Acc@5 99.609 (99.609)
Epoch: [234][64/196]	Time 0.155 (0.120)	Data 0.000 (0.010)	Loss 0.3149 (0.3330)	Acc@1 87.109 (88.323)	Acc@5 99.609 (99.700)
Epoch: [234][128/196]	Time 0.093 (0.117)	Data 0.000 (0.005)	Loss 0.3324 (0.3287)	Acc@1 86.719 (88.387)	Acc@5 99.219 (99.697)
Epoch: [234][192/196]	Time 0.098 (0.116)	Data 0.000 (0.004)	Loss 0.3079 (0.3326)	Acc@1 87.891 (88.346)	Acc@5 100.000 (99.666)
after train
test acc: 83.41
Epoche: [235/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [235][0/196]	Time 0.162 (0.162)	Data 0.400 (0.400)	Loss 0.3275 (0.3275)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [235][64/196]	Time 0.114 (0.109)	Data 0.000 (0.006)	Loss 0.3318 (0.3293)	Acc@1 89.062 (88.654)	Acc@5 99.219 (99.609)
Epoch: [235][128/196]	Time 0.093 (0.108)	Data 0.000 (0.003)	Loss 0.2824 (0.3249)	Acc@1 89.453 (88.672)	Acc@5 99.609 (99.637)
Epoch: [235][192/196]	Time 0.095 (0.110)	Data 0.000 (0.002)	Loss 0.3058 (0.3305)	Acc@1 89.062 (88.538)	Acc@5 100.000 (99.640)
after train
test acc: 82.87
Epoche: [236/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [236][0/196]	Time 0.186 (0.186)	Data 0.481 (0.481)	Loss 0.2907 (0.2907)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [236][64/196]	Time 0.133 (0.115)	Data 0.000 (0.008)	Loss 0.3201 (0.3246)	Acc@1 88.281 (88.480)	Acc@5 100.000 (99.669)
Epoch: [236][128/196]	Time 0.121 (0.113)	Data 0.000 (0.004)	Loss 0.3667 (0.3257)	Acc@1 89.844 (88.433)	Acc@5 99.609 (99.670)
Epoch: [236][192/196]	Time 0.095 (0.114)	Data 0.000 (0.003)	Loss 0.3238 (0.3309)	Acc@1 89.844 (88.370)	Acc@5 99.219 (99.648)
after train
test acc: 83.32
Epoche: [237/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [237][0/196]	Time 0.145 (0.145)	Data 0.407 (0.407)	Loss 0.3259 (0.3259)	Acc@1 89.453 (89.453)	Acc@5 100.000 (100.000)
Epoch: [237][64/196]	Time 0.087 (0.122)	Data 0.000 (0.007)	Loss 0.3738 (0.3288)	Acc@1 87.109 (88.636)	Acc@5 100.000 (99.615)
Epoch: [237][128/196]	Time 0.123 (0.115)	Data 0.000 (0.004)	Loss 0.3201 (0.3300)	Acc@1 91.016 (88.469)	Acc@5 100.000 (99.643)
Epoch: [237][192/196]	Time 0.099 (0.116)	Data 0.000 (0.002)	Loss 0.2811 (0.3296)	Acc@1 90.625 (88.492)	Acc@5 100.000 (99.632)
after train
test acc: 83.2
Epoche: [238/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [238][0/196]	Time 0.167 (0.167)	Data 0.560 (0.560)	Loss 0.3670 (0.3670)	Acc@1 89.062 (89.062)	Acc@5 99.219 (99.219)
Epoch: [238][64/196]	Time 0.101 (0.112)	Data 0.000 (0.009)	Loss 0.3061 (0.3234)	Acc@1 86.719 (88.516)	Acc@5 99.609 (99.700)
Epoch: [238][128/196]	Time 0.100 (0.110)	Data 0.000 (0.005)	Loss 0.3273 (0.3326)	Acc@1 89.062 (88.206)	Acc@5 100.000 (99.700)
Epoch: [238][192/196]	Time 0.096 (0.112)	Data 0.000 (0.003)	Loss 0.3604 (0.3335)	Acc@1 88.281 (88.190)	Acc@5 100.000 (99.672)
after train
test acc: 83.25
Epoche: [239/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [239][0/196]	Time 0.130 (0.130)	Data 0.456 (0.456)	Loss 0.3437 (0.3437)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [239][64/196]	Time 0.091 (0.113)	Data 0.000 (0.007)	Loss 0.3115 (0.3362)	Acc@1 89.453 (88.468)	Acc@5 99.609 (99.645)
Epoch: [239][128/196]	Time 0.094 (0.115)	Data 0.000 (0.004)	Loss 0.3615 (0.3275)	Acc@1 87.891 (88.629)	Acc@5 99.609 (99.718)
Epoch: [239][192/196]	Time 0.091 (0.114)	Data 0.000 (0.003)	Loss 0.3209 (0.3265)	Acc@1 88.672 (88.613)	Acc@5 100.000 (99.711)
after train
test acc: 83.82
Epoche: [240/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [240][0/196]	Time 0.223 (0.223)	Data 0.449 (0.449)	Loss 0.2471 (0.2471)	Acc@1 92.578 (92.578)	Acc@5 99.609 (99.609)
Epoch: [240][64/196]	Time 0.149 (0.118)	Data 0.000 (0.007)	Loss 0.3526 (0.3294)	Acc@1 87.500 (88.215)	Acc@5 100.000 (99.681)
Epoch: [240][128/196]	Time 0.129 (0.118)	Data 0.000 (0.004)	Loss 0.2420 (0.3281)	Acc@1 90.625 (88.421)	Acc@5 100.000 (99.670)
Epoch: [240][192/196]	Time 0.111 (0.117)	Data 0.000 (0.003)	Loss 0.3128 (0.3281)	Acc@1 88.672 (88.467)	Acc@5 99.219 (99.674)
after train
test acc: 81.26
[INFO] Storing checkpoint...
Max memory: 36.3414528
Traceback (most recent call last):
  File "main.py", line 943, in <module>
    main()
  File "main.py", line 609, in main
    print(' {:5.3f}s'.format(ende - start), end='  ')
UnboundLocalError: local variable 'ende' referenced before assignment
