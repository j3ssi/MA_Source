Net2Net 1
j: 1 bis 5
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1515
Files already downloaded and verified
width: 8

Arch Num:  [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
Max memory: 0.0573952
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:1/5; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [1][0/196]	Time 0.119 (0.119)	Data 0.287 (0.287)	Loss 2.4165 (2.4165)	Acc@1 7.812 (7.812)	Acc@5 55.078 (55.078)
Epoch: [1][64/196]	Time 0.095 (0.086)	Data 0.000 (0.005)	Loss 1.6935 (1.9604)	Acc@1 38.281 (23.930)	Acc@5 87.109 (78.888)
Epoch: [1][128/196]	Time 0.086 (0.085)	Data 0.000 (0.002)	Loss 1.4255 (1.7937)	Acc@1 46.094 (31.577)	Acc@5 93.750 (84.160)
Epoch: [1][192/196]	Time 0.088 (0.086)	Data 0.000 (0.002)	Loss 1.3430 (1.6890)	Acc@1 50.391 (35.970)	Acc@5 95.312 (86.699)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:2/5; Lr: 0.1
batch Size 256
Epoch: [2][0/196]	Time 0.124 (0.124)	Data 0.304 (0.304)	Loss 1.3761 (1.3761)	Acc@1 51.953 (51.953)	Acc@5 92.578 (92.578)
Epoch: [2][64/196]	Time 0.091 (0.087)	Data 0.000 (0.005)	Loss 1.3076 (1.3338)	Acc@1 54.688 (50.859)	Acc@5 92.188 (93.888)
Epoch: [2][128/196]	Time 0.081 (0.086)	Data 0.000 (0.003)	Loss 1.1742 (1.2843)	Acc@1 58.594 (52.965)	Acc@5 95.703 (94.334)
Epoch: [2][192/196]	Time 0.077 (0.086)	Data 0.000 (0.002)	Loss 1.0703 (1.2437)	Acc@1 62.891 (54.574)	Acc@5 96.484 (94.715)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:3/5; Lr: 0.1
batch Size 256
Epoch: [3][0/196]	Time 0.127 (0.127)	Data 0.272 (0.272)	Loss 1.0590 (1.0590)	Acc@1 58.594 (58.594)	Acc@5 96.484 (96.484)
Epoch: [3][64/196]	Time 0.084 (0.087)	Data 0.000 (0.004)	Loss 1.0371 (1.0807)	Acc@1 64.062 (60.980)	Acc@5 96.094 (95.950)
Epoch: [3][128/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.9633 (1.0457)	Acc@1 67.188 (62.203)	Acc@5 97.266 (96.342)
Epoch: [3][192/196]	Time 0.086 (0.086)	Data 0.000 (0.002)	Loss 0.9889 (1.0203)	Acc@1 63.672 (63.172)	Acc@5 95.312 (96.517)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:4/5; Lr: 0.1
batch Size 256
Epoch: [4][0/196]	Time 0.133 (0.133)	Data 0.301 (0.301)	Loss 1.0072 (1.0072)	Acc@1 64.844 (64.844)	Acc@5 98.438 (98.438)
Epoch: [4][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.8918 (0.9313)	Acc@1 70.703 (66.755)	Acc@5 97.266 (97.248)
Epoch: [4][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.9176 (0.9195)	Acc@1 69.141 (67.309)	Acc@5 96.094 (97.332)
Epoch: [4][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.8122 (0.9000)	Acc@1 74.609 (68.131)	Acc@5 98.047 (97.421)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:5/5; Lr: 0.1
batch Size 256
Epoch: [5][0/196]	Time 0.114 (0.114)	Data 0.336 (0.336)	Loss 0.8008 (0.8008)	Acc@1 71.875 (71.875)	Acc@5 98.047 (98.047)
Epoch: [5][64/196]	Time 0.087 (0.086)	Data 0.000 (0.005)	Loss 0.8562 (0.8416)	Acc@1 73.828 (70.114)	Acc@5 98.438 (98.011)
Epoch: [5][128/196]	Time 0.080 (0.086)	Data 0.000 (0.003)	Loss 0.8997 (0.8282)	Acc@1 68.359 (70.730)	Acc@5 98.047 (97.977)
Epoch: [5][192/196]	Time 0.090 (0.086)	Data 0.000 (0.002)	Loss 0.6635 (0.8089)	Acc@1 79.297 (71.488)	Acc@5 98.828 (97.992)
Max memory in training epoch: 33.3018624
[INFO] Storing checkpoint...
  55.16
Max memory: 51.3858048
 17.275s  j: 6 bis 10
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5203
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 6
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:6/10; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [6][0/196]	Time 0.151 (0.151)	Data 0.305 (0.305)	Loss 0.7074 (0.7074)	Acc@1 75.000 (75.000)	Acc@5 98.438 (98.438)
Epoch: [6][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.5391 (0.7568)	Acc@1 80.469 (73.546)	Acc@5 100.000 (98.221)
Epoch: [6][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.7113 (0.7508)	Acc@1 74.219 (73.789)	Acc@5 99.219 (98.256)
Epoch: [6][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.7422 (0.7395)	Acc@1 74.219 (74.170)	Acc@5 98.438 (98.292)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:7/10; Lr: 0.1
batch Size 256
Epoch: [7][0/196]	Time 0.116 (0.116)	Data 0.269 (0.269)	Loss 0.8392 (0.8392)	Acc@1 70.703 (70.703)	Acc@5 98.047 (98.047)
Epoch: [7][64/196]	Time 0.085 (0.089)	Data 0.000 (0.004)	Loss 0.5791 (0.7276)	Acc@1 80.469 (74.465)	Acc@5 98.438 (98.480)
Epoch: [7][128/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.5793 (0.7158)	Acc@1 79.688 (75.079)	Acc@5 99.219 (98.353)
Epoch: [7][192/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.7369 (0.7103)	Acc@1 74.219 (75.269)	Acc@5 98.438 (98.446)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:8/10; Lr: 0.1
batch Size 256
Epoch: [8][0/196]	Time 0.130 (0.130)	Data 0.288 (0.288)	Loss 0.6657 (0.6657)	Acc@1 75.781 (75.781)	Acc@5 97.656 (97.656)
Epoch: [8][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.6431 (0.6763)	Acc@1 80.859 (76.683)	Acc@5 98.438 (98.462)
Epoch: [8][128/196]	Time 0.092 (0.089)	Data 0.000 (0.002)	Loss 0.6285 (0.6767)	Acc@1 76.953 (76.526)	Acc@5 98.828 (98.543)
Epoch: [8][192/196]	Time 0.081 (0.089)	Data 0.000 (0.002)	Loss 0.6208 (0.6729)	Acc@1 79.297 (76.728)	Acc@5 99.219 (98.561)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:9/10; Lr: 0.1
batch Size 256
Epoch: [9][0/196]	Time 0.132 (0.132)	Data 0.348 (0.348)	Loss 0.7014 (0.7014)	Acc@1 76.562 (76.562)	Acc@5 96.875 (96.875)
Epoch: [9][64/196]	Time 0.096 (0.090)	Data 0.000 (0.006)	Loss 0.6333 (0.6404)	Acc@1 77.344 (77.740)	Acc@5 98.828 (98.660)
Epoch: [9][128/196]	Time 0.089 (0.090)	Data 0.000 (0.003)	Loss 0.6259 (0.6486)	Acc@1 78.125 (77.404)	Acc@5 98.047 (98.692)
Epoch: [9][192/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.6336 (0.6420)	Acc@1 80.469 (77.740)	Acc@5 98.047 (98.727)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:10/10; Lr: 0.1
batch Size 256
Epoch: [10][0/196]	Time 0.129 (0.129)	Data 0.302 (0.302)	Loss 0.7062 (0.7062)	Acc@1 75.391 (75.391)	Acc@5 98.828 (98.828)
Epoch: [10][64/196]	Time 0.093 (0.091)	Data 0.000 (0.005)	Loss 0.5909 (0.6202)	Acc@1 80.469 (78.600)	Acc@5 98.438 (98.660)
Epoch: [10][128/196]	Time 0.086 (0.090)	Data 0.000 (0.003)	Loss 0.7795 (0.6315)	Acc@1 74.219 (78.210)	Acc@5 98.828 (98.674)
Epoch: [10][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.6512 (0.6284)	Acc@1 76.953 (78.321)	Acc@5 98.828 (98.751)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  64.03
Max memory: 51.4381312
 17.770s  j: 11 bis 15
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7503
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 11
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:11/15; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [11][0/196]	Time 0.175 (0.175)	Data 0.266 (0.266)	Loss 0.6679 (0.6679)	Acc@1 79.297 (79.297)	Acc@5 98.438 (98.438)
Epoch: [11][64/196]	Time 0.087 (0.089)	Data 0.000 (0.004)	Loss 0.6114 (0.5796)	Acc@1 78.516 (79.874)	Acc@5 99.219 (99.038)
Epoch: [11][128/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.5358 (0.5878)	Acc@1 80.078 (79.757)	Acc@5 100.000 (98.961)
Epoch: [11][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.6897 (0.5950)	Acc@1 80.469 (79.507)	Acc@5 96.875 (98.903)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:12/15; Lr: 0.1
batch Size 256
Epoch: [12][0/196]	Time 0.125 (0.125)	Data 0.315 (0.315)	Loss 0.5863 (0.5863)	Acc@1 80.469 (80.469)	Acc@5 99.609 (99.609)
Epoch: [12][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.5011 (0.5957)	Acc@1 82.422 (79.507)	Acc@5 98.828 (98.792)
Epoch: [12][128/196]	Time 0.095 (0.088)	Data 0.000 (0.003)	Loss 0.6408 (0.5991)	Acc@1 79.297 (79.457)	Acc@5 97.656 (98.786)
Epoch: [12][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.5857 (0.5933)	Acc@1 82.812 (79.574)	Acc@5 99.609 (98.824)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:13/15; Lr: 0.1
batch Size 256
Epoch: [13][0/196]	Time 0.128 (0.128)	Data 0.287 (0.287)	Loss 0.6350 (0.6350)	Acc@1 77.734 (77.734)	Acc@5 100.000 (100.000)
Epoch: [13][64/196]	Time 0.098 (0.089)	Data 0.000 (0.005)	Loss 0.5924 (0.5907)	Acc@1 77.344 (79.651)	Acc@5 98.438 (98.876)
Epoch: [13][128/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.6264 (0.5819)	Acc@1 78.125 (80.063)	Acc@5 98.828 (98.867)
Epoch: [13][192/196]	Time 0.093 (0.087)	Data 0.000 (0.002)	Loss 0.5562 (0.5795)	Acc@1 80.469 (80.021)	Acc@5 98.828 (98.921)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:14/15; Lr: 0.1
batch Size 256
Epoch: [14][0/196]	Time 0.129 (0.129)	Data 0.300 (0.300)	Loss 0.6043 (0.6043)	Acc@1 77.734 (77.734)	Acc@5 98.438 (98.438)
Epoch: [14][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.4648 (0.5789)	Acc@1 84.375 (79.826)	Acc@5 99.609 (98.822)
Epoch: [14][128/196]	Time 0.081 (0.087)	Data 0.000 (0.003)	Loss 0.4986 (0.5684)	Acc@1 83.984 (80.417)	Acc@5 99.609 (98.910)
Epoch: [14][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.5549 (0.5702)	Acc@1 78.125 (80.278)	Acc@5 98.828 (98.903)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:15/15; Lr: 0.1
batch Size 256
Epoch: [15][0/196]	Time 0.143 (0.143)	Data 0.306 (0.306)	Loss 0.5559 (0.5559)	Acc@1 82.812 (82.812)	Acc@5 97.656 (97.656)
Epoch: [15][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.6012 (0.5532)	Acc@1 76.953 (81.112)	Acc@5 99.219 (98.840)
Epoch: [15][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.5298 (0.5479)	Acc@1 81.641 (81.083)	Acc@5 98.438 (98.937)
Epoch: [15][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.5998 (0.5477)	Acc@1 80.469 (81.120)	Acc@5 99.609 (98.998)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  63.98
Max memory: 51.4381312
 17.385s  j: 16 bis 20
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2265
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 16
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:16/20; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [16][0/196]	Time 0.147 (0.147)	Data 0.308 (0.308)	Loss 0.5302 (0.5302)	Acc@1 82.422 (82.422)	Acc@5 98.438 (98.438)
Epoch: [16][64/196]	Time 0.078 (0.089)	Data 0.000 (0.005)	Loss 0.6966 (0.5211)	Acc@1 78.516 (82.079)	Acc@5 98.438 (99.111)
Epoch: [16][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.5202 (0.5290)	Acc@1 81.250 (81.677)	Acc@5 99.219 (99.116)
Epoch: [16][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.5865 (0.5342)	Acc@1 79.688 (81.574)	Acc@5 98.438 (99.085)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:17/20; Lr: 0.1
batch Size 256
Epoch: [17][0/196]	Time 0.122 (0.122)	Data 0.316 (0.316)	Loss 0.5111 (0.5111)	Acc@1 83.203 (83.203)	Acc@5 98.828 (98.828)
Epoch: [17][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.4742 (0.5435)	Acc@1 81.641 (81.382)	Acc@5 99.609 (98.990)
Epoch: [17][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.4781 (0.5425)	Acc@1 82.812 (81.377)	Acc@5 100.000 (99.019)
Epoch: [17][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.4585 (0.5464)	Acc@1 82.812 (81.205)	Acc@5 99.609 (98.984)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:18/20; Lr: 0.1
batch Size 256
Epoch: [18][0/196]	Time 0.122 (0.122)	Data 0.299 (0.299)	Loss 0.6178 (0.6178)	Acc@1 78.516 (78.516)	Acc@5 98.828 (98.828)
Epoch: [18][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 0.5303 (0.5384)	Acc@1 82.031 (81.118)	Acc@5 99.219 (99.020)
Epoch: [18][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.6922 (0.5314)	Acc@1 76.172 (81.471)	Acc@5 98.828 (99.031)
Epoch: [18][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.5376 (0.5295)	Acc@1 82.031 (81.649)	Acc@5 97.656 (99.053)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:19/20; Lr: 0.1
batch Size 256
Epoch: [19][0/196]	Time 0.127 (0.127)	Data 0.278 (0.278)	Loss 0.5038 (0.5038)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [19][64/196]	Time 0.087 (0.090)	Data 0.000 (0.004)	Loss 0.5276 (0.5149)	Acc@1 82.422 (82.218)	Acc@5 99.219 (99.141)
Epoch: [19][128/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.5650 (0.5188)	Acc@1 78.906 (82.134)	Acc@5 98.828 (99.152)
Epoch: [19][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.5344 (0.5218)	Acc@1 83.203 (81.900)	Acc@5 99.219 (99.148)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:20/20; Lr: 0.1
batch Size 256
Epoch: [20][0/196]	Time 0.139 (0.139)	Data 0.275 (0.275)	Loss 0.5712 (0.5712)	Acc@1 82.422 (82.422)	Acc@5 97.656 (97.656)
Epoch: [20][64/196]	Time 0.087 (0.088)	Data 0.000 (0.004)	Loss 0.5645 (0.4972)	Acc@1 78.906 (82.596)	Acc@5 99.219 (99.219)
Epoch: [20][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.4432 (0.5180)	Acc@1 84.375 (82.098)	Acc@5 99.219 (99.125)
Epoch: [20][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.4892 (0.5198)	Acc@1 79.688 (82.122)	Acc@5 99.219 (99.083)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  69.2
Max memory: 51.4381312
 17.540s  j: 21 bis 25
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2967
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 21
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:21/25; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [21][0/196]	Time 0.157 (0.157)	Data 0.271 (0.271)	Loss 0.5642 (0.5642)	Acc@1 80.859 (80.859)	Acc@5 98.828 (98.828)
Epoch: [21][64/196]	Time 0.087 (0.091)	Data 0.000 (0.004)	Loss 0.4850 (0.4934)	Acc@1 83.594 (83.041)	Acc@5 98.828 (99.177)
Epoch: [21][128/196]	Time 0.098 (0.088)	Data 0.000 (0.002)	Loss 0.4933 (0.5038)	Acc@1 83.203 (82.716)	Acc@5 99.219 (99.146)
Epoch: [21][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.5244 (0.5098)	Acc@1 80.859 (82.454)	Acc@5 99.609 (99.093)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:22/25; Lr: 0.1
batch Size 256
Epoch: [22][0/196]	Time 0.138 (0.138)	Data 0.283 (0.283)	Loss 0.5156 (0.5156)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [22][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.4793 (0.5070)	Acc@1 83.594 (82.416)	Acc@5 99.609 (99.213)
Epoch: [22][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.5090 (0.5137)	Acc@1 81.250 (82.452)	Acc@5 99.609 (99.198)
Epoch: [22][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.4831 (0.5103)	Acc@1 80.469 (82.551)	Acc@5 99.609 (99.162)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:23/25; Lr: 0.1
batch Size 256
Epoch: [23][0/196]	Time 0.146 (0.146)	Data 0.292 (0.292)	Loss 0.5488 (0.5488)	Acc@1 80.469 (80.469)	Acc@5 99.609 (99.609)
Epoch: [23][64/196]	Time 0.095 (0.089)	Data 0.000 (0.005)	Loss 0.3696 (0.4953)	Acc@1 85.547 (82.903)	Acc@5 99.609 (99.195)
Epoch: [23][128/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.5168 (0.5085)	Acc@1 82.812 (82.446)	Acc@5 99.219 (99.122)
Epoch: [23][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.5858 (0.5073)	Acc@1 78.906 (82.400)	Acc@5 98.438 (99.107)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:24/25; Lr: 0.1
batch Size 256
Epoch: [24][0/196]	Time 0.127 (0.127)	Data 0.276 (0.276)	Loss 0.4451 (0.4451)	Acc@1 85.547 (85.547)	Acc@5 100.000 (100.000)
Epoch: [24][64/196]	Time 0.091 (0.089)	Data 0.000 (0.004)	Loss 0.4762 (0.4827)	Acc@1 82.812 (83.311)	Acc@5 98.047 (99.195)
Epoch: [24][128/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.5303 (0.4974)	Acc@1 80.859 (82.716)	Acc@5 99.219 (99.161)
Epoch: [24][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.4952 (0.4941)	Acc@1 82.031 (82.958)	Acc@5 99.219 (99.162)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:25/25; Lr: 0.1
batch Size 256
Epoch: [25][0/196]	Time 0.123 (0.123)	Data 0.314 (0.314)	Loss 0.4494 (0.4494)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [25][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.4753 (0.4914)	Acc@1 86.719 (83.023)	Acc@5 98.047 (99.117)
Epoch: [25][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.4456 (0.4994)	Acc@1 82.422 (82.782)	Acc@5 99.609 (99.146)
Epoch: [25][192/196]	Time 0.096 (0.088)	Data 0.000 (0.002)	Loss 0.4486 (0.4965)	Acc@1 84.766 (82.879)	Acc@5 99.609 (99.178)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  75.51
Max memory: 51.4381312
 17.567s  j: 26 bis 30
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3927
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 26
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:26/30; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [26][0/196]	Time 0.147 (0.147)	Data 0.286 (0.286)	Loss 0.4910 (0.4910)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [26][64/196]	Time 0.080 (0.089)	Data 0.000 (0.005)	Loss 0.5122 (0.4721)	Acc@1 82.422 (83.660)	Acc@5 99.609 (99.327)
Epoch: [26][128/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.4419 (0.4797)	Acc@1 85.547 (83.570)	Acc@5 99.609 (99.294)
Epoch: [26][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.5199 (0.4846)	Acc@1 82.031 (83.387)	Acc@5 100.000 (99.263)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:27/30; Lr: 0.1
batch Size 256
Epoch: [27][0/196]	Time 0.122 (0.122)	Data 0.264 (0.264)	Loss 0.5049 (0.5049)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [27][64/196]	Time 0.085 (0.088)	Data 0.000 (0.004)	Loss 0.4405 (0.4889)	Acc@1 84.375 (83.197)	Acc@5 98.438 (99.147)
Epoch: [27][128/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.5595 (0.4890)	Acc@1 79.688 (83.261)	Acc@5 99.219 (99.191)
Epoch: [27][192/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.4705 (0.4865)	Acc@1 84.766 (83.385)	Acc@5 99.219 (99.213)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:28/30; Lr: 0.1
batch Size 256
Epoch: [28][0/196]	Time 0.113 (0.113)	Data 0.300 (0.300)	Loss 0.5073 (0.5073)	Acc@1 82.812 (82.812)	Acc@5 99.609 (99.609)
Epoch: [28][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.4467 (0.4789)	Acc@1 86.328 (83.534)	Acc@5 99.219 (99.303)
Epoch: [28][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.3681 (0.4830)	Acc@1 86.719 (83.358)	Acc@5 99.609 (99.237)
Epoch: [28][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.5162 (0.4816)	Acc@1 81.641 (83.482)	Acc@5 98.828 (99.209)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:29/30; Lr: 0.1
batch Size 256
Epoch: [29][0/196]	Time 0.116 (0.116)	Data 0.315 (0.315)	Loss 0.4079 (0.4079)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [29][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.5467 (0.4944)	Acc@1 81.641 (83.209)	Acc@5 98.828 (99.231)
Epoch: [29][128/196]	Time 0.078 (0.087)	Data 0.000 (0.003)	Loss 0.5008 (0.4814)	Acc@1 82.812 (83.594)	Acc@5 99.609 (99.195)
Epoch: [29][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.5348 (0.4804)	Acc@1 81.250 (83.519)	Acc@5 99.219 (99.207)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:30/30; Lr: 0.1
batch Size 256
Epoch: [30][0/196]	Time 0.134 (0.134)	Data 0.273 (0.273)	Loss 0.5308 (0.5308)	Acc@1 82.422 (82.422)	Acc@5 98.438 (98.438)
Epoch: [30][64/196]	Time 0.108 (0.090)	Data 0.000 (0.004)	Loss 0.4314 (0.4871)	Acc@1 85.547 (83.137)	Acc@5 99.219 (99.189)
Epoch: [30][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.3770 (0.4797)	Acc@1 86.328 (83.397)	Acc@5 100.000 (99.258)
Epoch: [30][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.4211 (0.4777)	Acc@1 86.328 (83.454)	Acc@5 100.000 (99.251)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  77.88
Max memory: 51.4381312
 17.511s  j: 31 bis 35
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1781
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 31
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:31/35; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [31][0/196]	Time 0.148 (0.148)	Data 0.341 (0.341)	Loss 0.5136 (0.5136)	Acc@1 83.203 (83.203)	Acc@5 98.047 (98.047)
Epoch: [31][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.5033 (0.4657)	Acc@1 81.641 (84.038)	Acc@5 98.828 (99.201)
Epoch: [31][128/196]	Time 0.083 (0.089)	Data 0.000 (0.003)	Loss 0.4465 (0.4701)	Acc@1 81.250 (83.788)	Acc@5 99.609 (99.188)
Epoch: [31][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.4732 (0.4722)	Acc@1 82.422 (83.739)	Acc@5 98.828 (99.180)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:32/35; Lr: 0.1
batch Size 256
Epoch: [32][0/196]	Time 0.117 (0.117)	Data 0.320 (0.320)	Loss 0.4406 (0.4406)	Acc@1 81.250 (81.250)	Acc@5 99.609 (99.609)
Epoch: [32][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.4428 (0.4568)	Acc@1 84.766 (84.219)	Acc@5 99.609 (99.231)
Epoch: [32][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.4141 (0.4652)	Acc@1 85.156 (83.930)	Acc@5 100.000 (99.252)
Epoch: [32][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.5695 (0.4688)	Acc@1 80.859 (83.845)	Acc@5 99.219 (99.273)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:33/35; Lr: 0.1
batch Size 256
Epoch: [33][0/196]	Time 0.136 (0.136)	Data 0.304 (0.304)	Loss 0.4898 (0.4898)	Acc@1 83.984 (83.984)	Acc@5 98.047 (98.047)
Epoch: [33][64/196]	Time 0.080 (0.090)	Data 0.000 (0.005)	Loss 0.4342 (0.4659)	Acc@1 86.719 (83.816)	Acc@5 99.219 (99.225)
Epoch: [33][128/196]	Time 0.097 (0.089)	Data 0.000 (0.003)	Loss 0.4445 (0.4680)	Acc@1 83.594 (83.785)	Acc@5 99.609 (99.191)
Epoch: [33][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.4655 (0.4688)	Acc@1 83.984 (83.800)	Acc@5 100.000 (99.251)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:34/35; Lr: 0.1
batch Size 256
Epoch: [34][0/196]	Time 0.109 (0.109)	Data 0.335 (0.335)	Loss 0.4968 (0.4968)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [34][64/196]	Time 0.083 (0.086)	Data 0.000 (0.005)	Loss 0.5081 (0.4755)	Acc@1 84.766 (83.456)	Acc@5 98.828 (99.237)
Epoch: [34][128/196]	Time 0.095 (0.087)	Data 0.000 (0.003)	Loss 0.5223 (0.4784)	Acc@1 83.984 (83.352)	Acc@5 98.828 (99.225)
Epoch: [34][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.4822 (0.4753)	Acc@1 84.766 (83.574)	Acc@5 99.609 (99.217)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:35/35; Lr: 0.1
batch Size 256
Epoch: [35][0/196]	Time 0.147 (0.147)	Data 0.353 (0.353)	Loss 0.4370 (0.4370)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [35][64/196]	Time 0.094 (0.091)	Data 0.000 (0.006)	Loss 0.5160 (0.4679)	Acc@1 82.812 (84.014)	Acc@5 98.438 (99.213)
Epoch: [35][128/196]	Time 0.089 (0.089)	Data 0.000 (0.003)	Loss 0.4738 (0.4669)	Acc@1 84.375 (83.993)	Acc@5 98.828 (99.234)
Epoch: [35][192/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.4850 (0.4697)	Acc@1 82.812 (83.924)	Acc@5 99.609 (99.233)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  74.93
Max memory: 51.4381312
 17.756s  j: 36 bis 40
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5801
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 36
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:36/40; Lr: 0.1
batch Size 256
Epoch: [36][0/196]	Time 0.142 (0.142)	Data 0.349 (0.349)	Loss 0.4198 (0.4198)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [36][64/196]	Time 0.088 (0.088)	Data 0.000 (0.006)	Loss 0.3873 (0.4244)	Acc@1 85.938 (85.246)	Acc@5 100.000 (99.369)
Epoch: [36][128/196]	Time 0.099 (0.088)	Data 0.000 (0.003)	Loss 0.4053 (0.4433)	Acc@1 86.328 (84.678)	Acc@5 100.000 (99.294)
Epoch: [36][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.5738 (0.4462)	Acc@1 76.562 (84.551)	Acc@5 98.828 (99.318)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:37/40; Lr: 0.1
batch Size 256
Epoch: [37][0/196]	Time 0.130 (0.130)	Data 0.292 (0.292)	Loss 0.4067 (0.4067)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [37][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.4599 (0.4690)	Acc@1 83.984 (83.906)	Acc@5 98.828 (99.195)
Epoch: [37][128/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.3742 (0.4638)	Acc@1 87.891 (84.096)	Acc@5 100.000 (99.285)
Epoch: [37][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.5165 (0.4601)	Acc@1 79.688 (84.193)	Acc@5 99.609 (99.267)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:38/40; Lr: 0.1
batch Size 256
Epoch: [38][0/196]	Time 0.121 (0.121)	Data 0.309 (0.309)	Loss 0.3363 (0.3363)	Acc@1 89.453 (89.453)	Acc@5 100.000 (100.000)
Epoch: [38][64/196]	Time 0.091 (0.090)	Data 0.000 (0.005)	Loss 0.4274 (0.4482)	Acc@1 86.719 (84.603)	Acc@5 99.609 (99.351)
Epoch: [38][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.4273 (0.4544)	Acc@1 83.984 (84.375)	Acc@5 99.219 (99.337)
Epoch: [38][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.5354 (0.4622)	Acc@1 83.203 (84.146)	Acc@5 98.438 (99.275)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:39/40; Lr: 0.1
batch Size 256
Epoch: [39][0/196]	Time 0.141 (0.141)	Data 0.326 (0.326)	Loss 0.4654 (0.4654)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [39][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.4180 (0.4567)	Acc@1 85.156 (84.357)	Acc@5 99.609 (99.327)
Epoch: [39][128/196]	Time 0.095 (0.088)	Data 0.000 (0.003)	Loss 0.4685 (0.4599)	Acc@1 82.031 (84.345)	Acc@5 100.000 (99.252)
Epoch: [39][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.4889 (0.4585)	Acc@1 80.859 (84.260)	Acc@5 99.609 (99.263)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:40/40; Lr: 0.1
batch Size 256
Epoch: [40][0/196]	Time 0.128 (0.128)	Data 0.334 (0.334)	Loss 0.3668 (0.3668)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [40][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.4806 (0.4533)	Acc@1 83.203 (84.267)	Acc@5 98.828 (99.261)
Epoch: [40][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.5000 (0.4481)	Acc@1 81.641 (84.472)	Acc@5 98.047 (99.288)
Epoch: [40][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.3751 (0.4517)	Acc@1 88.672 (84.407)	Acc@5 99.609 (99.290)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  81.07
Max memory: 51.4381312
 17.580s  j: 41 bis 45
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7309
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 41
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:41/45; Lr: 0.1
batch Size 256
Epoch: [41][0/196]	Time 0.134 (0.134)	Data 0.300 (0.300)	Loss 0.3807 (0.3807)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [41][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.5103 (0.4330)	Acc@1 82.031 (85.138)	Acc@5 99.609 (99.393)
Epoch: [41][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.4918 (0.4420)	Acc@1 82.422 (84.841)	Acc@5 98.828 (99.322)
Epoch: [41][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.5091 (0.4422)	Acc@1 81.250 (84.711)	Acc@5 99.609 (99.342)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:42/45; Lr: 0.1
batch Size 256
Epoch: [42][0/196]	Time 0.132 (0.132)	Data 0.300 (0.300)	Loss 0.4636 (0.4636)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [42][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.4425 (0.4472)	Acc@1 83.594 (84.784)	Acc@5 98.828 (99.339)
Epoch: [42][128/196]	Time 0.090 (0.089)	Data 0.000 (0.003)	Loss 0.4564 (0.4455)	Acc@1 84.375 (84.754)	Acc@5 99.609 (99.288)
Epoch: [42][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.5377 (0.4438)	Acc@1 82.422 (84.794)	Acc@5 98.438 (99.318)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:43/45; Lr: 0.1
batch Size 256
Epoch: [43][0/196]	Time 0.131 (0.131)	Data 0.311 (0.311)	Loss 0.4178 (0.4178)	Acc@1 85.938 (85.938)	Acc@5 98.828 (98.828)
Epoch: [43][64/196]	Time 0.089 (0.090)	Data 0.000 (0.005)	Loss 0.4517 (0.4423)	Acc@1 84.766 (84.904)	Acc@5 99.219 (99.303)
Epoch: [43][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.4978 (0.4439)	Acc@1 82.422 (84.735)	Acc@5 99.219 (99.367)
Epoch: [43][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.4594 (0.4515)	Acc@1 84.766 (84.399)	Acc@5 99.219 (99.326)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:44/45; Lr: 0.1
batch Size 256
Epoch: [44][0/196]	Time 0.111 (0.111)	Data 0.304 (0.304)	Loss 0.3797 (0.3797)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [44][64/196]	Time 0.099 (0.090)	Data 0.000 (0.005)	Loss 0.4082 (0.4353)	Acc@1 86.328 (85.090)	Acc@5 99.219 (99.339)
Epoch: [44][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.4568 (0.4503)	Acc@1 83.594 (84.499)	Acc@5 99.609 (99.337)
Epoch: [44][192/196]	Time 0.092 (0.089)	Data 0.000 (0.002)	Loss 0.4236 (0.4505)	Acc@1 85.547 (84.480)	Acc@5 99.219 (99.330)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:45/45; Lr: 0.1
batch Size 256
Epoch: [45][0/196]	Time 0.128 (0.128)	Data 0.287 (0.287)	Loss 0.4610 (0.4610)	Acc@1 84.375 (84.375)	Acc@5 98.828 (98.828)
Epoch: [45][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.4208 (0.4560)	Acc@1 85.938 (84.321)	Acc@5 99.609 (99.291)
Epoch: [45][128/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.4000 (0.4460)	Acc@1 86.328 (84.629)	Acc@5 98.438 (99.334)
Epoch: [45][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.4882 (0.4513)	Acc@1 83.984 (84.551)	Acc@5 99.219 (99.308)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  75.43
Max memory: 51.4381312
 17.843s  j: 46 bis 50
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3168
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 46
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:46/50; Lr: 0.1
batch Size 256
Epoch: [46][0/196]	Time 0.136 (0.136)	Data 0.303 (0.303)	Loss 0.4281 (0.4281)	Acc@1 85.938 (85.938)	Acc@5 98.828 (98.828)
Epoch: [46][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.4422 (0.4142)	Acc@1 83.594 (85.859)	Acc@5 99.219 (99.471)
Epoch: [46][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.5296 (0.4284)	Acc@1 79.688 (85.305)	Acc@5 99.219 (99.400)
Epoch: [46][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.3386 (0.4364)	Acc@1 87.891 (84.992)	Acc@5 100.000 (99.348)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:47/50; Lr: 0.1
batch Size 256
Epoch: [47][0/196]	Time 0.125 (0.125)	Data 0.312 (0.312)	Loss 0.3782 (0.3782)	Acc@1 87.500 (87.500)	Acc@5 98.828 (98.828)
Epoch: [47][64/196]	Time 0.080 (0.088)	Data 0.000 (0.005)	Loss 0.4291 (0.4300)	Acc@1 85.156 (85.084)	Acc@5 99.219 (99.297)
Epoch: [47][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.3977 (0.4340)	Acc@1 85.938 (84.850)	Acc@5 99.219 (99.367)
Epoch: [47][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.5043 (0.4421)	Acc@1 82.422 (84.658)	Acc@5 98.047 (99.324)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:48/50; Lr: 0.1
batch Size 256
Epoch: [48][0/196]	Time 0.115 (0.115)	Data 0.300 (0.300)	Loss 0.4466 (0.4466)	Acc@1 83.203 (83.203)	Acc@5 98.828 (98.828)
Epoch: [48][64/196]	Time 0.087 (0.085)	Data 0.000 (0.005)	Loss 0.5103 (0.4400)	Acc@1 80.469 (84.597)	Acc@5 98.828 (99.345)
Epoch: [48][128/196]	Time 0.084 (0.085)	Data 0.000 (0.003)	Loss 0.3875 (0.4447)	Acc@1 85.938 (84.590)	Acc@5 100.000 (99.316)
Epoch: [48][192/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.4624 (0.4458)	Acc@1 82.812 (84.553)	Acc@5 100.000 (99.302)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:49/50; Lr: 0.1
batch Size 256
Epoch: [49][0/196]	Time 0.133 (0.133)	Data 0.282 (0.282)	Loss 0.5550 (0.5550)	Acc@1 83.203 (83.203)	Acc@5 98.047 (98.047)
Epoch: [49][64/196]	Time 0.081 (0.087)	Data 0.000 (0.005)	Loss 0.4574 (0.4356)	Acc@1 83.594 (84.760)	Acc@5 99.609 (99.381)
Epoch: [49][128/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.5076 (0.4367)	Acc@1 80.859 (84.850)	Acc@5 99.219 (99.349)
Epoch: [49][192/196]	Time 0.086 (0.086)	Data 0.000 (0.002)	Loss 0.4523 (0.4402)	Acc@1 83.984 (84.764)	Acc@5 100.000 (99.324)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:50/50; Lr: 0.1
batch Size 256
Epoch: [50][0/196]	Time 0.135 (0.135)	Data 0.301 (0.301)	Loss 0.3986 (0.3986)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [50][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.3935 (0.4328)	Acc@1 85.547 (85.072)	Acc@5 99.219 (99.291)
Epoch: [50][128/196]	Time 0.098 (0.087)	Data 0.000 (0.003)	Loss 0.4974 (0.4351)	Acc@1 82.422 (85.068)	Acc@5 99.609 (99.364)
Epoch: [50][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.4721 (0.4360)	Acc@1 82.812 (85.112)	Acc@5 99.609 (99.310)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  74.8
Max memory: 51.4381312
 17.436s  j: 51 bis 55
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5586
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 51
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:51/55; Lr: 0.1
batch Size 256
Epoch: [51][0/196]	Time 0.168 (0.168)	Data 0.298 (0.298)	Loss 0.4424 (0.4424)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [51][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.4226 (0.4330)	Acc@1 85.938 (85.012)	Acc@5 99.609 (99.375)
Epoch: [51][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.3648 (0.4327)	Acc@1 88.281 (85.068)	Acc@5 100.000 (99.337)
Epoch: [51][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.5111 (0.4326)	Acc@1 85.156 (84.966)	Acc@5 99.609 (99.373)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:52/55; Lr: 0.1
batch Size 256
Epoch: [52][0/196]	Time 0.130 (0.130)	Data 0.281 (0.281)	Loss 0.3968 (0.3968)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [52][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.4108 (0.4446)	Acc@1 87.891 (84.838)	Acc@5 99.609 (99.381)
Epoch: [52][128/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.4624 (0.4495)	Acc@1 85.156 (84.493)	Acc@5 98.828 (99.352)
Epoch: [52][192/196]	Time 0.079 (0.086)	Data 0.000 (0.002)	Loss 0.4705 (0.4443)	Acc@1 83.984 (84.751)	Acc@5 99.219 (99.350)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:53/55; Lr: 0.1
batch Size 256
Epoch: [53][0/196]	Time 0.128 (0.128)	Data 0.321 (0.321)	Loss 0.4481 (0.4481)	Acc@1 82.422 (82.422)	Acc@5 100.000 (100.000)
Epoch: [53][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.5052 (0.4486)	Acc@1 80.469 (84.351)	Acc@5 98.828 (99.291)
Epoch: [53][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.5084 (0.4462)	Acc@1 82.422 (84.517)	Acc@5 99.219 (99.337)
Epoch: [53][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.5070 (0.4444)	Acc@1 83.594 (84.628)	Acc@5 99.219 (99.292)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:54/55; Lr: 0.1
batch Size 256
Epoch: [54][0/196]	Time 0.132 (0.132)	Data 0.309 (0.309)	Loss 0.4499 (0.4499)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [54][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.3908 (0.4126)	Acc@1 85.547 (85.733)	Acc@5 98.828 (99.489)
Epoch: [54][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.4359 (0.4285)	Acc@1 82.812 (85.244)	Acc@5 100.000 (99.428)
Epoch: [54][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.5077 (0.4335)	Acc@1 83.594 (85.098)	Acc@5 98.438 (99.369)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:55/55; Lr: 0.1
batch Size 256
Epoch: [55][0/196]	Time 0.135 (0.135)	Data 0.303 (0.303)	Loss 0.3664 (0.3664)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [55][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.4194 (0.4135)	Acc@1 86.328 (85.577)	Acc@5 99.219 (99.441)
Epoch: [55][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.6070 (0.4207)	Acc@1 79.297 (85.392)	Acc@5 99.219 (99.410)
Epoch: [55][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.5303 (0.4273)	Acc@1 83.203 (85.237)	Acc@5 98.828 (99.413)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  76.86
Max memory: 51.4381312
 17.449s  j: 56 bis 60
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4164
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 56
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:56/60; Lr: 0.1
batch Size 256
Epoch: [56][0/196]	Time 0.159 (0.159)	Data 0.275 (0.275)	Loss 0.4496 (0.4496)	Acc@1 83.594 (83.594)	Acc@5 99.609 (99.609)
Epoch: [56][64/196]	Time 0.085 (0.090)	Data 0.000 (0.004)	Loss 0.5079 (0.3989)	Acc@1 83.203 (86.298)	Acc@5 98.828 (99.507)
Epoch: [56][128/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.4912 (0.4223)	Acc@1 80.469 (85.492)	Acc@5 99.219 (99.416)
Epoch: [56][192/196]	Time 0.081 (0.089)	Data 0.000 (0.002)	Loss 0.4462 (0.4243)	Acc@1 85.156 (85.419)	Acc@5 99.609 (99.387)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:57/60; Lr: 0.1
batch Size 256
Epoch: [57][0/196]	Time 0.126 (0.126)	Data 0.279 (0.279)	Loss 0.4444 (0.4444)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [57][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 0.4689 (0.4282)	Acc@1 82.422 (85.120)	Acc@5 98.828 (99.381)
Epoch: [57][128/196]	Time 0.084 (0.086)	Data 0.000 (0.002)	Loss 0.3733 (0.4303)	Acc@1 84.766 (85.171)	Acc@5 99.609 (99.358)
Epoch: [57][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.4137 (0.4354)	Acc@1 84.766 (85.067)	Acc@5 100.000 (99.346)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:58/60; Lr: 0.1
batch Size 256
Epoch: [58][0/196]	Time 0.132 (0.132)	Data 0.318 (0.318)	Loss 0.5318 (0.5318)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [58][64/196]	Time 0.079 (0.088)	Data 0.000 (0.005)	Loss 0.4014 (0.4350)	Acc@1 86.328 (84.994)	Acc@5 98.828 (99.291)
Epoch: [58][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.4206 (0.4366)	Acc@1 85.938 (84.926)	Acc@5 99.609 (99.291)
Epoch: [58][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.3963 (0.4365)	Acc@1 86.328 (84.873)	Acc@5 99.609 (99.281)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:59/60; Lr: 0.1
batch Size 256
Epoch: [59][0/196]	Time 0.126 (0.126)	Data 0.299 (0.299)	Loss 0.3180 (0.3180)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [59][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.4470 (0.4318)	Acc@1 83.984 (85.168)	Acc@5 100.000 (99.363)
Epoch: [59][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.5875 (0.4294)	Acc@1 82.031 (85.353)	Acc@5 98.438 (99.310)
Epoch: [59][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.3635 (0.4322)	Acc@1 84.375 (85.164)	Acc@5 99.609 (99.326)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:60/60; Lr: 0.1
batch Size 256
Epoch: [60][0/196]	Time 0.137 (0.137)	Data 0.287 (0.287)	Loss 0.3644 (0.3644)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [60][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.3852 (0.4195)	Acc@1 88.672 (85.511)	Acc@5 99.219 (99.519)
Epoch: [60][128/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.3915 (0.4237)	Acc@1 85.547 (85.320)	Acc@5 99.219 (99.416)
Epoch: [60][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.3135 (0.4332)	Acc@1 89.453 (85.037)	Acc@5 99.609 (99.338)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  74.01
Max memory: 51.4381312
 17.560s  j: 61 bis 65
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5121
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 61
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:61/65; Lr: 0.1
batch Size 256
Epoch: [61][0/196]	Time 0.160 (0.160)	Data 0.307 (0.307)	Loss 0.5331 (0.5331)	Acc@1 80.078 (80.078)	Acc@5 100.000 (100.000)
Epoch: [61][64/196]	Time 0.080 (0.090)	Data 0.000 (0.005)	Loss 0.4145 (0.4096)	Acc@1 84.766 (85.727)	Acc@5 99.219 (99.459)
Epoch: [61][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.4470 (0.4185)	Acc@1 85.547 (85.504)	Acc@5 98.438 (99.413)
Epoch: [61][192/196]	Time 0.093 (0.089)	Data 0.000 (0.002)	Loss 0.4025 (0.4228)	Acc@1 86.719 (85.397)	Acc@5 99.609 (99.383)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:62/65; Lr: 0.1
batch Size 256
Epoch: [62][0/196]	Time 0.128 (0.128)	Data 0.296 (0.296)	Loss 0.4288 (0.4288)	Acc@1 85.938 (85.938)	Acc@5 98.828 (98.828)
Epoch: [62][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.4852 (0.4191)	Acc@1 80.859 (85.715)	Acc@5 99.219 (99.435)
Epoch: [62][128/196]	Time 0.085 (0.089)	Data 0.000 (0.003)	Loss 0.3209 (0.4201)	Acc@1 90.625 (85.653)	Acc@5 99.219 (99.446)
Epoch: [62][192/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.3980 (0.4283)	Acc@1 86.328 (85.363)	Acc@5 100.000 (99.425)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:63/65; Lr: 0.1
batch Size 256
Epoch: [63][0/196]	Time 0.124 (0.124)	Data 0.290 (0.290)	Loss 0.3888 (0.3888)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [63][64/196]	Time 0.095 (0.090)	Data 0.000 (0.005)	Loss 0.4308 (0.4190)	Acc@1 84.766 (85.637)	Acc@5 99.219 (99.387)
Epoch: [63][128/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.4931 (0.4225)	Acc@1 82.422 (85.571)	Acc@5 99.609 (99.397)
Epoch: [63][192/196]	Time 0.093 (0.089)	Data 0.000 (0.002)	Loss 0.4165 (0.4226)	Acc@1 83.594 (85.464)	Acc@5 100.000 (99.389)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:64/65; Lr: 0.1
batch Size 256
Epoch: [64][0/196]	Time 0.139 (0.139)	Data 0.290 (0.290)	Loss 0.4484 (0.4484)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [64][64/196]	Time 0.093 (0.090)	Data 0.000 (0.005)	Loss 0.4127 (0.4220)	Acc@1 85.156 (85.487)	Acc@5 100.000 (99.453)
Epoch: [64][128/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.4097 (0.4184)	Acc@1 87.891 (85.577)	Acc@5 99.219 (99.437)
Epoch: [64][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.3397 (0.4278)	Acc@1 87.891 (85.351)	Acc@5 99.609 (99.377)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:65/65; Lr: 0.1
batch Size 256
Epoch: [65][0/196]	Time 0.125 (0.125)	Data 0.317 (0.317)	Loss 0.4532 (0.4532)	Acc@1 87.500 (87.500)	Acc@5 98.438 (98.438)
Epoch: [65][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.4967 (0.4368)	Acc@1 83.203 (85.060)	Acc@5 99.219 (99.405)
Epoch: [65][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.4300 (0.4270)	Acc@1 86.719 (85.329)	Acc@5 99.219 (99.410)
Epoch: [65][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.3734 (0.4245)	Acc@1 86.328 (85.385)	Acc@5 100.000 (99.352)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  79.45
Max memory: 51.4381312
 17.698s  j: 66 bis 70
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4960
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 66
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:66/70; Lr: 0.1
batch Size 256
Epoch: [66][0/196]	Time 0.171 (0.171)	Data 0.263 (0.263)	Loss 0.4037 (0.4037)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [66][64/196]	Time 0.086 (0.088)	Data 0.000 (0.004)	Loss 0.4162 (0.3954)	Acc@1 85.547 (86.454)	Acc@5 98.438 (99.459)
Epoch: [66][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.4640 (0.4075)	Acc@1 86.328 (86.149)	Acc@5 99.609 (99.440)
Epoch: [66][192/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.4200 (0.4157)	Acc@1 86.719 (85.719)	Acc@5 99.609 (99.405)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:67/70; Lr: 0.1
batch Size 256
Epoch: [67][0/196]	Time 0.106 (0.106)	Data 0.297 (0.297)	Loss 0.4771 (0.4771)	Acc@1 82.422 (82.422)	Acc@5 99.609 (99.609)
Epoch: [67][64/196]	Time 0.082 (0.087)	Data 0.000 (0.005)	Loss 0.3757 (0.4307)	Acc@1 86.719 (85.294)	Acc@5 98.438 (99.369)
Epoch: [67][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.4535 (0.4262)	Acc@1 84.375 (85.523)	Acc@5 99.609 (99.391)
Epoch: [67][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.4543 (0.4249)	Acc@1 85.156 (85.488)	Acc@5 99.609 (99.381)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:68/70; Lr: 0.1
batch Size 256
Epoch: [68][0/196]	Time 0.113 (0.113)	Data 0.304 (0.304)	Loss 0.3489 (0.3489)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [68][64/196]	Time 0.095 (0.088)	Data 0.000 (0.005)	Loss 0.4801 (0.4160)	Acc@1 85.938 (85.895)	Acc@5 99.609 (99.453)
Epoch: [68][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.4738 (0.4161)	Acc@1 83.203 (85.738)	Acc@5 99.609 (99.449)
Epoch: [68][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.4767 (0.4179)	Acc@1 81.250 (85.652)	Acc@5 99.219 (99.439)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:69/70; Lr: 0.1
batch Size 256
Epoch: [69][0/196]	Time 0.133 (0.133)	Data 0.282 (0.282)	Loss 0.4105 (0.4105)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [69][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.4782 (0.4360)	Acc@1 83.594 (84.898)	Acc@5 99.609 (99.369)
Epoch: [69][128/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.4680 (0.4261)	Acc@1 82.422 (85.332)	Acc@5 99.609 (99.416)
Epoch: [69][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.3834 (0.4270)	Acc@1 83.984 (85.326)	Acc@5 100.000 (99.395)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:70/70; Lr: 0.1
batch Size 256
Epoch: [70][0/196]	Time 0.118 (0.118)	Data 0.270 (0.270)	Loss 0.4165 (0.4165)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [70][64/196]	Time 0.083 (0.088)	Data 0.000 (0.004)	Loss 0.3810 (0.4109)	Acc@1 87.109 (85.877)	Acc@5 99.609 (99.387)
Epoch: [70][128/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.4077 (0.4162)	Acc@1 86.328 (85.650)	Acc@5 99.219 (99.376)
Epoch: [70][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.4309 (0.4163)	Acc@1 83.984 (85.654)	Acc@5 100.000 (99.401)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  78.48
Max memory: 51.4381312
 17.301s  j: 71 bis 75
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2948
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 71
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:71/75; Lr: 0.1
batch Size 256
Epoch: [71][0/196]	Time 0.151 (0.151)	Data 0.281 (0.281)	Loss 0.4136 (0.4136)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [71][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.3813 (0.3927)	Acc@1 89.453 (86.538)	Acc@5 100.000 (99.477)
Epoch: [71][128/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.3755 (0.4113)	Acc@1 86.719 (85.968)	Acc@5 99.219 (99.428)
Epoch: [71][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.4590 (0.4139)	Acc@1 83.984 (85.897)	Acc@5 99.609 (99.425)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:72/75; Lr: 0.1
batch Size 256
Epoch: [72][0/196]	Time 0.131 (0.131)	Data 0.309 (0.309)	Loss 0.2782 (0.2782)	Acc@1 87.891 (87.891)	Acc@5 100.000 (100.000)
Epoch: [72][64/196]	Time 0.077 (0.089)	Data 0.000 (0.005)	Loss 0.4977 (0.4114)	Acc@1 82.422 (86.112)	Acc@5 98.828 (99.351)
Epoch: [72][128/196]	Time 0.101 (0.088)	Data 0.000 (0.003)	Loss 0.4328 (0.4176)	Acc@1 83.984 (85.692)	Acc@5 99.609 (99.388)
Epoch: [72][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.5375 (0.4214)	Acc@1 80.469 (85.591)	Acc@5 98.828 (99.371)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:73/75; Lr: 0.1
batch Size 256
Epoch: [73][0/196]	Time 0.132 (0.132)	Data 0.287 (0.287)	Loss 0.3186 (0.3186)	Acc@1 89.844 (89.844)	Acc@5 99.219 (99.219)
Epoch: [73][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.4069 (0.4096)	Acc@1 84.375 (85.799)	Acc@5 99.609 (99.357)
Epoch: [73][128/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.2992 (0.4085)	Acc@1 91.406 (85.959)	Acc@5 99.219 (99.349)
Epoch: [73][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.3634 (0.4187)	Acc@1 87.500 (85.571)	Acc@5 100.000 (99.356)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:74/75; Lr: 0.1
batch Size 256
Epoch: [74][0/196]	Time 0.108 (0.108)	Data 0.312 (0.312)	Loss 0.4475 (0.4475)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [74][64/196]	Time 0.085 (0.086)	Data 0.000 (0.005)	Loss 0.3316 (0.4097)	Acc@1 88.672 (85.859)	Acc@5 99.219 (99.375)
Epoch: [74][128/196]	Time 0.092 (0.087)	Data 0.000 (0.003)	Loss 0.3826 (0.4120)	Acc@1 89.062 (85.841)	Acc@5 99.219 (99.419)
Epoch: [74][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.4197 (0.4196)	Acc@1 87.500 (85.557)	Acc@5 100.000 (99.415)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:75/75; Lr: 0.1
batch Size 256
Epoch: [75][0/196]	Time 0.136 (0.136)	Data 0.353 (0.353)	Loss 0.3681 (0.3681)	Acc@1 88.672 (88.672)	Acc@5 99.609 (99.609)
Epoch: [75][64/196]	Time 0.084 (0.090)	Data 0.000 (0.006)	Loss 0.4508 (0.3987)	Acc@1 81.641 (85.944)	Acc@5 100.000 (99.543)
Epoch: [75][128/196]	Time 0.090 (0.089)	Data 0.000 (0.003)	Loss 0.4027 (0.4086)	Acc@1 87.891 (85.707)	Acc@5 98.438 (99.479)
Epoch: [75][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.5235 (0.4180)	Acc@1 80.859 (85.460)	Acc@5 98.828 (99.443)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  78.39
Max memory: 51.4381312
 17.618s  j: 76 bis 80
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3124
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 76
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:76/80; Lr: 0.1
batch Size 256
Epoch: [76][0/196]	Time 0.156 (0.156)	Data 0.303 (0.303)	Loss 0.3687 (0.3687)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [76][64/196]	Time 0.087 (0.087)	Data 0.000 (0.005)	Loss 0.4270 (0.3842)	Acc@1 86.328 (86.797)	Acc@5 99.219 (99.429)
Epoch: [76][128/196]	Time 0.078 (0.087)	Data 0.000 (0.003)	Loss 0.4018 (0.4082)	Acc@1 85.547 (85.983)	Acc@5 99.219 (99.403)
Epoch: [76][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.4092 (0.4110)	Acc@1 85.156 (85.958)	Acc@5 100.000 (99.385)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:77/80; Lr: 0.1
batch Size 256
Epoch: [77][0/196]	Time 0.137 (0.137)	Data 0.293 (0.293)	Loss 0.4553 (0.4553)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [77][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.3884 (0.4133)	Acc@1 86.328 (85.493)	Acc@5 99.609 (99.393)
Epoch: [77][128/196]	Time 0.108 (0.088)	Data 0.000 (0.002)	Loss 0.3616 (0.4205)	Acc@1 87.109 (85.332)	Acc@5 100.000 (99.425)
Epoch: [77][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.4144 (0.4172)	Acc@1 85.547 (85.549)	Acc@5 100.000 (99.403)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:78/80; Lr: 0.1
batch Size 256
Epoch: [78][0/196]	Time 0.126 (0.126)	Data 0.308 (0.308)	Loss 0.4366 (0.4366)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [78][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.3591 (0.4007)	Acc@1 89.453 (86.184)	Acc@5 98.438 (99.501)
Epoch: [78][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.4509 (0.4067)	Acc@1 86.719 (85.934)	Acc@5 99.219 (99.494)
Epoch: [78][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.4782 (0.4115)	Acc@1 83.203 (85.788)	Acc@5 98.828 (99.468)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:79/80; Lr: 0.1
batch Size 256
Epoch: [79][0/196]	Time 0.128 (0.128)	Data 0.295 (0.295)	Loss 0.3783 (0.3783)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [79][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.3437 (0.4082)	Acc@1 87.891 (85.877)	Acc@5 100.000 (99.531)
Epoch: [79][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.4658 (0.4135)	Acc@1 82.031 (85.662)	Acc@5 98.828 (99.485)
Epoch: [79][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.4373 (0.4184)	Acc@1 85.156 (85.500)	Acc@5 100.000 (99.449)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:80/80; Lr: 0.1
batch Size 256
Epoch: [80][0/196]	Time 0.138 (0.138)	Data 0.276 (0.276)	Loss 0.3707 (0.3707)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [80][64/196]	Time 0.088 (0.088)	Data 0.000 (0.004)	Loss 0.3570 (0.4221)	Acc@1 87.109 (85.385)	Acc@5 98.828 (99.399)
Epoch: [80][128/196]	Time 0.093 (0.089)	Data 0.000 (0.002)	Loss 0.4296 (0.4126)	Acc@1 84.375 (85.783)	Acc@5 98.828 (99.455)
Epoch: [80][192/196]	Time 0.095 (0.088)	Data 0.000 (0.002)	Loss 0.4091 (0.4150)	Acc@1 85.938 (85.662)	Acc@5 99.609 (99.433)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  65.11
Max memory: 51.4381312
 17.656s  j: 81 bis 85
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3401
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 81
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:81/85; Lr: 0.1
batch Size 256
Epoch: [81][0/196]	Time 0.146 (0.146)	Data 0.281 (0.281)	Loss 0.4943 (0.4943)	Acc@1 83.203 (83.203)	Acc@5 98.828 (98.828)
Epoch: [81][64/196]	Time 0.095 (0.090)	Data 0.000 (0.005)	Loss 0.4085 (0.3973)	Acc@1 87.109 (86.358)	Acc@5 98.047 (99.435)
Epoch: [81][128/196]	Time 0.092 (0.089)	Data 0.000 (0.002)	Loss 0.4014 (0.4094)	Acc@1 86.328 (85.934)	Acc@5 100.000 (99.428)
Epoch: [81][192/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.4927 (0.4133)	Acc@1 83.594 (85.873)	Acc@5 99.609 (99.437)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:82/85; Lr: 0.1
batch Size 256
Epoch: [82][0/196]	Time 0.133 (0.133)	Data 0.314 (0.314)	Loss 0.4792 (0.4792)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [82][64/196]	Time 0.090 (0.091)	Data 0.000 (0.005)	Loss 0.4746 (0.3991)	Acc@1 83.984 (86.106)	Acc@5 98.828 (99.483)
Epoch: [82][128/196]	Time 0.087 (0.090)	Data 0.000 (0.003)	Loss 0.2926 (0.4064)	Acc@1 91.797 (85.947)	Acc@5 99.609 (99.403)
Epoch: [82][192/196]	Time 0.088 (0.090)	Data 0.000 (0.002)	Loss 0.3687 (0.4092)	Acc@1 85.156 (85.830)	Acc@5 99.609 (99.417)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:83/85; Lr: 0.1
batch Size 256
Epoch: [83][0/196]	Time 0.164 (0.164)	Data 0.281 (0.281)	Loss 0.5418 (0.5418)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [83][64/196]	Time 0.086 (0.091)	Data 0.000 (0.005)	Loss 0.4587 (0.4008)	Acc@1 82.422 (86.166)	Acc@5 99.609 (99.489)
Epoch: [83][128/196]	Time 0.085 (0.090)	Data 0.000 (0.002)	Loss 0.5525 (0.4084)	Acc@1 82.422 (85.907)	Acc@5 98.438 (99.388)
Epoch: [83][192/196]	Time 0.095 (0.089)	Data 0.000 (0.002)	Loss 0.2978 (0.4089)	Acc@1 89.453 (85.960)	Acc@5 99.609 (99.395)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:84/85; Lr: 0.1
batch Size 256
Epoch: [84][0/196]	Time 0.120 (0.120)	Data 0.303 (0.303)	Loss 0.3400 (0.3400)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [84][64/196]	Time 0.080 (0.090)	Data 0.000 (0.005)	Loss 0.4164 (0.4130)	Acc@1 87.891 (85.847)	Acc@5 100.000 (99.417)
Epoch: [84][128/196]	Time 0.089 (0.089)	Data 0.000 (0.003)	Loss 0.3928 (0.4064)	Acc@1 86.328 (86.195)	Acc@5 99.609 (99.440)
Epoch: [84][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.3737 (0.4099)	Acc@1 86.719 (86.006)	Acc@5 99.219 (99.439)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:85/85; Lr: 0.1
batch Size 256
Epoch: [85][0/196]	Time 0.142 (0.142)	Data 0.281 (0.281)	Loss 0.3376 (0.3376)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [85][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.4245 (0.3976)	Acc@1 85.156 (86.412)	Acc@5 100.000 (99.501)
Epoch: [85][128/196]	Time 0.080 (0.089)	Data 0.000 (0.002)	Loss 0.4320 (0.4026)	Acc@1 85.156 (86.268)	Acc@5 99.609 (99.422)
Epoch: [85][192/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.3744 (0.4085)	Acc@1 87.109 (86.099)	Acc@5 98.828 (99.381)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  72.66
Max memory: 51.4381312
 17.802s  j: 86 bis 90
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3254
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 86
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:86/90; Lr: 0.1
batch Size 256
Epoch: [86][0/196]	Time 0.138 (0.138)	Data 0.322 (0.322)	Loss 0.4448 (0.4448)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [86][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.4620 (0.3898)	Acc@1 83.594 (86.683)	Acc@5 99.609 (99.471)
Epoch: [86][128/196]	Time 0.091 (0.089)	Data 0.000 (0.003)	Loss 0.4499 (0.3954)	Acc@1 85.938 (86.416)	Acc@5 100.000 (99.458)
Epoch: [86][192/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.3915 (0.4069)	Acc@1 87.891 (86.126)	Acc@5 100.000 (99.462)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:87/90; Lr: 0.1
batch Size 256
Epoch: [87][0/196]	Time 0.119 (0.119)	Data 0.309 (0.309)	Loss 0.4555 (0.4555)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [87][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.4666 (0.4013)	Acc@1 83.594 (86.046)	Acc@5 99.609 (99.483)
Epoch: [87][128/196]	Time 0.088 (0.089)	Data 0.000 (0.003)	Loss 0.3586 (0.4091)	Acc@1 89.453 (85.813)	Acc@5 100.000 (99.506)
Epoch: [87][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.3936 (0.4051)	Acc@1 86.328 (86.061)	Acc@5 100.000 (99.514)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:88/90; Lr: 0.1
batch Size 256
Epoch: [88][0/196]	Time 0.128 (0.128)	Data 0.283 (0.283)	Loss 0.3958 (0.3958)	Acc@1 87.891 (87.891)	Acc@5 99.219 (99.219)
Epoch: [88][64/196]	Time 0.082 (0.090)	Data 0.000 (0.005)	Loss 0.3329 (0.3966)	Acc@1 89.062 (86.526)	Acc@5 100.000 (99.459)
Epoch: [88][128/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.4549 (0.4133)	Acc@1 84.766 (85.883)	Acc@5 99.219 (99.397)
Epoch: [88][192/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.4744 (0.4140)	Acc@1 82.812 (85.883)	Acc@5 99.609 (99.411)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:89/90; Lr: 0.1
batch Size 256
Epoch: [89][0/196]	Time 0.148 (0.148)	Data 0.318 (0.318)	Loss 0.4495 (0.4495)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [89][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.4848 (0.4157)	Acc@1 81.641 (85.529)	Acc@5 98.438 (99.459)
Epoch: [89][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.4192 (0.4091)	Acc@1 88.281 (85.853)	Acc@5 98.828 (99.458)
Epoch: [89][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.4801 (0.4085)	Acc@1 85.156 (85.907)	Acc@5 99.219 (99.441)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:90/90; Lr: 0.1
batch Size 256
Epoch: [90][0/196]	Time 0.126 (0.126)	Data 0.336 (0.336)	Loss 0.4323 (0.4323)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [90][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.4496 (0.4160)	Acc@1 83.594 (85.769)	Acc@5 99.609 (99.375)
Epoch: [90][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.3223 (0.4094)	Acc@1 89.844 (85.819)	Acc@5 100.000 (99.428)
Epoch: [90][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.4203 (0.4107)	Acc@1 84.375 (85.759)	Acc@5 100.000 (99.445)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  73.35
Max memory: 51.4381312
 17.596s  j: 91 bis 95
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3464
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 91
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:91/95; Lr: 0.1
batch Size 256
Epoch: [91][0/196]	Time 0.167 (0.167)	Data 0.324 (0.324)	Loss 0.4070 (0.4070)	Acc@1 89.062 (89.062)	Acc@5 99.219 (99.219)
Epoch: [91][64/196]	Time 0.101 (0.090)	Data 0.000 (0.005)	Loss 0.4059 (0.3896)	Acc@1 86.328 (86.755)	Acc@5 99.219 (99.459)
Epoch: [91][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.4901 (0.3990)	Acc@1 83.984 (86.470)	Acc@5 98.828 (99.434)
Epoch: [91][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.4743 (0.4041)	Acc@1 85.156 (86.269)	Acc@5 99.609 (99.447)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:92/95; Lr: 0.1
batch Size 256
Epoch: [92][0/196]	Time 0.124 (0.124)	Data 0.293 (0.293)	Loss 0.3891 (0.3891)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [92][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.2993 (0.4083)	Acc@1 89.844 (85.811)	Acc@5 99.609 (99.351)
Epoch: [92][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.3837 (0.4037)	Acc@1 87.500 (86.095)	Acc@5 100.000 (99.422)
Epoch: [92][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.3909 (0.4114)	Acc@1 83.984 (85.863)	Acc@5 100.000 (99.405)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:93/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [93][0/196]	Time 0.133 (0.133)	Data 0.268 (0.268)	Loss 0.3830 (0.3830)	Acc@1 86.328 (86.328)	Acc@5 98.828 (98.828)
Epoch: [93][64/196]	Time 0.081 (0.089)	Data 0.000 (0.004)	Loss 0.2682 (0.3296)	Acc@1 91.406 (89.008)	Acc@5 100.000 (99.531)
Epoch: [93][128/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.2749 (0.3066)	Acc@1 88.281 (89.741)	Acc@5 99.609 (99.603)
Epoch: [93][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.2503 (0.2937)	Acc@1 92.188 (90.123)	Acc@5 100.000 (99.664)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:94/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [94][0/196]	Time 0.129 (0.129)	Data 0.303 (0.303)	Loss 0.2424 (0.2424)	Acc@1 92.578 (92.578)	Acc@5 99.609 (99.609)
Epoch: [94][64/196]	Time 0.079 (0.089)	Data 0.000 (0.005)	Loss 0.2184 (0.2464)	Acc@1 92.188 (91.520)	Acc@5 100.000 (99.736)
Epoch: [94][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.2398 (0.2488)	Acc@1 93.750 (91.446)	Acc@5 100.000 (99.721)
Epoch: [94][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.2897 (0.2512)	Acc@1 88.672 (91.400)	Acc@5 100.000 (99.743)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:95/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [95][0/196]	Time 0.131 (0.131)	Data 0.297 (0.297)	Loss 0.1958 (0.1958)	Acc@1 91.797 (91.797)	Acc@5 99.609 (99.609)
Epoch: [95][64/196]	Time 0.088 (0.090)	Data 0.000 (0.005)	Loss 0.2362 (0.2369)	Acc@1 90.234 (92.061)	Acc@5 99.609 (99.760)
Epoch: [95][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.2419 (0.2395)	Acc@1 93.359 (91.939)	Acc@5 98.828 (99.758)
Epoch: [95][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.2470 (0.2368)	Acc@1 92.188 (91.961)	Acc@5 99.219 (99.763)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.57
Max memory: 51.4381312
 17.501s  j: 96 bis 100
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6181
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 96
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:96/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [96][0/196]	Time 0.139 (0.139)	Data 0.324 (0.324)	Loss 0.1649 (0.1649)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [96][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.2153 (0.2244)	Acc@1 93.750 (92.368)	Acc@5 99.609 (99.796)
Epoch: [96][128/196]	Time 0.090 (0.087)	Data 0.000 (0.003)	Loss 0.2189 (0.2250)	Acc@1 92.969 (92.318)	Acc@5 99.609 (99.803)
Epoch: [96][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.2142 (0.2284)	Acc@1 92.188 (92.139)	Acc@5 100.000 (99.798)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:97/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [97][0/196]	Time 0.124 (0.124)	Data 0.259 (0.259)	Loss 0.1666 (0.1666)	Acc@1 93.750 (93.750)	Acc@5 99.609 (99.609)
Epoch: [97][64/196]	Time 0.078 (0.088)	Data 0.000 (0.004)	Loss 0.1572 (0.2226)	Acc@1 94.531 (92.494)	Acc@5 100.000 (99.832)
Epoch: [97][128/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.2166 (0.2197)	Acc@1 93.359 (92.533)	Acc@5 100.000 (99.836)
Epoch: [97][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.2236 (0.2219)	Acc@1 91.016 (92.487)	Acc@5 99.219 (99.826)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:98/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [98][0/196]	Time 0.127 (0.127)	Data 0.267 (0.267)	Loss 0.2125 (0.2125)	Acc@1 94.531 (94.531)	Acc@5 99.609 (99.609)
Epoch: [98][64/196]	Time 0.087 (0.087)	Data 0.000 (0.004)	Loss 0.2670 (0.2152)	Acc@1 91.406 (92.638)	Acc@5 99.609 (99.880)
Epoch: [98][128/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.1925 (0.2146)	Acc@1 94.531 (92.672)	Acc@5 100.000 (99.836)
Epoch: [98][192/196]	Time 0.089 (0.086)	Data 0.000 (0.002)	Loss 0.2698 (0.2155)	Acc@1 91.406 (92.588)	Acc@5 99.609 (99.828)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:99/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [99][0/196]	Time 0.118 (0.118)	Data 0.308 (0.308)	Loss 0.1952 (0.1952)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [99][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.2057 (0.2047)	Acc@1 94.141 (93.059)	Acc@5 99.219 (99.856)
Epoch: [99][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.3091 (0.2075)	Acc@1 88.672 (92.878)	Acc@5 99.609 (99.861)
Epoch: [99][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.2052 (0.2080)	Acc@1 93.750 (92.944)	Acc@5 99.609 (99.820)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:100/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [100][0/196]	Time 0.123 (0.123)	Data 0.295 (0.295)	Loss 0.2087 (0.2087)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [100][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.2240 (0.2031)	Acc@1 91.797 (93.137)	Acc@5 100.000 (99.832)
Epoch: [100][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.2280 (0.2025)	Acc@1 90.234 (93.035)	Acc@5 100.000 (99.846)
Epoch: [100][192/196]	Time 0.100 (0.087)	Data 0.000 (0.002)	Loss 0.2213 (0.2031)	Acc@1 92.188 (92.995)	Acc@5 100.000 (99.854)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.77
Max memory: 51.4381312
 17.521s  j: 101 bis 105
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7379
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 101
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:101/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [101][0/196]	Time 0.127 (0.127)	Data 0.288 (0.288)	Loss 0.1650 (0.1650)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [101][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.1763 (0.1915)	Acc@1 94.141 (93.600)	Acc@5 100.000 (99.868)
Epoch: [101][128/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.1655 (0.1963)	Acc@1 93.359 (93.390)	Acc@5 100.000 (99.870)
Epoch: [101][192/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.1916 (0.2011)	Acc@1 92.969 (93.197)	Acc@5 100.000 (99.850)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:102/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [102][0/196]	Time 0.140 (0.140)	Data 0.300 (0.300)	Loss 0.1749 (0.1749)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [102][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.2224 (0.2016)	Acc@1 92.188 (92.891)	Acc@5 99.609 (99.862)
Epoch: [102][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.1211 (0.1990)	Acc@1 94.922 (93.084)	Acc@5 100.000 (99.852)
Epoch: [102][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.2278 (0.2000)	Acc@1 93.750 (93.116)	Acc@5 100.000 (99.860)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:103/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [103][0/196]	Time 0.123 (0.123)	Data 0.330 (0.330)	Loss 0.1331 (0.1331)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [103][64/196]	Time 0.093 (0.088)	Data 0.000 (0.005)	Loss 0.1712 (0.1872)	Acc@1 94.531 (93.666)	Acc@5 100.000 (99.874)
Epoch: [103][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.1966 (0.1911)	Acc@1 92.578 (93.529)	Acc@5 100.000 (99.852)
Epoch: [103][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.2070 (0.1955)	Acc@1 94.922 (93.305)	Acc@5 100.000 (99.844)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:104/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [104][0/196]	Time 0.115 (0.115)	Data 0.305 (0.305)	Loss 0.1924 (0.1924)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [104][64/196]	Time 0.083 (0.087)	Data 0.000 (0.005)	Loss 0.1896 (0.1860)	Acc@1 94.141 (93.636)	Acc@5 99.609 (99.886)
Epoch: [104][128/196]	Time 0.091 (0.087)	Data 0.000 (0.003)	Loss 0.2658 (0.1879)	Acc@1 91.406 (93.674)	Acc@5 99.609 (99.879)
Epoch: [104][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.2124 (0.1891)	Acc@1 92.188 (93.592)	Acc@5 100.000 (99.844)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:105/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [105][0/196]	Time 0.130 (0.130)	Data 0.286 (0.286)	Loss 0.2607 (0.2607)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [105][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.1296 (0.1844)	Acc@1 94.922 (93.510)	Acc@5 100.000 (99.844)
Epoch: [105][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1613 (0.1886)	Acc@1 95.312 (93.459)	Acc@5 100.000 (99.846)
Epoch: [105][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1691 (0.1880)	Acc@1 94.141 (93.477)	Acc@5 100.000 (99.868)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.12
Max memory: 51.4381312
 17.544s  j: 106 bis 110
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4010
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 106
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:106/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [106][0/196]	Time 0.157 (0.157)	Data 0.299 (0.299)	Loss 0.1976 (0.1976)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [106][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.1637 (0.1843)	Acc@1 93.750 (93.618)	Acc@5 100.000 (99.892)
Epoch: [106][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.2882 (0.1872)	Acc@1 91.406 (93.611)	Acc@5 99.219 (99.879)
Epoch: [106][192/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.2362 (0.1872)	Acc@1 92.969 (93.481)	Acc@5 100.000 (99.877)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:107/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [107][0/196]	Time 0.133 (0.133)	Data 0.284 (0.284)	Loss 0.2299 (0.2299)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [107][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.1730 (0.1844)	Acc@1 92.969 (93.702)	Acc@5 100.000 (99.898)
Epoch: [107][128/196]	Time 0.095 (0.089)	Data 0.000 (0.002)	Loss 0.1951 (0.1838)	Acc@1 92.188 (93.644)	Acc@5 99.219 (99.885)
Epoch: [107][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.2905 (0.1852)	Acc@1 90.234 (93.637)	Acc@5 100.000 (99.870)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:108/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [108][0/196]	Time 0.108 (0.108)	Data 0.302 (0.302)	Loss 0.2170 (0.2170)	Acc@1 94.141 (94.141)	Acc@5 99.609 (99.609)
Epoch: [108][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.1689 (0.1811)	Acc@1 94.141 (94.014)	Acc@5 100.000 (99.892)
Epoch: [108][128/196]	Time 0.094 (0.089)	Data 0.000 (0.003)	Loss 0.1781 (0.1809)	Acc@1 93.750 (94.010)	Acc@5 99.609 (99.891)
Epoch: [108][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.2442 (0.1806)	Acc@1 91.406 (93.963)	Acc@5 100.000 (99.885)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:109/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [109][0/196]	Time 0.135 (0.135)	Data 0.274 (0.274)	Loss 0.1346 (0.1346)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [109][64/196]	Time 0.097 (0.089)	Data 0.000 (0.004)	Loss 0.1824 (0.1706)	Acc@1 94.922 (94.056)	Acc@5 100.000 (99.892)
Epoch: [109][128/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 0.1566 (0.1752)	Acc@1 96.094 (93.965)	Acc@5 100.000 (99.900)
Epoch: [109][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.2503 (0.1807)	Acc@1 92.188 (93.774)	Acc@5 100.000 (99.881)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:110/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [110][0/196]	Time 0.117 (0.117)	Data 0.323 (0.323)	Loss 0.1900 (0.1900)	Acc@1 92.188 (92.188)	Acc@5 99.609 (99.609)
Epoch: [110][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.1214 (0.1726)	Acc@1 95.312 (93.990)	Acc@5 100.000 (99.868)
Epoch: [110][128/196]	Time 0.088 (0.089)	Data 0.000 (0.003)	Loss 0.1516 (0.1730)	Acc@1 93.750 (93.971)	Acc@5 100.000 (99.879)
Epoch: [110][192/196]	Time 0.093 (0.089)	Data 0.000 (0.002)	Loss 0.2305 (0.1770)	Acc@1 91.406 (93.829)	Acc@5 100.000 (99.877)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.41
Max memory: 51.4381312
 17.767s  j: 111 bis 115
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9294
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 111
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:111/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [111][0/196]	Time 0.175 (0.175)	Data 0.308 (0.308)	Loss 0.1557 (0.1557)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [111][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.1355 (0.1727)	Acc@1 95.312 (94.351)	Acc@5 100.000 (99.898)
Epoch: [111][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.2463 (0.1744)	Acc@1 90.234 (94.147)	Acc@5 100.000 (99.888)
Epoch: [111][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.2199 (0.1763)	Acc@1 92.188 (94.013)	Acc@5 99.609 (99.887)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:112/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [112][0/196]	Time 0.125 (0.125)	Data 0.280 (0.280)	Loss 0.1339 (0.1339)	Acc@1 96.094 (96.094)	Acc@5 99.609 (99.609)
Epoch: [112][64/196]	Time 0.080 (0.089)	Data 0.000 (0.005)	Loss 0.2203 (0.1747)	Acc@1 90.234 (93.912)	Acc@5 99.609 (99.838)
Epoch: [112][128/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.2241 (0.1715)	Acc@1 92.578 (94.059)	Acc@5 100.000 (99.882)
Epoch: [112][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.2122 (0.1758)	Acc@1 92.969 (93.987)	Acc@5 99.609 (99.883)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:113/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [113][0/196]	Time 0.151 (0.151)	Data 0.278 (0.278)	Loss 0.2199 (0.2199)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [113][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.2023 (0.1659)	Acc@1 92.188 (94.183)	Acc@5 100.000 (99.892)
Epoch: [113][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1697 (0.1705)	Acc@1 94.531 (94.095)	Acc@5 100.000 (99.894)
Epoch: [113][192/196]	Time 0.079 (0.088)	Data 0.000 (0.002)	Loss 0.1921 (0.1730)	Acc@1 94.141 (94.033)	Acc@5 100.000 (99.870)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:114/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [114][0/196]	Time 0.129 (0.129)	Data 0.300 (0.300)	Loss 0.1588 (0.1588)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [114][64/196]	Time 0.097 (0.086)	Data 0.000 (0.005)	Loss 0.1119 (0.1680)	Acc@1 96.484 (94.249)	Acc@5 100.000 (99.880)
Epoch: [114][128/196]	Time 0.080 (0.086)	Data 0.000 (0.003)	Loss 0.1871 (0.1684)	Acc@1 93.359 (94.244)	Acc@5 100.000 (99.870)
Epoch: [114][192/196]	Time 0.085 (0.086)	Data 0.000 (0.002)	Loss 0.2181 (0.1697)	Acc@1 92.969 (94.195)	Acc@5 100.000 (99.866)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:115/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [115][0/196]	Time 0.122 (0.122)	Data 0.307 (0.307)	Loss 0.1605 (0.1605)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [115][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.1826 (0.1649)	Acc@1 93.359 (94.207)	Acc@5 100.000 (99.898)
Epoch: [115][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.1745 (0.1678)	Acc@1 93.359 (94.107)	Acc@5 100.000 (99.897)
Epoch: [115][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.1870 (0.1715)	Acc@1 92.969 (94.009)	Acc@5 100.000 (99.889)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.47
Max memory: 51.4381312
 17.470s  j: 116 bis 120
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8058
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 116
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:116/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [116][0/196]	Time 0.171 (0.171)	Data 0.270 (0.270)	Loss 0.1515 (0.1515)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [116][64/196]	Time 0.084 (0.088)	Data 0.000 (0.004)	Loss 0.1834 (0.1656)	Acc@1 93.359 (94.225)	Acc@5 100.000 (99.904)
Epoch: [116][128/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.2314 (0.1700)	Acc@1 91.016 (94.132)	Acc@5 99.609 (99.912)
Epoch: [116][192/196]	Time 0.081 (0.086)	Data 0.000 (0.002)	Loss 0.1778 (0.1726)	Acc@1 93.750 (94.082)	Acc@5 100.000 (99.907)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:117/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [117][0/196]	Time 0.119 (0.119)	Data 0.296 (0.296)	Loss 0.1314 (0.1314)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [117][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.1158 (0.1583)	Acc@1 95.312 (94.459)	Acc@5 100.000 (99.898)
Epoch: [117][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.1971 (0.1623)	Acc@1 94.531 (94.386)	Acc@5 100.000 (99.888)
Epoch: [117][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.1334 (0.1669)	Acc@1 94.141 (94.280)	Acc@5 100.000 (99.883)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:118/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [118][0/196]	Time 0.127 (0.127)	Data 0.308 (0.308)	Loss 0.1381 (0.1381)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [118][64/196]	Time 0.081 (0.090)	Data 0.000 (0.005)	Loss 0.1674 (0.1667)	Acc@1 94.141 (94.321)	Acc@5 99.609 (99.892)
Epoch: [118][128/196]	Time 0.097 (0.089)	Data 0.000 (0.003)	Loss 0.1508 (0.1657)	Acc@1 94.922 (94.322)	Acc@5 100.000 (99.912)
Epoch: [118][192/196]	Time 0.078 (0.088)	Data 0.000 (0.002)	Loss 0.1789 (0.1642)	Acc@1 92.969 (94.309)	Acc@5 100.000 (99.917)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:119/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [119][0/196]	Time 0.118 (0.118)	Data 0.297 (0.297)	Loss 0.1623 (0.1623)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [119][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 0.1760 (0.1657)	Acc@1 93.359 (94.357)	Acc@5 99.609 (99.928)
Epoch: [119][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.2191 (0.1665)	Acc@1 92.969 (94.334)	Acc@5 100.000 (99.936)
Epoch: [119][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.1958 (0.1699)	Acc@1 92.578 (94.167)	Acc@5 100.000 (99.931)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:120/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [120][0/196]	Time 0.124 (0.124)	Data 0.308 (0.308)	Loss 0.1445 (0.1445)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [120][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.1839 (0.1610)	Acc@1 93.359 (94.363)	Acc@5 100.000 (99.946)
Epoch: [120][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.2557 (0.1604)	Acc@1 91.797 (94.480)	Acc@5 99.609 (99.906)
Epoch: [120][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.1883 (0.1669)	Acc@1 91.797 (94.213)	Acc@5 100.000 (99.915)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.31
Max memory: 51.4381312
 17.332s  j: 121 bis 125
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7876
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 121
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:121/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [121][0/196]	Time 0.155 (0.155)	Data 0.274 (0.274)	Loss 0.1340 (0.1340)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [121][64/196]	Time 0.086 (0.089)	Data 0.000 (0.004)	Loss 0.1262 (0.1567)	Acc@1 96.484 (94.555)	Acc@5 100.000 (99.940)
Epoch: [121][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.1651 (0.1596)	Acc@1 93.750 (94.428)	Acc@5 100.000 (99.903)
Epoch: [121][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.2031 (0.1638)	Acc@1 91.797 (94.274)	Acc@5 99.609 (99.903)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:122/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [122][0/196]	Time 0.157 (0.157)	Data 0.293 (0.293)	Loss 0.1776 (0.1776)	Acc@1 94.141 (94.141)	Acc@5 99.609 (99.609)
Epoch: [122][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.1754 (0.1579)	Acc@1 94.531 (94.573)	Acc@5 99.609 (99.886)
Epoch: [122][128/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.1585 (0.1588)	Acc@1 94.531 (94.571)	Acc@5 100.000 (99.909)
Epoch: [122][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.1980 (0.1627)	Acc@1 92.188 (94.416)	Acc@5 99.609 (99.905)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:123/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [123][0/196]	Time 0.131 (0.131)	Data 0.290 (0.290)	Loss 0.1732 (0.1732)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [123][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.1304 (0.1542)	Acc@1 96.484 (94.591)	Acc@5 100.000 (99.958)
Epoch: [123][128/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.2158 (0.1614)	Acc@1 91.406 (94.344)	Acc@5 99.609 (99.930)
Epoch: [123][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.1430 (0.1639)	Acc@1 96.094 (94.333)	Acc@5 100.000 (99.903)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:124/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [124][0/196]	Time 0.129 (0.129)	Data 0.317 (0.317)	Loss 0.1371 (0.1371)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [124][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.1274 (0.1600)	Acc@1 95.703 (94.357)	Acc@5 100.000 (99.916)
Epoch: [124][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.1610 (0.1620)	Acc@1 94.141 (94.310)	Acc@5 100.000 (99.912)
Epoch: [124][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.1480 (0.1631)	Acc@1 95.312 (94.305)	Acc@5 100.000 (99.913)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:125/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [125][0/196]	Time 0.126 (0.126)	Data 0.297 (0.297)	Loss 0.2028 (0.2028)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [125][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.1535 (0.1668)	Acc@1 94.531 (94.050)	Acc@5 99.609 (99.928)
Epoch: [125][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.1473 (0.1690)	Acc@1 94.141 (94.038)	Acc@5 100.000 (99.888)
Epoch: [125][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.1873 (0.1676)	Acc@1 93.750 (94.084)	Acc@5 99.609 (99.901)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.7
Max memory: 51.4381312
 17.544s  j: 126 bis 130
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4539
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 126
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:126/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [126][0/196]	Time 0.162 (0.162)	Data 0.272 (0.272)	Loss 0.1224 (0.1224)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [126][64/196]	Time 0.083 (0.090)	Data 0.000 (0.004)	Loss 0.1553 (0.1524)	Acc@1 94.141 (94.730)	Acc@5 100.000 (99.940)
Epoch: [126][128/196]	Time 0.098 (0.089)	Data 0.000 (0.002)	Loss 0.1054 (0.1572)	Acc@1 96.484 (94.537)	Acc@5 100.000 (99.918)
Epoch: [126][192/196]	Time 0.098 (0.088)	Data 0.000 (0.002)	Loss 0.1874 (0.1614)	Acc@1 92.969 (94.307)	Acc@5 100.000 (99.903)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:127/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [127][0/196]	Time 0.144 (0.144)	Data 0.307 (0.307)	Loss 0.1134 (0.1134)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [127][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.1877 (0.1502)	Acc@1 94.922 (94.766)	Acc@5 99.609 (99.916)
Epoch: [127][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.1590 (0.1578)	Acc@1 94.141 (94.516)	Acc@5 100.000 (99.903)
Epoch: [127][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.1586 (0.1629)	Acc@1 93.750 (94.266)	Acc@5 100.000 (99.905)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:128/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [128][0/196]	Time 0.131 (0.131)	Data 0.319 (0.319)	Loss 0.2070 (0.2070)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [128][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.1668 (0.1611)	Acc@1 94.141 (94.303)	Acc@5 100.000 (99.928)
Epoch: [128][128/196]	Time 0.099 (0.087)	Data 0.000 (0.003)	Loss 0.1578 (0.1629)	Acc@1 94.531 (94.244)	Acc@5 100.000 (99.915)
Epoch: [128][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.1059 (0.1626)	Acc@1 96.484 (94.339)	Acc@5 100.000 (99.911)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:129/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [129][0/196]	Time 0.134 (0.134)	Data 0.314 (0.314)	Loss 0.1600 (0.1600)	Acc@1 96.094 (96.094)	Acc@5 99.609 (99.609)
Epoch: [129][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.0799 (0.1530)	Acc@1 98.438 (94.706)	Acc@5 100.000 (99.856)
Epoch: [129][128/196]	Time 0.081 (0.088)	Data 0.000 (0.003)	Loss 0.1618 (0.1564)	Acc@1 93.750 (94.513)	Acc@5 100.000 (99.891)
Epoch: [129][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.1195 (0.1590)	Acc@1 96.875 (94.469)	Acc@5 100.000 (99.905)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:130/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [130][0/196]	Time 0.141 (0.141)	Data 0.272 (0.272)	Loss 0.2083 (0.2083)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [130][64/196]	Time 0.091 (0.089)	Data 0.000 (0.004)	Loss 0.1310 (0.1561)	Acc@1 95.703 (94.681)	Acc@5 99.609 (99.898)
Epoch: [130][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.2041 (0.1572)	Acc@1 92.969 (94.589)	Acc@5 99.609 (99.897)
Epoch: [130][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.1636 (0.1577)	Acc@1 93.750 (94.507)	Acc@5 100.000 (99.909)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  87.75
Max memory: 51.4381312
 17.421s  j: 131 bis 135
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5464
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:131/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [131][0/196]	Time 0.183 (0.183)	Data 0.286 (0.286)	Loss 0.2123 (0.2123)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [131][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.1275 (0.1458)	Acc@1 96.094 (94.940)	Acc@5 100.000 (99.946)
Epoch: [131][128/196]	Time 0.095 (0.088)	Data 0.000 (0.002)	Loss 0.1523 (0.1553)	Acc@1 93.359 (94.586)	Acc@5 100.000 (99.930)
Epoch: [131][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.1792 (0.1586)	Acc@1 92.578 (94.456)	Acc@5 100.000 (99.913)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:132/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [132][0/196]	Time 0.136 (0.136)	Data 0.284 (0.284)	Loss 0.1420 (0.1420)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [132][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.1252 (0.1589)	Acc@1 94.922 (94.387)	Acc@5 100.000 (99.946)
Epoch: [132][128/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.1003 (0.1610)	Acc@1 97.656 (94.434)	Acc@5 100.000 (99.915)
Epoch: [132][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.1392 (0.1635)	Acc@1 95.312 (94.290)	Acc@5 99.609 (99.907)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:133/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [133][0/196]	Time 0.122 (0.122)	Data 0.318 (0.318)	Loss 0.1636 (0.1636)	Acc@1 94.922 (94.922)	Acc@5 99.609 (99.609)
Epoch: [133][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.1927 (0.1559)	Acc@1 92.969 (94.489)	Acc@5 100.000 (99.916)
Epoch: [133][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.2004 (0.1567)	Acc@1 94.141 (94.431)	Acc@5 100.000 (99.909)
Epoch: [133][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.1468 (0.1609)	Acc@1 94.141 (94.327)	Acc@5 100.000 (99.905)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:134/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [134][0/196]	Time 0.121 (0.121)	Data 0.301 (0.301)	Loss 0.1948 (0.1948)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [134][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.1766 (0.1514)	Acc@1 95.312 (94.754)	Acc@5 100.000 (99.934)
Epoch: [134][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.1784 (0.1570)	Acc@1 92.578 (94.486)	Acc@5 100.000 (99.915)
Epoch: [134][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.1784 (0.1613)	Acc@1 92.188 (94.327)	Acc@5 100.000 (99.895)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:135/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [135][0/196]	Time 0.128 (0.128)	Data 0.342 (0.342)	Loss 0.1521 (0.1521)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [135][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.1315 (0.1599)	Acc@1 95.703 (94.273)	Acc@5 100.000 (99.952)
Epoch: [135][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.1805 (0.1609)	Acc@1 92.969 (94.265)	Acc@5 99.219 (99.927)
Epoch: [135][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.1578 (0.1616)	Acc@1 94.141 (94.242)	Acc@5 100.000 (99.921)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.87
Max memory: 51.4381312
 17.528s  j: 136 bis 140
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2944
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:136/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [136][0/196]	Time 0.171 (0.171)	Data 0.299 (0.299)	Loss 0.1011 (0.1011)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [136][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.1291 (0.1514)	Acc@1 95.312 (95.024)	Acc@5 100.000 (99.904)
Epoch: [136][128/196]	Time 0.080 (0.087)	Data 0.000 (0.003)	Loss 0.1472 (0.1538)	Acc@1 94.531 (94.792)	Acc@5 100.000 (99.918)
Epoch: [136][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.1696 (0.1572)	Acc@1 95.703 (94.600)	Acc@5 99.609 (99.913)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:137/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [137][0/196]	Time 0.134 (0.134)	Data 0.301 (0.301)	Loss 0.1759 (0.1759)	Acc@1 94.531 (94.531)	Acc@5 99.609 (99.609)
Epoch: [137][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.1401 (0.1610)	Acc@1 93.750 (94.333)	Acc@5 100.000 (99.958)
Epoch: [137][128/196]	Time 0.081 (0.088)	Data 0.000 (0.003)	Loss 0.1812 (0.1614)	Acc@1 94.531 (94.340)	Acc@5 100.000 (99.939)
Epoch: [137][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.2095 (0.1645)	Acc@1 92.578 (94.258)	Acc@5 100.000 (99.935)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:138/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [138][0/196]	Time 0.124 (0.124)	Data 0.313 (0.313)	Loss 0.1335 (0.1335)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [138][64/196]	Time 0.101 (0.087)	Data 0.000 (0.005)	Loss 0.1453 (0.1578)	Acc@1 93.750 (94.639)	Acc@5 100.000 (99.910)
Epoch: [138][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.1498 (0.1573)	Acc@1 96.484 (94.674)	Acc@5 100.000 (99.909)
Epoch: [138][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.1817 (0.1591)	Acc@1 94.531 (94.566)	Acc@5 99.609 (99.917)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:139/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [139][0/196]	Time 0.106 (0.106)	Data 0.282 (0.282)	Loss 0.1363 (0.1363)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [139][64/196]	Time 0.083 (0.087)	Data 0.000 (0.005)	Loss 0.0909 (0.1546)	Acc@1 97.266 (94.718)	Acc@5 100.000 (99.940)
Epoch: [139][128/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.1173 (0.1584)	Acc@1 95.312 (94.601)	Acc@5 100.000 (99.924)
Epoch: [139][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.1360 (0.1604)	Acc@1 96.094 (94.466)	Acc@5 100.000 (99.921)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:140/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [140][0/196]	Time 0.107 (0.107)	Data 0.326 (0.326)	Loss 0.1438 (0.1438)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [140][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.1305 (0.1482)	Acc@1 95.312 (94.838)	Acc@5 99.609 (99.934)
Epoch: [140][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.1220 (0.1533)	Acc@1 95.312 (94.598)	Acc@5 100.000 (99.921)
Epoch: [140][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.1483 (0.1600)	Acc@1 94.141 (94.359)	Acc@5 99.609 (99.917)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  87.43
Max memory: 51.4381312
 17.480s  j: 141 bis 145
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8503
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 141
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:141/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [141][0/196]	Time 0.155 (0.155)	Data 0.312 (0.312)	Loss 0.2104 (0.2104)	Acc@1 93.750 (93.750)	Acc@5 99.609 (99.609)
Epoch: [141][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.1571 (0.1414)	Acc@1 94.922 (95.162)	Acc@5 100.000 (99.922)
Epoch: [141][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.1907 (0.1473)	Acc@1 94.531 (94.861)	Acc@5 100.000 (99.906)
Epoch: [141][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.2123 (0.1563)	Acc@1 92.578 (94.521)	Acc@5 100.000 (99.909)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:142/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [142][0/196]	Time 0.131 (0.131)	Data 0.312 (0.312)	Loss 0.1261 (0.1261)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [142][64/196]	Time 0.080 (0.087)	Data 0.000 (0.005)	Loss 0.1436 (0.1477)	Acc@1 94.141 (94.916)	Acc@5 100.000 (99.946)
Epoch: [142][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.1455 (0.1546)	Acc@1 94.922 (94.601)	Acc@5 100.000 (99.924)
Epoch: [142][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.1467 (0.1553)	Acc@1 93.750 (94.574)	Acc@5 100.000 (99.919)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:143/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [143][0/196]	Time 0.139 (0.139)	Data 0.283 (0.283)	Loss 0.1820 (0.1820)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [143][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.2066 (0.1616)	Acc@1 92.188 (94.357)	Acc@5 100.000 (99.922)
Epoch: [143][128/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.1263 (0.1558)	Acc@1 95.312 (94.577)	Acc@5 100.000 (99.939)
Epoch: [143][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.2069 (0.1579)	Acc@1 94.141 (94.598)	Acc@5 99.609 (99.933)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:144/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [144][0/196]	Time 0.121 (0.121)	Data 0.315 (0.315)	Loss 0.1540 (0.1540)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [144][64/196]	Time 0.079 (0.088)	Data 0.000 (0.005)	Loss 0.1520 (0.1601)	Acc@1 94.531 (94.279)	Acc@5 100.000 (99.970)
Epoch: [144][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.1301 (0.1634)	Acc@1 96.484 (94.268)	Acc@5 100.000 (99.961)
Epoch: [144][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0889 (0.1613)	Acc@1 96.484 (94.345)	Acc@5 100.000 (99.941)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:145/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [145][0/196]	Time 0.119 (0.119)	Data 0.306 (0.306)	Loss 0.1525 (0.1525)	Acc@1 95.312 (95.312)	Acc@5 99.609 (99.609)
Epoch: [145][64/196]	Time 0.080 (0.088)	Data 0.000 (0.005)	Loss 0.2020 (0.1580)	Acc@1 94.531 (94.435)	Acc@5 99.219 (99.892)
Epoch: [145][128/196]	Time 0.077 (0.087)	Data 0.000 (0.003)	Loss 0.0893 (0.1599)	Acc@1 97.266 (94.453)	Acc@5 100.000 (99.918)
Epoch: [145][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.1434 (0.1631)	Acc@1 95.312 (94.351)	Acc@5 100.000 (99.911)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.03
Max memory: 51.4381312
 17.434s  j: 146 bis 150
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8870
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 146
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:146/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [146][0/196]	Time 0.172 (0.172)	Data 0.278 (0.278)	Loss 0.1801 (0.1801)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [146][64/196]	Time 0.088 (0.089)	Data 0.000 (0.004)	Loss 0.1761 (0.1471)	Acc@1 94.141 (94.910)	Acc@5 100.000 (99.958)
Epoch: [146][128/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.1245 (0.1526)	Acc@1 95.312 (94.686)	Acc@5 100.000 (99.939)
Epoch: [146][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1307 (0.1569)	Acc@1 94.922 (94.558)	Acc@5 100.000 (99.933)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:147/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [147][0/196]	Time 0.127 (0.127)	Data 0.276 (0.276)	Loss 0.1490 (0.1490)	Acc@1 93.750 (93.750)	Acc@5 99.609 (99.609)
Epoch: [147][64/196]	Time 0.082 (0.088)	Data 0.000 (0.004)	Loss 0.1622 (0.1541)	Acc@1 94.531 (94.760)	Acc@5 100.000 (99.940)
Epoch: [147][128/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.1172 (0.1576)	Acc@1 97.656 (94.537)	Acc@5 100.000 (99.924)
Epoch: [147][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.2023 (0.1583)	Acc@1 94.141 (94.495)	Acc@5 100.000 (99.917)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:148/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [148][0/196]	Time 0.114 (0.114)	Data 0.303 (0.303)	Loss 0.1098 (0.1098)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [148][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.1911 (0.1569)	Acc@1 93.750 (94.531)	Acc@5 99.609 (99.970)
Epoch: [148][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.1875 (0.1600)	Acc@1 92.188 (94.401)	Acc@5 100.000 (99.945)
Epoch: [148][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.1835 (0.1606)	Acc@1 91.797 (94.359)	Acc@5 100.000 (99.937)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:149/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [149][0/196]	Time 0.118 (0.118)	Data 0.316 (0.316)	Loss 0.1528 (0.1528)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [149][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.1297 (0.1583)	Acc@1 95.703 (94.423)	Acc@5 100.000 (99.898)
Epoch: [149][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.1849 (0.1600)	Acc@1 95.312 (94.474)	Acc@5 100.000 (99.909)
Epoch: [149][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.2763 (0.1648)	Acc@1 91.406 (94.246)	Acc@5 99.609 (99.917)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:150/150; Lr: 0.0010000000000000002
batch Size 256
Epoch: [150][0/196]	Time 0.110 (0.110)	Data 0.315 (0.315)	Loss 0.1763 (0.1763)	Acc@1 94.141 (94.141)	Acc@5 99.609 (99.609)
Epoch: [150][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.1109 (0.1350)	Acc@1 96.875 (95.349)	Acc@5 100.000 (99.952)
Epoch: [150][128/196]	Time 0.094 (0.088)	Data 0.000 (0.003)	Loss 0.1292 (0.1301)	Acc@1 95.312 (95.618)	Acc@5 99.609 (99.930)
Epoch: [150][192/196]	Time 0.079 (0.088)	Data 0.000 (0.002)	Loss 0.1024 (0.1262)	Acc@1 97.656 (95.802)	Acc@5 100.000 (99.941)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.1
Max memory: 51.4381312
 17.678s  j: 151 bis 155
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6000
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 151
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:151/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [151][0/196]	Time 0.150 (0.150)	Data 0.277 (0.277)	Loss 0.1329 (0.1329)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [151][64/196]	Time 0.089 (0.089)	Data 0.000 (0.004)	Loss 0.0823 (0.1140)	Acc@1 98.047 (96.184)	Acc@5 100.000 (99.958)
Epoch: [151][128/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.0843 (0.1153)	Acc@1 97.266 (96.200)	Acc@5 100.000 (99.967)
Epoch: [151][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.1050 (0.1124)	Acc@1 97.266 (96.314)	Acc@5 100.000 (99.960)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:152/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [152][0/196]	Time 0.137 (0.137)	Data 0.348 (0.348)	Loss 0.1085 (0.1085)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [152][64/196]	Time 0.087 (0.090)	Data 0.000 (0.006)	Loss 0.1347 (0.1031)	Acc@1 96.094 (96.689)	Acc@5 99.609 (99.958)
Epoch: [152][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.1216 (0.1048)	Acc@1 95.703 (96.633)	Acc@5 100.000 (99.955)
Epoch: [152][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.1619 (0.1044)	Acc@1 93.359 (96.590)	Acc@5 100.000 (99.968)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:153/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [153][0/196]	Time 0.122 (0.122)	Data 0.301 (0.301)	Loss 0.1050 (0.1050)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [153][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.1314 (0.1055)	Acc@1 96.094 (96.544)	Acc@5 100.000 (99.976)
Epoch: [153][128/196]	Time 0.095 (0.088)	Data 0.000 (0.003)	Loss 0.1145 (0.1032)	Acc@1 97.266 (96.672)	Acc@5 99.609 (99.955)
Epoch: [153][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.1074 (0.1019)	Acc@1 96.484 (96.721)	Acc@5 100.000 (99.960)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:154/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [154][0/196]	Time 0.141 (0.141)	Data 0.283 (0.283)	Loss 0.0787 (0.0787)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [154][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.1310 (0.0990)	Acc@1 95.703 (96.791)	Acc@5 100.000 (99.964)
Epoch: [154][128/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.1018 (0.0996)	Acc@1 97.656 (96.718)	Acc@5 99.609 (99.958)
Epoch: [154][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.1483 (0.0997)	Acc@1 94.141 (96.721)	Acc@5 100.000 (99.968)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:155/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [155][0/196]	Time 0.130 (0.130)	Data 0.299 (0.299)	Loss 0.0711 (0.0711)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [155][64/196]	Time 0.098 (0.088)	Data 0.000 (0.005)	Loss 0.1368 (0.0973)	Acc@1 94.141 (96.731)	Acc@5 100.000 (99.976)
Epoch: [155][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.0834 (0.0969)	Acc@1 97.656 (96.805)	Acc@5 99.609 (99.976)
Epoch: [155][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.1174 (0.0965)	Acc@1 96.484 (96.832)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.39
Max memory: 51.4381312
 17.551s  j: 156 bis 160
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1748
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 156
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:156/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [156][0/196]	Time 0.155 (0.155)	Data 0.305 (0.305)	Loss 0.1101 (0.1101)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [156][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.1244 (0.0944)	Acc@1 95.703 (96.953)	Acc@5 100.000 (99.952)
Epoch: [156][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.1055 (0.0929)	Acc@1 97.656 (96.999)	Acc@5 99.609 (99.945)
Epoch: [156][192/196]	Time 0.078 (0.087)	Data 0.000 (0.002)	Loss 0.0705 (0.0959)	Acc@1 98.047 (96.913)	Acc@5 100.000 (99.951)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:157/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [157][0/196]	Time 0.121 (0.121)	Data 0.319 (0.319)	Loss 0.0577 (0.0577)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [157][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0999 (0.0925)	Acc@1 96.484 (97.013)	Acc@5 100.000 (99.952)
Epoch: [157][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.1337 (0.0933)	Acc@1 94.922 (96.905)	Acc@5 100.000 (99.964)
Epoch: [157][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0722 (0.0947)	Acc@1 98.438 (96.920)	Acc@5 100.000 (99.962)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:158/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [158][0/196]	Time 0.129 (0.129)	Data 0.313 (0.313)	Loss 0.0815 (0.0815)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [158][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0983 (0.0889)	Acc@1 96.875 (97.121)	Acc@5 100.000 (99.988)
Epoch: [158][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0958 (0.0893)	Acc@1 97.266 (97.078)	Acc@5 100.000 (99.967)
Epoch: [158][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.1122 (0.0909)	Acc@1 98.047 (96.984)	Acc@5 100.000 (99.972)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:159/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [159][0/196]	Time 0.121 (0.121)	Data 0.298 (0.298)	Loss 0.0995 (0.0995)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [159][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0684 (0.0881)	Acc@1 98.047 (97.151)	Acc@5 100.000 (99.976)
Epoch: [159][128/196]	Time 0.083 (0.089)	Data 0.000 (0.003)	Loss 0.1425 (0.0889)	Acc@1 94.141 (97.157)	Acc@5 100.000 (99.973)
Epoch: [159][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0967 (0.0914)	Acc@1 96.875 (97.067)	Acc@5 100.000 (99.957)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:160/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [160][0/196]	Time 0.137 (0.137)	Data 0.299 (0.299)	Loss 0.0663 (0.0663)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [160][64/196]	Time 0.096 (0.089)	Data 0.000 (0.005)	Loss 0.0851 (0.0888)	Acc@1 96.484 (97.043)	Acc@5 100.000 (99.952)
Epoch: [160][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0800 (0.0878)	Acc@1 97.656 (97.163)	Acc@5 100.000 (99.961)
Epoch: [160][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0713 (0.0910)	Acc@1 99.219 (97.043)	Acc@5 100.000 (99.955)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.36
Max memory: 51.4381312
 17.451s  j: 161 bis 165
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5717
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 161
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:161/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [161][0/196]	Time 0.151 (0.151)	Data 0.262 (0.262)	Loss 0.1221 (0.1221)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [161][64/196]	Time 0.083 (0.088)	Data 0.000 (0.004)	Loss 0.0777 (0.0858)	Acc@1 97.266 (97.326)	Acc@5 100.000 (99.964)
Epoch: [161][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0726 (0.0882)	Acc@1 98.047 (97.132)	Acc@5 100.000 (99.967)
Epoch: [161][192/196]	Time 0.079 (0.088)	Data 0.000 (0.002)	Loss 0.0991 (0.0889)	Acc@1 97.656 (97.073)	Acc@5 100.000 (99.955)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:162/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [162][0/196]	Time 0.126 (0.126)	Data 0.269 (0.269)	Loss 0.0807 (0.0807)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [162][64/196]	Time 0.088 (0.089)	Data 0.000 (0.004)	Loss 0.1471 (0.0896)	Acc@1 93.359 (97.055)	Acc@5 99.219 (99.946)
Epoch: [162][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0889 (0.0895)	Acc@1 97.266 (97.045)	Acc@5 100.000 (99.961)
Epoch: [162][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.0525 (0.0894)	Acc@1 98.828 (97.029)	Acc@5 100.000 (99.964)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:163/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [163][0/196]	Time 0.116 (0.116)	Data 0.335 (0.335)	Loss 0.0539 (0.0539)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [163][64/196]	Time 0.090 (0.089)	Data 0.000 (0.005)	Loss 0.1096 (0.0886)	Acc@1 95.703 (97.145)	Acc@5 100.000 (99.958)
Epoch: [163][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0836 (0.0889)	Acc@1 97.656 (97.160)	Acc@5 100.000 (99.958)
Epoch: [163][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0833 (0.0886)	Acc@1 97.656 (97.164)	Acc@5 100.000 (99.955)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:164/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [164][0/196]	Time 0.115 (0.115)	Data 0.316 (0.316)	Loss 0.0911 (0.0911)	Acc@1 97.656 (97.656)	Acc@5 99.609 (99.609)
Epoch: [164][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.0664 (0.0825)	Acc@1 97.656 (97.284)	Acc@5 100.000 (99.976)
Epoch: [164][128/196]	Time 0.093 (0.089)	Data 0.000 (0.003)	Loss 0.0841 (0.0840)	Acc@1 96.875 (97.284)	Acc@5 100.000 (99.973)
Epoch: [164][192/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.1205 (0.0858)	Acc@1 96.094 (97.191)	Acc@5 100.000 (99.970)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:165/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [165][0/196]	Time 0.120 (0.120)	Data 0.349 (0.349)	Loss 0.0806 (0.0806)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [165][64/196]	Time 0.089 (0.089)	Data 0.000 (0.006)	Loss 0.0550 (0.0883)	Acc@1 98.828 (97.230)	Acc@5 100.000 (99.964)
Epoch: [165][128/196]	Time 0.091 (0.089)	Data 0.000 (0.003)	Loss 0.0979 (0.0893)	Acc@1 95.703 (97.087)	Acc@5 100.000 (99.964)
Epoch: [165][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0708 (0.0890)	Acc@1 97.656 (97.081)	Acc@5 100.000 (99.960)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.16
Max memory: 51.4381312
 17.669s  j: 166 bis 170
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7520
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 166
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:166/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [166][0/196]	Time 0.172 (0.172)	Data 0.269 (0.269)	Loss 0.0778 (0.0778)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [166][64/196]	Time 0.090 (0.089)	Data 0.000 (0.004)	Loss 0.1461 (0.0842)	Acc@1 94.531 (97.169)	Acc@5 99.609 (99.940)
Epoch: [166][128/196]	Time 0.099 (0.088)	Data 0.000 (0.002)	Loss 0.0838 (0.0829)	Acc@1 97.656 (97.290)	Acc@5 100.000 (99.961)
Epoch: [166][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0593 (0.0845)	Acc@1 99.219 (97.292)	Acc@5 100.000 (99.960)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:167/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [167][0/196]	Time 0.142 (0.142)	Data 0.265 (0.265)	Loss 0.0772 (0.0772)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [167][64/196]	Time 0.083 (0.089)	Data 0.000 (0.004)	Loss 0.1066 (0.0869)	Acc@1 96.094 (97.188)	Acc@5 100.000 (99.970)
Epoch: [167][128/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.1112 (0.0852)	Acc@1 96.094 (97.154)	Acc@5 100.000 (99.970)
Epoch: [167][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.1127 (0.0857)	Acc@1 96.094 (97.193)	Acc@5 100.000 (99.960)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:168/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [168][0/196]	Time 0.122 (0.122)	Data 0.279 (0.279)	Loss 0.0946 (0.0946)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [168][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.0891 (0.0812)	Acc@1 96.875 (97.446)	Acc@5 100.000 (99.964)
Epoch: [168][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0782 (0.0840)	Acc@1 97.266 (97.278)	Acc@5 100.000 (99.958)
Epoch: [168][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.1101 (0.0847)	Acc@1 96.094 (97.284)	Acc@5 100.000 (99.955)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:169/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [169][0/196]	Time 0.124 (0.124)	Data 0.315 (0.315)	Loss 0.0648 (0.0648)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [169][64/196]	Time 0.093 (0.088)	Data 0.000 (0.005)	Loss 0.1305 (0.0823)	Acc@1 96.094 (97.356)	Acc@5 99.609 (99.982)
Epoch: [169][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.0864 (0.0836)	Acc@1 96.875 (97.326)	Acc@5 100.000 (99.982)
Epoch: [169][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0732 (0.0835)	Acc@1 97.266 (97.298)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:170/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [170][0/196]	Time 0.124 (0.124)	Data 0.300 (0.300)	Loss 0.0488 (0.0488)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [170][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.0790 (0.0831)	Acc@1 96.875 (97.338)	Acc@5 100.000 (99.976)
Epoch: [170][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.0757 (0.0822)	Acc@1 96.875 (97.317)	Acc@5 100.000 (99.979)
Epoch: [170][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0544 (0.0824)	Acc@1 98.438 (97.332)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.44
Max memory: 51.4381312
 17.380s  j: 171 bis 175
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7663
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 171
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:171/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [171][0/196]	Time 0.159 (0.159)	Data 0.275 (0.275)	Loss 0.0975 (0.0975)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [171][64/196]	Time 0.089 (0.090)	Data 0.000 (0.004)	Loss 0.0733 (0.0877)	Acc@1 97.266 (97.049)	Acc@5 100.000 (99.952)
Epoch: [171][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0698 (0.0871)	Acc@1 97.266 (97.154)	Acc@5 100.000 (99.967)
Epoch: [171][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0616 (0.0849)	Acc@1 98.047 (97.245)	Acc@5 100.000 (99.970)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:172/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [172][0/196]	Time 0.118 (0.118)	Data 0.303 (0.303)	Loss 0.0824 (0.0824)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [172][64/196]	Time 0.091 (0.090)	Data 0.000 (0.005)	Loss 0.1081 (0.0831)	Acc@1 96.094 (97.314)	Acc@5 100.000 (99.982)
Epoch: [172][128/196]	Time 0.078 (0.089)	Data 0.000 (0.003)	Loss 0.0831 (0.0825)	Acc@1 97.266 (97.296)	Acc@5 100.000 (99.979)
Epoch: [172][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0576 (0.0818)	Acc@1 98.438 (97.322)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:173/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [173][0/196]	Time 0.124 (0.124)	Data 0.270 (0.270)	Loss 0.0662 (0.0662)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [173][64/196]	Time 0.089 (0.090)	Data 0.000 (0.004)	Loss 0.0692 (0.0800)	Acc@1 98.438 (97.470)	Acc@5 100.000 (99.970)
Epoch: [173][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1109 (0.0807)	Acc@1 95.703 (97.393)	Acc@5 99.609 (99.973)
Epoch: [173][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0748 (0.0808)	Acc@1 97.656 (97.385)	Acc@5 100.000 (99.968)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:174/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [174][0/196]	Time 0.124 (0.124)	Data 0.288 (0.288)	Loss 0.0961 (0.0961)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [174][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.1153 (0.0809)	Acc@1 95.703 (97.404)	Acc@5 100.000 (99.964)
Epoch: [174][128/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.1035 (0.0800)	Acc@1 98.047 (97.420)	Acc@5 99.609 (99.970)
Epoch: [174][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0759 (0.0796)	Acc@1 98.047 (97.466)	Acc@5 100.000 (99.972)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:175/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [175][0/196]	Time 0.111 (0.111)	Data 0.300 (0.300)	Loss 0.1347 (0.1347)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [175][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.1547 (0.0798)	Acc@1 95.703 (97.440)	Acc@5 100.000 (99.970)
Epoch: [175][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.0755 (0.0811)	Acc@1 97.266 (97.356)	Acc@5 100.000 (99.979)
Epoch: [175][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0983 (0.0816)	Acc@1 96.875 (97.330)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.27
Max memory: 51.4381312
 17.412s  j: 176 bis 180
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7446
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 176
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:176/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [176][0/196]	Time 0.133 (0.133)	Data 0.296 (0.296)	Loss 0.1614 (0.1614)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [176][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.0889 (0.0750)	Acc@1 96.875 (97.698)	Acc@5 100.000 (99.994)
Epoch: [176][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0753 (0.0785)	Acc@1 96.875 (97.456)	Acc@5 100.000 (99.982)
Epoch: [176][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0636 (0.0801)	Acc@1 98.047 (97.389)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:177/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [177][0/196]	Time 0.139 (0.139)	Data 0.286 (0.286)	Loss 0.0715 (0.0715)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [177][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.0378 (0.0774)	Acc@1 99.219 (97.476)	Acc@5 100.000 (99.970)
Epoch: [177][128/196]	Time 0.097 (0.088)	Data 0.000 (0.002)	Loss 0.0942 (0.0766)	Acc@1 96.484 (97.487)	Acc@5 100.000 (99.976)
Epoch: [177][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0703 (0.0802)	Acc@1 98.047 (97.381)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:178/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [178][0/196]	Time 0.124 (0.124)	Data 0.286 (0.286)	Loss 0.0690 (0.0690)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [178][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0883 (0.0794)	Acc@1 96.875 (97.362)	Acc@5 100.000 (99.976)
Epoch: [178][128/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0727 (0.0782)	Acc@1 97.656 (97.462)	Acc@5 100.000 (99.976)
Epoch: [178][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0618 (0.0786)	Acc@1 98.828 (97.460)	Acc@5 99.609 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:179/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [179][0/196]	Time 0.126 (0.126)	Data 0.319 (0.319)	Loss 0.0761 (0.0761)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [179][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0541 (0.0764)	Acc@1 99.219 (97.560)	Acc@5 100.000 (99.982)
Epoch: [179][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0834 (0.0804)	Acc@1 96.484 (97.384)	Acc@5 100.000 (99.976)
Epoch: [179][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0776 (0.0797)	Acc@1 97.266 (97.405)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:180/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [180][0/196]	Time 0.137 (0.137)	Data 0.285 (0.285)	Loss 0.0620 (0.0620)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [180][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0893 (0.0766)	Acc@1 97.266 (97.518)	Acc@5 100.000 (99.982)
Epoch: [180][128/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0677 (0.0785)	Acc@1 98.047 (97.462)	Acc@5 100.000 (99.958)
Epoch: [180][192/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.1306 (0.0787)	Acc@1 94.922 (97.440)	Acc@5 100.000 (99.968)
Max memory in training epoch: 33.3541888
Stage: 3
width of Layers: [8, 16, 32]
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 32
stage: 3; tobestage: 3
Num: 23
width: 32
stage: 3; tobestage: 3
Num: 24
width: 32
stage: 3; tobestage: 3
Num: 25
width: 32
stage: 3; tobestage: 3
Num: 26
width: 32
stage: 3; tobestage: 3
Num: 27
width: 32
stage: 3; tobestage: 3
Num: 28
width: 32
stage: 3; tobestage: 3
Num: 29
width: 32
stage: 3; tobestage: 3
Num: 30
width: 32
stage: 3; tobestage: 3
Num: 31
width: 32
stage: 3; tobestage: 3
Num: 32
width: 32
stage: 3; tobestage: 3
Num: 33
altList: ['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']
Residual ListI: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
Residual ListO: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
j: 23
Resiudual I
new width: 32; old width: 16
Residual O
old width1: 32; new width: 64
c:[[[ 2.56719105e-02  6.96685677e-03 -4.74904180e-02]
  [-9.63438153e-02 -7.43770450e-02 -9.46023017e-02]
  [-9.83627290e-02 -5.83105609e-02 -4.09301184e-02]]

 [[ 1.27612252e-03 -1.88882425e-02 -2.40730960e-03]
  [-2.22753547e-02 -4.26219292e-02 -1.47123942e-02]
  [-2.51251040e-03 -6.86452612e-02 -3.09819393e-02]]

 [[ 1.61540937e-02  2.05242615e-02  3.56477988e-03]
  [-2.03369912e-02 -2.29959954e-02 -6.45008907e-02]
  [ 3.21533084e-02  8.72580148e-03 -2.45237183e-02]]

 [[ 4.61803675e-02  8.25534202e-03  1.10124471e-03]
  [ 1.26046753e-02 -2.05424838e-02 -1.19743496e-02]
  [ 4.02936451e-02 -4.19378746e-03 -1.03867456e-01]]

 [[ 4.07801531e-02 -1.12694642e-02 -2.57833321e-02]
  [-5.10376990e-02 -4.74149436e-02  1.07863778e-02]
  [-4.14838679e-02  9.83569422e-04  9.33122933e-02]]

 [[ 2.27522082e-03 -7.17640892e-02 -1.29421026e-01]
  [-3.33357677e-02 -1.43696621e-01 -1.25545606e-01]
  [-4.14526723e-02 -7.65077993e-02 -1.67364292e-02]]

 [[ 1.48120336e-02 -4.00185734e-02 -5.08064516e-02]
  [-2.80124620e-02 -4.27792668e-02 -8.32424685e-02]
  [-9.69277695e-03 -9.17151105e-03 -5.01056239e-02]]

 [[ 2.95789856e-02  4.64756116e-02  5.23594916e-02]
  [ 4.13592719e-02  2.43236218e-02  5.42547628e-02]
  [-8.92265979e-03  3.80972936e-03  1.42144887e-02]]

 [[ 1.54766710e-02  1.62415393e-02 -1.64211262e-02]
  [ 1.11894786e-01  8.59164968e-02  1.60468221e-02]
  [ 1.06234096e-01  5.17469570e-02  4.34849411e-02]]

 [[-8.14637020e-02 -1.38610229e-01 -1.19540118e-01]
  [-1.89941242e-01 -1.71507686e-01 -1.08110987e-01]
  [-9.93244797e-02 -9.24734771e-02  2.67682094e-02]]

 [[-3.81741039e-02 -1.70196535e-03 -3.29866586e-03]
  [ 1.39206108e-02  5.28484285e-02  8.22060034e-02]
  [ 2.34295726e-02  7.95941427e-02  7.20245913e-02]]

 [[-3.78183424e-02  1.49851842e-02 -1.49176712e-03]
  [-6.56295791e-02 -4.72822785e-02 -5.78660443e-02]
  [-4.71109822e-02 -3.98219302e-02 -8.10777768e-02]]

 [[ 1.20584136e-02  3.21695507e-02 -4.17016819e-03]
  [ 3.34066823e-02  4.53319512e-02 -3.00426912e-02]
  [ 2.63726152e-02  3.67531031e-02 -7.12404549e-02]]

 [[ 6.93542138e-03 -6.52161166e-02 -3.34309489e-02]
  [-9.42287669e-02 -1.69515640e-01 -1.09246366e-01]
  [-6.94601387e-02 -1.02570631e-01 -7.10981190e-02]]

 [[-2.37213615e-02 -1.00087166e-01 -7.27868825e-02]
  [-4.19953605e-03 -6.34422526e-02 -5.03339469e-02]
  [ 2.95403264e-02 -8.97829793e-03  1.28798177e-02]]

 [[ 1.66881774e-02  3.58946994e-02  1.54308854e-02]
  [-5.19834459e-03 -3.59982671e-03 -2.76305550e-03]
  [-1.44377472e-02 -3.81492600e-02 -2.00146995e-02]]

 [[ 7.10213732e-04 -7.00749308e-02 -3.40483934e-02]
  [-9.14797857e-02 -1.66696191e-01 -1.12378940e-01]
  [-7.41521046e-02 -9.95431915e-02 -6.58011138e-02]]

 [[ 1.22599006e-02 -3.89575511e-02 -5.28684780e-02]
  [-3.30551639e-02 -5.03626540e-02 -8.25522020e-02]
  [-7.99440220e-03 -7.20855128e-03 -4.63729315e-02]]

 [[ 4.57824841e-02 -1.65860204e-03 -2.00575292e-02]
  [-4.96169142e-02 -4.25084271e-02  1.07206041e-02]
  [-3.90783101e-02 -3.31654865e-03  9.32038128e-02]]

 [[ 2.08485778e-02  1.94521174e-02  3.40973400e-03]
  [-1.71231050e-02 -1.83090698e-02 -6.81919083e-02]
  [ 3.42355967e-02  8.73187557e-03 -3.21409814e-02]]

 [[ 2.54438203e-02  4.66894545e-02  5.16328104e-02]
  [ 3.36339809e-02  1.97309665e-02  5.62952012e-02]
  [-1.20505095e-02 -3.23992479e-03  1.50656952e-02]]

 [[-2.25491319e-02 -9.92303491e-02 -7.00375512e-02]
  [-3.91200325e-03 -6.53206706e-02 -5.30263372e-02]
  [ 2.32132003e-02 -1.12058213e-02  6.28467184e-03]]

 [[-8.64230155e-04 -7.62517676e-02 -1.26715153e-01]
  [-2.87063252e-02 -1.41913339e-01 -1.26287133e-01]
  [-4.12185155e-02 -8.06627646e-02 -1.95951033e-02]]

 [[ 7.21555809e-03 -7.34349936e-02 -1.26402065e-01]
  [-2.97346003e-02 -1.41254023e-01 -1.27057463e-01]
  [-3.82925048e-02 -7.91965574e-02 -9.76745319e-03]]

 [[-3.43652107e-02  4.30340879e-03 -1.73492823e-03]
  [ 1.21680619e-02  4.49740514e-02  8.44135806e-02]
  [ 1.84357762e-02  7.43249208e-02  7.82318860e-02]]

 [[-1.50354079e-03 -6.71508461e-02 -2.87442803e-02]
  [-9.06569064e-02 -1.65835232e-01 -1.09280519e-01]
  [-6.93589151e-02 -9.86433402e-02 -6.74541891e-02]]

 [[ 8.99564754e-03 -6.90725073e-02 -1.24713197e-01]
  [-2.86002662e-02 -1.43823549e-01 -1.27978265e-01]
  [-4.17148359e-02 -7.94380903e-02 -1.53694944e-02]]

 [[ 9.69905779e-03  3.45879383e-02 -7.51071423e-03]
  [ 3.40153240e-02  4.34696227e-02 -2.71284617e-02]
  [ 2.58382391e-02  3.65352668e-02 -7.02050924e-02]]

 [[ 3.69315312e-05 -2.12908816e-02 -4.35680820e-04]
  [-2.25379057e-02 -3.88152786e-02 -1.48923183e-02]
  [-5.97139681e-03 -6.78765103e-02 -3.19458805e-02]]

 [[ 1.95458923e-02  1.47021143e-02  3.34250531e-03]
  [-1.97768081e-02 -2.48390809e-02 -7.23445117e-02]
  [ 3.31826620e-02  7.47070741e-03 -3.25974710e-02]]

 [[ 9.15979967e-03  3.21569629e-02 -3.20638227e-03]
  [ 3.67759727e-02  4.30911891e-02 -2.14095674e-02]
  [ 3.06220818e-02  3.41807529e-02 -7.78984576e-02]]

 [[ 2.68932674e-02  4.93533611e-02  5.62141538e-02]
  [ 3.67377624e-02  2.08628643e-02  5.60597256e-02]
  [-8.26928299e-03 -9.49354563e-03  1.53134894e-02]]]
 after e[l]: [ 0.01283596  0.00348343 -0.02374521]
 after e[l]: [-0.04817191 -0.03718852 -0.04730115]
 after e[l]: [-0.04918136 -0.02915528 -0.02046506]
 after e[l]: [ 0.00063806 -0.00944412 -0.00120365]
 after e[l]: [-0.01113768 -0.02131096 -0.0073562 ]
 after e[l]: [-0.00125626 -0.03432263 -0.01549097]
 after e[l]: [0.00807705 0.01026213 0.00178239]
 after e[l]: [-0.0101685  -0.011498   -0.03225045]
 after e[l]: [ 0.01607665  0.0043629  -0.01226186]
 after e[l]: [0.02309018 0.00412767 0.00055062]
 after e[l]: [ 0.00630234 -0.01027124 -0.00598717]
 after e[l]: [ 0.02014682 -0.00209689 -0.05193373]
 after e[l]: [ 0.02039008 -0.00563473 -0.01289167]
 after e[l]: [-0.02551885 -0.02370747  0.00539319]
 after e[l]: [-0.02074193  0.00049178  0.04665615]
 after e[l]: [ 0.00113761 -0.03588204 -0.06471051]
 after e[l]: [-0.01666788 -0.07184831 -0.0627728 ]
 after e[l]: [-0.02072634 -0.0382539  -0.00836821]
 after e[l]: [ 0.00740602 -0.02000929 -0.02540323]
 after e[l]: [-0.01400623 -0.02138963 -0.04162123]
 after e[l]: [-0.00484639 -0.00458576 -0.02505281]
 after e[l]: [0.01478949 0.02323781 0.02617975]
 after e[l]: [0.02067964 0.01216181 0.02712738]
 after e[l]: [-0.00446133  0.00190486  0.00710724]
 after e[l]: [ 0.00773834  0.00812077 -0.00821056]
 after e[l]: [0.05594739 0.04295825 0.00802341]
 after e[l]: [0.05311705 0.02587348 0.02174247]
 after e[l]: [-0.04073185 -0.06930511 -0.05977006]
 after e[l]: [-0.09497062 -0.08575384 -0.05405549]
 after e[l]: [-0.04966224 -0.04623674  0.0133841 ]
 after e[l]: [-0.01908705 -0.00085098 -0.00164933]
 after e[l]: [0.00696031 0.02642421 0.041103  ]
 after e[l]: [0.01171479 0.03979707 0.0360123 ]
 after e[l]: [-0.01890917  0.00749259 -0.00074588]
 after e[l]: [-0.03281479 -0.02364114 -0.02893302]
 after e[l]: [-0.02355549 -0.01991097 -0.04053889]
 after e[l]: [ 0.00602921  0.01608478 -0.00208508]
 after e[l]: [ 0.01670334  0.02266598 -0.01502135]
 after e[l]: [ 0.01318631  0.01837655 -0.03562023]
 after e[l]: [ 0.00346771 -0.03260806 -0.01671547]
 after e[l]: [-0.04711438 -0.08475782 -0.05462318]
 after e[l]: [-0.03473007 -0.05128532 -0.03554906]
 after e[l]: [-0.01186068 -0.05004358 -0.03639344]
 after e[l]: [-0.00209977 -0.03172113 -0.02516697]
 after e[l]: [ 0.01477016 -0.00448915  0.00643991]
 after e[l]: [0.00834409 0.01794735 0.00771544]
 after e[l]: [-0.00259917 -0.00179991 -0.00138153]
 after e[l]: [-0.00721887 -0.01907463 -0.01000735]
 after e[l]: [ 0.00035511 -0.03503747 -0.0170242 ]
 after e[l]: [-0.04573989 -0.0833481  -0.05618947]
 after e[l]: [-0.03707605 -0.0497716  -0.03290056]
 after e[l]: [ 0.00612995 -0.01947878 -0.02643424]
 after e[l]: [-0.01652758 -0.02518133 -0.0412761 ]
 after e[l]: [-0.0039972  -0.00360428 -0.02318647]
 after e[l]: [ 0.02289124 -0.0008293  -0.01002876]
 after e[l]: [-0.02480846 -0.02125421  0.0053603 ]
 after e[l]: [-0.01953916 -0.00165827  0.04660191]
 after e[l]: [0.01042429 0.00972606 0.00170487]
 after e[l]: [-0.00856155 -0.00915453 -0.03409595]
 after e[l]: [ 0.0171178   0.00436594 -0.01607049]
 after e[l]: [0.01272191 0.02334473 0.02581641]
 after e[l]: [0.01681699 0.00986548 0.0281476 ]
 after e[l]: [-0.00602525 -0.00161996  0.00753285]
 after e[l]: [-0.01127457 -0.04961517 -0.03501878]
 after e[l]: [-0.001956   -0.03266034 -0.02651317]
 after e[l]: [ 0.0116066  -0.00560291  0.00314234]
 after e[l]: [-0.00043212 -0.03812588 -0.06335758]
 after e[l]: [-0.01435316 -0.07095667 -0.06314357]
 after e[l]: [-0.02060926 -0.04033138 -0.00979755]
 after e[l]: [ 0.00360778 -0.0367175  -0.06320103]
 after e[l]: [-0.0148673  -0.07062701 -0.06352873]
 after e[l]: [-0.01914625 -0.03959828 -0.00488373]
 after e[l]: [-0.01718261  0.0021517  -0.00086746]
 after e[l]: [0.00608403 0.02248703 0.04220679]
 after e[l]: [0.00921789 0.03716246 0.03911594]
 after e[l]: [-0.00075177 -0.03357542 -0.01437214]
 after e[l]: [-0.04532845 -0.08291762 -0.05464026]
 after e[l]: [-0.03467946 -0.04932167 -0.03372709]
 after e[l]: [ 0.00449782 -0.03453625 -0.0623566 ]
 after e[l]: [-0.01430013 -0.07191177 -0.06398913]
 after e[l]: [-0.02085742 -0.03971905 -0.00768475]
 after e[l]: [ 0.00484953  0.01729397 -0.00375536]
 after e[l]: [ 0.01700766  0.02173481 -0.01356423]
 after e[l]: [ 0.01291912  0.01826763 -0.03510255]
 after e[l]: [ 1.8465766e-05 -1.0645441e-02 -2.1784041e-04]
 after e[l]: [-0.01126895 -0.01940764 -0.00744616]
 after e[l]: [-0.0029857  -0.03393826 -0.01597294]
 after e[l]: [0.00977295 0.00735106 0.00167125]
 after e[l]: [-0.0098884  -0.01241954 -0.03617226]
 after e[l]: [ 0.01659133  0.00373535 -0.01629874]
 after e[l]: [ 0.0045799   0.01607848 -0.00160319]
 after e[l]: [ 0.01838799  0.02154559 -0.01070478]
 after e[l]: [ 0.01531104  0.01709038 -0.03894923]
 after e[l]: [0.01344663 0.02467668 0.02810708]
 after e[l]: [0.01836888 0.01043143 0.02802986]
 after e[l]: [-0.00413464 -0.00474677  0.00765674]
c:[[[-6.64296448e-02 -8.19405243e-02 -8.23667124e-02]
  [ 5.14135114e-04 -2.51502842e-02  1.09430384e-02]
  [ 2.85837129e-02 -2.01342274e-02  3.21359432e-04]]

 [[-4.54215184e-02 -4.22780812e-02  8.18626676e-03]
  [-4.72566970e-02 -5.22014908e-02  9.32661816e-03]
  [-4.42480445e-02 -7.46795014e-02 -3.21294144e-02]]

 [[-8.07262957e-02 -1.20915279e-01 -1.05999194e-01]
  [-5.98756447e-02 -7.84578472e-02 -6.69155344e-02]
  [-1.40852816e-02  3.81927602e-02  5.59199750e-02]]

 [[ 6.35046363e-02  4.08972614e-02  2.36425214e-02]
  [ 7.77418688e-02  5.48963714e-03  6.20780862e-04]
  [ 3.11495680e-02 -7.53174955e-03  4.12342288e-02]]

 [[ 6.15890548e-02  1.18009476e-02 -2.19671465e-02]
  [ 8.72258246e-02  1.04021780e-01  4.03859317e-02]
  [ 3.66514400e-02  6.28872737e-02  2.13196669e-02]]

 [[-8.65534768e-02 -2.28865333e-02 -3.38705890e-02]
  [-3.65420692e-02 -1.42057277e-02 -1.81323010e-02]
  [ 3.32957655e-02  1.01673417e-02  1.75587572e-02]]

 [[ 3.42188999e-02 -1.04356706e-02 -4.74505499e-02]
  [ 3.28110941e-02  3.27756144e-02 -4.74209525e-02]
  [ 7.08649829e-02  3.62650231e-02 -3.62574612e-03]]

 [[-1.64010953e-02 -4.28508520e-02  2.03485750e-02]
  [-1.50362626e-02 -5.46605252e-02 -1.23538002e-02]
  [-6.86316472e-03 -4.75127101e-02 -4.79561575e-02]]

 [[ 7.37950811e-03 -6.75593764e-02 -7.70275965e-02]
  [-4.10693996e-02 -1.04565971e-01 -5.87479547e-02]
  [-3.00808996e-02 -2.88565718e-02  7.61281550e-02]]

 [[-4.11293954e-02 -6.30257130e-02 -8.99847075e-02]
  [-3.35346088e-02 -3.00866701e-02 -7.09279105e-02]
  [ 7.02783093e-02  8.83935988e-02  2.15787292e-02]]

 [[ 6.96316212e-02  6.71322569e-02  2.96995156e-02]
  [ 2.45977808e-02 -1.59298088e-02  1.25871587e-03]
  [-2.05391292e-02 -3.81386578e-02 -1.56378392e-02]]

 [[-4.22163692e-04  3.68716493e-02  5.46320453e-02]
  [-5.43262623e-03  5.45522533e-02  9.00455639e-02]
  [-4.89607304e-02  1.07030543e-02  7.64591843e-02]]

 [[-5.92474863e-02 -1.57603100e-02 -1.61540341e-02]
  [-4.58411239e-02 -8.56856629e-02 -5.23350611e-02]
  [-6.91457614e-02 -7.99808875e-02 -3.97063754e-02]]

 [[-1.93876084e-02  6.73373640e-02  6.68424293e-02]
  [ 2.47136392e-02  4.80339378e-02  2.18499396e-02]
  [-1.35694150e-04  3.57476212e-02  2.30904259e-02]]

 [[ 7.67250806e-02  6.83675110e-02  5.81551716e-02]
  [ 7.28829056e-02  8.89528692e-02  8.11851546e-02]
  [ 2.91121211e-02  5.06176800e-02  1.99710801e-02]]

 [[ 4.81949672e-02 -1.04811173e-02 -4.76888902e-02]
  [ 1.12714916e-01  2.57007852e-02 -5.05718067e-02]
  [ 8.48717242e-02 -4.54122433e-03 -5.83070815e-02]]

 [[-1.34658674e-02  6.43807426e-02  6.49120584e-02]
  [ 2.94352490e-02  5.31997941e-02  2.02982333e-02]
  [-2.52203527e-03  3.51606272e-02  1.81486495e-02]]

 [[ 4.07013595e-02 -7.90721923e-03 -4.74313013e-02]
  [ 2.94120479e-02  3.13526578e-02 -4.67561595e-02]
  [ 6.35876805e-02  3.87174748e-02 -2.10605189e-03]]

 [[ 6.17980137e-02  1.61853507e-02 -1.58975516e-02]
  [ 8.42330307e-02  1.06885098e-01  3.13278139e-02]
  [ 4.25179191e-02  6.64696544e-02  1.72535162e-02]]

 [[-8.06975588e-02 -1.23853929e-01 -1.03199720e-01]
  [-5.63828424e-02 -6.73857257e-02 -6.57841638e-02]
  [-1.28157791e-02  3.07888109e-02  5.66798262e-02]]

 [[-9.97332856e-03 -3.59341986e-02  2.58374214e-02]
  [-1.39803104e-02 -5.75891212e-02 -1.01468489e-02]
  [-5.23652509e-03 -4.44840305e-02 -4.84151542e-02]]

 [[ 7.64068663e-02  6.75821006e-02  5.21661192e-02]
  [ 7.44957402e-02  9.26573351e-02  8.12681764e-02]
  [ 3.15898210e-02  5.04841618e-02  2.26231329e-02]]

 [[-9.09599215e-02 -2.16761921e-02 -3.20696793e-02]
  [-3.90396044e-02 -8.60408694e-03 -2.09343415e-02]
  [ 2.71861199e-02  1.19719198e-02  1.25094885e-02]]

 [[-8.88598338e-02 -1.83179062e-02 -3.16848196e-02]
  [-3.58736925e-02 -9.88038629e-03 -2.58394666e-02]
  [ 3.28555070e-02  1.30048431e-02  1.18158851e-02]]

 [[ 6.93015680e-02  6.26197457e-02  3.23237106e-02]
  [ 2.67305132e-02 -1.97496563e-02  2.19872035e-03]
  [-1.83353890e-02 -3.86701189e-02 -1.06022460e-02]]

 [[-2.05296669e-02  6.33887947e-02  5.86902127e-02]
  [ 2.38101184e-02  5.10240570e-02  2.17575580e-02]
  [-1.11905414e-04  3.81933264e-02  2.32819151e-02]]

 [[-8.65135193e-02 -2.52065230e-02 -3.01142726e-02]
  [-3.72661464e-02 -1.05598504e-02 -2.18201578e-02]
  [ 3.42363268e-02  1.52528090e-02  1.57498885e-02]]

 [[-6.14244156e-02 -2.08335184e-02 -1.10857878e-02]
  [-5.38526177e-02 -8.84840116e-02 -5.19725680e-02]
  [-6.59458488e-02 -7.98256025e-02 -3.79881114e-02]]

 [[-4.99072112e-02 -3.60506959e-02  1.07068503e-02]
  [-4.54891734e-02 -4.64825146e-02  5.89649379e-03]
  [-4.64367718e-02 -7.75241554e-02 -2.69220434e-02]]

 [[-8.22341144e-02 -1.18834622e-01 -9.93539989e-02]
  [-5.77947758e-02 -6.79943189e-02 -7.18447268e-02]
  [-1.83841866e-02  3.27000841e-02  5.44787571e-02]]

 [[-6.86680973e-02 -2.12931894e-02 -1.09508773e-02]
  [-5.17927296e-02 -8.16731900e-02 -4.81105410e-02]
  [-6.72253519e-02 -8.48981217e-02 -3.85934561e-02]]

 [[-1.77302584e-02 -4.63795587e-02  2.45138388e-02]
  [-1.67159196e-02 -5.30479439e-02 -1.52187664e-02]
  [-3.13942181e-03 -4.48226482e-02 -4.23566848e-02]]]
 after e[l]: [-0.06642964 -0.08194052 -0.08236671]
 after e[l]: [ 0.00051414 -0.02515028  0.01094304]
 after e[l]: [ 0.02858371 -0.02013423  0.00032136]
 after e[l]: [-0.04542152 -0.04227808  0.00818627]
 after e[l]: [-0.0472567  -0.05220149  0.00932662]
 after e[l]: [-0.04424804 -0.0746795  -0.03212941]
 after e[l]: [-0.0807263  -0.12091528 -0.10599919]
 after e[l]: [-0.05987564 -0.07845785 -0.06691553]
 after e[l]: [-0.01408528  0.03819276  0.05591998]
 after e[l]: [0.06350464 0.04089726 0.02364252]
 after e[l]: [0.07774187 0.00548964 0.00062078]
 after e[l]: [ 0.03114957 -0.00753175  0.04123423]
 after e[l]: [ 0.06158905  0.01180095 -0.02196715]
 after e[l]: [0.08722582 0.10402178 0.04038593]
 after e[l]: [0.03665144 0.06288727 0.02131967]
 after e[l]: [-0.08655348 -0.02288653 -0.03387059]
 after e[l]: [-0.03654207 -0.01420573 -0.0181323 ]
 after e[l]: [0.03329577 0.01016734 0.01755876]
 after e[l]: [ 0.0342189  -0.01043567 -0.04745055]
 after e[l]: [ 0.03281109  0.03277561 -0.04742095]
 after e[l]: [ 0.07086498  0.03626502 -0.00362575]
 after e[l]: [-0.0164011  -0.04285085  0.02034857]
 after e[l]: [-0.01503626 -0.05466053 -0.0123538 ]
 after e[l]: [-0.00686316 -0.04751271 -0.04795616]
 after e[l]: [ 0.00737951 -0.06755938 -0.0770276 ]
 after e[l]: [-0.0410694  -0.10456597 -0.05874795]
 after e[l]: [-0.0300809  -0.02885657  0.07612815]
 after e[l]: [-0.0411294  -0.06302571 -0.08998471]
 after e[l]: [-0.03353461 -0.03008667 -0.07092791]
 after e[l]: [0.07027831 0.0883936  0.02157873]
 after e[l]: [0.06963162 0.06713226 0.02969952]
 after e[l]: [ 0.02459778 -0.01592981  0.00125872]
 after e[l]: [-0.02053913 -0.03813866 -0.01563784]
 after e[l]: [-0.00042216  0.03687165  0.05463205]
 after e[l]: [-0.00543263  0.05455225  0.09004556]
 after e[l]: [-0.04896073  0.01070305  0.07645918]
 after e[l]: [-0.05924749 -0.01576031 -0.01615403]
 after e[l]: [-0.04584112 -0.08568566 -0.05233506]
 after e[l]: [-0.06914576 -0.07998089 -0.03970638]
 after e[l]: [-0.01938761  0.06733736  0.06684243]
 after e[l]: [0.02471364 0.04803394 0.02184994]
 after e[l]: [-0.00013569  0.03574762  0.02309043]
 after e[l]: [0.07672508 0.06836751 0.05815517]
 after e[l]: [0.07288291 0.08895287 0.08118515]
 after e[l]: [0.02911212 0.05061768 0.01997108]
 after e[l]: [ 0.04819497 -0.01048112 -0.04768889]
 after e[l]: [ 0.11271492  0.02570079 -0.05057181]
 after e[l]: [ 0.08487172 -0.00454122 -0.05830708]
 after e[l]: [-0.01346587  0.06438074  0.06491206]
 after e[l]: [0.02943525 0.05319979 0.02029823]
 after e[l]: [-0.00252204  0.03516063  0.01814865]
 after e[l]: [ 0.04070136 -0.00790722 -0.0474313 ]
 after e[l]: [ 0.02941205  0.03135266 -0.04675616]
 after e[l]: [ 0.06358768  0.03871747 -0.00210605]
 after e[l]: [ 0.06179801  0.01618535 -0.01589755]
 after e[l]: [0.08423303 0.1068851  0.03132781]
 after e[l]: [0.04251792 0.06646965 0.01725352]
 after e[l]: [-0.08069756 -0.12385393 -0.10319972]
 after e[l]: [-0.05638284 -0.06738573 -0.06578416]
 after e[l]: [-0.01281578  0.03078881  0.05667983]
 after e[l]: [-0.00997333 -0.0359342   0.02583742]
 after e[l]: [-0.01398031 -0.05758912 -0.01014685]
 after e[l]: [-0.00523653 -0.04448403 -0.04841515]
 after e[l]: [0.07640687 0.0675821  0.05216612]
 after e[l]: [0.07449574 0.09265734 0.08126818]
 after e[l]: [0.03158982 0.05048416 0.02262313]
 after e[l]: [-0.09095992 -0.02167619 -0.03206968]
 after e[l]: [-0.0390396  -0.00860409 -0.02093434]
 after e[l]: [0.02718612 0.01197192 0.01250949]
 after e[l]: [-0.08885983 -0.01831791 -0.03168482]
 after e[l]: [-0.03587369 -0.00988039 -0.02583947]
 after e[l]: [0.03285551 0.01300484 0.01181589]
 after e[l]: [0.06930157 0.06261975 0.03232371]
 after e[l]: [ 0.02673051 -0.01974966  0.00219872]
 after e[l]: [-0.01833539 -0.03867012 -0.01060225]
 after e[l]: [-0.02052967  0.06338879  0.05869021]
 after e[l]: [0.02381012 0.05102406 0.02175756]
 after e[l]: [-0.00011191  0.03819333  0.02328192]
 after e[l]: [-0.08651352 -0.02520652 -0.03011427]
 after e[l]: [-0.03726615 -0.01055985 -0.02182016]
 after e[l]: [0.03423633 0.01525281 0.01574989]
 after e[l]: [-0.06142442 -0.02083352 -0.01108579]
 after e[l]: [-0.05385262 -0.08848401 -0.05197257]
 after e[l]: [-0.06594585 -0.0798256  -0.03798811]
 after e[l]: [-0.04990721 -0.0360507   0.01070685]
 after e[l]: [-0.04548917 -0.04648251  0.00589649]
 after e[l]: [-0.04643677 -0.07752416 -0.02692204]
 after e[l]: [-0.08223411 -0.11883462 -0.099354  ]
 after e[l]: [-0.05779478 -0.06799432 -0.07184473]
 after e[l]: [-0.01838419  0.03270008  0.05447876]
 after e[l]: [-0.0686681  -0.02129319 -0.01095088]
 after e[l]: [-0.05179273 -0.08167319 -0.04811054]
 after e[l]: [-0.06722535 -0.08489812 -0.03859346]
 after e[l]: [-0.01773026 -0.04637956  0.02451384]
 after e[l]: [-0.01671592 -0.05304794 -0.01521877]
 after e[l]: [-0.00313942 -0.04482265 -0.04235668]
c:[[[-3.82391028e-02 -1.19360454e-01 -5.17870262e-02]
  [ 1.12869358e-02 -7.56419972e-02 -5.07623851e-02]
  [-1.51377926e-02 -7.15145767e-02 -2.97968499e-02]]

 [[-3.66089530e-02 -3.83871421e-02 -3.55996266e-02]
  [-4.62674610e-02 -6.34003058e-02 -6.30077720e-02]
  [-4.71349806e-02 -9.79952887e-02 -1.04127556e-01]]

 [[-2.18506549e-02 -2.46095732e-02 -3.61611508e-03]
  [-5.80794886e-02 -7.73924589e-02 -5.22174239e-02]
  [-9.35724080e-02 -4.86861691e-02 -2.67406870e-02]]

 [[ 1.37820449e-02  3.82736139e-02  3.29893618e-03]
  [-4.40533943e-02  2.59671221e-03  1.34782279e-02]
  [ 9.19794012e-03  4.00934033e-02  5.79430833e-02]]

 [[-2.58377977e-02 -1.37716746e-02  4.92235683e-02]
  [-3.15069668e-02 -4.06682715e-02  2.55013481e-02]
  [-2.44672894e-02 -4.88978513e-02 -7.59751231e-07]]

 [[ 6.52916878e-02  3.20720449e-02  4.55220090e-03]
  [ 7.42911249e-02  9.00005996e-02  1.44732576e-02]
  [ 3.62554495e-03  5.87754771e-02  3.15928645e-03]]

 [[-8.83316323e-02 -9.47866142e-02 -8.77953395e-02]
  [-3.88468653e-02 -4.18797135e-02 -4.83708717e-02]
  [ 3.68429795e-02  1.42791299e-02  1.05458675e-02]]

 [[ 1.48701193e-02  8.32082480e-02  6.46426976e-02]
  [-2.17819307e-02  7.49530420e-02  7.19323680e-02]
  [-1.10924253e-02  3.94976027e-02  7.18085989e-02]]

 [[-6.23655990e-02 -3.15374248e-02  4.03153384e-03]
  [-4.76809740e-02 -5.37346825e-02 -2.18028054e-02]
  [-1.26036163e-02  1.05804997e-02  4.99248467e-02]]

 [[-1.48494514e-02 -9.75237321e-03  2.95129213e-02]
  [-9.65627376e-03 -4.42226827e-02 -3.52181047e-02]
  [-1.76681522e-02 -5.11812270e-02 -4.81534470e-03]]

 [[-1.15528340e-02  4.39766757e-02  5.93048446e-02]
  [ 5.65740392e-02  1.11711308e-01  1.34914041e-01]
  [ 9.37961861e-02  5.46956323e-02  9.21331570e-02]]

 [[ 2.85645891e-02  3.58203277e-02  4.67462884e-03]
  [ 8.06147531e-02  1.16595156e-01  5.29723987e-02]
  [ 5.84772862e-02  1.04371518e-01  4.58131470e-02]]

 [[-7.68125504e-02 -9.90719348e-02 -2.50751190e-02]
  [-7.47711286e-02 -6.89785704e-02 -5.88599183e-02]
  [ 6.65077865e-02  4.54876833e-02  4.88570370e-02]]

 [[ 5.36292046e-02  2.64643179e-03  6.27205614e-03]
  [ 8.00113976e-02  3.85019891e-02  1.05891321e-02]
  [ 6.76819459e-02  9.37585831e-02  6.08122535e-02]]

 [[ 1.07179796e-02  2.39411555e-02  2.66147647e-02]
  [ 2.82516740e-02  5.09088188e-02  1.34554114e-02]
  [ 2.94676665e-02  1.41091226e-02 -2.51008430e-03]]

 [[-3.06099914e-02 -6.56358823e-02 -6.84245974e-02]
  [-6.02861494e-02 -5.43001480e-02 -6.70106038e-02]
  [-2.21202113e-02 -5.96601404e-02 -4.14352603e-02]]

 [[ 5.72075099e-02  7.28510902e-04  7.42620463e-03]
  [ 7.76249617e-02  3.02325264e-02  1.58383437e-02]
  [ 6.40371144e-02  9.51923802e-02  5.13797328e-02]]

 [[-9.88345966e-02 -9.57938284e-02 -8.84839743e-02]
  [-4.65116091e-02 -3.81518565e-02 -4.85235639e-02]
  [ 3.82462665e-02  1.42616956e-02  1.01894485e-02]]

 [[-2.22267862e-02 -1.35291032e-02  5.12050502e-02]
  [-3.05551477e-02 -3.66121121e-02  1.76596660e-02]
  [-1.97226070e-02 -4.88652289e-02 -1.23667461e-03]]

 [[-2.56949533e-02 -2.45513469e-02 -3.90314008e-03]
  [-6.01291507e-02 -7.87060261e-02 -5.29629700e-02]
  [-9.46570784e-02 -4.80068177e-02 -2.16362160e-02]]

 [[ 1.17766410e-02  8.30077827e-02  6.78739399e-02]
  [-1.68934707e-02  7.25604445e-02  7.24090859e-02]
  [-8.39505997e-03  4.05329652e-02  6.31770268e-02]]

 [[ 1.62036084e-02  2.03708597e-02  2.58046780e-02]
  [ 3.34589556e-02  5.18862419e-02  1.72164906e-02]
  [ 2.74004079e-02  1.53172603e-02  3.53233167e-03]]

 [[ 6.90190941e-02  3.50308008e-02 -2.97194324e-03]
  [ 7.48100281e-02  9.01060998e-02  1.59794502e-02]
  [ 1.81773480e-03  5.79379611e-02  6.95425225e-03]]

 [[ 6.59285411e-02  3.17595080e-02 -5.42193826e-04]
  [ 7.46523961e-02  8.90325978e-02  1.30757373e-02]
  [ 3.79144307e-03  6.34408519e-02  1.15792770e-02]]

 [[-6.44841045e-03  4.70868573e-02  6.08557723e-02]
  [ 4.65917364e-02  1.12249218e-01  1.36091322e-01]
  [ 8.88108611e-02  6.00397177e-02  8.85814652e-02]]

 [[ 6.16182685e-02  6.46576704e-03 -2.58385087e-03]
  [ 7.96205997e-02  3.80068123e-02  1.40125258e-02]
  [ 6.31442219e-02  9.57237035e-02  6.22604042e-02]]

 [[ 6.43256009e-02  3.49719636e-02  5.59638580e-03]
  [ 6.98688701e-02  9.26256329e-02  1.62859485e-02]
  [-3.69846326e-04  6.07819557e-02  9.54381749e-03]]

 [[-7.77598768e-02 -9.29273888e-02 -2.59296298e-02]
  [-6.98335618e-02 -6.49654940e-02 -5.62920421e-02]
  [ 6.76827133e-02  4.58608456e-02  4.60878201e-02]]

 [[-3.50746773e-02 -4.16086502e-02 -2.98985112e-02]
  [-4.40878682e-02 -6.61966577e-02 -5.94366454e-02]
  [-4.79687080e-02 -9.90576893e-02 -1.01809077e-01]]

 [[-3.32074277e-02 -2.51800325e-02 -3.60999606e-04]
  [-5.47564216e-02 -7.78574944e-02 -5.18609546e-02]
  [-9.84104946e-02 -5.26001677e-02 -2.34467052e-02]]

 [[-7.34121948e-02 -9.59461555e-02 -2.61309221e-02]
  [-6.75933436e-02 -6.17514700e-02 -5.72649129e-02]
  [ 6.73677176e-02  4.18480821e-02  5.12042791e-02]]

 [[ 1.23467818e-02  8.53830203e-02  6.79791495e-02]
  [-2.23622713e-02  7.51044825e-02  7.26153925e-02]
  [-1.37356222e-02  4.06681374e-02  6.07310906e-02]]]
 after e[l]: [-0.0382391  -0.11936045 -0.05178703]
 after e[l]: [ 0.01128694 -0.075642   -0.05076239]
 after e[l]: [-0.01513779 -0.07151458 -0.02979685]
 after e[l]: [-0.03660895 -0.03838714 -0.03559963]
 after e[l]: [-0.04626746 -0.06340031 -0.06300777]
 after e[l]: [-0.04713498 -0.09799529 -0.10412756]
 after e[l]: [-0.02185065 -0.02460957 -0.00361612]
 after e[l]: [-0.05807949 -0.07739246 -0.05221742]
 after e[l]: [-0.09357241 -0.04868617 -0.02674069]
 after e[l]: [0.01378204 0.03827361 0.00329894]
 after e[l]: [-0.04405339  0.00259671  0.01347823]
 after e[l]: [0.00919794 0.0400934  0.05794308]
 after e[l]: [-0.0258378  -0.01377167  0.04922357]
 after e[l]: [-0.03150697 -0.04066827  0.02550135]
 after e[l]: [-2.4467289e-02 -4.8897851e-02 -7.5975123e-07]
 after e[l]: [0.06529169 0.03207204 0.0045522 ]
 after e[l]: [0.07429112 0.0900006  0.01447326]
 after e[l]: [0.00362554 0.05877548 0.00315929]
 after e[l]: [-0.08833163 -0.09478661 -0.08779534]
 after e[l]: [-0.03884687 -0.04187971 -0.04837087]
 after e[l]: [0.03684298 0.01427913 0.01054587]
 after e[l]: [0.01487012 0.08320825 0.0646427 ]
 after e[l]: [-0.02178193  0.07495304  0.07193237]
 after e[l]: [-0.01109243  0.0394976   0.0718086 ]
 after e[l]: [-0.0623656  -0.03153742  0.00403153]
 after e[l]: [-0.04768097 -0.05373468 -0.02180281]
 after e[l]: [-0.01260362  0.0105805   0.04992485]
 after e[l]: [-0.01484945 -0.00975237  0.02951292]
 after e[l]: [-0.00965627 -0.04422268 -0.0352181 ]
 after e[l]: [-0.01766815 -0.05118123 -0.00481534]
 after e[l]: [-0.01155283  0.04397668  0.05930484]
 after e[l]: [0.05657404 0.11171131 0.13491404]
 after e[l]: [0.09379619 0.05469563 0.09213316]
 after e[l]: [0.02856459 0.03582033 0.00467463]
 after e[l]: [0.08061475 0.11659516 0.0529724 ]
 after e[l]: [0.05847729 0.10437152 0.04581315]
 after e[l]: [-0.07681255 -0.09907193 -0.02507512]
 after e[l]: [-0.07477113 -0.06897857 -0.05885992]
 after e[l]: [0.06650779 0.04548768 0.04885704]
 after e[l]: [0.0536292  0.00264643 0.00627206]
 after e[l]: [0.0800114  0.03850199 0.01058913]
 after e[l]: [0.06768195 0.09375858 0.06081225]
 after e[l]: [0.01071798 0.02394116 0.02661476]
 after e[l]: [0.02825167 0.05090882 0.01345541]
 after e[l]: [ 0.02946767  0.01410912 -0.00251008]
 after e[l]: [-0.03060999 -0.06563588 -0.0684246 ]
 after e[l]: [-0.06028615 -0.05430015 -0.0670106 ]
 after e[l]: [-0.02212021 -0.05966014 -0.04143526]
 after e[l]: [0.05720751 0.00072851 0.0074262 ]
 after e[l]: [0.07762496 0.03023253 0.01583834]
 after e[l]: [0.06403711 0.09519238 0.05137973]
 after e[l]: [-0.0988346  -0.09579383 -0.08848397]
 after e[l]: [-0.04651161 -0.03815186 -0.04852356]
 after e[l]: [0.03824627 0.0142617  0.01018945]
 after e[l]: [-0.02222679 -0.0135291   0.05120505]
 after e[l]: [-0.03055515 -0.03661211  0.01765967]
 after e[l]: [-0.01972261 -0.04886523 -0.00123667]
 after e[l]: [-0.02569495 -0.02455135 -0.00390314]
 after e[l]: [-0.06012915 -0.07870603 -0.05296297]
 after e[l]: [-0.09465708 -0.04800682 -0.02163622]
 after e[l]: [0.01177664 0.08300778 0.06787394]
 after e[l]: [-0.01689347  0.07256044  0.07240909]
 after e[l]: [-0.00839506  0.04053297  0.06317703]
 after e[l]: [0.01620361 0.02037086 0.02580468]
 after e[l]: [0.03345896 0.05188624 0.01721649]
 after e[l]: [0.02740041 0.01531726 0.00353233]
 after e[l]: [ 0.06901909  0.0350308  -0.00297194]
 after e[l]: [0.07481003 0.0901061  0.01597945]
 after e[l]: [0.00181773 0.05793796 0.00695425]
 after e[l]: [ 0.06592854  0.03175951 -0.00054219]
 after e[l]: [0.0746524  0.0890326  0.01307574]
 after e[l]: [0.00379144 0.06344085 0.01157928]
 after e[l]: [-0.00644841  0.04708686  0.06085577]
 after e[l]: [0.04659174 0.11224922 0.13609132]
 after e[l]: [0.08881086 0.06003972 0.08858147]
 after e[l]: [ 0.06161827  0.00646577 -0.00258385]
 after e[l]: [0.0796206  0.03800681 0.01401253]
 after e[l]: [0.06314422 0.0957237  0.0622604 ]
 after e[l]: [0.0643256  0.03497196 0.00559639]
 after e[l]: [0.06986887 0.09262563 0.01628595]
 after e[l]: [-0.00036985  0.06078196  0.00954382]
 after e[l]: [-0.07775988 -0.09292739 -0.02592963]
 after e[l]: [-0.06983356 -0.06496549 -0.05629204]
 after e[l]: [0.06768271 0.04586085 0.04608782]
 after e[l]: [-0.03507468 -0.04160865 -0.02989851]
 after e[l]: [-0.04408787 -0.06619666 -0.05943665]
 after e[l]: [-0.04796871 -0.09905769 -0.10180908]
 after e[l]: [-0.03320743 -0.02518003 -0.000361  ]
 after e[l]: [-0.05475642 -0.07785749 -0.05186095]
 after e[l]: [-0.09841049 -0.05260017 -0.02344671]
 after e[l]: [-0.07341219 -0.09594616 -0.02613092]
 after e[l]: [-0.06759334 -0.06175147 -0.05726491]
 after e[l]: [0.06736772 0.04184808 0.05120428]
 after e[l]: [0.01234678 0.08538302 0.06797915]
 after e[l]: [-0.02236227  0.07510448  0.07261539]
 after e[l]: [-0.01373562  0.04066814  0.06073109]
c:[[[-6.79736782e-04 -7.75956607e-04  2.62352079e-02]
  [ 1.95058770e-02 -6.24479093e-02 -1.55205699e-02]
  [ 3.43684643e-03 -7.63331354e-02 -6.01853505e-02]]

 [[ 8.60254914e-02 -7.46954465e-03 -1.38120621e-01]
  [ 1.15304925e-01 -2.11555548e-02 -1.67348295e-01]
  [ 5.07764816e-02 -4.25148457e-02 -1.32384911e-01]]

 [[-2.44922191e-02 -3.40261832e-02  1.87237971e-02]
  [-4.58381809e-02 -5.99067919e-02 -8.70929623e-04]
  [-5.22636920e-02 -5.49368970e-02  7.44810048e-03]]

 [[-2.54861210e-02  3.21974494e-02  3.78441997e-02]
  [ 2.53282115e-02  3.85781229e-02  2.35532746e-02]
  [ 1.93513725e-02  5.80318831e-02  9.48838703e-03]]

 [[ 5.36920549e-03  7.42358193e-02  4.72187661e-02]
  [ 3.87100391e-02  1.23698480e-01  1.07154876e-01]
  [-9.59252752e-03  9.96548161e-02  6.36196360e-02]]

 [[-8.69887918e-02 -8.27879459e-02 -2.67882203e-03]
  [-5.68855964e-02  2.70298775e-02  9.95750353e-02]
  [-5.51801994e-02  8.46814588e-02  1.25615597e-01]]

 [[ 9.14452411e-03  1.88212693e-02  5.51048759e-03]
  [ 4.42324579e-02  1.02973342e-01  7.88438097e-02]
  [ 4.14536372e-02  8.19855928e-02  6.78898096e-02]]

 [[-6.88360352e-03  1.52865434e-02  1.89146958e-02]
  [-3.15215252e-02 -6.91006240e-03  2.32380978e-03]
  [-4.25849222e-02  2.92831585e-02  2.33235434e-02]]

 [[ 4.62192632e-02  6.52452260e-02  4.99349162e-02]
  [ 8.71229713e-06 -1.99009422e-02 -5.78955486e-02]
  [-8.26338027e-03 -7.08023692e-03 -5.25927134e-02]]

 [[-8.13487694e-02  3.84874083e-02  2.27697510e-02]
  [-6.81118891e-02  5.03149442e-03  2.57752556e-02]
  [-3.60374078e-02 -2.13379841e-02 -1.42371692e-02]]

 [[ 5.92185035e-02 -1.13284995e-03  2.47712079e-02]
  [ 3.18573862e-02 -3.53983766e-03 -4.84122932e-02]
  [ 2.05995031e-02 -3.40728015e-02 -7.52620846e-02]]

 [[-2.84967311e-02 -4.72282432e-02 -3.06619573e-02]
  [-6.38181716e-03 -6.74774721e-02 -9.45030246e-03]
  [ 2.23259553e-02 -2.58370154e-02  2.19361857e-02]]

 [[ 6.66317344e-02  4.20884900e-02  8.98447260e-03]
  [ 6.74307644e-02  1.98876187e-02 -5.55316322e-02]
  [ 8.90410691e-02 -3.30566280e-02 -6.23195916e-02]]

 [[-1.02374569e-01 -5.59474491e-02 -2.26590130e-02]
  [-7.00420439e-02 -7.30806738e-02 -1.94251146e-02]
  [-2.68879160e-02 -2.15338543e-02 -5.52352751e-03]]

 [[-3.44644710e-02 -4.65130657e-02 -2.00137077e-03]
  [-4.64970917e-02 -1.16038164e-02  6.02606013e-02]
  [-4.60630171e-02  5.69929816e-02  5.07539809e-02]]

 [[ 6.11103959e-02  3.73511985e-02  4.02485766e-02]
  [ 6.25786707e-02  1.58909857e-02  1.47089204e-02]
  [-7.45034218e-03 -2.45412011e-02 -2.66359095e-02]]

 [[-1.04725406e-01 -5.30956723e-02 -1.75792351e-02]
  [-6.62941188e-02 -7.44486302e-02 -1.11563094e-02]
  [-2.98734587e-02 -1.54572893e-02 -9.88133252e-03]]

 [[ 7.39848288e-03  1.40777649e-02  7.98743218e-03]
  [ 4.63599637e-02  1.01421140e-01  7.60240853e-02]
  [ 4.37098518e-02  8.37842077e-02  6.60727397e-02]]

 [[ 3.15506244e-04  6.62323385e-02  5.00089191e-02]
  [ 3.38521339e-02  1.22582778e-01  1.07207946e-01]
  [-1.01887090e-02  1.03330486e-01  6.59628436e-02]]

 [[-2.56112050e-02 -3.70409563e-02  2.74187457e-02]
  [-4.88061495e-02 -6.11695461e-02  5.38802287e-03]
  [-5.11268936e-02 -5.12042530e-02  8.80146865e-03]]

 [[-2.65042041e-03  1.36978179e-02  2.44238228e-02]
  [-3.00336387e-02 -7.99495913e-03  2.03164900e-03]
  [-3.34670693e-02  2.75195185e-02  2.73620356e-02]]

 [[-3.25525701e-02 -4.33309227e-02  6.97701611e-03]
  [-4.55588289e-02 -1.25611490e-02  5.91411777e-02]
  [-3.91216725e-02  5.81662096e-02  5.10676205e-02]]

 [[-8.52702335e-02 -8.69818181e-02 -3.61812708e-04]
  [-5.66889904e-02  3.20216939e-02  9.84509960e-02]
  [-6.17862754e-02  8.37122276e-02  1.22282229e-01]]

 [[-8.42316374e-02 -8.48860145e-02 -9.46418568e-03]
  [-5.36899604e-02  3.52925733e-02  9.95518938e-02]
  [-5.60424253e-02  8.23037922e-02  1.28032640e-01]]

 [[ 6.25822470e-02 -7.32411630e-04  2.24535763e-02]
  [ 3.50496098e-02 -4.44217212e-03 -5.04972711e-02]
  [ 2.79171132e-02 -3.26783769e-02 -7.70426765e-02]]

 [[-9.99317095e-02 -5.50278276e-02 -1.48696238e-02]
  [-7.04657212e-02 -7.69884661e-02 -1.62207838e-02]
  [-2.89994255e-02 -1.75669231e-02 -3.56793450e-03]]

 [[-8.85467529e-02 -8.07058141e-02 -2.03648326e-03]
  [-5.80119677e-02  3.61731201e-02  9.75171253e-02]
  [-5.58766834e-02  8.41533989e-02  1.27384558e-01]]

 [[ 6.23158403e-02  3.53312902e-02  9.19136126e-03]
  [ 6.70251846e-02  1.88180860e-02 -5.34810722e-02]
  [ 8.29591006e-02 -3.43032777e-02 -5.76219074e-02]]

 [[ 8.81181210e-02 -6.77314727e-03 -1.37036160e-01]
  [ 1.06210805e-01 -2.56941449e-02 -1.67016864e-01]
  [ 4.25217971e-02 -4.68731299e-02 -1.34609893e-01]]

 [[-3.13481875e-02 -3.75040695e-02  1.93838347e-02]
  [-4.57633547e-02 -6.58339411e-02  8.86458065e-03]
  [-5.33646010e-02 -5.24458848e-02  8.95490311e-03]]

 [[ 6.93684444e-02  3.21963094e-02  1.02875875e-02]
  [ 6.99518025e-02  1.83303505e-02 -5.71524352e-02]
  [ 8.64003375e-02 -3.04471850e-02 -6.56872168e-02]]

 [[-3.08313873e-03  1.04047190e-02  1.32973529e-02]
  [-3.43552753e-02 -8.96284916e-03  5.47308521e-03]
  [-3.20162512e-02  2.68291961e-02  2.44932640e-02]]]
 after e[l]: [-0.00067974 -0.00077596  0.02623521]
 after e[l]: [ 0.01950588 -0.06244791 -0.01552057]
 after e[l]: [ 0.00343685 -0.07633314 -0.06018535]
 after e[l]: [ 0.08602549 -0.00746954 -0.13812062]
 after e[l]: [ 0.11530492 -0.02115555 -0.1673483 ]
 after e[l]: [ 0.05077648 -0.04251485 -0.13238491]
 after e[l]: [-0.02449222 -0.03402618  0.0187238 ]
 after e[l]: [-0.04583818 -0.05990679 -0.00087093]
 after e[l]: [-0.05226369 -0.0549369   0.0074481 ]
 after e[l]: [-0.02548612  0.03219745  0.0378442 ]
 after e[l]: [0.02532821 0.03857812 0.02355327]
 after e[l]: [0.01935137 0.05803188 0.00948839]
 after e[l]: [0.00536921 0.07423582 0.04721877]
 after e[l]: [0.03871004 0.12369848 0.10715488]
 after e[l]: [-0.00959253  0.09965482  0.06361964]
 after e[l]: [-0.08698879 -0.08278795 -0.00267882]
 after e[l]: [-0.0568856   0.02702988  0.09957504]
 after e[l]: [-0.0551802   0.08468146  0.1256156 ]
 after e[l]: [0.00914452 0.01882127 0.00551049]
 after e[l]: [0.04423246 0.10297334 0.07884381]
 after e[l]: [0.04145364 0.08198559 0.06788981]
 after e[l]: [-0.0068836   0.01528654  0.0189147 ]
 after e[l]: [-0.03152153 -0.00691006  0.00232381]
 after e[l]: [-0.04258492  0.02928316  0.02332354]
 after e[l]: [0.04621926 0.06524523 0.04993492]
 after e[l]: [ 8.7122971e-06 -1.9900942e-02 -5.7895549e-02]
 after e[l]: [-0.00826338 -0.00708024 -0.05259271]
 after e[l]: [-0.08134877  0.03848741  0.02276975]
 after e[l]: [-0.06811189  0.00503149  0.02577526]
 after e[l]: [-0.03603741 -0.02133798 -0.01423717]
 after e[l]: [ 0.0592185  -0.00113285  0.02477121]
 after e[l]: [ 0.03185739 -0.00353984 -0.04841229]
 after e[l]: [ 0.0205995  -0.0340728  -0.07526208]
 after e[l]: [-0.02849673 -0.04722824 -0.03066196]
 after e[l]: [-0.00638182 -0.06747747 -0.0094503 ]
 after e[l]: [ 0.02232596 -0.02583702  0.02193619]
 after e[l]: [0.06663173 0.04208849 0.00898447]
 after e[l]: [ 0.06743076  0.01988762 -0.05553163]
 after e[l]: [ 0.08904107 -0.03305663 -0.06231959]
 after e[l]: [-0.10237457 -0.05594745 -0.02265901]
 after e[l]: [-0.07004204 -0.07308067 -0.01942511]
 after e[l]: [-0.02688792 -0.02153385 -0.00552353]
 after e[l]: [-0.03446447 -0.04651307 -0.00200137]
 after e[l]: [-0.04649709 -0.01160382  0.0602606 ]
 after e[l]: [-0.04606302  0.05699298  0.05075398]
 after e[l]: [0.0611104  0.0373512  0.04024858]
 after e[l]: [0.06257867 0.01589099 0.01470892]
 after e[l]: [-0.00745034 -0.0245412  -0.02663591]
 after e[l]: [-0.10472541 -0.05309567 -0.01757924]
 after e[l]: [-0.06629412 -0.07444863 -0.01115631]
 after e[l]: [-0.02987346 -0.01545729 -0.00988133]
 after e[l]: [0.00739848 0.01407776 0.00798743]
 after e[l]: [0.04635996 0.10142114 0.07602409]
 after e[l]: [0.04370985 0.08378421 0.06607274]
 after e[l]: [0.00031551 0.06623234 0.05000892]
 after e[l]: [0.03385213 0.12258278 0.10720795]
 after e[l]: [-0.01018871  0.10333049  0.06596284]
 after e[l]: [-0.02561121 -0.03704096  0.02741875]
 after e[l]: [-0.04880615 -0.06116955  0.00538802]
 after e[l]: [-0.05112689 -0.05120425  0.00880147]
 after e[l]: [-0.00265042  0.01369782  0.02442382]
 after e[l]: [-0.03003364 -0.00799496  0.00203165]
 after e[l]: [-0.03346707  0.02751952  0.02736204]
 after e[l]: [-0.03255257 -0.04333092  0.00697702]
 after e[l]: [-0.04555883 -0.01256115  0.05914118]
 after e[l]: [-0.03912167  0.05816621  0.05106762]
 after e[l]: [-0.08527023 -0.08698182 -0.00036181]
 after e[l]: [-0.05668899  0.03202169  0.098451  ]
 after e[l]: [-0.06178628  0.08371223  0.12228223]
 after e[l]: [-0.08423164 -0.08488601 -0.00946419]
 after e[l]: [-0.05368996  0.03529257  0.09955189]
 after e[l]: [-0.05604243  0.08230379  0.12803264]
 after e[l]: [ 0.06258225 -0.00073241  0.02245358]
 after e[l]: [ 0.03504961 -0.00444217 -0.05049727]
 after e[l]: [ 0.02791711 -0.03267838 -0.07704268]
 after e[l]: [-0.09993171 -0.05502783 -0.01486962]
 after e[l]: [-0.07046572 -0.07698847 -0.01622078]
 after e[l]: [-0.02899943 -0.01756692 -0.00356793]
 after e[l]: [-0.08854675 -0.08070581 -0.00203648]
 after e[l]: [-0.05801197  0.03617312  0.09751713]
 after e[l]: [-0.05587668  0.0841534   0.12738456]
 after e[l]: [0.06231584 0.03533129 0.00919136]
 after e[l]: [ 0.06702518  0.01881809 -0.05348107]
 after e[l]: [ 0.0829591  -0.03430328 -0.05762191]
 after e[l]: [ 0.08811812 -0.00677315 -0.13703616]
 after e[l]: [ 0.10621081 -0.02569414 -0.16701686]
 after e[l]: [ 0.0425218  -0.04687313 -0.1346099 ]
 after e[l]: [-0.03134819 -0.03750407  0.01938383]
 after e[l]: [-0.04576335 -0.06583394  0.00886458]
 after e[l]: [-0.0533646  -0.05244588  0.0089549 ]
 after e[l]: [0.06936844 0.03219631 0.01028759]
 after e[l]: [ 0.0699518   0.01833035 -0.05715244]
 after e[l]: [ 0.08640034 -0.03044719 -0.06568722]
 after e[l]: [-0.00308314  0.01040472  0.01329735]
 after e[l]: [-0.03435528 -0.00896285  0.00547309]
 after e[l]: [-0.03201625  0.0268292   0.02449326]
c:[[[ 2.15954836e-02  4.50051725e-02 -1.73162073e-02]
  [ 4.34291810e-02  6.83001578e-02  3.83299589e-02]
  [ 6.12293370e-03  9.79847927e-03 -8.17411393e-03]]

 [[ 1.76916476e-02 -4.10271175e-02 -1.20549295e-02]
  [-8.23399201e-02 -7.77576715e-02 -1.04517145e-02]
  [-3.72625478e-02 -7.45885968e-02 -3.77163179e-02]]

 [[ 4.19596024e-03 -1.31106831e-03 -3.35819758e-02]
  [ 2.67723035e-02  7.33098760e-02  1.60860922e-02]
  [ 1.78562906e-02  7.57140070e-02  5.71487658e-02]]

 [[-2.72898991e-02 -1.09441287e-03 -3.50952111e-02]
  [-1.02880523e-02  4.17555533e-02  1.03256935e-02]
  [-3.08137015e-02  3.07072653e-03  1.05177611e-02]]

 [[ 3.14713530e-02  4.32419702e-02 -5.96127920e-02]
  [ 3.68970516e-03  5.32946624e-02  4.67122942e-02]
  [-7.00253919e-02 -3.52307200e-03  2.78954059e-02]]

 [[ 1.23604750e-02  3.82792018e-03  1.20594613e-01]
  [-7.67316744e-02 -3.29293869e-02  9.28256810e-02]
  [-7.31573701e-02 -1.38779432e-01  1.61758773e-02]]

 [[-4.07602824e-02  9.68871731e-03  3.30823138e-02]
  [-5.92899844e-02 -1.69457626e-02  1.67982113e-02]
  [-3.37265953e-02 -7.57079348e-02 -4.56617959e-02]]

 [[ 4.10782769e-02  3.48742120e-02  3.25296335e-02]
  [-6.63126470e-04  1.66243166e-02 -1.03347786e-02]
  [-7.33640343e-02 -4.79701534e-02 -7.82647431e-02]]

 [[ 7.22085834e-02  1.01009138e-01  3.67109179e-02]
  [-8.31216872e-02 -2.80785467e-02 -5.17130792e-02]
  [-8.27461109e-02 -9.30348933e-02 -6.20339587e-02]]

 [[-2.88367178e-02 -3.52038331e-02 -2.99648009e-02]
  [-5.84773906e-03 -3.94204408e-02 -6.64535910e-02]
  [ 3.09289675e-02  3.11626233e-02  3.41367684e-02]]

 [[-9.17615518e-02 -9.18383375e-02 -4.39803861e-02]
  [-1.00293517e-01 -1.60140023e-01 -9.39278454e-02]
  [ 6.61564320e-02 -3.16518135e-02 -6.53600916e-02]]

 [[ 1.51754925e-02  1.95601340e-02  6.54048100e-02]
  [ 1.30165461e-02 -7.14021223e-03  4.27256264e-02]
  [-1.54194497e-02 -1.90869048e-02 -3.71512920e-02]]

 [[-1.04295164e-01 -1.41218171e-01 -1.22324415e-01]
  [ 1.90299731e-02 -1.12662269e-02 -6.20210804e-02]
  [ 1.11206025e-01  1.39502928e-01  4.41551842e-02]]

 [[-1.35454545e-02  3.27424780e-02 -3.11471876e-02]
  [-8.34156200e-03  2.62139346e-02 -1.07160434e-02]
  [-3.77233699e-02 -3.60905044e-02 -1.50375031e-02]]

 [[ 2.12010853e-02  6.59360550e-03  1.42952707e-02]
  [ 2.34895069e-02 -3.37419217e-03 -4.22185101e-02]
  [-3.30916494e-02 -7.51416609e-02 -4.00397740e-02]]

 [[ 4.21524569e-02  8.22901204e-02 -1.65849298e-04]
  [-1.35584446e-02  3.48652937e-02 -3.83071674e-05]
  [-4.25245799e-02 -5.33633381e-02 -2.88499966e-02]]

 [[-4.28429153e-03  2.76924036e-02 -3.44837494e-02]
  [-8.93557351e-03  2.70796679e-02 -1.32785803e-02]
  [-4.00484726e-02 -3.47727537e-02 -1.86502356e-02]]

 [[-4.30382490e-02  1.59217566e-02  3.37757394e-02]
  [-5.88931777e-02 -2.00828314e-02  1.41972313e-02]
  [-3.74582298e-02 -7.40833953e-02 -3.87346111e-02]]

 [[ 2.85600368e-02  4.62832488e-02 -5.62120453e-02]
  [ 9.00534261e-03  5.36885262e-02  4.64351177e-02]
  [-6.75689876e-02 -2.30340112e-04  2.90303491e-02]]

 [[ 5.44282096e-03  2.57647410e-03 -4.07613292e-02]
  [ 3.61508057e-02  7.08772913e-02  9.58704669e-03]
  [ 8.91150162e-03  7.83312395e-02  6.05379939e-02]]

 [[ 4.06181663e-02  3.60700749e-02  2.80597229e-02]
  [ 2.62342626e-04  1.47293340e-02 -7.92567991e-03]
  [-7.74826482e-02 -4.70834784e-02 -7.99629167e-02]]

 [[ 2.24070381e-02  6.03658194e-03  1.56492777e-02]
  [ 2.23627128e-02 -2.42895517e-03 -3.92069258e-02]
  [-2.88519487e-02 -7.03266636e-02 -3.30963582e-02]]

 [[ 5.34828287e-03  1.71787723e-03  1.17367133e-01]
  [-7.37729371e-02 -3.50712724e-02  9.54729766e-02]
  [-7.69585297e-02 -1.42800808e-01  1.75304208e-02]]

 [[ 1.33982208e-02  1.13918353e-02  1.27733231e-01]
  [-7.24078566e-02 -3.54225412e-02  1.00743324e-01]
  [-7.35806525e-02 -1.39866605e-01  1.69429537e-02]]

 [[-8.53449032e-02 -9.26824287e-02 -4.61990461e-02]
  [-1.00797363e-01 -1.53060898e-01 -9.21510085e-02]
  [ 5.65101467e-02 -2.80736201e-02 -6.77553043e-02]]

 [[-1.42792612e-03  3.11980024e-02 -3.11238132e-02]
  [-7.55906058e-03  2.75742561e-02 -2.05263831e-02]
  [-4.03004438e-02 -3.43788043e-02 -1.74164902e-02]]

 [[ 1.54127898e-02  5.99224633e-03  1.20041870e-01]
  [-7.17997625e-02 -3.16068269e-02  9.20202434e-02]
  [-7.95957968e-02 -1.37744665e-01  1.40231056e-02]]

 [[-1.11032054e-01 -1.49578080e-01 -1.17183283e-01]
  [ 2.39631794e-02 -1.12124979e-02 -6.10231943e-02]
  [ 1.12833306e-01  1.42615840e-01  3.77403237e-02]]

 [[ 1.51687032e-02 -3.66170704e-02 -1.39254862e-02]
  [-7.59044960e-02 -8.28624219e-02 -8.10612086e-03]
  [-3.56512554e-02 -7.09691569e-02 -3.63799557e-02]]

 [[ 2.74844846e-04 -3.33171966e-03 -3.76205817e-02]
  [ 3.05446591e-02  6.91467747e-02  8.68491270e-03]
  [ 1.05665317e-02  7.87146464e-02  5.79339601e-02]]

 [[-1.06152572e-01 -1.42120093e-01 -1.23302475e-01]
  [ 2.14710161e-02 -1.12306057e-02 -6.14558198e-02]
  [ 1.17667392e-01  1.36508495e-01  4.51993383e-02]]

 [[ 4.11852822e-02  3.68084200e-02  3.65184322e-02]
  [-7.96801614e-05  1.69929843e-02 -1.71983999e-03]
  [-7.78061748e-02 -5.90415150e-02 -8.35316330e-02]]]
 after e[l]: [ 0.02159548  0.04500517 -0.01731621]
 after e[l]: [0.04342918 0.06830016 0.03832996]
 after e[l]: [ 0.00612293  0.00979848 -0.00817411]
 after e[l]: [ 0.01769165 -0.04102712 -0.01205493]
 after e[l]: [-0.08233992 -0.07775767 -0.01045171]
 after e[l]: [-0.03726255 -0.0745886  -0.03771632]
 after e[l]: [ 0.00419596 -0.00131107 -0.03358198]
 after e[l]: [0.0267723  0.07330988 0.01608609]
 after e[l]: [0.01785629 0.07571401 0.05714877]
 after e[l]: [-0.0272899  -0.00109441 -0.03509521]
 after e[l]: [-0.01028805  0.04175555  0.01032569]
 after e[l]: [-0.0308137   0.00307073  0.01051776]
 after e[l]: [ 0.03147135  0.04324197 -0.05961279]
 after e[l]: [0.00368971 0.05329466 0.04671229]
 after e[l]: [-0.07002539 -0.00352307  0.02789541]
 after e[l]: [0.01236048 0.00382792 0.12059461]
 after e[l]: [-0.07673167 -0.03292939  0.09282568]
 after e[l]: [-0.07315737 -0.13877943  0.01617588]
 after e[l]: [-0.04076028  0.00968872  0.03308231]
 after e[l]: [-0.05928998 -0.01694576  0.01679821]
 after e[l]: [-0.0337266  -0.07570793 -0.0456618 ]
 after e[l]: [0.04107828 0.03487421 0.03252963]
 after e[l]: [-0.00066313  0.01662432 -0.01033478]
 after e[l]: [-0.07336403 -0.04797015 -0.07826474]
 after e[l]: [0.07220858 0.10100914 0.03671092]
 after e[l]: [-0.08312169 -0.02807855 -0.05171308]
 after e[l]: [-0.08274611 -0.09303489 -0.06203396]
 after e[l]: [-0.02883672 -0.03520383 -0.0299648 ]
 after e[l]: [-0.00584774 -0.03942044 -0.06645359]
 after e[l]: [0.03092897 0.03116262 0.03413677]
 after e[l]: [-0.09176155 -0.09183834 -0.04398039]
 after e[l]: [-0.10029352 -0.16014002 -0.09392785]
 after e[l]: [ 0.06615643 -0.03165181 -0.06536009]
 after e[l]: [0.01517549 0.01956013 0.06540481]
 after e[l]: [ 0.01301655 -0.00714021  0.04272563]
 after e[l]: [-0.01541945 -0.0190869  -0.03715129]
 after e[l]: [-0.10429516 -0.14121817 -0.12232441]
 after e[l]: [ 0.01902997 -0.01126623 -0.06202108]
 after e[l]: [0.11120602 0.13950293 0.04415518]
 after e[l]: [-0.01354545  0.03274248 -0.03114719]
 after e[l]: [-0.00834156  0.02621393 -0.01071604]
 after e[l]: [-0.03772337 -0.0360905  -0.0150375 ]
 after e[l]: [0.02120109 0.00659361 0.01429527]
 after e[l]: [ 0.02348951 -0.00337419 -0.04221851]
 after e[l]: [-0.03309165 -0.07514166 -0.04003977]
 after e[l]: [ 0.04215246  0.08229012 -0.00016585]
 after e[l]: [-1.3558445e-02  3.4865294e-02 -3.8307167e-05]
 after e[l]: [-0.04252458 -0.05336334 -0.02885   ]
 after e[l]: [-0.00428429  0.0276924  -0.03448375]
 after e[l]: [-0.00893557  0.02707967 -0.01327858]
 after e[l]: [-0.04004847 -0.03477275 -0.01865024]
 after e[l]: [-0.04303825  0.01592176  0.03377574]
 after e[l]: [-0.05889318 -0.02008283  0.01419723]
 after e[l]: [-0.03745823 -0.0740834  -0.03873461]
 after e[l]: [ 0.02856004  0.04628325 -0.05621205]
 after e[l]: [0.00900534 0.05368853 0.04643512]
 after e[l]: [-0.06756899 -0.00023034  0.02903035]
 after e[l]: [ 0.00544282  0.00257647 -0.04076133]
 after e[l]: [0.03615081 0.07087729 0.00958705]
 after e[l]: [0.0089115  0.07833124 0.06053799]
 after e[l]: [0.04061817 0.03607007 0.02805972]
 after e[l]: [ 0.00026234  0.01472933 -0.00792568]
 after e[l]: [-0.07748265 -0.04708348 -0.07996292]
 after e[l]: [0.02240704 0.00603658 0.01564928]
 after e[l]: [ 0.02236271 -0.00242896 -0.03920693]
 after e[l]: [-0.02885195 -0.07032666 -0.03309636]
 after e[l]: [0.00534828 0.00171788 0.11736713]
 after e[l]: [-0.07377294 -0.03507127  0.09547298]
 after e[l]: [-0.07695853 -0.14280081  0.01753042]
 after e[l]: [0.01339822 0.01139184 0.12773323]
 after e[l]: [-0.07240786 -0.03542254  0.10074332]
 after e[l]: [-0.07358065 -0.1398666   0.01694295]
 after e[l]: [-0.0853449  -0.09268243 -0.04619905]
 after e[l]: [-0.10079736 -0.1530609  -0.09215101]
 after e[l]: [ 0.05651015 -0.02807362 -0.0677553 ]
 after e[l]: [-0.00142793  0.031198   -0.03112381]
 after e[l]: [-0.00755906  0.02757426 -0.02052638]
 after e[l]: [-0.04030044 -0.0343788  -0.01741649]
 after e[l]: [0.01541279 0.00599225 0.12004187]
 after e[l]: [-0.07179976 -0.03160683  0.09202024]
 after e[l]: [-0.0795958  -0.13774467  0.01402311]
 after e[l]: [-0.11103205 -0.14957808 -0.11718328]
 after e[l]: [ 0.02396318 -0.0112125  -0.06102319]
 after e[l]: [0.11283331 0.14261584 0.03774032]
 after e[l]: [ 0.0151687  -0.03661707 -0.01392549]
 after e[l]: [-0.0759045  -0.08286242 -0.00810612]
 after e[l]: [-0.03565126 -0.07096916 -0.03637996]
 after e[l]: [ 0.00027484 -0.00333172 -0.03762058]
 after e[l]: [0.03054466 0.06914677 0.00868491]
 after e[l]: [0.01056653 0.07871465 0.05793396]
 after e[l]: [-0.10615257 -0.1421201  -0.12330247]
 after e[l]: [ 0.02147102 -0.01123061 -0.06145582]
 after e[l]: [0.11766739 0.1365085  0.04519934]
 after e[l]: [0.04118528 0.03680842 0.03651843]
 after e[l]: [-7.9680161e-05  1.6992984e-02 -1.7198400e-03]
 after e[l]: [-0.07780617 -0.05904151 -0.08353163]
c:[[[-4.31948937e-02  8.73991288e-04 -2.74574179e-02]
  [-6.42253160e-02  2.56661512e-02 -3.20661329e-02]
  [-5.96792512e-02  3.26238871e-02 -3.07534542e-02]]

 [[-3.02257743e-02  4.87640835e-02  7.34754093e-03]
  [ 7.84001052e-02  1.32877380e-01  1.38081595e-01]
  [ 5.88266850e-02  1.40986517e-01  1.14547700e-01]]

 [[-3.42560634e-02 -2.43230890e-02 -1.73057616e-02]
  [ 1.15747433e-02  5.41059449e-02 -1.94236562e-02]
  [ 1.39947971e-02  4.51579913e-02  4.29377751e-03]]

 [[-1.31569421e-02 -6.01732768e-02 -6.08914830e-02]
  [ 3.45932879e-02  6.45999098e-03  2.82862037e-02]
  [-4.26797848e-03  1.91995350e-03  5.03647234e-03]]

 [[-3.39830443e-02 -1.89900510e-02 -2.96754856e-02]
  [-3.24657410e-02 -1.08511010e-02  8.14045966e-03]
  [-6.49480149e-02 -6.47070631e-02 -4.53294590e-02]]

 [[ 8.66359472e-03  9.01531801e-03 -2.12539770e-02]
  [-2.57553365e-02  1.73137663e-03 -1.21211521e-02]
  [-1.31869852e-03  6.64148331e-02  5.72321303e-02]]

 [[-3.78337912e-02  2.11415091e-03  2.64052078e-02]
  [-5.95348049e-03 -1.99882383e-03  2.24022311e-03]
  [-5.04916580e-03 -4.73820828e-02  3.13209312e-04]]

 [[-1.59374196e-02  3.82621586e-02  1.80970542e-02]
  [-4.20992151e-02  2.64762715e-02  1.60896033e-02]
  [ 6.60416158e-03  3.23545747e-02  1.07694035e-02]]

 [[-3.01253106e-02 -7.27054179e-02 -3.52447405e-02]
  [-4.27121669e-02 -5.82110807e-02 -7.09169358e-02]
  [ 7.71301473e-03 -1.41305514e-02  2.41001515e-04]]

 [[-9.00345296e-03 -1.36621268e-02 -1.09335948e-02]
  [ 3.73868607e-02  3.54841389e-02  2.07647849e-02]
  [-1.58842690e-02 -2.27265731e-02 -3.49979475e-02]]

 [[-8.25508907e-02 -5.93938157e-02 -5.10642119e-02]
  [ 1.98774487e-02  6.46994170e-03  2.40713879e-02]
  [ 2.90159676e-02  2.10445132e-02 -3.42382863e-03]]

 [[-7.20930770e-02  3.87101289e-04  8.44411831e-03]
  [-5.44001944e-02 -4.90977131e-02 -1.30289951e-02]
  [-7.00497553e-02 -4.48616557e-02 -8.77408832e-02]]

 [[-6.21640421e-02 -6.28735721e-02  3.59398872e-02]
  [-4.90390584e-02 -2.59706229e-02  2.08755564e-02]
  [-3.23926918e-02 -2.44795922e-02  6.02976838e-03]]

 [[-3.14393677e-02  1.31610706e-02  2.60375766e-03]
  [-4.26462945e-03  7.40935802e-02  5.56753688e-02]
  [ 4.01654653e-02  9.98312980e-02  8.04996938e-02]]

 [[ 3.79347429e-02  7.55819380e-02  6.14470243e-02]
  [-3.52301709e-02 -3.52159701e-02  3.02514667e-03]
  [-4.77861613e-02 -2.53534187e-02  2.47957353e-02]]

 [[-1.12340013e-02 -2.34272443e-02 -7.20157325e-02]
  [-9.86155216e-03  1.30433887e-02 -4.15132381e-02]
  [-5.18728532e-02 -1.10681057e-02 -3.04177925e-02]]

 [[-2.78403498e-02  1.09538715e-02  8.77690874e-03]
  [-6.37090812e-03  7.67244250e-02  5.68733774e-02]
  [ 4.25429717e-02  1.00694172e-01  7.96387494e-02]]

 [[-3.08491923e-02  5.35016553e-03  2.99461409e-02]
  [-3.49443406e-03 -1.82569947e-03  7.52628257e-04]
  [-7.86507595e-03 -3.95132788e-02 -3.00717982e-03]]

 [[-3.23601700e-02 -1.62566397e-02 -3.62010002e-02]
  [-3.05435583e-02 -7.10208528e-03  1.04869679e-02]
  [-6.10718690e-02 -6.06679469e-02 -4.69182394e-02]]

 [[-2.28643231e-02 -2.08467804e-02 -2.07335837e-02]
  [ 1.03246486e-02  4.95645069e-02 -1.44479331e-02]
  [ 6.18677353e-03  4.47304398e-02  1.19877586e-04]]

 [[-1.42630348e-02  3.65411080e-02  1.51446387e-02]
  [-4.00726721e-02  2.61277743e-02  1.82476398e-02]
  [ 6.48219744e-03  4.20987830e-02  1.05810324e-02]]

 [[ 4.63448912e-02  7.49750510e-02  5.74865453e-02]
  [-3.93961370e-02 -3.34459245e-02  7.34037394e-03]
  [-4.34112698e-02 -2.72379089e-02  2.69459821e-02]]

 [[ 1.18130427e-02  1.53188659e-02 -1.95829608e-02]
  [-2.31451560e-02 -3.34566459e-03 -4.56352578e-03]
  [-8.27083830e-03  6.34312406e-02  6.41157255e-02]]

 [[ 6.39600307e-03  1.24669531e-02 -1.83302928e-02]
  [-2.46850941e-02 -1.50360202e-03 -8.76615290e-03]
  [-4.68854560e-03  6.37503341e-02  6.13346435e-02]]

 [[-7.87714422e-02 -6.39904737e-02 -5.00791967e-02]
  [ 1.42658278e-02  9.69301257e-03  1.78800579e-02]
  [ 2.86984071e-02  1.98661517e-02 -1.09482836e-02]]

 [[-2.58734394e-02  8.78539402e-03  9.34382901e-03]
  [-1.87859638e-03  7.62725696e-02  5.68736866e-02]
  [ 3.69067974e-02  9.36411917e-02  7.52362460e-02]]

 [[ 1.67556405e-02  1.71585344e-02 -1.85664687e-02]
  [-2.08010599e-02  1.85898432e-06 -1.18271299e-02]
  [-5.87904220e-03  5.91726862e-02  5.92718385e-02]]

 [[-6.34075105e-02 -5.97167686e-02  4.11814936e-02]
  [-4.60660532e-02 -3.27082612e-02  2.52398364e-02]
  [-3.23027633e-02 -2.14802120e-02  6.51724683e-03]]

 [[-2.60912329e-02  4.63310741e-02  1.11932140e-02]
  [ 7.77378455e-02  1.32636175e-01  1.41262829e-01]
  [ 6.11310937e-02  1.40984133e-01  1.13530070e-01]]

 [[-3.04128658e-02 -2.18436830e-02 -2.42698286e-02]
  [ 1.59804542e-02  5.01400381e-02 -2.14369390e-02]
  [ 1.25502348e-02  4.31888513e-02  7.36711128e-03]]

 [[-6.49517998e-02 -5.98392114e-02  3.16342041e-02]
  [-4.80362326e-02 -2.78518274e-02  2.43536402e-02]
  [-3.03157736e-02 -2.78718825e-02  2.78054411e-03]]

 [[-1.57408658e-02  3.67351100e-02  1.73941776e-02]
  [-4.46214825e-02  2.85253394e-02  1.58323515e-02]
  [ 5.84063120e-03  3.58229578e-02  9.27238818e-03]]]
 after e[l]: [-0.04319489  0.00087399 -0.02745742]
 after e[l]: [-0.06422532  0.02566615 -0.03206613]
 after e[l]: [-0.05967925  0.03262389 -0.03075345]
 after e[l]: [-0.03022577  0.04876408  0.00734754]
 after e[l]: [0.07840011 0.13287738 0.1380816 ]
 after e[l]: [0.05882668 0.14098652 0.1145477 ]
 after e[l]: [-0.03425606 -0.02432309 -0.01730576]
 after e[l]: [ 0.01157474  0.05410594 -0.01942366]
 after e[l]: [0.0139948  0.04515799 0.00429378]
 after e[l]: [-0.01315694 -0.06017328 -0.06089148]
 after e[l]: [0.03459329 0.00645999 0.0282862 ]
 after e[l]: [-0.00426798  0.00191995  0.00503647]
 after e[l]: [-0.03398304 -0.01899005 -0.02967549]
 after e[l]: [-0.03246574 -0.0108511   0.00814046]
 after e[l]: [-0.06494801 -0.06470706 -0.04532946]
 after e[l]: [ 0.00866359  0.00901532 -0.02125398]
 after e[l]: [-0.02575534  0.00173138 -0.01212115]
 after e[l]: [-0.0013187   0.06641483  0.05723213]
 after e[l]: [-0.03783379  0.00211415  0.02640521]
 after e[l]: [-0.00595348 -0.00199882  0.00224022]
 after e[l]: [-0.00504917 -0.04738208  0.00031321]
 after e[l]: [-0.01593742  0.03826216  0.01809705]
 after e[l]: [-0.04209922  0.02647627  0.0160896 ]
 after e[l]: [0.00660416 0.03235457 0.0107694 ]
 after e[l]: [-0.03012531 -0.07270542 -0.03524474]
 after e[l]: [-0.04271217 -0.05821108 -0.07091694]
 after e[l]: [ 0.00771301 -0.01413055  0.000241  ]
 after e[l]: [-0.00900345 -0.01366213 -0.01093359]
 after e[l]: [0.03738686 0.03548414 0.02076478]
 after e[l]: [-0.01588427 -0.02272657 -0.03499795]
 after e[l]: [-0.08255089 -0.05939382 -0.05106421]
 after e[l]: [0.01987745 0.00646994 0.02407139]
 after e[l]: [ 0.02901597  0.02104451 -0.00342383]
 after e[l]: [-0.07209308  0.0003871   0.00844412]
 after e[l]: [-0.05440019 -0.04909771 -0.013029  ]
 after e[l]: [-0.07004976 -0.04486166 -0.08774088]
 after e[l]: [-0.06216404 -0.06287357  0.03593989]
 after e[l]: [-0.04903906 -0.02597062  0.02087556]
 after e[l]: [-0.03239269 -0.02447959  0.00602977]
 after e[l]: [-0.03143937  0.01316107  0.00260376]
 after e[l]: [-0.00426463  0.07409358  0.05567537]
 after e[l]: [0.04016547 0.0998313  0.08049969]
 after e[l]: [0.03793474 0.07558194 0.06144702]
 after e[l]: [-0.03523017 -0.03521597  0.00302515]
 after e[l]: [-0.04778616 -0.02535342  0.02479574]
 after e[l]: [-0.011234   -0.02342724 -0.07201573]
 after e[l]: [-0.00986155  0.01304339 -0.04151324]
 after e[l]: [-0.05187285 -0.01106811 -0.03041779]
 after e[l]: [-0.02784035  0.01095387  0.00877691]
 after e[l]: [-0.00637091  0.07672442  0.05687338]
 after e[l]: [0.04254297 0.10069417 0.07963875]
 after e[l]: [-0.03084919  0.00535017  0.02994614]
 after e[l]: [-0.00349443 -0.0018257   0.00075263]
 after e[l]: [-0.00786508 -0.03951328 -0.00300718]
 after e[l]: [-0.03236017 -0.01625664 -0.036201  ]
 after e[l]: [-0.03054356 -0.00710209  0.01048697]
 after e[l]: [-0.06107187 -0.06066795 -0.04691824]
 after e[l]: [-0.02286432 -0.02084678 -0.02073358]
 after e[l]: [ 0.01032465  0.04956451 -0.01444793]
 after e[l]: [0.00618677 0.04473044 0.00011988]
 after e[l]: [-0.01426303  0.03654111  0.01514464]
 after e[l]: [-0.04007267  0.02612777  0.01824764]
 after e[l]: [0.0064822  0.04209878 0.01058103]
 after e[l]: [0.04634489 0.07497505 0.05748655]
 after e[l]: [-0.03939614 -0.03344592  0.00734037]
 after e[l]: [-0.04341127 -0.02723791  0.02694598]
 after e[l]: [ 0.01181304  0.01531887 -0.01958296]
 after e[l]: [-0.02314516 -0.00334566 -0.00456353]
 after e[l]: [-0.00827084  0.06343124  0.06411573]
 after e[l]: [ 0.006396    0.01246695 -0.01833029]
 after e[l]: [-0.02468509 -0.0015036  -0.00876615]
 after e[l]: [-0.00468855  0.06375033  0.06133464]
 after e[l]: [-0.07877144 -0.06399047 -0.0500792 ]
 after e[l]: [0.01426583 0.00969301 0.01788006]
 after e[l]: [ 0.02869841  0.01986615 -0.01094828]
 after e[l]: [-0.02587344  0.00878539  0.00934383]
 after e[l]: [-0.0018786   0.07627257  0.05687369]
 after e[l]: [0.0369068  0.09364119 0.07523625]
 after e[l]: [ 0.01675564  0.01715853 -0.01856647]
 after e[l]: [-2.0801060e-02  1.8589843e-06 -1.1827130e-02]
 after e[l]: [-0.00587904  0.05917269  0.05927184]
 after e[l]: [-0.06340751 -0.05971677  0.04118149]
 after e[l]: [-0.04606605 -0.03270826  0.02523984]
 after e[l]: [-0.03230276 -0.02148021  0.00651725]
 after e[l]: [-0.02609123  0.04633107  0.01119321]
 after e[l]: [0.07773785 0.13263617 0.14126283]
 after e[l]: [0.06113109 0.14098413 0.11353007]
 after e[l]: [-0.03041287 -0.02184368 -0.02426983]
 after e[l]: [ 0.01598045  0.05014004 -0.02143694]
 after e[l]: [0.01255023 0.04318885 0.00736711]
 after e[l]: [-0.0649518  -0.05983921  0.0316342 ]
 after e[l]: [-0.04803623 -0.02785183  0.02435364]
 after e[l]: [-0.03031577 -0.02787188  0.00278054]
 after e[l]: [-0.01574087  0.03673511  0.01739418]
 after e[l]: [-0.04462148  0.02852534  0.01583235]
 after e[l]: [0.00584063 0.03582296 0.00927239]
c:[[[-1.02984287e-01 -4.43502776e-02  7.16292337e-02]
  [-6.18635714e-02 -4.77473401e-02  5.66993020e-02]
  [-6.01329505e-02 -4.82161753e-02  2.92810611e-02]]

 [[-3.01724728e-02 -3.74538861e-02 -2.01108237e-03]
  [-6.33400083e-02 -1.01592906e-01 -6.07474186e-02]
  [-2.19024308e-02 -1.10211685e-01 -9.11319628e-02]]

 [[-6.00684471e-02 -3.04220226e-02  7.34821940e-03]
  [-8.17988515e-02 -9.96119678e-02 -2.87553817e-02]
  [-2.41121426e-02 -9.93057787e-02 -4.50764708e-02]]

 [[ 8.72712061e-02  3.38715799e-02 -1.45727759e-02]
  [ 3.05780228e-02  9.26143304e-03 -1.19384415e-02]
  [-1.70241036e-02 -1.77360531e-02  4.73213615e-03]]

 [[ 1.36129453e-03  2.11997759e-02  1.89489853e-02]
  [-8.91192704e-02 -8.05529878e-02 -1.46071184e-02]
  [-4.87434193e-02 -7.10216984e-02 -2.28233496e-03]]

 [[-2.37222333e-02 -2.44641490e-02  3.28000486e-02]
  [-5.18947393e-02 -1.08470052e-01 -3.13272476e-02]
  [-3.69987972e-02 -8.49134177e-02 -7.86240324e-02]]

 [[-7.67876487e-03  2.34680716e-02 -2.43755486e-02]
  [ 2.99224071e-02  2.66971868e-02  4.17536534e-02]
  [ 5.09643368e-02  5.63537478e-02  2.23692097e-02]]

 [[ 4.50688079e-02  1.20041240e-02  1.36986449e-02]
  [ 1.60696674e-02 -7.52097666e-02 -1.55007644e-02]
  [ 1.52125827e-03 -3.80477458e-02 -5.52799143e-02]]

 [[-5.91056384e-02 -4.60738037e-03 -1.86493304e-02]
  [-3.54279624e-03 -1.32309971e-02 -2.26979591e-02]
  [-2.58786585e-02  2.67780363e-03  7.53511116e-02]]

 [[ 6.11750223e-02  7.69186020e-02 -1.90913752e-02]
  [ 1.16907477e-01  2.07668290e-01  1.58869907e-01]
  [-1.51569853e-02  1.21836647e-01  1.20758168e-01]]

 [[-4.91547994e-02 -5.22221476e-02 -6.43099323e-02]
  [-5.13353683e-02 -6.58019781e-02 -1.01995744e-01]
  [-1.82011407e-02 -3.18268351e-02 -3.58130708e-02]]

 [[-3.47591974e-02 -3.36472690e-02  2.86078472e-02]
  [-3.99779864e-02 -7.05596879e-02  1.81162904e-03]
  [ 2.67137680e-02 -5.62114604e-02 -2.92863492e-02]]

 [[ 2.60704253e-02  2.02594325e-02 -2.11807545e-02]
  [ 1.64981559e-02  2.70386077e-02  3.10240425e-02]
  [-1.54116086e-03 -2.61092298e-02  2.88232043e-03]]

 [[ 3.03709717e-03 -5.73267415e-02 -5.22800498e-02]
  [ 2.36237999e-02 -4.16181721e-02 -5.61708920e-02]
  [-2.48745829e-02  3.65878642e-03  1.87822022e-02]]

 [[-5.04531562e-02 -1.50609287e-02  3.18767317e-02]
  [-4.28036675e-02 -8.76687765e-02 -1.46183828e-02]
  [-1.27990274e-02 -6.40834644e-02  1.97941363e-02]]

 [[-6.65479526e-02 -7.77968541e-02 -3.48961540e-02]
  [-6.77902345e-03 -5.89890452e-03 -7.97734410e-02]
  [ 9.37487278e-03  4.25366201e-02  2.71013007e-02]]

 [[-2.04149750e-04 -5.83030954e-02 -5.24511673e-02]
  [ 2.33609341e-02 -3.80671471e-02 -6.04954213e-02]
  [-1.88189559e-02 -3.59255029e-03  1.83912124e-02]]

 [[-7.33271148e-03  2.87153013e-02 -2.63193529e-02]
  [ 3.13769281e-02  2.84917615e-02  4.48336899e-02]
  [ 4.89945263e-02  5.48009910e-02  2.36766245e-02]]

 [[ 6.69229357e-03  1.97524559e-02  1.78081077e-02]
  [-9.17864144e-02 -7.46080801e-02 -6.92039123e-03]
  [-4.28796746e-02 -6.77104369e-02 -7.05625210e-03]]

 [[-6.14560507e-02 -3.46946679e-02  1.04653118e-02]
  [-8.08277652e-02 -9.28345174e-02 -2.63130497e-02]
  [-2.01617554e-02 -9.79206339e-02 -4.46529239e-02]]

 [[ 4.65177335e-02  1.29850460e-02  1.79431476e-02]
  [ 2.05164608e-02 -6.52740821e-02 -2.51703188e-02]
  [ 3.81067302e-03 -3.70560326e-02 -5.36748357e-02]]

 [[-5.00275083e-02 -9.08103772e-03  3.15748341e-02]
  [-4.21409756e-02 -8.62323195e-02 -2.22192705e-02]
  [-1.04416646e-02 -6.26906157e-02  1.84231605e-02]]

 [[-3.06158178e-02 -2.22729575e-02  3.09278294e-02]
  [-5.33878282e-02 -1.04406327e-01 -3.25718373e-02]
  [-2.88064070e-02 -8.29731822e-02 -7.09920973e-02]]

 [[-2.95749716e-02 -2.32257526e-02  3.37789394e-02]
  [-5.65099232e-02 -1.06347881e-01 -3.22343037e-02]
  [-3.34846638e-02 -7.68918023e-02 -7.50065595e-02]]

 [[-5.21227755e-02 -5.22214994e-02 -6.77599981e-02]
  [-4.72329259e-02 -6.70175999e-02 -1.00610405e-01]
  [-1.36879953e-02 -2.76935510e-02 -3.68511863e-02]]

 [[-8.51068646e-04 -5.75270429e-02 -5.63149899e-02]
  [ 2.69911718e-02 -3.71105187e-02 -5.43714203e-02]
  [-2.55301576e-02  3.44345951e-03  2.53910087e-02]]

 [[-2.81143915e-02 -2.50172913e-02  2.76243947e-02]
  [-5.39850220e-02 -1.03708521e-01 -3.38342600e-02]
  [-2.84108929e-02 -7.42143318e-02 -7.44012445e-02]]

 [[ 2.48837657e-02  2.32357569e-02 -2.45397761e-02]
  [ 1.59395114e-02  3.73453684e-02  3.04612610e-02]
  [-1.53036113e-03 -2.80837063e-02  4.03699605e-03]]

 [[-2.77843401e-02 -3.79195139e-02  2.84281769e-03]
  [-6.49671257e-02 -1.03096344e-01 -5.66847920e-02]
  [-2.16873027e-02 -1.05911687e-01 -9.27573964e-02]]

 [[-6.02754913e-02 -2.21570842e-02  7.82878511e-03]
  [-7.87471533e-02 -9.86777395e-02 -2.80645862e-02]
  [-2.37545446e-02 -1.00630909e-01 -4.48005572e-02]]

 [[ 2.29869727e-02  2.40591615e-02 -1.94565151e-02]
  [ 1.70698483e-02  3.79186049e-02  3.35335545e-02]
  [-4.43799328e-03 -2.91985925e-02  7.30755413e-03]]

 [[ 4.87687737e-02  1.09107848e-02  1.76523123e-02]
  [ 1.40229743e-02 -6.73841685e-02 -1.37404157e-02]
  [ 3.21485987e-03 -4.27767076e-02 -5.59636578e-02]]]
 after e[l]: [-0.10298429 -0.04435028  0.07162923]
 after e[l]: [-0.06186357 -0.04774734  0.0566993 ]
 after e[l]: [-0.06013295 -0.04821618  0.02928106]
 after e[l]: [-0.03017247 -0.03745389 -0.00201108]
 after e[l]: [-0.06334001 -0.10159291 -0.06074742]
 after e[l]: [-0.02190243 -0.11021169 -0.09113196]
 after e[l]: [-0.06006845 -0.03042202  0.00734822]
 after e[l]: [-0.08179885 -0.09961197 -0.02875538]
 after e[l]: [-0.02411214 -0.09930578 -0.04507647]
 after e[l]: [ 0.08727121  0.03387158 -0.01457278]
 after e[l]: [ 0.03057802  0.00926143 -0.01193844]
 after e[l]: [-0.0170241  -0.01773605  0.00473214]
 after e[l]: [0.00136129 0.02119978 0.01894899]
 after e[l]: [-0.08911927 -0.08055299 -0.01460712]
 after e[l]: [-0.04874342 -0.0710217  -0.00228233]
 after e[l]: [-0.02372223 -0.02446415  0.03280005]
 after e[l]: [-0.05189474 -0.10847005 -0.03132725]
 after e[l]: [-0.0369988  -0.08491342 -0.07862403]
 after e[l]: [-0.00767876  0.02346807 -0.02437555]
 after e[l]: [0.02992241 0.02669719 0.04175365]
 after e[l]: [0.05096434 0.05635375 0.02236921]
 after e[l]: [0.04506881 0.01200412 0.01369864]
 after e[l]: [ 0.01606967 -0.07520977 -0.01550076]
 after e[l]: [ 0.00152126 -0.03804775 -0.05527991]
 after e[l]: [-0.05910564 -0.00460738 -0.01864933]
 after e[l]: [-0.0035428  -0.013231   -0.02269796]
 after e[l]: [-0.02587866  0.0026778   0.07535111]
 after e[l]: [ 0.06117502  0.0769186  -0.01909138]
 after e[l]: [0.11690748 0.20766829 0.1588699 ]
 after e[l]: [-0.01515699  0.12183665  0.12075817]
 after e[l]: [-0.0491548  -0.05222215 -0.06430993]
 after e[l]: [-0.05133537 -0.06580198 -0.10199574]
 after e[l]: [-0.01820114 -0.03182684 -0.03581307]
 after e[l]: [-0.0347592  -0.03364727  0.02860785]
 after e[l]: [-0.03997799 -0.07055969  0.00181163]
 after e[l]: [ 0.02671377 -0.05621146 -0.02928635]
 after e[l]: [ 0.02607043  0.02025943 -0.02118075]
 after e[l]: [0.01649816 0.02703861 0.03102404]
 after e[l]: [-0.00154116 -0.02610923  0.00288232]
 after e[l]: [ 0.0030371  -0.05732674 -0.05228005]
 after e[l]: [ 0.0236238  -0.04161817 -0.05617089]
 after e[l]: [-0.02487458  0.00365879  0.0187822 ]
 after e[l]: [-0.05045316 -0.01506093  0.03187673]
 after e[l]: [-0.04280367 -0.08766878 -0.01461838]
 after e[l]: [-0.01279903 -0.06408346  0.01979414]
 after e[l]: [-0.06654795 -0.07779685 -0.03489615]
 after e[l]: [-0.00677902 -0.0058989  -0.07977344]
 after e[l]: [0.00937487 0.04253662 0.0271013 ]
 after e[l]: [-0.00020415 -0.0583031  -0.05245117]
 after e[l]: [ 0.02336093 -0.03806715 -0.06049542]
 after e[l]: [-0.01881896 -0.00359255  0.01839121]
 after e[l]: [-0.00733271  0.0287153  -0.02631935]
 after e[l]: [0.03137693 0.02849176 0.04483369]
 after e[l]: [0.04899453 0.05480099 0.02367662]
 after e[l]: [0.00669229 0.01975246 0.01780811]
 after e[l]: [-0.09178641 -0.07460808 -0.00692039]
 after e[l]: [-0.04287967 -0.06771044 -0.00705625]
 after e[l]: [-0.06145605 -0.03469467  0.01046531]
 after e[l]: [-0.08082777 -0.09283452 -0.02631305]
 after e[l]: [-0.02016176 -0.09792063 -0.04465292]
 after e[l]: [0.04651773 0.01298505 0.01794315]
 after e[l]: [ 0.02051646 -0.06527408 -0.02517032]
 after e[l]: [ 0.00381067 -0.03705603 -0.05367484]
 after e[l]: [-0.05002751 -0.00908104  0.03157483]
 after e[l]: [-0.04214098 -0.08623232 -0.02221927]
 after e[l]: [-0.01044166 -0.06269062  0.01842316]
 after e[l]: [-0.03061582 -0.02227296  0.03092783]
 after e[l]: [-0.05338783 -0.10440633 -0.03257184]
 after e[l]: [-0.02880641 -0.08297318 -0.0709921 ]
 after e[l]: [-0.02957497 -0.02322575  0.03377894]
 after e[l]: [-0.05650992 -0.10634788 -0.0322343 ]
 after e[l]: [-0.03348466 -0.0768918  -0.07500656]
 after e[l]: [-0.05212278 -0.0522215  -0.06776   ]
 after e[l]: [-0.04723293 -0.0670176  -0.10061041]
 after e[l]: [-0.013688   -0.02769355 -0.03685119]
 after e[l]: [-0.00085107 -0.05752704 -0.05631499]
 after e[l]: [ 0.02699117 -0.03711052 -0.05437142]
 after e[l]: [-0.02553016  0.00344346  0.02539101]
 after e[l]: [-0.02811439 -0.02501729  0.02762439]
 after e[l]: [-0.05398502 -0.10370852 -0.03383426]
 after e[l]: [-0.02841089 -0.07421433 -0.07440124]
 after e[l]: [ 0.02488377  0.02323576 -0.02453978]
 after e[l]: [0.01593951 0.03734537 0.03046126]
 after e[l]: [-0.00153036 -0.02808371  0.004037  ]
 after e[l]: [-0.02778434 -0.03791951  0.00284282]
 after e[l]: [-0.06496713 -0.10309634 -0.05668479]
 after e[l]: [-0.0216873  -0.10591169 -0.0927574 ]
 after e[l]: [-0.06027549 -0.02215708  0.00782879]
 after e[l]: [-0.07874715 -0.09867774 -0.02806459]
 after e[l]: [-0.02375454 -0.10063091 -0.04480056]
 after e[l]: [ 0.02298697  0.02405916 -0.01945652]
 after e[l]: [0.01706985 0.0379186  0.03353355]
 after e[l]: [-0.00443799 -0.02919859  0.00730755]
 after e[l]: [0.04876877 0.01091078 0.01765231]
 after e[l]: [ 0.01402297 -0.06738417 -0.01374042]
 after e[l]: [ 0.00321486 -0.04277671 -0.05596366]
c:[[[-0.08661643 -0.07375823 -0.06359775]
  [-0.04459013 -0.11439024 -0.1042935 ]
  [-0.06455573 -0.12465213 -0.09445261]]

 [[-0.03857005 -0.00229999 -0.01059422]
  [-0.01071863 -0.00266998 -0.01007956]
  [ 0.00820327  0.02248289  0.00486168]]

 [[ 0.04637633  0.00559385 -0.05946338]
  [ 0.09740932 -0.00816855 -0.07496394]
  [ 0.09688979  0.03739551 -0.01545187]]

 [[-0.03181196 -0.05338499 -0.04898433]
  [-0.00553393  0.03228755  0.04164097]
  [ 0.0389716   0.10145306  0.11105175]]

 [[-0.02150679 -0.05797541 -0.03252969]
  [-0.01155007 -0.04213713 -0.04460546]
  [-0.00406752  0.01077188 -0.02251595]]

 [[-0.01593336 -0.06351673 -0.07012281]
  [-0.04648046 -0.1371782  -0.09899043]
  [-0.03505366 -0.10895571 -0.07921354]]

 [[-0.02504245 -0.00779752  0.04371376]
  [ 0.01937917 -0.01671877 -0.00447229]
  [ 0.01317994 -0.02178693  0.01768486]]

 [[ 0.02744813  0.07703726  0.11365665]
  [ 0.07095636  0.11908201  0.12146001]
  [ 0.02180577  0.08171766  0.06456873]]

 [[ 0.0073239   0.07738429  0.07447263]
  [ 0.04598601  0.05762056  0.0238334 ]
  [ 0.01099098 -0.01794922  0.00750734]]

 [[ 0.0241856   0.06719844  0.04905885]
  [ 0.01978998  0.05186893  0.05366278]
  [ 0.0440548   0.04125741  0.05001027]]

 [[-0.01579486  0.0164631  -0.0017889 ]
  [-0.02030434 -0.0262711  -0.05359548]
  [ 0.03781225 -0.02609352 -0.08915953]]

 [[-0.00238723 -0.01552073  0.01221311]
  [ 0.04379775  0.02285762  0.0297631 ]
  [-0.00722721 -0.01877327  0.00675457]]

 [[-0.00793074 -0.04335941 -0.01562468]
  [-0.04321925 -0.07756343 -0.04788545]
  [-0.07601303 -0.10545862 -0.10259566]]

 [[ 0.03318693  0.02714672  0.01536221]
  [-0.00128323 -0.00736026 -0.01641373]
  [-0.06300531 -0.00869362 -0.04327327]]

 [[ 0.05750177 -0.01738435 -0.06870171]
  [ 0.04442924 -0.07156893 -0.03175588]
  [-0.00168909 -0.03766869 -0.00833699]]

 [[-0.03171571 -0.04852022 -0.02962456]
  [-0.00414745 -0.05334146 -0.07541149]
  [-0.04836054 -0.03062964 -0.03488077]]

 [[ 0.03040074  0.02368185  0.01590524]
  [ 0.00281828 -0.00778202 -0.02123742]
  [-0.05149208 -0.01261807 -0.0407961 ]]

 [[-0.02658012 -0.01292611  0.03853398]
  [ 0.01080747 -0.01530221 -0.00500082]
  [ 0.00864924 -0.02366138  0.01326171]]

 [[-0.01804799 -0.06337876 -0.0300138 ]
  [-0.01126975 -0.03897591 -0.04005831]
  [-0.00438335  0.01246473 -0.02564579]]

 [[ 0.04694002  0.00766122 -0.06003393]
  [ 0.09885344 -0.00530801 -0.07631337]
  [ 0.09853119  0.03595556 -0.00683482]]

 [[ 0.02714436  0.07344872  0.11365942]
  [ 0.07642758  0.12425461  0.12586568]
  [ 0.02235523  0.08289631  0.06365945]]

 [[ 0.06231669 -0.02404198 -0.07302666]
  [ 0.04534629 -0.07736134 -0.03626435]
  [-0.00781904 -0.03893549 -0.0155833 ]]

 [[-0.01457617 -0.06861881 -0.07015724]
  [-0.04491394 -0.13818865 -0.09712015]
  [-0.02985518 -0.11225001 -0.08190259]]

 [[-0.01351042 -0.0671872  -0.0672904 ]
  [-0.04970291 -0.13120149 -0.1006594 ]
  [-0.03678206 -0.10540424 -0.08169319]]

 [[-0.01883415  0.01546112  0.00301633]
  [-0.01579603 -0.02316155 -0.05182818]
  [ 0.03701662 -0.0207871  -0.08627198]]

 [[ 0.03533848  0.01747037  0.01137126]
  [ 0.00436043 -0.00577073 -0.02010319]
  [-0.05742493 -0.00824133 -0.03906213]]

 [[-0.00694923 -0.07012641 -0.06978375]
  [-0.04528973 -0.13665809 -0.097874  ]
  [-0.03510907 -0.1083821  -0.07770213]]

 [[-0.00384029 -0.04088054 -0.01611694]
  [-0.04537905 -0.07279044 -0.05725942]
  [-0.07674296 -0.1112422  -0.09995447]]

 [[-0.03409968  0.00121571 -0.01933667]
  [-0.01137829 -0.00193992 -0.00579169]
  [ 0.00455     0.02128651  0.00386136]]

 [[ 0.05348428  0.00583313 -0.0637641 ]
  [ 0.09383918 -0.01037824 -0.07239038]
  [ 0.09623874  0.0364706  -0.01242976]]

 [[-0.00248833 -0.0419145  -0.0195165 ]
  [-0.04974352 -0.07428456 -0.04476899]
  [-0.07498731 -0.10881643 -0.10217731]]

 [[ 0.02585297  0.07485161  0.11726359]
  [ 0.07246286  0.12440012  0.13243333]
  [ 0.02021062  0.08521806  0.06405515]]]
 after e[l]: [-0.08661643 -0.07375823 -0.06359775]
 after e[l]: [-0.04459013 -0.11439024 -0.1042935 ]
 after e[l]: [-0.06455573 -0.12465213 -0.09445261]
 after e[l]: [-0.03857005 -0.00229999 -0.01059422]
 after e[l]: [-0.01071863 -0.00266998 -0.01007956]
 after e[l]: [0.00820327 0.02248289 0.00486168]
 after e[l]: [ 0.04637633  0.00559385 -0.05946338]
 after e[l]: [ 0.09740932 -0.00816855 -0.07496394]
 after e[l]: [ 0.09688979  0.03739551 -0.01545187]
 after e[l]: [-0.03181196 -0.05338499 -0.04898433]
 after e[l]: [-0.00553393  0.03228755  0.04164097]
 after e[l]: [0.0389716  0.10145306 0.11105175]
 after e[l]: [-0.02150679 -0.05797541 -0.03252969]
 after e[l]: [-0.01155007 -0.04213713 -0.04460546]
 after e[l]: [-0.00406752  0.01077188 -0.02251595]
 after e[l]: [-0.01593336 -0.06351673 -0.07012281]
 after e[l]: [-0.04648046 -0.1371782  -0.09899043]
 after e[l]: [-0.03505366 -0.10895571 -0.07921354]
 after e[l]: [-0.02504245 -0.00779752  0.04371376]
 after e[l]: [ 0.01937917 -0.01671877 -0.00447229]
 after e[l]: [ 0.01317994 -0.02178693  0.01768486]
 after e[l]: [0.02744813 0.07703726 0.11365665]
 after e[l]: [0.07095636 0.11908201 0.12146001]
 after e[l]: [0.02180577 0.08171766 0.06456873]
 after e[l]: [0.0073239  0.07738429 0.07447263]
 after e[l]: [0.04598601 0.05762056 0.0238334 ]
 after e[l]: [ 0.01099098 -0.01794922  0.00750734]
 after e[l]: [0.0241856  0.06719844 0.04905885]
 after e[l]: [0.01978998 0.05186893 0.05366278]
 after e[l]: [0.0440548  0.04125741 0.05001027]
 after e[l]: [-0.01579486  0.0164631  -0.0017889 ]
 after e[l]: [-0.02030434 -0.0262711  -0.05359548]
 after e[l]: [ 0.03781225 -0.02609352 -0.08915953]
 after e[l]: [-0.00238723 -0.01552073  0.01221311]
 after e[l]: [0.04379775 0.02285762 0.0297631 ]
 after e[l]: [-0.00722721 -0.01877327  0.00675457]
 after e[l]: [-0.00793074 -0.04335941 -0.01562468]
 after e[l]: [-0.04321925 -0.07756343 -0.04788545]
 after e[l]: [-0.07601303 -0.10545862 -0.10259566]
 after e[l]: [0.03318693 0.02714672 0.01536221]
 after e[l]: [-0.00128323 -0.00736026 -0.01641373]
 after e[l]: [-0.06300531 -0.00869362 -0.04327327]
 after e[l]: [ 0.05750177 -0.01738435 -0.06870171]
 after e[l]: [ 0.04442924 -0.07156893 -0.03175588]
 after e[l]: [-0.00168909 -0.03766869 -0.00833699]
 after e[l]: [-0.03171571 -0.04852022 -0.02962456]
 after e[l]: [-0.00414745 -0.05334146 -0.07541149]
 after e[l]: [-0.04836054 -0.03062964 -0.03488077]
 after e[l]: [0.03040074 0.02368185 0.01590524]
 after e[l]: [ 0.00281828 -0.00778202 -0.02123742]
 after e[l]: [-0.05149208 -0.01261807 -0.0407961 ]
 after e[l]: [-0.02658012 -0.01292611  0.03853398]
 after e[l]: [ 0.01080747 -0.01530221 -0.00500082]
 after e[l]: [ 0.00864924 -0.02366138  0.01326171]
 after e[l]: [-0.01804799 -0.06337876 -0.0300138 ]
 after e[l]: [-0.01126975 -0.03897591 -0.04005831]
 after e[l]: [-0.00438335  0.01246473 -0.02564579]
 after e[l]: [ 0.04694002  0.00766122 -0.06003393]
 after e[l]: [ 0.09885344 -0.00530801 -0.07631337]
 after e[l]: [ 0.09853119  0.03595556 -0.00683482]
 after e[l]: [0.02714436 0.07344872 0.11365942]
 after e[l]: [0.07642758 0.12425461 0.12586568]
 after e[l]: [0.02235523 0.08289631 0.06365945]
 after e[l]: [ 0.06231669 -0.02404198 -0.07302666]
 after e[l]: [ 0.04534629 -0.07736134 -0.03626435]
 after e[l]: [-0.00781904 -0.03893549 -0.0155833 ]
 after e[l]: [-0.01457617 -0.06861881 -0.07015724]
 after e[l]: [-0.04491394 -0.13818865 -0.09712015]
 after e[l]: [-0.02985518 -0.11225001 -0.08190259]
 after e[l]: [-0.01351042 -0.0671872  -0.0672904 ]
 after e[l]: [-0.04970291 -0.13120149 -0.1006594 ]
 after e[l]: [-0.03678206 -0.10540424 -0.08169319]
 after e[l]: [-0.01883415  0.01546112  0.00301633]
 after e[l]: [-0.01579603 -0.02316155 -0.05182818]
 after e[l]: [ 0.03701662 -0.0207871  -0.08627198]
 after e[l]: [0.03533848 0.01747037 0.01137126]
 after e[l]: [ 0.00436043 -0.00577073 -0.02010319]
 after e[l]: [-0.05742493 -0.00824133 -0.03906213]
 after e[l]: [-0.00694923 -0.07012641 -0.06978375]
 after e[l]: [-0.04528973 -0.13665809 -0.097874  ]
 after e[l]: [-0.03510907 -0.1083821  -0.07770213]
 after e[l]: [-0.00384029 -0.04088054 -0.01611694]
 after e[l]: [-0.04537905 -0.07279044 -0.05725942]
 after e[l]: [-0.07674296 -0.1112422  -0.09995447]
 after e[l]: [-0.03409968  0.00121571 -0.01933667]
 after e[l]: [-0.01137829 -0.00193992 -0.00579169]
 after e[l]: [0.00455    0.02128651 0.00386136]
 after e[l]: [ 0.05348428  0.00583313 -0.0637641 ]
 after e[l]: [ 0.09383918 -0.01037824 -0.07239038]
 after e[l]: [ 0.09623874  0.0364706  -0.01242976]
 after e[l]: [-0.00248833 -0.0419145  -0.0195165 ]
 after e[l]: [-0.04974352 -0.07428456 -0.04476899]
 after e[l]: [-0.07498731 -0.10881643 -0.10217731]
 after e[l]: [0.02585297 0.07485161 0.11726359]
 after e[l]: [0.07246286 0.12440012 0.13243333]
 after e[l]: [0.02021062 0.08521806 0.06405515]
c:[[[ 0.02479656 -0.03851367 -0.06226046]
  [-0.01603816 -0.09355488 -0.0887109 ]
  [-0.0064286  -0.05362313 -0.0322369 ]]

 [[-0.00584249 -0.00550481  0.02447794]
  [-0.02113234 -0.01218665 -0.03200999]
  [-0.08019317 -0.068235   -0.06098996]]

 [[-0.03854929 -0.12550645 -0.087745  ]
  [-0.05402484 -0.1607078  -0.09782537]
  [ 0.01301311 -0.08153342 -0.03303304]]

 [[-0.03551772  0.00867765 -0.03681715]
  [ 0.00036004  0.00159791 -0.03098023]
  [ 0.02394387  0.0214964  -0.06279119]]

 [[ 0.08678456  0.01487486 -0.01984761]
  [ 0.05873878 -0.02468991 -0.047034  ]
  [ 0.07694921  0.02960984 -0.0469012 ]]

 [[-0.16028081 -0.13451903 -0.08137569]
  [-0.06439662 -0.09632582 -0.0267053 ]
  [ 0.02824397 -0.01447038  0.01459734]]

 [[ 0.00879113  0.05510482  0.03126122]
  [-0.01341211  0.03946173  0.04718447]
  [-0.03127599 -0.0351084   0.01156429]]

 [[ 0.02952015 -0.02446342 -0.03312956]
  [ 0.09369549  0.02611618 -0.00862086]
  [ 0.04742668  0.03374053  0.05836488]]

 [[ 0.00255813  0.04403391 -0.01727624]
  [-0.02361865 -0.01153813 -0.07295246]
  [-0.04196674 -0.01008568 -0.0414476 ]]

 [[ 0.0479231   0.05194515  0.07472469]
  [ 0.08491269  0.11719572  0.10403313]
  [ 0.07088474  0.10916523  0.10500995]]

 [[ 0.06801601  0.0450072   0.02934824]
  [ 0.07894969  0.06180174  0.01466668]
  [ 0.08384062  0.10340587  0.03277593]]

 [[-0.05140063 -0.05192247 -0.01809395]
  [ 0.01910392 -0.01663328  0.00689679]
  [ 0.05150362  0.08246635  0.0254399 ]]

 [[-0.0331457  -0.00503441 -0.01210599]
  [-0.06844865 -0.04451608 -0.00935025]
  [-0.08885156 -0.09785828 -0.11123365]]

 [[-0.05516019 -0.02708868 -0.06566223]
  [-0.01366542 -0.02638309 -0.04764867]
  [ 0.04129641 -0.01659261  0.0032252 ]]

 [[-0.09576204 -0.08548094 -0.06059   ]
  [-0.03377518 -0.05064173  0.00679474]
  [ 0.02520737  0.02927666  0.08152828]]

 [[ 0.05268725  0.02521077  0.04342666]
  [ 0.04267218  0.02229311  0.01197628]
  [-0.0437956  -0.02499178 -0.04419562]]

 [[-0.05339498 -0.029933   -0.06570331]
  [-0.01272201 -0.03719316 -0.04882925]
  [ 0.03808161 -0.02054952  0.00416266]]

 [[ 0.00919527  0.06139851  0.03406667]
  [-0.0108507   0.03792385  0.04153961]
  [-0.03313447 -0.03353931  0.0042958 ]]

 [[ 0.0883409   0.01366237 -0.02420988]
  [ 0.0582797  -0.01860439 -0.05155545]
  [ 0.08158396  0.02859112 -0.04574985]]

 [[-0.04766953 -0.12559436 -0.09293988]
  [-0.05382693 -0.16236384 -0.10204368]
  [ 0.02143503 -0.08359111 -0.03395355]]

 [[ 0.03253288 -0.02741994 -0.03196787]
  [ 0.09997123  0.01760646 -0.00689668]
  [ 0.04692236  0.04288492  0.06169732]]

 [[-0.0923144  -0.07944173 -0.0598174 ]
  [-0.03181377 -0.0531292   0.0048761 ]
  [ 0.01961024  0.0237167   0.076647  ]]

 [[-0.15830824 -0.12916835 -0.08339576]
  [-0.05781591 -0.1025681  -0.02285067]
  [ 0.02392675 -0.01467014  0.00921225]]

 [[-0.16395952 -0.136233   -0.09113175]
  [-0.05753316 -0.09941757 -0.02150846]
  [ 0.03045946 -0.0156665   0.00969907]]

 [[ 0.06530713  0.04456041  0.03724634]
  [ 0.08802938  0.05756892  0.01319094]
  [ 0.082507    0.1109262   0.02834931]]

 [[-0.05068664 -0.02950671 -0.06450809]
  [-0.00809279 -0.03781413 -0.05122355]
  [ 0.04659479 -0.02233632  0.00384303]]

 [[-0.16386501 -0.12975512 -0.08017521]
  [-0.05454445 -0.10203206 -0.01902056]
  [ 0.0234071  -0.01358741  0.01465003]]

 [[-0.03394824  0.00043498 -0.0040595 ]
  [-0.07400726 -0.04675815 -0.01196907]
  [-0.09068063 -0.09953674 -0.10946159]]

 [[-0.00664685 -0.0056914   0.02106899]
  [-0.01994318 -0.00620945 -0.03069422]
  [-0.08075904 -0.06826691 -0.06348066]]

 [[-0.04494568 -0.12303513 -0.08400742]
  [-0.05327779 -0.15402605 -0.10287175]
  [ 0.0179377  -0.08783123 -0.03378732]]

 [[-0.03490395 -0.00315308 -0.01376034]
  [-0.0727592  -0.04684569 -0.01345599]
  [-0.09003365 -0.09456849 -0.11440997]]

 [[ 0.02818923 -0.02837723 -0.03656993]
  [ 0.10355627  0.01935012 -0.00878735]
  [ 0.04704532  0.03710375  0.06022146]]]
 after e[l]: [ 0.01239828 -0.01925683 -0.03113023]
 after e[l]: [-0.00801908 -0.04677744 -0.04435545]
 after e[l]: [-0.0032143  -0.02681157 -0.01611845]
 after e[l]: [-0.00292125 -0.00275241  0.01223897]
 after e[l]: [-0.01056617 -0.00609333 -0.01600499]
 after e[l]: [-0.04009658 -0.0341175  -0.03049498]
 after e[l]: [-0.01927464 -0.06275322 -0.0438725 ]
 after e[l]: [-0.02701242 -0.0803539  -0.04891269]
 after e[l]: [ 0.00650656 -0.04076671 -0.01651652]
 after e[l]: [-0.01775886  0.00433883 -0.01840857]
 after e[l]: [ 0.00018002  0.00079896 -0.01549012]
 after e[l]: [ 0.01197193  0.0107482  -0.0313956 ]
 after e[l]: [ 0.04339228  0.00743743 -0.00992381]
 after e[l]: [ 0.02936939 -0.01234496 -0.023517  ]
 after e[l]: [ 0.0384746   0.01480492 -0.0234506 ]
 after e[l]: [-0.0801404  -0.06725951 -0.04068784]
 after e[l]: [-0.03219831 -0.04816291 -0.01335265]
 after e[l]: [ 0.01412199 -0.00723519  0.00729867]
 after e[l]: [0.00439556 0.02755241 0.01563061]
 after e[l]: [-0.00670605  0.01973086  0.02359223]
 after e[l]: [-0.015638   -0.0175542   0.00578214]
 after e[l]: [ 0.01476007 -0.01223171 -0.01656478]
 after e[l]: [ 0.04684775  0.01305809 -0.00431043]
 after e[l]: [0.02371334 0.01687026 0.02918244]
 after e[l]: [ 0.00127906  0.02201695 -0.00863812]
 after e[l]: [-0.01180932 -0.00576907 -0.03647623]
 after e[l]: [-0.02098337 -0.00504284 -0.0207238 ]
 after e[l]: [0.02396155 0.02597258 0.03736234]
 after e[l]: [0.04245635 0.05859786 0.05201657]
 after e[l]: [0.03544237 0.05458261 0.05250498]
 after e[l]: [0.034008   0.0225036  0.01467412]
 after e[l]: [0.03947484 0.03090087 0.00733334]
 after e[l]: [0.04192031 0.05170294 0.01638796]
 after e[l]: [-0.02570032 -0.02596124 -0.00904697]
 after e[l]: [ 0.00955196 -0.00831664  0.0034484 ]
 after e[l]: [0.02575181 0.04123317 0.01271995]
 after e[l]: [-0.01657285 -0.00251721 -0.00605299]
 after e[l]: [-0.03422432 -0.02225804 -0.00467513]
 after e[l]: [-0.04442578 -0.04892914 -0.05561683]
 after e[l]: [-0.02758009 -0.01354434 -0.03283111]
 after e[l]: [-0.00683271 -0.01319154 -0.02382433]
 after e[l]: [ 0.0206482 -0.0082963  0.0016126]
 after e[l]: [-0.04788102 -0.04274047 -0.030295  ]
 after e[l]: [-0.01688759 -0.02532087  0.00339737]
 after e[l]: [0.01260368 0.01463833 0.04076414]
 after e[l]: [0.02634363 0.01260539 0.02171333]
 after e[l]: [0.02133609 0.01114655 0.00598814]
 after e[l]: [-0.0218978  -0.01249589 -0.02209781]
 after e[l]: [-0.02669749 -0.0149665  -0.03285166]
 after e[l]: [-0.00636101 -0.01859658 -0.02441462]
 after e[l]: [ 0.01904081 -0.01027476  0.00208133]
 after e[l]: [0.00459763 0.03069925 0.01703333]
 after e[l]: [-0.00542535  0.01896193  0.0207698 ]
 after e[l]: [-0.01656724 -0.01676966  0.0021479 ]
 after e[l]: [ 0.04417045  0.00683118 -0.01210494]
 after e[l]: [ 0.02913985 -0.00930219 -0.02577773]
 after e[l]: [ 0.04079198  0.01429556 -0.02287493]
 after e[l]: [-0.02383477 -0.06279718 -0.04646994]
 after e[l]: [-0.02691347 -0.08118192 -0.05102184]
 after e[l]: [ 0.01071752 -0.04179556 -0.01697678]
 after e[l]: [ 0.01626644 -0.01370997 -0.01598393]
 after e[l]: [ 0.04998562  0.00880323 -0.00344834]
 after e[l]: [0.02346118 0.02144246 0.03084866]
 after e[l]: [-0.0461572  -0.03972087 -0.0299087 ]
 after e[l]: [-0.01590688 -0.0265646   0.00243805]
 after e[l]: [0.00980512 0.01185835 0.0383235 ]
 after e[l]: [-0.07915412 -0.06458417 -0.04169788]
 after e[l]: [-0.02890795 -0.05128405 -0.01142533]
 after e[l]: [ 0.01196337 -0.00733507  0.00460612]
 after e[l]: [-0.08197976 -0.0681165  -0.04556587]
 after e[l]: [-0.02876658 -0.04970879 -0.01075423]
 after e[l]: [ 0.01522973 -0.00783325  0.00484954]
 after e[l]: [0.03265357 0.02228021 0.01862317]
 after e[l]: [0.04401469 0.02878446 0.00659547]
 after e[l]: [0.0412535  0.0554631  0.01417466]
 after e[l]: [-0.02534332 -0.01475336 -0.03225404]
 after e[l]: [-0.00404639 -0.01890707 -0.02561177]
 after e[l]: [ 0.0232974  -0.01116816  0.00192151]
 after e[l]: [-0.08193251 -0.06487756 -0.0400876 ]
 after e[l]: [-0.02727222 -0.05101603 -0.00951028]
 after e[l]: [ 0.01170355 -0.0067937   0.00732501]
 after e[l]: [-0.01697412  0.00021749 -0.00202975]
 after e[l]: [-0.03700363 -0.02337908 -0.00598454]
 after e[l]: [-0.04534031 -0.04976837 -0.0547308 ]
 after e[l]: [-0.00332343 -0.0028457   0.01053449]
 after e[l]: [-0.00997159 -0.00310472 -0.01534711]
 after e[l]: [-0.04037952 -0.03413346 -0.03174033]
 after e[l]: [-0.02247284 -0.06151756 -0.04200371]
 after e[l]: [-0.0266389  -0.07701302 -0.05143587]
 after e[l]: [ 0.00896885 -0.04391561 -0.01689366]
 after e[l]: [-0.01745198 -0.00157654 -0.00688017]
 after e[l]: [-0.0363796  -0.02342285 -0.00672799]
 after e[l]: [-0.04501683 -0.04728425 -0.05720498]
 after e[l]: [ 0.01409462 -0.01418862 -0.01828496]
 after e[l]: [ 0.05177813  0.00967506 -0.00439367]
 after e[l]: [0.02352266 0.01855188 0.03011073]
c:[[[ 7.47824972e-03  2.24484429e-02  1.39211314e-02]
  [-6.51976317e-02 -3.54723781e-02 -1.90804377e-02]
  [-7.60832354e-02 -6.53064921e-02 -1.45177497e-02]]

 [[-8.00515264e-02 -5.54058328e-02 -1.12140691e-03]
  [-6.04197420e-02 -6.72033988e-03  4.75855991e-02]
  [-1.38227260e-02  1.95768494e-02  5.32209538e-02]]

 [[ 5.43327965e-02 -1.16692167e-02 -5.14507927e-02]
  [ 4.78273742e-02  1.08699622e-02 -4.47524413e-02]
  [-3.43915522e-02  9.57483146e-03  5.89185543e-02]]

 [[ 1.32934442e-02  7.99371377e-02  3.84388678e-02]
  [-1.73308533e-02  3.29581760e-02  3.53081413e-02]
  [ 1.46614632e-03 -3.21080200e-02 -3.03094164e-02]]

 [[ 3.81188293e-04 -2.37785932e-02 -6.23567700e-02]
  [ 9.95304212e-02  9.40749422e-02  5.91604374e-02]
  [ 4.20068316e-02  9.94895697e-02  6.00643158e-02]]

 [[ 2.04139086e-03  5.92125952e-02 -1.78834889e-02]
  [ 3.09498161e-02  1.00433141e-01  3.52785736e-02]
  [ 4.80534881e-02  1.11347064e-01  1.02501765e-01]]

 [[ 5.89378662e-02  2.81675179e-02 -4.43652505e-03]
  [ 1.86677687e-02  4.03903723e-02 -4.20319708e-03]
  [-6.20776378e-02 -2.17111502e-02 -1.51595902e-02]]

 [[ 7.92728141e-02  7.41501004e-02  2.32465602e-02]
  [ 7.47848302e-02  5.61231412e-02  2.53937449e-02]
  [ 9.74338781e-03  1.29627949e-02  8.50234472e-04]]

 [[-8.52848589e-03  8.42609908e-03 -2.48042513e-02]
  [ 9.39082056e-02  1.07688800e-01  1.33073973e-02]
  [ 8.18796530e-02  1.58650294e-01  7.73120150e-02]]

 [[ 2.20123213e-02  3.03630829e-02 -8.73849764e-02]
  [ 8.60373750e-02  1.22345097e-01 -2.27099489e-02]
  [ 7.33780712e-02  1.12530380e-01  4.43918183e-02]]

 [[ 3.45891975e-02  3.61709557e-02  6.68748543e-02]
  [ 7.02702925e-02  7.14816004e-02  9.36167613e-02]
  [-5.42259729e-03  7.67487288e-02  2.56341044e-02]]

 [[-1.68038551e-02  7.35429861e-03  5.96028045e-02]
  [-4.96735312e-02 -4.51412834e-02 -1.19263427e-02]
  [ 2.75401846e-02 -1.46859605e-02 -4.53067198e-02]]

 [[-6.96547404e-02 -8.44275579e-03  1.44275560e-04]
  [ 8.70806258e-03  3.91078778e-02  1.48182809e-02]
  [ 3.80448997e-02  6.41072467e-02  8.01601410e-02]]

 [[-5.53402230e-02 -5.31625636e-02 -4.51458804e-02]
  [ 2.44657043e-02 -5.58171188e-03 -4.04038429e-02]
  [ 6.00440651e-02  4.56518978e-02 -1.98556781e-02]]

 [[-2.32927762e-02 -4.80202287e-02  1.15000727e-02]
  [-7.66769201e-02 -8.62437189e-02 -5.00579849e-02]
  [ 2.16250704e-03 -1.22186644e-02 -1.08628850e-02]]

 [[ 1.95292942e-02 -1.35538867e-02  1.10185100e-02]
  [ 4.71848622e-03 -3.20931263e-02 -2.11772807e-02]
  [-2.25434490e-02 -3.19359154e-02 -2.07143743e-02]]

 [[-5.21740057e-02 -4.86493483e-02 -4.96629477e-02]
  [ 2.71096919e-02  6.70209556e-05 -3.26690748e-02]
  [ 5.91836125e-02  4.63067144e-02 -1.88272875e-02]]

 [[ 4.93250601e-02  2.88717169e-02 -5.75096579e-03]
  [ 1.72042158e-02  4.05595489e-02 -4.22169548e-03]
  [-6.14120588e-02 -2.22122036e-02 -1.81775782e-02]]

 [[-4.03916277e-03 -2.20970903e-02 -6.28270060e-02]
  [ 1.00114740e-01  9.60303769e-02  5.60629852e-02]
  [ 4.30389829e-02  1.02451839e-01  5.52371293e-02]]

 [[ 5.84931336e-02 -8.65564030e-03 -5.22312000e-02]
  [ 4.67371866e-02  3.57245747e-03 -4.07393351e-02]
  [-2.87731886e-02  8.15519597e-03  5.75463846e-02]]

 [[ 8.29110965e-02  7.57976547e-02  2.73119994e-02]
  [ 7.08451942e-02  5.98314479e-02  2.31115557e-02]
  [ 1.33156693e-02  1.50623284e-02  1.77888328e-03]]

 [[-1.32390745e-02 -5.01030013e-02  8.14062357e-03]
  [-7.58848339e-02 -8.11770856e-02 -5.50397709e-02]
  [ 5.24169626e-03 -1.55450664e-02 -1.24525642e-02]]

 [[ 6.10793382e-03  6.34692907e-02 -1.42821325e-02]
  [ 2.66654640e-02  9.75876451e-02  3.10788471e-02]
  [ 4.85353321e-02  1.12592161e-01  1.05376847e-01]]

 [[ 7.33965309e-03  5.84686212e-02 -1.50167393e-02]
  [ 2.35590264e-02  9.67238545e-02  3.55012044e-02]
  [ 5.14842197e-02  1.09192662e-01  1.06410474e-01]]

 [[ 2.58678142e-02  4.01354805e-02  6.67158514e-02]
  [ 6.73938468e-02  7.34213516e-02  9.75903124e-02]
  [-7.80853257e-03  6.78604320e-02  2.87227836e-02]]

 [[-5.83320968e-02 -4.67266142e-02 -4.47971001e-02]
  [ 2.79884599e-02  2.12328765e-03 -4.16358188e-02]
  [ 6.30251616e-02  4.60520498e-02 -1.70704890e-02]]

 [[ 3.52345756e-03  5.69339395e-02 -1.28179956e-02]
  [ 2.59378739e-02  1.04294069e-01  3.43699604e-02]
  [ 4.54336777e-02  1.10378832e-01  1.09936081e-01]]

 [[-6.96946830e-02 -1.17345629e-02 -1.13161525e-03]
  [ 5.91604039e-03  3.82110775e-02  1.16906781e-02]
  [ 4.03983593e-02  6.25754818e-02  8.08624923e-02]]

 [[-8.31052810e-02 -5.52781075e-02 -2.34381645e-04]
  [-6.01824336e-02 -6.02130825e-03  4.71413508e-02]
  [-9.74359922e-03  1.22006787e-02  5.19771986e-02]]

 [[ 5.73721342e-02 -1.07124038e-02 -5.17725982e-02]
  [ 4.61003333e-02  1.12680448e-02 -3.99693586e-02]
  [-2.84600444e-02  9.63159744e-03  5.61605729e-02]]

 [[-6.58910573e-02 -1.74243040e-02 -8.60659871e-04]
  [ 3.89272044e-03  4.23984379e-02  1.94870699e-02]
  [ 3.98690253e-02  5.99068552e-02  8.66653547e-02]]

 [[ 8.56102258e-02  7.37700388e-02  2.52103284e-02]
  [ 6.98898360e-02  6.01338446e-02  2.53050383e-02]
  [ 1.11820716e-02  9.02403798e-03  4.35793726e-03]]]
Traceback (most recent call last):
  File "main.py", line 965, in <module>
    main()
  File "main.py", line 558, in main
    model = model.wider(3, 2, out_size=None, weight_norm=None, random_init=False, addNoise=True)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 883, in wider
    f[m] = f[m] / ct.get(listindices[idx])
TypeError: unsupported operand type(s) for /: 'float' and 'NoneType'
j: 181 bis 185
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5140
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 176
Max memory: 0.1097216
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:176/180; Lr: 0.0010000000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [176][0/196]	Time 0.161 (0.161)	Data 0.265 (0.265)	Loss 0.1080 (0.1080)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [176][64/196]	Time 0.084 (0.088)	Data 0.000 (0.004)	Loss 0.0567 (0.0772)	Acc@1 97.266 (97.464)	Acc@5 100.000 (99.988)
Epoch: [176][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.1001 (0.0786)	Acc@1 95.703 (97.444)	Acc@5 100.000 (99.985)
Epoch: [176][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0952 (0.0791)	Acc@1 96.484 (97.426)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:177/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [177][0/196]	Time 0.130 (0.130)	Data 0.267 (0.267)	Loss 0.0548 (0.0548)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [177][64/196]	Time 0.087 (0.090)	Data 0.000 (0.004)	Loss 0.0776 (0.0781)	Acc@1 96.875 (97.362)	Acc@5 100.000 (99.976)
Epoch: [177][128/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.0885 (0.0791)	Acc@1 96.875 (97.402)	Acc@5 99.609 (99.979)
Epoch: [177][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0612 (0.0790)	Acc@1 98.047 (97.377)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:178/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [178][0/196]	Time 0.120 (0.120)	Data 0.321 (0.321)	Loss 0.0820 (0.0820)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [178][64/196]	Time 0.098 (0.090)	Data 0.000 (0.005)	Loss 0.0678 (0.0791)	Acc@1 98.438 (97.608)	Acc@5 100.000 (99.988)
Epoch: [178][128/196]	Time 0.100 (0.089)	Data 0.000 (0.003)	Loss 0.1272 (0.0801)	Acc@1 95.703 (97.502)	Acc@5 100.000 (99.982)
Epoch: [178][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0919 (0.0801)	Acc@1 97.266 (97.462)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:179/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [179][0/196]	Time 0.135 (0.135)	Data 0.301 (0.301)	Loss 0.0709 (0.0709)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [179][64/196]	Time 0.109 (0.088)	Data 0.000 (0.005)	Loss 0.0844 (0.0761)	Acc@1 96.875 (97.596)	Acc@5 100.000 (99.982)
Epoch: [179][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.1327 (0.0784)	Acc@1 95.703 (97.508)	Acc@5 100.000 (99.982)
Epoch: [179][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0745 (0.0788)	Acc@1 97.656 (97.468)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:180/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [180][0/196]	Time 0.153 (0.153)	Data 0.276 (0.276)	Loss 0.0658 (0.0658)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [180][64/196]	Time 0.092 (0.089)	Data 0.000 (0.004)	Loss 0.0616 (0.0750)	Acc@1 98.047 (97.638)	Acc@5 100.000 (99.982)
Epoch: [180][128/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0924 (0.0754)	Acc@1 97.656 (97.644)	Acc@5 100.000 (99.973)
Epoch: [180][192/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.0969 (0.0778)	Acc@1 98.047 (97.519)	Acc@5 99.609 (99.978)
Max memory in training epoch: 33.3541888
Model: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
[INFO] Storing checkpoint...
  90.24
Max memory: 51.4381312
 17.335s  j: 186 bis 190
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3292
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 181
Max memory: 0.1097216
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:181/185; Lr: 0.0009000000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [181][0/196]	Time 0.166 (0.166)	Data 0.293 (0.293)	Loss 0.0577 (0.0577)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [181][64/196]	Time 0.092 (0.091)	Data 0.000 (0.005)	Loss 0.0450 (0.0772)	Acc@1 98.828 (97.410)	Acc@5 100.000 (99.988)
Epoch: [181][128/196]	Time 0.085 (0.090)	Data 0.000 (0.002)	Loss 0.1120 (0.0783)	Acc@1 96.094 (97.363)	Acc@5 100.000 (99.988)
Epoch: [181][192/196]	Time 0.078 (0.089)	Data 0.000 (0.002)	Loss 0.0637 (0.0785)	Acc@1 98.047 (97.413)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:182/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [182][0/196]	Time 0.122 (0.122)	Data 0.311 (0.311)	Loss 0.0804 (0.0804)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [182][64/196]	Time 0.101 (0.090)	Data 0.000 (0.005)	Loss 0.0949 (0.0741)	Acc@1 96.875 (97.674)	Acc@5 100.000 (99.988)
Epoch: [182][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0636 (0.0733)	Acc@1 99.219 (97.744)	Acc@5 100.000 (99.979)
Epoch: [182][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0699 (0.0761)	Acc@1 98.438 (97.589)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:183/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [183][0/196]	Time 0.134 (0.134)	Data 0.306 (0.306)	Loss 0.0483 (0.0483)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [183][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.0886 (0.0749)	Acc@1 96.484 (97.596)	Acc@5 100.000 (99.988)
Epoch: [183][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.1050 (0.0774)	Acc@1 98.047 (97.469)	Acc@5 99.609 (99.985)
Epoch: [183][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0459 (0.0778)	Acc@1 99.219 (97.436)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:184/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [184][0/196]	Time 0.138 (0.138)	Data 0.259 (0.259)	Loss 0.0586 (0.0586)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [184][64/196]	Time 0.085 (0.090)	Data 0.000 (0.004)	Loss 0.0455 (0.0722)	Acc@1 98.047 (97.608)	Acc@5 100.000 (99.994)
Epoch: [184][128/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.0875 (0.0759)	Acc@1 96.484 (97.490)	Acc@5 100.000 (99.991)
Epoch: [184][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0874 (0.0763)	Acc@1 96.484 (97.492)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:185/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [185][0/196]	Time 0.117 (0.117)	Data 0.332 (0.332)	Loss 0.0491 (0.0491)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [185][64/196]	Time 0.094 (0.087)	Data 0.000 (0.005)	Loss 0.1078 (0.0710)	Acc@1 95.703 (97.704)	Acc@5 100.000 (99.988)
Epoch: [185][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0948 (0.0736)	Acc@1 97.266 (97.641)	Acc@5 100.000 (99.991)
Epoch: [185][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0958 (0.0750)	Acc@1 96.484 (97.587)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.38
Max memory: 51.4381312
 17.547s  j: 191 bis 195
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5984
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 186
Max memory: 0.1097216
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:186/190; Lr: 0.0008100000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [186][0/196]	Time 0.143 (0.143)	Data 0.302 (0.302)	Loss 0.0714 (0.0714)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [186][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0581 (0.0787)	Acc@1 98.047 (97.422)	Acc@5 100.000 (99.976)
Epoch: [186][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0998 (0.0752)	Acc@1 96.875 (97.602)	Acc@5 100.000 (99.979)
Epoch: [186][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0734 (0.0745)	Acc@1 98.828 (97.610)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:187/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [187][0/196]	Time 0.125 (0.125)	Data 0.318 (0.318)	Loss 0.0942 (0.0942)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [187][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.1116 (0.0717)	Acc@1 96.484 (97.656)	Acc@5 100.000 (99.976)
Epoch: [187][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.0930 (0.0746)	Acc@1 96.875 (97.596)	Acc@5 100.000 (99.973)
Epoch: [187][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0656 (0.0764)	Acc@1 98.047 (97.509)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:188/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [188][0/196]	Time 0.123 (0.123)	Data 0.285 (0.285)	Loss 0.0661 (0.0661)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [188][64/196]	Time 0.086 (0.087)	Data 0.000 (0.005)	Loss 0.0751 (0.0708)	Acc@1 98.438 (97.758)	Acc@5 100.000 (99.988)
Epoch: [188][128/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0504 (0.0720)	Acc@1 98.828 (97.765)	Acc@5 100.000 (99.988)
Epoch: [188][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0771 (0.0742)	Acc@1 97.266 (97.670)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:189/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [189][0/196]	Time 0.131 (0.131)	Data 0.303 (0.303)	Loss 0.0755 (0.0755)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [189][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.0650 (0.0745)	Acc@1 98.047 (97.536)	Acc@5 100.000 (99.982)
Epoch: [189][128/196]	Time 0.080 (0.088)	Data 0.000 (0.003)	Loss 0.0645 (0.0733)	Acc@1 98.828 (97.611)	Acc@5 100.000 (99.988)
Epoch: [189][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0946 (0.0726)	Acc@1 97.656 (97.654)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:190/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [190][0/196]	Time 0.126 (0.126)	Data 0.328 (0.328)	Loss 0.0520 (0.0520)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [190][64/196]	Time 0.097 (0.089)	Data 0.000 (0.005)	Loss 0.0622 (0.0731)	Acc@1 98.438 (97.590)	Acc@5 100.000 (99.982)
Epoch: [190][128/196]	Time 0.081 (0.088)	Data 0.000 (0.003)	Loss 0.1118 (0.0732)	Acc@1 96.094 (97.605)	Acc@5 99.609 (99.979)
Epoch: [190][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.1078 (0.0740)	Acc@1 95.312 (97.602)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.15
Max memory: 51.4381312
 17.616s  j: 196 bis 200
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8082
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 191
Max memory: 0.1097216
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:191/195; Lr: 0.0007290000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [191][0/196]	Time 0.151 (0.151)	Data 0.302 (0.302)	Loss 0.0615 (0.0615)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [191][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.0436 (0.0731)	Acc@1 98.828 (97.644)	Acc@5 100.000 (99.994)
Epoch: [191][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.0637 (0.0713)	Acc@1 98.438 (97.680)	Acc@5 100.000 (99.991)
Epoch: [191][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0752 (0.0719)	Acc@1 97.656 (97.679)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:192/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [192][0/196]	Time 0.125 (0.125)	Data 0.309 (0.309)	Loss 0.0639 (0.0639)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [192][64/196]	Time 0.090 (0.089)	Data 0.000 (0.005)	Loss 0.0674 (0.0697)	Acc@1 97.656 (97.770)	Acc@5 100.000 (99.970)
Epoch: [192][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0822 (0.0714)	Acc@1 96.875 (97.674)	Acc@5 100.000 (99.979)
Epoch: [192][192/196]	Time 0.094 (0.089)	Data 0.000 (0.002)	Loss 0.1115 (0.0723)	Acc@1 96.484 (97.672)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:193/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [193][0/196]	Time 0.107 (0.107)	Data 0.323 (0.323)	Loss 0.0726 (0.0726)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [193][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.0851 (0.0744)	Acc@1 98.047 (97.584)	Acc@5 100.000 (99.994)
Epoch: [193][128/196]	Time 0.091 (0.089)	Data 0.000 (0.003)	Loss 0.0676 (0.0741)	Acc@1 97.266 (97.502)	Acc@5 100.000 (99.988)
Epoch: [193][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.0744 (0.0729)	Acc@1 97.656 (97.577)	Acc@5 99.609 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:194/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [194][0/196]	Time 0.124 (0.124)	Data 0.302 (0.302)	Loss 0.0521 (0.0521)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [194][64/196]	Time 0.097 (0.090)	Data 0.000 (0.005)	Loss 0.1052 (0.0718)	Acc@1 95.312 (97.800)	Acc@5 100.000 (99.976)
Epoch: [194][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.1075 (0.0727)	Acc@1 95.703 (97.702)	Acc@5 100.000 (99.982)
Epoch: [194][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.1186 (0.0729)	Acc@1 95.703 (97.672)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:195/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [195][0/196]	Time 0.111 (0.111)	Data 0.344 (0.344)	Loss 0.0639 (0.0639)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [195][64/196]	Time 0.086 (0.090)	Data 0.000 (0.006)	Loss 0.0638 (0.0697)	Acc@1 98.438 (97.939)	Acc@5 100.000 (99.982)
Epoch: [195][128/196]	Time 0.083 (0.089)	Data 0.000 (0.003)	Loss 0.0977 (0.0685)	Acc@1 96.484 (97.950)	Acc@5 100.000 (99.988)
Epoch: [195][192/196]	Time 0.094 (0.089)	Data 0.000 (0.002)	Loss 0.0649 (0.0684)	Acc@1 97.656 (97.901)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.38
Max memory: 51.4381312
 17.793s  j: 201 bis 205
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7003
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 196
Max memory: 0.1097216
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:196/200; Lr: 0.0006561000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [196][0/196]	Time 0.142 (0.142)	Data 0.268 (0.268)	Loss 0.0630 (0.0630)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [196][64/196]	Time 0.103 (0.089)	Data 0.000 (0.004)	Loss 0.0776 (0.0699)	Acc@1 96.094 (97.722)	Acc@5 100.000 (99.982)
Epoch: [196][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0758 (0.0713)	Acc@1 98.047 (97.717)	Acc@5 100.000 (99.982)
Epoch: [196][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0560 (0.0703)	Acc@1 98.438 (97.741)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:197/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [197][0/196]	Time 0.122 (0.122)	Data 0.353 (0.353)	Loss 0.0505 (0.0505)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [197][64/196]	Time 0.086 (0.089)	Data 0.000 (0.006)	Loss 0.0872 (0.0702)	Acc@1 97.266 (97.686)	Acc@5 100.000 (99.976)
Epoch: [197][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0495 (0.0704)	Acc@1 98.828 (97.723)	Acc@5 100.000 (99.976)
Epoch: [197][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0690 (0.0703)	Acc@1 97.656 (97.735)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:198/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [198][0/196]	Time 0.123 (0.123)	Data 0.294 (0.294)	Loss 0.0463 (0.0463)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [198][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0471 (0.0712)	Acc@1 98.828 (97.800)	Acc@5 100.000 (99.976)
Epoch: [198][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.0559 (0.0713)	Acc@1 98.047 (97.753)	Acc@5 100.000 (99.973)
Epoch: [198][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0687 (0.0716)	Acc@1 97.656 (97.755)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:199/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [199][0/196]	Time 0.144 (0.144)	Data 0.313 (0.313)	Loss 0.0503 (0.0503)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [199][64/196]	Time 0.086 (0.087)	Data 0.000 (0.005)	Loss 0.0836 (0.0686)	Acc@1 97.656 (97.794)	Acc@5 100.000 (100.000)
Epoch: [199][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.0760 (0.0706)	Acc@1 97.266 (97.747)	Acc@5 100.000 (99.991)
Epoch: [199][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0510 (0.0704)	Acc@1 98.828 (97.755)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:200/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [200][0/196]	Time 0.109 (0.109)	Data 0.298 (0.298)	Loss 0.0842 (0.0842)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [200][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.0694 (0.0720)	Acc@1 98.047 (97.662)	Acc@5 99.609 (99.964)
Epoch: [200][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0754 (0.0698)	Acc@1 97.656 (97.720)	Acc@5 100.000 (99.979)
Epoch: [200][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0838 (0.0704)	Acc@1 96.875 (97.705)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.24
Max memory: 51.4381312
 17.545s  j: 206 bis 210
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1693
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 201
Max memory: 0.1097216
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:201/205; Lr: 0.0005904900000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [201][0/196]	Time 0.159 (0.159)	Data 0.276 (0.276)	Loss 0.0488 (0.0488)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [201][64/196]	Time 0.086 (0.089)	Data 0.000 (0.004)	Loss 0.0467 (0.0711)	Acc@1 98.828 (97.614)	Acc@5 100.000 (99.982)
Epoch: [201][128/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.0907 (0.0691)	Acc@1 96.875 (97.835)	Acc@5 100.000 (99.982)
Epoch: [201][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.0414 (0.0691)	Acc@1 99.219 (97.814)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:202/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [202][0/196]	Time 0.121 (0.121)	Data 0.361 (0.361)	Loss 0.0992 (0.0992)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [202][64/196]	Time 0.089 (0.088)	Data 0.000 (0.006)	Loss 0.0590 (0.0684)	Acc@1 97.656 (97.825)	Acc@5 100.000 (99.970)
Epoch: [202][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0685 (0.0696)	Acc@1 97.266 (97.829)	Acc@5 100.000 (99.976)
Epoch: [202][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0484 (0.0697)	Acc@1 98.438 (97.802)	Acc@5 100.000 (99.968)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:203/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [203][0/196]	Time 0.117 (0.117)	Data 0.293 (0.293)	Loss 0.0644 (0.0644)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [203][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.0860 (0.0655)	Acc@1 97.266 (97.897)	Acc@5 100.000 (99.982)
Epoch: [203][128/196]	Time 0.101 (0.087)	Data 0.000 (0.002)	Loss 0.0751 (0.0661)	Acc@1 97.266 (97.914)	Acc@5 100.000 (99.979)
Epoch: [203][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.0637 (0.0673)	Acc@1 98.438 (97.867)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:204/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [204][0/196]	Time 0.121 (0.121)	Data 0.306 (0.306)	Loss 0.0617 (0.0617)	Acc@1 99.219 (99.219)	Acc@5 99.609 (99.609)
Epoch: [204][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.0622 (0.0692)	Acc@1 97.656 (97.716)	Acc@5 100.000 (99.976)
Epoch: [204][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.0728 (0.0689)	Acc@1 97.656 (97.744)	Acc@5 100.000 (99.985)
Epoch: [204][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0436 (0.0690)	Acc@1 98.828 (97.764)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:205/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [205][0/196]	Time 0.135 (0.135)	Data 0.336 (0.336)	Loss 0.0587 (0.0587)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [205][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0773 (0.0631)	Acc@1 97.656 (98.059)	Acc@5 100.000 (99.982)
Epoch: [205][128/196]	Time 0.080 (0.088)	Data 0.000 (0.003)	Loss 0.0530 (0.0649)	Acc@1 98.828 (97.935)	Acc@5 100.000 (99.982)
Epoch: [205][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0732 (0.0668)	Acc@1 96.484 (97.873)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.38
Max memory: 51.4381312
 17.601s  j: 211 bis 215
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2043
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 206
Max memory: 0.1097216
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:206/210; Lr: 0.0005314410000000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [206][0/196]	Time 0.151 (0.151)	Data 0.285 (0.285)	Loss 0.0526 (0.0526)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [206][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.0932 (0.0637)	Acc@1 95.703 (98.071)	Acc@5 100.000 (99.988)
Epoch: [206][128/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.1092 (0.0659)	Acc@1 96.875 (97.992)	Acc@5 100.000 (99.982)
Epoch: [206][192/196]	Time 0.094 (0.087)	Data 0.000 (0.002)	Loss 0.0746 (0.0664)	Acc@1 97.266 (97.940)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:207/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [207][0/196]	Time 0.110 (0.110)	Data 0.297 (0.297)	Loss 0.1137 (0.1137)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [207][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0594 (0.0673)	Acc@1 98.047 (97.812)	Acc@5 100.000 (99.964)
Epoch: [207][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.0960 (0.0678)	Acc@1 97.656 (97.856)	Acc@5 100.000 (99.970)
Epoch: [207][192/196]	Time 0.081 (0.086)	Data 0.000 (0.002)	Loss 0.0648 (0.0674)	Acc@1 97.656 (97.863)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:208/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [208][0/196]	Time 0.114 (0.114)	Data 0.302 (0.302)	Loss 0.0428 (0.0428)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [208][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.0447 (0.0690)	Acc@1 98.438 (97.837)	Acc@5 100.000 (99.982)
Epoch: [208][128/196]	Time 0.077 (0.087)	Data 0.000 (0.003)	Loss 0.0793 (0.0672)	Acc@1 98.047 (97.862)	Acc@5 100.000 (99.982)
Epoch: [208][192/196]	Time 0.091 (0.086)	Data 0.000 (0.002)	Loss 0.0571 (0.0683)	Acc@1 98.438 (97.836)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:209/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [209][0/196]	Time 0.119 (0.119)	Data 0.336 (0.336)	Loss 0.0506 (0.0506)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [209][64/196]	Time 0.087 (0.087)	Data 0.000 (0.005)	Loss 0.1046 (0.0662)	Acc@1 96.484 (97.855)	Acc@5 100.000 (99.970)
Epoch: [209][128/196]	Time 0.095 (0.087)	Data 0.000 (0.003)	Loss 0.0516 (0.0662)	Acc@1 97.656 (97.859)	Acc@5 100.000 (99.982)
Epoch: [209][192/196]	Time 0.077 (0.087)	Data 0.000 (0.002)	Loss 0.0585 (0.0677)	Acc@1 98.438 (97.820)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:210/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [210][0/196]	Time 0.132 (0.132)	Data 0.299 (0.299)	Loss 0.0605 (0.0605)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [210][64/196]	Time 0.104 (0.089)	Data 0.000 (0.005)	Loss 0.0550 (0.0689)	Acc@1 97.656 (97.710)	Acc@5 100.000 (99.988)
Epoch: [210][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0664 (0.0676)	Acc@1 98.438 (97.762)	Acc@5 100.000 (99.982)
Epoch: [210][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0516 (0.0674)	Acc@1 98.828 (97.804)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.32
Max memory: 51.4381312
 17.522s  j: 216 bis 220
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2059
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 211
Max memory: 0.1097216
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:211/215; Lr: 0.0004782969000000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [211][0/196]	Time 0.152 (0.152)	Data 0.316 (0.316)	Loss 0.0441 (0.0441)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [211][64/196]	Time 0.109 (0.090)	Data 0.000 (0.005)	Loss 0.0520 (0.0654)	Acc@1 98.047 (97.963)	Acc@5 100.000 (99.988)
Epoch: [211][128/196]	Time 0.082 (0.089)	Data 0.000 (0.003)	Loss 0.0443 (0.0656)	Acc@1 98.828 (97.965)	Acc@5 100.000 (99.991)
Epoch: [211][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0843 (0.0652)	Acc@1 96.875 (97.942)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:212/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [212][0/196]	Time 0.105 (0.105)	Data 0.304 (0.304)	Loss 0.0661 (0.0661)	Acc@1 97.266 (97.266)	Acc@5 99.609 (99.609)
Epoch: [212][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0535 (0.0639)	Acc@1 98.047 (97.999)	Acc@5 100.000 (99.976)
Epoch: [212][128/196]	Time 0.090 (0.089)	Data 0.000 (0.003)	Loss 0.0568 (0.0651)	Acc@1 98.047 (97.986)	Acc@5 100.000 (99.982)
Epoch: [212][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0630 (0.0663)	Acc@1 98.438 (97.913)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:213/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [213][0/196]	Time 0.122 (0.122)	Data 0.332 (0.332)	Loss 0.0467 (0.0467)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [213][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0808 (0.0668)	Acc@1 97.266 (97.957)	Acc@5 100.000 (99.976)
Epoch: [213][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.0594 (0.0660)	Acc@1 97.656 (97.920)	Acc@5 100.000 (99.982)
Epoch: [213][192/196]	Time 0.079 (0.088)	Data 0.000 (0.002)	Loss 0.1095 (0.0662)	Acc@1 96.875 (97.893)	Acc@5 99.609 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:214/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [214][0/196]	Time 0.127 (0.127)	Data 0.290 (0.290)	Loss 0.0979 (0.0979)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [214][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.0778 (0.0685)	Acc@1 98.828 (97.746)	Acc@5 100.000 (99.994)
Epoch: [214][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0327 (0.0664)	Acc@1 100.000 (97.762)	Acc@5 100.000 (99.985)
Epoch: [214][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0536 (0.0676)	Acc@1 99.219 (97.755)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:215/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [215][0/196]	Time 0.131 (0.131)	Data 0.295 (0.295)	Loss 0.0644 (0.0644)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [215][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.0336 (0.0667)	Acc@1 99.609 (97.879)	Acc@5 100.000 (99.976)
Epoch: [215][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.0521 (0.0668)	Acc@1 97.656 (97.844)	Acc@5 100.000 (99.976)
Epoch: [215][192/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.0673 (0.0664)	Acc@1 98.047 (97.887)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.22
Max memory: 51.4381312
 17.701s  j: 221 bis 225
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5023
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 216
Max memory: 0.1097216
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:216/220; Lr: 0.0004304672100000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [216][0/196]	Time 0.188 (0.188)	Data 0.326 (0.326)	Loss 0.0612 (0.0612)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [216][64/196]	Time 0.079 (0.090)	Data 0.000 (0.005)	Loss 0.0882 (0.0606)	Acc@1 97.266 (98.071)	Acc@5 100.000 (99.988)
Epoch: [216][128/196]	Time 0.092 (0.089)	Data 0.000 (0.003)	Loss 0.0625 (0.0638)	Acc@1 96.875 (97.950)	Acc@5 100.000 (99.988)
Epoch: [216][192/196]	Time 0.095 (0.089)	Data 0.000 (0.002)	Loss 0.0979 (0.0648)	Acc@1 97.656 (97.921)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:217/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [217][0/196]	Time 0.135 (0.135)	Data 0.282 (0.282)	Loss 0.0481 (0.0481)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [217][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0451 (0.0638)	Acc@1 98.047 (97.999)	Acc@5 100.000 (99.982)
Epoch: [217][128/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0602 (0.0645)	Acc@1 98.438 (97.959)	Acc@5 100.000 (99.976)
Epoch: [217][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0674 (0.0654)	Acc@1 98.047 (97.925)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:218/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [218][0/196]	Time 0.116 (0.116)	Data 0.277 (0.277)	Loss 0.0502 (0.0502)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [218][64/196]	Time 0.090 (0.090)	Data 0.000 (0.004)	Loss 0.0585 (0.0642)	Acc@1 98.047 (97.939)	Acc@5 100.000 (99.982)
Epoch: [218][128/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.0655 (0.0647)	Acc@1 98.828 (97.956)	Acc@5 100.000 (99.985)
Epoch: [218][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0847 (0.0641)	Acc@1 96.875 (97.992)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:219/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [219][0/196]	Time 0.132 (0.132)	Data 0.331 (0.331)	Loss 0.0617 (0.0617)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [219][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0452 (0.0629)	Acc@1 98.828 (97.957)	Acc@5 100.000 (99.976)
Epoch: [219][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.0496 (0.0649)	Acc@1 98.438 (97.920)	Acc@5 100.000 (99.976)
Epoch: [219][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0646 (0.0653)	Acc@1 98.047 (97.921)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:220/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [220][0/196]	Time 0.119 (0.119)	Data 0.302 (0.302)	Loss 0.1343 (0.1343)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [220][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.0363 (0.0692)	Acc@1 98.828 (97.873)	Acc@5 100.000 (99.976)
Epoch: [220][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.0437 (0.0675)	Acc@1 98.438 (97.892)	Acc@5 100.000 (99.976)
Epoch: [220][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0823 (0.0671)	Acc@1 98.047 (97.909)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.17
Max memory: 51.4381312
 17.530s  j: 226 bis 230
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7992
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 221
Max memory: 0.1097216
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:221/225; Lr: 0.0003874204890000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [221][0/196]	Time 0.175 (0.175)	Data 0.287 (0.287)	Loss 0.0323 (0.0323)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [221][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.0491 (0.0629)	Acc@1 98.828 (98.005)	Acc@5 100.000 (99.994)
Epoch: [221][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0944 (0.0648)	Acc@1 96.875 (97.923)	Acc@5 100.000 (99.994)
Epoch: [221][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0592 (0.0647)	Acc@1 98.047 (97.915)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:222/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [222][0/196]	Time 0.133 (0.133)	Data 0.299 (0.299)	Loss 0.0384 (0.0384)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [222][64/196]	Time 0.095 (0.089)	Data 0.000 (0.005)	Loss 0.0531 (0.0651)	Acc@1 98.047 (97.945)	Acc@5 100.000 (99.988)
Epoch: [222][128/196]	Time 0.081 (0.087)	Data 0.000 (0.003)	Loss 0.0477 (0.0646)	Acc@1 98.438 (97.883)	Acc@5 100.000 (99.985)
Epoch: [222][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0627 (0.0631)	Acc@1 98.438 (97.996)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:223/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [223][0/196]	Time 0.126 (0.126)	Data 0.316 (0.316)	Loss 0.0503 (0.0503)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [223][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.0593 (0.0646)	Acc@1 98.828 (98.065)	Acc@5 100.000 (99.982)
Epoch: [223][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0386 (0.0638)	Acc@1 98.828 (98.029)	Acc@5 100.000 (99.991)
Epoch: [223][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0525 (0.0646)	Acc@1 98.828 (97.996)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:224/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [224][0/196]	Time 0.122 (0.122)	Data 0.309 (0.309)	Loss 0.0536 (0.0536)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [224][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0665 (0.0642)	Acc@1 97.656 (97.957)	Acc@5 100.000 (100.000)
Epoch: [224][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.0452 (0.0644)	Acc@1 98.438 (97.953)	Acc@5 100.000 (99.994)
Epoch: [224][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0966 (0.0643)	Acc@1 96.875 (97.978)	Acc@5 99.609 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:225/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [225][0/196]	Time 0.123 (0.123)	Data 0.288 (0.288)	Loss 0.0644 (0.0644)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [225][64/196]	Time 0.084 (0.091)	Data 0.000 (0.005)	Loss 0.0396 (0.0645)	Acc@1 99.219 (98.029)	Acc@5 100.000 (99.994)
Epoch: [225][128/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.0418 (0.0639)	Acc@1 98.828 (98.041)	Acc@5 100.000 (99.991)
Epoch: [225][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.0688 (0.0641)	Acc@1 98.047 (98.023)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.3
Max memory: 51.4381312
 17.745s  j: 231 bis 235
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2915
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 226
Max memory: 0.1097216
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:226/230; Lr: 0.00034867844010000006
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [226][0/196]	Time 0.161 (0.161)	Data 0.260 (0.260)	Loss 0.0649 (0.0649)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [226][64/196]	Time 0.086 (0.089)	Data 0.000 (0.004)	Loss 0.0549 (0.0632)	Acc@1 98.438 (98.125)	Acc@5 100.000 (99.970)
Epoch: [226][128/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 0.0409 (0.0617)	Acc@1 98.828 (98.120)	Acc@5 100.000 (99.976)
Epoch: [226][192/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 0.0404 (0.0632)	Acc@1 98.438 (98.035)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:227/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [227][0/196]	Time 0.123 (0.123)	Data 0.312 (0.312)	Loss 0.0707 (0.0707)	Acc@1 98.047 (98.047)	Acc@5 99.609 (99.609)
Epoch: [227][64/196]	Time 0.092 (0.088)	Data 0.000 (0.005)	Loss 0.0503 (0.0654)	Acc@1 97.656 (97.897)	Acc@5 100.000 (99.982)
Epoch: [227][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.0719 (0.0623)	Acc@1 97.266 (98.014)	Acc@5 100.000 (99.985)
Epoch: [227][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.0560 (0.0617)	Acc@1 98.828 (98.069)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:228/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [228][0/196]	Time 0.126 (0.126)	Data 0.300 (0.300)	Loss 0.0583 (0.0583)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [228][64/196]	Time 0.097 (0.090)	Data 0.000 (0.005)	Loss 0.0779 (0.0624)	Acc@1 97.266 (98.023)	Acc@5 100.000 (99.982)
Epoch: [228][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0409 (0.0637)	Acc@1 98.828 (97.965)	Acc@5 100.000 (99.976)
Epoch: [228][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0583 (0.0632)	Acc@1 98.047 (97.970)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:229/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [229][0/196]	Time 0.130 (0.130)	Data 0.303 (0.303)	Loss 0.0583 (0.0583)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [229][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.0528 (0.0635)	Acc@1 98.438 (98.005)	Acc@5 100.000 (99.994)
Epoch: [229][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0716 (0.0633)	Acc@1 98.438 (98.017)	Acc@5 100.000 (99.982)
Epoch: [229][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0787 (0.0636)	Acc@1 96.875 (97.972)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:230/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [230][0/196]	Time 0.131 (0.131)	Data 0.268 (0.268)	Loss 0.0764 (0.0764)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [230][64/196]	Time 0.080 (0.089)	Data 0.000 (0.004)	Loss 0.0642 (0.0635)	Acc@1 98.047 (98.107)	Acc@5 100.000 (99.982)
Epoch: [230][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0740 (0.0650)	Acc@1 97.656 (98.001)	Acc@5 100.000 (99.976)
Epoch: [230][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0746 (0.0632)	Acc@1 97.656 (98.045)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.19
Max memory: 51.4381312
 17.548s  j: 236 bis 240
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4517
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 231
Max memory: 0.1097216
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:231/235; Lr: 0.00031381059609000004
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [231][0/196]	Time 0.130 (0.130)	Data 0.307 (0.307)	Loss 0.0716 (0.0716)	Acc@1 97.656 (97.656)	Acc@5 99.609 (99.609)
Epoch: [231][64/196]	Time 0.079 (0.088)	Data 0.000 (0.005)	Loss 0.0707 (0.0600)	Acc@1 97.656 (98.149)	Acc@5 100.000 (99.982)
Epoch: [231][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0666 (0.0599)	Acc@1 97.656 (98.153)	Acc@5 100.000 (99.982)
Epoch: [231][192/196]	Time 0.084 (0.086)	Data 0.000 (0.002)	Loss 0.0577 (0.0620)	Acc@1 97.266 (98.091)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:232/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [232][0/196]	Time 0.142 (0.142)	Data 0.357 (0.357)	Loss 0.0538 (0.0538)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [232][64/196]	Time 0.082 (0.089)	Data 0.000 (0.006)	Loss 0.0747 (0.0602)	Acc@1 97.656 (98.125)	Acc@5 100.000 (99.994)
Epoch: [232][128/196]	Time 0.098 (0.088)	Data 0.000 (0.003)	Loss 0.0481 (0.0621)	Acc@1 98.828 (98.071)	Acc@5 100.000 (99.988)
Epoch: [232][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0436 (0.0624)	Acc@1 99.219 (98.061)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:233/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [233][0/196]	Time 0.113 (0.113)	Data 0.317 (0.317)	Loss 0.0469 (0.0469)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [233][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0534 (0.0640)	Acc@1 98.828 (97.993)	Acc@5 100.000 (99.976)
Epoch: [233][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0653 (0.0614)	Acc@1 98.828 (98.101)	Acc@5 100.000 (99.985)
Epoch: [233][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0708 (0.0620)	Acc@1 98.047 (98.081)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:234/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [234][0/196]	Time 0.131 (0.131)	Data 0.283 (0.283)	Loss 0.1012 (0.1012)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [234][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0431 (0.0612)	Acc@1 99.219 (98.077)	Acc@5 100.000 (99.964)
Epoch: [234][128/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0237 (0.0623)	Acc@1 99.609 (98.020)	Acc@5 100.000 (99.973)
Epoch: [234][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0431 (0.0624)	Acc@1 97.656 (98.008)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:235/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [235][0/196]	Time 0.124 (0.124)	Data 0.334 (0.334)	Loss 0.0328 (0.0328)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [235][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0981 (0.0612)	Acc@1 96.875 (98.203)	Acc@5 99.609 (99.976)
Epoch: [235][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0775 (0.0620)	Acc@1 97.656 (98.098)	Acc@5 100.000 (99.976)
Epoch: [235][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.0689 (0.0613)	Acc@1 97.656 (98.089)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.29
Max memory: 51.4381312
 17.608s  j: 241 bis 245
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9781
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 236
Max memory: 0.1097216
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:236/240; Lr: 0.00028242953648100003
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [236][0/196]	Time 0.151 (0.151)	Data 0.279 (0.279)	Loss 0.0640 (0.0640)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [236][64/196]	Time 0.081 (0.089)	Data 0.000 (0.005)	Loss 0.0678 (0.0641)	Acc@1 97.656 (97.933)	Acc@5 100.000 (99.988)
Epoch: [236][128/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0468 (0.0632)	Acc@1 98.438 (98.053)	Acc@5 100.000 (99.979)
Epoch: [236][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0960 (0.0629)	Acc@1 96.875 (98.014)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:237/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [237][0/196]	Time 0.125 (0.125)	Data 0.306 (0.306)	Loss 0.0587 (0.0587)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [237][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.0672 (0.0620)	Acc@1 97.656 (98.095)	Acc@5 100.000 (99.970)
Epoch: [237][128/196]	Time 0.085 (0.089)	Data 0.000 (0.003)	Loss 0.1025 (0.0637)	Acc@1 96.484 (97.998)	Acc@5 100.000 (99.985)
Epoch: [237][192/196]	Time 0.096 (0.089)	Data 0.000 (0.002)	Loss 0.0457 (0.0627)	Acc@1 99.219 (98.051)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:238/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [238][0/196]	Time 0.127 (0.127)	Data 0.306 (0.306)	Loss 0.0473 (0.0473)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [238][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.0573 (0.0585)	Acc@1 98.438 (98.191)	Acc@5 100.000 (99.994)
Epoch: [238][128/196]	Time 0.090 (0.089)	Data 0.000 (0.003)	Loss 0.0852 (0.0609)	Acc@1 97.266 (98.071)	Acc@5 99.609 (99.982)
Epoch: [238][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.0833 (0.0605)	Acc@1 97.266 (98.106)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:239/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [239][0/196]	Time 0.137 (0.137)	Data 0.287 (0.287)	Loss 0.0392 (0.0392)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [239][64/196]	Time 0.088 (0.090)	Data 0.000 (0.005)	Loss 0.1329 (0.0612)	Acc@1 96.094 (98.023)	Acc@5 100.000 (99.970)
Epoch: [239][128/196]	Time 0.085 (0.090)	Data 0.000 (0.002)	Loss 0.0682 (0.0632)	Acc@1 98.438 (97.992)	Acc@5 100.000 (99.973)
Epoch: [239][192/196]	Time 0.080 (0.089)	Data 0.000 (0.002)	Loss 0.0605 (0.0629)	Acc@1 98.438 (98.033)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:240/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [240][0/196]	Time 0.140 (0.140)	Data 0.293 (0.293)	Loss 0.0312 (0.0312)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [240][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.0818 (0.0604)	Acc@1 96.875 (98.119)	Acc@5 100.000 (99.982)
Epoch: [240][128/196]	Time 0.102 (0.088)	Data 0.000 (0.002)	Loss 0.0551 (0.0609)	Acc@1 97.266 (98.035)	Acc@5 100.000 (99.991)
Epoch: [240][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0419 (0.0617)	Acc@1 99.609 (98.035)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.25
Max memory: 51.4381312
 17.703s  j: 246 bis 250
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4789
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 241
Max memory: 0.1097216
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:241/245; Lr: 0.00025418658283290005
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [241][0/196]	Time 0.154 (0.154)	Data 0.296 (0.296)	Loss 0.0633 (0.0633)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [241][64/196]	Time 0.080 (0.086)	Data 0.000 (0.005)	Loss 0.0702 (0.0598)	Acc@1 97.656 (98.095)	Acc@5 100.000 (100.000)
Epoch: [241][128/196]	Time 0.084 (0.086)	Data 0.000 (0.003)	Loss 0.0566 (0.0594)	Acc@1 98.828 (98.126)	Acc@5 100.000 (99.994)
Epoch: [241][192/196]	Time 0.084 (0.086)	Data 0.000 (0.002)	Loss 0.0499 (0.0602)	Acc@1 98.828 (98.087)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:242/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [242][0/196]	Time 0.122 (0.122)	Data 0.306 (0.306)	Loss 0.0596 (0.0596)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [242][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.0538 (0.0627)	Acc@1 99.219 (97.969)	Acc@5 100.000 (99.982)
Epoch: [242][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0425 (0.0611)	Acc@1 98.438 (98.011)	Acc@5 100.000 (99.988)
Epoch: [242][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0600 (0.0621)	Acc@1 98.828 (98.004)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:243/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [243][0/196]	Time 0.133 (0.133)	Data 0.300 (0.300)	Loss 0.1110 (0.1110)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [243][64/196]	Time 0.097 (0.088)	Data 0.000 (0.005)	Loss 0.0576 (0.0596)	Acc@1 98.438 (98.263)	Acc@5 100.000 (99.994)
Epoch: [243][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0634 (0.0584)	Acc@1 98.438 (98.235)	Acc@5 100.000 (99.994)
Epoch: [243][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0853 (0.0585)	Acc@1 97.656 (98.225)	Acc@5 99.609 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:244/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [244][0/196]	Time 0.129 (0.129)	Data 0.298 (0.298)	Loss 0.0451 (0.0451)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [244][64/196]	Time 0.096 (0.089)	Data 0.000 (0.005)	Loss 0.0345 (0.0607)	Acc@1 99.219 (98.101)	Acc@5 100.000 (99.994)
Epoch: [244][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0720 (0.0598)	Acc@1 97.656 (98.120)	Acc@5 100.000 (99.994)
Epoch: [244][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0708 (0.0605)	Acc@1 97.656 (98.112)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:245/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [245][0/196]	Time 0.124 (0.124)	Data 0.326 (0.326)	Loss 0.0641 (0.0641)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [245][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0536 (0.0588)	Acc@1 99.219 (98.083)	Acc@5 100.000 (100.000)
Epoch: [245][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0559 (0.0606)	Acc@1 97.656 (98.068)	Acc@5 100.000 (99.985)
Epoch: [245][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0588 (0.0605)	Acc@1 98.047 (98.065)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.29
Max memory: 51.4381312
 17.496s  j: 251 bis 255
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7476
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 246
Max memory: 0.1097216
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:246/250; Lr: 0.00022876792454961005
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [246][0/196]	Time 0.136 (0.136)	Data 0.291 (0.291)	Loss 0.0488 (0.0488)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [246][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.0570 (0.0609)	Acc@1 98.047 (98.119)	Acc@5 100.000 (99.982)
Epoch: [246][128/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.0710 (0.0608)	Acc@1 97.656 (98.138)	Acc@5 100.000 (99.991)
Epoch: [246][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0585 (0.0613)	Acc@1 97.266 (98.104)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:247/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [247][0/196]	Time 0.134 (0.134)	Data 0.313 (0.313)	Loss 0.0719 (0.0719)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [247][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.0764 (0.0596)	Acc@1 96.484 (98.167)	Acc@5 100.000 (99.982)
Epoch: [247][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.0379 (0.0581)	Acc@1 99.609 (98.226)	Acc@5 100.000 (99.976)
Epoch: [247][192/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.0461 (0.0602)	Acc@1 98.438 (98.089)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:248/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [248][0/196]	Time 0.136 (0.136)	Data 0.263 (0.263)	Loss 0.0635 (0.0635)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [248][64/196]	Time 0.085 (0.089)	Data 0.000 (0.004)	Loss 0.0726 (0.0612)	Acc@1 98.438 (98.035)	Acc@5 100.000 (99.988)
Epoch: [248][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.1182 (0.0619)	Acc@1 95.703 (97.998)	Acc@5 100.000 (99.979)
Epoch: [248][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0507 (0.0609)	Acc@1 98.828 (98.051)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:249/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [249][0/196]	Time 0.134 (0.134)	Data 0.296 (0.296)	Loss 0.0749 (0.0749)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [249][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0293 (0.0631)	Acc@1 99.219 (98.029)	Acc@5 100.000 (100.000)
Epoch: [249][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0671 (0.0604)	Acc@1 98.438 (98.113)	Acc@5 100.000 (99.997)
Epoch: [249][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0698 (0.0605)	Acc@1 98.047 (98.132)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:250/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [250][0/196]	Time 0.130 (0.130)	Data 0.314 (0.314)	Loss 0.0623 (0.0623)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [250][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0827 (0.0610)	Acc@1 96.875 (98.161)	Acc@5 100.000 (99.994)
Epoch: [250][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.0732 (0.0618)	Acc@1 97.266 (98.080)	Acc@5 100.000 (99.979)
Epoch: [250][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0545 (0.0617)	Acc@1 98.438 (98.071)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.12
Max memory: 51.4381312
 17.546s  j: 256 bis 260
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8778
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 251
Max memory: 0.1097216
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:251/255; Lr: 0.00020589113209464906
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [251][0/196]	Time 0.125 (0.125)	Data 0.295 (0.295)	Loss 0.0363 (0.0363)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [251][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0788 (0.0608)	Acc@1 96.875 (98.125)	Acc@5 100.000 (99.982)
Epoch: [251][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.0751 (0.0597)	Acc@1 96.484 (98.168)	Acc@5 100.000 (99.979)
Epoch: [251][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0397 (0.0593)	Acc@1 99.609 (98.185)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:252/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [252][0/196]	Time 0.122 (0.122)	Data 0.343 (0.343)	Loss 0.0630 (0.0630)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [252][64/196]	Time 0.086 (0.087)	Data 0.000 (0.006)	Loss 0.0657 (0.0600)	Acc@1 98.438 (98.083)	Acc@5 100.000 (99.982)
Epoch: [252][128/196]	Time 0.090 (0.087)	Data 0.000 (0.003)	Loss 0.0755 (0.0590)	Acc@1 98.438 (98.171)	Acc@5 100.000 (99.979)
Epoch: [252][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0754 (0.0583)	Acc@1 98.047 (98.217)	Acc@5 99.609 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:253/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [253][0/196]	Time 0.107 (0.107)	Data 0.324 (0.324)	Loss 0.0658 (0.0658)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [253][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.0625 (0.0618)	Acc@1 97.656 (98.095)	Acc@5 100.000 (100.000)
Epoch: [253][128/196]	Time 0.090 (0.087)	Data 0.000 (0.003)	Loss 0.0542 (0.0612)	Acc@1 98.438 (98.110)	Acc@5 100.000 (100.000)
Epoch: [253][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0453 (0.0605)	Acc@1 99.219 (98.140)	Acc@5 100.000 (99.998)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:254/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [254][0/196]	Time 0.126 (0.126)	Data 0.335 (0.335)	Loss 0.0514 (0.0514)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [254][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.0631 (0.0612)	Acc@1 98.047 (98.101)	Acc@5 100.000 (99.988)
Epoch: [254][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0549 (0.0609)	Acc@1 98.047 (98.110)	Acc@5 100.000 (99.979)
Epoch: [254][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0789 (0.0603)	Acc@1 96.875 (98.132)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:255/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [255][0/196]	Time 0.112 (0.112)	Data 0.316 (0.316)	Loss 0.0734 (0.0734)	Acc@1 98.438 (98.438)	Acc@5 99.609 (99.609)
Epoch: [255][64/196]	Time 0.101 (0.088)	Data 0.000 (0.005)	Loss 0.0606 (0.0605)	Acc@1 99.219 (98.125)	Acc@5 100.000 (99.964)
Epoch: [255][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0373 (0.0610)	Acc@1 99.609 (98.032)	Acc@5 100.000 (99.970)
Epoch: [255][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.1042 (0.0599)	Acc@1 96.484 (98.104)	Acc@5 99.609 (99.974)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.16
Max memory: 51.4381312
 17.431s  j: 261 bis 265
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 541
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 256
Max memory: 0.1097216
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:256/260; Lr: 0.00018530201888518417
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [256][0/196]	Time 0.148 (0.148)	Data 0.318 (0.318)	Loss 0.0579 (0.0579)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [256][64/196]	Time 0.097 (0.089)	Data 0.000 (0.005)	Loss 0.0542 (0.0604)	Acc@1 98.047 (98.191)	Acc@5 100.000 (99.994)
Epoch: [256][128/196]	Time 0.092 (0.089)	Data 0.000 (0.003)	Loss 0.0541 (0.0612)	Acc@1 98.828 (98.141)	Acc@5 100.000 (99.991)
Epoch: [256][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0691 (0.0606)	Acc@1 97.656 (98.176)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:257/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [257][0/196]	Time 0.107 (0.107)	Data 0.329 (0.329)	Loss 0.0392 (0.0392)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [257][64/196]	Time 0.078 (0.089)	Data 0.000 (0.005)	Loss 0.0427 (0.0591)	Acc@1 97.656 (98.221)	Acc@5 100.000 (99.994)
Epoch: [257][128/196]	Time 0.088 (0.089)	Data 0.000 (0.003)	Loss 0.0761 (0.0599)	Acc@1 97.266 (98.156)	Acc@5 100.000 (99.979)
Epoch: [257][192/196]	Time 0.094 (0.088)	Data 0.000 (0.002)	Loss 0.0847 (0.0604)	Acc@1 96.875 (98.148)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:258/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [258][0/196]	Time 0.119 (0.119)	Data 0.302 (0.302)	Loss 0.0655 (0.0655)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [258][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.0481 (0.0602)	Acc@1 98.438 (98.227)	Acc@5 100.000 (99.988)
Epoch: [258][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.0521 (0.0591)	Acc@1 98.438 (98.223)	Acc@5 100.000 (99.988)
Epoch: [258][192/196]	Time 0.097 (0.088)	Data 0.000 (0.002)	Loss 0.0760 (0.0594)	Acc@1 97.266 (98.203)	Acc@5 99.609 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:259/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [259][0/196]	Time 0.136 (0.136)	Data 0.289 (0.289)	Loss 0.0514 (0.0514)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [259][64/196]	Time 0.095 (0.089)	Data 0.000 (0.005)	Loss 0.0533 (0.0570)	Acc@1 98.438 (98.287)	Acc@5 100.000 (99.982)
Epoch: [259][128/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0730 (0.0587)	Acc@1 97.656 (98.159)	Acc@5 100.000 (99.988)
Epoch: [259][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0738 (0.0593)	Acc@1 98.438 (98.195)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:260/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [260][0/196]	Time 0.143 (0.143)	Data 0.296 (0.296)	Loss 0.0885 (0.0885)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [260][64/196]	Time 0.097 (0.088)	Data 0.000 (0.005)	Loss 0.0610 (0.0602)	Acc@1 98.047 (98.185)	Acc@5 100.000 (99.982)
Epoch: [260][128/196]	Time 0.081 (0.087)	Data 0.000 (0.003)	Loss 0.0340 (0.0598)	Acc@1 99.219 (98.126)	Acc@5 100.000 (99.985)
Epoch: [260][192/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.0371 (0.0602)	Acc@1 98.438 (98.158)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.34
Max memory: 51.4381312
 17.396s  j: 266 bis 270
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9784
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 261
Max memory: 0.1097216
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:261/265; Lr: 0.00016677181699666576
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [261][0/196]	Time 0.164 (0.164)	Data 0.268 (0.268)	Loss 0.0824 (0.0824)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [261][64/196]	Time 0.095 (0.089)	Data 0.000 (0.004)	Loss 0.0636 (0.0587)	Acc@1 98.047 (98.119)	Acc@5 100.000 (99.982)
Epoch: [261][128/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.0569 (0.0594)	Acc@1 97.656 (98.120)	Acc@5 100.000 (99.979)
Epoch: [261][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0368 (0.0597)	Acc@1 98.828 (98.128)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:262/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [262][0/196]	Time 0.122 (0.122)	Data 0.284 (0.284)	Loss 0.0474 (0.0474)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [262][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0417 (0.0614)	Acc@1 99.609 (98.077)	Acc@5 100.000 (99.988)
Epoch: [262][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0651 (0.0594)	Acc@1 98.047 (98.189)	Acc@5 100.000 (99.985)
Epoch: [262][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0922 (0.0588)	Acc@1 97.656 (98.209)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:263/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [263][0/196]	Time 0.134 (0.134)	Data 0.313 (0.313)	Loss 0.0890 (0.0890)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [263][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.0639 (0.0580)	Acc@1 98.047 (98.131)	Acc@5 100.000 (99.988)
Epoch: [263][128/196]	Time 0.089 (0.089)	Data 0.000 (0.003)	Loss 0.0442 (0.0584)	Acc@1 99.609 (98.162)	Acc@5 100.000 (99.985)
Epoch: [263][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.0247 (0.0590)	Acc@1 100.000 (98.152)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:264/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [264][0/196]	Time 0.126 (0.126)	Data 0.312 (0.312)	Loss 0.0423 (0.0423)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [264][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.0756 (0.0570)	Acc@1 97.266 (98.257)	Acc@5 100.000 (99.988)
Epoch: [264][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0768 (0.0583)	Acc@1 98.047 (98.216)	Acc@5 100.000 (99.988)
Epoch: [264][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0629 (0.0587)	Acc@1 97.266 (98.197)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:265/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [265][0/196]	Time 0.106 (0.106)	Data 0.293 (0.293)	Loss 0.0290 (0.0290)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [265][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.0641 (0.0605)	Acc@1 97.656 (98.083)	Acc@5 100.000 (99.982)
Epoch: [265][128/196]	Time 0.079 (0.088)	Data 0.000 (0.002)	Loss 0.0412 (0.0604)	Acc@1 99.219 (98.053)	Acc@5 100.000 (99.979)
Epoch: [265][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0681 (0.0604)	Acc@1 97.656 (98.045)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.3
Max memory: 51.4381312
 17.583s  j: 271 bis 275
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7146
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 266
Max memory: 0.1097216
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:266/270; Lr: 0.0001500946352969992
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [266][0/196]	Time 0.158 (0.158)	Data 0.259 (0.259)	Loss 0.0624 (0.0624)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [266][64/196]	Time 0.087 (0.090)	Data 0.000 (0.004)	Loss 0.0489 (0.0572)	Acc@1 98.438 (98.359)	Acc@5 100.000 (99.994)
Epoch: [266][128/196]	Time 0.077 (0.088)	Data 0.000 (0.002)	Loss 0.0393 (0.0578)	Acc@1 99.219 (98.247)	Acc@5 100.000 (99.997)
Epoch: [266][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0546 (0.0577)	Acc@1 97.656 (98.229)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:267/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [267][0/196]	Time 0.120 (0.120)	Data 0.301 (0.301)	Loss 0.0698 (0.0698)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [267][64/196]	Time 0.083 (0.087)	Data 0.000 (0.005)	Loss 0.0437 (0.0579)	Acc@1 98.438 (98.149)	Acc@5 100.000 (99.994)
Epoch: [267][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.0457 (0.0589)	Acc@1 99.219 (98.162)	Acc@5 100.000 (99.994)
Epoch: [267][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0704 (0.0590)	Acc@1 97.656 (98.150)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:268/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [268][0/196]	Time 0.119 (0.119)	Data 0.351 (0.351)	Loss 0.0500 (0.0500)	Acc@1 98.047 (98.047)	Acc@5 99.609 (99.609)
Epoch: [268][64/196]	Time 0.086 (0.088)	Data 0.000 (0.006)	Loss 0.0391 (0.0597)	Acc@1 98.828 (98.263)	Acc@5 100.000 (99.982)
Epoch: [268][128/196]	Time 0.099 (0.087)	Data 0.000 (0.003)	Loss 0.0837 (0.0604)	Acc@1 96.484 (98.153)	Acc@5 100.000 (99.979)
Epoch: [268][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0764 (0.0600)	Acc@1 98.438 (98.156)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:269/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [269][0/196]	Time 0.112 (0.112)	Data 0.283 (0.283)	Loss 0.0397 (0.0397)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [269][64/196]	Time 0.095 (0.087)	Data 0.000 (0.005)	Loss 0.0816 (0.0602)	Acc@1 97.266 (98.119)	Acc@5 100.000 (100.000)
Epoch: [269][128/196]	Time 0.085 (0.086)	Data 0.000 (0.002)	Loss 0.0386 (0.0590)	Acc@1 99.219 (98.195)	Acc@5 100.000 (99.997)
Epoch: [269][192/196]	Time 0.085 (0.086)	Data 0.000 (0.002)	Loss 0.0758 (0.0593)	Acc@1 96.484 (98.116)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:270/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [270][0/196]	Time 0.122 (0.122)	Data 0.336 (0.336)	Loss 0.0789 (0.0789)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [270][64/196]	Time 0.094 (0.089)	Data 0.000 (0.005)	Loss 0.0717 (0.0556)	Acc@1 97.656 (98.221)	Acc@5 100.000 (99.976)
Epoch: [270][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.0521 (0.0578)	Acc@1 98.438 (98.186)	Acc@5 100.000 (99.979)
Epoch: [270][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0511 (0.0579)	Acc@1 98.438 (98.201)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.37
Max memory: 51.4381312
 17.515s  j: 276 bis 280
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1765
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 271
Max memory: 0.1097216
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:271/275; Lr: 0.0001350851717672993
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [271][0/196]	Time 0.134 (0.134)	Data 0.302 (0.302)	Loss 0.0283 (0.0283)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [271][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0710 (0.0584)	Acc@1 98.438 (98.161)	Acc@5 99.219 (99.970)
Epoch: [271][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0318 (0.0590)	Acc@1 99.609 (98.147)	Acc@5 100.000 (99.979)
Epoch: [271][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0690 (0.0594)	Acc@1 96.875 (98.162)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:272/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [272][0/196]	Time 0.131 (0.131)	Data 0.277 (0.277)	Loss 0.0814 (0.0814)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [272][64/196]	Time 0.089 (0.090)	Data 0.000 (0.004)	Loss 0.1009 (0.0590)	Acc@1 96.094 (98.137)	Acc@5 99.609 (99.982)
Epoch: [272][128/196]	Time 0.095 (0.088)	Data 0.000 (0.002)	Loss 0.0547 (0.0595)	Acc@1 98.438 (98.101)	Acc@5 100.000 (99.982)
Epoch: [272][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0601 (0.0588)	Acc@1 97.656 (98.134)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:273/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [273][0/196]	Time 0.127 (0.127)	Data 0.312 (0.312)	Loss 0.0537 (0.0537)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [273][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0418 (0.0560)	Acc@1 98.438 (98.233)	Acc@5 100.000 (99.994)
Epoch: [273][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0423 (0.0568)	Acc@1 99.219 (98.213)	Acc@5 100.000 (99.997)
Epoch: [273][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0314 (0.0576)	Acc@1 98.828 (98.148)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:274/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [274][0/196]	Time 0.129 (0.129)	Data 0.348 (0.348)	Loss 0.0282 (0.0282)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [274][64/196]	Time 0.089 (0.090)	Data 0.000 (0.006)	Loss 0.0352 (0.0583)	Acc@1 98.828 (98.209)	Acc@5 100.000 (99.982)
Epoch: [274][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0646 (0.0592)	Acc@1 97.656 (98.120)	Acc@5 100.000 (99.979)
Epoch: [274][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0637 (0.0596)	Acc@1 98.438 (98.134)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:275/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [275][0/196]	Time 0.133 (0.133)	Data 0.312 (0.312)	Loss 0.0805 (0.0805)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [275][64/196]	Time 0.088 (0.087)	Data 0.000 (0.005)	Loss 0.0909 (0.0610)	Acc@1 98.047 (98.059)	Acc@5 100.000 (99.982)
Epoch: [275][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.0503 (0.0587)	Acc@1 98.438 (98.189)	Acc@5 100.000 (99.979)
Epoch: [275][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.0457 (0.0581)	Acc@1 98.828 (98.174)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.3
Max memory: 51.4381312
 17.452s  j: 281 bis 285
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9570
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 276
Max memory: 0.1097216
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:276/280; Lr: 0.00012157665459056936
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [276][0/196]	Time 0.147 (0.147)	Data 0.320 (0.320)	Loss 0.0564 (0.0564)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [276][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0855 (0.0560)	Acc@1 97.266 (98.239)	Acc@5 100.000 (99.982)
Epoch: [276][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.0756 (0.0588)	Acc@1 98.047 (98.153)	Acc@5 100.000 (99.985)
Epoch: [276][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.0372 (0.0575)	Acc@1 99.219 (98.203)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:277/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [277][0/196]	Time 0.138 (0.138)	Data 0.298 (0.298)	Loss 0.0449 (0.0449)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [277][64/196]	Time 0.080 (0.089)	Data 0.000 (0.005)	Loss 0.0385 (0.0556)	Acc@1 99.219 (98.323)	Acc@5 100.000 (99.976)
Epoch: [277][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.0398 (0.0558)	Acc@1 99.609 (98.328)	Acc@5 100.000 (99.985)
Epoch: [277][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0579 (0.0573)	Acc@1 98.828 (98.270)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:278/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [278][0/196]	Time 0.127 (0.127)	Data 0.295 (0.295)	Loss 0.0920 (0.0920)	Acc@1 96.875 (96.875)	Acc@5 99.609 (99.609)
Epoch: [278][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.0362 (0.0576)	Acc@1 98.828 (98.281)	Acc@5 100.000 (99.976)
Epoch: [278][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0848 (0.0567)	Acc@1 97.656 (98.283)	Acc@5 100.000 (99.985)
Epoch: [278][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0630 (0.0576)	Acc@1 98.828 (98.231)	Acc@5 99.609 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:279/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [279][0/196]	Time 0.124 (0.124)	Data 0.302 (0.302)	Loss 0.0295 (0.0295)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [279][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.0230 (0.0570)	Acc@1 99.609 (98.317)	Acc@5 100.000 (99.988)
Epoch: [279][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.0565 (0.0580)	Acc@1 98.828 (98.253)	Acc@5 100.000 (99.985)
Epoch: [279][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0629 (0.0581)	Acc@1 97.656 (98.247)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:280/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [280][0/196]	Time 0.117 (0.117)	Data 0.301 (0.301)	Loss 0.0426 (0.0426)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [280][64/196]	Time 0.083 (0.087)	Data 0.000 (0.005)	Loss 0.0523 (0.0608)	Acc@1 98.047 (98.065)	Acc@5 100.000 (99.970)
Epoch: [280][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.0646 (0.0605)	Acc@1 98.047 (98.135)	Acc@5 100.000 (99.973)
Epoch: [280][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0670 (0.0596)	Acc@1 98.438 (98.180)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.27
Max memory: 51.4381312
 17.393s  j: 286 bis 290
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5920
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 281
Max memory: 0.1097216
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:281/285; Lr: 0.00010941898913151243
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [281][0/196]	Time 0.137 (0.137)	Data 0.311 (0.311)	Loss 0.0704 (0.0704)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [281][64/196]	Time 0.093 (0.088)	Data 0.000 (0.005)	Loss 0.0404 (0.0553)	Acc@1 98.438 (98.305)	Acc@5 100.000 (99.988)
Epoch: [281][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.1117 (0.0570)	Acc@1 97.266 (98.216)	Acc@5 100.000 (99.985)
Epoch: [281][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0506 (0.0588)	Acc@1 98.438 (98.176)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:282/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [282][0/196]	Time 0.125 (0.125)	Data 0.270 (0.270)	Loss 0.0521 (0.0521)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [282][64/196]	Time 0.090 (0.088)	Data 0.000 (0.004)	Loss 0.0529 (0.0585)	Acc@1 98.438 (98.143)	Acc@5 100.000 (99.994)
Epoch: [282][128/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.1102 (0.0589)	Acc@1 96.094 (98.117)	Acc@5 100.000 (99.988)
Epoch: [282][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0613 (0.0595)	Acc@1 98.438 (98.124)	Acc@5 99.609 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:283/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [283][0/196]	Time 0.107 (0.107)	Data 0.299 (0.299)	Loss 0.0519 (0.0519)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [283][64/196]	Time 0.081 (0.089)	Data 0.000 (0.005)	Loss 0.0475 (0.0591)	Acc@1 98.438 (98.221)	Acc@5 100.000 (99.994)
Epoch: [283][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0662 (0.0587)	Acc@1 97.656 (98.144)	Acc@5 100.000 (99.997)
Epoch: [283][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0508 (0.0587)	Acc@1 97.656 (98.158)	Acc@5 100.000 (99.998)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:284/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [284][0/196]	Time 0.110 (0.110)	Data 0.328 (0.328)	Loss 0.0908 (0.0908)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [284][64/196]	Time 0.099 (0.088)	Data 0.000 (0.005)	Loss 0.0476 (0.0584)	Acc@1 98.438 (98.245)	Acc@5 100.000 (99.970)
Epoch: [284][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.0385 (0.0585)	Acc@1 99.219 (98.189)	Acc@5 100.000 (99.976)
Epoch: [284][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0598 (0.0587)	Acc@1 98.047 (98.156)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:285/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [285][0/196]	Time 0.141 (0.141)	Data 0.318 (0.318)	Loss 0.0392 (0.0392)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [285][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.0754 (0.0600)	Acc@1 96.875 (98.155)	Acc@5 100.000 (99.988)
Epoch: [285][128/196]	Time 0.079 (0.088)	Data 0.000 (0.003)	Loss 0.0539 (0.0590)	Acc@1 98.828 (98.192)	Acc@5 100.000 (99.982)
Epoch: [285][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0552 (0.0595)	Acc@1 97.266 (98.162)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.29
Max memory: 51.4381312
 17.552s  j: 291 bis 295
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9051
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 286
Max memory: 0.1097216
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:286/290; Lr: 9.847709021836118e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [286][0/196]	Time 0.159 (0.159)	Data 0.283 (0.283)	Loss 0.0727 (0.0727)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [286][64/196]	Time 0.093 (0.091)	Data 0.000 (0.005)	Loss 0.0370 (0.0620)	Acc@1 98.438 (98.125)	Acc@5 100.000 (99.982)
Epoch: [286][128/196]	Time 0.088 (0.090)	Data 0.000 (0.002)	Loss 0.0417 (0.0602)	Acc@1 99.219 (98.168)	Acc@5 100.000 (99.982)
Epoch: [286][192/196]	Time 0.084 (0.090)	Data 0.000 (0.002)	Loss 0.0412 (0.0597)	Acc@1 99.219 (98.158)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:287/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [287][0/196]	Time 0.133 (0.133)	Data 0.297 (0.297)	Loss 0.0521 (0.0521)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [287][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.0530 (0.0600)	Acc@1 98.047 (98.131)	Acc@5 100.000 (99.988)
Epoch: [287][128/196]	Time 0.095 (0.090)	Data 0.000 (0.003)	Loss 0.0431 (0.0589)	Acc@1 98.438 (98.171)	Acc@5 100.000 (99.988)
Epoch: [287][192/196]	Time 0.083 (0.090)	Data 0.000 (0.002)	Loss 0.0799 (0.0596)	Acc@1 96.875 (98.130)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:288/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [288][0/196]	Time 0.128 (0.128)	Data 0.305 (0.305)	Loss 0.0526 (0.0526)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [288][64/196]	Time 0.098 (0.090)	Data 0.000 (0.005)	Loss 0.0587 (0.0564)	Acc@1 98.438 (98.245)	Acc@5 100.000 (99.994)
Epoch: [288][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.0728 (0.0587)	Acc@1 96.094 (98.138)	Acc@5 100.000 (99.979)
Epoch: [288][192/196]	Time 0.092 (0.089)	Data 0.000 (0.002)	Loss 0.1005 (0.0593)	Acc@1 96.875 (98.077)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:289/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [289][0/196]	Time 0.109 (0.109)	Data 0.318 (0.318)	Loss 0.0658 (0.0658)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [289][64/196]	Time 0.099 (0.092)	Data 0.000 (0.005)	Loss 0.0497 (0.0558)	Acc@1 98.438 (98.245)	Acc@5 100.000 (99.994)
Epoch: [289][128/196]	Time 0.088 (0.090)	Data 0.000 (0.003)	Loss 0.0915 (0.0579)	Acc@1 97.266 (98.159)	Acc@5 100.000 (99.994)
Epoch: [289][192/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.0459 (0.0579)	Acc@1 98.828 (98.168)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:290/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [290][0/196]	Time 0.141 (0.141)	Data 0.289 (0.289)	Loss 0.0609 (0.0609)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [290][64/196]	Time 0.089 (0.091)	Data 0.000 (0.005)	Loss 0.0791 (0.0591)	Acc@1 96.094 (98.041)	Acc@5 100.000 (99.994)
Epoch: [290][128/196]	Time 0.086 (0.090)	Data 0.000 (0.002)	Loss 0.0504 (0.0576)	Acc@1 99.219 (98.162)	Acc@5 100.000 (99.991)
Epoch: [290][192/196]	Time 0.082 (0.090)	Data 0.000 (0.002)	Loss 0.0564 (0.0581)	Acc@1 97.656 (98.160)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.29
Max memory: 51.4381312
 17.921s  j: 296 bis 300
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6355
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 291
Max memory: 0.1097216
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:291/295; Lr: 8.862938119652506e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [291][0/196]	Time 0.144 (0.144)	Data 0.317 (0.317)	Loss 0.0506 (0.0506)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [291][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.0489 (0.0586)	Acc@1 97.266 (98.167)	Acc@5 100.000 (99.970)
Epoch: [291][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0567 (0.0570)	Acc@1 98.438 (98.180)	Acc@5 100.000 (99.982)
Epoch: [291][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0431 (0.0566)	Acc@1 98.438 (98.203)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:292/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [292][0/196]	Time 0.131 (0.131)	Data 0.296 (0.296)	Loss 0.0416 (0.0416)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [292][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.0462 (0.0538)	Acc@1 99.219 (98.486)	Acc@5 100.000 (99.988)
Epoch: [292][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0561 (0.0556)	Acc@1 98.828 (98.365)	Acc@5 100.000 (99.982)
Epoch: [292][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0523 (0.0565)	Acc@1 98.438 (98.340)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:293/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [293][0/196]	Time 0.132 (0.132)	Data 0.280 (0.280)	Loss 0.0647 (0.0647)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [293][64/196]	Time 0.081 (0.086)	Data 0.000 (0.005)	Loss 0.0557 (0.0596)	Acc@1 98.438 (98.179)	Acc@5 100.000 (99.988)
Epoch: [293][128/196]	Time 0.078 (0.086)	Data 0.000 (0.002)	Loss 0.0478 (0.0595)	Acc@1 98.047 (98.159)	Acc@5 100.000 (99.985)
Epoch: [293][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0328 (0.0593)	Acc@1 99.219 (98.108)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:294/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [294][0/196]	Time 0.135 (0.135)	Data 0.305 (0.305)	Loss 0.0318 (0.0318)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [294][64/196]	Time 0.101 (0.090)	Data 0.000 (0.005)	Loss 0.0455 (0.0555)	Acc@1 99.219 (98.227)	Acc@5 100.000 (99.988)
Epoch: [294][128/196]	Time 0.081 (0.089)	Data 0.000 (0.003)	Loss 0.0550 (0.0574)	Acc@1 98.438 (98.186)	Acc@5 100.000 (99.988)
Epoch: [294][192/196]	Time 0.095 (0.088)	Data 0.000 (0.002)	Loss 0.0623 (0.0568)	Acc@1 97.656 (98.235)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:295/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [295][0/196]	Time 0.127 (0.127)	Data 0.307 (0.307)	Loss 0.0766 (0.0766)	Acc@1 98.047 (98.047)	Acc@5 99.609 (99.609)
Epoch: [295][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.0367 (0.0597)	Acc@1 98.828 (98.161)	Acc@5 100.000 (99.976)
Epoch: [295][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0511 (0.0574)	Acc@1 98.047 (98.204)	Acc@5 100.000 (99.982)
Epoch: [295][192/196]	Time 0.096 (0.088)	Data 0.000 (0.002)	Loss 0.0610 (0.0574)	Acc@1 97.656 (98.211)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.27
Max memory: 51.4381312
 17.715s  j: 301 bis 305
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3482
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 296
Max memory: 0.1097216
lr: 7.976644307687256e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:296/300; Lr: 7.976644307687256e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [296][0/196]	Time 0.150 (0.150)	Data 0.283 (0.283)	Loss 0.0453 (0.0453)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [296][64/196]	Time 0.095 (0.088)	Data 0.000 (0.005)	Loss 0.0447 (0.0571)	Acc@1 98.828 (98.239)	Acc@5 100.000 (99.988)
Epoch: [296][128/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0547 (0.0582)	Acc@1 98.047 (98.147)	Acc@5 100.000 (99.988)
Epoch: [296][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0817 (0.0575)	Acc@1 96.484 (98.178)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 7.976644307687256e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:297/300; Lr: 7.976644307687256e-05
batch Size 256
Epoch: [297][0/196]	Time 0.123 (0.123)	Data 0.302 (0.302)	Loss 0.1344 (0.1344)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [297][64/196]	Time 0.093 (0.087)	Data 0.000 (0.005)	Loss 0.0526 (0.0580)	Acc@1 98.047 (98.263)	Acc@5 100.000 (99.982)
Epoch: [297][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.0549 (0.0571)	Acc@1 98.047 (98.271)	Acc@5 100.000 (99.985)
Epoch: [297][192/196]	Time 0.081 (0.086)	Data 0.000 (0.002)	Loss 0.0776 (0.0587)	Acc@1 97.656 (98.187)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 7.976644307687256e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:298/300; Lr: 7.976644307687256e-05
batch Size 256
Epoch: [298][0/196]	Time 0.125 (0.125)	Data 0.262 (0.262)	Loss 0.0715 (0.0715)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [298][64/196]	Time 0.082 (0.086)	Data 0.000 (0.004)	Loss 0.0707 (0.0562)	Acc@1 98.047 (98.365)	Acc@5 99.609 (99.970)
Epoch: [298][128/196]	Time 0.098 (0.086)	Data 0.000 (0.002)	Loss 0.0789 (0.0559)	Acc@1 97.266 (98.332)	Acc@5 100.000 (99.979)
Epoch: [298][192/196]	Time 0.091 (0.086)	Data 0.000 (0.002)	Loss 0.0739 (0.0564)	Acc@1 97.656 (98.304)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 7.976644307687256e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:299/300; Lr: 7.976644307687256e-05
batch Size 256
Epoch: [299][0/196]	Time 0.134 (0.134)	Data 0.320 (0.320)	Loss 0.0520 (0.0520)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [299][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.0579 (0.0566)	Acc@1 99.219 (98.359)	Acc@5 100.000 (99.988)
Epoch: [299][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0546 (0.0577)	Acc@1 97.656 (98.262)	Acc@5 100.000 (99.994)
Epoch: [299][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0655 (0.0579)	Acc@1 98.438 (98.237)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 7.976644307687256e-05
args.lr: 7.17897987691853e-05
lr: 7.17897987691853e-05
Epoche:300/300; Lr: 7.976644307687256e-05
batch Size 256
Epoch: [300][0/196]	Time 0.130 (0.130)	Data 0.317 (0.317)	Loss 0.0388 (0.0388)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [300][64/196]	Time 0.101 (0.088)	Data 0.000 (0.005)	Loss 0.0721 (0.0555)	Acc@1 98.438 (98.347)	Acc@5 100.000 (99.982)
Epoch: [300][128/196]	Time 0.095 (0.087)	Data 0.000 (0.003)	Loss 0.0531 (0.0565)	Acc@1 98.438 (98.244)	Acc@5 100.000 (99.979)
Epoch: [300][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0411 (0.0558)	Acc@1 98.828 (98.263)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.29
Max memory: 51.4381312
 17.433s  j: 306 bis 310
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4205
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 301
Max memory: 0.1097216
lr: 7.17897987691853e-05
args.lr: 7.17897987691853e-05
lr: 7.17897987691853e-05
Epoche:301/305; Lr: 7.17897987691853e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [301][0/196]	Time 0.149 (0.149)	Data 0.295 (0.295)	Loss 0.0723 (0.0723)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [301][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.1055 (0.0604)	Acc@1 97.266 (98.065)	Acc@5 100.000 (99.988)
Epoch: [301][128/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.0674 (0.0573)	Acc@1 98.438 (98.219)	Acc@5 99.609 (99.985)
Epoch: [301][192/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.0960 (0.0580)	Acc@1 97.266 (98.235)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 7.17897987691853e-05
args.lr: 7.17897987691853e-05
lr: 7.17897987691853e-05
Epoche:302/305; Lr: 7.17897987691853e-05
batch Size 256
Epoch: [302][0/196]	Time 0.127 (0.127)	Data 0.302 (0.302)	Loss 0.0743 (0.0743)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [302][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.0704 (0.0576)	Acc@1 97.266 (98.305)	Acc@5 100.000 (99.976)
Epoch: [302][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.0578 (0.0588)	Acc@1 98.438 (98.265)	Acc@5 100.000 (99.982)
Epoch: [302][192/196]	Time 0.096 (0.087)	Data 0.000 (0.002)	Loss 0.0741 (0.0583)	Acc@1 97.266 (98.225)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 7.17897987691853e-05
args.lr: 7.17897987691853e-05
lr: 7.17897987691853e-05
Epoche:303/305; Lr: 7.17897987691853e-05
batch Size 256
Epoch: [303][0/196]	Time 0.126 (0.126)	Data 0.282 (0.282)	Loss 0.0601 (0.0601)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [303][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.0872 (0.0611)	Acc@1 97.266 (98.113)	Acc@5 100.000 (99.988)
Epoch: [303][128/196]	Time 0.077 (0.089)	Data 0.000 (0.002)	Loss 0.0266 (0.0581)	Acc@1 99.609 (98.195)	Acc@5 100.000 (99.988)
Epoch: [303][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0343 (0.0574)	Acc@1 100.000 (98.251)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 7.17897987691853e-05
args.lr: 7.17897987691853e-05
lr: 7.17897987691853e-05
Epoche:304/305; Lr: 7.17897987691853e-05
batch Size 256
Epoch: [304][0/196]	Time 0.135 (0.135)	Data 0.327 (0.327)	Loss 0.0386 (0.0386)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [304][64/196]	Time 0.096 (0.089)	Data 0.000 (0.005)	Loss 0.0585 (0.0540)	Acc@1 98.828 (98.444)	Acc@5 100.000 (99.988)
Epoch: [304][128/196]	Time 0.090 (0.089)	Data 0.000 (0.003)	Loss 0.0299 (0.0562)	Acc@1 99.219 (98.392)	Acc@5 100.000 (99.994)
Epoch: [304][192/196]	Time 0.095 (0.089)	Data 0.000 (0.002)	Loss 0.0674 (0.0578)	Acc@1 98.047 (98.272)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 7.17897987691853e-05
args.lr: 6.461081889226677e-05
lr: 6.461081889226677e-05
Epoche:305/305; Lr: 7.17897987691853e-05
batch Size 256
Epoch: [305][0/196]	Time 0.116 (0.116)	Data 0.306 (0.306)	Loss 0.0597 (0.0597)	Acc@1 98.438 (98.438)	Acc@5 99.609 (99.609)
Epoch: [305][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.0463 (0.0575)	Acc@1 99.219 (98.257)	Acc@5 100.000 (99.994)
Epoch: [305][128/196]	Time 0.096 (0.089)	Data 0.000 (0.003)	Loss 0.0311 (0.0576)	Acc@1 100.000 (98.226)	Acc@5 100.000 (99.985)
Epoch: [305][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0522 (0.0577)	Acc@1 98.438 (98.213)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.21
Max memory: 51.4381312
 17.797s  j: 311 bis 315
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5340
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 306
Max memory: 0.1097216
lr: 6.461081889226677e-05
args.lr: 6.461081889226677e-05
lr: 6.461081889226677e-05
Epoche:306/310; Lr: 6.461081889226677e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [306][0/196]	Time 0.161 (0.161)	Data 0.262 (0.262)	Loss 0.0559 (0.0559)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [306][64/196]	Time 0.087 (0.089)	Data 0.000 (0.004)	Loss 0.0790 (0.0577)	Acc@1 96.484 (98.179)	Acc@5 100.000 (99.988)
Epoch: [306][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0617 (0.0572)	Acc@1 97.266 (98.229)	Acc@5 100.000 (99.985)
Epoch: [306][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.0908 (0.0576)	Acc@1 95.703 (98.142)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 6.461081889226677e-05
args.lr: 6.461081889226677e-05
lr: 6.461081889226677e-05
Epoche:307/310; Lr: 6.461081889226677e-05
batch Size 256
Epoch: [307][0/196]	Time 0.115 (0.115)	Data 0.303 (0.303)	Loss 0.0581 (0.0581)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [307][64/196]	Time 0.080 (0.087)	Data 0.000 (0.005)	Loss 0.0541 (0.0558)	Acc@1 98.047 (98.167)	Acc@5 100.000 (99.988)
Epoch: [307][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.0884 (0.0564)	Acc@1 97.266 (98.198)	Acc@5 100.000 (99.994)
Epoch: [307][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0840 (0.0579)	Acc@1 98.047 (98.156)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 6.461081889226677e-05
args.lr: 6.461081889226677e-05
lr: 6.461081889226677e-05
Epoche:308/310; Lr: 6.461081889226677e-05
batch Size 256
Epoch: [308][0/196]	Time 0.135 (0.135)	Data 0.286 (0.286)	Loss 0.0443 (0.0443)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [308][64/196]	Time 0.098 (0.089)	Data 0.000 (0.005)	Loss 0.0954 (0.0603)	Acc@1 95.703 (98.029)	Acc@5 100.000 (99.988)
Epoch: [308][128/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0478 (0.0599)	Acc@1 98.438 (98.098)	Acc@5 100.000 (99.982)
Epoch: [308][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.0444 (0.0585)	Acc@1 98.828 (98.142)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 6.461081889226677e-05
args.lr: 6.461081889226677e-05
lr: 6.461081889226677e-05
Epoche:309/310; Lr: 6.461081889226677e-05
batch Size 256
Epoch: [309][0/196]	Time 0.132 (0.132)	Data 0.296 (0.296)	Loss 0.0641 (0.0641)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [309][64/196]	Time 0.094 (0.089)	Data 0.000 (0.005)	Loss 0.0495 (0.0592)	Acc@1 98.438 (98.155)	Acc@5 100.000 (99.976)
Epoch: [309][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0607 (0.0566)	Acc@1 98.047 (98.301)	Acc@5 100.000 (99.988)
Epoch: [309][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0668 (0.0566)	Acc@1 98.047 (98.286)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 6.461081889226677e-05
args.lr: 5.81497370030401e-05
lr: 5.81497370030401e-05
Epoche:310/310; Lr: 6.461081889226677e-05
batch Size 256
Epoch: [310][0/196]	Time 0.130 (0.130)	Data 0.317 (0.317)	Loss 0.0598 (0.0598)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [310][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0730 (0.0578)	Acc@1 97.266 (98.125)	Acc@5 100.000 (99.994)
Epoch: [310][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0497 (0.0576)	Acc@1 99.219 (98.168)	Acc@5 100.000 (99.979)
Epoch: [310][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0558 (0.0576)	Acc@1 98.438 (98.221)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.2
Max memory: 51.4381312
 17.553s  j: 316 bis 320
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9121
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 311
Max memory: 0.1097216
lr: 5.81497370030401e-05
args.lr: 5.81497370030401e-05
lr: 5.81497370030401e-05
Epoche:311/315; Lr: 5.81497370030401e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [311][0/196]	Time 0.172 (0.172)	Data 0.265 (0.265)	Loss 0.0444 (0.0444)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [311][64/196]	Time 0.086 (0.088)	Data 0.000 (0.004)	Loss 0.0657 (0.0594)	Acc@1 96.875 (98.095)	Acc@5 100.000 (99.970)
Epoch: [311][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0532 (0.0574)	Acc@1 97.656 (98.198)	Acc@5 100.000 (99.979)
Epoch: [311][192/196]	Time 0.078 (0.088)	Data 0.000 (0.002)	Loss 0.0475 (0.0567)	Acc@1 98.047 (98.243)	Acc@5 99.609 (99.980)
Max memory in training epoch: 33.3541888
lr: 5.81497370030401e-05
args.lr: 5.81497370030401e-05
lr: 5.81497370030401e-05
Epoche:312/315; Lr: 5.81497370030401e-05
batch Size 256
Epoch: [312][0/196]	Time 0.124 (0.124)	Data 0.287 (0.287)	Loss 0.0937 (0.0937)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [312][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0910 (0.0597)	Acc@1 97.656 (98.197)	Acc@5 100.000 (99.982)
Epoch: [312][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0596 (0.0577)	Acc@1 98.047 (98.286)	Acc@5 100.000 (99.976)
Epoch: [312][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0638 (0.0573)	Acc@1 97.656 (98.263)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 5.81497370030401e-05
args.lr: 5.81497370030401e-05
lr: 5.81497370030401e-05
Epoche:313/315; Lr: 5.81497370030401e-05
batch Size 256
Epoch: [313][0/196]	Time 0.131 (0.131)	Data 0.305 (0.305)	Loss 0.0470 (0.0470)	Acc@1 99.219 (99.219)	Acc@5 99.609 (99.609)
Epoch: [313][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0661 (0.0577)	Acc@1 97.656 (98.209)	Acc@5 100.000 (99.976)
Epoch: [313][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.0581 (0.0588)	Acc@1 97.656 (98.168)	Acc@5 100.000 (99.976)
Epoch: [313][192/196]	Time 0.093 (0.087)	Data 0.000 (0.002)	Loss 0.0670 (0.0585)	Acc@1 96.484 (98.122)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 5.81497370030401e-05
args.lr: 5.81497370030401e-05
lr: 5.81497370030401e-05
Epoche:314/315; Lr: 5.81497370030401e-05
batch Size 256
Epoch: [314][0/196]	Time 0.145 (0.145)	Data 0.282 (0.282)	Loss 0.0996 (0.0996)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [314][64/196]	Time 0.097 (0.088)	Data 0.000 (0.005)	Loss 0.0694 (0.0629)	Acc@1 97.656 (97.981)	Acc@5 100.000 (99.982)
Epoch: [314][128/196]	Time 0.102 (0.088)	Data 0.000 (0.002)	Loss 0.0706 (0.0612)	Acc@1 97.266 (98.080)	Acc@5 100.000 (99.982)
Epoch: [314][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0735 (0.0594)	Acc@1 98.047 (98.128)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 5.81497370030401e-05
args.lr: 5.233476330273609e-05
lr: 5.233476330273609e-05
Epoche:315/315; Lr: 5.81497370030401e-05
batch Size 256
Epoch: [315][0/196]	Time 0.128 (0.128)	Data 0.349 (0.349)	Loss 0.0642 (0.0642)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [315][64/196]	Time 0.086 (0.088)	Data 0.000 (0.006)	Loss 0.0903 (0.0545)	Acc@1 97.266 (98.377)	Acc@5 100.000 (99.988)
Epoch: [315][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.0595 (0.0558)	Acc@1 97.656 (98.265)	Acc@5 100.000 (99.991)
Epoch: [315][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0316 (0.0563)	Acc@1 99.609 (98.298)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.21
Max memory: 51.4381312
 17.488s  j: 321 bis 325
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6772
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 316
Max memory: 0.1097216
lr: 5.233476330273609e-05
args.lr: 5.233476330273609e-05
lr: 5.233476330273609e-05
Epoche:316/320; Lr: 5.233476330273609e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [316][0/196]	Time 0.148 (0.148)	Data 0.287 (0.287)	Loss 0.0451 (0.0451)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [316][64/196]	Time 0.093 (0.088)	Data 0.000 (0.005)	Loss 0.0667 (0.0566)	Acc@1 97.266 (98.173)	Acc@5 100.000 (99.994)
Epoch: [316][128/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0683 (0.0574)	Acc@1 98.047 (98.210)	Acc@5 100.000 (99.994)
Epoch: [316][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0591 (0.0575)	Acc@1 98.047 (98.201)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 5.233476330273609e-05
args.lr: 5.233476330273609e-05
lr: 5.233476330273609e-05
Epoche:317/320; Lr: 5.233476330273609e-05
batch Size 256
Epoch: [317][0/196]	Time 0.139 (0.139)	Data 0.286 (0.286)	Loss 0.0542 (0.0542)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [317][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.0862 (0.0571)	Acc@1 97.656 (98.305)	Acc@5 100.000 (99.994)
Epoch: [317][128/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0803 (0.0576)	Acc@1 96.875 (98.219)	Acc@5 100.000 (99.997)
Epoch: [317][192/196]	Time 0.097 (0.088)	Data 0.000 (0.002)	Loss 0.0529 (0.0579)	Acc@1 98.047 (98.191)	Acc@5 100.000 (99.996)
Max memory in training epoch: 33.3541888
lr: 5.233476330273609e-05
args.lr: 5.233476330273609e-05
lr: 5.233476330273609e-05
Epoche:318/320; Lr: 5.233476330273609e-05
batch Size 256
Epoch: [318][0/196]	Time 0.125 (0.125)	Data 0.325 (0.325)	Loss 0.0949 (0.0949)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [318][64/196]	Time 0.095 (0.088)	Data 0.000 (0.005)	Loss 0.0612 (0.0542)	Acc@1 98.438 (98.407)	Acc@5 100.000 (99.976)
Epoch: [318][128/196]	Time 0.090 (0.087)	Data 0.000 (0.003)	Loss 0.0776 (0.0558)	Acc@1 97.656 (98.313)	Acc@5 100.000 (99.985)
Epoch: [318][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0812 (0.0574)	Acc@1 96.484 (98.255)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 5.233476330273609e-05
args.lr: 5.233476330273609e-05
lr: 5.233476330273609e-05
Epoche:319/320; Lr: 5.233476330273609e-05
batch Size 256
Epoch: [319][0/196]	Time 0.133 (0.133)	Data 0.326 (0.326)	Loss 0.0634 (0.0634)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [319][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0654 (0.0556)	Acc@1 98.047 (98.335)	Acc@5 100.000 (99.982)
Epoch: [319][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0795 (0.0560)	Acc@1 97.656 (98.235)	Acc@5 100.000 (99.985)
Epoch: [319][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0630 (0.0568)	Acc@1 98.047 (98.219)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 5.233476330273609e-05
args.lr: 4.7101286972462485e-05
lr: 4.7101286972462485e-05
Epoche:320/320; Lr: 5.233476330273609e-05
batch Size 256
Epoch: [320][0/196]	Time 0.128 (0.128)	Data 0.292 (0.292)	Loss 0.0378 (0.0378)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [320][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0363 (0.0566)	Acc@1 99.609 (98.263)	Acc@5 100.000 (99.988)
Epoch: [320][128/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.0679 (0.0561)	Acc@1 97.656 (98.286)	Acc@5 100.000 (99.985)
Epoch: [320][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0324 (0.0561)	Acc@1 100.000 (98.274)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.27
Max memory: 51.4381312
 17.440s  j: 326 bis 330
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 833
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 321
Max memory: 0.1097216
lr: 4.7101286972462485e-05
args.lr: 4.7101286972462485e-05
lr: 4.7101286972462485e-05
Epoche:321/325; Lr: 4.7101286972462485e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [321][0/196]	Time 0.154 (0.154)	Data 0.288 (0.288)	Loss 0.0438 (0.0438)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [321][64/196]	Time 0.081 (0.087)	Data 0.000 (0.005)	Loss 0.0235 (0.0524)	Acc@1 99.219 (98.383)	Acc@5 100.000 (100.000)
Epoch: [321][128/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0744 (0.0545)	Acc@1 96.875 (98.295)	Acc@5 100.000 (99.994)
Epoch: [321][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0698 (0.0564)	Acc@1 96.875 (98.191)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 4.7101286972462485e-05
args.lr: 4.7101286972462485e-05
lr: 4.7101286972462485e-05
Epoche:322/325; Lr: 4.7101286972462485e-05
batch Size 256
Epoch: [322][0/196]	Time 0.110 (0.110)	Data 0.283 (0.283)	Loss 0.0719 (0.0719)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [322][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0455 (0.0589)	Acc@1 98.438 (98.185)	Acc@5 100.000 (99.988)
Epoch: [322][128/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.0866 (0.0579)	Acc@1 96.875 (98.213)	Acc@5 100.000 (99.979)
Epoch: [322][192/196]	Time 0.087 (0.086)	Data 0.000 (0.002)	Loss 0.0502 (0.0574)	Acc@1 98.828 (98.253)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 4.7101286972462485e-05
args.lr: 4.7101286972462485e-05
lr: 4.7101286972462485e-05
Epoche:323/325; Lr: 4.7101286972462485e-05
batch Size 256
Epoch: [323][0/196]	Time 0.126 (0.126)	Data 0.325 (0.325)	Loss 0.0626 (0.0626)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [323][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.0674 (0.0552)	Acc@1 97.266 (98.173)	Acc@5 100.000 (99.988)
Epoch: [323][128/196]	Time 0.103 (0.088)	Data 0.000 (0.003)	Loss 0.0558 (0.0558)	Acc@1 98.438 (98.223)	Acc@5 100.000 (99.991)
Epoch: [323][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0353 (0.0559)	Acc@1 98.828 (98.209)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 4.7101286972462485e-05
args.lr: 4.7101286972462485e-05
lr: 4.7101286972462485e-05
Epoche:324/325; Lr: 4.7101286972462485e-05
batch Size 256
Epoch: [324][0/196]	Time 0.136 (0.136)	Data 0.295 (0.295)	Loss 0.0543 (0.0543)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [324][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0446 (0.0558)	Acc@1 98.438 (98.365)	Acc@5 100.000 (99.988)
Epoch: [324][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.0540 (0.0556)	Acc@1 98.047 (98.380)	Acc@5 100.000 (99.982)
Epoch: [324][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0651 (0.0560)	Acc@1 97.656 (98.332)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 4.7101286972462485e-05
args.lr: 4.239115827521624e-05
lr: 4.239115827521624e-05
Epoche:325/325; Lr: 4.7101286972462485e-05
batch Size 256
Epoch: [325][0/196]	Time 0.125 (0.125)	Data 0.297 (0.297)	Loss 0.0448 (0.0448)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [325][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0731 (0.0563)	Acc@1 97.266 (98.185)	Acc@5 100.000 (99.994)
Epoch: [325][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0363 (0.0564)	Acc@1 99.219 (98.238)	Acc@5 100.000 (99.991)
Epoch: [325][192/196]	Time 0.077 (0.087)	Data 0.000 (0.002)	Loss 0.0603 (0.0572)	Acc@1 98.438 (98.233)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.26
Max memory: 51.4381312
 17.439s  j: 331 bis 335
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1770
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 326
Max memory: 0.1097216
lr: 4.239115827521624e-05
args.lr: 4.239115827521624e-05
lr: 4.239115827521624e-05
Epoche:326/330; Lr: 4.239115827521624e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [326][0/196]	Time 0.137 (0.137)	Data 0.265 (0.265)	Loss 0.0724 (0.0724)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [326][64/196]	Time 0.084 (0.087)	Data 0.000 (0.004)	Loss 0.0442 (0.0574)	Acc@1 99.219 (98.347)	Acc@5 100.000 (99.982)
Epoch: [326][128/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0561 (0.0561)	Acc@1 98.438 (98.350)	Acc@5 100.000 (99.985)
Epoch: [326][192/196]	Time 0.108 (0.087)	Data 0.000 (0.002)	Loss 0.0565 (0.0570)	Acc@1 98.047 (98.284)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 4.239115827521624e-05
args.lr: 4.239115827521624e-05
lr: 4.239115827521624e-05
Epoche:327/330; Lr: 4.239115827521624e-05
batch Size 256
Epoch: [327][0/196]	Time 0.107 (0.107)	Data 0.313 (0.313)	Loss 0.0719 (0.0719)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [327][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.0455 (0.0602)	Acc@1 98.828 (98.035)	Acc@5 100.000 (99.976)
Epoch: [327][128/196]	Time 0.099 (0.087)	Data 0.000 (0.003)	Loss 0.0520 (0.0582)	Acc@1 98.438 (98.201)	Acc@5 100.000 (99.979)
Epoch: [327][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.0669 (0.0582)	Acc@1 98.047 (98.166)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 4.239115827521624e-05
args.lr: 4.239115827521624e-05
lr: 4.239115827521624e-05
Epoche:328/330; Lr: 4.239115827521624e-05
batch Size 256
Epoch: [328][0/196]	Time 0.123 (0.123)	Data 0.303 (0.303)	Loss 0.0764 (0.0764)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [328][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0476 (0.0572)	Acc@1 98.047 (98.215)	Acc@5 100.000 (99.982)
Epoch: [328][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.0545 (0.0587)	Acc@1 98.438 (98.183)	Acc@5 100.000 (99.988)
Epoch: [328][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0356 (0.0574)	Acc@1 99.609 (98.203)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 4.239115827521624e-05
args.lr: 4.239115827521624e-05
lr: 4.239115827521624e-05
Epoche:329/330; Lr: 4.239115827521624e-05
batch Size 256
Epoch: [329][0/196]	Time 0.124 (0.124)	Data 0.312 (0.312)	Loss 0.1109 (0.1109)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [329][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0664 (0.0568)	Acc@1 98.438 (98.209)	Acc@5 100.000 (100.000)
Epoch: [329][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.0394 (0.0568)	Acc@1 98.828 (98.189)	Acc@5 100.000 (99.994)
Epoch: [329][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0413 (0.0567)	Acc@1 99.219 (98.209)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 4.239115827521624e-05
args.lr: 3.8152042447694614e-05
lr: 3.8152042447694614e-05
Epoche:330/330; Lr: 4.239115827521624e-05
batch Size 256
Epoch: [330][0/196]	Time 0.124 (0.124)	Data 0.335 (0.335)	Loss 0.0700 (0.0700)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [330][64/196]	Time 0.106 (0.088)	Data 0.000 (0.005)	Loss 0.0565 (0.0588)	Acc@1 98.438 (98.317)	Acc@5 100.000 (99.988)
Epoch: [330][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.0912 (0.0586)	Acc@1 96.875 (98.219)	Acc@5 100.000 (99.988)
Epoch: [330][192/196]	Time 0.087 (0.086)	Data 0.000 (0.002)	Loss 0.0400 (0.0591)	Acc@1 98.828 (98.187)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.18
Max memory: 51.4381312
 17.354s  j: 336 bis 340
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4616
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 331
Max memory: 0.1097216
lr: 3.8152042447694614e-05
args.lr: 3.8152042447694614e-05
lr: 3.8152042447694614e-05
Epoche:331/335; Lr: 3.8152042447694614e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [331][0/196]	Time 0.156 (0.156)	Data 0.306 (0.306)	Loss 0.0559 (0.0559)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [331][64/196]	Time 0.090 (0.091)	Data 0.000 (0.005)	Loss 0.0437 (0.0542)	Acc@1 98.828 (98.347)	Acc@5 100.000 (99.988)
Epoch: [331][128/196]	Time 0.089 (0.090)	Data 0.000 (0.003)	Loss 0.0467 (0.0553)	Acc@1 99.219 (98.322)	Acc@5 100.000 (99.982)
Epoch: [331][192/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.0568 (0.0552)	Acc@1 98.438 (98.312)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 3.8152042447694614e-05
args.lr: 3.8152042447694614e-05
lr: 3.8152042447694614e-05
Epoche:332/335; Lr: 3.8152042447694614e-05
batch Size 256
Epoch: [332][0/196]	Time 0.099 (0.099)	Data 0.293 (0.293)	Loss 0.0369 (0.0369)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [332][64/196]	Time 0.090 (0.089)	Data 0.000 (0.005)	Loss 0.0313 (0.0579)	Acc@1 99.609 (98.263)	Acc@5 100.000 (99.988)
Epoch: [332][128/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.0647 (0.0579)	Acc@1 98.047 (98.235)	Acc@5 100.000 (99.991)
Epoch: [332][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.0418 (0.0577)	Acc@1 98.828 (98.221)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 3.8152042447694614e-05
args.lr: 3.8152042447694614e-05
lr: 3.8152042447694614e-05
Epoche:333/335; Lr: 3.8152042447694614e-05
batch Size 256
Epoch: [333][0/196]	Time 0.127 (0.127)	Data 0.296 (0.296)	Loss 0.0324 (0.0324)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [333][64/196]	Time 0.094 (0.091)	Data 0.000 (0.005)	Loss 0.0634 (0.0564)	Acc@1 98.438 (98.335)	Acc@5 100.000 (99.982)
Epoch: [333][128/196]	Time 0.082 (0.090)	Data 0.000 (0.003)	Loss 0.0722 (0.0577)	Acc@1 97.266 (98.265)	Acc@5 100.000 (99.985)
Epoch: [333][192/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.0762 (0.0575)	Acc@1 97.266 (98.255)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 3.8152042447694614e-05
args.lr: 3.8152042447694614e-05
lr: 3.8152042447694614e-05
Epoche:334/335; Lr: 3.8152042447694614e-05
batch Size 256
Epoch: [334][0/196]	Time 0.125 (0.125)	Data 0.303 (0.303)	Loss 0.0790 (0.0790)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [334][64/196]	Time 0.090 (0.089)	Data 0.000 (0.005)	Loss 0.0688 (0.0585)	Acc@1 98.047 (98.227)	Acc@5 99.609 (99.970)
Epoch: [334][128/196]	Time 0.100 (0.089)	Data 0.000 (0.003)	Loss 0.0571 (0.0558)	Acc@1 98.438 (98.313)	Acc@5 100.000 (99.976)
Epoch: [334][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0481 (0.0565)	Acc@1 98.438 (98.298)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 3.8152042447694614e-05
args.lr: 3.433683820292515e-05
lr: 3.433683820292515e-05
Epoche:335/335; Lr: 3.8152042447694614e-05
batch Size 256
Epoch: [335][0/196]	Time 0.141 (0.141)	Data 0.326 (0.326)	Loss 0.0730 (0.0730)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [335][64/196]	Time 0.091 (0.091)	Data 0.000 (0.005)	Loss 0.0703 (0.0571)	Acc@1 97.266 (98.197)	Acc@5 100.000 (99.988)
Epoch: [335][128/196]	Time 0.088 (0.090)	Data 0.000 (0.003)	Loss 0.0417 (0.0552)	Acc@1 99.219 (98.307)	Acc@5 100.000 (99.982)
Epoch: [335][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.1030 (0.0569)	Acc@1 97.266 (98.239)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.23
Max memory: 51.4381312
 17.895s  j: 341 bis 345
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5108
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 336
Max memory: 0.1097216
lr: 3.433683820292515e-05
args.lr: 3.433683820292515e-05
lr: 3.433683820292515e-05
Epoche:336/340; Lr: 3.433683820292515e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [336][0/196]	Time 0.130 (0.130)	Data 0.296 (0.296)	Loss 0.0267 (0.0267)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [336][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.1086 (0.0554)	Acc@1 96.875 (98.299)	Acc@5 100.000 (99.988)
Epoch: [336][128/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0374 (0.0566)	Acc@1 98.047 (98.286)	Acc@5 100.000 (99.988)
Epoch: [336][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0465 (0.0577)	Acc@1 98.047 (98.259)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 3.433683820292515e-05
args.lr: 3.433683820292515e-05
lr: 3.433683820292515e-05
Epoche:337/340; Lr: 3.433683820292515e-05
batch Size 256
Epoch: [337][0/196]	Time 0.130 (0.130)	Data 0.320 (0.320)	Loss 0.0678 (0.0678)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [337][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0489 (0.0552)	Acc@1 98.438 (98.269)	Acc@5 100.000 (99.982)
Epoch: [337][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.0517 (0.0572)	Acc@1 98.438 (98.183)	Acc@5 100.000 (99.985)
Epoch: [337][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0448 (0.0572)	Acc@1 99.219 (98.227)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 3.433683820292515e-05
args.lr: 3.433683820292515e-05
lr: 3.433683820292515e-05
Epoche:338/340; Lr: 3.433683820292515e-05
batch Size 256
Epoch: [338][0/196]	Time 0.114 (0.114)	Data 0.295 (0.295)	Loss 0.0472 (0.0472)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [338][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.0693 (0.0569)	Acc@1 97.266 (98.293)	Acc@5 100.000 (99.982)
Epoch: [338][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.0727 (0.0585)	Acc@1 98.047 (98.192)	Acc@5 100.000 (99.985)
Epoch: [338][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0509 (0.0584)	Acc@1 99.219 (98.197)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 3.433683820292515e-05
args.lr: 3.433683820292515e-05
lr: 3.433683820292515e-05
Epoche:339/340; Lr: 3.433683820292515e-05
batch Size 256
Epoch: [339][0/196]	Time 0.127 (0.127)	Data 0.301 (0.301)	Loss 0.0643 (0.0643)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [339][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.0967 (0.0568)	Acc@1 97.266 (98.179)	Acc@5 100.000 (99.994)
Epoch: [339][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.0386 (0.0564)	Acc@1 98.828 (98.274)	Acc@5 100.000 (99.988)
Epoch: [339][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0384 (0.0565)	Acc@1 98.828 (98.282)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 3.433683820292515e-05
args.lr: 3.090315438263264e-05
lr: 3.090315438263264e-05
Epoche:340/340; Lr: 3.433683820292515e-05
batch Size 256
Epoch: [340][0/196]	Time 0.110 (0.110)	Data 0.280 (0.280)	Loss 0.0647 (0.0647)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [340][64/196]	Time 0.100 (0.087)	Data 0.000 (0.005)	Loss 0.0656 (0.0542)	Acc@1 98.438 (98.317)	Acc@5 99.609 (99.988)
Epoch: [340][128/196]	Time 0.082 (0.086)	Data 0.000 (0.002)	Loss 0.0496 (0.0560)	Acc@1 98.438 (98.256)	Acc@5 100.000 (99.985)
Epoch: [340][192/196]	Time 0.085 (0.086)	Data 0.000 (0.002)	Loss 0.0521 (0.0572)	Acc@1 98.047 (98.191)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.25
Max memory: 51.4381312
 17.188s  j: 346 bis 350
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8869
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 341
Max memory: 0.1097216
lr: 3.090315438263264e-05
args.lr: 3.090315438263264e-05
lr: 3.090315438263264e-05
Epoche:341/345; Lr: 3.090315438263264e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [341][0/196]	Time 0.131 (0.131)	Data 0.316 (0.316)	Loss 0.0543 (0.0543)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [341][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0942 (0.0560)	Acc@1 97.656 (98.287)	Acc@5 100.000 (99.976)
Epoch: [341][128/196]	Time 0.092 (0.087)	Data 0.000 (0.003)	Loss 0.0687 (0.0563)	Acc@1 97.656 (98.277)	Acc@5 100.000 (99.979)
Epoch: [341][192/196]	Time 0.093 (0.087)	Data 0.000 (0.002)	Loss 0.0643 (0.0558)	Acc@1 98.438 (98.326)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 3.090315438263264e-05
args.lr: 3.090315438263264e-05
lr: 3.090315438263264e-05
Epoche:342/345; Lr: 3.090315438263264e-05
batch Size 256
Epoch: [342][0/196]	Time 0.141 (0.141)	Data 0.306 (0.306)	Loss 0.0403 (0.0403)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [342][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0413 (0.0566)	Acc@1 98.828 (98.293)	Acc@5 100.000 (99.988)
Epoch: [342][128/196]	Time 0.098 (0.088)	Data 0.000 (0.003)	Loss 0.0548 (0.0564)	Acc@1 99.219 (98.283)	Acc@5 100.000 (99.991)
Epoch: [342][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0471 (0.0565)	Acc@1 98.828 (98.247)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 3.090315438263264e-05
args.lr: 3.090315438263264e-05
lr: 3.090315438263264e-05
Epoche:343/345; Lr: 3.090315438263264e-05
batch Size 256
Epoch: [343][0/196]	Time 0.101 (0.101)	Data 0.311 (0.311)	Loss 0.0500 (0.0500)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [343][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.0605 (0.0588)	Acc@1 96.875 (98.185)	Acc@5 100.000 (99.988)
Epoch: [343][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.0499 (0.0575)	Acc@1 98.828 (98.229)	Acc@5 100.000 (99.982)
Epoch: [343][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0698 (0.0565)	Acc@1 98.047 (98.225)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 3.090315438263264e-05
args.lr: 3.090315438263264e-05
lr: 3.090315438263264e-05
Epoche:344/345; Lr: 3.090315438263264e-05
batch Size 256
Epoch: [344][0/196]	Time 0.116 (0.116)	Data 0.299 (0.299)	Loss 0.0620 (0.0620)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [344][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0685 (0.0591)	Acc@1 97.266 (98.089)	Acc@5 100.000 (99.976)
Epoch: [344][128/196]	Time 0.078 (0.088)	Data 0.000 (0.003)	Loss 0.0593 (0.0571)	Acc@1 98.047 (98.186)	Acc@5 100.000 (99.982)
Epoch: [344][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0595 (0.0564)	Acc@1 98.047 (98.247)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 3.090315438263264e-05
args.lr: 2.7812838944369376e-05
lr: 2.7812838944369376e-05
Epoche:345/345; Lr: 3.090315438263264e-05
batch Size 256
Epoch: [345][0/196]	Time 0.140 (0.140)	Data 0.280 (0.280)	Loss 0.0512 (0.0512)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [345][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0559 (0.0576)	Acc@1 98.047 (98.113)	Acc@5 100.000 (99.988)
Epoch: [345][128/196]	Time 0.091 (0.086)	Data 0.000 (0.002)	Loss 0.0484 (0.0570)	Acc@1 98.438 (98.192)	Acc@5 100.000 (99.985)
Epoch: [345][192/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.0679 (0.0567)	Acc@1 97.656 (98.235)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.29
Max memory: 51.4381312
 17.283s  j: 351 bis 355
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 647
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 346
Max memory: 0.1097216
lr: 2.7812838944369376e-05
args.lr: 2.7812838944369376e-05
lr: 2.7812838944369376e-05
Epoche:346/350; Lr: 2.7812838944369376e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [346][0/196]	Time 0.157 (0.157)	Data 0.334 (0.334)	Loss 0.0666 (0.0666)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [346][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0824 (0.0554)	Acc@1 97.266 (98.179)	Acc@5 100.000 (99.988)
Epoch: [346][128/196]	Time 0.092 (0.088)	Data 0.000 (0.003)	Loss 0.0486 (0.0568)	Acc@1 99.219 (98.147)	Acc@5 100.000 (99.982)
Epoch: [346][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0481 (0.0560)	Acc@1 99.219 (98.217)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 2.7812838944369376e-05
args.lr: 2.7812838944369376e-05
lr: 2.7812838944369376e-05
Epoche:347/350; Lr: 2.7812838944369376e-05
batch Size 256
Epoch: [347][0/196]	Time 0.132 (0.132)	Data 0.294 (0.294)	Loss 0.0631 (0.0631)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [347][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.0582 (0.0561)	Acc@1 97.656 (98.347)	Acc@5 100.000 (99.988)
Epoch: [347][128/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0514 (0.0574)	Acc@1 98.438 (98.301)	Acc@5 100.000 (99.976)
Epoch: [347][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0596 (0.0569)	Acc@1 98.047 (98.306)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 2.7812838944369376e-05
args.lr: 2.7812838944369376e-05
lr: 2.7812838944369376e-05
Epoche:348/350; Lr: 2.7812838944369376e-05
batch Size 256
Epoch: [348][0/196]	Time 0.126 (0.126)	Data 0.328 (0.328)	Loss 0.0433 (0.0433)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [348][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0402 (0.0556)	Acc@1 98.828 (98.281)	Acc@5 100.000 (99.988)
Epoch: [348][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0462 (0.0564)	Acc@1 98.438 (98.271)	Acc@5 100.000 (99.991)
Epoch: [348][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0386 (0.0562)	Acc@1 99.609 (98.278)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 2.7812838944369376e-05
args.lr: 2.7812838944369376e-05
lr: 2.7812838944369376e-05
Epoche:349/350; Lr: 2.7812838944369376e-05
batch Size 256
Epoch: [349][0/196]	Time 0.134 (0.134)	Data 0.324 (0.324)	Loss 0.0646 (0.0646)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [349][64/196]	Time 0.092 (0.088)	Data 0.000 (0.005)	Loss 0.0540 (0.0567)	Acc@1 97.266 (98.131)	Acc@5 100.000 (99.994)
Epoch: [349][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.0613 (0.0557)	Acc@1 98.828 (98.226)	Acc@5 100.000 (99.994)
Epoch: [349][192/196]	Time 0.081 (0.086)	Data 0.000 (0.002)	Loss 0.0403 (0.0564)	Acc@1 98.828 (98.227)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 2.7812838944369376e-05
args.lr: 2.503155504993244e-05
lr: 2.503155504993244e-05
Epoche:350/350; Lr: 2.7812838944369376e-05
batch Size 256
Epoch: [350][0/196]	Time 0.123 (0.123)	Data 0.280 (0.280)	Loss 0.0573 (0.0573)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [350][64/196]	Time 0.087 (0.086)	Data 0.000 (0.005)	Loss 0.0384 (0.0528)	Acc@1 98.828 (98.462)	Acc@5 100.000 (99.982)
Epoch: [350][128/196]	Time 0.085 (0.086)	Data 0.000 (0.002)	Loss 0.0959 (0.0529)	Acc@1 98.438 (98.456)	Acc@5 99.609 (99.982)
Epoch: [350][192/196]	Time 0.085 (0.086)	Data 0.000 (0.002)	Loss 0.0639 (0.0538)	Acc@1 98.438 (98.401)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.28
Max memory: 51.4381312
 17.262s  j: 356 bis 360
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5302
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 351
Max memory: 0.1097216
lr: 2.503155504993244e-05
args.lr: 2.503155504993244e-05
lr: 2.503155504993244e-05
Epoche:351/355; Lr: 2.503155504993244e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [351][0/196]	Time 0.148 (0.148)	Data 0.295 (0.295)	Loss 0.0622 (0.0622)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [351][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.0258 (0.0595)	Acc@1 100.000 (98.209)	Acc@5 100.000 (99.988)
Epoch: [351][128/196]	Time 0.092 (0.089)	Data 0.000 (0.002)	Loss 0.0626 (0.0590)	Acc@1 98.438 (98.174)	Acc@5 100.000 (99.988)
Epoch: [351][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0566 (0.0589)	Acc@1 98.438 (98.203)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 2.503155504993244e-05
args.lr: 2.503155504993244e-05
lr: 2.503155504993244e-05
Epoche:352/355; Lr: 2.503155504993244e-05
batch Size 256
Epoch: [352][0/196]	Time 0.133 (0.133)	Data 0.294 (0.294)	Loss 0.0644 (0.0644)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [352][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0516 (0.0560)	Acc@1 98.438 (98.359)	Acc@5 99.609 (99.994)
Epoch: [352][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0746 (0.0555)	Acc@1 98.047 (98.322)	Acc@5 100.000 (99.991)
Epoch: [352][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0605 (0.0553)	Acc@1 97.656 (98.330)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 2.503155504993244e-05
args.lr: 2.503155504993244e-05
lr: 2.503155504993244e-05
Epoche:353/355; Lr: 2.503155504993244e-05
batch Size 256
Epoch: [353][0/196]	Time 0.127 (0.127)	Data 0.269 (0.269)	Loss 0.0711 (0.0711)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [353][64/196]	Time 0.088 (0.089)	Data 0.000 (0.004)	Loss 0.0539 (0.0558)	Acc@1 98.438 (98.269)	Acc@5 99.609 (99.970)
Epoch: [353][128/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0474 (0.0559)	Acc@1 97.656 (98.189)	Acc@5 100.000 (99.982)
Epoch: [353][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0405 (0.0562)	Acc@1 98.438 (98.207)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 2.503155504993244e-05
args.lr: 2.503155504993244e-05
lr: 2.503155504993244e-05
Epoche:354/355; Lr: 2.503155504993244e-05
batch Size 256
Epoch: [354][0/196]	Time 0.128 (0.128)	Data 0.325 (0.325)	Loss 0.0563 (0.0563)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [354][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.0638 (0.0545)	Acc@1 97.656 (98.287)	Acc@5 100.000 (99.976)
Epoch: [354][128/196]	Time 0.081 (0.089)	Data 0.000 (0.003)	Loss 0.0482 (0.0550)	Acc@1 98.047 (98.295)	Acc@5 100.000 (99.988)
Epoch: [354][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0465 (0.0558)	Acc@1 98.438 (98.284)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 2.503155504993244e-05
args.lr: 2.2528399544939195e-05
lr: 2.2528399544939195e-05
Epoche:355/355; Lr: 2.503155504993244e-05
batch Size 256
Epoch: [355][0/196]	Time 0.115 (0.115)	Data 0.319 (0.319)	Loss 0.0459 (0.0459)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [355][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0527 (0.0554)	Acc@1 98.828 (98.251)	Acc@5 100.000 (99.982)
Epoch: [355][128/196]	Time 0.083 (0.089)	Data 0.000 (0.003)	Loss 0.0455 (0.0566)	Acc@1 98.047 (98.295)	Acc@5 100.000 (99.988)
Epoch: [355][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0538 (0.0562)	Acc@1 98.047 (98.284)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.28
Max memory: 51.4381312
 17.716s  Net2Net 2
j: 1 bis 5
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3153
Files already downloaded and verified
width: 8

Arch Num:  [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
Max memory: 0.0573952
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:1/5; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [1][0/196]	Time 0.155 (0.155)	Data 0.294 (0.294)	Loss 2.5146 (2.5146)	Acc@1 8.203 (8.203)	Acc@5 48.438 (48.438)
Epoch: [1][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 1.8527 (2.0363)	Acc@1 32.031 (22.656)	Acc@5 83.203 (75.781)
Epoch: [1][128/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 1.6346 (1.8857)	Acc@1 37.500 (28.110)	Acc@5 88.281 (81.604)
Epoch: [1][192/196]	Time 0.096 (0.087)	Data 0.000 (0.002)	Loss 1.4811 (1.7861)	Acc@1 45.703 (31.979)	Acc@5 92.969 (84.458)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:2/5; Lr: 0.1
batch Size 256
Epoch: [2][0/196]	Time 0.112 (0.112)	Data 0.311 (0.311)	Loss 1.6184 (1.6184)	Acc@1 42.578 (42.578)	Acc@5 88.672 (88.672)
Epoch: [2][64/196]	Time 0.096 (0.087)	Data 0.000 (0.005)	Loss 1.4310 (1.4551)	Acc@1 48.438 (46.238)	Acc@5 94.141 (92.145)
Epoch: [2][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 1.2797 (1.3997)	Acc@1 54.688 (48.734)	Acc@5 92.578 (92.796)
Epoch: [2][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 1.2510 (1.3404)	Acc@1 55.859 (51.000)	Acc@5 92.578 (93.533)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:3/5; Lr: 0.1
batch Size 256
Epoch: [3][0/196]	Time 0.127 (0.127)	Data 0.272 (0.272)	Loss 1.1435 (1.1435)	Acc@1 63.672 (63.672)	Acc@5 93.750 (93.750)
Epoch: [3][64/196]	Time 0.085 (0.087)	Data 0.000 (0.004)	Loss 1.1998 (1.1337)	Acc@1 55.469 (59.044)	Acc@5 96.094 (95.595)
Epoch: [3][128/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 1.0224 (1.0979)	Acc@1 64.844 (60.544)	Acc@5 96.875 (96.006)
Epoch: [3][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 1.0364 (1.0634)	Acc@1 61.719 (61.707)	Acc@5 96.875 (96.284)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:4/5; Lr: 0.1
batch Size 256
Epoch: [4][0/196]	Time 0.125 (0.125)	Data 0.270 (0.270)	Loss 1.0780 (1.0780)	Acc@1 61.328 (61.328)	Acc@5 96.484 (96.484)
Epoch: [4][64/196]	Time 0.078 (0.088)	Data 0.000 (0.004)	Loss 0.8728 (0.9633)	Acc@1 67.969 (65.216)	Acc@5 98.828 (97.085)
Epoch: [4][128/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.9081 (0.9369)	Acc@1 67.188 (66.073)	Acc@5 96.484 (97.229)
Epoch: [4][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 1.0940 (0.9210)	Acc@1 61.719 (66.904)	Acc@5 94.922 (97.280)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:5/5; Lr: 0.1
batch Size 256
Epoch: [5][0/196]	Time 0.107 (0.107)	Data 0.306 (0.306)	Loss 0.8472 (0.8472)	Acc@1 71.484 (71.484)	Acc@5 97.266 (97.266)
Epoch: [5][64/196]	Time 0.081 (0.087)	Data 0.000 (0.005)	Loss 0.8367 (0.8515)	Acc@1 74.219 (70.054)	Acc@5 96.875 (97.692)
Epoch: [5][128/196]	Time 0.094 (0.086)	Data 0.000 (0.003)	Loss 0.8697 (0.8347)	Acc@1 73.828 (70.585)	Acc@5 96.094 (97.865)
Epoch: [5][192/196]	Time 0.087 (0.086)	Data 0.000 (0.002)	Loss 0.7666 (0.8216)	Acc@1 69.922 (71.071)	Acc@5 97.656 (97.905)
Max memory in training epoch: 33.3018624
[INFO] Storing checkpoint...
  62.08
Max memory: 51.3858048
 17.287s  j: 6 bis 10
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8364
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 6
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:6/10; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [6][0/196]	Time 0.150 (0.150)	Data 0.291 (0.291)	Loss 0.8255 (0.8255)	Acc@1 69.531 (69.531)	Acc@5 98.047 (98.047)
Epoch: [6][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.8778 (0.7564)	Acc@1 71.094 (73.468)	Acc@5 97.266 (98.185)
Epoch: [6][128/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.7848 (0.7607)	Acc@1 71.875 (73.277)	Acc@5 98.828 (98.159)
Epoch: [6][192/196]	Time 0.094 (0.087)	Data 0.000 (0.002)	Loss 0.8071 (0.7566)	Acc@1 71.875 (73.468)	Acc@5 97.656 (98.142)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:7/10; Lr: 0.1
batch Size 256
Epoch: [7][0/196]	Time 0.131 (0.131)	Data 0.307 (0.307)	Loss 0.6379 (0.6379)	Acc@1 76.953 (76.953)	Acc@5 97.656 (97.656)
Epoch: [7][64/196]	Time 0.095 (0.086)	Data 0.000 (0.005)	Loss 0.7471 (0.7152)	Acc@1 72.656 (75.361)	Acc@5 98.438 (98.389)
Epoch: [7][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.7141 (0.7047)	Acc@1 76.953 (75.572)	Acc@5 98.047 (98.389)
Epoch: [7][192/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.6559 (0.7086)	Acc@1 77.734 (75.295)	Acc@5 98.438 (98.361)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:8/10; Lr: 0.1
batch Size 256
Epoch: [8][0/196]	Time 0.123 (0.123)	Data 0.313 (0.313)	Loss 0.6835 (0.6835)	Acc@1 76.172 (76.172)	Acc@5 98.828 (98.828)
Epoch: [8][64/196]	Time 0.092 (0.084)	Data 0.000 (0.005)	Loss 0.7744 (0.6740)	Acc@1 73.828 (76.532)	Acc@5 97.266 (98.516)
Epoch: [8][128/196]	Time 0.080 (0.085)	Data 0.000 (0.003)	Loss 0.6411 (0.6783)	Acc@1 77.734 (76.502)	Acc@5 99.219 (98.537)
Epoch: [8][192/196]	Time 0.087 (0.086)	Data 0.000 (0.002)	Loss 0.4959 (0.6715)	Acc@1 84.766 (76.708)	Acc@5 98.438 (98.559)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:9/10; Lr: 0.1
batch Size 256
Epoch: [9][0/196]	Time 0.126 (0.126)	Data 0.308 (0.308)	Loss 0.6638 (0.6638)	Acc@1 77.344 (77.344)	Acc@5 99.609 (99.609)
Epoch: [9][64/196]	Time 0.096 (0.088)	Data 0.000 (0.005)	Loss 0.6763 (0.6465)	Acc@1 75.000 (77.476)	Acc@5 98.438 (98.690)
Epoch: [9][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.5684 (0.6411)	Acc@1 79.688 (77.689)	Acc@5 99.219 (98.625)
Epoch: [9][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.6099 (0.6439)	Acc@1 79.297 (77.574)	Acc@5 99.219 (98.605)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:10/10; Lr: 0.1
batch Size 256
Epoch: [10][0/196]	Time 0.120 (0.120)	Data 0.333 (0.333)	Loss 0.6421 (0.6421)	Acc@1 78.125 (78.125)	Acc@5 98.828 (98.828)
Epoch: [10][64/196]	Time 0.085 (0.087)	Data 0.000 (0.005)	Loss 0.5458 (0.6329)	Acc@1 82.031 (78.377)	Acc@5 98.828 (98.666)
Epoch: [10][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.6210 (0.6253)	Acc@1 81.641 (78.558)	Acc@5 99.219 (98.737)
Epoch: [10][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.6975 (0.6192)	Acc@1 76.953 (78.678)	Acc@5 98.438 (98.743)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  66.16
Max memory: 51.4381312
 17.372s  j: 11 bis 15
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 707
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 11
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:11/15; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [11][0/196]	Time 0.159 (0.159)	Data 0.316 (0.316)	Loss 0.6099 (0.6099)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [11][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.5937 (0.5848)	Acc@1 78.906 (79.802)	Acc@5 98.438 (98.936)
Epoch: [11][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.5832 (0.5976)	Acc@1 80.859 (79.391)	Acc@5 99.219 (98.883)
Epoch: [11][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.6354 (0.6017)	Acc@1 79.297 (79.293)	Acc@5 98.438 (98.844)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:12/15; Lr: 0.1
batch Size 256
Epoch: [12][0/196]	Time 0.125 (0.125)	Data 0.285 (0.285)	Loss 0.5546 (0.5546)	Acc@1 78.906 (78.906)	Acc@5 98.828 (98.828)
Epoch: [12][64/196]	Time 0.088 (0.087)	Data 0.000 (0.005)	Loss 0.5044 (0.5923)	Acc@1 84.375 (79.862)	Acc@5 100.000 (98.906)
Epoch: [12][128/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.6868 (0.5925)	Acc@1 75.000 (79.651)	Acc@5 99.609 (98.892)
Epoch: [12][192/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.6136 (0.5907)	Acc@1 80.078 (79.762)	Acc@5 98.438 (98.850)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:13/15; Lr: 0.1
batch Size 256
Epoch: [13][0/196]	Time 0.123 (0.123)	Data 0.289 (0.289)	Loss 0.5710 (0.5710)	Acc@1 81.250 (81.250)	Acc@5 100.000 (100.000)
Epoch: [13][64/196]	Time 0.097 (0.089)	Data 0.000 (0.005)	Loss 0.5731 (0.5828)	Acc@1 80.078 (80.006)	Acc@5 98.828 (98.882)
Epoch: [13][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.6293 (0.5789)	Acc@1 79.688 (79.939)	Acc@5 98.438 (98.916)
Epoch: [13][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.6587 (0.5752)	Acc@1 78.906 (80.117)	Acc@5 99.219 (98.913)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:14/15; Lr: 0.1
batch Size 256
Epoch: [14][0/196]	Time 0.124 (0.124)	Data 0.302 (0.302)	Loss 0.6682 (0.6682)	Acc@1 75.000 (75.000)	Acc@5 99.219 (99.219)
Epoch: [14][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.6246 (0.5716)	Acc@1 78.516 (80.204)	Acc@5 98.438 (98.924)
Epoch: [14][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.5325 (0.5668)	Acc@1 82.422 (80.478)	Acc@5 99.219 (98.977)
Epoch: [14][192/196]	Time 0.094 (0.087)	Data 0.000 (0.002)	Loss 0.5148 (0.5635)	Acc@1 83.984 (80.598)	Acc@5 99.219 (98.958)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:15/15; Lr: 0.1
batch Size 256
Epoch: [15][0/196]	Time 0.134 (0.134)	Data 0.289 (0.289)	Loss 0.4824 (0.4824)	Acc@1 82.031 (82.031)	Acc@5 98.047 (98.047)
Epoch: [15][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.5136 (0.5448)	Acc@1 83.984 (81.274)	Acc@5 98.438 (98.942)
Epoch: [15][128/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.5427 (0.5460)	Acc@1 80.859 (81.226)	Acc@5 99.219 (98.992)
Epoch: [15][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.7128 (0.5475)	Acc@1 73.047 (81.189)	Acc@5 98.438 (99.012)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  71.71
Max memory: 51.4381312
 17.535s  j: 16 bis 20
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9253
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 16
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:16/20; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [16][0/196]	Time 0.133 (0.133)	Data 0.325 (0.325)	Loss 0.4515 (0.4515)	Acc@1 82.812 (82.812)	Acc@5 99.609 (99.609)
Epoch: [16][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.6243 (0.5164)	Acc@1 78.125 (81.851)	Acc@5 97.656 (99.093)
Epoch: [16][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.5009 (0.5344)	Acc@1 79.297 (81.308)	Acc@5 99.219 (99.016)
Epoch: [16][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.5159 (0.5381)	Acc@1 80.859 (81.248)	Acc@5 99.609 (99.031)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:17/20; Lr: 0.1
batch Size 256
Epoch: [17][0/196]	Time 0.141 (0.141)	Data 0.287 (0.287)	Loss 0.5072 (0.5072)	Acc@1 84.766 (84.766)	Acc@5 98.047 (98.047)
Epoch: [17][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.4677 (0.5512)	Acc@1 83.984 (80.745)	Acc@5 99.219 (98.990)
Epoch: [17][128/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.4439 (0.5407)	Acc@1 85.156 (81.389)	Acc@5 99.609 (99.001)
Epoch: [17][192/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.4490 (0.5388)	Acc@1 84.766 (81.483)	Acc@5 98.438 (99.008)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:18/20; Lr: 0.1
batch Size 256
Epoch: [18][0/196]	Time 0.134 (0.134)	Data 0.314 (0.314)	Loss 0.4417 (0.4417)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [18][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.5500 (0.5392)	Acc@1 81.641 (81.508)	Acc@5 99.609 (99.014)
Epoch: [18][128/196]	Time 0.086 (0.090)	Data 0.000 (0.003)	Loss 0.5003 (0.5341)	Acc@1 83.594 (81.653)	Acc@5 99.219 (99.055)
Epoch: [18][192/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.5762 (0.5324)	Acc@1 80.469 (81.635)	Acc@5 100.000 (99.085)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:19/20; Lr: 0.1
batch Size 256
Epoch: [19][0/196]	Time 0.125 (0.125)	Data 0.306 (0.306)	Loss 0.5620 (0.5620)	Acc@1 80.078 (80.078)	Acc@5 98.828 (98.828)
Epoch: [19][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.5525 (0.5199)	Acc@1 79.688 (81.983)	Acc@5 99.219 (99.237)
Epoch: [19][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.5679 (0.5246)	Acc@1 82.422 (82.040)	Acc@5 97.656 (99.113)
Epoch: [19][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.5793 (0.5233)	Acc@1 80.859 (81.971)	Acc@5 98.438 (99.109)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:20/20; Lr: 0.1
batch Size 256
Epoch: [20][0/196]	Time 0.135 (0.135)	Data 0.296 (0.296)	Loss 0.5221 (0.5221)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [20][64/196]	Time 0.096 (0.089)	Data 0.000 (0.005)	Loss 0.4829 (0.5134)	Acc@1 86.328 (82.476)	Acc@5 99.219 (99.159)
Epoch: [20][128/196]	Time 0.094 (0.088)	Data 0.000 (0.003)	Loss 0.4975 (0.5148)	Acc@1 83.594 (82.389)	Acc@5 99.609 (99.122)
Epoch: [20][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.4734 (0.5140)	Acc@1 84.375 (82.363)	Acc@5 98.828 (99.093)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  74.4
Max memory: 51.4381312
 17.626s  j: 21 bis 25
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5246
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 21
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:21/25; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [21][0/196]	Time 0.163 (0.163)	Data 0.288 (0.288)	Loss 0.4250 (0.4250)	Acc@1 83.984 (83.984)	Acc@5 100.000 (100.000)
Epoch: [21][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.4523 (0.4985)	Acc@1 83.203 (82.825)	Acc@5 99.609 (99.141)
Epoch: [21][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.4405 (0.5024)	Acc@1 82.031 (82.688)	Acc@5 100.000 (99.131)
Epoch: [21][192/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.4730 (0.5075)	Acc@1 83.984 (82.541)	Acc@5 98.438 (99.152)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:22/25; Lr: 0.1
batch Size 256
Epoch: [22][0/196]	Time 0.136 (0.136)	Data 0.306 (0.306)	Loss 0.5086 (0.5086)	Acc@1 83.203 (83.203)	Acc@5 97.266 (97.266)
Epoch: [22][64/196]	Time 0.080 (0.086)	Data 0.000 (0.005)	Loss 0.5068 (0.5083)	Acc@1 82.031 (82.278)	Acc@5 99.219 (99.062)
Epoch: [22][128/196]	Time 0.102 (0.086)	Data 0.000 (0.003)	Loss 0.4511 (0.5050)	Acc@1 86.328 (82.392)	Acc@5 98.438 (99.128)
Epoch: [22][192/196]	Time 0.087 (0.086)	Data 0.000 (0.002)	Loss 0.5905 (0.5092)	Acc@1 80.078 (82.375)	Acc@5 98.438 (99.146)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:23/25; Lr: 0.1
batch Size 256
Epoch: [23][0/196]	Time 0.130 (0.130)	Data 0.313 (0.313)	Loss 0.4998 (0.4998)	Acc@1 83.984 (83.984)	Acc@5 98.047 (98.047)
Epoch: [23][64/196]	Time 0.095 (0.090)	Data 0.000 (0.005)	Loss 0.5565 (0.4958)	Acc@1 79.688 (82.873)	Acc@5 98.438 (99.243)
Epoch: [23][128/196]	Time 0.089 (0.089)	Data 0.000 (0.003)	Loss 0.4551 (0.5024)	Acc@1 82.812 (82.707)	Acc@5 100.000 (99.149)
Epoch: [23][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.5175 (0.5023)	Acc@1 83.984 (82.614)	Acc@5 99.219 (99.168)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:24/25; Lr: 0.1
batch Size 256
Epoch: [24][0/196]	Time 0.130 (0.130)	Data 0.312 (0.312)	Loss 0.4738 (0.4738)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [24][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.4150 (0.4862)	Acc@1 86.328 (83.552)	Acc@5 99.219 (99.333)
Epoch: [24][128/196]	Time 0.094 (0.087)	Data 0.000 (0.003)	Loss 0.4709 (0.4944)	Acc@1 85.938 (83.224)	Acc@5 98.828 (99.237)
Epoch: [24][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.4554 (0.4976)	Acc@1 83.594 (83.074)	Acc@5 99.609 (99.156)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:25/25; Lr: 0.1
batch Size 256
Epoch: [25][0/196]	Time 0.117 (0.117)	Data 0.304 (0.304)	Loss 0.4293 (0.4293)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [25][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.5896 (0.4914)	Acc@1 80.469 (82.903)	Acc@5 98.828 (99.297)
Epoch: [25][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.4638 (0.4872)	Acc@1 83.984 (82.985)	Acc@5 99.219 (99.264)
Epoch: [25][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.5256 (0.4925)	Acc@1 82.422 (82.920)	Acc@5 99.219 (99.186)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  79.36
Max memory: 51.4381312
 17.493s  j: 26 bis 30
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2889
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 26
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:26/30; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [26][0/196]	Time 0.164 (0.164)	Data 0.289 (0.289)	Loss 0.5545 (0.5545)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [26][64/196]	Time 0.083 (0.087)	Data 0.000 (0.005)	Loss 0.5761 (0.4810)	Acc@1 80.078 (83.510)	Acc@5 100.000 (99.303)
Epoch: [26][128/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.3900 (0.4791)	Acc@1 86.328 (83.688)	Acc@5 99.609 (99.231)
Epoch: [26][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.4275 (0.4808)	Acc@1 83.984 (83.582)	Acc@5 99.219 (99.205)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:27/30; Lr: 0.1
batch Size 256
Epoch: [27][0/196]	Time 0.101 (0.101)	Data 0.304 (0.304)	Loss 0.5020 (0.5020)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [27][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.5767 (0.4902)	Acc@1 82.812 (82.999)	Acc@5 98.047 (99.038)
Epoch: [27][128/196]	Time 0.097 (0.088)	Data 0.000 (0.003)	Loss 0.5563 (0.4837)	Acc@1 81.250 (83.512)	Acc@5 99.609 (99.164)
Epoch: [27][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.4783 (0.4875)	Acc@1 82.031 (83.318)	Acc@5 98.438 (99.152)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:28/30; Lr: 0.1
batch Size 256
Epoch: [28][0/196]	Time 0.123 (0.123)	Data 0.322 (0.322)	Loss 0.4651 (0.4651)	Acc@1 83.203 (83.203)	Acc@5 100.000 (100.000)
Epoch: [28][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.5152 (0.4795)	Acc@1 81.250 (83.371)	Acc@5 99.219 (99.219)
Epoch: [28][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.4542 (0.4757)	Acc@1 83.984 (83.533)	Acc@5 98.438 (99.240)
Epoch: [28][192/196]	Time 0.097 (0.087)	Data 0.000 (0.002)	Loss 0.4791 (0.4755)	Acc@1 83.203 (83.650)	Acc@5 99.219 (99.235)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:29/30; Lr: 0.1
batch Size 256
Epoch: [29][0/196]	Time 0.127 (0.127)	Data 0.291 (0.291)	Loss 0.4963 (0.4963)	Acc@1 80.078 (80.078)	Acc@5 99.219 (99.219)
Epoch: [29][64/196]	Time 0.080 (0.088)	Data 0.000 (0.005)	Loss 0.5270 (0.4650)	Acc@1 83.203 (83.906)	Acc@5 98.828 (99.183)
Epoch: [29][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.3709 (0.4713)	Acc@1 87.500 (83.948)	Acc@5 100.000 (99.188)
Epoch: [29][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.5281 (0.4764)	Acc@1 82.812 (83.766)	Acc@5 98.828 (99.188)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:30/30; Lr: 0.1
batch Size 256
Epoch: [30][0/196]	Time 0.135 (0.135)	Data 0.280 (0.280)	Loss 0.5121 (0.5121)	Acc@1 80.859 (80.859)	Acc@5 99.219 (99.219)
Epoch: [30][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.4771 (0.4750)	Acc@1 85.156 (83.317)	Acc@5 99.219 (99.351)
Epoch: [30][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.3828 (0.4733)	Acc@1 87.891 (83.542)	Acc@5 99.609 (99.279)
Epoch: [30][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.5105 (0.4741)	Acc@1 83.984 (83.638)	Acc@5 98.828 (99.247)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  74.28
Max memory: 51.4381312
 17.552s  j: 31 bis 35
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9470
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 31
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:31/35; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [31][0/196]	Time 0.133 (0.133)	Data 0.312 (0.312)	Loss 0.4912 (0.4912)	Acc@1 83.203 (83.203)	Acc@5 98.828 (98.828)
Epoch: [31][64/196]	Time 0.082 (0.087)	Data 0.000 (0.005)	Loss 0.4592 (0.4501)	Acc@1 80.859 (84.633)	Acc@5 100.000 (99.381)
Epoch: [31][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.4626 (0.4600)	Acc@1 85.156 (84.287)	Acc@5 99.609 (99.304)
Epoch: [31][192/196]	Time 0.086 (0.086)	Data 0.000 (0.002)	Loss 0.4934 (0.4653)	Acc@1 79.688 (84.041)	Acc@5 100.000 (99.257)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:32/35; Lr: 0.1
batch Size 256
Epoch: [32][0/196]	Time 0.130 (0.130)	Data 0.320 (0.320)	Loss 0.4575 (0.4575)	Acc@1 83.203 (83.203)	Acc@5 100.000 (100.000)
Epoch: [32][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.4083 (0.4602)	Acc@1 86.719 (83.990)	Acc@5 100.000 (99.297)
Epoch: [32][128/196]	Time 0.092 (0.088)	Data 0.000 (0.003)	Loss 0.4394 (0.4706)	Acc@1 85.938 (83.836)	Acc@5 99.609 (99.249)
Epoch: [32][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.4451 (0.4738)	Acc@1 86.719 (83.748)	Acc@5 100.000 (99.267)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:33/35; Lr: 0.1
batch Size 256
Epoch: [33][0/196]	Time 0.156 (0.156)	Data 0.287 (0.287)	Loss 0.4057 (0.4057)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [33][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.4499 (0.4480)	Acc@1 82.812 (84.675)	Acc@5 100.000 (99.387)
Epoch: [33][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.4776 (0.4541)	Acc@1 84.375 (84.472)	Acc@5 99.609 (99.352)
Epoch: [33][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.4412 (0.4578)	Acc@1 84.375 (84.318)	Acc@5 99.609 (99.298)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:34/35; Lr: 0.1
batch Size 256
Epoch: [34][0/196]	Time 0.125 (0.125)	Data 0.322 (0.322)	Loss 0.5149 (0.5149)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [34][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.5557 (0.4685)	Acc@1 79.688 (83.588)	Acc@5 99.609 (99.477)
Epoch: [34][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.4925 (0.4658)	Acc@1 83.984 (83.803)	Acc@5 99.219 (99.397)
Epoch: [34][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.5453 (0.4658)	Acc@1 81.641 (83.843)	Acc@5 99.219 (99.330)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:35/35; Lr: 0.1
batch Size 256
Epoch: [35][0/196]	Time 0.122 (0.122)	Data 0.301 (0.301)	Loss 0.3936 (0.3936)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [35][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.5032 (0.4348)	Acc@1 83.203 (85.006)	Acc@5 99.219 (99.411)
Epoch: [35][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.4860 (0.4492)	Acc@1 84.375 (84.402)	Acc@5 99.219 (99.364)
Epoch: [35][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.4783 (0.4610)	Acc@1 83.594 (84.021)	Acc@5 98.828 (99.342)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  73.99
Max memory: 51.4381312
 17.532s  j: 36 bis 40
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5189
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 36
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:36/40; Lr: 0.1
batch Size 256
Epoch: [36][0/196]	Time 0.143 (0.143)	Data 0.286 (0.286)	Loss 0.3222 (0.3222)	Acc@1 89.062 (89.062)	Acc@5 99.609 (99.609)
Epoch: [36][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 0.4173 (0.4319)	Acc@1 87.109 (85.282)	Acc@5 98.438 (99.345)
Epoch: [36][128/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.5438 (0.4491)	Acc@1 79.297 (84.532)	Acc@5 99.219 (99.313)
Epoch: [36][192/196]	Time 0.076 (0.086)	Data 0.000 (0.002)	Loss 0.4728 (0.4581)	Acc@1 79.688 (84.237)	Acc@5 98.828 (99.281)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:37/40; Lr: 0.1
batch Size 256
Epoch: [37][0/196]	Time 0.110 (0.110)	Data 0.334 (0.334)	Loss 0.4007 (0.4007)	Acc@1 86.328 (86.328)	Acc@5 99.609 (99.609)
Epoch: [37][64/196]	Time 0.087 (0.087)	Data 0.000 (0.005)	Loss 0.4415 (0.4441)	Acc@1 83.594 (84.639)	Acc@5 98.828 (99.429)
Epoch: [37][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.4849 (0.4608)	Acc@1 83.203 (83.987)	Acc@5 98.438 (99.307)
Epoch: [37][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.4249 (0.4595)	Acc@1 86.328 (84.096)	Acc@5 99.219 (99.296)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:38/40; Lr: 0.1
batch Size 256
Epoch: [38][0/196]	Time 0.130 (0.130)	Data 0.293 (0.293)	Loss 0.4040 (0.4040)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [38][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.3799 (0.4473)	Acc@1 87.500 (84.525)	Acc@5 99.219 (99.441)
Epoch: [38][128/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.5031 (0.4483)	Acc@1 86.328 (84.566)	Acc@5 98.047 (99.391)
Epoch: [38][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.4745 (0.4504)	Acc@1 86.328 (84.610)	Acc@5 98.828 (99.364)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:39/40; Lr: 0.1
batch Size 256
Epoch: [39][0/196]	Time 0.116 (0.116)	Data 0.309 (0.309)	Loss 0.5651 (0.5651)	Acc@1 80.859 (80.859)	Acc@5 98.828 (98.828)
Epoch: [39][64/196]	Time 0.080 (0.088)	Data 0.000 (0.005)	Loss 0.4037 (0.4565)	Acc@1 86.719 (84.105)	Acc@5 99.609 (99.351)
Epoch: [39][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.4468 (0.4550)	Acc@1 84.375 (84.348)	Acc@5 98.828 (99.288)
Epoch: [39][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.3810 (0.4562)	Acc@1 84.375 (84.254)	Acc@5 99.609 (99.296)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:40/40; Lr: 0.1
batch Size 256
Epoch: [40][0/196]	Time 0.128 (0.128)	Data 0.278 (0.278)	Loss 0.3996 (0.3996)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [40][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.5786 (0.4405)	Acc@1 80.078 (84.982)	Acc@5 99.219 (99.321)
Epoch: [40][128/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.4339 (0.4525)	Acc@1 87.500 (84.348)	Acc@5 99.609 (99.304)
Epoch: [40][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.3948 (0.4493)	Acc@1 84.766 (84.486)	Acc@5 100.000 (99.316)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  77.41
Max memory: 51.4381312
 17.470s  j: 41 bis 45
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3610
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 41
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:41/45; Lr: 0.1
batch Size 256
Epoch: [41][0/196]	Time 0.150 (0.150)	Data 0.259 (0.259)	Loss 0.4624 (0.4624)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [41][64/196]	Time 0.106 (0.089)	Data 0.000 (0.004)	Loss 0.3737 (0.4198)	Acc@1 85.938 (85.613)	Acc@5 100.000 (99.459)
Epoch: [41][128/196]	Time 0.093 (0.089)	Data 0.000 (0.002)	Loss 0.3856 (0.4394)	Acc@1 87.891 (84.990)	Acc@5 100.000 (99.340)
Epoch: [41][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.5111 (0.4460)	Acc@1 82.031 (84.715)	Acc@5 99.219 (99.340)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:42/45; Lr: 0.1
batch Size 256
Epoch: [42][0/196]	Time 0.118 (0.118)	Data 0.292 (0.292)	Loss 0.4190 (0.4190)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [42][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.4262 (0.4482)	Acc@1 88.281 (84.393)	Acc@5 99.219 (99.327)
Epoch: [42][128/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.5733 (0.4414)	Acc@1 81.641 (84.708)	Acc@5 99.609 (99.322)
Epoch: [42][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.4774 (0.4479)	Acc@1 81.641 (84.539)	Acc@5 100.000 (99.320)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:43/45; Lr: 0.1
batch Size 256
Epoch: [43][0/196]	Time 0.131 (0.131)	Data 0.342 (0.342)	Loss 0.5012 (0.5012)	Acc@1 80.859 (80.859)	Acc@5 99.609 (99.609)
Epoch: [43][64/196]	Time 0.088 (0.090)	Data 0.000 (0.005)	Loss 0.4012 (0.4256)	Acc@1 85.938 (85.475)	Acc@5 99.609 (99.483)
Epoch: [43][128/196]	Time 0.088 (0.089)	Data 0.000 (0.003)	Loss 0.4290 (0.4360)	Acc@1 85.547 (84.914)	Acc@5 98.438 (99.410)
Epoch: [43][192/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.5298 (0.4421)	Acc@1 83.203 (84.713)	Acc@5 98.438 (99.393)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:44/45; Lr: 0.1
batch Size 256
Epoch: [44][0/196]	Time 0.123 (0.123)	Data 0.309 (0.309)	Loss 0.3963 (0.3963)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [44][64/196]	Time 0.088 (0.090)	Data 0.000 (0.005)	Loss 0.3840 (0.4407)	Acc@1 87.500 (84.958)	Acc@5 98.438 (99.405)
Epoch: [44][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.4130 (0.4467)	Acc@1 86.328 (84.793)	Acc@5 98.828 (99.349)
Epoch: [44][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.5713 (0.4454)	Acc@1 80.078 (84.877)	Acc@5 99.609 (99.324)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:45/45; Lr: 0.1
batch Size 256
Epoch: [45][0/196]	Time 0.116 (0.116)	Data 0.310 (0.310)	Loss 0.4401 (0.4401)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [45][64/196]	Time 0.081 (0.090)	Data 0.000 (0.005)	Loss 0.3036 (0.4496)	Acc@1 89.062 (84.639)	Acc@5 100.000 (99.273)
Epoch: [45][128/196]	Time 0.083 (0.090)	Data 0.000 (0.003)	Loss 0.4267 (0.4461)	Acc@1 84.375 (84.793)	Acc@5 99.219 (99.304)
Epoch: [45][192/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.5041 (0.4463)	Acc@1 82.422 (84.658)	Acc@5 99.219 (99.320)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  70.83
Max memory: 51.4381312
 17.864s  j: 46 bis 50
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4615
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 46
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:46/50; Lr: 0.1
batch Size 256
Epoch: [46][0/196]	Time 0.134 (0.134)	Data 0.325 (0.325)	Loss 0.4493 (0.4493)	Acc@1 84.375 (84.375)	Acc@5 97.656 (97.656)
Epoch: [46][64/196]	Time 0.093 (0.090)	Data 0.000 (0.005)	Loss 0.4686 (0.4215)	Acc@1 85.547 (85.294)	Acc@5 98.438 (99.357)
Epoch: [46][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.5230 (0.4321)	Acc@1 80.859 (85.177)	Acc@5 98.828 (99.373)
Epoch: [46][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.3699 (0.4364)	Acc@1 85.938 (85.039)	Acc@5 100.000 (99.332)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:47/50; Lr: 0.1
batch Size 256
Epoch: [47][0/196]	Time 0.136 (0.136)	Data 0.307 (0.307)	Loss 0.4956 (0.4956)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [47][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.3777 (0.4322)	Acc@1 85.938 (85.000)	Acc@5 98.828 (99.405)
Epoch: [47][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.4297 (0.4424)	Acc@1 84.766 (84.754)	Acc@5 99.219 (99.319)
Epoch: [47][192/196]	Time 0.096 (0.088)	Data 0.000 (0.002)	Loss 0.4997 (0.4418)	Acc@1 85.547 (84.747)	Acc@5 99.219 (99.308)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:48/50; Lr: 0.1
batch Size 256
Epoch: [48][0/196]	Time 0.144 (0.144)	Data 0.303 (0.303)	Loss 0.4051 (0.4051)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [48][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.3247 (0.4350)	Acc@1 91.406 (85.349)	Acc@5 99.609 (99.327)
Epoch: [48][128/196]	Time 0.092 (0.089)	Data 0.000 (0.003)	Loss 0.5235 (0.4446)	Acc@1 80.859 (84.666)	Acc@5 98.828 (99.334)
Epoch: [48][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.4110 (0.4442)	Acc@1 85.938 (84.654)	Acc@5 100.000 (99.330)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:49/50; Lr: 0.1
batch Size 256
Epoch: [49][0/196]	Time 0.104 (0.104)	Data 0.291 (0.291)	Loss 0.3863 (0.3863)	Acc@1 87.891 (87.891)	Acc@5 99.219 (99.219)
Epoch: [49][64/196]	Time 0.095 (0.089)	Data 0.000 (0.005)	Loss 0.4366 (0.4386)	Acc@1 83.594 (84.874)	Acc@5 99.609 (99.291)
Epoch: [49][128/196]	Time 0.097 (0.089)	Data 0.000 (0.002)	Loss 0.4718 (0.4362)	Acc@1 83.594 (85.035)	Acc@5 99.609 (99.340)
Epoch: [49][192/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.4776 (0.4440)	Acc@1 81.250 (84.733)	Acc@5 99.609 (99.344)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:50/50; Lr: 0.1
batch Size 256
Epoch: [50][0/196]	Time 0.139 (0.139)	Data 0.313 (0.313)	Loss 0.4509 (0.4509)	Acc@1 82.422 (82.422)	Acc@5 100.000 (100.000)
Epoch: [50][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.4569 (0.4214)	Acc@1 84.766 (85.631)	Acc@5 98.438 (99.465)
Epoch: [50][128/196]	Time 0.103 (0.089)	Data 0.000 (0.003)	Loss 0.4393 (0.4287)	Acc@1 85.156 (85.395)	Acc@5 98.828 (99.406)
Epoch: [50][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.4182 (0.4302)	Acc@1 82.031 (85.355)	Acc@5 99.609 (99.366)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  78.34
Max memory: 51.4381312
 17.803s  j: 51 bis 55
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7598
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 51
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:51/55; Lr: 0.1
batch Size 256
Epoch: [51][0/196]	Time 0.161 (0.161)	Data 0.279 (0.279)	Loss 0.4276 (0.4276)	Acc@1 88.672 (88.672)	Acc@5 99.219 (99.219)
Epoch: [51][64/196]	Time 0.081 (0.089)	Data 0.000 (0.005)	Loss 0.4313 (0.3955)	Acc@1 85.156 (86.412)	Acc@5 100.000 (99.495)
Epoch: [51][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.4431 (0.4214)	Acc@1 84.766 (85.508)	Acc@5 99.219 (99.437)
Epoch: [51][192/196]	Time 0.096 (0.088)	Data 0.000 (0.002)	Loss 0.5048 (0.4287)	Acc@1 83.203 (85.221)	Acc@5 99.609 (99.399)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:52/55; Lr: 0.1
batch Size 256
Epoch: [52][0/196]	Time 0.138 (0.138)	Data 0.342 (0.342)	Loss 0.4177 (0.4177)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [52][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.4701 (0.4335)	Acc@1 85.156 (84.940)	Acc@5 98.828 (99.423)
Epoch: [52][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.4555 (0.4317)	Acc@1 85.156 (85.129)	Acc@5 98.438 (99.361)
Epoch: [52][192/196]	Time 0.093 (0.087)	Data 0.000 (0.002)	Loss 0.4479 (0.4380)	Acc@1 84.375 (84.901)	Acc@5 100.000 (99.346)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:53/55; Lr: 0.1
batch Size 256
Epoch: [53][0/196]	Time 0.131 (0.131)	Data 0.346 (0.346)	Loss 0.4086 (0.4086)	Acc@1 85.938 (85.938)	Acc@5 98.828 (98.828)
Epoch: [53][64/196]	Time 0.085 (0.088)	Data 0.000 (0.006)	Loss 0.4484 (0.4238)	Acc@1 83.984 (85.667)	Acc@5 100.000 (99.381)
Epoch: [53][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.4203 (0.4273)	Acc@1 82.812 (85.359)	Acc@5 98.828 (99.370)
Epoch: [53][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.4888 (0.4312)	Acc@1 85.938 (85.219)	Acc@5 99.219 (99.348)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:54/55; Lr: 0.1
batch Size 256
Epoch: [54][0/196]	Time 0.135 (0.135)	Data 0.314 (0.314)	Loss 0.4047 (0.4047)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [54][64/196]	Time 0.091 (0.087)	Data 0.000 (0.005)	Loss 0.3022 (0.4232)	Acc@1 90.625 (85.343)	Acc@5 99.219 (99.471)
Epoch: [54][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.4274 (0.4292)	Acc@1 85.938 (85.153)	Acc@5 100.000 (99.422)
Epoch: [54][192/196]	Time 0.083 (0.086)	Data 0.000 (0.002)	Loss 0.3825 (0.4318)	Acc@1 86.328 (85.081)	Acc@5 99.219 (99.403)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:55/55; Lr: 0.1
batch Size 256
Epoch: [55][0/196]	Time 0.122 (0.122)	Data 0.302 (0.302)	Loss 0.3979 (0.3979)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [55][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.4508 (0.4169)	Acc@1 84.375 (85.733)	Acc@5 99.219 (99.357)
Epoch: [55][128/196]	Time 0.083 (0.086)	Data 0.000 (0.003)	Loss 0.3514 (0.4286)	Acc@1 89.062 (85.326)	Acc@5 100.000 (99.346)
Epoch: [55][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.4170 (0.4290)	Acc@1 85.547 (85.423)	Acc@5 100.000 (99.332)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  79.14
Max memory: 51.4381312
 17.320s  j: 56 bis 60
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9075
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 56
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:56/60; Lr: 0.1
batch Size 256
Epoch: [56][0/196]	Time 0.155 (0.155)	Data 0.270 (0.270)	Loss 0.3846 (0.3846)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [56][64/196]	Time 0.087 (0.089)	Data 0.000 (0.004)	Loss 0.3320 (0.4025)	Acc@1 89.062 (86.148)	Acc@5 100.000 (99.513)
Epoch: [56][128/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.3991 (0.4150)	Acc@1 85.938 (85.804)	Acc@5 99.609 (99.470)
Epoch: [56][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.3958 (0.4255)	Acc@1 87.500 (85.413)	Acc@5 99.609 (99.435)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:57/60; Lr: 0.1
batch Size 256
Epoch: [57][0/196]	Time 0.124 (0.124)	Data 0.303 (0.303)	Loss 0.4482 (0.4482)	Acc@1 86.719 (86.719)	Acc@5 98.438 (98.438)
Epoch: [57][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.5364 (0.4366)	Acc@1 83.594 (85.306)	Acc@5 99.609 (99.249)
Epoch: [57][128/196]	Time 0.096 (0.088)	Data 0.000 (0.003)	Loss 0.4962 (0.4298)	Acc@1 81.641 (85.365)	Acc@5 100.000 (99.373)
Epoch: [57][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.4143 (0.4276)	Acc@1 84.766 (85.434)	Acc@5 98.828 (99.369)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:58/60; Lr: 0.1
batch Size 256
Epoch: [58][0/196]	Time 0.119 (0.119)	Data 0.304 (0.304)	Loss 0.3802 (0.3802)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [58][64/196]	Time 0.095 (0.088)	Data 0.000 (0.005)	Loss 0.3826 (0.4285)	Acc@1 85.547 (85.090)	Acc@5 100.000 (99.393)
Epoch: [58][128/196]	Time 0.089 (0.087)	Data 0.000 (0.003)	Loss 0.3962 (0.4385)	Acc@1 85.938 (84.896)	Acc@5 99.219 (99.364)
Epoch: [58][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.4701 (0.4350)	Acc@1 81.250 (85.045)	Acc@5 99.609 (99.381)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:59/60; Lr: 0.1
batch Size 256
Epoch: [59][0/196]	Time 0.147 (0.147)	Data 0.314 (0.314)	Loss 0.4430 (0.4430)	Acc@1 85.938 (85.938)	Acc@5 98.047 (98.047)
Epoch: [59][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.4186 (0.4364)	Acc@1 86.719 (85.060)	Acc@5 98.828 (99.291)
Epoch: [59][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.5522 (0.4347)	Acc@1 81.250 (85.114)	Acc@5 98.828 (99.340)
Epoch: [59][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.4912 (0.4314)	Acc@1 84.375 (85.110)	Acc@5 99.219 (99.369)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:60/60; Lr: 0.1
batch Size 256
Epoch: [60][0/196]	Time 0.112 (0.112)	Data 0.303 (0.303)	Loss 0.4282 (0.4282)	Acc@1 85.547 (85.547)	Acc@5 98.828 (98.828)
Epoch: [60][64/196]	Time 0.087 (0.087)	Data 0.000 (0.005)	Loss 0.3988 (0.4238)	Acc@1 87.109 (85.673)	Acc@5 99.219 (99.399)
Epoch: [60][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.5044 (0.4264)	Acc@1 82.812 (85.359)	Acc@5 98.828 (99.385)
Epoch: [60][192/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.5686 (0.4265)	Acc@1 80.078 (85.383)	Acc@5 99.219 (99.399)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  75.72
Max memory: 51.4381312
 17.477s  j: 61 bis 65
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1563
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 61
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:61/65; Lr: 0.1
batch Size 256
Epoch: [61][0/196]	Time 0.159 (0.159)	Data 0.296 (0.296)	Loss 0.3691 (0.3691)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [61][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.3928 (0.3953)	Acc@1 85.938 (86.382)	Acc@5 99.609 (99.549)
Epoch: [61][128/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.3947 (0.4079)	Acc@1 87.500 (85.919)	Acc@5 99.219 (99.470)
Epoch: [61][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.4216 (0.4158)	Acc@1 86.719 (85.676)	Acc@5 98.438 (99.417)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:62/65; Lr: 0.1
batch Size 256
Epoch: [62][0/196]	Time 0.109 (0.109)	Data 0.315 (0.315)	Loss 0.3720 (0.3720)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [62][64/196]	Time 0.077 (0.089)	Data 0.000 (0.005)	Loss 0.4504 (0.4225)	Acc@1 82.422 (85.433)	Acc@5 100.000 (99.369)
Epoch: [62][128/196]	Time 0.096 (0.088)	Data 0.000 (0.003)	Loss 0.3905 (0.4244)	Acc@1 85.547 (85.498)	Acc@5 99.219 (99.403)
Epoch: [62][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.4651 (0.4250)	Acc@1 85.156 (85.510)	Acc@5 98.828 (99.362)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:63/65; Lr: 0.1
batch Size 256
Epoch: [63][0/196]	Time 0.104 (0.104)	Data 0.294 (0.294)	Loss 0.4643 (0.4643)	Acc@1 82.422 (82.422)	Acc@5 100.000 (100.000)
Epoch: [63][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.4214 (0.4048)	Acc@1 85.156 (85.950)	Acc@5 99.219 (99.417)
Epoch: [63][128/196]	Time 0.077 (0.087)	Data 0.000 (0.003)	Loss 0.4222 (0.4192)	Acc@1 85.938 (85.595)	Acc@5 98.438 (99.403)
Epoch: [63][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.4884 (0.4211)	Acc@1 84.375 (85.608)	Acc@5 98.438 (99.389)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:64/65; Lr: 0.1
batch Size 256
Epoch: [64][0/196]	Time 0.129 (0.129)	Data 0.285 (0.285)	Loss 0.3781 (0.3781)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [64][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.4731 (0.4142)	Acc@1 83.594 (85.823)	Acc@5 98.438 (99.393)
Epoch: [64][128/196]	Time 0.095 (0.087)	Data 0.000 (0.002)	Loss 0.5088 (0.4215)	Acc@1 83.203 (85.650)	Acc@5 98.828 (99.379)
Epoch: [64][192/196]	Time 0.086 (0.086)	Data 0.000 (0.002)	Loss 0.4803 (0.4240)	Acc@1 83.203 (85.565)	Acc@5 99.219 (99.383)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:65/65; Lr: 0.1
batch Size 256
Epoch: [65][0/196]	Time 0.130 (0.130)	Data 0.306 (0.306)	Loss 0.3468 (0.3468)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [65][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.3050 (0.4220)	Acc@1 91.016 (85.667)	Acc@5 100.000 (99.423)
Epoch: [65][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.4238 (0.4169)	Acc@1 85.547 (85.732)	Acc@5 99.219 (99.458)
Epoch: [65][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.4072 (0.4180)	Acc@1 85.938 (85.636)	Acc@5 99.609 (99.429)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  76.57
Max memory: 51.4381312
 17.344s  j: 66 bis 70
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1351
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 66
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:66/70; Lr: 0.1
batch Size 256
Epoch: [66][0/196]	Time 0.135 (0.135)	Data 0.328 (0.328)	Loss 0.3530 (0.3530)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [66][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.4243 (0.3926)	Acc@1 84.375 (86.653)	Acc@5 99.219 (99.477)
Epoch: [66][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.3982 (0.4062)	Acc@1 86.719 (86.086)	Acc@5 99.609 (99.443)
Epoch: [66][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.4111 (0.4163)	Acc@1 84.766 (85.778)	Acc@5 98.828 (99.415)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:67/70; Lr: 0.1
batch Size 256
Epoch: [67][0/196]	Time 0.139 (0.139)	Data 0.323 (0.323)	Loss 0.3538 (0.3538)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [67][64/196]	Time 0.094 (0.090)	Data 0.000 (0.005)	Loss 0.3663 (0.4108)	Acc@1 87.500 (85.835)	Acc@5 99.609 (99.489)
Epoch: [67][128/196]	Time 0.096 (0.089)	Data 0.000 (0.003)	Loss 0.4089 (0.4201)	Acc@1 85.938 (85.489)	Acc@5 100.000 (99.473)
Epoch: [67][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.3990 (0.4214)	Acc@1 87.891 (85.423)	Acc@5 99.219 (99.472)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:68/70; Lr: 0.1
batch Size 256
Epoch: [68][0/196]	Time 0.135 (0.135)	Data 0.321 (0.321)	Loss 0.4424 (0.4424)	Acc@1 84.375 (84.375)	Acc@5 98.828 (98.828)
Epoch: [68][64/196]	Time 0.092 (0.091)	Data 0.000 (0.005)	Loss 0.4526 (0.4105)	Acc@1 85.156 (85.865)	Acc@5 99.219 (99.423)
Epoch: [68][128/196]	Time 0.083 (0.090)	Data 0.000 (0.003)	Loss 0.5084 (0.4176)	Acc@1 85.938 (85.680)	Acc@5 99.219 (99.352)
Epoch: [68][192/196]	Time 0.094 (0.089)	Data 0.000 (0.002)	Loss 0.3838 (0.4151)	Acc@1 87.500 (85.798)	Acc@5 99.609 (99.399)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:69/70; Lr: 0.1
batch Size 256
Epoch: [69][0/196]	Time 0.118 (0.118)	Data 0.320 (0.320)	Loss 0.3336 (0.3336)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [69][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.4845 (0.4160)	Acc@1 83.203 (85.631)	Acc@5 99.609 (99.435)
Epoch: [69][128/196]	Time 0.092 (0.088)	Data 0.000 (0.003)	Loss 0.4106 (0.4163)	Acc@1 86.719 (85.644)	Acc@5 99.609 (99.394)
Epoch: [69][192/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 0.3770 (0.4172)	Acc@1 87.109 (85.685)	Acc@5 99.609 (99.391)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:70/70; Lr: 0.1
batch Size 256
Epoch: [70][0/196]	Time 0.125 (0.125)	Data 0.341 (0.341)	Loss 0.4815 (0.4815)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [70][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.4692 (0.4045)	Acc@1 80.859 (85.871)	Acc@5 99.219 (99.447)
Epoch: [70][128/196]	Time 0.092 (0.089)	Data 0.000 (0.003)	Loss 0.3944 (0.4094)	Acc@1 85.938 (85.804)	Acc@5 99.609 (99.410)
Epoch: [70][192/196]	Time 0.095 (0.088)	Data 0.000 (0.002)	Loss 0.4007 (0.4135)	Acc@1 85.938 (85.689)	Acc@5 100.000 (99.413)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  75.01
Max memory: 51.4381312
 17.711s  j: 71 bis 75
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5431
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 71
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:71/75; Lr: 0.1
batch Size 256
Epoch: [71][0/196]	Time 0.128 (0.128)	Data 0.313 (0.313)	Loss 0.3899 (0.3899)	Acc@1 85.547 (85.547)	Acc@5 100.000 (100.000)
Epoch: [71][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.3791 (0.3957)	Acc@1 85.938 (86.472)	Acc@5 100.000 (99.417)
Epoch: [71][128/196]	Time 0.101 (0.089)	Data 0.000 (0.003)	Loss 0.3327 (0.4068)	Acc@1 87.891 (86.040)	Acc@5 99.609 (99.422)
Epoch: [71][192/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.3186 (0.4097)	Acc@1 89.062 (85.899)	Acc@5 99.609 (99.454)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:72/75; Lr: 0.1
batch Size 256
Epoch: [72][0/196]	Time 0.118 (0.118)	Data 0.278 (0.278)	Loss 0.3715 (0.3715)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [72][64/196]	Time 0.087 (0.089)	Data 0.000 (0.004)	Loss 0.4166 (0.4143)	Acc@1 85.156 (85.739)	Acc@5 98.828 (99.477)
Epoch: [72][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.4283 (0.4129)	Acc@1 85.938 (85.668)	Acc@5 99.609 (99.458)
Epoch: [72][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.3517 (0.4174)	Acc@1 87.891 (85.549)	Acc@5 99.219 (99.447)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:73/75; Lr: 0.1
batch Size 256
Epoch: [73][0/196]	Time 0.121 (0.121)	Data 0.308 (0.308)	Loss 0.3719 (0.3719)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [73][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.4064 (0.4121)	Acc@1 86.719 (85.847)	Acc@5 100.000 (99.489)
Epoch: [73][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.4300 (0.4075)	Acc@1 87.109 (86.086)	Acc@5 98.828 (99.455)
Epoch: [73][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.4169 (0.4148)	Acc@1 85.547 (85.804)	Acc@5 99.219 (99.439)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:74/75; Lr: 0.1
batch Size 256
Epoch: [74][0/196]	Time 0.113 (0.113)	Data 0.260 (0.260)	Loss 0.4043 (0.4043)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [74][64/196]	Time 0.115 (0.088)	Data 0.000 (0.004)	Loss 0.4166 (0.4226)	Acc@1 87.891 (85.691)	Acc@5 99.219 (99.345)
Epoch: [74][128/196]	Time 0.079 (0.088)	Data 0.000 (0.002)	Loss 0.3945 (0.4191)	Acc@1 86.328 (85.674)	Acc@5 99.609 (99.428)
Epoch: [74][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.3662 (0.4151)	Acc@1 88.672 (85.816)	Acc@5 98.828 (99.437)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:75/75; Lr: 0.1
batch Size 256
Epoch: [75][0/196]	Time 0.130 (0.130)	Data 0.267 (0.267)	Loss 0.3909 (0.3909)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [75][64/196]	Time 0.080 (0.089)	Data 0.000 (0.004)	Loss 0.4351 (0.4070)	Acc@1 83.594 (86.052)	Acc@5 99.609 (99.447)
Epoch: [75][128/196]	Time 0.094 (0.089)	Data 0.000 (0.002)	Loss 0.4520 (0.4117)	Acc@1 83.594 (85.928)	Acc@5 98.828 (99.410)
Epoch: [75][192/196]	Time 0.092 (0.089)	Data 0.000 (0.002)	Loss 0.5381 (0.4208)	Acc@1 79.297 (85.464)	Acc@5 99.219 (99.405)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  68.22
Max memory: 51.4381312
 17.729s  j: 76 bis 80
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3772
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 76
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:76/80; Lr: 0.1
batch Size 256
Epoch: [76][0/196]	Time 0.151 (0.151)	Data 0.280 (0.280)	Loss 0.3968 (0.3968)	Acc@1 86.328 (86.328)	Acc@5 99.609 (99.609)
Epoch: [76][64/196]	Time 0.083 (0.087)	Data 0.000 (0.005)	Loss 0.3915 (0.3784)	Acc@1 87.500 (86.959)	Acc@5 99.219 (99.573)
Epoch: [76][128/196]	Time 0.087 (0.086)	Data 0.000 (0.002)	Loss 0.3200 (0.3928)	Acc@1 87.500 (86.589)	Acc@5 98.828 (99.491)
Epoch: [76][192/196]	Time 0.095 (0.086)	Data 0.000 (0.002)	Loss 0.3498 (0.4027)	Acc@1 88.281 (86.269)	Acc@5 98.828 (99.439)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:77/80; Lr: 0.1
batch Size 256
Epoch: [77][0/196]	Time 0.135 (0.135)	Data 0.316 (0.316)	Loss 0.4104 (0.4104)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [77][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.4621 (0.4099)	Acc@1 82.812 (86.094)	Acc@5 99.219 (99.363)
Epoch: [77][128/196]	Time 0.092 (0.087)	Data 0.000 (0.003)	Loss 0.2915 (0.3989)	Acc@1 90.625 (86.501)	Acc@5 99.219 (99.416)
Epoch: [77][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.5237 (0.4098)	Acc@1 83.984 (86.077)	Acc@5 99.219 (99.439)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:78/80; Lr: 0.1
batch Size 256
Epoch: [78][0/196]	Time 0.129 (0.129)	Data 0.301 (0.301)	Loss 0.4223 (0.4223)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [78][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.3843 (0.4081)	Acc@1 85.156 (85.751)	Acc@5 99.609 (99.483)
Epoch: [78][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.4845 (0.4075)	Acc@1 83.984 (85.835)	Acc@5 99.219 (99.482)
Epoch: [78][192/196]	Time 0.084 (0.086)	Data 0.000 (0.002)	Loss 0.3898 (0.4101)	Acc@1 87.500 (85.826)	Acc@5 99.609 (99.468)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:79/80; Lr: 0.1
batch Size 256
Epoch: [79][0/196]	Time 0.122 (0.122)	Data 0.306 (0.306)	Loss 0.3838 (0.3838)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [79][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.5140 (0.4118)	Acc@1 81.250 (85.697)	Acc@5 99.609 (99.507)
Epoch: [79][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.4869 (0.4103)	Acc@1 84.766 (85.771)	Acc@5 99.609 (99.494)
Epoch: [79][192/196]	Time 0.095 (0.087)	Data 0.000 (0.002)	Loss 0.3508 (0.4099)	Acc@1 87.109 (85.826)	Acc@5 99.609 (99.484)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:80/80; Lr: 0.1
batch Size 256
Epoch: [80][0/196]	Time 0.124 (0.124)	Data 0.346 (0.346)	Loss 0.3438 (0.3438)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [80][64/196]	Time 0.086 (0.086)	Data 0.000 (0.006)	Loss 0.3936 (0.4143)	Acc@1 85.547 (85.685)	Acc@5 99.609 (99.387)
Epoch: [80][128/196]	Time 0.087 (0.086)	Data 0.000 (0.003)	Loss 0.3713 (0.4175)	Acc@1 87.891 (85.598)	Acc@5 100.000 (99.419)
Epoch: [80][192/196]	Time 0.079 (0.086)	Data 0.000 (0.002)	Loss 0.3429 (0.4147)	Acc@1 86.328 (85.664)	Acc@5 100.000 (99.447)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  80.01
Max memory: 51.4381312
 17.270s  j: 81 bis 85
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5120
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 81
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:81/85; Lr: 0.1
batch Size 256
Epoch: [81][0/196]	Time 0.153 (0.153)	Data 0.308 (0.308)	Loss 0.3693 (0.3693)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [81][64/196]	Time 0.086 (0.087)	Data 0.000 (0.005)	Loss 0.3387 (0.3821)	Acc@1 87.891 (86.917)	Acc@5 99.609 (99.549)
Epoch: [81][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.3658 (0.3944)	Acc@1 86.719 (86.319)	Acc@5 100.000 (99.525)
Epoch: [81][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.4484 (0.4016)	Acc@1 81.250 (86.110)	Acc@5 99.219 (99.496)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:82/85; Lr: 0.1
batch Size 256
Epoch: [82][0/196]	Time 0.139 (0.139)	Data 0.297 (0.297)	Loss 0.3308 (0.3308)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [82][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.4257 (0.3948)	Acc@1 85.156 (86.526)	Acc@5 99.219 (99.459)
Epoch: [82][128/196]	Time 0.076 (0.087)	Data 0.000 (0.003)	Loss 0.3333 (0.4033)	Acc@1 89.844 (86.186)	Acc@5 98.828 (99.455)
Epoch: [82][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.4412 (0.4093)	Acc@1 88.281 (86.012)	Acc@5 100.000 (99.456)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:83/85; Lr: 0.1
batch Size 256
Epoch: [83][0/196]	Time 0.156 (0.156)	Data 0.325 (0.325)	Loss 0.4207 (0.4207)	Acc@1 86.328 (86.328)	Acc@5 99.219 (99.219)
Epoch: [83][64/196]	Time 0.093 (0.087)	Data 0.000 (0.005)	Loss 0.3796 (0.3927)	Acc@1 86.719 (86.304)	Acc@5 100.000 (99.441)
Epoch: [83][128/196]	Time 0.087 (0.086)	Data 0.000 (0.003)	Loss 0.4931 (0.4009)	Acc@1 83.203 (85.986)	Acc@5 98.828 (99.443)
Epoch: [83][192/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.3868 (0.4053)	Acc@1 85.938 (85.871)	Acc@5 100.000 (99.405)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:84/85; Lr: 0.1
batch Size 256
Epoch: [84][0/196]	Time 0.130 (0.130)	Data 0.320 (0.320)	Loss 0.3663 (0.3663)	Acc@1 90.234 (90.234)	Acc@5 99.219 (99.219)
Epoch: [84][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.4601 (0.4067)	Acc@1 85.547 (85.998)	Acc@5 99.609 (99.507)
Epoch: [84][128/196]	Time 0.081 (0.087)	Data 0.000 (0.003)	Loss 0.5134 (0.4081)	Acc@1 82.422 (86.062)	Acc@5 99.219 (99.531)
Epoch: [84][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.4680 (0.4104)	Acc@1 85.938 (85.952)	Acc@5 98.828 (99.510)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:85/85; Lr: 0.1
batch Size 256
Epoch: [85][0/196]	Time 0.109 (0.109)	Data 0.319 (0.319)	Loss 0.3972 (0.3972)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [85][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.4839 (0.4101)	Acc@1 83.594 (85.901)	Acc@5 98.828 (99.453)
Epoch: [85][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.4196 (0.4060)	Acc@1 85.547 (86.174)	Acc@5 100.000 (99.467)
Epoch: [85][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.3983 (0.4079)	Acc@1 85.156 (86.093)	Acc@5 100.000 (99.445)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  72.86
Max memory: 51.4381312
 17.371s  j: 86 bis 90
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1713
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 86
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:86/90; Lr: 0.1
batch Size 256
Epoch: [86][0/196]	Time 0.181 (0.181)	Data 0.301 (0.301)	Loss 0.3084 (0.3084)	Acc@1 90.625 (90.625)	Acc@5 99.219 (99.219)
Epoch: [86][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.3169 (0.3707)	Acc@1 89.062 (87.163)	Acc@5 99.609 (99.549)
Epoch: [86][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.4276 (0.3930)	Acc@1 85.156 (86.425)	Acc@5 98.828 (99.522)
Epoch: [86][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.4123 (0.4014)	Acc@1 87.109 (86.182)	Acc@5 99.609 (99.478)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:87/90; Lr: 0.1
batch Size 256
Epoch: [87][0/196]	Time 0.137 (0.137)	Data 0.263 (0.263)	Loss 0.3827 (0.3827)	Acc@1 87.500 (87.500)	Acc@5 98.828 (98.828)
Epoch: [87][64/196]	Time 0.086 (0.088)	Data 0.000 (0.004)	Loss 0.3869 (0.4171)	Acc@1 87.109 (85.421)	Acc@5 99.609 (99.441)
Epoch: [87][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.3768 (0.4076)	Acc@1 85.547 (85.835)	Acc@5 99.609 (99.455)
Epoch: [87][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.4056 (0.4089)	Acc@1 84.766 (85.964)	Acc@5 99.609 (99.460)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:88/90; Lr: 0.1
batch Size 256
Epoch: [88][0/196]	Time 0.127 (0.127)	Data 0.270 (0.270)	Loss 0.3075 (0.3075)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [88][64/196]	Time 0.079 (0.086)	Data 0.000 (0.004)	Loss 0.4394 (0.3877)	Acc@1 84.766 (86.725)	Acc@5 99.219 (99.483)
Epoch: [88][128/196]	Time 0.084 (0.086)	Data 0.000 (0.002)	Loss 0.4469 (0.3995)	Acc@1 87.109 (86.292)	Acc@5 100.000 (99.403)
Epoch: [88][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.3848 (0.4031)	Acc@1 86.719 (86.186)	Acc@5 99.609 (99.411)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:89/90; Lr: 0.1
batch Size 256
Epoch: [89][0/196]	Time 0.106 (0.106)	Data 0.305 (0.305)	Loss 0.4657 (0.4657)	Acc@1 81.250 (81.250)	Acc@5 99.609 (99.609)
Epoch: [89][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.4457 (0.4137)	Acc@1 83.984 (85.715)	Acc@5 99.609 (99.393)
Epoch: [89][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.3835 (0.4095)	Acc@1 87.891 (85.889)	Acc@5 99.219 (99.455)
Epoch: [89][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.3829 (0.4095)	Acc@1 87.891 (85.875)	Acc@5 99.219 (99.466)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:90/90; Lr: 0.1
batch Size 256
Epoch: [90][0/196]	Time 0.122 (0.122)	Data 0.313 (0.313)	Loss 0.3605 (0.3605)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [90][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.3021 (0.4004)	Acc@1 89.062 (86.256)	Acc@5 100.000 (99.435)
Epoch: [90][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.3912 (0.3954)	Acc@1 87.500 (86.546)	Acc@5 99.609 (99.443)
Epoch: [90][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.4210 (0.3997)	Acc@1 85.938 (86.365)	Acc@5 99.219 (99.447)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  78.96
Max memory: 51.4381312
 17.555s  j: 91 bis 95
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2951
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 91
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:91/95; Lr: 0.1
batch Size 256
Epoch: [91][0/196]	Time 0.134 (0.134)	Data 0.307 (0.307)	Loss 0.3593 (0.3593)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [91][64/196]	Time 0.082 (0.090)	Data 0.000 (0.005)	Loss 0.3950 (0.3899)	Acc@1 83.594 (86.575)	Acc@5 99.609 (99.543)
Epoch: [91][128/196]	Time 0.092 (0.089)	Data 0.000 (0.003)	Loss 0.3669 (0.3907)	Acc@1 85.547 (86.483)	Acc@5 99.609 (99.522)
Epoch: [91][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.4562 (0.3962)	Acc@1 83.594 (86.239)	Acc@5 99.609 (99.506)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:92/95; Lr: 0.1
batch Size 256
Epoch: [92][0/196]	Time 0.135 (0.135)	Data 0.290 (0.290)	Loss 0.4128 (0.4128)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [92][64/196]	Time 0.108 (0.090)	Data 0.000 (0.005)	Loss 0.3780 (0.3977)	Acc@1 87.891 (85.992)	Acc@5 98.438 (99.453)
Epoch: [92][128/196]	Time 0.087 (0.090)	Data 0.000 (0.002)	Loss 0.4112 (0.4018)	Acc@1 84.766 (86.016)	Acc@5 100.000 (99.403)
Epoch: [92][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.3483 (0.4070)	Acc@1 86.328 (85.792)	Acc@5 100.000 (99.423)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:93/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [93][0/196]	Time 0.121 (0.121)	Data 0.305 (0.305)	Loss 0.3857 (0.3857)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [93][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.2873 (0.3328)	Acc@1 90.625 (88.708)	Acc@5 99.219 (99.615)
Epoch: [93][128/196]	Time 0.088 (0.090)	Data 0.000 (0.003)	Loss 0.2452 (0.3030)	Acc@1 90.234 (89.689)	Acc@5 100.000 (99.676)
Epoch: [93][192/196]	Time 0.092 (0.089)	Data 0.000 (0.002)	Loss 0.2638 (0.2921)	Acc@1 91.797 (90.048)	Acc@5 100.000 (99.702)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:94/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [94][0/196]	Time 0.121 (0.121)	Data 0.319 (0.319)	Loss 0.2789 (0.2789)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [94][64/196]	Time 0.098 (0.092)	Data 0.000 (0.005)	Loss 0.2431 (0.2549)	Acc@1 92.578 (91.226)	Acc@5 99.219 (99.748)
Epoch: [94][128/196]	Time 0.086 (0.091)	Data 0.000 (0.003)	Loss 0.2533 (0.2548)	Acc@1 91.797 (91.385)	Acc@5 99.609 (99.758)
Epoch: [94][192/196]	Time 0.084 (0.090)	Data 0.000 (0.002)	Loss 0.2350 (0.2500)	Acc@1 94.531 (91.503)	Acc@5 99.609 (99.775)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:95/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [95][0/196]	Time 0.126 (0.126)	Data 0.293 (0.293)	Loss 0.2044 (0.2044)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [95][64/196]	Time 0.088 (0.091)	Data 0.000 (0.005)	Loss 0.2004 (0.2338)	Acc@1 92.578 (92.169)	Acc@5 99.609 (99.760)
Epoch: [95][128/196]	Time 0.085 (0.090)	Data 0.000 (0.002)	Loss 0.2413 (0.2347)	Acc@1 93.359 (91.985)	Acc@5 99.219 (99.779)
Epoch: [95][192/196]	Time 0.089 (0.090)	Data 0.000 (0.002)	Loss 0.2111 (0.2351)	Acc@1 92.188 (92.052)	Acc@5 100.000 (99.792)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.17
Max memory: 51.4381312
 17.948s  j: 96 bis 100
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2990
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 96
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:96/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [96][0/196]	Time 0.151 (0.151)	Data 0.307 (0.307)	Loss 0.1727 (0.1727)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [96][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.2427 (0.2254)	Acc@1 89.844 (92.326)	Acc@5 100.000 (99.802)
Epoch: [96][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.2169 (0.2230)	Acc@1 91.406 (92.499)	Acc@5 100.000 (99.821)
Epoch: [96][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.2394 (0.2238)	Acc@1 92.188 (92.428)	Acc@5 99.609 (99.802)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:97/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [97][0/196]	Time 0.122 (0.122)	Data 0.317 (0.317)	Loss 0.2123 (0.2123)	Acc@1 92.969 (92.969)	Acc@5 99.609 (99.609)
Epoch: [97][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.1954 (0.2180)	Acc@1 94.531 (92.572)	Acc@5 100.000 (99.826)
Epoch: [97][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.1863 (0.2178)	Acc@1 92.188 (92.633)	Acc@5 100.000 (99.827)
Epoch: [97][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.1622 (0.2185)	Acc@1 94.922 (92.576)	Acc@5 100.000 (99.816)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:98/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [98][0/196]	Time 0.131 (0.131)	Data 0.308 (0.308)	Loss 0.2620 (0.2620)	Acc@1 90.234 (90.234)	Acc@5 100.000 (100.000)
Epoch: [98][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.1973 (0.2125)	Acc@1 91.016 (92.794)	Acc@5 100.000 (99.838)
Epoch: [98][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.2191 (0.2082)	Acc@1 91.797 (92.948)	Acc@5 99.609 (99.846)
Epoch: [98][192/196]	Time 0.095 (0.087)	Data 0.000 (0.002)	Loss 0.2111 (0.2110)	Acc@1 94.141 (92.738)	Acc@5 100.000 (99.834)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:99/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [99][0/196]	Time 0.104 (0.104)	Data 0.301 (0.301)	Loss 0.2059 (0.2059)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [99][64/196]	Time 0.096 (0.088)	Data 0.000 (0.005)	Loss 0.1890 (0.2020)	Acc@1 94.531 (93.119)	Acc@5 100.000 (99.838)
Epoch: [99][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.1540 (0.2021)	Acc@1 94.531 (93.072)	Acc@5 100.000 (99.858)
Epoch: [99][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.2740 (0.2067)	Acc@1 89.844 (92.938)	Acc@5 99.609 (99.832)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:100/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [100][0/196]	Time 0.140 (0.140)	Data 0.327 (0.327)	Loss 0.1674 (0.1674)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [100][64/196]	Time 0.086 (0.087)	Data 0.000 (0.005)	Loss 0.2086 (0.1961)	Acc@1 92.969 (93.329)	Acc@5 100.000 (99.832)
Epoch: [100][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.2032 (0.1961)	Acc@1 94.141 (93.371)	Acc@5 99.609 (99.803)
Epoch: [100][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.2299 (0.2002)	Acc@1 91.016 (93.193)	Acc@5 100.000 (99.820)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.91
Max memory: 51.4381312
 17.524s  j: 101 bis 105
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 680
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 101
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:101/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [101][0/196]	Time 0.142 (0.142)	Data 0.325 (0.325)	Loss 0.2022 (0.2022)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [101][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.1917 (0.1946)	Acc@1 94.141 (93.425)	Acc@5 99.219 (99.892)
Epoch: [101][128/196]	Time 0.093 (0.087)	Data 0.000 (0.003)	Loss 0.2070 (0.1996)	Acc@1 92.578 (93.160)	Acc@5 100.000 (99.855)
Epoch: [101][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.1846 (0.1980)	Acc@1 94.531 (93.224)	Acc@5 100.000 (99.858)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:102/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [102][0/196]	Time 0.131 (0.131)	Data 0.289 (0.289)	Loss 0.1998 (0.1998)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [102][64/196]	Time 0.088 (0.085)	Data 0.000 (0.005)	Loss 0.1803 (0.1921)	Acc@1 94.531 (93.522)	Acc@5 100.000 (99.820)
Epoch: [102][128/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.1864 (0.1930)	Acc@1 91.406 (93.423)	Acc@5 100.000 (99.849)
Epoch: [102][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.2331 (0.1941)	Acc@1 91.406 (93.321)	Acc@5 99.609 (99.844)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:103/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [103][0/196]	Time 0.113 (0.113)	Data 0.300 (0.300)	Loss 0.1650 (0.1650)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [103][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.1658 (0.1819)	Acc@1 93.359 (93.720)	Acc@5 100.000 (99.892)
Epoch: [103][128/196]	Time 0.081 (0.087)	Data 0.000 (0.003)	Loss 0.1897 (0.1931)	Acc@1 94.531 (93.293)	Acc@5 99.609 (99.882)
Epoch: [103][192/196]	Time 0.077 (0.087)	Data 0.000 (0.002)	Loss 0.1633 (0.1942)	Acc@1 94.922 (93.270)	Acc@5 100.000 (99.875)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:104/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [104][0/196]	Time 0.136 (0.136)	Data 0.286 (0.286)	Loss 0.2037 (0.2037)	Acc@1 92.969 (92.969)	Acc@5 99.609 (99.609)
Epoch: [104][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.1931 (0.1844)	Acc@1 94.531 (93.774)	Acc@5 100.000 (99.886)
Epoch: [104][128/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.1759 (0.1904)	Acc@1 92.578 (93.529)	Acc@5 100.000 (99.879)
Epoch: [104][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.1268 (0.1874)	Acc@1 94.531 (93.647)	Acc@5 100.000 (99.866)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:105/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [105][0/196]	Time 0.146 (0.146)	Data 0.299 (0.299)	Loss 0.1890 (0.1890)	Acc@1 94.141 (94.141)	Acc@5 99.609 (99.609)
Epoch: [105][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.1239 (0.1861)	Acc@1 94.922 (93.846)	Acc@5 100.000 (99.886)
Epoch: [105][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.1907 (0.1866)	Acc@1 93.359 (93.626)	Acc@5 100.000 (99.876)
Epoch: [105][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.1976 (0.1860)	Acc@1 92.578 (93.608)	Acc@5 99.609 (99.875)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.42
Max memory: 51.4381312
 17.513s  j: 106 bis 110
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5964
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 106
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:106/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [106][0/196]	Time 0.141 (0.141)	Data 0.281 (0.281)	Loss 0.1293 (0.1293)	Acc@1 96.094 (96.094)	Acc@5 99.609 (99.609)
Epoch: [106][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.2205 (0.1748)	Acc@1 94.141 (94.105)	Acc@5 99.609 (99.886)
Epoch: [106][128/196]	Time 0.097 (0.089)	Data 0.000 (0.002)	Loss 0.2226 (0.1770)	Acc@1 92.969 (94.035)	Acc@5 99.609 (99.870)
Epoch: [106][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.2384 (0.1804)	Acc@1 90.234 (93.890)	Acc@5 100.000 (99.858)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:107/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [107][0/196]	Time 0.130 (0.130)	Data 0.273 (0.273)	Loss 0.1619 (0.1619)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [107][64/196]	Time 0.087 (0.090)	Data 0.000 (0.004)	Loss 0.1784 (0.1830)	Acc@1 92.969 (93.684)	Acc@5 100.000 (99.874)
Epoch: [107][128/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.1684 (0.1779)	Acc@1 95.312 (93.917)	Acc@5 100.000 (99.882)
Epoch: [107][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.2271 (0.1803)	Acc@1 92.188 (93.841)	Acc@5 100.000 (99.891)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:108/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [108][0/196]	Time 0.121 (0.121)	Data 0.298 (0.298)	Loss 0.1492 (0.1492)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [108][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.1661 (0.1725)	Acc@1 95.703 (94.093)	Acc@5 99.609 (99.874)
Epoch: [108][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.1426 (0.1763)	Acc@1 96.094 (94.044)	Acc@5 100.000 (99.864)
Epoch: [108][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.1939 (0.1777)	Acc@1 93.359 (93.932)	Acc@5 100.000 (99.875)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:109/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [109][0/196]	Time 0.139 (0.139)	Data 0.270 (0.270)	Loss 0.2231 (0.2231)	Acc@1 91.406 (91.406)	Acc@5 99.609 (99.609)
Epoch: [109][64/196]	Time 0.087 (0.089)	Data 0.000 (0.004)	Loss 0.1468 (0.1813)	Acc@1 94.922 (93.708)	Acc@5 100.000 (99.850)
Epoch: [109][128/196]	Time 0.096 (0.089)	Data 0.000 (0.002)	Loss 0.1351 (0.1801)	Acc@1 93.750 (93.801)	Acc@5 100.000 (99.873)
Epoch: [109][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.1993 (0.1789)	Acc@1 91.016 (93.801)	Acc@5 100.000 (99.889)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:110/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [110][0/196]	Time 0.131 (0.131)	Data 0.295 (0.295)	Loss 0.2319 (0.2319)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [110][64/196]	Time 0.094 (0.089)	Data 0.000 (0.005)	Loss 0.1604 (0.1736)	Acc@1 93.750 (94.135)	Acc@5 100.000 (99.916)
Epoch: [110][128/196]	Time 0.081 (0.088)	Data 0.000 (0.003)	Loss 0.1893 (0.1744)	Acc@1 94.531 (94.089)	Acc@5 99.609 (99.900)
Epoch: [110][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1882 (0.1752)	Acc@1 92.969 (94.015)	Acc@5 99.609 (99.887)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.9
Max memory: 51.4381312
 17.588s  j: 111 bis 115
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1148
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 111
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:111/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [111][0/196]	Time 0.147 (0.147)	Data 0.320 (0.320)	Loss 0.0950 (0.0950)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [111][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.1655 (0.1614)	Acc@1 94.531 (94.501)	Acc@5 100.000 (99.898)
Epoch: [111][128/196]	Time 0.079 (0.087)	Data 0.000 (0.003)	Loss 0.1489 (0.1633)	Acc@1 95.703 (94.353)	Acc@5 100.000 (99.897)
Epoch: [111][192/196]	Time 0.096 (0.087)	Data 0.000 (0.002)	Loss 0.2109 (0.1706)	Acc@1 93.359 (94.070)	Acc@5 99.219 (99.887)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:112/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [112][0/196]	Time 0.134 (0.134)	Data 0.264 (0.264)	Loss 0.1048 (0.1048)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [112][64/196]	Time 0.099 (0.092)	Data 0.000 (0.004)	Loss 0.1711 (0.1671)	Acc@1 93.750 (94.459)	Acc@5 100.000 (99.886)
Epoch: [112][128/196]	Time 0.087 (0.090)	Data 0.000 (0.002)	Loss 0.1960 (0.1684)	Acc@1 93.359 (94.286)	Acc@5 100.000 (99.903)
Epoch: [112][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.1340 (0.1714)	Acc@1 95.703 (94.173)	Acc@5 100.000 (99.895)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:113/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [113][0/196]	Time 0.129 (0.129)	Data 0.292 (0.292)	Loss 0.1582 (0.1582)	Acc@1 94.922 (94.922)	Acc@5 99.609 (99.609)
Epoch: [113][64/196]	Time 0.093 (0.090)	Data 0.000 (0.005)	Loss 0.2118 (0.1667)	Acc@1 93.359 (94.207)	Acc@5 99.609 (99.922)
Epoch: [113][128/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.1582 (0.1661)	Acc@1 94.922 (94.340)	Acc@5 100.000 (99.909)
Epoch: [113][192/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.1983 (0.1692)	Acc@1 94.141 (94.211)	Acc@5 100.000 (99.901)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:114/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [114][0/196]	Time 0.116 (0.116)	Data 0.352 (0.352)	Loss 0.1374 (0.1374)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [114][64/196]	Time 0.081 (0.089)	Data 0.000 (0.006)	Loss 0.1842 (0.1661)	Acc@1 94.922 (94.195)	Acc@5 100.000 (99.886)
Epoch: [114][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.1324 (0.1647)	Acc@1 95.703 (94.286)	Acc@5 100.000 (99.891)
Epoch: [114][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.1539 (0.1695)	Acc@1 94.531 (94.193)	Acc@5 100.000 (99.895)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:115/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [115][0/196]	Time 0.123 (0.123)	Data 0.312 (0.312)	Loss 0.1421 (0.1421)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [115][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.1654 (0.1618)	Acc@1 94.141 (94.471)	Acc@5 100.000 (99.892)
Epoch: [115][128/196]	Time 0.083 (0.089)	Data 0.000 (0.003)	Loss 0.1944 (0.1657)	Acc@1 92.969 (94.280)	Acc@5 100.000 (99.903)
Epoch: [115][192/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.1786 (0.1669)	Acc@1 93.359 (94.298)	Acc@5 100.000 (99.901)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.37
Max memory: 51.4381312
 17.802s  j: 116 bis 120
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6870
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 116
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:116/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [116][0/196]	Time 0.140 (0.140)	Data 0.284 (0.284)	Loss 0.1589 (0.1589)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [116][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.1491 (0.1625)	Acc@1 94.141 (94.243)	Acc@5 100.000 (99.958)
Epoch: [116][128/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.1768 (0.1612)	Acc@1 93.750 (94.356)	Acc@5 100.000 (99.930)
Epoch: [116][192/196]	Time 0.098 (0.089)	Data 0.000 (0.002)	Loss 0.1270 (0.1619)	Acc@1 95.312 (94.430)	Acc@5 100.000 (99.923)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:117/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [117][0/196]	Time 0.129 (0.129)	Data 0.305 (0.305)	Loss 0.1468 (0.1468)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [117][64/196]	Time 0.080 (0.090)	Data 0.000 (0.005)	Loss 0.1459 (0.1593)	Acc@1 93.750 (94.495)	Acc@5 100.000 (99.922)
Epoch: [117][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.1506 (0.1610)	Acc@1 95.312 (94.525)	Acc@5 100.000 (99.903)
Epoch: [117][192/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.1276 (0.1657)	Acc@1 95.703 (94.343)	Acc@5 99.609 (99.891)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:118/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [118][0/196]	Time 0.114 (0.114)	Data 0.338 (0.338)	Loss 0.1448 (0.1448)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)
Epoch: [118][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.1309 (0.1557)	Acc@1 97.266 (94.706)	Acc@5 100.000 (99.946)
Epoch: [118][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.1993 (0.1591)	Acc@1 92.188 (94.507)	Acc@5 100.000 (99.927)
Epoch: [118][192/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.1988 (0.1630)	Acc@1 93.750 (94.400)	Acc@5 99.219 (99.913)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:119/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [119][0/196]	Time 0.137 (0.137)	Data 0.326 (0.326)	Loss 0.1284 (0.1284)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [119][64/196]	Time 0.086 (0.092)	Data 0.000 (0.005)	Loss 0.1438 (0.1579)	Acc@1 95.312 (94.718)	Acc@5 100.000 (99.934)
Epoch: [119][128/196]	Time 0.088 (0.090)	Data 0.000 (0.003)	Loss 0.1605 (0.1588)	Acc@1 94.141 (94.568)	Acc@5 100.000 (99.927)
Epoch: [119][192/196]	Time 0.090 (0.090)	Data 0.000 (0.002)	Loss 0.1944 (0.1635)	Acc@1 91.797 (94.345)	Acc@5 100.000 (99.905)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:120/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [120][0/196]	Time 0.116 (0.116)	Data 0.301 (0.301)	Loss 0.0873 (0.0873)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [120][64/196]	Time 0.092 (0.091)	Data 0.000 (0.005)	Loss 0.1799 (0.1587)	Acc@1 93.750 (94.429)	Acc@5 100.000 (99.922)
Epoch: [120][128/196]	Time 0.082 (0.090)	Data 0.000 (0.003)	Loss 0.1755 (0.1611)	Acc@1 94.141 (94.310)	Acc@5 100.000 (99.909)
Epoch: [120][192/196]	Time 0.091 (0.090)	Data 0.000 (0.002)	Loss 0.1572 (0.1624)	Acc@1 93.359 (94.220)	Acc@5 100.000 (99.917)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.25
Max memory: 51.4381312
 17.948s  j: 121 bis 125
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3396
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 121
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:121/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [121][0/196]	Time 0.141 (0.141)	Data 0.271 (0.271)	Loss 0.1196 (0.1196)	Acc@1 97.266 (97.266)	Acc@5 99.609 (99.609)
Epoch: [121][64/196]	Time 0.097 (0.090)	Data 0.000 (0.004)	Loss 0.1644 (0.1486)	Acc@1 93.750 (94.994)	Acc@5 100.000 (99.904)
Epoch: [121][128/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.1357 (0.1517)	Acc@1 96.094 (94.831)	Acc@5 100.000 (99.921)
Epoch: [121][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.2039 (0.1557)	Acc@1 92.969 (94.695)	Acc@5 100.000 (99.901)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:122/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [122][0/196]	Time 0.117 (0.117)	Data 0.294 (0.294)	Loss 0.0810 (0.0810)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [122][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.1758 (0.1532)	Acc@1 94.531 (94.814)	Acc@5 100.000 (99.946)
Epoch: [122][128/196]	Time 0.102 (0.088)	Data 0.000 (0.002)	Loss 0.1437 (0.1575)	Acc@1 94.531 (94.546)	Acc@5 100.000 (99.930)
Epoch: [122][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.1431 (0.1605)	Acc@1 94.531 (94.485)	Acc@5 100.000 (99.917)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:123/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [123][0/196]	Time 0.114 (0.114)	Data 0.333 (0.333)	Loss 0.1684 (0.1684)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [123][64/196]	Time 0.091 (0.090)	Data 0.000 (0.005)	Loss 0.1421 (0.1522)	Acc@1 94.531 (94.886)	Acc@5 100.000 (99.916)
Epoch: [123][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.1057 (0.1593)	Acc@1 96.094 (94.501)	Acc@5 100.000 (99.927)
Epoch: [123][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.2069 (0.1603)	Acc@1 93.750 (94.505)	Acc@5 99.219 (99.915)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:124/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [124][0/196]	Time 0.138 (0.138)	Data 0.319 (0.319)	Loss 0.1667 (0.1667)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [124][64/196]	Time 0.090 (0.091)	Data 0.000 (0.005)	Loss 0.1103 (0.1588)	Acc@1 96.094 (94.579)	Acc@5 100.000 (99.904)
Epoch: [124][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.1432 (0.1549)	Acc@1 95.312 (94.686)	Acc@5 100.000 (99.906)
Epoch: [124][192/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.1672 (0.1562)	Acc@1 92.578 (94.677)	Acc@5 99.609 (99.901)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:125/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [125][0/196]	Time 0.121 (0.121)	Data 0.334 (0.334)	Loss 0.1740 (0.1740)	Acc@1 94.531 (94.531)	Acc@5 99.609 (99.609)
Epoch: [125][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.1816 (0.1518)	Acc@1 92.969 (94.754)	Acc@5 100.000 (99.898)
Epoch: [125][128/196]	Time 0.094 (0.088)	Data 0.000 (0.003)	Loss 0.2215 (0.1554)	Acc@1 91.797 (94.680)	Acc@5 99.609 (99.918)
Epoch: [125][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.1366 (0.1580)	Acc@1 96.094 (94.525)	Acc@5 100.000 (99.919)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.9
Max memory: 51.4381312
 17.381s  j: 126 bis 130
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7133
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 126
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:126/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [126][0/196]	Time 0.155 (0.155)	Data 0.292 (0.292)	Loss 0.1437 (0.1437)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [126][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.1621 (0.1455)	Acc@1 95.703 (95.174)	Acc@5 100.000 (99.892)
Epoch: [126][128/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.1348 (0.1522)	Acc@1 94.531 (94.919)	Acc@5 100.000 (99.921)
Epoch: [126][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.1686 (0.1550)	Acc@1 94.531 (94.758)	Acc@5 100.000 (99.925)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:127/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [127][0/196]	Time 0.139 (0.139)	Data 0.293 (0.293)	Loss 0.1293 (0.1293)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [127][64/196]	Time 0.087 (0.092)	Data 0.000 (0.005)	Loss 0.1091 (0.1577)	Acc@1 95.703 (94.537)	Acc@5 100.000 (99.898)
Epoch: [127][128/196]	Time 0.092 (0.090)	Data 0.000 (0.002)	Loss 0.1740 (0.1569)	Acc@1 94.531 (94.519)	Acc@5 100.000 (99.918)
Epoch: [127][192/196]	Time 0.090 (0.090)	Data 0.000 (0.002)	Loss 0.1649 (0.1586)	Acc@1 93.359 (94.414)	Acc@5 100.000 (99.919)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:128/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [128][0/196]	Time 0.124 (0.124)	Data 0.320 (0.320)	Loss 0.1564 (0.1564)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [128][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.1675 (0.1596)	Acc@1 94.141 (94.441)	Acc@5 99.609 (99.898)
Epoch: [128][128/196]	Time 0.101 (0.089)	Data 0.000 (0.003)	Loss 0.1692 (0.1571)	Acc@1 94.531 (94.540)	Acc@5 100.000 (99.903)
Epoch: [128][192/196]	Time 0.092 (0.089)	Data 0.000 (0.002)	Loss 0.1381 (0.1612)	Acc@1 95.312 (94.353)	Acc@5 100.000 (99.887)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:129/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [129][0/196]	Time 0.137 (0.137)	Data 0.333 (0.333)	Loss 0.1375 (0.1375)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [129][64/196]	Time 0.088 (0.091)	Data 0.000 (0.005)	Loss 0.2064 (0.1502)	Acc@1 93.750 (94.790)	Acc@5 99.609 (99.916)
Epoch: [129][128/196]	Time 0.080 (0.089)	Data 0.000 (0.003)	Loss 0.1818 (0.1549)	Acc@1 92.969 (94.571)	Acc@5 100.000 (99.924)
Epoch: [129][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.1374 (0.1586)	Acc@1 95.703 (94.493)	Acc@5 100.000 (99.915)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:130/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [130][0/196]	Time 0.124 (0.124)	Data 0.315 (0.315)	Loss 0.1951 (0.1951)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [130][64/196]	Time 0.089 (0.091)	Data 0.000 (0.005)	Loss 0.1599 (0.1554)	Acc@1 94.531 (94.615)	Acc@5 100.000 (99.940)
Epoch: [130][128/196]	Time 0.095 (0.088)	Data 0.000 (0.003)	Loss 0.1821 (0.1563)	Acc@1 93.750 (94.677)	Acc@5 100.000 (99.942)
Epoch: [130][192/196]	Time 0.099 (0.088)	Data 0.000 (0.002)	Loss 0.0822 (0.1583)	Acc@1 98.828 (94.564)	Acc@5 99.609 (99.915)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  87.13
Max memory: 51.4381312
 17.694s  j: 131 bis 135
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6374
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:131/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [131][0/196]	Time 0.118 (0.118)	Data 0.320 (0.320)	Loss 0.0930 (0.0930)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [131][64/196]	Time 0.095 (0.089)	Data 0.000 (0.005)	Loss 0.1566 (0.1435)	Acc@1 94.922 (94.988)	Acc@5 100.000 (99.946)
Epoch: [131][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.1924 (0.1477)	Acc@1 92.188 (94.873)	Acc@5 100.000 (99.949)
Epoch: [131][192/196]	Time 0.078 (0.088)	Data 0.000 (0.002)	Loss 0.2566 (0.1529)	Acc@1 91.016 (94.632)	Acc@5 100.000 (99.945)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:132/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [132][0/196]	Time 0.139 (0.139)	Data 0.265 (0.265)	Loss 0.1213 (0.1213)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [132][64/196]	Time 0.084 (0.088)	Data 0.000 (0.004)	Loss 0.1432 (0.1463)	Acc@1 94.141 (94.880)	Acc@5 100.000 (99.934)
Epoch: [132][128/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 0.2364 (0.1551)	Acc@1 89.844 (94.543)	Acc@5 100.000 (99.933)
Epoch: [132][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.0873 (0.1545)	Acc@1 97.266 (94.667)	Acc@5 100.000 (99.929)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:133/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [133][0/196]	Time 0.104 (0.104)	Data 0.299 (0.299)	Loss 0.1683 (0.1683)	Acc@1 94.141 (94.141)	Acc@5 99.609 (99.609)
Epoch: [133][64/196]	Time 0.081 (0.089)	Data 0.000 (0.005)	Loss 0.1362 (0.1594)	Acc@1 95.312 (94.507)	Acc@5 100.000 (99.928)
Epoch: [133][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.1737 (0.1601)	Acc@1 94.141 (94.525)	Acc@5 100.000 (99.918)
Epoch: [133][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.1542 (0.1598)	Acc@1 95.312 (94.539)	Acc@5 100.000 (99.919)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:134/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [134][0/196]	Time 0.128 (0.128)	Data 0.327 (0.327)	Loss 0.1686 (0.1686)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [134][64/196]	Time 0.090 (0.091)	Data 0.000 (0.005)	Loss 0.2056 (0.1523)	Acc@1 92.969 (94.832)	Acc@5 100.000 (99.904)
Epoch: [134][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.1479 (0.1544)	Acc@1 94.922 (94.668)	Acc@5 100.000 (99.921)
Epoch: [134][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.1798 (0.1594)	Acc@1 93.750 (94.501)	Acc@5 100.000 (99.915)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:135/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [135][0/196]	Time 0.105 (0.105)	Data 0.286 (0.286)	Loss 0.1885 (0.1885)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [135][64/196]	Time 0.085 (0.086)	Data 0.000 (0.005)	Loss 0.1102 (0.1495)	Acc@1 96.094 (94.742)	Acc@5 100.000 (99.922)
Epoch: [135][128/196]	Time 0.087 (0.086)	Data 0.000 (0.002)	Loss 0.1280 (0.1509)	Acc@1 94.922 (94.683)	Acc@5 100.000 (99.933)
Epoch: [135][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.1559 (0.1524)	Acc@1 94.922 (94.624)	Acc@5 99.609 (99.925)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  87.63
Max memory: 51.4381312
 17.343s  j: 136 bis 140
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3711
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:136/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [136][0/196]	Time 0.144 (0.144)	Data 0.293 (0.293)	Loss 0.1383 (0.1383)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [136][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.1220 (0.1466)	Acc@1 94.922 (94.910)	Acc@5 100.000 (99.970)
Epoch: [136][128/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.1979 (0.1516)	Acc@1 93.359 (94.713)	Acc@5 100.000 (99.952)
Epoch: [136][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.1280 (0.1576)	Acc@1 96.094 (94.483)	Acc@5 100.000 (99.945)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:137/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [137][0/196]	Time 0.125 (0.125)	Data 0.300 (0.300)	Loss 0.0969 (0.0969)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [137][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.1990 (0.1566)	Acc@1 94.141 (94.627)	Acc@5 100.000 (99.952)
Epoch: [137][128/196]	Time 0.082 (0.089)	Data 0.000 (0.003)	Loss 0.1602 (0.1558)	Acc@1 94.922 (94.619)	Acc@5 100.000 (99.945)
Epoch: [137][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1269 (0.1556)	Acc@1 96.094 (94.634)	Acc@5 100.000 (99.941)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:138/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [138][0/196]	Time 0.109 (0.109)	Data 0.303 (0.303)	Loss 0.2113 (0.2113)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [138][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.1881 (0.1521)	Acc@1 94.531 (94.688)	Acc@5 99.609 (99.916)
Epoch: [138][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.1088 (0.1555)	Acc@1 96.875 (94.628)	Acc@5 100.000 (99.918)
Epoch: [138][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.1698 (0.1577)	Acc@1 94.531 (94.596)	Acc@5 100.000 (99.921)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:139/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [139][0/196]	Time 0.137 (0.137)	Data 0.294 (0.294)	Loss 0.1244 (0.1244)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [139][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.1545 (0.1542)	Acc@1 94.922 (94.645)	Acc@5 100.000 (99.928)
Epoch: [139][128/196]	Time 0.097 (0.088)	Data 0.000 (0.003)	Loss 0.1583 (0.1546)	Acc@1 94.531 (94.664)	Acc@5 100.000 (99.909)
Epoch: [139][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.1679 (0.1570)	Acc@1 93.359 (94.602)	Acc@5 100.000 (99.913)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:140/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [140][0/196]	Time 0.127 (0.127)	Data 0.329 (0.329)	Loss 0.1656 (0.1656)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [140][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.1654 (0.1472)	Acc@1 95.703 (94.922)	Acc@5 100.000 (99.958)
Epoch: [140][128/196]	Time 0.096 (0.089)	Data 0.000 (0.003)	Loss 0.2149 (0.1527)	Acc@1 90.625 (94.722)	Acc@5 99.609 (99.933)
Epoch: [140][192/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.2082 (0.1567)	Acc@1 93.359 (94.525)	Acc@5 99.609 (99.939)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.15
Max memory: 51.4381312
 17.818s  j: 141 bis 145
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5625
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 141
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:141/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [141][0/196]	Time 0.148 (0.148)	Data 0.290 (0.290)	Loss 0.1722 (0.1722)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [141][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.1818 (0.1453)	Acc@1 92.969 (94.928)	Acc@5 99.609 (99.940)
Epoch: [141][128/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.1339 (0.1531)	Acc@1 96.094 (94.677)	Acc@5 100.000 (99.927)
Epoch: [141][192/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.1248 (0.1573)	Acc@1 94.141 (94.610)	Acc@5 100.000 (99.925)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:142/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [142][0/196]	Time 0.122 (0.122)	Data 0.320 (0.320)	Loss 0.1397 (0.1397)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [142][64/196]	Time 0.082 (0.087)	Data 0.000 (0.005)	Loss 0.1283 (0.1523)	Acc@1 94.922 (94.934)	Acc@5 100.000 (99.928)
Epoch: [142][128/196]	Time 0.084 (0.086)	Data 0.000 (0.003)	Loss 0.1444 (0.1533)	Acc@1 94.531 (94.810)	Acc@5 100.000 (99.942)
Epoch: [142][192/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.1123 (0.1568)	Acc@1 96.094 (94.630)	Acc@5 100.000 (99.933)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:143/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [143][0/196]	Time 0.124 (0.124)	Data 0.300 (0.300)	Loss 0.1386 (0.1386)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [143][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.1343 (0.1524)	Acc@1 95.703 (94.651)	Acc@5 99.609 (99.946)
Epoch: [143][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.1999 (0.1537)	Acc@1 93.359 (94.598)	Acc@5 99.609 (99.933)
Epoch: [143][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.1531 (0.1550)	Acc@1 95.312 (94.566)	Acc@5 100.000 (99.937)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:144/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [144][0/196]	Time 0.128 (0.128)	Data 0.282 (0.282)	Loss 0.1371 (0.1371)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [144][64/196]	Time 0.080 (0.087)	Data 0.000 (0.005)	Loss 0.0832 (0.1546)	Acc@1 98.047 (94.345)	Acc@5 100.000 (99.940)
Epoch: [144][128/196]	Time 0.089 (0.085)	Data 0.000 (0.002)	Loss 0.1246 (0.1500)	Acc@1 95.703 (94.637)	Acc@5 100.000 (99.933)
Epoch: [144][192/196]	Time 0.082 (0.085)	Data 0.000 (0.002)	Loss 0.1533 (0.1547)	Acc@1 94.922 (94.485)	Acc@5 100.000 (99.923)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:145/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [145][0/196]	Time 0.105 (0.105)	Data 0.347 (0.347)	Loss 0.1221 (0.1221)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [145][64/196]	Time 0.085 (0.086)	Data 0.000 (0.006)	Loss 0.1406 (0.1616)	Acc@1 94.922 (94.543)	Acc@5 100.000 (99.898)
Epoch: [145][128/196]	Time 0.080 (0.086)	Data 0.000 (0.003)	Loss 0.1292 (0.1594)	Acc@1 93.750 (94.474)	Acc@5 100.000 (99.933)
Epoch: [145][192/196]	Time 0.083 (0.086)	Data 0.000 (0.002)	Loss 0.2149 (0.1589)	Acc@1 91.016 (94.483)	Acc@5 99.609 (99.929)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.57
Max memory: 51.4381312
 17.239s  j: 146 bis 150
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6487
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 146
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:146/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [146][0/196]	Time 0.146 (0.146)	Data 0.296 (0.296)	Loss 0.1579 (0.1579)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [146][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.1513 (0.1395)	Acc@1 95.703 (95.042)	Acc@5 100.000 (99.952)
Epoch: [146][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.1784 (0.1480)	Acc@1 92.578 (94.707)	Acc@5 100.000 (99.933)
Epoch: [146][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.1997 (0.1547)	Acc@1 93.750 (94.564)	Acc@5 100.000 (99.919)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:147/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [147][0/196]	Time 0.105 (0.105)	Data 0.334 (0.334)	Loss 0.1206 (0.1206)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [147][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.1071 (0.1563)	Acc@1 95.703 (94.429)	Acc@5 100.000 (99.958)
Epoch: [147][128/196]	Time 0.081 (0.088)	Data 0.000 (0.003)	Loss 0.1246 (0.1592)	Acc@1 93.750 (94.407)	Acc@5 100.000 (99.942)
Epoch: [147][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1827 (0.1596)	Acc@1 94.141 (94.422)	Acc@5 100.000 (99.921)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:148/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [148][0/196]	Time 0.125 (0.125)	Data 0.306 (0.306)	Loss 0.1133 (0.1133)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [148][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.1422 (0.1442)	Acc@1 94.531 (95.066)	Acc@5 100.000 (99.940)
Epoch: [148][128/196]	Time 0.085 (0.089)	Data 0.000 (0.003)	Loss 0.1603 (0.1495)	Acc@1 92.578 (94.804)	Acc@5 100.000 (99.915)
Epoch: [148][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.1859 (0.1558)	Acc@1 94.141 (94.598)	Acc@5 100.000 (99.923)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:149/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [149][0/196]	Time 0.124 (0.124)	Data 0.313 (0.313)	Loss 0.1296 (0.1296)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [149][64/196]	Time 0.080 (0.087)	Data 0.000 (0.005)	Loss 0.1477 (0.1530)	Acc@1 93.750 (94.796)	Acc@5 100.000 (99.922)
Epoch: [149][128/196]	Time 0.090 (0.087)	Data 0.000 (0.003)	Loss 0.1770 (0.1538)	Acc@1 93.750 (94.680)	Acc@5 100.000 (99.921)
Epoch: [149][192/196]	Time 0.095 (0.087)	Data 0.000 (0.002)	Loss 0.1499 (0.1586)	Acc@1 94.531 (94.442)	Acc@5 100.000 (99.923)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:150/150; Lr: 0.0010000000000000002
batch Size 256
Epoch: [150][0/196]	Time 0.131 (0.131)	Data 0.303 (0.303)	Loss 0.1279 (0.1279)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [150][64/196]	Time 0.096 (0.089)	Data 0.000 (0.005)	Loss 0.1293 (0.1363)	Acc@1 94.922 (95.367)	Acc@5 100.000 (99.964)
Epoch: [150][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.1043 (0.1302)	Acc@1 96.094 (95.609)	Acc@5 100.000 (99.961)
Epoch: [150][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.1056 (0.1244)	Acc@1 96.875 (95.859)	Acc@5 100.000 (99.962)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.13
Max memory: 51.4381312
 17.600s  j: 151 bis 155
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2852
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 151
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:151/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [151][0/196]	Time 0.147 (0.147)	Data 0.321 (0.321)	Loss 0.1023 (0.1023)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [151][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.1632 (0.1084)	Acc@1 93.359 (96.418)	Acc@5 99.609 (99.952)
Epoch: [151][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0969 (0.1088)	Acc@1 97.656 (96.439)	Acc@5 99.609 (99.958)
Epoch: [151][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1316 (0.1092)	Acc@1 96.094 (96.446)	Acc@5 100.000 (99.955)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:152/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [152][0/196]	Time 0.126 (0.126)	Data 0.304 (0.304)	Loss 0.1773 (0.1773)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [152][64/196]	Time 0.082 (0.091)	Data 0.000 (0.005)	Loss 0.1017 (0.1002)	Acc@1 96.094 (96.641)	Acc@5 100.000 (99.982)
Epoch: [152][128/196]	Time 0.093 (0.089)	Data 0.000 (0.003)	Loss 0.0821 (0.1029)	Acc@1 97.656 (96.596)	Acc@5 100.000 (99.976)
Epoch: [152][192/196]	Time 0.077 (0.088)	Data 0.000 (0.002)	Loss 0.0744 (0.1029)	Acc@1 98.047 (96.622)	Acc@5 100.000 (99.972)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:153/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [153][0/196]	Time 0.118 (0.118)	Data 0.296 (0.296)	Loss 0.1082 (0.1082)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [153][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.1441 (0.0976)	Acc@1 95.312 (96.953)	Acc@5 99.609 (99.952)
Epoch: [153][128/196]	Time 0.079 (0.088)	Data 0.000 (0.003)	Loss 0.1259 (0.0989)	Acc@1 94.922 (96.827)	Acc@5 100.000 (99.958)
Epoch: [153][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1113 (0.1000)	Acc@1 95.312 (96.792)	Acc@5 100.000 (99.957)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:154/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [154][0/196]	Time 0.124 (0.124)	Data 0.315 (0.315)	Loss 0.0907 (0.0907)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [154][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 0.1091 (0.0971)	Acc@1 96.094 (96.851)	Acc@5 100.000 (99.988)
Epoch: [154][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.0737 (0.0976)	Acc@1 97.656 (96.869)	Acc@5 100.000 (99.970)
Epoch: [154][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.1052 (0.0967)	Acc@1 94.922 (96.905)	Acc@5 100.000 (99.966)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:155/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [155][0/196]	Time 0.124 (0.124)	Data 0.344 (0.344)	Loss 0.1216 (0.1216)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [155][64/196]	Time 0.101 (0.088)	Data 0.000 (0.006)	Loss 0.1198 (0.0918)	Acc@1 96.484 (97.145)	Acc@5 100.000 (99.964)
Epoch: [155][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.0939 (0.0960)	Acc@1 96.484 (97.008)	Acc@5 100.000 (99.961)
Epoch: [155][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.1484 (0.0957)	Acc@1 94.141 (96.881)	Acc@5 99.609 (99.962)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.27
Max memory: 51.4381312
 17.578s  j: 156 bis 160
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8670
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 156
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:156/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [156][0/196]	Time 0.125 (0.125)	Data 0.288 (0.288)	Loss 0.1074 (0.1074)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [156][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.1224 (0.0946)	Acc@1 95.703 (96.863)	Acc@5 100.000 (99.988)
Epoch: [156][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0870 (0.0927)	Acc@1 96.875 (96.966)	Acc@5 100.000 (99.988)
Epoch: [156][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0942 (0.0923)	Acc@1 97.266 (97.031)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:157/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [157][0/196]	Time 0.113 (0.113)	Data 0.304 (0.304)	Loss 0.1623 (0.1623)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [157][64/196]	Time 0.088 (0.087)	Data 0.000 (0.005)	Loss 0.0853 (0.0921)	Acc@1 98.828 (97.049)	Acc@5 100.000 (99.994)
Epoch: [157][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.1096 (0.0932)	Acc@1 96.484 (97.045)	Acc@5 100.000 (99.985)
Epoch: [157][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.1093 (0.0933)	Acc@1 96.094 (97.009)	Acc@5 99.609 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:158/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [158][0/196]	Time 0.126 (0.126)	Data 0.291 (0.291)	Loss 0.1650 (0.1650)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [158][64/196]	Time 0.079 (0.088)	Data 0.000 (0.005)	Loss 0.0838 (0.0901)	Acc@1 96.875 (97.145)	Acc@5 100.000 (99.970)
Epoch: [158][128/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.0985 (0.0932)	Acc@1 97.266 (96.978)	Acc@5 100.000 (99.961)
Epoch: [158][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.1041 (0.0931)	Acc@1 95.703 (97.005)	Acc@5 100.000 (99.962)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:159/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [159][0/196]	Time 0.131 (0.131)	Data 0.289 (0.289)	Loss 0.0924 (0.0924)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [159][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.1015 (0.0877)	Acc@1 96.094 (97.404)	Acc@5 100.000 (99.970)
Epoch: [159][128/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.1233 (0.0901)	Acc@1 95.703 (97.244)	Acc@5 99.609 (99.961)
Epoch: [159][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0773 (0.0899)	Acc@1 97.656 (97.175)	Acc@5 100.000 (99.970)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:160/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [160][0/196]	Time 0.124 (0.124)	Data 0.302 (0.302)	Loss 0.0907 (0.0907)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [160][64/196]	Time 0.083 (0.087)	Data 0.000 (0.005)	Loss 0.0704 (0.0881)	Acc@1 98.828 (97.332)	Acc@5 100.000 (99.970)
Epoch: [160][128/196]	Time 0.101 (0.088)	Data 0.000 (0.003)	Loss 0.0716 (0.0870)	Acc@1 97.266 (97.308)	Acc@5 100.000 (99.964)
Epoch: [160][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0885 (0.0887)	Acc@1 96.875 (97.227)	Acc@5 100.000 (99.970)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.2
Max memory: 51.4381312
 17.538s  j: 161 bis 165
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1634
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 161
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:161/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [161][0/196]	Time 0.168 (0.168)	Data 0.292 (0.292)	Loss 0.1085 (0.1085)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [161][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.1349 (0.0868)	Acc@1 95.703 (97.338)	Acc@5 100.000 (99.988)
Epoch: [161][128/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.1011 (0.0881)	Acc@1 96.094 (97.232)	Acc@5 100.000 (99.985)
Epoch: [161][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0752 (0.0887)	Acc@1 97.266 (97.140)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:162/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [162][0/196]	Time 0.128 (0.128)	Data 0.255 (0.255)	Loss 0.0601 (0.0601)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [162][64/196]	Time 0.092 (0.089)	Data 0.000 (0.004)	Loss 0.0922 (0.0897)	Acc@1 97.656 (97.079)	Acc@5 100.000 (99.982)
Epoch: [162][128/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0887 (0.0885)	Acc@1 97.266 (97.190)	Acc@5 100.000 (99.973)
Epoch: [162][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.1132 (0.0886)	Acc@1 96.875 (97.211)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:163/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [163][0/196]	Time 0.118 (0.118)	Data 0.302 (0.302)	Loss 0.1056 (0.1056)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [163][64/196]	Time 0.088 (0.087)	Data 0.000 (0.005)	Loss 0.0848 (0.0873)	Acc@1 98.047 (97.302)	Acc@5 100.000 (99.952)
Epoch: [163][128/196]	Time 0.096 (0.087)	Data 0.000 (0.003)	Loss 0.1058 (0.0877)	Acc@1 96.484 (97.187)	Acc@5 100.000 (99.961)
Epoch: [163][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0777 (0.0867)	Acc@1 98.047 (97.266)	Acc@5 100.000 (99.964)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:164/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [164][0/196]	Time 0.127 (0.127)	Data 0.270 (0.270)	Loss 0.0838 (0.0838)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [164][64/196]	Time 0.083 (0.088)	Data 0.000 (0.004)	Loss 0.0464 (0.0815)	Acc@1 99.219 (97.356)	Acc@5 100.000 (100.000)
Epoch: [164][128/196]	Time 0.095 (0.087)	Data 0.000 (0.002)	Loss 0.0667 (0.0822)	Acc@1 98.438 (97.399)	Acc@5 100.000 (99.985)
Epoch: [164][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.1406 (0.0846)	Acc@1 94.922 (97.278)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:165/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [165][0/196]	Time 0.135 (0.135)	Data 0.299 (0.299)	Loss 0.1114 (0.1114)	Acc@1 96.484 (96.484)	Acc@5 99.609 (99.609)
Epoch: [165][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0984 (0.0859)	Acc@1 96.875 (97.266)	Acc@5 100.000 (99.982)
Epoch: [165][128/196]	Time 0.077 (0.088)	Data 0.000 (0.003)	Loss 0.1131 (0.0868)	Acc@1 96.094 (97.208)	Acc@5 100.000 (99.979)
Epoch: [165][192/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.0789 (0.0871)	Acc@1 97.266 (97.175)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.2
Max memory: 51.4381312
 17.474s  j: 166 bis 170
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2168
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 166
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:166/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [166][0/196]	Time 0.152 (0.152)	Data 0.284 (0.284)	Loss 0.0451 (0.0451)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [166][64/196]	Time 0.082 (0.090)	Data 0.000 (0.005)	Loss 0.0694 (0.0875)	Acc@1 98.438 (97.163)	Acc@5 100.000 (99.964)
Epoch: [166][128/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.0573 (0.0881)	Acc@1 99.219 (97.172)	Acc@5 100.000 (99.961)
Epoch: [166][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0831 (0.0863)	Acc@1 97.656 (97.270)	Acc@5 100.000 (99.966)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:167/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [167][0/196]	Time 0.143 (0.143)	Data 0.312 (0.312)	Loss 0.0951 (0.0951)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [167][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.0954 (0.0865)	Acc@1 96.484 (97.254)	Acc@5 100.000 (99.982)
Epoch: [167][128/196]	Time 0.097 (0.089)	Data 0.000 (0.003)	Loss 0.1065 (0.0868)	Acc@1 95.312 (97.211)	Acc@5 100.000 (99.979)
Epoch: [167][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.0685 (0.0859)	Acc@1 98.047 (97.233)	Acc@5 100.000 (99.972)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:168/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [168][0/196]	Time 0.128 (0.128)	Data 0.312 (0.312)	Loss 0.0732 (0.0732)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [168][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0833 (0.0834)	Acc@1 97.266 (97.314)	Acc@5 100.000 (99.994)
Epoch: [168][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0834 (0.0849)	Acc@1 98.828 (97.305)	Acc@5 100.000 (99.979)
Epoch: [168][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0599 (0.0837)	Acc@1 97.656 (97.326)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:169/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [169][0/196]	Time 0.129 (0.129)	Data 0.294 (0.294)	Loss 0.0760 (0.0760)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [169][64/196]	Time 0.078 (0.088)	Data 0.000 (0.005)	Loss 0.0832 (0.0832)	Acc@1 97.266 (97.356)	Acc@5 100.000 (99.970)
Epoch: [169][128/196]	Time 0.080 (0.088)	Data 0.000 (0.003)	Loss 0.0631 (0.0824)	Acc@1 97.656 (97.393)	Acc@5 100.000 (99.982)
Epoch: [169][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1065 (0.0811)	Acc@1 96.875 (97.434)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:170/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [170][0/196]	Time 0.134 (0.134)	Data 0.320 (0.320)	Loss 0.1319 (0.1319)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [170][64/196]	Time 0.081 (0.089)	Data 0.000 (0.005)	Loss 0.0907 (0.0810)	Acc@1 96.094 (97.350)	Acc@5 100.000 (99.976)
Epoch: [170][128/196]	Time 0.079 (0.089)	Data 0.000 (0.003)	Loss 0.1110 (0.0813)	Acc@1 96.875 (97.378)	Acc@5 100.000 (99.982)
Epoch: [170][192/196]	Time 0.095 (0.088)	Data 0.000 (0.002)	Loss 0.0930 (0.0821)	Acc@1 96.875 (97.351)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.05
Max memory: 51.4381312
 17.675s  j: 171 bis 175
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 408
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 171
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:171/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [171][0/196]	Time 0.148 (0.148)	Data 0.276 (0.276)	Loss 0.0993 (0.0993)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [171][64/196]	Time 0.083 (0.089)	Data 0.000 (0.004)	Loss 0.0858 (0.0780)	Acc@1 97.656 (97.626)	Acc@5 100.000 (100.000)
Epoch: [171][128/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.1029 (0.0809)	Acc@1 98.047 (97.490)	Acc@5 100.000 (99.985)
Epoch: [171][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0762 (0.0811)	Acc@1 98.047 (97.482)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:172/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [172][0/196]	Time 0.124 (0.124)	Data 0.309 (0.309)	Loss 0.0897 (0.0897)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [172][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.0952 (0.0818)	Acc@1 97.266 (97.404)	Acc@5 100.000 (99.982)
Epoch: [172][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.0997 (0.0835)	Acc@1 97.266 (97.341)	Acc@5 100.000 (99.979)
Epoch: [172][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0800 (0.0818)	Acc@1 96.094 (97.387)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:173/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [173][0/196]	Time 0.124 (0.124)	Data 0.282 (0.282)	Loss 0.0484 (0.0484)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [173][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.1067 (0.0797)	Acc@1 97.656 (97.500)	Acc@5 100.000 (99.988)
Epoch: [173][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0731 (0.0818)	Acc@1 97.656 (97.429)	Acc@5 100.000 (99.985)
Epoch: [173][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0834 (0.0820)	Acc@1 97.656 (97.438)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:174/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [174][0/196]	Time 0.120 (0.120)	Data 0.313 (0.313)	Loss 0.0924 (0.0924)	Acc@1 97.266 (97.266)	Acc@5 99.609 (99.609)
Epoch: [174][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.1356 (0.0778)	Acc@1 96.094 (97.512)	Acc@5 100.000 (99.982)
Epoch: [174][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0534 (0.0813)	Acc@1 98.047 (97.366)	Acc@5 100.000 (99.973)
Epoch: [174][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0746 (0.0832)	Acc@1 97.656 (97.298)	Acc@5 99.609 (99.964)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:175/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [175][0/196]	Time 0.115 (0.115)	Data 0.310 (0.310)	Loss 0.0700 (0.0700)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [175][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.1050 (0.0814)	Acc@1 97.266 (97.446)	Acc@5 99.609 (99.976)
Epoch: [175][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0903 (0.0807)	Acc@1 96.875 (97.411)	Acc@5 100.000 (99.976)
Epoch: [175][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0627 (0.0792)	Acc@1 98.047 (97.482)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.1
Max memory: 51.4381312
 17.644s  j: 176 bis 180
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3917
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 176
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:176/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [176][0/196]	Time 0.154 (0.154)	Data 0.310 (0.310)	Loss 0.0780 (0.0780)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [176][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.0541 (0.0754)	Acc@1 98.828 (97.494)	Acc@5 100.000 (99.976)
Epoch: [176][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.0879 (0.0787)	Acc@1 96.875 (97.411)	Acc@5 100.000 (99.979)
Epoch: [176][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0833 (0.0793)	Acc@1 96.875 (97.411)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:177/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [177][0/196]	Time 0.119 (0.119)	Data 0.272 (0.272)	Loss 0.0692 (0.0692)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [177][64/196]	Time 0.089 (0.090)	Data 0.000 (0.004)	Loss 0.0482 (0.0744)	Acc@1 98.828 (97.602)	Acc@5 100.000 (99.976)
Epoch: [177][128/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.0606 (0.0785)	Acc@1 97.656 (97.420)	Acc@5 100.000 (99.979)
Epoch: [177][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0987 (0.0774)	Acc@1 96.094 (97.521)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:178/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [178][0/196]	Time 0.114 (0.114)	Data 0.302 (0.302)	Loss 0.0589 (0.0589)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [178][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0790 (0.0782)	Acc@1 97.656 (97.524)	Acc@5 100.000 (99.982)
Epoch: [178][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.0792 (0.0772)	Acc@1 96.875 (97.553)	Acc@5 100.000 (99.988)
Epoch: [178][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0716 (0.0788)	Acc@1 97.266 (97.460)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:179/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [179][0/196]	Time 0.119 (0.119)	Data 0.304 (0.304)	Loss 0.0497 (0.0497)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [179][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.0715 (0.0729)	Acc@1 97.656 (97.903)	Acc@5 100.000 (99.976)
Epoch: [179][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0801 (0.0756)	Acc@1 98.047 (97.677)	Acc@5 100.000 (99.976)
Epoch: [179][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0896 (0.0766)	Acc@1 97.266 (97.553)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:180/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [180][0/196]	Time 0.132 (0.132)	Data 0.283 (0.283)	Loss 0.0616 (0.0616)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [180][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.1000 (0.0754)	Acc@1 96.094 (97.626)	Acc@5 100.000 (99.982)
Epoch: [180][128/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0562 (0.0765)	Acc@1 97.656 (97.508)	Acc@5 100.000 (99.985)
Epoch: [180][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0807 (0.0770)	Acc@1 97.266 (97.490)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
Stage: 3
width of Layers: [8, 16, 32]
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 32
stage: 3; tobestage: 3
Num: 23
width: 32
stage: 3; tobestage: 3
Num: 24
width: 32
stage: 3; tobestage: 3
Num: 25
width: 32
stage: 3; tobestage: 3
Num: 26
width: 32
stage: 3; tobestage: 3
Num: 27
width: 32
stage: 3; tobestage: 3
Num: 28
width: 32
stage: 3; tobestage: 3
Num: 29
width: 32
stage: 3; tobestage: 3
Num: 30
width: 32
stage: 3; tobestage: 3
Num: 31
width: 32
stage: 3; tobestage: 3
Num: 32
width: 32
stage: 3; tobestage: 3
Num: 33
altList: ['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']
Residual ListI: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
Residual ListO: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
j: 23
Resiudual I
new width: 32; old width: 16
Residual O
old width1: 32; new width: 64
c:[[[ 4.97711170e-03 -6.83591068e-02 -6.98395520e-02]
  [ 3.99371088e-02  1.28835710e-02  8.63808114e-03]
  [-1.97887421e-03 -3.25999036e-02 -6.58759698e-02]]

 [[ 8.45123734e-03 -3.74369696e-02 -3.70972715e-02]
  [-3.10412999e-02 -3.64209488e-02 -3.87550965e-02]
  [ 2.07577292e-02 -4.46465192e-03 -5.40537201e-03]]

 [[-6.35153130e-02 -8.16162303e-02 -4.64408137e-02]
  [-5.51507585e-02 -9.27193165e-02 -9.60411206e-02]
  [-6.80586770e-02 -7.54618049e-02 -6.17111251e-02]]

 [[-1.29953688e-02 -1.71775967e-02 -2.01345608e-02]
  [ 6.94957981e-03  1.44098187e-02  2.42265929e-02]
  [ 3.77417505e-02  8.09821766e-03  2.71940362e-02]]

 [[-5.19782118e-02 -3.59822214e-02 -3.73545550e-02]
  [-6.04988867e-03  4.00940217e-02  3.00194547e-02]
  [-3.14862188e-03  4.11191583e-02  6.79229945e-02]]

 [[ 5.21502364e-03  2.21885052e-02  4.54396047e-02]
  [-1.08526982e-01 -8.07637274e-02 -4.90395017e-02]
  [-7.49039352e-02 -3.83984596e-02 -5.44806533e-02]]

 [[-1.09547926e-02 -2.28625443e-02 -4.88443337e-02]
  [-3.82489450e-02 -1.34506552e-02 -3.79947349e-02]
  [-5.90792447e-02 -4.96783368e-02 -5.72806299e-02]]

 [[-1.70429889e-02 -1.51273189e-02 -5.48498295e-02]
  [ 2.11039241e-02  4.87918966e-02  1.93987079e-02]
  [ 9.27076787e-02  1.50700524e-01  8.59333128e-02]]

 [[-3.80270332e-02 -4.34767976e-02 -5.38202152e-02]
  [ 2.86440868e-02  1.80602055e-02  3.63411196e-03]
  [ 5.67822419e-02  3.98775786e-02  2.32131034e-02]]

 [[-6.84079826e-02 -4.44532596e-02 -5.35069443e-02]
  [-7.99919367e-02 -5.33770621e-02 -7.34186694e-02]
  [-1.29579995e-02 -3.85264345e-02 -4.21149395e-02]]

 [[ 4.32490138e-04 -1.31135026e-03  3.03593948e-02]
  [-1.39575899e-02 -5.89511869e-03  6.90810243e-03]
  [-6.19956516e-02 -1.62879061e-02 -2.41537653e-02]]

 [[ 3.65870781e-02  3.78871430e-03 -1.84305534e-02]
  [-9.85895470e-03  5.73258288e-03 -2.26852838e-02]
  [-4.92373668e-02 -5.65720769e-03 -4.31622565e-02]]

 [[-3.91559303e-02 -6.73006848e-02 -4.28975150e-02]
  [-7.73367658e-02 -8.06983933e-02 -4.13134284e-02]
  [-7.70368055e-02 -6.56608716e-02 -5.36249690e-02]]

 [[ 8.42426792e-02  6.28106892e-02  7.59276748e-02]
  [ 6.21873289e-02  3.00409328e-02  3.78571711e-02]
  [ 9.15165171e-02 -5.38137974e-03  4.78291102e-02]]

 [[-1.25028556e-02  2.16457099e-02 -3.92222253e-04]
  [ 1.85795631e-02  7.63686225e-02  2.76264530e-02]
  [-2.36809980e-02  4.53247176e-03 -2.57012732e-02]]

 [[ 8.56110752e-02  1.95828304e-02  4.00436856e-02]
  [ 1.14013499e-03 -5.63871711e-02 -2.41787322e-02]
  [ 5.13198860e-02  2.03130301e-02 -9.05134622e-03]]

 [[-5.07961027e-02 -3.37432064e-02 -4.06111814e-02]
  [-9.84328054e-03  4.42122705e-02  3.24844792e-02]
  [-6.38677320e-03  4.67547588e-02  6.92672357e-02]]

 [[ 2.54055718e-03 -6.54030070e-02 -6.32222071e-02]
  [ 3.73926349e-02  9.69351642e-03  1.13575291e-02]
  [-6.77900109e-03 -3.45862843e-02 -6.84270635e-02]]

 [[ 6.82900706e-03 -3.41409966e-02 -3.25664394e-02]
  [-3.61417048e-02 -3.77241075e-02 -3.78011651e-02]
  [ 1.89009961e-02  1.26145926e-04 -4.39845119e-03]]

 [[-1.08720325e-02 -1.67172942e-02 -1.95774734e-02]
  [ 9.26794484e-03  8.60244222e-03  2.12526843e-02]
  [ 4.03356329e-02  1.35826115e-02  2.40282640e-02]]

 [[-1.40746087e-02 -2.00171769e-02 -2.02821847e-02]
  [ 1.05555393e-02  1.41586494e-02  2.36253478e-02]
  [ 3.77993509e-02  1.11350631e-02  2.34402586e-02]]

 [[-6.89589530e-02 -4.27795425e-02 -5.18172905e-02]
  [-8.64741802e-02 -5.31980097e-02 -7.03837126e-02]
  [-1.60712302e-02 -4.06465083e-02 -3.68537456e-02]]

 [[-1.74571816e-02 -1.48105863e-02 -4.77752015e-02]
  [ 2.29519811e-02  4.93404195e-02  1.74513198e-02]
  [ 9.48907658e-02  1.45915195e-01  8.34907591e-02]]

 [[-3.53798270e-02 -3.24638858e-02 -5.12653887e-02]
  [ 2.10701451e-02  2.04358529e-02  4.77889692e-03]
  [ 5.83305992e-02  3.95094268e-02  2.52627656e-02]]

 [[ 6.06337283e-03  2.73846462e-03  2.99395230e-02]
  [-1.22751519e-02 -8.87996238e-03  8.46502371e-03]
  [-5.88852391e-02 -1.58427507e-02 -2.60004327e-02]]

 [[ 9.01854262e-02  6.39377832e-02  7.34820515e-02]
  [ 6.30710572e-02  2.89560687e-02  3.92944813e-02]
  [ 8.80236626e-02 -4.07525944e-03  4.11120243e-02]]

 [[ 3.87187898e-02  1.08418462e-03 -1.90661997e-02]
  [-1.27789136e-02  5.78501914e-03 -2.13582590e-02]
  [-5.49674667e-02 -1.11477384e-02 -3.90965864e-02]]

 [[-1.18012605e-02 -2.09016241e-02 -2.00980976e-02]
  [ 4.85807611e-03  1.07905623e-02  2.64840741e-02]
  [ 3.58174741e-02  9.48895607e-03  2.17651967e-02]]

 [[-6.45408332e-02 -3.81189249e-02 -5.55448495e-02]
  [-8.13507587e-02 -5.36881164e-02 -7.15043098e-02]
  [-1.73196271e-02 -3.88754420e-02 -3.90583575e-02]]

 [[ 1.68988258e-02 -3.43424343e-02 -3.81254032e-02]
  [-3.49150449e-02 -3.74839194e-02 -3.73006798e-02]
  [ 2.51145624e-02 -1.62040873e-03 -1.77414354e-03]]

 [[-1.13670602e-02 -3.00722383e-02 -5.24315313e-02]
  [-3.30309346e-02 -1.20305372e-02 -3.21254320e-02]
  [-5.84983714e-02 -4.42845188e-02 -6.09414503e-02]]

 [[-1.62162613e-02 -1.47113455e-02 -5.85894249e-02]
  [ 2.67065447e-02  4.84071225e-02  1.35065876e-02]
  [ 9.33838040e-02  1.49419278e-01  8.50209445e-02]]]
Traceback (most recent call last):
  File "main.py", line 965, in <module>
    main()
  File "main.py", line 558, in main
    model = model.wider(3, 2, out_size=None, weight_norm=None, random_init=False, addNoise=True)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 883, in wider
    f[m] = f[m] / ct.get(listindices[idx])
TypeError: unsupported operand type(s) for /: 'float' and 'NoneType'
j: 181 bis 185
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9412
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 176
Max memory: 0.1097216
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:176/180; Lr: 0.0010000000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [176][0/196]	Time 0.128 (0.128)	Data 0.301 (0.301)	Loss 0.0618 (0.0618)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [176][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.0585 (0.0833)	Acc@1 98.047 (97.344)	Acc@5 100.000 (99.976)
Epoch: [176][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0767 (0.0794)	Acc@1 97.266 (97.472)	Acc@5 100.000 (99.976)
Epoch: [176][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0816 (0.0794)	Acc@1 97.656 (97.454)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:177/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [177][0/196]	Time 0.130 (0.130)	Data 0.322 (0.322)	Loss 0.0848 (0.0848)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [177][64/196]	Time 0.080 (0.088)	Data 0.000 (0.005)	Loss 0.0733 (0.0762)	Acc@1 97.266 (97.536)	Acc@5 100.000 (99.994)
Epoch: [177][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.1009 (0.0773)	Acc@1 98.047 (97.508)	Acc@5 100.000 (99.988)
Epoch: [177][192/196]	Time 0.093 (0.087)	Data 0.000 (0.002)	Loss 0.1044 (0.0806)	Acc@1 95.703 (97.357)	Acc@5 99.609 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:178/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [178][0/196]	Time 0.108 (0.108)	Data 0.310 (0.310)	Loss 0.0750 (0.0750)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [178][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0620 (0.0803)	Acc@1 98.047 (97.500)	Acc@5 100.000 (99.970)
Epoch: [178][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.0630 (0.0774)	Acc@1 98.047 (97.556)	Acc@5 99.609 (99.976)
Epoch: [178][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.1056 (0.0785)	Acc@1 96.875 (97.513)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:179/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [179][0/196]	Time 0.126 (0.126)	Data 0.323 (0.323)	Loss 0.0532 (0.0532)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [179][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.0765 (0.0750)	Acc@1 98.047 (97.560)	Acc@5 100.000 (99.976)
Epoch: [179][128/196]	Time 0.092 (0.088)	Data 0.000 (0.003)	Loss 0.0570 (0.0780)	Acc@1 98.047 (97.462)	Acc@5 100.000 (99.976)
Epoch: [179][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0719 (0.0784)	Acc@1 97.656 (97.492)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:180/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [180][0/196]	Time 0.106 (0.106)	Data 0.316 (0.316)	Loss 0.0847 (0.0847)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [180][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.0596 (0.0756)	Acc@1 97.656 (97.512)	Acc@5 100.000 (99.976)
Epoch: [180][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0955 (0.0757)	Acc@1 96.875 (97.532)	Acc@5 100.000 (99.979)
Epoch: [180][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0840 (0.0755)	Acc@1 97.656 (97.589)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
Model: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
[INFO] Storing checkpoint...
  89.93
Max memory: 51.4381312
 17.615s  j: 186 bis 190
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4936
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 181
Max memory: 0.1097216
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:181/185; Lr: 0.0009000000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [181][0/196]	Time 0.186 (0.186)	Data 0.304 (0.304)	Loss 0.0578 (0.0578)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [181][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.1152 (0.0748)	Acc@1 95.703 (97.626)	Acc@5 100.000 (99.988)
Epoch: [181][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0638 (0.0753)	Acc@1 98.047 (97.617)	Acc@5 100.000 (99.982)
Epoch: [181][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0957 (0.0756)	Acc@1 96.875 (97.650)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:182/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [182][0/196]	Time 0.106 (0.106)	Data 0.263 (0.263)	Loss 0.0834 (0.0834)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [182][64/196]	Time 0.082 (0.086)	Data 0.000 (0.004)	Loss 0.0803 (0.0756)	Acc@1 96.875 (97.614)	Acc@5 100.000 (99.988)
Epoch: [182][128/196]	Time 0.098 (0.087)	Data 0.000 (0.002)	Loss 0.0796 (0.0748)	Acc@1 98.438 (97.635)	Acc@5 100.000 (99.985)
Epoch: [182][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.1290 (0.0767)	Acc@1 94.922 (97.591)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:183/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [183][0/196]	Time 0.120 (0.120)	Data 0.301 (0.301)	Loss 0.0436 (0.0436)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [183][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0673 (0.0729)	Acc@1 99.219 (97.716)	Acc@5 100.000 (99.994)
Epoch: [183][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.0765 (0.0755)	Acc@1 97.266 (97.656)	Acc@5 100.000 (99.985)
Epoch: [183][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0903 (0.0748)	Acc@1 97.656 (97.695)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:184/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [184][0/196]	Time 0.126 (0.126)	Data 0.282 (0.282)	Loss 0.0708 (0.0708)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [184][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.0816 (0.0728)	Acc@1 97.266 (97.572)	Acc@5 100.000 (100.000)
Epoch: [184][128/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0966 (0.0762)	Acc@1 97.266 (97.523)	Acc@5 100.000 (99.997)
Epoch: [184][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0826 (0.0773)	Acc@1 96.875 (97.502)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:185/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [185][0/196]	Time 0.124 (0.124)	Data 0.306 (0.306)	Loss 0.0755 (0.0755)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [185][64/196]	Time 0.097 (0.089)	Data 0.000 (0.005)	Loss 0.0757 (0.0738)	Acc@1 97.266 (97.608)	Acc@5 100.000 (99.988)
Epoch: [185][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0942 (0.0747)	Acc@1 96.875 (97.608)	Acc@5 100.000 (99.979)
Epoch: [185][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0825 (0.0762)	Acc@1 98.047 (97.571)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.18
Max memory: 51.4381312
 17.557s  j: 191 bis 195
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6345
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 186
Max memory: 0.1097216
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:186/190; Lr: 0.0008100000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [186][0/196]	Time 0.142 (0.142)	Data 0.326 (0.326)	Loss 0.0916 (0.0916)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [186][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.0668 (0.0731)	Acc@1 97.656 (97.800)	Acc@5 100.000 (99.982)
Epoch: [186][128/196]	Time 0.097 (0.088)	Data 0.000 (0.003)	Loss 0.0566 (0.0747)	Acc@1 99.219 (97.723)	Acc@5 100.000 (99.979)
Epoch: [186][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0931 (0.0745)	Acc@1 97.266 (97.717)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:187/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [187][0/196]	Time 0.130 (0.130)	Data 0.283 (0.283)	Loss 0.0530 (0.0530)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [187][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0925 (0.0750)	Acc@1 97.266 (97.656)	Acc@5 100.000 (99.982)
Epoch: [187][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1161 (0.0776)	Acc@1 96.484 (97.562)	Acc@5 100.000 (99.982)
Epoch: [187][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0696 (0.0767)	Acc@1 96.875 (97.614)	Acc@5 99.609 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:188/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [188][0/196]	Time 0.126 (0.126)	Data 0.279 (0.279)	Loss 0.0788 (0.0788)	Acc@1 97.266 (97.266)	Acc@5 99.609 (99.609)
Epoch: [188][64/196]	Time 0.091 (0.090)	Data 0.000 (0.004)	Loss 0.0956 (0.0746)	Acc@1 97.266 (97.758)	Acc@5 99.609 (99.946)
Epoch: [188][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0860 (0.0725)	Acc@1 97.656 (97.732)	Acc@5 100.000 (99.967)
Epoch: [188][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0987 (0.0732)	Acc@1 96.094 (97.685)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:189/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [189][0/196]	Time 0.127 (0.127)	Data 0.280 (0.280)	Loss 0.0517 (0.0517)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [189][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.0657 (0.0714)	Acc@1 97.266 (97.650)	Acc@5 100.000 (99.982)
Epoch: [189][128/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0738 (0.0722)	Acc@1 96.484 (97.653)	Acc@5 100.000 (99.982)
Epoch: [189][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0762 (0.0744)	Acc@1 98.047 (97.587)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:190/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [190][0/196]	Time 0.112 (0.112)	Data 0.303 (0.303)	Loss 0.0512 (0.0512)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [190][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0602 (0.0745)	Acc@1 97.656 (97.524)	Acc@5 100.000 (99.970)
Epoch: [190][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0689 (0.0736)	Acc@1 97.656 (97.568)	Acc@5 100.000 (99.982)
Epoch: [190][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.0914 (0.0748)	Acc@1 97.656 (97.585)	Acc@5 99.609 (99.970)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.1
Max memory: 51.4381312
 17.583s  j: 196 bis 200
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6831
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 191
Max memory: 0.1097216
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:191/195; Lr: 0.0007290000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [191][0/196]	Time 0.163 (0.163)	Data 0.271 (0.271)	Loss 0.0745 (0.0745)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [191][64/196]	Time 0.087 (0.088)	Data 0.000 (0.004)	Loss 0.0572 (0.0721)	Acc@1 98.047 (97.602)	Acc@5 100.000 (99.994)
Epoch: [191][128/196]	Time 0.089 (0.086)	Data 0.000 (0.002)	Loss 0.0952 (0.0732)	Acc@1 97.656 (97.596)	Acc@5 100.000 (99.994)
Epoch: [191][192/196]	Time 0.082 (0.086)	Data 0.000 (0.002)	Loss 0.0506 (0.0727)	Acc@1 98.828 (97.664)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:192/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [192][0/196]	Time 0.122 (0.122)	Data 0.312 (0.312)	Loss 0.0811 (0.0811)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [192][64/196]	Time 0.104 (0.089)	Data 0.000 (0.005)	Loss 0.0422 (0.0713)	Acc@1 98.828 (97.674)	Acc@5 100.000 (99.994)
Epoch: [192][128/196]	Time 0.079 (0.088)	Data 0.000 (0.003)	Loss 0.1190 (0.0721)	Acc@1 94.531 (97.611)	Acc@5 100.000 (99.982)
Epoch: [192][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0774 (0.0732)	Acc@1 97.266 (97.587)	Acc@5 99.609 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:193/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [193][0/196]	Time 0.133 (0.133)	Data 0.311 (0.311)	Loss 0.1052 (0.1052)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [193][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0850 (0.0702)	Acc@1 96.875 (97.837)	Acc@5 100.000 (99.982)
Epoch: [193][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.0635 (0.0695)	Acc@1 98.047 (97.865)	Acc@5 100.000 (99.979)
Epoch: [193][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0681 (0.0705)	Acc@1 96.875 (97.838)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:194/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [194][0/196]	Time 0.125 (0.125)	Data 0.294 (0.294)	Loss 0.1006 (0.1006)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [194][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0794 (0.0707)	Acc@1 96.875 (97.740)	Acc@5 100.000 (99.982)
Epoch: [194][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.0520 (0.0712)	Acc@1 98.438 (97.714)	Acc@5 100.000 (99.988)
Epoch: [194][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.1017 (0.0718)	Acc@1 96.484 (97.707)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:195/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [195][0/196]	Time 0.137 (0.137)	Data 0.284 (0.284)	Loss 0.0755 (0.0755)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [195][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0730 (0.0699)	Acc@1 97.266 (97.722)	Acc@5 99.609 (99.982)
Epoch: [195][128/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.0615 (0.0706)	Acc@1 97.266 (97.726)	Acc@5 100.000 (99.979)
Epoch: [195][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0857 (0.0719)	Acc@1 97.656 (97.719)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.99
Max memory: 51.4381312
 17.349s  j: 201 bis 205
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1598
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 196
Max memory: 0.1097216
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:196/200; Lr: 0.0006561000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [196][0/196]	Time 0.166 (0.166)	Data 0.291 (0.291)	Loss 0.0496 (0.0496)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [196][64/196]	Time 0.091 (0.085)	Data 0.000 (0.005)	Loss 0.1178 (0.0708)	Acc@1 95.703 (97.794)	Acc@5 100.000 (99.976)
Epoch: [196][128/196]	Time 0.086 (0.086)	Data 0.000 (0.002)	Loss 0.0734 (0.0717)	Acc@1 96.484 (97.699)	Acc@5 100.000 (99.976)
Epoch: [196][192/196]	Time 0.091 (0.086)	Data 0.000 (0.002)	Loss 0.0825 (0.0717)	Acc@1 98.438 (97.713)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:197/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [197][0/196]	Time 0.132 (0.132)	Data 0.312 (0.312)	Loss 0.0559 (0.0559)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [197][64/196]	Time 0.080 (0.088)	Data 0.000 (0.005)	Loss 0.0662 (0.0708)	Acc@1 98.438 (97.674)	Acc@5 100.000 (100.000)
Epoch: [197][128/196]	Time 0.092 (0.088)	Data 0.000 (0.003)	Loss 0.0496 (0.0702)	Acc@1 98.828 (97.720)	Acc@5 100.000 (99.994)
Epoch: [197][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.0804 (0.0698)	Acc@1 97.656 (97.782)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:198/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [198][0/196]	Time 0.118 (0.118)	Data 0.335 (0.335)	Loss 0.1026 (0.1026)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [198][64/196]	Time 0.080 (0.089)	Data 0.000 (0.005)	Loss 0.0798 (0.0699)	Acc@1 98.047 (97.812)	Acc@5 100.000 (99.988)
Epoch: [198][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0989 (0.0715)	Acc@1 95.703 (97.762)	Acc@5 100.000 (99.979)
Epoch: [198][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0768 (0.0709)	Acc@1 97.266 (97.788)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:199/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [199][0/196]	Time 0.123 (0.123)	Data 0.297 (0.297)	Loss 0.0641 (0.0641)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [199][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0802 (0.0693)	Acc@1 97.656 (97.837)	Acc@5 99.609 (99.988)
Epoch: [199][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.1189 (0.0695)	Acc@1 95.703 (97.802)	Acc@5 100.000 (99.991)
Epoch: [199][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0699 (0.0706)	Acc@1 98.047 (97.766)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:200/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [200][0/196]	Time 0.128 (0.128)	Data 0.293 (0.293)	Loss 0.1194 (0.1194)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [200][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 0.0613 (0.0662)	Acc@1 98.047 (97.855)	Acc@5 100.000 (99.970)
Epoch: [200][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.0576 (0.0671)	Acc@1 98.438 (97.817)	Acc@5 100.000 (99.973)
Epoch: [200][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.1084 (0.0686)	Acc@1 96.484 (97.808)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.2
Max memory: 51.4381312
 17.428s  j: 206 bis 210
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 719
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 201
Max memory: 0.1097216
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:201/205; Lr: 0.0005904900000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [201][0/196]	Time 0.169 (0.169)	Data 0.291 (0.291)	Loss 0.0772 (0.0772)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [201][64/196]	Time 0.084 (0.091)	Data 0.000 (0.005)	Loss 0.0807 (0.0666)	Acc@1 97.656 (97.993)	Acc@5 100.000 (99.988)
Epoch: [201][128/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.0470 (0.0688)	Acc@1 98.438 (97.871)	Acc@5 100.000 (99.985)
Epoch: [201][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0397 (0.0690)	Acc@1 98.828 (97.851)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:202/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [202][0/196]	Time 0.132 (0.132)	Data 0.298 (0.298)	Loss 0.0688 (0.0688)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [202][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.0747 (0.0688)	Acc@1 97.656 (97.939)	Acc@5 100.000 (99.988)
Epoch: [202][128/196]	Time 0.085 (0.089)	Data 0.000 (0.003)	Loss 0.0841 (0.0714)	Acc@1 96.484 (97.732)	Acc@5 100.000 (99.985)
Epoch: [202][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.0506 (0.0698)	Acc@1 98.047 (97.796)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:203/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [203][0/196]	Time 0.117 (0.117)	Data 0.337 (0.337)	Loss 0.0578 (0.0578)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [203][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.0688 (0.0660)	Acc@1 97.266 (97.993)	Acc@5 100.000 (99.976)
Epoch: [203][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.1056 (0.0687)	Acc@1 96.484 (97.811)	Acc@5 100.000 (99.985)
Epoch: [203][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0517 (0.0676)	Acc@1 98.438 (97.889)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:204/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [204][0/196]	Time 0.124 (0.124)	Data 0.319 (0.319)	Loss 0.0452 (0.0452)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [204][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0643 (0.0717)	Acc@1 98.047 (97.674)	Acc@5 100.000 (99.988)
Epoch: [204][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0466 (0.0693)	Acc@1 99.219 (97.847)	Acc@5 100.000 (99.982)
Epoch: [204][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0584 (0.0694)	Acc@1 98.438 (97.820)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:205/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [205][0/196]	Time 0.108 (0.108)	Data 0.310 (0.310)	Loss 0.0987 (0.0987)	Acc@1 97.266 (97.266)	Acc@5 99.609 (99.609)
Epoch: [205][64/196]	Time 0.094 (0.089)	Data 0.000 (0.005)	Loss 0.0889 (0.0683)	Acc@1 97.266 (97.897)	Acc@5 100.000 (99.970)
Epoch: [205][128/196]	Time 0.101 (0.089)	Data 0.000 (0.003)	Loss 0.0860 (0.0681)	Acc@1 96.875 (97.868)	Acc@5 100.000 (99.982)
Epoch: [205][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.0807 (0.0683)	Acc@1 96.875 (97.865)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.34
Max memory: 51.4381312
 17.765s  j: 211 bis 215
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5947
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 206
Max memory: 0.1097216
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:206/210; Lr: 0.0005314410000000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [206][0/196]	Time 0.144 (0.144)	Data 0.319 (0.319)	Loss 0.0612 (0.0612)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [206][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.0899 (0.0668)	Acc@1 96.484 (97.993)	Acc@5 100.000 (99.988)
Epoch: [206][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.0913 (0.0687)	Acc@1 96.875 (97.886)	Acc@5 100.000 (99.985)
Epoch: [206][192/196]	Time 0.079 (0.088)	Data 0.000 (0.002)	Loss 0.0783 (0.0696)	Acc@1 96.875 (97.810)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:207/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [207][0/196]	Time 0.104 (0.104)	Data 0.329 (0.329)	Loss 0.0472 (0.0472)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [207][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 0.0784 (0.0656)	Acc@1 97.266 (97.957)	Acc@5 100.000 (99.970)
Epoch: [207][128/196]	Time 0.102 (0.087)	Data 0.000 (0.003)	Loss 0.0415 (0.0661)	Acc@1 98.438 (97.998)	Acc@5 100.000 (99.976)
Epoch: [207][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0573 (0.0660)	Acc@1 98.047 (97.984)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:208/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [208][0/196]	Time 0.123 (0.123)	Data 0.343 (0.343)	Loss 0.1128 (0.1128)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [208][64/196]	Time 0.081 (0.087)	Data 0.000 (0.005)	Loss 0.0405 (0.0670)	Acc@1 99.219 (97.891)	Acc@5 100.000 (99.988)
Epoch: [208][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0317 (0.0680)	Acc@1 99.609 (97.835)	Acc@5 100.000 (99.991)
Epoch: [208][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.0746 (0.0685)	Acc@1 98.438 (97.840)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:209/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [209][0/196]	Time 0.124 (0.124)	Data 0.338 (0.338)	Loss 0.0783 (0.0783)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [209][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.0547 (0.0660)	Acc@1 98.438 (97.915)	Acc@5 100.000 (99.982)
Epoch: [209][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0613 (0.0650)	Acc@1 98.047 (97.929)	Acc@5 100.000 (99.985)
Epoch: [209][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0883 (0.0662)	Acc@1 98.047 (97.885)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:210/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [210][0/196]	Time 0.119 (0.119)	Data 0.301 (0.301)	Loss 0.0625 (0.0625)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [210][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0762 (0.0661)	Acc@1 97.656 (97.927)	Acc@5 100.000 (99.982)
Epoch: [210][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.0701 (0.0666)	Acc@1 97.656 (97.902)	Acc@5 100.000 (99.991)
Epoch: [210][192/196]	Time 0.094 (0.087)	Data 0.000 (0.002)	Loss 0.0808 (0.0669)	Acc@1 97.266 (97.877)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.21
Max memory: 51.4381312
 17.391s  j: 216 bis 220
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3878
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 211
Max memory: 0.1097216
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:211/215; Lr: 0.0004782969000000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [211][0/196]	Time 0.144 (0.144)	Data 0.299 (0.299)	Loss 0.0549 (0.0549)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [211][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.1427 (0.0658)	Acc@1 94.141 (97.987)	Acc@5 100.000 (99.976)
Epoch: [211][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0904 (0.0674)	Acc@1 96.484 (97.895)	Acc@5 100.000 (99.982)
Epoch: [211][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0633 (0.0675)	Acc@1 97.266 (97.883)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:212/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [212][0/196]	Time 0.124 (0.124)	Data 0.267 (0.267)	Loss 0.0928 (0.0928)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [212][64/196]	Time 0.086 (0.088)	Data 0.000 (0.004)	Loss 0.0853 (0.0632)	Acc@1 97.266 (98.011)	Acc@5 100.000 (100.000)
Epoch: [212][128/196]	Time 0.100 (0.088)	Data 0.000 (0.002)	Loss 0.0660 (0.0647)	Acc@1 98.438 (97.965)	Acc@5 100.000 (99.985)
Epoch: [212][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0704 (0.0644)	Acc@1 98.438 (97.960)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:213/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [213][0/196]	Time 0.128 (0.128)	Data 0.304 (0.304)	Loss 0.0635 (0.0635)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [213][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0910 (0.0646)	Acc@1 97.266 (97.849)	Acc@5 100.000 (99.994)
Epoch: [213][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.0447 (0.0651)	Acc@1 98.828 (97.941)	Acc@5 100.000 (99.991)
Epoch: [213][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0607 (0.0657)	Acc@1 98.047 (97.954)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:214/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [214][0/196]	Time 0.133 (0.133)	Data 0.302 (0.302)	Loss 0.0506 (0.0506)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [214][64/196]	Time 0.093 (0.088)	Data 0.000 (0.005)	Loss 0.0403 (0.0658)	Acc@1 99.219 (97.849)	Acc@5 100.000 (100.000)
Epoch: [214][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.1203 (0.0672)	Acc@1 95.312 (97.835)	Acc@5 100.000 (99.997)
Epoch: [214][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0854 (0.0664)	Acc@1 96.875 (97.828)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:215/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [215][0/196]	Time 0.115 (0.115)	Data 0.366 (0.366)	Loss 0.0435 (0.0435)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [215][64/196]	Time 0.084 (0.088)	Data 0.000 (0.006)	Loss 0.0450 (0.0632)	Acc@1 98.438 (98.029)	Acc@5 100.000 (99.994)
Epoch: [215][128/196]	Time 0.092 (0.087)	Data 0.000 (0.003)	Loss 0.0649 (0.0646)	Acc@1 98.047 (98.008)	Acc@5 100.000 (99.997)
Epoch: [215][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.0780 (0.0647)	Acc@1 97.266 (98.023)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.03
Max memory: 51.4381312
 17.492s  j: 221 bis 225
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2221
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 216
Max memory: 0.1097216
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:216/220; Lr: 0.0004304672100000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [216][0/196]	Time 0.149 (0.149)	Data 0.338 (0.338)	Loss 0.0490 (0.0490)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [216][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.0748 (0.0651)	Acc@1 98.047 (97.963)	Acc@5 100.000 (99.982)
Epoch: [216][128/196]	Time 0.094 (0.088)	Data 0.000 (0.003)	Loss 0.0528 (0.0644)	Acc@1 99.609 (98.047)	Acc@5 100.000 (99.991)
Epoch: [216][192/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 0.0490 (0.0650)	Acc@1 99.219 (98.023)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:217/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [217][0/196]	Time 0.107 (0.107)	Data 0.305 (0.305)	Loss 0.0417 (0.0417)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [217][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.0426 (0.0626)	Acc@1 99.219 (98.095)	Acc@5 100.000 (99.994)
Epoch: [217][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0813 (0.0643)	Acc@1 98.828 (97.965)	Acc@5 100.000 (99.994)
Epoch: [217][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.1058 (0.0651)	Acc@1 95.703 (97.946)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:218/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [218][0/196]	Time 0.138 (0.138)	Data 0.304 (0.304)	Loss 0.0420 (0.0420)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [218][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.0493 (0.0653)	Acc@1 98.438 (98.005)	Acc@5 100.000 (99.982)
Epoch: [218][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.0471 (0.0647)	Acc@1 98.828 (97.998)	Acc@5 100.000 (99.979)
Epoch: [218][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0579 (0.0653)	Acc@1 98.438 (97.964)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:219/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [219][0/196]	Time 0.124 (0.124)	Data 0.318 (0.318)	Loss 0.0808 (0.0808)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [219][64/196]	Time 0.092 (0.088)	Data 0.000 (0.005)	Loss 0.0510 (0.0637)	Acc@1 98.047 (98.059)	Acc@5 100.000 (99.982)
Epoch: [219][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0582 (0.0653)	Acc@1 98.438 (97.923)	Acc@5 100.000 (99.988)
Epoch: [219][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0927 (0.0658)	Acc@1 96.875 (97.889)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:220/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [220][0/196]	Time 0.111 (0.111)	Data 0.262 (0.262)	Loss 0.0489 (0.0489)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [220][64/196]	Time 0.097 (0.087)	Data 0.000 (0.004)	Loss 0.0460 (0.0644)	Acc@1 98.828 (97.969)	Acc@5 100.000 (99.994)
Epoch: [220][128/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.0461 (0.0635)	Acc@1 98.828 (98.038)	Acc@5 100.000 (99.991)
Epoch: [220][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0336 (0.0643)	Acc@1 99.219 (98.021)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.08
Max memory: 51.4381312
 17.432s  j: 226 bis 230
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 666
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 221
Max memory: 0.1097216
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:221/225; Lr: 0.0003874204890000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [221][0/196]	Time 0.161 (0.161)	Data 0.294 (0.294)	Loss 0.0652 (0.0652)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [221][64/196]	Time 0.098 (0.090)	Data 0.000 (0.005)	Loss 0.0635 (0.0649)	Acc@1 98.047 (97.867)	Acc@5 100.000 (99.988)
Epoch: [221][128/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0519 (0.0638)	Acc@1 98.047 (97.929)	Acc@5 100.000 (99.991)
Epoch: [221][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.0735 (0.0637)	Acc@1 98.047 (97.982)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:222/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [222][0/196]	Time 0.130 (0.130)	Data 0.302 (0.302)	Loss 0.0678 (0.0678)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [222][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.0652 (0.0653)	Acc@1 98.047 (97.855)	Acc@5 100.000 (99.988)
Epoch: [222][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.0749 (0.0660)	Acc@1 97.266 (97.838)	Acc@5 100.000 (99.988)
Epoch: [222][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0591 (0.0657)	Acc@1 98.438 (97.897)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:223/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [223][0/196]	Time 0.124 (0.124)	Data 0.302 (0.302)	Loss 0.0552 (0.0552)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [223][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0763 (0.0647)	Acc@1 97.656 (97.843)	Acc@5 100.000 (99.988)
Epoch: [223][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0517 (0.0645)	Acc@1 98.438 (97.920)	Acc@5 100.000 (99.985)
Epoch: [223][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0270 (0.0645)	Acc@1 100.000 (97.911)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:224/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [224][0/196]	Time 0.120 (0.120)	Data 0.330 (0.330)	Loss 0.0809 (0.0809)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [224][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0587 (0.0623)	Acc@1 97.656 (98.059)	Acc@5 100.000 (99.988)
Epoch: [224][128/196]	Time 0.095 (0.088)	Data 0.000 (0.003)	Loss 0.0855 (0.0648)	Acc@1 96.875 (97.977)	Acc@5 100.000 (99.988)
Epoch: [224][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0521 (0.0648)	Acc@1 97.656 (97.970)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:225/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [225][0/196]	Time 0.129 (0.129)	Data 0.300 (0.300)	Loss 0.0367 (0.0367)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [225][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.0314 (0.0611)	Acc@1 99.609 (98.107)	Acc@5 100.000 (100.000)
Epoch: [225][128/196]	Time 0.088 (0.089)	Data 0.000 (0.003)	Loss 0.0431 (0.0630)	Acc@1 98.438 (98.041)	Acc@5 100.000 (99.994)
Epoch: [225][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0358 (0.0622)	Acc@1 99.219 (98.063)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.11
Max memory: 51.4381312
 17.706s  j: 231 bis 235
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4796
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 226
Max memory: 0.1097216
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:226/230; Lr: 0.00034867844010000006
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [226][0/196]	Time 0.148 (0.148)	Data 0.352 (0.352)	Loss 0.0497 (0.0497)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [226][64/196]	Time 0.087 (0.089)	Data 0.000 (0.006)	Loss 0.0799 (0.0626)	Acc@1 97.656 (98.119)	Acc@5 100.000 (99.970)
Epoch: [226][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0450 (0.0638)	Acc@1 98.828 (98.023)	Acc@5 100.000 (99.985)
Epoch: [226][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0422 (0.0647)	Acc@1 98.438 (97.984)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:227/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [227][0/196]	Time 0.115 (0.115)	Data 0.303 (0.303)	Loss 0.0619 (0.0619)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [227][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.0490 (0.0636)	Acc@1 98.438 (98.041)	Acc@5 100.000 (99.994)
Epoch: [227][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.0519 (0.0634)	Acc@1 98.438 (98.047)	Acc@5 100.000 (99.997)
Epoch: [227][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0945 (0.0632)	Acc@1 97.656 (98.059)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:228/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [228][0/196]	Time 0.110 (0.110)	Data 0.330 (0.330)	Loss 0.0811 (0.0811)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [228][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0577 (0.0645)	Acc@1 98.047 (97.921)	Acc@5 100.000 (99.994)
Epoch: [228][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.1181 (0.0653)	Acc@1 96.484 (97.941)	Acc@5 100.000 (99.991)
Epoch: [228][192/196]	Time 0.078 (0.087)	Data 0.000 (0.002)	Loss 0.0797 (0.0646)	Acc@1 97.266 (97.982)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:229/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [229][0/196]	Time 0.147 (0.147)	Data 0.287 (0.287)	Loss 0.0656 (0.0656)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [229][64/196]	Time 0.090 (0.089)	Data 0.000 (0.005)	Loss 0.0577 (0.0617)	Acc@1 98.438 (97.999)	Acc@5 100.000 (99.976)
Epoch: [229][128/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0501 (0.0619)	Acc@1 98.438 (98.035)	Acc@5 100.000 (99.985)
Epoch: [229][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0556 (0.0618)	Acc@1 98.047 (98.053)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:230/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [230][0/196]	Time 0.125 (0.125)	Data 0.316 (0.316)	Loss 0.0691 (0.0691)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [230][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0370 (0.0619)	Acc@1 98.828 (98.143)	Acc@5 100.000 (99.994)
Epoch: [230][128/196]	Time 0.078 (0.087)	Data 0.000 (0.003)	Loss 0.0436 (0.0624)	Acc@1 98.828 (98.077)	Acc@5 100.000 (99.988)
Epoch: [230][192/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.0618 (0.0620)	Acc@1 98.438 (98.079)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.07
Max memory: 51.4381312
 17.480s  j: 236 bis 240
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5891
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 231
Max memory: 0.1097216
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:231/235; Lr: 0.00031381059609000004
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [231][0/196]	Time 0.157 (0.157)	Data 0.270 (0.270)	Loss 0.0718 (0.0718)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [231][64/196]	Time 0.083 (0.088)	Data 0.000 (0.004)	Loss 0.0635 (0.0620)	Acc@1 97.656 (98.029)	Acc@5 100.000 (99.988)
Epoch: [231][128/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 0.0919 (0.0628)	Acc@1 96.484 (98.017)	Acc@5 100.000 (99.991)
Epoch: [231][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0418 (0.0625)	Acc@1 98.438 (98.043)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:232/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [232][0/196]	Time 0.142 (0.142)	Data 0.336 (0.336)	Loss 0.0743 (0.0743)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [232][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0791 (0.0626)	Acc@1 97.266 (98.089)	Acc@5 100.000 (99.994)
Epoch: [232][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.0583 (0.0618)	Acc@1 99.219 (98.153)	Acc@5 100.000 (99.988)
Epoch: [232][192/196]	Time 0.094 (0.088)	Data 0.000 (0.002)	Loss 0.0360 (0.0620)	Acc@1 99.219 (98.124)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:233/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [233][0/196]	Time 0.144 (0.144)	Data 0.308 (0.308)	Loss 0.0441 (0.0441)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [233][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.0700 (0.0623)	Acc@1 97.656 (98.107)	Acc@5 100.000 (99.994)
Epoch: [233][128/196]	Time 0.079 (0.088)	Data 0.000 (0.003)	Loss 0.0470 (0.0635)	Acc@1 99.219 (97.998)	Acc@5 100.000 (99.988)
Epoch: [233][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0542 (0.0630)	Acc@1 98.047 (98.019)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:234/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [234][0/196]	Time 0.139 (0.139)	Data 0.315 (0.315)	Loss 0.0438 (0.0438)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [234][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.0872 (0.0613)	Acc@1 97.266 (98.113)	Acc@5 100.000 (99.988)
Epoch: [234][128/196]	Time 0.101 (0.089)	Data 0.000 (0.003)	Loss 0.0484 (0.0612)	Acc@1 98.828 (98.080)	Acc@5 100.000 (99.991)
Epoch: [234][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0436 (0.0623)	Acc@1 98.828 (98.057)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:235/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [235][0/196]	Time 0.138 (0.138)	Data 0.259 (0.259)	Loss 0.0976 (0.0976)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [235][64/196]	Time 0.093 (0.089)	Data 0.000 (0.004)	Loss 0.1008 (0.0604)	Acc@1 96.484 (98.107)	Acc@5 100.000 (99.994)
Epoch: [235][128/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0822 (0.0629)	Acc@1 96.875 (98.023)	Acc@5 100.000 (99.982)
Epoch: [235][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0651 (0.0634)	Acc@1 98.047 (97.968)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.19
Max memory: 51.4381312
 17.564s  j: 241 bis 245
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4030
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 236
Max memory: 0.1097216
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:236/240; Lr: 0.00028242953648100003
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [236][0/196]	Time 0.181 (0.181)	Data 0.285 (0.285)	Loss 0.0384 (0.0384)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [236][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.0504 (0.0613)	Acc@1 98.438 (98.185)	Acc@5 100.000 (99.970)
Epoch: [236][128/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.0607 (0.0621)	Acc@1 98.438 (98.107)	Acc@5 100.000 (99.979)
Epoch: [236][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0505 (0.0631)	Acc@1 99.219 (98.079)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:237/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [237][0/196]	Time 0.119 (0.119)	Data 0.349 (0.349)	Loss 0.0667 (0.0667)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [237][64/196]	Time 0.086 (0.088)	Data 0.000 (0.006)	Loss 0.0912 (0.0584)	Acc@1 97.266 (98.287)	Acc@5 100.000 (99.994)
Epoch: [237][128/196]	Time 0.094 (0.088)	Data 0.000 (0.003)	Loss 0.0761 (0.0582)	Acc@1 98.047 (98.247)	Acc@5 100.000 (99.994)
Epoch: [237][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0649 (0.0599)	Acc@1 97.656 (98.166)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:238/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [238][0/196]	Time 0.132 (0.132)	Data 0.316 (0.316)	Loss 0.0383 (0.0383)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [238][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.0577 (0.0635)	Acc@1 98.438 (97.957)	Acc@5 100.000 (99.994)
Epoch: [238][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0621 (0.0611)	Acc@1 98.047 (98.053)	Acc@5 100.000 (99.991)
Epoch: [238][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0885 (0.0615)	Acc@1 96.484 (98.051)	Acc@5 99.609 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:239/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [239][0/196]	Time 0.116 (0.116)	Data 0.273 (0.273)	Loss 0.0454 (0.0454)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [239][64/196]	Time 0.086 (0.089)	Data 0.000 (0.004)	Loss 0.0827 (0.0632)	Acc@1 97.656 (98.101)	Acc@5 100.000 (99.994)
Epoch: [239][128/196]	Time 0.100 (0.088)	Data 0.000 (0.002)	Loss 0.0488 (0.0617)	Acc@1 98.828 (98.126)	Acc@5 100.000 (99.982)
Epoch: [239][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0510 (0.0609)	Acc@1 98.438 (98.162)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:240/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [240][0/196]	Time 0.132 (0.132)	Data 0.278 (0.278)	Loss 0.0700 (0.0700)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [240][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0657 (0.0595)	Acc@1 97.656 (98.197)	Acc@5 100.000 (99.994)
Epoch: [240][128/196]	Time 0.098 (0.088)	Data 0.000 (0.002)	Loss 0.0384 (0.0590)	Acc@1 99.219 (98.186)	Acc@5 100.000 (99.997)
Epoch: [240][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0413 (0.0599)	Acc@1 99.609 (98.150)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.3
Max memory: 51.4381312
 17.646s  j: 246 bis 250
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7405
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 241
Max memory: 0.1097216
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:241/245; Lr: 0.00025418658283290005
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [241][0/196]	Time 0.135 (0.135)	Data 0.341 (0.341)	Loss 0.0858 (0.0858)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [241][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.0617 (0.0628)	Acc@1 97.656 (98.041)	Acc@5 100.000 (99.982)
Epoch: [241][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.0539 (0.0621)	Acc@1 98.047 (98.068)	Acc@5 100.000 (99.988)
Epoch: [241][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0777 (0.0627)	Acc@1 97.266 (98.067)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:242/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [242][0/196]	Time 0.112 (0.112)	Data 0.323 (0.323)	Loss 0.0978 (0.0978)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [242][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0267 (0.0612)	Acc@1 99.219 (98.071)	Acc@5 100.000 (99.988)
Epoch: [242][128/196]	Time 0.099 (0.088)	Data 0.000 (0.003)	Loss 0.0664 (0.0610)	Acc@1 98.047 (98.101)	Acc@5 100.000 (99.991)
Epoch: [242][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0577 (0.0616)	Acc@1 98.047 (98.089)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:243/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [243][0/196]	Time 0.145 (0.145)	Data 0.274 (0.274)	Loss 0.0619 (0.0619)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [243][64/196]	Time 0.091 (0.089)	Data 0.000 (0.004)	Loss 0.0718 (0.0586)	Acc@1 98.438 (98.317)	Acc@5 100.000 (99.982)
Epoch: [243][128/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0660 (0.0606)	Acc@1 98.047 (98.204)	Acc@5 100.000 (99.991)
Epoch: [243][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0643 (0.0611)	Acc@1 97.656 (98.166)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:244/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [244][0/196]	Time 0.123 (0.123)	Data 0.296 (0.296)	Loss 0.0595 (0.0595)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [244][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0779 (0.0617)	Acc@1 97.656 (98.113)	Acc@5 100.000 (99.988)
Epoch: [244][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0480 (0.0604)	Acc@1 99.609 (98.177)	Acc@5 100.000 (99.988)
Epoch: [244][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0407 (0.0610)	Acc@1 98.438 (98.168)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:245/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [245][0/196]	Time 0.139 (0.139)	Data 0.277 (0.277)	Loss 0.0418 (0.0418)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [245][64/196]	Time 0.089 (0.089)	Data 0.000 (0.004)	Loss 0.0604 (0.0616)	Acc@1 98.047 (98.071)	Acc@5 100.000 (99.976)
Epoch: [245][128/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0570 (0.0587)	Acc@1 97.656 (98.204)	Acc@5 100.000 (99.982)
Epoch: [245][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0679 (0.0595)	Acc@1 96.875 (98.176)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.25
Max memory: 51.4381312
 17.688s  j: 251 bis 255
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5351
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 246
Max memory: 0.1097216
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:246/250; Lr: 0.00022876792454961005
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [246][0/196]	Time 0.161 (0.161)	Data 0.309 (0.309)	Loss 0.0628 (0.0628)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [246][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.0851 (0.0594)	Acc@1 97.266 (98.149)	Acc@5 100.000 (99.988)
Epoch: [246][128/196]	Time 0.099 (0.088)	Data 0.000 (0.003)	Loss 0.0600 (0.0606)	Acc@1 98.438 (98.195)	Acc@5 100.000 (99.991)
Epoch: [246][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0669 (0.0617)	Acc@1 98.047 (98.091)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:247/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [247][0/196]	Time 0.122 (0.122)	Data 0.288 (0.288)	Loss 0.0511 (0.0511)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [247][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.0695 (0.0599)	Acc@1 97.656 (98.077)	Acc@5 100.000 (99.994)
Epoch: [247][128/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0478 (0.0608)	Acc@1 98.438 (98.098)	Acc@5 100.000 (99.994)
Epoch: [247][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0533 (0.0605)	Acc@1 98.828 (98.148)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:248/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [248][0/196]	Time 0.130 (0.130)	Data 0.282 (0.282)	Loss 0.0519 (0.0519)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [248][64/196]	Time 0.085 (0.086)	Data 0.000 (0.005)	Loss 0.0592 (0.0615)	Acc@1 98.828 (98.149)	Acc@5 100.000 (99.994)
Epoch: [248][128/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0805 (0.0599)	Acc@1 97.656 (98.165)	Acc@5 100.000 (99.991)
Epoch: [248][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0338 (0.0611)	Acc@1 99.609 (98.124)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:249/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [249][0/196]	Time 0.120 (0.120)	Data 0.312 (0.312)	Loss 0.0307 (0.0307)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [249][64/196]	Time 0.096 (0.088)	Data 0.000 (0.005)	Loss 0.0865 (0.0577)	Acc@1 97.266 (98.191)	Acc@5 99.609 (99.982)
Epoch: [249][128/196]	Time 0.100 (0.088)	Data 0.000 (0.003)	Loss 0.0556 (0.0587)	Acc@1 97.266 (98.159)	Acc@5 100.000 (99.982)
Epoch: [249][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0554 (0.0596)	Acc@1 98.438 (98.118)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:250/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [250][0/196]	Time 0.131 (0.131)	Data 0.318 (0.318)	Loss 0.0440 (0.0440)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [250][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.0590 (0.0603)	Acc@1 98.828 (98.233)	Acc@5 99.609 (99.982)
Epoch: [250][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0356 (0.0613)	Acc@1 99.219 (98.129)	Acc@5 100.000 (99.976)
Epoch: [250][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0472 (0.0615)	Acc@1 98.438 (98.138)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.25
Max memory: 51.4381312
 17.635s  j: 256 bis 260
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9487
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 251
Max memory: 0.1097216
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:251/255; Lr: 0.00020589113209464906
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [251][0/196]	Time 0.169 (0.169)	Data 0.298 (0.298)	Loss 0.0429 (0.0429)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [251][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0476 (0.0602)	Acc@1 98.438 (98.059)	Acc@5 100.000 (99.994)
Epoch: [251][128/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0735 (0.0588)	Acc@1 98.438 (98.204)	Acc@5 100.000 (99.994)
Epoch: [251][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0823 (0.0600)	Acc@1 96.875 (98.158)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:252/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [252][0/196]	Time 0.122 (0.122)	Data 0.324 (0.324)	Loss 0.0646 (0.0646)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [252][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0554 (0.0602)	Acc@1 98.438 (98.143)	Acc@5 100.000 (100.000)
Epoch: [252][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.0256 (0.0602)	Acc@1 99.609 (98.147)	Acc@5 100.000 (99.994)
Epoch: [252][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0496 (0.0614)	Acc@1 98.438 (98.083)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:253/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [253][0/196]	Time 0.124 (0.124)	Data 0.302 (0.302)	Loss 0.0466 (0.0466)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [253][64/196]	Time 0.097 (0.089)	Data 0.000 (0.005)	Loss 0.0541 (0.0576)	Acc@1 98.828 (98.263)	Acc@5 100.000 (99.976)
Epoch: [253][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.0639 (0.0575)	Acc@1 98.047 (98.219)	Acc@5 100.000 (99.988)
Epoch: [253][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1185 (0.0593)	Acc@1 96.484 (98.154)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:254/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [254][0/196]	Time 0.121 (0.121)	Data 0.282 (0.282)	Loss 0.0713 (0.0713)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [254][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0808 (0.0633)	Acc@1 97.656 (97.933)	Acc@5 100.000 (99.994)
Epoch: [254][128/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0461 (0.0625)	Acc@1 98.438 (97.977)	Acc@5 100.000 (99.991)
Epoch: [254][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0589 (0.0614)	Acc@1 98.047 (98.069)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:255/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [255][0/196]	Time 0.125 (0.125)	Data 0.307 (0.307)	Loss 0.0431 (0.0431)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [255][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0437 (0.0593)	Acc@1 98.828 (98.239)	Acc@5 100.000 (99.964)
Epoch: [255][128/196]	Time 0.085 (0.089)	Data 0.000 (0.003)	Loss 0.0612 (0.0594)	Acc@1 98.438 (98.223)	Acc@5 100.000 (99.979)
Epoch: [255][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.0607 (0.0598)	Acc@1 98.047 (98.191)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.26
Max memory: 51.4381312
 17.692s  j: 261 bis 265
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2061
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 256
Max memory: 0.1097216
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:256/260; Lr: 0.00018530201888518417
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [256][0/196]	Time 0.142 (0.142)	Data 0.324 (0.324)	Loss 0.0639 (0.0639)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [256][64/196]	Time 0.099 (0.088)	Data 0.000 (0.005)	Loss 0.0488 (0.0562)	Acc@1 98.828 (98.275)	Acc@5 100.000 (100.000)
Epoch: [256][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.0624 (0.0597)	Acc@1 97.656 (98.086)	Acc@5 100.000 (99.988)
Epoch: [256][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0545 (0.0609)	Acc@1 99.219 (98.039)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:257/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [257][0/196]	Time 0.129 (0.129)	Data 0.307 (0.307)	Loss 0.0341 (0.0341)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [257][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0554 (0.0624)	Acc@1 99.219 (97.999)	Acc@5 100.000 (99.982)
Epoch: [257][128/196]	Time 0.092 (0.087)	Data 0.000 (0.003)	Loss 0.0718 (0.0613)	Acc@1 97.656 (98.026)	Acc@5 100.000 (99.985)
Epoch: [257][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0636 (0.0617)	Acc@1 96.875 (98.043)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:258/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [258][0/196]	Time 0.127 (0.127)	Data 0.268 (0.268)	Loss 0.0554 (0.0554)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [258][64/196]	Time 0.088 (0.088)	Data 0.000 (0.004)	Loss 0.0856 (0.0610)	Acc@1 96.875 (98.089)	Acc@5 100.000 (99.988)
Epoch: [258][128/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0795 (0.0614)	Acc@1 96.484 (98.041)	Acc@5 100.000 (99.991)
Epoch: [258][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.0578 (0.0605)	Acc@1 98.047 (98.110)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:259/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [259][0/196]	Time 0.135 (0.135)	Data 0.285 (0.285)	Loss 0.0328 (0.0328)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [259][64/196]	Time 0.097 (0.089)	Data 0.000 (0.005)	Loss 0.0393 (0.0570)	Acc@1 98.438 (98.311)	Acc@5 100.000 (99.982)
Epoch: [259][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0595 (0.0585)	Acc@1 98.047 (98.219)	Acc@5 100.000 (99.985)
Epoch: [259][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0536 (0.0600)	Acc@1 98.047 (98.158)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:260/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [260][0/196]	Time 0.118 (0.118)	Data 0.327 (0.327)	Loss 0.0417 (0.0417)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [260][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0670 (0.0581)	Acc@1 98.047 (98.239)	Acc@5 100.000 (99.988)
Epoch: [260][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.1096 (0.0600)	Acc@1 95.703 (98.198)	Acc@5 100.000 (99.985)
Epoch: [260][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0608 (0.0598)	Acc@1 98.438 (98.231)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.09
Max memory: 51.4381312
 17.524s  j: 266 bis 270
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6722
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 261
Max memory: 0.1097216
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:261/265; Lr: 0.00016677181699666576
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [261][0/196]	Time 0.162 (0.162)	Data 0.303 (0.303)	Loss 0.0686 (0.0686)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [261][64/196]	Time 0.088 (0.090)	Data 0.000 (0.005)	Loss 0.0308 (0.0594)	Acc@1 99.609 (98.197)	Acc@5 100.000 (99.994)
Epoch: [261][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.0615 (0.0589)	Acc@1 98.047 (98.241)	Acc@5 100.000 (99.994)
Epoch: [261][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0856 (0.0590)	Acc@1 97.266 (98.227)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:262/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [262][0/196]	Time 0.142 (0.142)	Data 0.279 (0.279)	Loss 0.0455 (0.0455)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [262][64/196]	Time 0.092 (0.087)	Data 0.000 (0.005)	Loss 0.0441 (0.0570)	Acc@1 98.828 (98.167)	Acc@5 100.000 (99.994)
Epoch: [262][128/196]	Time 0.100 (0.088)	Data 0.000 (0.002)	Loss 0.0425 (0.0578)	Acc@1 98.438 (98.189)	Acc@5 100.000 (99.988)
Epoch: [262][192/196]	Time 0.094 (0.087)	Data 0.000 (0.002)	Loss 0.0618 (0.0579)	Acc@1 97.266 (98.197)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:263/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [263][0/196]	Time 0.127 (0.127)	Data 0.292 (0.292)	Loss 0.0425 (0.0425)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [263][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.0717 (0.0569)	Acc@1 97.656 (98.287)	Acc@5 100.000 (100.000)
Epoch: [263][128/196]	Time 0.078 (0.087)	Data 0.000 (0.002)	Loss 0.0708 (0.0587)	Acc@1 98.047 (98.232)	Acc@5 100.000 (99.994)
Epoch: [263][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.0720 (0.0599)	Acc@1 98.047 (98.170)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:264/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [264][0/196]	Time 0.125 (0.125)	Data 0.305 (0.305)	Loss 0.0657 (0.0657)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [264][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0698 (0.0601)	Acc@1 97.656 (98.197)	Acc@5 100.000 (99.976)
Epoch: [264][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0420 (0.0619)	Acc@1 99.609 (98.123)	Acc@5 100.000 (99.985)
Epoch: [264][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0404 (0.0612)	Acc@1 98.828 (98.112)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:265/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [265][0/196]	Time 0.104 (0.104)	Data 0.321 (0.321)	Loss 0.0666 (0.0666)	Acc@1 96.875 (96.875)	Acc@5 99.609 (99.609)
Epoch: [265][64/196]	Time 0.089 (0.087)	Data 0.000 (0.005)	Loss 0.0418 (0.0612)	Acc@1 99.219 (98.053)	Acc@5 100.000 (99.994)
Epoch: [265][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.0461 (0.0602)	Acc@1 99.609 (98.156)	Acc@5 100.000 (99.988)
Epoch: [265][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.1073 (0.0598)	Acc@1 96.094 (98.172)	Acc@5 99.609 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.17
Max memory: 51.4381312
 17.459s  j: 271 bis 275
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2646
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 266
Max memory: 0.1097216
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:266/270; Lr: 0.0001500946352969992
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [266][0/196]	Time 0.142 (0.142)	Data 0.293 (0.293)	Loss 0.0382 (0.0382)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [266][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.0398 (0.0584)	Acc@1 99.609 (98.191)	Acc@5 100.000 (99.982)
Epoch: [266][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0353 (0.0588)	Acc@1 99.609 (98.180)	Acc@5 100.000 (99.985)
Epoch: [266][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.0642 (0.0576)	Acc@1 97.266 (98.189)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:267/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [267][0/196]	Time 0.140 (0.140)	Data 0.334 (0.334)	Loss 0.0516 (0.0516)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [267][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.0532 (0.0612)	Acc@1 98.828 (98.095)	Acc@5 100.000 (100.000)
Epoch: [267][128/196]	Time 0.081 (0.089)	Data 0.000 (0.003)	Loss 0.0437 (0.0592)	Acc@1 98.828 (98.192)	Acc@5 100.000 (99.994)
Epoch: [267][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.0775 (0.0604)	Acc@1 97.266 (98.089)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:268/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [268][0/196]	Time 0.127 (0.127)	Data 0.315 (0.315)	Loss 0.0694 (0.0694)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [268][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.0682 (0.0634)	Acc@1 98.047 (97.939)	Acc@5 100.000 (99.982)
Epoch: [268][128/196]	Time 0.090 (0.087)	Data 0.000 (0.003)	Loss 0.0583 (0.0589)	Acc@1 98.438 (98.216)	Acc@5 100.000 (99.988)
Epoch: [268][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0609 (0.0604)	Acc@1 97.266 (98.122)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:269/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [269][0/196]	Time 0.130 (0.130)	Data 0.287 (0.287)	Loss 0.0590 (0.0590)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [269][64/196]	Time 0.092 (0.088)	Data 0.000 (0.005)	Loss 0.0610 (0.0625)	Acc@1 98.438 (98.041)	Acc@5 100.000 (99.988)
Epoch: [269][128/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0452 (0.0606)	Acc@1 98.438 (98.083)	Acc@5 100.000 (99.985)
Epoch: [269][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0590 (0.0592)	Acc@1 98.047 (98.162)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:270/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [270][0/196]	Time 0.116 (0.116)	Data 0.299 (0.299)	Loss 0.0790 (0.0790)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [270][64/196]	Time 0.080 (0.089)	Data 0.000 (0.005)	Loss 0.0371 (0.0601)	Acc@1 99.609 (98.233)	Acc@5 100.000 (99.994)
Epoch: [270][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.1005 (0.0582)	Acc@1 95.703 (98.268)	Acc@5 100.000 (99.994)
Epoch: [270][192/196]	Time 0.101 (0.088)	Data 0.000 (0.002)	Loss 0.0613 (0.0602)	Acc@1 98.438 (98.170)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.18
Max memory: 51.4381312
 17.612s  j: 276 bis 280
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 543
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 271
Max memory: 0.1097216
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:271/275; Lr: 0.0001350851717672993
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [271][0/196]	Time 0.148 (0.148)	Data 0.323 (0.323)	Loss 0.0475 (0.0475)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [271][64/196]	Time 0.094 (0.088)	Data 0.000 (0.005)	Loss 0.0445 (0.0577)	Acc@1 99.219 (98.323)	Acc@5 100.000 (99.988)
Epoch: [271][128/196]	Time 0.079 (0.087)	Data 0.000 (0.003)	Loss 0.0610 (0.0587)	Acc@1 98.438 (98.183)	Acc@5 100.000 (99.994)
Epoch: [271][192/196]	Time 0.090 (0.086)	Data 0.000 (0.002)	Loss 0.0420 (0.0590)	Acc@1 98.828 (98.162)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:272/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [272][0/196]	Time 0.126 (0.126)	Data 0.327 (0.327)	Loss 0.0559 (0.0559)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [272][64/196]	Time 0.096 (0.089)	Data 0.000 (0.005)	Loss 0.0562 (0.0601)	Acc@1 98.828 (98.065)	Acc@5 100.000 (99.994)
Epoch: [272][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0639 (0.0599)	Acc@1 98.438 (98.156)	Acc@5 100.000 (99.988)
Epoch: [272][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0371 (0.0598)	Acc@1 99.219 (98.120)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:273/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [273][0/196]	Time 0.125 (0.125)	Data 0.269 (0.269)	Loss 0.0494 (0.0494)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [273][64/196]	Time 0.084 (0.088)	Data 0.000 (0.004)	Loss 0.0401 (0.0575)	Acc@1 99.219 (98.323)	Acc@5 100.000 (99.982)
Epoch: [273][128/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0508 (0.0575)	Acc@1 99.219 (98.286)	Acc@5 100.000 (99.991)
Epoch: [273][192/196]	Time 0.094 (0.088)	Data 0.000 (0.002)	Loss 0.0827 (0.0577)	Acc@1 97.266 (98.290)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:274/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [274][0/196]	Time 0.149 (0.149)	Data 0.283 (0.283)	Loss 0.0510 (0.0510)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [274][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.0628 (0.0547)	Acc@1 98.047 (98.317)	Acc@5 100.000 (99.988)
Epoch: [274][128/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0458 (0.0576)	Acc@1 98.828 (98.183)	Acc@5 100.000 (99.988)
Epoch: [274][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0627 (0.0577)	Acc@1 98.047 (98.195)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:275/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [275][0/196]	Time 0.108 (0.108)	Data 0.292 (0.292)	Loss 0.0537 (0.0537)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [275][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0521 (0.0614)	Acc@1 98.438 (98.137)	Acc@5 100.000 (99.982)
Epoch: [275][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0787 (0.0603)	Acc@1 97.656 (98.144)	Acc@5 100.000 (99.988)
Epoch: [275][192/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.0286 (0.0589)	Acc@1 99.609 (98.229)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.22
Max memory: 51.4381312
 17.459s  j: 281 bis 285
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5314
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 276
Max memory: 0.1097216
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:276/280; Lr: 0.00012157665459056936
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [276][0/196]	Time 0.161 (0.161)	Data 0.257 (0.257)	Loss 0.0645 (0.0645)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [276][64/196]	Time 0.084 (0.087)	Data 0.000 (0.004)	Loss 0.0738 (0.0574)	Acc@1 97.656 (98.191)	Acc@5 100.000 (100.000)
Epoch: [276][128/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0935 (0.0571)	Acc@1 97.656 (98.268)	Acc@5 100.000 (99.997)
Epoch: [276][192/196]	Time 0.089 (0.087)	Data 0.000 (0.001)	Loss 0.0411 (0.0581)	Acc@1 99.219 (98.229)	Acc@5 100.000 (99.996)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:277/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [277][0/196]	Time 0.126 (0.126)	Data 0.311 (0.311)	Loss 0.0313 (0.0313)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [277][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0544 (0.0580)	Acc@1 98.438 (98.263)	Acc@5 100.000 (99.982)
Epoch: [277][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.0363 (0.0570)	Acc@1 98.828 (98.292)	Acc@5 100.000 (99.982)
Epoch: [277][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0759 (0.0576)	Acc@1 96.875 (98.247)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:278/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [278][0/196]	Time 0.122 (0.122)	Data 0.266 (0.266)	Loss 0.0544 (0.0544)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [278][64/196]	Time 0.083 (0.089)	Data 0.000 (0.004)	Loss 0.0379 (0.0577)	Acc@1 98.828 (98.191)	Acc@5 100.000 (99.982)
Epoch: [278][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1056 (0.0587)	Acc@1 95.312 (98.180)	Acc@5 100.000 (99.988)
Epoch: [278][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0804 (0.0580)	Acc@1 97.266 (98.217)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:279/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [279][0/196]	Time 0.122 (0.122)	Data 0.281 (0.281)	Loss 0.1020 (0.1020)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [279][64/196]	Time 0.098 (0.090)	Data 0.000 (0.005)	Loss 0.0520 (0.0594)	Acc@1 98.047 (98.095)	Acc@5 100.000 (100.000)
Epoch: [279][128/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.0506 (0.0574)	Acc@1 98.047 (98.223)	Acc@5 100.000 (100.000)
Epoch: [279][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0475 (0.0575)	Acc@1 98.828 (98.231)	Acc@5 100.000 (100.000)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:280/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [280][0/196]	Time 0.127 (0.127)	Data 0.326 (0.326)	Loss 0.0381 (0.0381)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [280][64/196]	Time 0.090 (0.089)	Data 0.000 (0.005)	Loss 0.0728 (0.0595)	Acc@1 97.266 (98.113)	Acc@5 100.000 (99.988)
Epoch: [280][128/196]	Time 0.106 (0.088)	Data 0.000 (0.003)	Loss 0.0433 (0.0569)	Acc@1 98.828 (98.207)	Acc@5 100.000 (99.994)
Epoch: [280][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0444 (0.0582)	Acc@1 98.438 (98.203)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.28
Max memory: 51.4381312
 17.632s  j: 286 bis 290
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3684
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 281
Max memory: 0.1097216
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:281/285; Lr: 0.00010941898913151243
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [281][0/196]	Time 0.142 (0.142)	Data 0.301 (0.301)	Loss 0.0326 (0.0326)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [281][64/196]	Time 0.090 (0.089)	Data 0.000 (0.005)	Loss 0.0727 (0.0584)	Acc@1 96.484 (98.179)	Acc@5 100.000 (99.982)
Epoch: [281][128/196]	Time 0.088 (0.089)	Data 0.000 (0.003)	Loss 0.0518 (0.0588)	Acc@1 98.047 (98.141)	Acc@5 100.000 (99.988)
Epoch: [281][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.0427 (0.0577)	Acc@1 99.219 (98.158)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:282/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [282][0/196]	Time 0.147 (0.147)	Data 0.303 (0.303)	Loss 0.0593 (0.0593)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [282][64/196]	Time 0.084 (0.091)	Data 0.000 (0.005)	Loss 0.0434 (0.0571)	Acc@1 98.828 (98.263)	Acc@5 100.000 (99.982)
Epoch: [282][128/196]	Time 0.090 (0.090)	Data 0.000 (0.003)	Loss 0.0728 (0.0583)	Acc@1 97.656 (98.201)	Acc@5 100.000 (99.988)
Epoch: [282][192/196]	Time 0.088 (0.090)	Data 0.000 (0.002)	Loss 0.0462 (0.0575)	Acc@1 98.828 (98.247)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:283/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [283][0/196]	Time 0.123 (0.123)	Data 0.329 (0.329)	Loss 0.0688 (0.0688)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [283][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.0503 (0.0584)	Acc@1 98.828 (98.239)	Acc@5 100.000 (99.988)
Epoch: [283][128/196]	Time 0.085 (0.090)	Data 0.000 (0.003)	Loss 0.0459 (0.0583)	Acc@1 98.828 (98.238)	Acc@5 100.000 (99.988)
Epoch: [283][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.1189 (0.0587)	Acc@1 96.094 (98.207)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:284/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [284][0/196]	Time 0.120 (0.120)	Data 0.343 (0.343)	Loss 0.0519 (0.0519)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [284][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.0625 (0.0593)	Acc@1 97.266 (98.131)	Acc@5 100.000 (99.994)
Epoch: [284][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.0448 (0.0588)	Acc@1 99.219 (98.226)	Acc@5 100.000 (99.991)
Epoch: [284][192/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.0692 (0.0599)	Acc@1 96.875 (98.176)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:285/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [285][0/196]	Time 0.108 (0.108)	Data 0.316 (0.316)	Loss 0.0607 (0.0607)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [285][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0569 (0.0582)	Acc@1 98.438 (98.131)	Acc@5 100.000 (99.988)
Epoch: [285][128/196]	Time 0.089 (0.090)	Data 0.000 (0.003)	Loss 0.0555 (0.0582)	Acc@1 98.047 (98.180)	Acc@5 100.000 (99.988)
Epoch: [285][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.0723 (0.0579)	Acc@1 97.266 (98.185)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.14
Max memory: 51.4381312
 17.741s  j: 291 bis 295
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6172
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 286
Max memory: 0.1097216
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:286/290; Lr: 9.847709021836118e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [286][0/196]	Time 0.143 (0.143)	Data 0.289 (0.289)	Loss 0.0580 (0.0580)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [286][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0532 (0.0577)	Acc@1 98.438 (98.233)	Acc@5 100.000 (99.994)
Epoch: [286][128/196]	Time 0.089 (0.086)	Data 0.000 (0.002)	Loss 0.0506 (0.0569)	Acc@1 98.438 (98.256)	Acc@5 100.000 (99.997)
Epoch: [286][192/196]	Time 0.087 (0.086)	Data 0.000 (0.002)	Loss 0.0395 (0.0573)	Acc@1 99.219 (98.251)	Acc@5 100.000 (99.998)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:287/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [287][0/196]	Time 0.116 (0.116)	Data 0.338 (0.338)	Loss 0.0528 (0.0528)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [287][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.0599 (0.0610)	Acc@1 98.047 (98.065)	Acc@5 100.000 (99.976)
Epoch: [287][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.0534 (0.0585)	Acc@1 98.438 (98.232)	Acc@5 100.000 (99.985)
Epoch: [287][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0575 (0.0589)	Acc@1 98.438 (98.195)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:288/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [288][0/196]	Time 0.130 (0.130)	Data 0.280 (0.280)	Loss 0.0564 (0.0564)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [288][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0343 (0.0539)	Acc@1 99.219 (98.401)	Acc@5 100.000 (100.000)
Epoch: [288][128/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.0626 (0.0587)	Acc@1 98.438 (98.223)	Acc@5 100.000 (99.985)
Epoch: [288][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0718 (0.0586)	Acc@1 97.656 (98.257)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:289/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [289][0/196]	Time 0.124 (0.124)	Data 0.301 (0.301)	Loss 0.0600 (0.0600)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [289][64/196]	Time 0.080 (0.088)	Data 0.000 (0.005)	Loss 0.0315 (0.0567)	Acc@1 99.609 (98.287)	Acc@5 100.000 (99.994)
Epoch: [289][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0802 (0.0581)	Acc@1 97.656 (98.286)	Acc@5 100.000 (99.982)
Epoch: [289][192/196]	Time 0.097 (0.087)	Data 0.000 (0.002)	Loss 0.0589 (0.0585)	Acc@1 98.828 (98.251)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:290/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [290][0/196]	Time 0.129 (0.129)	Data 0.341 (0.341)	Loss 0.0842 (0.0842)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [290][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0434 (0.0539)	Acc@1 98.828 (98.365)	Acc@5 100.000 (100.000)
Epoch: [290][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.0563 (0.0572)	Acc@1 98.438 (98.198)	Acc@5 100.000 (99.988)
Epoch: [290][192/196]	Time 0.083 (0.086)	Data 0.000 (0.002)	Loss 0.0470 (0.0572)	Acc@1 97.656 (98.217)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.1
Max memory: 51.4381312
 17.339s  j: 296 bis 300
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1594
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 291
Max memory: 0.1097216
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:291/295; Lr: 8.862938119652506e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [291][0/196]	Time 0.158 (0.158)	Data 0.294 (0.294)	Loss 0.0339 (0.0339)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [291][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0225 (0.0571)	Acc@1 100.000 (98.245)	Acc@5 100.000 (99.994)
Epoch: [291][128/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0627 (0.0585)	Acc@1 98.047 (98.244)	Acc@5 100.000 (99.991)
Epoch: [291][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0622 (0.0598)	Acc@1 98.047 (98.195)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:292/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [292][0/196]	Time 0.154 (0.154)	Data 0.318 (0.318)	Loss 0.0469 (0.0469)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [292][64/196]	Time 0.093 (0.090)	Data 0.000 (0.005)	Loss 0.0628 (0.0614)	Acc@1 97.656 (97.987)	Acc@5 100.000 (99.994)
Epoch: [292][128/196]	Time 0.093 (0.089)	Data 0.000 (0.003)	Loss 0.0586 (0.0590)	Acc@1 98.438 (98.117)	Acc@5 100.000 (99.997)
Epoch: [292][192/196]	Time 0.081 (0.089)	Data 0.000 (0.002)	Loss 0.0923 (0.0605)	Acc@1 96.484 (98.077)	Acc@5 100.000 (99.996)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:293/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [293][0/196]	Time 0.157 (0.157)	Data 0.265 (0.265)	Loss 0.0529 (0.0529)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [293][64/196]	Time 0.085 (0.090)	Data 0.000 (0.004)	Loss 0.0743 (0.0573)	Acc@1 97.656 (98.227)	Acc@5 100.000 (99.988)
Epoch: [293][128/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.0352 (0.0565)	Acc@1 99.219 (98.259)	Acc@5 100.000 (99.991)
Epoch: [293][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0727 (0.0572)	Acc@1 96.875 (98.247)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:294/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [294][0/196]	Time 0.136 (0.136)	Data 0.324 (0.324)	Loss 0.0370 (0.0370)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [294][64/196]	Time 0.096 (0.089)	Data 0.000 (0.005)	Loss 0.0540 (0.0600)	Acc@1 98.438 (98.125)	Acc@5 100.000 (99.982)
Epoch: [294][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.0807 (0.0607)	Acc@1 96.875 (98.110)	Acc@5 100.000 (99.985)
Epoch: [294][192/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.0833 (0.0611)	Acc@1 97.266 (98.102)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:295/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [295][0/196]	Time 0.119 (0.119)	Data 0.292 (0.292)	Loss 0.0386 (0.0386)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [295][64/196]	Time 0.079 (0.089)	Data 0.000 (0.005)	Loss 0.0692 (0.0602)	Acc@1 97.266 (98.107)	Acc@5 100.000 (100.000)
Epoch: [295][128/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0364 (0.0598)	Acc@1 99.609 (98.147)	Acc@5 100.000 (99.991)
Epoch: [295][192/196]	Time 0.093 (0.087)	Data 0.000 (0.002)	Loss 0.0452 (0.0587)	Acc@1 98.047 (98.205)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.22
Max memory: 51.4381312
 17.401s  j: 301 bis 305
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4870
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 296
Max memory: 0.1097216
lr: 7.976644307687256e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:296/300; Lr: 7.976644307687256e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [296][0/196]	Time 0.138 (0.138)	Data 0.298 (0.298)	Loss 0.0410 (0.0410)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [296][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.0491 (0.0598)	Acc@1 98.828 (98.077)	Acc@5 100.000 (99.994)
Epoch: [296][128/196]	Time 0.092 (0.087)	Data 0.000 (0.003)	Loss 0.0639 (0.0592)	Acc@1 97.266 (98.135)	Acc@5 99.609 (99.988)
Epoch: [296][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0382 (0.0582)	Acc@1 99.609 (98.197)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 7.976644307687256e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:297/300; Lr: 7.976644307687256e-05
batch Size 256
Epoch: [297][0/196]	Time 0.137 (0.137)	Data 0.303 (0.303)	Loss 0.0338 (0.0338)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [297][64/196]	Time 0.103 (0.088)	Data 0.000 (0.005)	Loss 0.0475 (0.0552)	Acc@1 98.438 (98.353)	Acc@5 100.000 (99.988)
Epoch: [297][128/196]	Time 0.093 (0.087)	Data 0.000 (0.003)	Loss 0.0458 (0.0564)	Acc@1 98.828 (98.316)	Acc@5 100.000 (99.979)
Epoch: [297][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0438 (0.0565)	Acc@1 98.438 (98.318)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 7.976644307687256e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:298/300; Lr: 7.976644307687256e-05
batch Size 256
Epoch: [298][0/196]	Time 0.132 (0.132)	Data 0.292 (0.292)	Loss 0.0608 (0.0608)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [298][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.0581 (0.0563)	Acc@1 98.828 (98.299)	Acc@5 100.000 (99.988)
Epoch: [298][128/196]	Time 0.078 (0.088)	Data 0.000 (0.003)	Loss 0.0598 (0.0567)	Acc@1 98.828 (98.310)	Acc@5 100.000 (99.988)
Epoch: [298][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0685 (0.0565)	Acc@1 96.875 (98.326)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 7.976644307687256e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:299/300; Lr: 7.976644307687256e-05
batch Size 256
Epoch: [299][0/196]	Time 0.131 (0.131)	Data 0.300 (0.300)	Loss 0.0637 (0.0637)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [299][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.0606 (0.0591)	Acc@1 97.266 (98.203)	Acc@5 100.000 (99.994)
Epoch: [299][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.0367 (0.0577)	Acc@1 98.828 (98.265)	Acc@5 100.000 (99.997)
Epoch: [299][192/196]	Time 0.097 (0.087)	Data 0.000 (0.002)	Loss 0.0808 (0.0586)	Acc@1 98.828 (98.213)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 7.976644307687256e-05
args.lr: 7.17897987691853e-05
lr: 7.17897987691853e-05
Epoche:300/300; Lr: 7.976644307687256e-05
batch Size 256
Epoch: [300][0/196]	Time 0.141 (0.141)	Data 0.310 (0.310)	Loss 0.0421 (0.0421)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [300][64/196]	Time 0.090 (0.087)	Data 0.000 (0.005)	Loss 0.0818 (0.0567)	Acc@1 96.875 (98.233)	Acc@5 100.000 (99.988)
Epoch: [300][128/196]	Time 0.086 (0.085)	Data 0.000 (0.003)	Loss 0.0261 (0.0561)	Acc@1 99.219 (98.295)	Acc@5 100.000 (99.985)
Epoch: [300][192/196]	Time 0.081 (0.086)	Data 0.000 (0.002)	Loss 0.0651 (0.0577)	Acc@1 98.047 (98.241)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.17
Max memory: 51.4381312
 17.151s  j: 306 bis 310
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4592
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 301
Max memory: 0.1097216
lr: 7.17897987691853e-05
args.lr: 7.17897987691853e-05
lr: 7.17897987691853e-05
Epoche:301/305; Lr: 7.17897987691853e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [301][0/196]	Time 0.147 (0.147)	Data 0.293 (0.293)	Loss 0.0510 (0.0510)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [301][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0375 (0.0582)	Acc@1 99.219 (98.113)	Acc@5 100.000 (99.982)
Epoch: [301][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0380 (0.0572)	Acc@1 98.828 (98.168)	Acc@5 100.000 (99.991)
Epoch: [301][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0993 (0.0577)	Acc@1 97.266 (98.160)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 7.17897987691853e-05
args.lr: 7.17897987691853e-05
lr: 7.17897987691853e-05
Epoche:302/305; Lr: 7.17897987691853e-05
batch Size 256
Epoch: [302][0/196]	Time 0.135 (0.135)	Data 0.282 (0.282)	Loss 0.0561 (0.0561)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [302][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0424 (0.0550)	Acc@1 98.828 (98.425)	Acc@5 100.000 (99.988)
Epoch: [302][128/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0572 (0.0568)	Acc@1 98.047 (98.344)	Acc@5 100.000 (99.991)
Epoch: [302][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0338 (0.0565)	Acc@1 99.609 (98.346)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 7.17897987691853e-05
args.lr: 7.17897987691853e-05
lr: 7.17897987691853e-05
Epoche:303/305; Lr: 7.17897987691853e-05
batch Size 256
Epoch: [303][0/196]	Time 0.130 (0.130)	Data 0.312 (0.312)	Loss 0.0623 (0.0623)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [303][64/196]	Time 0.090 (0.087)	Data 0.002 (0.005)	Loss 0.0714 (0.0564)	Acc@1 98.047 (98.359)	Acc@5 100.000 (99.988)
Epoch: [303][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.0357 (0.0578)	Acc@1 99.219 (98.292)	Acc@5 100.000 (99.991)
Epoch: [303][192/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.0459 (0.0577)	Acc@1 98.828 (98.284)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 7.17897987691853e-05
args.lr: 7.17897987691853e-05
lr: 7.17897987691853e-05
Epoche:304/305; Lr: 7.17897987691853e-05
batch Size 256
Epoch: [304][0/196]	Time 0.141 (0.141)	Data 0.288 (0.288)	Loss 0.0915 (0.0915)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [304][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.0550 (0.0567)	Acc@1 98.047 (98.323)	Acc@5 100.000 (99.994)
Epoch: [304][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0613 (0.0565)	Acc@1 96.875 (98.307)	Acc@5 100.000 (99.991)
Epoch: [304][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0692 (0.0570)	Acc@1 97.656 (98.284)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 7.17897987691853e-05
args.lr: 6.461081889226677e-05
lr: 6.461081889226677e-05
Epoche:305/305; Lr: 7.17897987691853e-05
batch Size 256
Epoch: [305][0/196]	Time 0.119 (0.119)	Data 0.320 (0.320)	Loss 0.0442 (0.0442)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [305][64/196]	Time 0.086 (0.085)	Data 0.000 (0.005)	Loss 0.0850 (0.0559)	Acc@1 95.703 (98.305)	Acc@5 100.000 (99.982)
Epoch: [305][128/196]	Time 0.080 (0.086)	Data 0.000 (0.003)	Loss 0.0419 (0.0555)	Acc@1 99.219 (98.365)	Acc@5 100.000 (99.982)
Epoch: [305][192/196]	Time 0.083 (0.086)	Data 0.000 (0.002)	Loss 0.0355 (0.0557)	Acc@1 99.219 (98.326)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.18
Max memory: 51.4381312
 17.359s  j: 311 bis 315
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2866
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 306
Max memory: 0.1097216
lr: 6.461081889226677e-05
args.lr: 6.461081889226677e-05
lr: 6.461081889226677e-05
Epoche:306/310; Lr: 6.461081889226677e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [306][0/196]	Time 0.163 (0.163)	Data 0.272 (0.272)	Loss 0.0697 (0.0697)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [306][64/196]	Time 0.087 (0.090)	Data 0.000 (0.004)	Loss 0.0490 (0.0579)	Acc@1 98.438 (98.239)	Acc@5 100.000 (99.982)
Epoch: [306][128/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.0655 (0.0569)	Acc@1 98.047 (98.304)	Acc@5 100.000 (99.991)
Epoch: [306][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.0481 (0.0574)	Acc@1 98.828 (98.249)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 6.461081889226677e-05
args.lr: 6.461081889226677e-05
lr: 6.461081889226677e-05
Epoche:307/310; Lr: 6.461081889226677e-05
batch Size 256
Epoch: [307][0/196]	Time 0.127 (0.127)	Data 0.335 (0.335)	Loss 0.0415 (0.0415)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [307][64/196]	Time 0.096 (0.092)	Data 0.000 (0.005)	Loss 0.0402 (0.0595)	Acc@1 98.828 (98.155)	Acc@5 100.000 (99.988)
Epoch: [307][128/196]	Time 0.088 (0.089)	Data 0.000 (0.003)	Loss 0.0765 (0.0598)	Acc@1 96.875 (98.174)	Acc@5 100.000 (99.985)
Epoch: [307][192/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.0770 (0.0593)	Acc@1 98.047 (98.172)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 6.461081889226677e-05
args.lr: 6.461081889226677e-05
lr: 6.461081889226677e-05
Epoche:308/310; Lr: 6.461081889226677e-05
batch Size 256
Epoch: [308][0/196]	Time 0.123 (0.123)	Data 0.295 (0.295)	Loss 0.0781 (0.0781)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [308][64/196]	Time 0.092 (0.090)	Data 0.000 (0.005)	Loss 0.0501 (0.0545)	Acc@1 98.828 (98.365)	Acc@5 100.000 (99.988)
Epoch: [308][128/196]	Time 0.090 (0.089)	Data 0.000 (0.003)	Loss 0.0557 (0.0555)	Acc@1 98.438 (98.322)	Acc@5 100.000 (99.991)
Epoch: [308][192/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.0496 (0.0554)	Acc@1 98.438 (98.298)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 6.461081889226677e-05
args.lr: 6.461081889226677e-05
lr: 6.461081889226677e-05
Epoche:309/310; Lr: 6.461081889226677e-05
batch Size 256
Epoch: [309][0/196]	Time 0.131 (0.131)	Data 0.324 (0.324)	Loss 0.0692 (0.0692)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [309][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0584 (0.0570)	Acc@1 98.047 (98.215)	Acc@5 100.000 (99.988)
Epoch: [309][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0476 (0.0575)	Acc@1 98.438 (98.223)	Acc@5 100.000 (99.991)
Epoch: [309][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0467 (0.0572)	Acc@1 98.828 (98.276)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 6.461081889226677e-05
args.lr: 5.81497370030401e-05
lr: 5.81497370030401e-05
Epoche:310/310; Lr: 6.461081889226677e-05
batch Size 256
Epoch: [310][0/196]	Time 0.134 (0.134)	Data 0.305 (0.305)	Loss 0.0638 (0.0638)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [310][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0583 (0.0563)	Acc@1 98.047 (98.167)	Acc@5 100.000 (100.000)
Epoch: [310][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.0457 (0.0570)	Acc@1 98.828 (98.195)	Acc@5 100.000 (99.994)
Epoch: [310][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0651 (0.0574)	Acc@1 98.047 (98.199)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.14
Max memory: 51.4381312
 17.680s  j: 316 bis 320
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1529
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 311
Max memory: 0.1097216
lr: 5.81497370030401e-05
args.lr: 5.81497370030401e-05
lr: 5.81497370030401e-05
Epoche:311/315; Lr: 5.81497370030401e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [311][0/196]	Time 0.151 (0.151)	Data 0.280 (0.280)	Loss 0.0872 (0.0872)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [311][64/196]	Time 0.088 (0.088)	Data 0.000 (0.004)	Loss 0.0514 (0.0614)	Acc@1 98.438 (98.077)	Acc@5 100.000 (99.970)
Epoch: [311][128/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0699 (0.0614)	Acc@1 97.656 (98.080)	Acc@5 100.000 (99.982)
Epoch: [311][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0471 (0.0601)	Acc@1 98.438 (98.132)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 5.81497370030401e-05
args.lr: 5.81497370030401e-05
lr: 5.81497370030401e-05
Epoche:312/315; Lr: 5.81497370030401e-05
batch Size 256
Epoch: [312][0/196]	Time 0.114 (0.114)	Data 0.278 (0.278)	Loss 0.0580 (0.0580)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [312][64/196]	Time 0.087 (0.089)	Data 0.000 (0.004)	Loss 0.1163 (0.0580)	Acc@1 95.703 (98.257)	Acc@5 100.000 (99.994)
Epoch: [312][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0620 (0.0577)	Acc@1 97.656 (98.274)	Acc@5 100.000 (99.991)
Epoch: [312][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0601 (0.0582)	Acc@1 98.047 (98.270)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 5.81497370030401e-05
args.lr: 5.81497370030401e-05
lr: 5.81497370030401e-05
Epoche:313/315; Lr: 5.81497370030401e-05
batch Size 256
Epoch: [313][0/196]	Time 0.142 (0.142)	Data 0.304 (0.304)	Loss 0.0567 (0.0567)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [313][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.0619 (0.0567)	Acc@1 97.656 (98.311)	Acc@5 100.000 (99.988)
Epoch: [313][128/196]	Time 0.080 (0.089)	Data 0.000 (0.003)	Loss 0.0701 (0.0589)	Acc@1 96.484 (98.168)	Acc@5 100.000 (99.985)
Epoch: [313][192/196]	Time 0.099 (0.088)	Data 0.000 (0.002)	Loss 0.0411 (0.0577)	Acc@1 99.219 (98.176)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 5.81497370030401e-05
args.lr: 5.81497370030401e-05
lr: 5.81497370030401e-05
Epoche:314/315; Lr: 5.81497370030401e-05
batch Size 256
Epoch: [314][0/196]	Time 0.131 (0.131)	Data 0.316 (0.316)	Loss 0.0672 (0.0672)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [314][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.0349 (0.0561)	Acc@1 99.609 (98.299)	Acc@5 100.000 (99.988)
Epoch: [314][128/196]	Time 0.096 (0.088)	Data 0.000 (0.003)	Loss 0.0694 (0.0568)	Acc@1 97.266 (98.219)	Acc@5 100.000 (99.991)
Epoch: [314][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.0866 (0.0570)	Acc@1 97.656 (98.201)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 5.81497370030401e-05
args.lr: 5.233476330273609e-05
lr: 5.233476330273609e-05
Epoche:315/315; Lr: 5.81497370030401e-05
batch Size 256
Epoch: [315][0/196]	Time 0.139 (0.139)	Data 0.315 (0.315)	Loss 0.0949 (0.0949)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [315][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.0280 (0.0596)	Acc@1 99.609 (98.149)	Acc@5 100.000 (99.982)
Epoch: [315][128/196]	Time 0.081 (0.088)	Data 0.000 (0.003)	Loss 0.0647 (0.0588)	Acc@1 98.828 (98.162)	Acc@5 100.000 (99.991)
Epoch: [315][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0600 (0.0576)	Acc@1 96.875 (98.219)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.14
Max memory: 51.4381312
 17.612s  j: 321 bis 325
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1796
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 316
Max memory: 0.1097216
lr: 5.233476330273609e-05
args.lr: 5.233476330273609e-05
lr: 5.233476330273609e-05
Epoche:316/320; Lr: 5.233476330273609e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [316][0/196]	Time 0.169 (0.169)	Data 0.313 (0.313)	Loss 0.0785 (0.0785)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [316][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.0754 (0.0573)	Acc@1 97.656 (98.203)	Acc@5 100.000 (99.982)
Epoch: [316][128/196]	Time 0.096 (0.088)	Data 0.000 (0.003)	Loss 0.0550 (0.0568)	Acc@1 97.656 (98.192)	Acc@5 100.000 (99.991)
Epoch: [316][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0692 (0.0575)	Acc@1 98.828 (98.191)	Acc@5 99.609 (99.986)
Max memory in training epoch: 33.3541888
lr: 5.233476330273609e-05
args.lr: 5.233476330273609e-05
lr: 5.233476330273609e-05
Epoche:317/320; Lr: 5.233476330273609e-05
batch Size 256
Epoch: [317][0/196]	Time 0.123 (0.123)	Data 0.292 (0.292)	Loss 0.0293 (0.0293)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [317][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.0503 (0.0563)	Acc@1 98.828 (98.341)	Acc@5 100.000 (99.970)
Epoch: [317][128/196]	Time 0.094 (0.088)	Data 0.000 (0.002)	Loss 0.0330 (0.0578)	Acc@1 99.609 (98.256)	Acc@5 100.000 (99.973)
Epoch: [317][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0407 (0.0582)	Acc@1 98.828 (98.267)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 5.233476330273609e-05
args.lr: 5.233476330273609e-05
lr: 5.233476330273609e-05
Epoche:318/320; Lr: 5.233476330273609e-05
batch Size 256
Epoch: [318][0/196]	Time 0.113 (0.113)	Data 0.303 (0.303)	Loss 0.0544 (0.0544)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [318][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.0442 (0.0566)	Acc@1 98.047 (98.335)	Acc@5 100.000 (99.994)
Epoch: [318][128/196]	Time 0.078 (0.087)	Data 0.000 (0.003)	Loss 0.0808 (0.0563)	Acc@1 96.875 (98.262)	Acc@5 100.000 (99.994)
Epoch: [318][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0583 (0.0576)	Acc@1 98.047 (98.199)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 5.233476330273609e-05
args.lr: 5.233476330273609e-05
lr: 5.233476330273609e-05
Epoche:319/320; Lr: 5.233476330273609e-05
batch Size 256
Epoch: [319][0/196]	Time 0.136 (0.136)	Data 0.299 (0.299)	Loss 0.0519 (0.0519)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [319][64/196]	Time 0.082 (0.087)	Data 0.000 (0.005)	Loss 0.0677 (0.0563)	Acc@1 98.047 (98.221)	Acc@5 100.000 (99.982)
Epoch: [319][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.0606 (0.0567)	Acc@1 97.656 (98.262)	Acc@5 100.000 (99.979)
Epoch: [319][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0588 (0.0567)	Acc@1 98.438 (98.272)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 5.233476330273609e-05
args.lr: 4.7101286972462485e-05
lr: 4.7101286972462485e-05
Epoche:320/320; Lr: 5.233476330273609e-05
batch Size 256
Epoch: [320][0/196]	Time 0.117 (0.117)	Data 0.301 (0.301)	Loss 0.0383 (0.0383)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [320][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 0.0645 (0.0547)	Acc@1 97.656 (98.323)	Acc@5 100.000 (100.000)
Epoch: [320][128/196]	Time 0.092 (0.087)	Data 0.000 (0.003)	Loss 0.0814 (0.0571)	Acc@1 97.266 (98.268)	Acc@5 100.000 (99.988)
Epoch: [320][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.0577 (0.0577)	Acc@1 97.656 (98.245)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.17
Max memory: 51.4381312
 17.397s  j: 326 bis 330
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 55
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 321
Max memory: 0.1097216
lr: 4.7101286972462485e-05
args.lr: 4.7101286972462485e-05
lr: 4.7101286972462485e-05
Epoche:321/325; Lr: 4.7101286972462485e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [321][0/196]	Time 0.187 (0.187)	Data 0.271 (0.271)	Loss 0.0357 (0.0357)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [321][64/196]	Time 0.092 (0.089)	Data 0.000 (0.004)	Loss 0.0656 (0.0556)	Acc@1 97.656 (98.161)	Acc@5 100.000 (99.994)
Epoch: [321][128/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0521 (0.0562)	Acc@1 98.047 (98.210)	Acc@5 100.000 (99.988)
Epoch: [321][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.0558 (0.0564)	Acc@1 97.656 (98.199)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 4.7101286972462485e-05
args.lr: 4.7101286972462485e-05
lr: 4.7101286972462485e-05
Epoche:322/325; Lr: 4.7101286972462485e-05
batch Size 256
Epoch: [322][0/196]	Time 0.126 (0.126)	Data 0.305 (0.305)	Loss 0.0496 (0.0496)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [322][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0478 (0.0600)	Acc@1 98.828 (98.077)	Acc@5 100.000 (99.970)
Epoch: [322][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0479 (0.0588)	Acc@1 98.438 (98.171)	Acc@5 100.000 (99.985)
Epoch: [322][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0603 (0.0589)	Acc@1 97.656 (98.168)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 4.7101286972462485e-05
args.lr: 4.7101286972462485e-05
lr: 4.7101286972462485e-05
Epoche:323/325; Lr: 4.7101286972462485e-05
batch Size 256
Epoch: [323][0/196]	Time 0.127 (0.127)	Data 0.282 (0.282)	Loss 0.0541 (0.0541)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [323][64/196]	Time 0.091 (0.090)	Data 0.000 (0.005)	Loss 0.0712 (0.0556)	Acc@1 96.875 (98.251)	Acc@5 100.000 (100.000)
Epoch: [323][128/196]	Time 0.081 (0.089)	Data 0.000 (0.002)	Loss 0.0592 (0.0559)	Acc@1 98.438 (98.274)	Acc@5 100.000 (99.997)
Epoch: [323][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0860 (0.0568)	Acc@1 96.875 (98.227)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 4.7101286972462485e-05
args.lr: 4.7101286972462485e-05
lr: 4.7101286972462485e-05
Epoche:324/325; Lr: 4.7101286972462485e-05
batch Size 256
Epoch: [324][0/196]	Time 0.127 (0.127)	Data 0.296 (0.296)	Loss 0.0773 (0.0773)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [324][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0396 (0.0562)	Acc@1 98.828 (98.281)	Acc@5 100.000 (99.994)
Epoch: [324][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0501 (0.0573)	Acc@1 98.047 (98.207)	Acc@5 100.000 (99.994)
Epoch: [324][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.1408 (0.0578)	Acc@1 94.922 (98.211)	Acc@5 99.609 (99.992)
Max memory in training epoch: 33.3541888
lr: 4.7101286972462485e-05
args.lr: 4.239115827521624e-05
lr: 4.239115827521624e-05
Epoche:325/325; Lr: 4.7101286972462485e-05
batch Size 256
Epoch: [325][0/196]	Time 0.119 (0.119)	Data 0.287 (0.287)	Loss 0.0445 (0.0445)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [325][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.0739 (0.0595)	Acc@1 97.656 (98.161)	Acc@5 99.609 (99.994)
Epoch: [325][128/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.0597 (0.0581)	Acc@1 98.047 (98.189)	Acc@5 100.000 (99.997)
Epoch: [325][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0470 (0.0573)	Acc@1 98.828 (98.235)	Acc@5 100.000 (99.996)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.15
Max memory: 51.4381312
 17.435s  j: 331 bis 335
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6038
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 326
Max memory: 0.1097216
lr: 4.239115827521624e-05
args.lr: 4.239115827521624e-05
lr: 4.239115827521624e-05
Epoche:326/330; Lr: 4.239115827521624e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [326][0/196]	Time 0.171 (0.171)	Data 0.304 (0.304)	Loss 0.0537 (0.0537)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [326][64/196]	Time 0.082 (0.090)	Data 0.000 (0.005)	Loss 0.0664 (0.0561)	Acc@1 97.656 (98.287)	Acc@5 100.000 (99.994)
Epoch: [326][128/196]	Time 0.091 (0.089)	Data 0.000 (0.003)	Loss 0.0596 (0.0580)	Acc@1 98.438 (98.207)	Acc@5 100.000 (99.991)
Epoch: [326][192/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.0371 (0.0572)	Acc@1 99.219 (98.247)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 4.239115827521624e-05
args.lr: 4.239115827521624e-05
lr: 4.239115827521624e-05
Epoche:327/330; Lr: 4.239115827521624e-05
batch Size 256
Epoch: [327][0/196]	Time 0.124 (0.124)	Data 0.299 (0.299)	Loss 0.0365 (0.0365)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [327][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.0367 (0.0567)	Acc@1 99.219 (98.209)	Acc@5 100.000 (99.976)
Epoch: [327][128/196]	Time 0.085 (0.089)	Data 0.000 (0.003)	Loss 0.0444 (0.0559)	Acc@1 98.828 (98.332)	Acc@5 100.000 (99.976)
Epoch: [327][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0351 (0.0555)	Acc@1 99.609 (98.336)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 4.239115827521624e-05
args.lr: 4.239115827521624e-05
lr: 4.239115827521624e-05
Epoche:328/330; Lr: 4.239115827521624e-05
batch Size 256
Epoch: [328][0/196]	Time 0.118 (0.118)	Data 0.292 (0.292)	Loss 0.0337 (0.0337)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [328][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.0754 (0.0586)	Acc@1 97.656 (98.191)	Acc@5 100.000 (99.982)
Epoch: [328][128/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0414 (0.0577)	Acc@1 100.000 (98.277)	Acc@5 100.000 (99.991)
Epoch: [328][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0757 (0.0572)	Acc@1 96.875 (98.292)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 4.239115827521624e-05
args.lr: 4.239115827521624e-05
lr: 4.239115827521624e-05
Epoche:329/330; Lr: 4.239115827521624e-05
batch Size 256
Epoch: [329][0/196]	Time 0.138 (0.138)	Data 0.298 (0.298)	Loss 0.0608 (0.0608)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [329][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0774 (0.0575)	Acc@1 96.875 (98.161)	Acc@5 100.000 (99.994)
Epoch: [329][128/196]	Time 0.095 (0.088)	Data 0.000 (0.003)	Loss 0.0512 (0.0581)	Acc@1 98.438 (98.186)	Acc@5 100.000 (99.991)
Epoch: [329][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0377 (0.0566)	Acc@1 99.219 (98.225)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 4.239115827521624e-05
args.lr: 3.8152042447694614e-05
lr: 3.8152042447694614e-05
Epoche:330/330; Lr: 4.239115827521624e-05
batch Size 256
Epoch: [330][0/196]	Time 0.139 (0.139)	Data 0.301 (0.301)	Loss 0.0362 (0.0362)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [330][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.0356 (0.0578)	Acc@1 98.828 (98.155)	Acc@5 100.000 (100.000)
Epoch: [330][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.0608 (0.0551)	Acc@1 98.047 (98.265)	Acc@5 100.000 (99.991)
Epoch: [330][192/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.0675 (0.0560)	Acc@1 97.656 (98.274)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.16
Max memory: 51.4381312
 17.704s  j: 336 bis 340
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1573
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 331
Max memory: 0.1097216
lr: 3.8152042447694614e-05
args.lr: 3.8152042447694614e-05
lr: 3.8152042447694614e-05
Epoche:331/335; Lr: 3.8152042447694614e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [331][0/196]	Time 0.152 (0.152)	Data 0.286 (0.286)	Loss 0.0396 (0.0396)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [331][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.0560 (0.0592)	Acc@1 98.047 (98.173)	Acc@5 100.000 (99.982)
Epoch: [331][128/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0484 (0.0565)	Acc@1 98.828 (98.268)	Acc@5 100.000 (99.988)
Epoch: [331][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0625 (0.0561)	Acc@1 98.047 (98.267)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 3.8152042447694614e-05
args.lr: 3.8152042447694614e-05
lr: 3.8152042447694614e-05
Epoche:332/335; Lr: 3.8152042447694614e-05
batch Size 256
Epoch: [332][0/196]	Time 0.115 (0.115)	Data 0.302 (0.302)	Loss 0.0744 (0.0744)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [332][64/196]	Time 0.083 (0.087)	Data 0.000 (0.005)	Loss 0.0478 (0.0565)	Acc@1 98.828 (98.257)	Acc@5 100.000 (100.000)
Epoch: [332][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.0587 (0.0571)	Acc@1 97.656 (98.268)	Acc@5 100.000 (99.994)
Epoch: [332][192/196]	Time 0.094 (0.087)	Data 0.000 (0.002)	Loss 0.0399 (0.0572)	Acc@1 99.219 (98.272)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 3.8152042447694614e-05
args.lr: 3.8152042447694614e-05
lr: 3.8152042447694614e-05
Epoche:333/335; Lr: 3.8152042447694614e-05
batch Size 256
Epoch: [333][0/196]	Time 0.124 (0.124)	Data 0.314 (0.314)	Loss 0.0569 (0.0569)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [333][64/196]	Time 0.090 (0.087)	Data 0.000 (0.005)	Loss 0.0541 (0.0582)	Acc@1 98.047 (98.203)	Acc@5 100.000 (99.994)
Epoch: [333][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0569 (0.0574)	Acc@1 98.438 (98.210)	Acc@5 100.000 (99.985)
Epoch: [333][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0836 (0.0578)	Acc@1 98.047 (98.207)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 3.8152042447694614e-05
args.lr: 3.8152042447694614e-05
lr: 3.8152042447694614e-05
Epoche:334/335; Lr: 3.8152042447694614e-05
batch Size 256
Epoch: [334][0/196]	Time 0.126 (0.126)	Data 0.307 (0.307)	Loss 0.0461 (0.0461)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [334][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0652 (0.0587)	Acc@1 98.828 (98.137)	Acc@5 100.000 (99.982)
Epoch: [334][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0684 (0.0559)	Acc@1 96.484 (98.259)	Acc@5 100.000 (99.991)
Epoch: [334][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0360 (0.0565)	Acc@1 100.000 (98.292)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 3.8152042447694614e-05
args.lr: 3.433683820292515e-05
lr: 3.433683820292515e-05
Epoche:335/335; Lr: 3.8152042447694614e-05
batch Size 256
Epoch: [335][0/196]	Time 0.124 (0.124)	Data 0.272 (0.272)	Loss 0.0544 (0.0544)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [335][64/196]	Time 0.083 (0.088)	Data 0.000 (0.004)	Loss 0.0456 (0.0581)	Acc@1 98.828 (98.197)	Acc@5 100.000 (99.988)
Epoch: [335][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0763 (0.0590)	Acc@1 96.875 (98.162)	Acc@5 100.000 (99.985)
Epoch: [335][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0365 (0.0570)	Acc@1 98.828 (98.229)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.19
Max memory: 51.4381312
 17.427s  j: 341 bis 345
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8521
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 336
Max memory: 0.1097216
lr: 3.433683820292515e-05
args.lr: 3.433683820292515e-05
lr: 3.433683820292515e-05
Epoche:336/340; Lr: 3.433683820292515e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [336][0/196]	Time 0.149 (0.149)	Data 0.278 (0.278)	Loss 0.0695 (0.0695)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [336][64/196]	Time 0.090 (0.089)	Data 0.000 (0.004)	Loss 0.0416 (0.0590)	Acc@1 99.609 (98.227)	Acc@5 100.000 (99.988)
Epoch: [336][128/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0545 (0.0566)	Acc@1 98.438 (98.304)	Acc@5 100.000 (99.991)
Epoch: [336][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0400 (0.0566)	Acc@1 98.438 (98.328)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 3.433683820292515e-05
args.lr: 3.433683820292515e-05
lr: 3.433683820292515e-05
Epoche:337/340; Lr: 3.433683820292515e-05
batch Size 256
Epoch: [337][0/196]	Time 0.121 (0.121)	Data 0.305 (0.305)	Loss 0.0545 (0.0545)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [337][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.0724 (0.0549)	Acc@1 98.047 (98.371)	Acc@5 100.000 (99.994)
Epoch: [337][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0775 (0.0548)	Acc@1 98.047 (98.335)	Acc@5 100.000 (99.994)
Epoch: [337][192/196]	Time 0.078 (0.087)	Data 0.000 (0.002)	Loss 0.0985 (0.0565)	Acc@1 95.703 (98.257)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 3.433683820292515e-05
args.lr: 3.433683820292515e-05
lr: 3.433683820292515e-05
Epoche:338/340; Lr: 3.433683820292515e-05
batch Size 256
Epoch: [338][0/196]	Time 0.121 (0.121)	Data 0.299 (0.299)	Loss 0.0553 (0.0553)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [338][64/196]	Time 0.090 (0.087)	Data 0.000 (0.005)	Loss 0.0462 (0.0573)	Acc@1 98.828 (98.203)	Acc@5 100.000 (99.976)
Epoch: [338][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0874 (0.0568)	Acc@1 96.875 (98.328)	Acc@5 99.609 (99.985)
Epoch: [338][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0525 (0.0576)	Acc@1 98.828 (98.267)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 3.433683820292515e-05
args.lr: 3.433683820292515e-05
lr: 3.433683820292515e-05
Epoche:339/340; Lr: 3.433683820292515e-05
batch Size 256
Epoch: [339][0/196]	Time 0.133 (0.133)	Data 0.321 (0.321)	Loss 0.0518 (0.0518)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [339][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.0644 (0.0560)	Acc@1 97.656 (98.275)	Acc@5 100.000 (99.994)
Epoch: [339][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0598 (0.0565)	Acc@1 98.047 (98.259)	Acc@5 100.000 (99.997)
Epoch: [339][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.0576 (0.0559)	Acc@1 98.438 (98.310)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 3.433683820292515e-05
args.lr: 3.090315438263264e-05
lr: 3.090315438263264e-05
Epoche:340/340; Lr: 3.433683820292515e-05
batch Size 256
Epoch: [340][0/196]	Time 0.130 (0.130)	Data 0.267 (0.267)	Loss 0.0659 (0.0659)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [340][64/196]	Time 0.078 (0.089)	Data 0.000 (0.004)	Loss 0.0274 (0.0558)	Acc@1 99.609 (98.227)	Acc@5 100.000 (99.982)
Epoch: [340][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0408 (0.0551)	Acc@1 99.609 (98.265)	Acc@5 100.000 (99.988)
Epoch: [340][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0517 (0.0548)	Acc@1 98.828 (98.290)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.09
Max memory: 51.4381312
 17.519s  j: 346 bis 350
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7697
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 341
Max memory: 0.1097216
lr: 3.090315438263264e-05
args.lr: 3.090315438263264e-05
lr: 3.090315438263264e-05
Epoche:341/345; Lr: 3.090315438263264e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [341][0/196]	Time 0.161 (0.161)	Data 0.285 (0.285)	Loss 0.0392 (0.0392)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [341][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.0782 (0.0565)	Acc@1 97.266 (98.245)	Acc@5 100.000 (99.994)
Epoch: [341][128/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.0771 (0.0573)	Acc@1 97.656 (98.250)	Acc@5 100.000 (99.988)
Epoch: [341][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.0747 (0.0568)	Acc@1 98.438 (98.272)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 3.090315438263264e-05
args.lr: 3.090315438263264e-05
lr: 3.090315438263264e-05
Epoche:342/345; Lr: 3.090315438263264e-05
batch Size 256
Epoch: [342][0/196]	Time 0.121 (0.121)	Data 0.302 (0.302)	Loss 0.0564 (0.0564)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [342][64/196]	Time 0.084 (0.092)	Data 0.000 (0.005)	Loss 0.0523 (0.0551)	Acc@1 98.828 (98.425)	Acc@5 100.000 (99.994)
Epoch: [342][128/196]	Time 0.087 (0.090)	Data 0.000 (0.003)	Loss 0.0391 (0.0557)	Acc@1 99.219 (98.371)	Acc@5 100.000 (99.991)
Epoch: [342][192/196]	Time 0.085 (0.090)	Data 0.000 (0.002)	Loss 0.0273 (0.0559)	Acc@1 99.609 (98.357)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 3.090315438263264e-05
args.lr: 3.090315438263264e-05
lr: 3.090315438263264e-05
Epoche:343/345; Lr: 3.090315438263264e-05
batch Size 256
Epoch: [343][0/196]	Time 0.105 (0.105)	Data 0.326 (0.326)	Loss 0.0549 (0.0549)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [343][64/196]	Time 0.089 (0.090)	Data 0.000 (0.005)	Loss 0.0523 (0.0553)	Acc@1 98.047 (98.371)	Acc@5 100.000 (100.000)
Epoch: [343][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.0399 (0.0542)	Acc@1 98.828 (98.350)	Acc@5 100.000 (100.000)
Epoch: [343][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0802 (0.0557)	Acc@1 97.656 (98.272)	Acc@5 100.000 (99.996)
Max memory in training epoch: 33.3541888
lr: 3.090315438263264e-05
args.lr: 3.090315438263264e-05
lr: 3.090315438263264e-05
Epoche:344/345; Lr: 3.090315438263264e-05
batch Size 256
Epoch: [344][0/196]	Time 0.140 (0.140)	Data 0.280 (0.280)	Loss 0.0612 (0.0612)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [344][64/196]	Time 0.097 (0.091)	Data 0.000 (0.005)	Loss 0.0721 (0.0577)	Acc@1 97.656 (98.125)	Acc@5 100.000 (99.994)
Epoch: [344][128/196]	Time 0.095 (0.091)	Data 0.000 (0.002)	Loss 0.0720 (0.0577)	Acc@1 96.875 (98.126)	Acc@5 100.000 (99.997)
Epoch: [344][192/196]	Time 0.092 (0.090)	Data 0.000 (0.002)	Loss 0.0296 (0.0579)	Acc@1 99.609 (98.126)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 3.090315438263264e-05
args.lr: 2.7812838944369376e-05
lr: 2.7812838944369376e-05
Epoche:345/345; Lr: 3.090315438263264e-05
batch Size 256
Epoch: [345][0/196]	Time 0.137 (0.137)	Data 0.286 (0.286)	Loss 0.0603 (0.0603)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [345][64/196]	Time 0.091 (0.090)	Data 0.000 (0.005)	Loss 0.0718 (0.0591)	Acc@1 97.266 (98.221)	Acc@5 100.000 (99.970)
Epoch: [345][128/196]	Time 0.112 (0.090)	Data 0.000 (0.002)	Loss 0.0449 (0.0564)	Acc@1 98.438 (98.271)	Acc@5 100.000 (99.982)
Epoch: [345][192/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.0619 (0.0567)	Acc@1 98.047 (98.243)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.08
Max memory: 51.4381312
 17.811s  j: 351 bis 355
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3049
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 346
Max memory: 0.1097216
lr: 2.7812838944369376e-05
args.lr: 2.7812838944369376e-05
lr: 2.7812838944369376e-05
Epoche:346/350; Lr: 2.7812838944369376e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [346][0/196]	Time 0.146 (0.146)	Data 0.313 (0.313)	Loss 0.0350 (0.0350)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [346][64/196]	Time 0.081 (0.090)	Data 0.000 (0.005)	Loss 0.0469 (0.0576)	Acc@1 97.266 (98.221)	Acc@5 100.000 (99.988)
Epoch: [346][128/196]	Time 0.091 (0.089)	Data 0.000 (0.003)	Loss 0.0310 (0.0575)	Acc@1 100.000 (98.226)	Acc@5 100.000 (99.991)
Epoch: [346][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0587 (0.0569)	Acc@1 97.266 (98.235)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 2.7812838944369376e-05
args.lr: 2.7812838944369376e-05
lr: 2.7812838944369376e-05
Epoche:347/350; Lr: 2.7812838944369376e-05
batch Size 256
Epoch: [347][0/196]	Time 0.113 (0.113)	Data 0.300 (0.300)	Loss 0.0736 (0.0736)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [347][64/196]	Time 0.082 (0.087)	Data 0.000 (0.005)	Loss 0.0304 (0.0552)	Acc@1 99.219 (98.329)	Acc@5 100.000 (99.982)
Epoch: [347][128/196]	Time 0.089 (0.087)	Data 0.000 (0.003)	Loss 0.0455 (0.0580)	Acc@1 98.438 (98.250)	Acc@5 100.000 (99.985)
Epoch: [347][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0434 (0.0568)	Acc@1 98.828 (98.296)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 2.7812838944369376e-05
args.lr: 2.7812838944369376e-05
lr: 2.7812838944369376e-05
Epoche:348/350; Lr: 2.7812838944369376e-05
batch Size 256
Epoch: [348][0/196]	Time 0.129 (0.129)	Data 0.286 (0.286)	Loss 0.0509 (0.0509)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [348][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0579 (0.0549)	Acc@1 98.047 (98.353)	Acc@5 100.000 (99.994)
Epoch: [348][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0541 (0.0565)	Acc@1 98.047 (98.259)	Acc@5 100.000 (99.997)
Epoch: [348][192/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.0621 (0.0576)	Acc@1 97.656 (98.199)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 2.7812838944369376e-05
args.lr: 2.7812838944369376e-05
lr: 2.7812838944369376e-05
Epoche:349/350; Lr: 2.7812838944369376e-05
batch Size 256
Epoch: [349][0/196]	Time 0.126 (0.126)	Data 0.313 (0.313)	Loss 0.0667 (0.0667)	Acc@1 98.047 (98.047)	Acc@5 99.609 (99.609)
Epoch: [349][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0505 (0.0563)	Acc@1 98.047 (98.305)	Acc@5 100.000 (99.988)
Epoch: [349][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0634 (0.0566)	Acc@1 96.875 (98.283)	Acc@5 100.000 (99.985)
Epoch: [349][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0920 (0.0574)	Acc@1 96.875 (98.265)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 2.7812838944369376e-05
args.lr: 2.503155504993244e-05
lr: 2.503155504993244e-05
Epoche:350/350; Lr: 2.7812838944369376e-05
batch Size 256
Epoch: [350][0/196]	Time 0.115 (0.115)	Data 0.311 (0.311)	Loss 0.0746 (0.0746)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [350][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0400 (0.0610)	Acc@1 98.438 (98.065)	Acc@5 100.000 (99.988)
Epoch: [350][128/196]	Time 0.094 (0.087)	Data 0.000 (0.003)	Loss 0.0550 (0.0583)	Acc@1 97.266 (98.229)	Acc@5 100.000 (99.994)
Epoch: [350][192/196]	Time 0.095 (0.087)	Data 0.000 (0.002)	Loss 0.0799 (0.0573)	Acc@1 96.094 (98.257)	Acc@5 100.000 (99.996)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.06
Max memory: 51.4381312
 17.440s  j: 356 bis 360
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9910
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 351
Max memory: 0.1097216
lr: 2.503155504993244e-05
args.lr: 2.503155504993244e-05
lr: 2.503155504993244e-05
Epoche:351/355; Lr: 2.503155504993244e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [351][0/196]	Time 0.163 (0.163)	Data 0.322 (0.322)	Loss 0.0513 (0.0513)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [351][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.0661 (0.0565)	Acc@1 98.438 (98.281)	Acc@5 100.000 (100.000)
Epoch: [351][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0908 (0.0572)	Acc@1 96.875 (98.223)	Acc@5 99.609 (99.997)
Epoch: [351][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.0452 (0.0562)	Acc@1 98.438 (98.255)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 2.503155504993244e-05
args.lr: 2.503155504993244e-05
lr: 2.503155504993244e-05
Epoche:352/355; Lr: 2.503155504993244e-05
batch Size 256
Epoch: [352][0/196]	Time 0.133 (0.133)	Data 0.298 (0.298)	Loss 0.0573 (0.0573)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [352][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.0562 (0.0576)	Acc@1 98.438 (98.251)	Acc@5 100.000 (99.994)
Epoch: [352][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0837 (0.0589)	Acc@1 97.266 (98.171)	Acc@5 100.000 (99.985)
Epoch: [352][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0582 (0.0585)	Acc@1 98.438 (98.209)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 2.503155504993244e-05
args.lr: 2.503155504993244e-05
lr: 2.503155504993244e-05
Epoche:353/355; Lr: 2.503155504993244e-05
batch Size 256
Epoch: [353][0/196]	Time 0.130 (0.130)	Data 0.277 (0.277)	Loss 0.0990 (0.0990)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [353][64/196]	Time 0.082 (0.088)	Data 0.000 (0.004)	Loss 0.0657 (0.0569)	Acc@1 98.438 (98.293)	Acc@5 100.000 (99.982)
Epoch: [353][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0561 (0.0556)	Acc@1 97.656 (98.286)	Acc@5 100.000 (99.985)
Epoch: [353][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.0440 (0.0561)	Acc@1 98.047 (98.247)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 2.503155504993244e-05
args.lr: 2.503155504993244e-05
lr: 2.503155504993244e-05
Epoche:354/355; Lr: 2.503155504993244e-05
batch Size 256
Epoch: [354][0/196]	Time 0.119 (0.119)	Data 0.315 (0.315)	Loss 0.0516 (0.0516)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [354][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0833 (0.0555)	Acc@1 96.875 (98.305)	Acc@5 100.000 (99.988)
Epoch: [354][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0547 (0.0554)	Acc@1 98.047 (98.280)	Acc@5 100.000 (99.988)
Epoch: [354][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0268 (0.0563)	Acc@1 99.609 (98.286)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 2.503155504993244e-05
args.lr: 2.2528399544939195e-05
lr: 2.2528399544939195e-05
Epoche:355/355; Lr: 2.503155504993244e-05
batch Size 256
Epoch: [355][0/196]	Time 0.136 (0.136)	Data 0.275 (0.275)	Loss 0.0540 (0.0540)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [355][64/196]	Time 0.091 (0.088)	Data 0.000 (0.004)	Loss 0.0768 (0.0585)	Acc@1 97.266 (98.191)	Acc@5 100.000 (99.988)
Epoch: [355][128/196]	Time 0.094 (0.088)	Data 0.000 (0.002)	Loss 0.0407 (0.0576)	Acc@1 99.219 (98.213)	Acc@5 100.000 (99.988)
Epoch: [355][192/196]	Time 0.098 (0.088)	Data 0.000 (0.002)	Loss 0.0461 (0.0585)	Acc@1 99.219 (98.164)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.07
Max memory: 51.4381312
 17.563s  Net2Net 3
j: 1 bis 5
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5646
Files already downloaded and verified
width: 8

Arch Num:  [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
Max memory: 0.0573952
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:1/5; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [1][0/196]	Time 0.156 (0.156)	Data 0.300 (0.300)	Loss 2.5260 (2.5260)	Acc@1 11.328 (11.328)	Acc@5 51.562 (51.562)
Epoch: [1][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 1.7685 (2.0473)	Acc@1 34.375 (22.921)	Acc@5 86.328 (74.609)
Epoch: [1][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 1.5901 (1.8737)	Acc@1 39.453 (28.982)	Acc@5 89.844 (81.335)
Epoch: [1][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 1.4717 (1.7524)	Acc@1 43.359 (33.835)	Acc@5 91.797 (84.685)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:2/5; Lr: 0.1
batch Size 256
Epoch: [2][0/196]	Time 0.126 (0.126)	Data 0.316 (0.316)	Loss 1.5091 (1.5091)	Acc@1 47.266 (47.266)	Acc@5 92.188 (92.188)
Epoch: [2][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 1.2662 (1.3420)	Acc@1 55.078 (51.292)	Acc@5 94.922 (93.702)
Epoch: [2][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 1.1978 (1.2891)	Acc@1 55.469 (53.028)	Acc@5 93.359 (94.210)
Epoch: [2][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 1.1404 (1.2397)	Acc@1 58.203 (54.993)	Acc@5 96.875 (94.558)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:3/5; Lr: 0.1
batch Size 256
Epoch: [3][0/196]	Time 0.128 (0.128)	Data 0.280 (0.280)	Loss 0.8829 (0.8829)	Acc@1 70.703 (70.703)	Acc@5 98.438 (98.438)
Epoch: [3][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 1.0372 (1.0628)	Acc@1 62.891 (61.761)	Acc@5 97.266 (96.298)
Epoch: [3][128/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 1.0375 (1.0372)	Acc@1 60.938 (62.664)	Acc@5 98.047 (96.457)
Epoch: [3][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.9398 (1.0091)	Acc@1 70.312 (63.783)	Acc@5 94.922 (96.707)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:4/5; Lr: 0.1
batch Size 256
Epoch: [4][0/196]	Time 0.123 (0.123)	Data 0.305 (0.305)	Loss 0.9484 (0.9484)	Acc@1 67.578 (67.578)	Acc@5 95.703 (95.703)
Epoch: [4][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 1.0687 (0.9098)	Acc@1 60.938 (67.434)	Acc@5 98.047 (97.536)
Epoch: [4][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.8916 (0.8888)	Acc@1 66.797 (68.384)	Acc@5 97.656 (97.532)
Epoch: [4][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.9209 (0.8780)	Acc@1 68.750 (68.803)	Acc@5 96.484 (97.551)
Max memory in training epoch: 33.3018624
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:5/5; Lr: 0.1
batch Size 256
Epoch: [5][0/196]	Time 0.104 (0.104)	Data 0.298 (0.298)	Loss 0.7913 (0.7913)	Acc@1 72.266 (72.266)	Acc@5 98.047 (98.047)
Epoch: [5][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.8889 (0.7938)	Acc@1 67.578 (72.194)	Acc@5 97.266 (98.029)
Epoch: [5][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.7441 (0.8008)	Acc@1 74.219 (72.123)	Acc@5 97.266 (97.917)
Epoch: [5][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.9190 (0.7941)	Acc@1 66.406 (72.312)	Acc@5 96.094 (97.929)
Max memory in training epoch: 33.3018624
[INFO] Storing checkpoint...
  70.5
Max memory: 51.3858048
 17.400s  j: 6 bis 10
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6560
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 6
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:6/10; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [6][0/196]	Time 0.153 (0.153)	Data 0.300 (0.300)	Loss 0.6855 (0.6855)	Acc@1 78.125 (78.125)	Acc@5 97.656 (97.656)
Epoch: [6][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.7193 (0.7432)	Acc@1 72.266 (74.309)	Acc@5 99.219 (98.125)
Epoch: [6][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.6958 (0.7401)	Acc@1 78.125 (74.225)	Acc@5 99.219 (98.129)
Epoch: [6][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.7346 (0.7362)	Acc@1 75.781 (74.401)	Acc@5 99.219 (98.182)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:7/10; Lr: 0.1
batch Size 256
Epoch: [7][0/196]	Time 0.105 (0.105)	Data 0.316 (0.316)	Loss 0.7108 (0.7108)	Acc@1 75.000 (75.000)	Acc@5 97.266 (97.266)
Epoch: [7][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.7123 (0.7014)	Acc@1 75.000 (75.607)	Acc@5 97.266 (98.233)
Epoch: [7][128/196]	Time 0.097 (0.088)	Data 0.000 (0.003)	Loss 0.7104 (0.6996)	Acc@1 74.219 (75.624)	Acc@5 99.219 (98.332)
Epoch: [7][192/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 0.8149 (0.7034)	Acc@1 69.922 (75.514)	Acc@5 98.438 (98.344)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:8/10; Lr: 0.1
batch Size 256
Epoch: [8][0/196]	Time 0.121 (0.121)	Data 0.311 (0.311)	Loss 0.7237 (0.7237)	Acc@1 77.734 (77.734)	Acc@5 98.438 (98.438)
Epoch: [8][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.6241 (0.6886)	Acc@1 76.562 (76.172)	Acc@5 98.438 (98.395)
Epoch: [8][128/196]	Time 0.079 (0.088)	Data 0.000 (0.003)	Loss 0.6877 (0.6701)	Acc@1 75.391 (76.793)	Acc@5 99.219 (98.510)
Epoch: [8][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.6603 (0.6621)	Acc@1 76.953 (77.093)	Acc@5 98.828 (98.563)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:9/10; Lr: 0.1
batch Size 256
Epoch: [9][0/196]	Time 0.127 (0.127)	Data 0.268 (0.268)	Loss 0.6383 (0.6383)	Acc@1 78.125 (78.125)	Acc@5 97.266 (97.266)
Epoch: [9][64/196]	Time 0.084 (0.088)	Data 0.000 (0.004)	Loss 0.4661 (0.6266)	Acc@1 84.375 (78.444)	Acc@5 99.219 (98.708)
Epoch: [9][128/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.7065 (0.6314)	Acc@1 73.047 (78.131)	Acc@5 99.609 (98.707)
Epoch: [9][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.6285 (0.6371)	Acc@1 80.078 (77.874)	Acc@5 98.828 (98.753)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:10/10; Lr: 0.1
batch Size 256
Epoch: [10][0/196]	Time 0.130 (0.130)	Data 0.272 (0.272)	Loss 0.6596 (0.6596)	Acc@1 79.297 (79.297)	Acc@5 98.047 (98.047)
Epoch: [10][64/196]	Time 0.100 (0.088)	Data 0.000 (0.004)	Loss 0.6226 (0.6045)	Acc@1 79.297 (78.798)	Acc@5 98.828 (98.894)
Epoch: [10][128/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.6128 (0.6131)	Acc@1 75.391 (78.709)	Acc@5 100.000 (98.801)
Epoch: [10][192/196]	Time 0.085 (0.086)	Data 0.000 (0.002)	Loss 0.6539 (0.6138)	Acc@1 77.734 (78.692)	Acc@5 98.047 (98.771)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  65.76
Max memory: 51.4381312
 17.154s  j: 11 bis 15
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6672
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 11
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:11/15; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [11][0/196]	Time 0.151 (0.151)	Data 0.280 (0.280)	Loss 0.5541 (0.5541)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [11][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.6339 (0.5866)	Acc@1 78.125 (79.736)	Acc@5 97.656 (98.762)
Epoch: [11][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.6482 (0.5980)	Acc@1 78.125 (79.261)	Acc@5 98.828 (98.786)
Epoch: [11][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.5863 (0.5952)	Acc@1 81.250 (79.349)	Acc@5 98.438 (98.816)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:12/15; Lr: 0.1
batch Size 256
Epoch: [12][0/196]	Time 0.137 (0.137)	Data 0.278 (0.278)	Loss 0.4708 (0.4708)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [12][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.5038 (0.5917)	Acc@1 82.812 (79.453)	Acc@5 99.609 (98.978)
Epoch: [12][128/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.5710 (0.5853)	Acc@1 79.688 (79.669)	Acc@5 98.438 (98.901)
Epoch: [12][192/196]	Time 0.097 (0.088)	Data 0.000 (0.002)	Loss 0.6341 (0.5869)	Acc@1 75.391 (79.580)	Acc@5 99.609 (98.869)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:13/15; Lr: 0.1
batch Size 256
Epoch: [13][0/196]	Time 0.141 (0.141)	Data 0.294 (0.294)	Loss 0.6259 (0.6259)	Acc@1 77.734 (77.734)	Acc@5 98.828 (98.828)
Epoch: [13][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.4943 (0.5706)	Acc@1 82.031 (80.168)	Acc@5 100.000 (98.942)
Epoch: [13][128/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.5845 (0.5716)	Acc@1 80.078 (80.163)	Acc@5 98.438 (98.973)
Epoch: [13][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.5728 (0.5713)	Acc@1 78.125 (80.222)	Acc@5 98.047 (98.954)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:14/15; Lr: 0.1
batch Size 256
Epoch: [14][0/196]	Time 0.123 (0.123)	Data 0.348 (0.348)	Loss 0.4971 (0.4971)	Acc@1 81.641 (81.641)	Acc@5 99.609 (99.609)
Epoch: [14][64/196]	Time 0.088 (0.089)	Data 0.000 (0.006)	Loss 0.5828 (0.5654)	Acc@1 78.906 (80.553)	Acc@5 98.047 (98.870)
Epoch: [14][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.5772 (0.5660)	Acc@1 77.734 (80.420)	Acc@5 99.219 (98.934)
Epoch: [14][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.4223 (0.5654)	Acc@1 84.375 (80.455)	Acc@5 99.219 (98.956)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:15/15; Lr: 0.1
batch Size 256
Epoch: [15][0/196]	Time 0.130 (0.130)	Data 0.293 (0.293)	Loss 0.4816 (0.4816)	Acc@1 82.812 (82.812)	Acc@5 99.609 (99.609)
Epoch: [15][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.5049 (0.5416)	Acc@1 83.203 (81.310)	Acc@5 99.219 (99.123)
Epoch: [15][128/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.5193 (0.5450)	Acc@1 82.812 (81.323)	Acc@5 98.828 (99.001)
Epoch: [15][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.5693 (0.5448)	Acc@1 77.734 (81.327)	Acc@5 99.219 (99.022)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  77.78
Max memory: 51.4381312
 17.467s  j: 16 bis 20
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8658
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 16
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:16/20; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [16][0/196]	Time 0.134 (0.134)	Data 0.287 (0.287)	Loss 0.5673 (0.5673)	Acc@1 81.641 (81.641)	Acc@5 97.656 (97.656)
Epoch: [16][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.6312 (0.5238)	Acc@1 76.953 (81.725)	Acc@5 99.609 (99.056)
Epoch: [16][128/196]	Time 0.094 (0.088)	Data 0.000 (0.002)	Loss 0.4622 (0.5300)	Acc@1 83.203 (81.538)	Acc@5 100.000 (99.052)
Epoch: [16][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.6344 (0.5321)	Acc@1 78.125 (81.507)	Acc@5 99.219 (99.031)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:17/20; Lr: 0.1
batch Size 256
Epoch: [17][0/196]	Time 0.145 (0.145)	Data 0.284 (0.284)	Loss 0.5378 (0.5378)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [17][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.5082 (0.5336)	Acc@1 83.594 (81.490)	Acc@5 99.219 (99.129)
Epoch: [17][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.5322 (0.5333)	Acc@1 81.250 (81.507)	Acc@5 98.828 (99.101)
Epoch: [17][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.4310 (0.5309)	Acc@1 85.938 (81.545)	Acc@5 99.219 (99.148)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:18/20; Lr: 0.1
batch Size 256
Epoch: [18][0/196]	Time 0.155 (0.155)	Data 0.311 (0.311)	Loss 0.5199 (0.5199)	Acc@1 83.594 (83.594)	Acc@5 98.828 (98.828)
Epoch: [18][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.4321 (0.5203)	Acc@1 84.375 (82.055)	Acc@5 98.828 (99.105)
Epoch: [18][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.5260 (0.5282)	Acc@1 84.766 (81.719)	Acc@5 98.828 (99.098)
Epoch: [18][192/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.7230 (0.5242)	Acc@1 75.391 (81.928)	Acc@5 99.609 (99.152)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:19/20; Lr: 0.1
batch Size 256
Epoch: [19][0/196]	Time 0.127 (0.127)	Data 0.284 (0.284)	Loss 0.3943 (0.3943)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [19][64/196]	Time 0.094 (0.089)	Data 0.000 (0.005)	Loss 0.5140 (0.5241)	Acc@1 83.203 (81.815)	Acc@5 99.609 (99.135)
Epoch: [19][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.5055 (0.5131)	Acc@1 80.078 (82.116)	Acc@5 99.609 (99.146)
Epoch: [19][192/196]	Time 0.097 (0.087)	Data 0.000 (0.002)	Loss 0.5164 (0.5164)	Acc@1 81.250 (81.973)	Acc@5 99.609 (99.109)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:20/20; Lr: 0.1
batch Size 256
Epoch: [20][0/196]	Time 0.115 (0.115)	Data 0.301 (0.301)	Loss 0.4307 (0.4307)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [20][64/196]	Time 0.080 (0.084)	Data 0.000 (0.005)	Loss 0.4642 (0.5021)	Acc@1 85.156 (82.933)	Acc@5 99.219 (99.141)
Epoch: [20][128/196]	Time 0.081 (0.084)	Data 0.000 (0.003)	Loss 0.4204 (0.5085)	Acc@1 86.719 (82.534)	Acc@5 99.609 (99.201)
Epoch: [20][192/196]	Time 0.087 (0.085)	Data 0.000 (0.002)	Loss 0.4838 (0.5088)	Acc@1 84.375 (82.507)	Acc@5 98.438 (99.194)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  72.47
Max memory: 51.4381312
 17.066s  j: 21 bis 25
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6692
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 21
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:21/25; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [21][0/196]	Time 0.127 (0.127)	Data 0.303 (0.303)	Loss 0.4530 (0.4530)	Acc@1 83.984 (83.984)	Acc@5 100.000 (100.000)
Epoch: [21][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.5791 (0.4790)	Acc@1 82.422 (83.576)	Acc@5 98.828 (99.135)
Epoch: [21][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.5431 (0.4939)	Acc@1 84.375 (83.037)	Acc@5 98.828 (99.104)
Epoch: [21][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.5097 (0.4980)	Acc@1 82.422 (82.873)	Acc@5 99.609 (99.134)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:22/25; Lr: 0.1
batch Size 256
Epoch: [22][0/196]	Time 0.107 (0.107)	Data 0.323 (0.323)	Loss 0.3464 (0.3464)	Acc@1 88.672 (88.672)	Acc@5 99.219 (99.219)
Epoch: [22][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.5069 (0.5108)	Acc@1 82.422 (82.206)	Acc@5 98.828 (99.135)
Epoch: [22][128/196]	Time 0.091 (0.089)	Data 0.000 (0.003)	Loss 0.5470 (0.5017)	Acc@1 82.031 (82.528)	Acc@5 98.828 (99.137)
Epoch: [22][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.4385 (0.5017)	Acc@1 85.156 (82.600)	Acc@5 100.000 (99.136)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:23/25; Lr: 0.1
batch Size 256
Epoch: [23][0/196]	Time 0.136 (0.136)	Data 0.270 (0.270)	Loss 0.5125 (0.5125)	Acc@1 81.641 (81.641)	Acc@5 100.000 (100.000)
Epoch: [23][64/196]	Time 0.097 (0.089)	Data 0.000 (0.004)	Loss 0.4568 (0.4938)	Acc@1 85.156 (82.951)	Acc@5 99.219 (99.243)
Epoch: [23][128/196]	Time 0.092 (0.089)	Data 0.000 (0.002)	Loss 0.5107 (0.4903)	Acc@1 82.812 (83.124)	Acc@5 98.438 (99.170)
Epoch: [23][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.4195 (0.4906)	Acc@1 85.156 (83.122)	Acc@5 99.609 (99.164)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:24/25; Lr: 0.1
batch Size 256
Epoch: [24][0/196]	Time 0.147 (0.147)	Data 0.286 (0.286)	Loss 0.4286 (0.4286)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [24][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.5417 (0.4898)	Acc@1 80.078 (82.981)	Acc@5 99.609 (99.225)
Epoch: [24][128/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.5818 (0.4920)	Acc@1 80.859 (83.003)	Acc@5 98.828 (99.170)
Epoch: [24][192/196]	Time 0.093 (0.089)	Data 0.000 (0.002)	Loss 0.6329 (0.4965)	Acc@1 76.953 (82.944)	Acc@5 98.438 (99.164)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:25/25; Lr: 0.1
batch Size 256
Epoch: [25][0/196]	Time 0.126 (0.126)	Data 0.285 (0.285)	Loss 0.5036 (0.5036)	Acc@1 83.594 (83.594)	Acc@5 99.609 (99.609)
Epoch: [25][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.5705 (0.4945)	Acc@1 78.906 (83.173)	Acc@5 99.609 (99.285)
Epoch: [25][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.5939 (0.4861)	Acc@1 80.078 (83.288)	Acc@5 98.828 (99.273)
Epoch: [25][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.4339 (0.4903)	Acc@1 83.594 (83.155)	Acc@5 100.000 (99.219)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  76.86
Max memory: 51.4381312
 17.495s  j: 26 bis 30
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 922
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 26
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:26/30; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [26][0/196]	Time 0.139 (0.139)	Data 0.311 (0.311)	Loss 0.5234 (0.5234)	Acc@1 82.812 (82.812)	Acc@5 97.656 (97.656)
Epoch: [26][64/196]	Time 0.080 (0.090)	Data 0.000 (0.005)	Loss 0.5113 (0.4702)	Acc@1 83.203 (83.978)	Acc@5 98.438 (99.237)
Epoch: [26][128/196]	Time 0.080 (0.089)	Data 0.000 (0.003)	Loss 0.5225 (0.4786)	Acc@1 85.156 (83.545)	Acc@5 98.438 (99.179)
Epoch: [26][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.6003 (0.4801)	Acc@1 81.250 (83.507)	Acc@5 99.219 (99.166)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:27/30; Lr: 0.1
batch Size 256
Epoch: [27][0/196]	Time 0.120 (0.120)	Data 0.297 (0.297)	Loss 0.4820 (0.4820)	Acc@1 80.859 (80.859)	Acc@5 98.828 (98.828)
Epoch: [27][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.5267 (0.4650)	Acc@1 83.594 (84.062)	Acc@5 99.219 (99.225)
Epoch: [27][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.4882 (0.4748)	Acc@1 83.594 (83.778)	Acc@5 99.219 (99.207)
Epoch: [27][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.4767 (0.4783)	Acc@1 84.375 (83.501)	Acc@5 99.219 (99.225)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:28/30; Lr: 0.1
batch Size 256
Epoch: [28][0/196]	Time 0.114 (0.114)	Data 0.279 (0.279)	Loss 0.4035 (0.4035)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [28][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.4666 (0.4763)	Acc@1 81.641 (83.588)	Acc@5 99.609 (99.285)
Epoch: [28][128/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.4938 (0.4707)	Acc@1 80.469 (83.757)	Acc@5 99.609 (99.322)
Epoch: [28][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.4992 (0.4769)	Acc@1 84.375 (83.472)	Acc@5 99.609 (99.318)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:29/30; Lr: 0.1
batch Size 256
Epoch: [29][0/196]	Time 0.128 (0.128)	Data 0.345 (0.345)	Loss 0.3969 (0.3969)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [29][64/196]	Time 0.086 (0.088)	Data 0.000 (0.006)	Loss 0.4646 (0.4645)	Acc@1 82.812 (84.002)	Acc@5 100.000 (99.297)
Epoch: [29][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.4788 (0.4716)	Acc@1 83.984 (83.830)	Acc@5 98.828 (99.228)
Epoch: [29][192/196]	Time 0.086 (0.086)	Data 0.000 (0.002)	Loss 0.4673 (0.4748)	Acc@1 83.203 (83.741)	Acc@5 98.047 (99.211)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:30/30; Lr: 0.1
batch Size 256
Epoch: [30][0/196]	Time 0.108 (0.108)	Data 0.287 (0.287)	Loss 0.4041 (0.4041)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [30][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.4114 (0.4743)	Acc@1 85.547 (83.720)	Acc@5 99.219 (99.315)
Epoch: [30][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.4330 (0.4740)	Acc@1 85.938 (83.824)	Acc@5 99.609 (99.261)
Epoch: [30][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.3496 (0.4711)	Acc@1 89.453 (83.827)	Acc@5 100.000 (99.243)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  75.47
Max memory: 51.4381312
 17.497s  j: 31 bis 35
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 422
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 31
Max memory: 0.1097216
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:31/35; Lr: 0.1
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [31][0/196]	Time 0.164 (0.164)	Data 0.276 (0.276)	Loss 0.4174 (0.4174)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [31][64/196]	Time 0.085 (0.089)	Data 0.000 (0.004)	Loss 0.3895 (0.4363)	Acc@1 84.375 (84.772)	Acc@5 99.609 (99.309)
Epoch: [31][128/196]	Time 0.095 (0.088)	Data 0.000 (0.002)	Loss 0.4899 (0.4536)	Acc@1 82.031 (84.239)	Acc@5 99.219 (99.310)
Epoch: [31][192/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 0.4693 (0.4550)	Acc@1 84.766 (84.280)	Acc@5 100.000 (99.312)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:32/35; Lr: 0.1
batch Size 256
Epoch: [32][0/196]	Time 0.135 (0.135)	Data 0.310 (0.310)	Loss 0.4356 (0.4356)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [32][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.4216 (0.4715)	Acc@1 83.594 (83.606)	Acc@5 99.219 (99.369)
Epoch: [32][128/196]	Time 0.102 (0.089)	Data 0.000 (0.003)	Loss 0.4423 (0.4670)	Acc@1 85.547 (83.727)	Acc@5 98.828 (99.358)
Epoch: [32][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.4963 (0.4674)	Acc@1 82.422 (83.855)	Acc@5 98.828 (99.330)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:33/35; Lr: 0.1
batch Size 256
Epoch: [33][0/196]	Time 0.120 (0.120)	Data 0.349 (0.349)	Loss 0.3220 (0.3220)	Acc@1 90.234 (90.234)	Acc@5 99.219 (99.219)
Epoch: [33][64/196]	Time 0.088 (0.088)	Data 0.000 (0.006)	Loss 0.4827 (0.4631)	Acc@1 83.984 (84.081)	Acc@5 98.438 (99.411)
Epoch: [33][128/196]	Time 0.089 (0.087)	Data 0.000 (0.003)	Loss 0.4920 (0.4560)	Acc@1 83.594 (84.357)	Acc@5 99.219 (99.376)
Epoch: [33][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.4840 (0.4613)	Acc@1 82.031 (84.160)	Acc@5 99.219 (99.377)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.1
lr: 0.1
1
Epoche:34/35; Lr: 0.1
batch Size 256
Epoch: [34][0/196]	Time 0.121 (0.121)	Data 0.309 (0.309)	Loss 0.4684 (0.4684)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [34][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.4682 (0.4535)	Acc@1 83.203 (84.459)	Acc@5 99.609 (99.261)
Epoch: [34][128/196]	Time 0.085 (0.086)	Data 0.000 (0.003)	Loss 0.4330 (0.4587)	Acc@1 87.500 (84.333)	Acc@5 98.828 (99.282)
Epoch: [34][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.6537 (0.4555)	Acc@1 79.297 (84.415)	Acc@5 98.438 (99.302)
Max memory in training epoch: 33.3541888
lr: 0.1
args.lr: 0.09000000000000001
lr: 0.09000000000000001
1
Epoche:35/35; Lr: 0.1
batch Size 256
Epoch: [35][0/196]	Time 0.129 (0.129)	Data 0.283 (0.283)	Loss 0.3433 (0.3433)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [35][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.3116 (0.4449)	Acc@1 87.109 (84.591)	Acc@5 100.000 (99.225)
Epoch: [35][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.4140 (0.4451)	Acc@1 86.719 (84.732)	Acc@5 99.609 (99.264)
Epoch: [35][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.5189 (0.4473)	Acc@1 80.859 (84.733)	Acc@5 99.219 (99.269)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  78.59
Max memory: 51.4381312
 17.573s  j: 36 bis 40
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1344
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 36
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:36/40; Lr: 0.1
batch Size 256
Epoch: [36][0/196]	Time 0.140 (0.140)	Data 0.302 (0.302)	Loss 0.4027 (0.4027)	Acc@1 86.328 (86.328)	Acc@5 99.609 (99.609)
Epoch: [36][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.5232 (0.4353)	Acc@1 84.766 (85.030)	Acc@5 98.828 (99.297)
Epoch: [36][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.4665 (0.4493)	Acc@1 85.547 (84.587)	Acc@5 98.438 (99.243)
Epoch: [36][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.4738 (0.4556)	Acc@1 85.547 (84.318)	Acc@5 99.609 (99.284)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:37/40; Lr: 0.1
batch Size 256
Epoch: [37][0/196]	Time 0.121 (0.121)	Data 0.292 (0.292)	Loss 0.4220 (0.4220)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [37][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.5190 (0.4431)	Acc@1 84.375 (84.916)	Acc@5 98.828 (99.375)
Epoch: [37][128/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.5552 (0.4441)	Acc@1 82.031 (84.726)	Acc@5 98.828 (99.388)
Epoch: [37][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.3860 (0.4487)	Acc@1 84.375 (84.472)	Acc@5 99.609 (99.369)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:38/40; Lr: 0.1
batch Size 256
Epoch: [38][0/196]	Time 0.130 (0.130)	Data 0.312 (0.312)	Loss 0.3682 (0.3682)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [38][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.4718 (0.4571)	Acc@1 82.812 (84.056)	Acc@5 100.000 (99.261)
Epoch: [38][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.4401 (0.4550)	Acc@1 83.203 (84.257)	Acc@5 99.609 (99.270)
Epoch: [38][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.5365 (0.4571)	Acc@1 81.641 (84.203)	Acc@5 100.000 (99.298)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:39/40; Lr: 0.1
batch Size 256
Epoch: [39][0/196]	Time 0.105 (0.105)	Data 0.278 (0.278)	Loss 0.4637 (0.4637)	Acc@1 83.203 (83.203)	Acc@5 100.000 (100.000)
Epoch: [39][64/196]	Time 0.083 (0.090)	Data 0.000 (0.004)	Loss 0.5222 (0.4496)	Acc@1 80.469 (84.549)	Acc@5 99.219 (99.321)
Epoch: [39][128/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.4492 (0.4450)	Acc@1 85.938 (84.723)	Acc@5 98.438 (99.343)
Epoch: [39][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.5836 (0.4526)	Acc@1 78.125 (84.393)	Acc@5 100.000 (99.328)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:40/40; Lr: 0.1
batch Size 256
Epoch: [40][0/196]	Time 0.110 (0.110)	Data 0.272 (0.272)	Loss 0.3620 (0.3620)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [40][64/196]	Time 0.088 (0.090)	Data 0.000 (0.004)	Loss 0.3759 (0.4333)	Acc@1 87.891 (85.264)	Acc@5 99.219 (99.321)
Epoch: [40][128/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.4504 (0.4318)	Acc@1 86.719 (85.205)	Acc@5 98.438 (99.328)
Epoch: [40][192/196]	Time 0.094 (0.088)	Data 0.000 (0.002)	Loss 0.4195 (0.4407)	Acc@1 85.938 (84.893)	Acc@5 99.609 (99.330)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  79.37
Max memory: 51.4381312
 17.614s  j: 41 bis 45
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8570
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 41
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:41/45; Lr: 0.1
batch Size 256
Epoch: [41][0/196]	Time 0.164 (0.164)	Data 0.285 (0.285)	Loss 0.4734 (0.4734)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [41][64/196]	Time 0.082 (0.087)	Data 0.000 (0.005)	Loss 0.4649 (0.4234)	Acc@1 83.594 (85.565)	Acc@5 99.219 (99.387)
Epoch: [41][128/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.4733 (0.4294)	Acc@1 85.156 (85.420)	Acc@5 98.828 (99.358)
Epoch: [41][192/196]	Time 0.089 (0.086)	Data 0.000 (0.002)	Loss 0.4283 (0.4344)	Acc@1 85.156 (85.172)	Acc@5 99.219 (99.364)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:42/45; Lr: 0.1
batch Size 256
Epoch: [42][0/196]	Time 0.125 (0.125)	Data 0.308 (0.308)	Loss 0.3804 (0.3804)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [42][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.4128 (0.4285)	Acc@1 85.547 (85.216)	Acc@5 99.609 (99.435)
Epoch: [42][128/196]	Time 0.097 (0.088)	Data 0.000 (0.003)	Loss 0.4661 (0.4401)	Acc@1 83.594 (84.869)	Acc@5 98.438 (99.370)
Epoch: [42][192/196]	Time 0.096 (0.087)	Data 0.000 (0.002)	Loss 0.4045 (0.4417)	Acc@1 85.938 (84.731)	Acc@5 99.609 (99.379)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:43/45; Lr: 0.1
batch Size 256
Epoch: [43][0/196]	Time 0.124 (0.124)	Data 0.335 (0.335)	Loss 0.3483 (0.3483)	Acc@1 89.453 (89.453)	Acc@5 99.219 (99.219)
Epoch: [43][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.4387 (0.4398)	Acc@1 85.156 (84.802)	Acc@5 100.000 (99.441)
Epoch: [43][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.4702 (0.4360)	Acc@1 82.812 (84.875)	Acc@5 100.000 (99.449)
Epoch: [43][192/196]	Time 0.088 (0.086)	Data 0.000 (0.002)	Loss 0.4444 (0.4411)	Acc@1 85.938 (84.654)	Acc@5 99.219 (99.383)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:44/45; Lr: 0.1
batch Size 256
Epoch: [44][0/196]	Time 0.128 (0.128)	Data 0.303 (0.303)	Loss 0.3794 (0.3794)	Acc@1 88.672 (88.672)	Acc@5 99.609 (99.609)
Epoch: [44][64/196]	Time 0.086 (0.087)	Data 0.000 (0.005)	Loss 0.4396 (0.4377)	Acc@1 85.547 (85.024)	Acc@5 99.609 (99.279)
Epoch: [44][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.4591 (0.4345)	Acc@1 84.766 (85.244)	Acc@5 98.438 (99.297)
Epoch: [44][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.4004 (0.4344)	Acc@1 85.547 (85.075)	Acc@5 99.609 (99.344)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:45/45; Lr: 0.1
batch Size 256
Epoch: [45][0/196]	Time 0.153 (0.153)	Data 0.274 (0.274)	Loss 0.3766 (0.3766)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [45][64/196]	Time 0.082 (0.087)	Data 0.000 (0.004)	Loss 0.4630 (0.4357)	Acc@1 85.547 (84.844)	Acc@5 98.047 (99.417)
Epoch: [45][128/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.3900 (0.4281)	Acc@1 86.328 (85.299)	Acc@5 100.000 (99.413)
Epoch: [45][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.4986 (0.4334)	Acc@1 80.469 (85.108)	Acc@5 100.000 (99.395)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  78.44
Max memory: 51.4381312
 17.361s  j: 46 bis 50
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 490
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 46
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:46/50; Lr: 0.1
batch Size 256
Epoch: [46][0/196]	Time 0.150 (0.150)	Data 0.318 (0.318)	Loss 0.4180 (0.4180)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [46][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.4197 (0.4139)	Acc@1 85.547 (85.709)	Acc@5 99.609 (99.447)
Epoch: [46][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.4182 (0.4235)	Acc@1 87.109 (85.462)	Acc@5 100.000 (99.425)
Epoch: [46][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.4075 (0.4276)	Acc@1 85.938 (85.326)	Acc@5 98.828 (99.381)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:47/50; Lr: 0.1
batch Size 256
Epoch: [47][0/196]	Time 0.136 (0.136)	Data 0.316 (0.316)	Loss 0.4024 (0.4024)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [47][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.4327 (0.4265)	Acc@1 83.594 (85.222)	Acc@5 99.609 (99.465)
Epoch: [47][128/196]	Time 0.081 (0.088)	Data 0.000 (0.003)	Loss 0.3576 (0.4342)	Acc@1 89.453 (85.035)	Acc@5 100.000 (99.413)
Epoch: [47][192/196]	Time 0.093 (0.087)	Data 0.000 (0.002)	Loss 0.5135 (0.4363)	Acc@1 78.906 (84.952)	Acc@5 99.219 (99.391)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:48/50; Lr: 0.1
batch Size 256
Epoch: [48][0/196]	Time 0.132 (0.132)	Data 0.346 (0.346)	Loss 0.4827 (0.4827)	Acc@1 85.547 (85.547)	Acc@5 98.438 (98.438)
Epoch: [48][64/196]	Time 0.085 (0.090)	Data 0.000 (0.006)	Loss 0.5368 (0.4398)	Acc@1 80.859 (85.024)	Acc@5 98.438 (99.363)
Epoch: [48][128/196]	Time 0.085 (0.089)	Data 0.000 (0.003)	Loss 0.4966 (0.4370)	Acc@1 82.422 (84.996)	Acc@5 98.828 (99.382)
Epoch: [48][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.4600 (0.4323)	Acc@1 82.812 (85.087)	Acc@5 99.219 (99.425)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:49/50; Lr: 0.1
batch Size 256
Epoch: [49][0/196]	Time 0.126 (0.126)	Data 0.300 (0.300)	Loss 0.5313 (0.5313)	Acc@1 83.594 (83.594)	Acc@5 99.609 (99.609)
Epoch: [49][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.4661 (0.4292)	Acc@1 84.766 (85.090)	Acc@5 98.438 (99.375)
Epoch: [49][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.3754 (0.4316)	Acc@1 86.719 (84.981)	Acc@5 99.219 (99.376)
Epoch: [49][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.3807 (0.4344)	Acc@1 87.109 (84.982)	Acc@5 99.609 (99.371)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:50/50; Lr: 0.1
batch Size 256
Epoch: [50][0/196]	Time 0.128 (0.128)	Data 0.330 (0.330)	Loss 0.4307 (0.4307)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [50][64/196]	Time 0.098 (0.090)	Data 0.000 (0.005)	Loss 0.3180 (0.4231)	Acc@1 89.062 (85.300)	Acc@5 100.000 (99.417)
Epoch: [50][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.4914 (0.4348)	Acc@1 83.203 (84.923)	Acc@5 98.047 (99.364)
Epoch: [50][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.4560 (0.4268)	Acc@1 84.766 (85.209)	Acc@5 98.828 (99.389)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  75.63
Max memory: 51.4381312
 17.738s  j: 51 bis 55
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8006
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 51
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:51/55; Lr: 0.1
batch Size 256
Epoch: [51][0/196]	Time 0.158 (0.158)	Data 0.297 (0.297)	Loss 0.4001 (0.4001)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [51][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.4901 (0.3887)	Acc@1 82.031 (86.641)	Acc@5 99.609 (99.441)
Epoch: [51][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.4632 (0.4088)	Acc@1 84.766 (85.944)	Acc@5 98.828 (99.410)
Epoch: [51][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.3741 (0.4216)	Acc@1 90.234 (85.533)	Acc@5 100.000 (99.383)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:52/55; Lr: 0.1
batch Size 256
Epoch: [52][0/196]	Time 0.133 (0.133)	Data 0.311 (0.311)	Loss 0.4091 (0.4091)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [52][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.4187 (0.4191)	Acc@1 83.594 (85.673)	Acc@5 99.609 (99.435)
Epoch: [52][128/196]	Time 0.081 (0.087)	Data 0.000 (0.003)	Loss 0.3836 (0.4210)	Acc@1 87.109 (85.610)	Acc@5 99.219 (99.410)
Epoch: [52][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.4039 (0.4201)	Acc@1 86.719 (85.618)	Acc@5 99.219 (99.411)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:53/55; Lr: 0.1
batch Size 256
Epoch: [53][0/196]	Time 0.119 (0.119)	Data 0.320 (0.320)	Loss 0.3488 (0.3488)	Acc@1 87.891 (87.891)	Acc@5 98.828 (98.828)
Epoch: [53][64/196]	Time 0.088 (0.086)	Data 0.000 (0.005)	Loss 0.4005 (0.4042)	Acc@1 85.938 (86.136)	Acc@5 100.000 (99.339)
Epoch: [53][128/196]	Time 0.095 (0.086)	Data 0.000 (0.003)	Loss 0.4518 (0.4166)	Acc@1 83.984 (85.629)	Acc@5 100.000 (99.388)
Epoch: [53][192/196]	Time 0.106 (0.086)	Data 0.000 (0.002)	Loss 0.3849 (0.4281)	Acc@1 85.547 (85.312)	Acc@5 100.000 (99.379)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:54/55; Lr: 0.1
batch Size 256
Epoch: [54][0/196]	Time 0.129 (0.129)	Data 0.314 (0.314)	Loss 0.3597 (0.3597)	Acc@1 89.453 (89.453)	Acc@5 100.000 (100.000)
Epoch: [54][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.3597 (0.4253)	Acc@1 87.109 (85.210)	Acc@5 99.609 (99.387)
Epoch: [54][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.4177 (0.4214)	Acc@1 86.719 (85.426)	Acc@5 98.828 (99.385)
Epoch: [54][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.4582 (0.4264)	Acc@1 85.547 (85.312)	Acc@5 99.219 (99.401)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:55/55; Lr: 0.1
batch Size 256
Epoch: [55][0/196]	Time 0.127 (0.127)	Data 0.315 (0.315)	Loss 0.4406 (0.4406)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [55][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.3464 (0.4250)	Acc@1 87.109 (85.048)	Acc@5 100.000 (99.453)
Epoch: [55][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.4117 (0.4212)	Acc@1 86.328 (85.353)	Acc@5 99.609 (99.449)
Epoch: [55][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.4243 (0.4220)	Acc@1 86.328 (85.373)	Acc@5 99.609 (99.417)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  76.19
Max memory: 51.4381312
 17.563s  j: 56 bis 60
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7894
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 56
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:56/60; Lr: 0.1
batch Size 256
Epoch: [56][0/196]	Time 0.168 (0.168)	Data 0.285 (0.285)	Loss 0.5366 (0.5366)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [56][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.4498 (0.3998)	Acc@1 85.547 (86.472)	Acc@5 100.000 (99.489)
Epoch: [56][128/196]	Time 0.094 (0.089)	Data 0.000 (0.002)	Loss 0.3634 (0.4084)	Acc@1 87.891 (85.959)	Acc@5 100.000 (99.449)
Epoch: [56][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.4658 (0.4103)	Acc@1 83.203 (85.838)	Acc@5 99.219 (99.421)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:57/60; Lr: 0.1
batch Size 256
Epoch: [57][0/196]	Time 0.120 (0.120)	Data 0.316 (0.316)	Loss 0.4408 (0.4408)	Acc@1 87.500 (87.500)	Acc@5 98.828 (98.828)
Epoch: [57][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.3908 (0.4336)	Acc@1 85.547 (85.090)	Acc@5 99.609 (99.417)
Epoch: [57][128/196]	Time 0.097 (0.088)	Data 0.000 (0.003)	Loss 0.3276 (0.4247)	Acc@1 89.844 (85.317)	Acc@5 100.000 (99.352)
Epoch: [57][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.3862 (0.4206)	Acc@1 87.500 (85.494)	Acc@5 99.219 (99.377)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:58/60; Lr: 0.1
batch Size 256
Epoch: [58][0/196]	Time 0.137 (0.137)	Data 0.307 (0.307)	Loss 0.3416 (0.3416)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [58][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.4000 (0.4211)	Acc@1 87.109 (85.457)	Acc@5 99.609 (99.537)
Epoch: [58][128/196]	Time 0.088 (0.089)	Data 0.000 (0.003)	Loss 0.3737 (0.4261)	Acc@1 84.375 (85.196)	Acc@5 99.219 (99.464)
Epoch: [58][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.3806 (0.4229)	Acc@1 85.938 (85.363)	Acc@5 100.000 (99.439)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:59/60; Lr: 0.1
batch Size 256
Epoch: [59][0/196]	Time 0.136 (0.136)	Data 0.309 (0.309)	Loss 0.4385 (0.4385)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [59][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.3748 (0.4186)	Acc@1 87.109 (85.361)	Acc@5 100.000 (99.495)
Epoch: [59][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.5467 (0.4180)	Acc@1 78.516 (85.444)	Acc@5 98.828 (99.479)
Epoch: [59][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.4377 (0.4208)	Acc@1 88.281 (85.413)	Acc@5 97.656 (99.452)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:60/60; Lr: 0.1
batch Size 256
Epoch: [60][0/196]	Time 0.126 (0.126)	Data 0.305 (0.305)	Loss 0.4380 (0.4380)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [60][64/196]	Time 0.096 (0.088)	Data 0.000 (0.005)	Loss 0.4223 (0.4210)	Acc@1 85.938 (85.487)	Acc@5 98.828 (99.405)
Epoch: [60][128/196]	Time 0.097 (0.088)	Data 0.000 (0.003)	Loss 0.4614 (0.4206)	Acc@1 83.984 (85.492)	Acc@5 99.609 (99.406)
Epoch: [60][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.3368 (0.4222)	Acc@1 88.281 (85.419)	Acc@5 100.000 (99.393)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  73.3
Max memory: 51.4381312
 17.589s  j: 61 bis 65
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5923
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 61
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:61/65; Lr: 0.1
batch Size 256
Epoch: [61][0/196]	Time 0.148 (0.148)	Data 0.294 (0.294)	Loss 0.4076 (0.4076)	Acc@1 87.109 (87.109)	Acc@5 98.828 (98.828)
Epoch: [61][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.3874 (0.3883)	Acc@1 85.938 (86.430)	Acc@5 99.219 (99.483)
Epoch: [61][128/196]	Time 0.099 (0.088)	Data 0.000 (0.002)	Loss 0.3817 (0.4011)	Acc@1 87.500 (86.222)	Acc@5 99.219 (99.473)
Epoch: [61][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.3598 (0.4103)	Acc@1 86.719 (85.923)	Acc@5 100.000 (99.441)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:62/65; Lr: 0.1
batch Size 256
Epoch: [62][0/196]	Time 0.124 (0.124)	Data 0.316 (0.316)	Loss 0.3588 (0.3588)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [62][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.3135 (0.4125)	Acc@1 88.672 (85.721)	Acc@5 100.000 (99.555)
Epoch: [62][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.3286 (0.4146)	Acc@1 87.891 (85.677)	Acc@5 99.609 (99.488)
Epoch: [62][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.4768 (0.4173)	Acc@1 80.078 (85.693)	Acc@5 99.219 (99.452)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:63/65; Lr: 0.1
batch Size 256
Epoch: [63][0/196]	Time 0.108 (0.108)	Data 0.291 (0.291)	Loss 0.3439 (0.3439)	Acc@1 89.062 (89.062)	Acc@5 99.219 (99.219)
Epoch: [63][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.4783 (0.3975)	Acc@1 84.766 (86.364)	Acc@5 99.609 (99.447)
Epoch: [63][128/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.3330 (0.4029)	Acc@1 87.109 (86.074)	Acc@5 100.000 (99.419)
Epoch: [63][192/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.4364 (0.4090)	Acc@1 85.547 (85.810)	Acc@5 100.000 (99.429)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:64/65; Lr: 0.1
batch Size 256
Epoch: [64][0/196]	Time 0.124 (0.124)	Data 0.307 (0.307)	Loss 0.4343 (0.4343)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [64][64/196]	Time 0.098 (0.089)	Data 0.000 (0.005)	Loss 0.3875 (0.4217)	Acc@1 86.719 (85.559)	Acc@5 100.000 (99.381)
Epoch: [64][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.3416 (0.4200)	Acc@1 87.500 (85.595)	Acc@5 100.000 (99.422)
Epoch: [64][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.4149 (0.4161)	Acc@1 84.375 (85.804)	Acc@5 100.000 (99.433)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:65/65; Lr: 0.1
batch Size 256
Epoch: [65][0/196]	Time 0.130 (0.130)	Data 0.279 (0.279)	Loss 0.3859 (0.3859)	Acc@1 87.891 (87.891)	Acc@5 98.828 (98.828)
Epoch: [65][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.4307 (0.3997)	Acc@1 85.156 (86.298)	Acc@5 99.219 (99.477)
Epoch: [65][128/196]	Time 0.092 (0.089)	Data 0.000 (0.002)	Loss 0.3494 (0.4040)	Acc@1 86.719 (86.116)	Acc@5 100.000 (99.470)
Epoch: [65][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.4293 (0.4081)	Acc@1 86.328 (86.025)	Acc@5 99.609 (99.468)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  75.35
Max memory: 51.4381312
 17.570s  j: 66 bis 70
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6168
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 66
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:66/70; Lr: 0.1
batch Size 256
Epoch: [66][0/196]	Time 0.140 (0.140)	Data 0.320 (0.320)	Loss 0.3669 (0.3669)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [66][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.2869 (0.3836)	Acc@1 90.625 (86.767)	Acc@5 99.609 (99.525)
Epoch: [66][128/196]	Time 0.090 (0.089)	Data 0.000 (0.003)	Loss 0.3918 (0.3990)	Acc@1 87.891 (86.192)	Acc@5 98.828 (99.503)
Epoch: [66][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.4156 (0.4086)	Acc@1 84.766 (85.942)	Acc@5 99.609 (99.462)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:67/70; Lr: 0.1
batch Size 256
Epoch: [67][0/196]	Time 0.120 (0.120)	Data 0.312 (0.312)	Loss 0.4306 (0.4306)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [67][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.4573 (0.3984)	Acc@1 85.938 (86.190)	Acc@5 98.828 (99.447)
Epoch: [67][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.3840 (0.4093)	Acc@1 87.109 (85.959)	Acc@5 99.609 (99.458)
Epoch: [67][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.4766 (0.4124)	Acc@1 85.938 (85.871)	Acc@5 99.219 (99.421)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:68/70; Lr: 0.1
batch Size 256
Epoch: [68][0/196]	Time 0.128 (0.128)	Data 0.299 (0.299)	Loss 0.4155 (0.4155)	Acc@1 83.984 (83.984)	Acc@5 98.828 (98.828)
Epoch: [68][64/196]	Time 0.094 (0.089)	Data 0.000 (0.005)	Loss 0.3666 (0.3978)	Acc@1 86.719 (86.328)	Acc@5 99.219 (99.501)
Epoch: [68][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.3642 (0.4040)	Acc@1 86.719 (86.047)	Acc@5 99.219 (99.437)
Epoch: [68][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.3969 (0.4076)	Acc@1 83.984 (85.925)	Acc@5 98.828 (99.421)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:69/70; Lr: 0.1
batch Size 256
Epoch: [69][0/196]	Time 0.120 (0.120)	Data 0.268 (0.268)	Loss 0.3613 (0.3613)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [69][64/196]	Time 0.097 (0.090)	Data 0.000 (0.004)	Loss 0.3736 (0.3985)	Acc@1 88.281 (86.322)	Acc@5 100.000 (99.429)
Epoch: [69][128/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.4142 (0.4069)	Acc@1 83.203 (85.934)	Acc@5 99.219 (99.446)
Epoch: [69][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.4760 (0.4089)	Acc@1 82.422 (85.846)	Acc@5 99.219 (99.427)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:70/70; Lr: 0.1
batch Size 256
Epoch: [70][0/196]	Time 0.131 (0.131)	Data 0.308 (0.308)	Loss 0.3822 (0.3822)	Acc@1 85.547 (85.547)	Acc@5 100.000 (100.000)
Epoch: [70][64/196]	Time 0.090 (0.089)	Data 0.000 (0.005)	Loss 0.3974 (0.4099)	Acc@1 85.547 (85.956)	Acc@5 99.609 (99.399)
Epoch: [70][128/196]	Time 0.090 (0.089)	Data 0.000 (0.003)	Loss 0.3804 (0.4050)	Acc@1 88.672 (86.255)	Acc@5 100.000 (99.410)
Epoch: [70][192/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.3741 (0.4061)	Acc@1 89.453 (86.178)	Acc@5 100.000 (99.411)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  83.31
Max memory: 51.4381312
 17.769s  j: 71 bis 75
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5296
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 71
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:71/75; Lr: 0.1
batch Size 256
Epoch: [71][0/196]	Time 0.145 (0.145)	Data 0.303 (0.303)	Loss 0.3714 (0.3714)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [71][64/196]	Time 0.085 (0.091)	Data 0.000 (0.005)	Loss 0.2984 (0.3853)	Acc@1 88.281 (86.917)	Acc@5 99.609 (99.519)
Epoch: [71][128/196]	Time 0.098 (0.090)	Data 0.000 (0.003)	Loss 0.5928 (0.3987)	Acc@1 78.516 (86.313)	Acc@5 98.438 (99.476)
Epoch: [71][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.5407 (0.4079)	Acc@1 81.641 (86.067)	Acc@5 99.609 (99.456)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:72/75; Lr: 0.1
batch Size 256
Epoch: [72][0/196]	Time 0.128 (0.128)	Data 0.293 (0.293)	Loss 0.4272 (0.4272)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [72][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.3561 (0.4154)	Acc@1 87.500 (85.721)	Acc@5 100.000 (99.447)
Epoch: [72][128/196]	Time 0.102 (0.090)	Data 0.000 (0.002)	Loss 0.3400 (0.4081)	Acc@1 87.891 (85.983)	Acc@5 99.609 (99.446)
Epoch: [72][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.4813 (0.4121)	Acc@1 84.375 (85.814)	Acc@5 99.219 (99.452)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:73/75; Lr: 0.1
batch Size 256
Epoch: [73][0/196]	Time 0.127 (0.127)	Data 0.340 (0.340)	Loss 0.3340 (0.3340)	Acc@1 85.547 (85.547)	Acc@5 100.000 (100.000)
Epoch: [73][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.3877 (0.4006)	Acc@1 89.062 (86.334)	Acc@5 100.000 (99.477)
Epoch: [73][128/196]	Time 0.092 (0.089)	Data 0.000 (0.003)	Loss 0.3328 (0.4062)	Acc@1 87.109 (86.056)	Acc@5 99.609 (99.428)
Epoch: [73][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.4080 (0.4111)	Acc@1 88.281 (85.846)	Acc@5 100.000 (99.452)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:74/75; Lr: 0.1
batch Size 256
Epoch: [74][0/196]	Time 0.118 (0.118)	Data 0.279 (0.279)	Loss 0.3894 (0.3894)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [74][64/196]	Time 0.085 (0.091)	Data 0.000 (0.005)	Loss 0.3615 (0.3946)	Acc@1 86.328 (86.118)	Acc@5 100.000 (99.501)
Epoch: [74][128/196]	Time 0.097 (0.089)	Data 0.000 (0.002)	Loss 0.3967 (0.4001)	Acc@1 86.328 (86.047)	Acc@5 99.609 (99.452)
Epoch: [74][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.4256 (0.4079)	Acc@1 87.109 (85.988)	Acc@5 98.828 (99.427)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:75/75; Lr: 0.1
batch Size 256
Epoch: [75][0/196]	Time 0.145 (0.145)	Data 0.265 (0.265)	Loss 0.4128 (0.4128)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [75][64/196]	Time 0.093 (0.091)	Data 0.000 (0.004)	Loss 0.3240 (0.4123)	Acc@1 92.578 (85.781)	Acc@5 99.219 (99.375)
Epoch: [75][128/196]	Time 0.080 (0.089)	Data 0.000 (0.002)	Loss 0.3479 (0.4079)	Acc@1 88.281 (86.113)	Acc@5 99.609 (99.373)
Epoch: [75][192/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.3598 (0.4045)	Acc@1 86.719 (86.201)	Acc@5 100.000 (99.407)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  79.61
Max memory: 51.4381312
 17.764s  j: 76 bis 80
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3485
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 76
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:76/80; Lr: 0.1
batch Size 256
Epoch: [76][0/196]	Time 0.157 (0.157)	Data 0.296 (0.296)	Loss 0.3121 (0.3121)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [76][64/196]	Time 0.082 (0.087)	Data 0.000 (0.005)	Loss 0.3469 (0.3725)	Acc@1 86.719 (87.097)	Acc@5 100.000 (99.543)
Epoch: [76][128/196]	Time 0.090 (0.086)	Data 0.000 (0.002)	Loss 0.4229 (0.3890)	Acc@1 85.547 (86.458)	Acc@5 99.219 (99.491)
Epoch: [76][192/196]	Time 0.082 (0.086)	Data 0.000 (0.002)	Loss 0.3702 (0.3965)	Acc@1 89.062 (86.280)	Acc@5 98.828 (99.470)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:77/80; Lr: 0.1
batch Size 256
Epoch: [77][0/196]	Time 0.133 (0.133)	Data 0.282 (0.282)	Loss 0.4764 (0.4764)	Acc@1 84.766 (84.766)	Acc@5 98.438 (98.438)
Epoch: [77][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 0.4499 (0.4009)	Acc@1 82.812 (86.316)	Acc@5 99.609 (99.429)
Epoch: [77][128/196]	Time 0.096 (0.087)	Data 0.000 (0.002)	Loss 0.5604 (0.3982)	Acc@1 81.250 (86.286)	Acc@5 98.828 (99.497)
Epoch: [77][192/196]	Time 0.084 (0.086)	Data 0.000 (0.002)	Loss 0.4099 (0.4043)	Acc@1 84.766 (86.103)	Acc@5 99.609 (99.460)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:78/80; Lr: 0.1
batch Size 256
Epoch: [78][0/196]	Time 0.138 (0.138)	Data 0.285 (0.285)	Loss 0.3443 (0.3443)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [78][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.3283 (0.4042)	Acc@1 87.891 (85.901)	Acc@5 99.609 (99.489)
Epoch: [78][128/196]	Time 0.079 (0.088)	Data 0.000 (0.002)	Loss 0.4395 (0.4001)	Acc@1 86.328 (86.192)	Acc@5 98.828 (99.470)
Epoch: [78][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.4347 (0.4052)	Acc@1 85.938 (86.027)	Acc@5 99.219 (99.443)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:79/80; Lr: 0.1
batch Size 256
Epoch: [79][0/196]	Time 0.115 (0.115)	Data 0.284 (0.284)	Loss 0.4972 (0.4972)	Acc@1 83.203 (83.203)	Acc@5 100.000 (100.000)
Epoch: [79][64/196]	Time 0.093 (0.088)	Data 0.000 (0.005)	Loss 0.3387 (0.4001)	Acc@1 87.500 (86.028)	Acc@5 100.000 (99.507)
Epoch: [79][128/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.3505 (0.3983)	Acc@1 86.719 (86.252)	Acc@5 100.000 (99.440)
Epoch: [79][192/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.3041 (0.4028)	Acc@1 89.062 (86.018)	Acc@5 99.609 (99.441)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:80/80; Lr: 0.1
batch Size 256
Epoch: [80][0/196]	Time 0.116 (0.116)	Data 0.288 (0.288)	Loss 0.4155 (0.4155)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [80][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.3787 (0.4036)	Acc@1 85.938 (86.040)	Acc@5 100.000 (99.453)
Epoch: [80][128/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.3187 (0.4025)	Acc@1 90.234 (86.025)	Acc@5 100.000 (99.397)
Epoch: [80][192/196]	Time 0.095 (0.087)	Data 0.000 (0.002)	Loss 0.3127 (0.3995)	Acc@1 89.844 (86.164)	Acc@5 100.000 (99.449)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  80.38
Max memory: 51.4381312
 17.431s  j: 81 bis 85
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6773
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 81
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:81/85; Lr: 0.1
batch Size 256
Epoch: [81][0/196]	Time 0.152 (0.152)	Data 0.251 (0.251)	Loss 0.2638 (0.2638)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [81][64/196]	Time 0.086 (0.085)	Data 0.000 (0.004)	Loss 0.5033 (0.3869)	Acc@1 83.984 (86.683)	Acc@5 97.656 (99.543)
Epoch: [81][128/196]	Time 0.080 (0.086)	Data 0.000 (0.002)	Loss 0.3731 (0.3960)	Acc@1 86.719 (86.383)	Acc@5 100.000 (99.464)
Epoch: [81][192/196]	Time 0.088 (0.086)	Data 0.000 (0.001)	Loss 0.4249 (0.3976)	Acc@1 85.156 (86.350)	Acc@5 99.219 (99.458)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:82/85; Lr: 0.1
batch Size 256
Epoch: [82][0/196]	Time 0.122 (0.122)	Data 0.322 (0.322)	Loss 0.5046 (0.5046)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [82][64/196]	Time 0.096 (0.088)	Data 0.000 (0.005)	Loss 0.4537 (0.4044)	Acc@1 83.594 (85.901)	Acc@5 99.219 (99.525)
Epoch: [82][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.3912 (0.4023)	Acc@1 85.938 (86.183)	Acc@5 98.828 (99.488)
Epoch: [82][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.5062 (0.4079)	Acc@1 82.422 (86.035)	Acc@5 98.828 (99.443)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:83/85; Lr: 0.1
batch Size 256
Epoch: [83][0/196]	Time 0.135 (0.135)	Data 0.269 (0.269)	Loss 0.4094 (0.4094)	Acc@1 86.328 (86.328)	Acc@5 98.828 (98.828)
Epoch: [83][64/196]	Time 0.081 (0.088)	Data 0.000 (0.004)	Loss 0.4026 (0.4074)	Acc@1 85.938 (85.847)	Acc@5 99.219 (99.459)
Epoch: [83][128/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.2990 (0.4009)	Acc@1 91.016 (86.095)	Acc@5 100.000 (99.467)
Epoch: [83][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.3844 (0.4008)	Acc@1 87.109 (86.146)	Acc@5 99.219 (99.476)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:84/85; Lr: 0.1
batch Size 256
Epoch: [84][0/196]	Time 0.129 (0.129)	Data 0.288 (0.288)	Loss 0.3956 (0.3956)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [84][64/196]	Time 0.080 (0.089)	Data 0.000 (0.005)	Loss 0.3553 (0.3875)	Acc@1 85.938 (86.581)	Acc@5 99.219 (99.555)
Epoch: [84][128/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.4756 (0.3970)	Acc@1 82.812 (86.255)	Acc@5 98.438 (99.479)
Epoch: [84][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.4718 (0.3969)	Acc@1 83.203 (86.296)	Acc@5 98.438 (99.480)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:85/85; Lr: 0.1
batch Size 256
Epoch: [85][0/196]	Time 0.144 (0.144)	Data 0.343 (0.343)	Loss 0.5168 (0.5168)	Acc@1 80.859 (80.859)	Acc@5 98.438 (98.438)
Epoch: [85][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.3724 (0.4036)	Acc@1 87.500 (86.100)	Acc@5 99.219 (99.447)
Epoch: [85][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.4225 (0.4055)	Acc@1 88.281 (86.222)	Acc@5 99.609 (99.449)
Epoch: [85][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.4171 (0.4012)	Acc@1 84.375 (86.263)	Acc@5 98.438 (99.443)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  75.16
Max memory: 51.4381312
 17.615s  j: 86 bis 90
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2456
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 86
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:86/90; Lr: 0.1
batch Size 256
Epoch: [86][0/196]	Time 0.157 (0.157)	Data 0.320 (0.320)	Loss 0.3418 (0.3418)	Acc@1 89.453 (89.453)	Acc@5 100.000 (100.000)
Epoch: [86][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.5013 (0.3755)	Acc@1 82.812 (87.001)	Acc@5 98.438 (99.507)
Epoch: [86][128/196]	Time 0.090 (0.089)	Data 0.000 (0.003)	Loss 0.3263 (0.3828)	Acc@1 88.672 (86.834)	Acc@5 100.000 (99.473)
Epoch: [86][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.3481 (0.3927)	Acc@1 89.062 (86.484)	Acc@5 99.219 (99.460)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:87/90; Lr: 0.1
batch Size 256
Epoch: [87][0/196]	Time 0.126 (0.126)	Data 0.310 (0.310)	Loss 0.4023 (0.4023)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [87][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.3811 (0.3952)	Acc@1 87.891 (86.298)	Acc@5 99.219 (99.507)
Epoch: [87][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.4547 (0.4017)	Acc@1 83.594 (86.277)	Acc@5 99.219 (99.443)
Epoch: [87][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.4864 (0.4010)	Acc@1 83.984 (86.207)	Acc@5 98.828 (99.470)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:88/90; Lr: 0.1
batch Size 256
Epoch: [88][0/196]	Time 0.137 (0.137)	Data 0.273 (0.273)	Loss 0.4233 (0.4233)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [88][64/196]	Time 0.090 (0.090)	Data 0.000 (0.004)	Loss 0.3874 (0.3975)	Acc@1 86.328 (86.226)	Acc@5 99.219 (99.435)
Epoch: [88][128/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.3974 (0.3992)	Acc@1 85.156 (86.222)	Acc@5 99.609 (99.428)
Epoch: [88][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.3684 (0.3989)	Acc@1 86.328 (86.261)	Acc@5 99.609 (99.456)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:89/90; Lr: 0.1
batch Size 256
Epoch: [89][0/196]	Time 0.129 (0.129)	Data 0.299 (0.299)	Loss 0.3904 (0.3904)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [89][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.3859 (0.3953)	Acc@1 87.109 (86.677)	Acc@5 98.828 (99.381)
Epoch: [89][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.3083 (0.4011)	Acc@1 89.844 (86.386)	Acc@5 100.000 (99.410)
Epoch: [89][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.4685 (0.3998)	Acc@1 85.156 (86.251)	Acc@5 99.219 (99.439)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:90/90; Lr: 0.1
batch Size 256
Epoch: [90][0/196]	Time 0.118 (0.118)	Data 0.296 (0.296)	Loss 0.3463 (0.3463)	Acc@1 89.062 (89.062)	Acc@5 99.609 (99.609)
Epoch: [90][64/196]	Time 0.091 (0.087)	Data 0.000 (0.005)	Loss 0.3745 (0.3994)	Acc@1 87.109 (86.352)	Acc@5 100.000 (99.477)
Epoch: [90][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.4640 (0.3918)	Acc@1 83.203 (86.673)	Acc@5 99.219 (99.482)
Epoch: [90][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.4618 (0.3943)	Acc@1 87.500 (86.559)	Acc@5 99.609 (99.447)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  80.39
Max memory: 51.4381312
 17.505s  j: 91 bis 95
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 869
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 91
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:91/95; Lr: 0.1
batch Size 256
Epoch: [91][0/196]	Time 0.166 (0.166)	Data 0.257 (0.257)	Loss 0.3338 (0.3338)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [91][64/196]	Time 0.084 (0.088)	Data 0.000 (0.004)	Loss 0.4112 (0.3680)	Acc@1 86.719 (87.416)	Acc@5 99.219 (99.489)
Epoch: [91][128/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.3978 (0.3832)	Acc@1 87.891 (86.834)	Acc@5 98.438 (99.479)
Epoch: [91][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.3920 (0.3870)	Acc@1 86.328 (86.664)	Acc@5 98.828 (99.488)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:92/95; Lr: 0.1
batch Size 256
Epoch: [92][0/196]	Time 0.119 (0.119)	Data 0.295 (0.295)	Loss 0.3776 (0.3776)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [92][64/196]	Time 0.088 (0.087)	Data 0.000 (0.005)	Loss 0.4139 (0.4015)	Acc@1 86.719 (86.262)	Acc@5 99.219 (99.483)
Epoch: [92][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.4995 (0.4058)	Acc@1 80.078 (86.016)	Acc@5 100.000 (99.464)
Epoch: [92][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.5191 (0.4050)	Acc@1 81.641 (86.010)	Acc@5 99.609 (99.429)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:93/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [93][0/196]	Time 0.131 (0.131)	Data 0.297 (0.297)	Loss 0.3554 (0.3554)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [93][64/196]	Time 0.096 (0.087)	Data 0.000 (0.005)	Loss 0.3727 (0.3168)	Acc@1 87.891 (89.273)	Acc@5 99.609 (99.639)
Epoch: [93][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.3043 (0.2957)	Acc@1 89.844 (89.986)	Acc@5 99.609 (99.655)
Epoch: [93][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.2483 (0.2851)	Acc@1 90.234 (90.293)	Acc@5 99.609 (99.711)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:94/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [94][0/196]	Time 0.136 (0.136)	Data 0.292 (0.292)	Loss 0.2977 (0.2977)	Acc@1 90.625 (90.625)	Acc@5 99.609 (99.609)
Epoch: [94][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.2258 (0.2477)	Acc@1 92.188 (91.845)	Acc@5 100.000 (99.772)
Epoch: [94][128/196]	Time 0.094 (0.088)	Data 0.000 (0.003)	Loss 0.3761 (0.2411)	Acc@1 87.891 (91.985)	Acc@5 99.609 (99.800)
Epoch: [94][192/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.2275 (0.2429)	Acc@1 91.406 (91.870)	Acc@5 99.609 (99.794)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:95/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [95][0/196]	Time 0.108 (0.108)	Data 0.330 (0.330)	Loss 0.2554 (0.2554)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [95][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.2509 (0.2299)	Acc@1 92.188 (92.350)	Acc@5 100.000 (99.814)
Epoch: [95][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.3026 (0.2274)	Acc@1 89.062 (92.324)	Acc@5 99.219 (99.803)
Epoch: [95][192/196]	Time 0.083 (0.086)	Data 0.000 (0.002)	Loss 0.1925 (0.2266)	Acc@1 93.359 (92.315)	Acc@5 100.000 (99.808)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.51
Max memory: 51.4381312
 17.340s  j: 96 bis 100
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9062
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 96
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:96/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [96][0/196]	Time 0.156 (0.156)	Data 0.270 (0.270)	Loss 0.2155 (0.2155)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [96][64/196]	Time 0.086 (0.088)	Data 0.000 (0.004)	Loss 0.2023 (0.2154)	Acc@1 93.359 (92.758)	Acc@5 100.000 (99.850)
Epoch: [96][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.2200 (0.2176)	Acc@1 94.141 (92.660)	Acc@5 100.000 (99.827)
Epoch: [96][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.2326 (0.2198)	Acc@1 92.578 (92.576)	Acc@5 100.000 (99.824)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:97/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [97][0/196]	Time 0.126 (0.126)	Data 0.312 (0.312)	Loss 0.1778 (0.1778)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [97][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.2689 (0.2018)	Acc@1 90.234 (93.203)	Acc@5 99.609 (99.832)
Epoch: [97][128/196]	Time 0.092 (0.088)	Data 0.000 (0.003)	Loss 0.2295 (0.2090)	Acc@1 92.969 (93.023)	Acc@5 99.609 (99.824)
Epoch: [97][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.1862 (0.2118)	Acc@1 94.531 (92.902)	Acc@5 100.000 (99.828)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:98/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [98][0/196]	Time 0.116 (0.116)	Data 0.341 (0.341)	Loss 0.1782 (0.1782)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [98][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.2200 (0.2053)	Acc@1 90.625 (93.071)	Acc@5 100.000 (99.850)
Epoch: [98][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.1594 (0.2068)	Acc@1 94.922 (92.941)	Acc@5 100.000 (99.861)
Epoch: [98][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.1988 (0.2077)	Acc@1 94.141 (92.882)	Acc@5 100.000 (99.854)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:99/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [99][0/196]	Time 0.102 (0.102)	Data 0.305 (0.305)	Loss 0.2631 (0.2631)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [99][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.2442 (0.1938)	Acc@1 92.188 (93.425)	Acc@5 100.000 (99.856)
Epoch: [99][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.1651 (0.1998)	Acc@1 93.750 (93.023)	Acc@5 100.000 (99.864)
Epoch: [99][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.2064 (0.2017)	Acc@1 92.578 (92.993)	Acc@5 99.609 (99.875)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:100/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [100][0/196]	Time 0.143 (0.143)	Data 0.303 (0.303)	Loss 0.1823 (0.1823)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [100][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.1408 (0.2044)	Acc@1 96.094 (92.957)	Acc@5 100.000 (99.832)
Epoch: [100][128/196]	Time 0.091 (0.087)	Data 0.000 (0.003)	Loss 0.1700 (0.2009)	Acc@1 93.750 (93.156)	Acc@5 100.000 (99.858)
Epoch: [100][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.1825 (0.1978)	Acc@1 94.922 (93.272)	Acc@5 100.000 (99.866)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.93
Max memory: 51.4381312
 17.446s  j: 101 bis 105
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5131
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 101
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:101/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [101][0/196]	Time 0.157 (0.157)	Data 0.301 (0.301)	Loss 0.2558 (0.2558)	Acc@1 90.234 (90.234)	Acc@5 100.000 (100.000)
Epoch: [101][64/196]	Time 0.088 (0.091)	Data 0.000 (0.005)	Loss 0.2111 (0.1892)	Acc@1 93.750 (93.510)	Acc@5 100.000 (99.868)
Epoch: [101][128/196]	Time 0.080 (0.090)	Data 0.000 (0.003)	Loss 0.2112 (0.1909)	Acc@1 93.750 (93.484)	Acc@5 99.609 (99.852)
Epoch: [101][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.1763 (0.1930)	Acc@1 93.750 (93.446)	Acc@5 100.000 (99.850)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:102/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [102][0/196]	Time 0.146 (0.146)	Data 0.284 (0.284)	Loss 0.1983 (0.1983)	Acc@1 94.922 (94.922)	Acc@5 99.609 (99.609)
Epoch: [102][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.2363 (0.1990)	Acc@1 91.406 (93.203)	Acc@5 99.219 (99.832)
Epoch: [102][128/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.1314 (0.1941)	Acc@1 95.703 (93.320)	Acc@5 100.000 (99.849)
Epoch: [102][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.1689 (0.1928)	Acc@1 94.141 (93.363)	Acc@5 99.609 (99.858)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:103/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [103][0/196]	Time 0.137 (0.137)	Data 0.321 (0.321)	Loss 0.1706 (0.1706)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [103][64/196]	Time 0.091 (0.091)	Data 0.000 (0.005)	Loss 0.1686 (0.1869)	Acc@1 93.359 (93.456)	Acc@5 100.000 (99.808)
Epoch: [103][128/196]	Time 0.085 (0.089)	Data 0.000 (0.003)	Loss 0.1224 (0.1852)	Acc@1 96.484 (93.517)	Acc@5 100.000 (99.843)
Epoch: [103][192/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.1173 (0.1840)	Acc@1 96.484 (93.602)	Acc@5 100.000 (99.860)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:104/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [104][0/196]	Time 0.141 (0.141)	Data 0.317 (0.317)	Loss 0.1769 (0.1769)	Acc@1 92.188 (92.188)	Acc@5 99.609 (99.609)
Epoch: [104][64/196]	Time 0.085 (0.091)	Data 0.000 (0.005)	Loss 0.1926 (0.1758)	Acc@1 92.969 (94.020)	Acc@5 100.000 (99.910)
Epoch: [104][128/196]	Time 0.079 (0.090)	Data 0.000 (0.003)	Loss 0.1133 (0.1800)	Acc@1 95.312 (93.920)	Acc@5 100.000 (99.885)
Epoch: [104][192/196]	Time 0.080 (0.090)	Data 0.000 (0.002)	Loss 0.1677 (0.1833)	Acc@1 94.141 (93.776)	Acc@5 99.609 (99.877)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:105/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [105][0/196]	Time 0.100 (0.100)	Data 0.287 (0.287)	Loss 0.1583 (0.1583)	Acc@1 94.531 (94.531)	Acc@5 99.609 (99.609)
Epoch: [105][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.1733 (0.1760)	Acc@1 96.094 (94.219)	Acc@5 100.000 (99.850)
Epoch: [105][128/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.1158 (0.1805)	Acc@1 95.703 (93.838)	Acc@5 100.000 (99.855)
Epoch: [105][192/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.2117 (0.1830)	Acc@1 92.969 (93.754)	Acc@5 99.609 (99.864)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.8
Max memory: 51.4381312
 17.760s  j: 106 bis 110
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6895
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 106
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:106/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [106][0/196]	Time 0.147 (0.147)	Data 0.301 (0.301)	Loss 0.2006 (0.2006)	Acc@1 93.359 (93.359)	Acc@5 99.609 (99.609)
Epoch: [106][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.2561 (0.1650)	Acc@1 91.797 (94.177)	Acc@5 99.609 (99.904)
Epoch: [106][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.1302 (0.1745)	Acc@1 96.484 (93.953)	Acc@5 100.000 (99.870)
Epoch: [106][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.1788 (0.1786)	Acc@1 92.188 (93.819)	Acc@5 100.000 (99.864)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:107/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [107][0/196]	Time 0.130 (0.130)	Data 0.315 (0.315)	Loss 0.1925 (0.1925)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [107][64/196]	Time 0.095 (0.090)	Data 0.000 (0.005)	Loss 0.1787 (0.1728)	Acc@1 92.969 (94.189)	Acc@5 99.609 (99.844)
Epoch: [107][128/196]	Time 0.081 (0.089)	Data 0.000 (0.003)	Loss 0.1653 (0.1761)	Acc@1 93.750 (94.041)	Acc@5 100.000 (99.852)
Epoch: [107][192/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.1600 (0.1754)	Acc@1 93.750 (94.043)	Acc@5 100.000 (99.860)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:108/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [108][0/196]	Time 0.133 (0.133)	Data 0.257 (0.257)	Loss 0.1194 (0.1194)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [108][64/196]	Time 0.087 (0.089)	Data 0.000 (0.004)	Loss 0.1726 (0.1662)	Acc@1 95.312 (94.345)	Acc@5 100.000 (99.910)
Epoch: [108][128/196]	Time 0.096 (0.088)	Data 0.000 (0.002)	Loss 0.1726 (0.1693)	Acc@1 93.750 (94.141)	Acc@5 100.000 (99.915)
Epoch: [108][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.1765 (0.1743)	Acc@1 94.531 (94.007)	Acc@5 99.219 (99.883)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:109/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [109][0/196]	Time 0.131 (0.131)	Data 0.321 (0.321)	Loss 0.2315 (0.2315)	Acc@1 93.359 (93.359)	Acc@5 99.219 (99.219)
Epoch: [109][64/196]	Time 0.080 (0.090)	Data 0.000 (0.005)	Loss 0.1805 (0.1697)	Acc@1 93.750 (94.201)	Acc@5 99.219 (99.844)
Epoch: [109][128/196]	Time 0.081 (0.088)	Data 0.000 (0.003)	Loss 0.1451 (0.1713)	Acc@1 95.312 (94.135)	Acc@5 100.000 (99.879)
Epoch: [109][192/196]	Time 0.094 (0.088)	Data 0.000 (0.002)	Loss 0.1029 (0.1716)	Acc@1 96.484 (94.100)	Acc@5 100.000 (99.889)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:110/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [110][0/196]	Time 0.128 (0.128)	Data 0.322 (0.322)	Loss 0.1394 (0.1394)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)
Epoch: [110][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.1511 (0.1664)	Acc@1 94.922 (94.237)	Acc@5 100.000 (99.904)
Epoch: [110][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.2045 (0.1700)	Acc@1 92.188 (94.171)	Acc@5 100.000 (99.900)
Epoch: [110][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1663 (0.1725)	Acc@1 93.750 (94.019)	Acc@5 100.000 (99.885)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.68
Max memory: 51.4381312
 17.691s  j: 111 bis 115
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6256
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 111
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:111/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [111][0/196]	Time 0.151 (0.151)	Data 0.310 (0.310)	Loss 0.1805 (0.1805)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [111][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.2028 (0.1661)	Acc@1 92.578 (94.261)	Acc@5 99.609 (99.856)
Epoch: [111][128/196]	Time 0.088 (0.089)	Data 0.000 (0.003)	Loss 0.1551 (0.1629)	Acc@1 93.750 (94.365)	Acc@5 100.000 (99.870)
Epoch: [111][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1219 (0.1678)	Acc@1 96.094 (94.211)	Acc@5 100.000 (99.885)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:112/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [112][0/196]	Time 0.118 (0.118)	Data 0.302 (0.302)	Loss 0.2104 (0.2104)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [112][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.2419 (0.1662)	Acc@1 90.625 (94.123)	Acc@5 99.609 (99.928)
Epoch: [112][128/196]	Time 0.089 (0.089)	Data 0.000 (0.003)	Loss 0.1544 (0.1648)	Acc@1 95.703 (94.295)	Acc@5 100.000 (99.888)
Epoch: [112][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.1736 (0.1675)	Acc@1 95.312 (94.224)	Acc@5 100.000 (99.885)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:113/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [113][0/196]	Time 0.120 (0.120)	Data 0.298 (0.298)	Loss 0.1409 (0.1409)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [113][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.1245 (0.1583)	Acc@1 94.922 (94.645)	Acc@5 100.000 (99.916)
Epoch: [113][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.1686 (0.1605)	Acc@1 94.141 (94.513)	Acc@5 99.609 (99.912)
Epoch: [113][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.1256 (0.1650)	Acc@1 96.484 (94.311)	Acc@5 100.000 (99.905)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:114/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [114][0/196]	Time 0.130 (0.130)	Data 0.296 (0.296)	Loss 0.1691 (0.1691)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [114][64/196]	Time 0.088 (0.090)	Data 0.000 (0.005)	Loss 0.1757 (0.1568)	Acc@1 94.141 (94.772)	Acc@5 100.000 (99.928)
Epoch: [114][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.1146 (0.1612)	Acc@1 95.312 (94.537)	Acc@5 100.000 (99.903)
Epoch: [114][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1725 (0.1633)	Acc@1 93.750 (94.426)	Acc@5 100.000 (99.897)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:115/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [115][0/196]	Time 0.128 (0.128)	Data 0.277 (0.277)	Loss 0.1638 (0.1638)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [115][64/196]	Time 0.087 (0.090)	Data 0.000 (0.004)	Loss 0.1840 (0.1632)	Acc@1 93.359 (94.147)	Acc@5 100.000 (99.916)
Epoch: [115][128/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.2144 (0.1620)	Acc@1 91.797 (94.289)	Acc@5 100.000 (99.900)
Epoch: [115][192/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.1312 (0.1647)	Acc@1 96.094 (94.264)	Acc@5 100.000 (99.899)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.7
Max memory: 51.4381312
 17.728s  j: 116 bis 120
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 683
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 116
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:116/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [116][0/196]	Time 0.146 (0.146)	Data 0.300 (0.300)	Loss 0.1902 (0.1902)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [116][64/196]	Time 0.089 (0.091)	Data 0.000 (0.005)	Loss 0.1924 (0.1566)	Acc@1 94.141 (94.645)	Acc@5 100.000 (99.892)
Epoch: [116][128/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.1530 (0.1620)	Acc@1 95.312 (94.383)	Acc@5 99.219 (99.897)
Epoch: [116][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1285 (0.1617)	Acc@1 95.312 (94.416)	Acc@5 99.609 (99.905)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:117/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [117][0/196]	Time 0.141 (0.141)	Data 0.302 (0.302)	Loss 0.1688 (0.1688)	Acc@1 93.359 (93.359)	Acc@5 99.609 (99.609)
Epoch: [117][64/196]	Time 0.104 (0.088)	Data 0.000 (0.005)	Loss 0.1191 (0.1576)	Acc@1 96.875 (94.621)	Acc@5 99.609 (99.874)
Epoch: [117][128/196]	Time 0.092 (0.088)	Data 0.000 (0.003)	Loss 0.1530 (0.1617)	Acc@1 95.703 (94.401)	Acc@5 100.000 (99.876)
Epoch: [117][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.1549 (0.1612)	Acc@1 95.312 (94.479)	Acc@5 100.000 (99.895)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:118/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [118][0/196]	Time 0.138 (0.138)	Data 0.315 (0.315)	Loss 0.1726 (0.1726)	Acc@1 93.359 (93.359)	Acc@5 99.609 (99.609)
Epoch: [118][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.1781 (0.1563)	Acc@1 93.359 (94.639)	Acc@5 99.609 (99.910)
Epoch: [118][128/196]	Time 0.081 (0.089)	Data 0.000 (0.003)	Loss 0.1568 (0.1602)	Acc@1 94.922 (94.474)	Acc@5 100.000 (99.903)
Epoch: [118][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.1649 (0.1622)	Acc@1 93.750 (94.424)	Acc@5 100.000 (99.907)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:119/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [119][0/196]	Time 0.140 (0.140)	Data 0.306 (0.306)	Loss 0.0773 (0.0773)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [119][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.1488 (0.1495)	Acc@1 93.359 (94.796)	Acc@5 100.000 (99.922)
Epoch: [119][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.1653 (0.1554)	Acc@1 95.312 (94.655)	Acc@5 99.609 (99.924)
Epoch: [119][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1642 (0.1565)	Acc@1 92.969 (94.560)	Acc@5 100.000 (99.919)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:120/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [120][0/196]	Time 0.134 (0.134)	Data 0.286 (0.286)	Loss 0.1539 (0.1539)	Acc@1 94.141 (94.141)	Acc@5 99.609 (99.609)
Epoch: [120][64/196]	Time 0.090 (0.089)	Data 0.000 (0.005)	Loss 0.1400 (0.1570)	Acc@1 95.703 (94.591)	Acc@5 100.000 (99.910)
Epoch: [120][128/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.1785 (0.1546)	Acc@1 92.188 (94.613)	Acc@5 100.000 (99.921)
Epoch: [120][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1793 (0.1547)	Acc@1 93.750 (94.618)	Acc@5 100.000 (99.921)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.56
Max memory: 51.4381312
 17.622s  j: 121 bis 125
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6068
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 121
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:121/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [121][0/196]	Time 0.147 (0.147)	Data 0.272 (0.272)	Loss 0.1308 (0.1308)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [121][64/196]	Time 0.088 (0.089)	Data 0.000 (0.004)	Loss 0.1689 (0.1382)	Acc@1 93.750 (95.102)	Acc@5 100.000 (99.916)
Epoch: [121][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1062 (0.1490)	Acc@1 97.266 (94.870)	Acc@5 99.609 (99.918)
Epoch: [121][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.2400 (0.1550)	Acc@1 92.578 (94.655)	Acc@5 100.000 (99.919)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:122/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [122][0/196]	Time 0.122 (0.122)	Data 0.303 (0.303)	Loss 0.1402 (0.1402)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [122][64/196]	Time 0.083 (0.087)	Data 0.000 (0.005)	Loss 0.1800 (0.1558)	Acc@1 93.750 (94.669)	Acc@5 100.000 (99.922)
Epoch: [122][128/196]	Time 0.090 (0.087)	Data 0.000 (0.003)	Loss 0.1230 (0.1590)	Acc@1 94.531 (94.468)	Acc@5 100.000 (99.933)
Epoch: [122][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.1865 (0.1591)	Acc@1 92.188 (94.481)	Acc@5 100.000 (99.913)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:123/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [123][0/196]	Time 0.131 (0.131)	Data 0.310 (0.310)	Loss 0.1884 (0.1884)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [123][64/196]	Time 0.089 (0.090)	Data 0.000 (0.005)	Loss 0.1134 (0.1579)	Acc@1 97.266 (94.573)	Acc@5 100.000 (99.928)
Epoch: [123][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.1332 (0.1580)	Acc@1 95.312 (94.640)	Acc@5 100.000 (99.942)
Epoch: [123][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.1811 (0.1584)	Acc@1 94.531 (94.582)	Acc@5 99.609 (99.937)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:124/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [124][0/196]	Time 0.132 (0.132)	Data 0.287 (0.287)	Loss 0.1811 (0.1811)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [124][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.2189 (0.1505)	Acc@1 92.578 (94.808)	Acc@5 100.000 (99.934)
Epoch: [124][128/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.1628 (0.1542)	Acc@1 95.312 (94.692)	Acc@5 100.000 (99.921)
Epoch: [124][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.1240 (0.1567)	Acc@1 95.703 (94.586)	Acc@5 100.000 (99.925)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:125/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [125][0/196]	Time 0.128 (0.128)	Data 0.340 (0.340)	Loss 0.1482 (0.1482)	Acc@1 95.312 (95.312)	Acc@5 99.609 (99.609)
Epoch: [125][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.2061 (0.1514)	Acc@1 94.531 (94.814)	Acc@5 100.000 (99.910)
Epoch: [125][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.1619 (0.1523)	Acc@1 94.141 (94.746)	Acc@5 100.000 (99.927)
Epoch: [125][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.1227 (0.1547)	Acc@1 96.875 (94.624)	Acc@5 100.000 (99.927)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.11
Max memory: 51.4381312
 17.635s  j: 126 bis 130
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9693
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 126
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:126/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [126][0/196]	Time 0.167 (0.167)	Data 0.329 (0.329)	Loss 0.1555 (0.1555)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [126][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.1634 (0.1454)	Acc@1 93.750 (95.000)	Acc@5 100.000 (99.922)
Epoch: [126][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.1439 (0.1494)	Acc@1 95.312 (94.849)	Acc@5 100.000 (99.930)
Epoch: [126][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.1787 (0.1540)	Acc@1 94.922 (94.687)	Acc@5 99.219 (99.915)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:127/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [127][0/196]	Time 0.117 (0.117)	Data 0.293 (0.293)	Loss 0.1635 (0.1635)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [127][64/196]	Time 0.092 (0.088)	Data 0.000 (0.005)	Loss 0.1259 (0.1471)	Acc@1 96.484 (94.994)	Acc@5 100.000 (99.946)
Epoch: [127][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1321 (0.1485)	Acc@1 96.484 (94.916)	Acc@5 100.000 (99.936)
Epoch: [127][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.1495 (0.1532)	Acc@1 94.531 (94.768)	Acc@5 100.000 (99.925)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:128/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [128][0/196]	Time 0.107 (0.107)	Data 0.305 (0.305)	Loss 0.1638 (0.1638)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)
Epoch: [128][64/196]	Time 0.095 (0.085)	Data 0.000 (0.005)	Loss 0.1432 (0.1499)	Acc@1 96.484 (94.826)	Acc@5 100.000 (99.946)
Epoch: [128][128/196]	Time 0.092 (0.086)	Data 0.000 (0.003)	Loss 0.1568 (0.1511)	Acc@1 93.359 (94.795)	Acc@5 100.000 (99.945)
Epoch: [128][192/196]	Time 0.081 (0.086)	Data 0.000 (0.002)	Loss 0.1121 (0.1528)	Acc@1 97.656 (94.699)	Acc@5 100.000 (99.929)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:129/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [129][0/196]	Time 0.124 (0.124)	Data 0.276 (0.276)	Loss 0.1754 (0.1754)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [129][64/196]	Time 0.087 (0.088)	Data 0.000 (0.004)	Loss 0.1788 (0.1605)	Acc@1 92.188 (94.345)	Acc@5 100.000 (99.856)
Epoch: [129][128/196]	Time 0.095 (0.087)	Data 0.000 (0.002)	Loss 0.1759 (0.1562)	Acc@1 93.359 (94.437)	Acc@5 99.609 (99.891)
Epoch: [129][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.1947 (0.1586)	Acc@1 93.750 (94.394)	Acc@5 100.000 (99.909)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:130/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [130][0/196]	Time 0.119 (0.119)	Data 0.300 (0.300)	Loss 0.1734 (0.1734)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [130][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.1282 (0.1415)	Acc@1 96.094 (95.228)	Acc@5 100.000 (99.928)
Epoch: [130][128/196]	Time 0.092 (0.089)	Data 0.000 (0.003)	Loss 0.1636 (0.1462)	Acc@1 96.094 (94.934)	Acc@5 99.609 (99.924)
Epoch: [130][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.1310 (0.1515)	Acc@1 95.703 (94.790)	Acc@5 100.000 (99.929)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.5
Max memory: 51.4381312
 17.529s  j: 131 bis 135
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4604
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:131/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [131][0/196]	Time 0.168 (0.168)	Data 0.274 (0.274)	Loss 0.1533 (0.1533)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [131][64/196]	Time 0.086 (0.092)	Data 0.000 (0.004)	Loss 0.1644 (0.1424)	Acc@1 94.141 (94.886)	Acc@5 100.000 (99.940)
Epoch: [131][128/196]	Time 0.083 (0.090)	Data 0.000 (0.002)	Loss 0.1246 (0.1453)	Acc@1 96.484 (94.946)	Acc@5 100.000 (99.942)
Epoch: [131][192/196]	Time 0.085 (0.090)	Data 0.000 (0.002)	Loss 0.1126 (0.1507)	Acc@1 96.484 (94.724)	Acc@5 100.000 (99.929)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:132/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [132][0/196]	Time 0.103 (0.103)	Data 0.304 (0.304)	Loss 0.1829 (0.1829)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [132][64/196]	Time 0.095 (0.091)	Data 0.000 (0.005)	Loss 0.1232 (0.1449)	Acc@1 95.312 (95.210)	Acc@5 100.000 (99.952)
Epoch: [132][128/196]	Time 0.089 (0.090)	Data 0.000 (0.003)	Loss 0.1852 (0.1488)	Acc@1 94.141 (94.852)	Acc@5 100.000 (99.918)
Epoch: [132][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.1157 (0.1478)	Acc@1 95.703 (94.825)	Acc@5 100.000 (99.927)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:133/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [133][0/196]	Time 0.137 (0.137)	Data 0.263 (0.263)	Loss 0.0904 (0.0904)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [133][64/196]	Time 0.092 (0.090)	Data 0.000 (0.004)	Loss 0.1915 (0.1458)	Acc@1 94.531 (95.012)	Acc@5 99.609 (99.922)
Epoch: [133][128/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.1473 (0.1512)	Acc@1 94.531 (94.755)	Acc@5 99.609 (99.924)
Epoch: [133][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.1490 (0.1554)	Acc@1 95.312 (94.564)	Acc@5 99.609 (99.911)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:134/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [134][0/196]	Time 0.140 (0.140)	Data 0.288 (0.288)	Loss 0.1508 (0.1508)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [134][64/196]	Time 0.089 (0.091)	Data 0.000 (0.005)	Loss 0.2090 (0.1472)	Acc@1 92.188 (94.784)	Acc@5 99.609 (99.922)
Epoch: [134][128/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.1309 (0.1520)	Acc@1 93.750 (94.689)	Acc@5 100.000 (99.921)
Epoch: [134][192/196]	Time 0.080 (0.089)	Data 0.000 (0.002)	Loss 0.1588 (0.1553)	Acc@1 94.141 (94.576)	Acc@5 100.000 (99.923)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:135/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [135][0/196]	Time 0.130 (0.130)	Data 0.296 (0.296)	Loss 0.1998 (0.1998)	Acc@1 93.750 (93.750)	Acc@5 99.609 (99.609)
Epoch: [135][64/196]	Time 0.089 (0.091)	Data 0.000 (0.005)	Loss 0.1803 (0.1538)	Acc@1 94.531 (94.681)	Acc@5 100.000 (99.922)
Epoch: [135][128/196]	Time 0.085 (0.089)	Data 0.000 (0.003)	Loss 0.1756 (0.1537)	Acc@1 92.969 (94.616)	Acc@5 100.000 (99.927)
Epoch: [135][192/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.1901 (0.1535)	Acc@1 92.969 (94.594)	Acc@5 100.000 (99.921)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.23
Max memory: 51.4381312
 17.867s  j: 136 bis 140
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2395
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:136/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [136][0/196]	Time 0.161 (0.161)	Data 0.286 (0.286)	Loss 0.1575 (0.1575)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [136][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.1842 (0.1455)	Acc@1 93.750 (95.084)	Acc@5 99.609 (99.952)
Epoch: [136][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.1186 (0.1503)	Acc@1 96.484 (94.843)	Acc@5 100.000 (99.936)
Epoch: [136][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.1419 (0.1522)	Acc@1 95.703 (94.756)	Acc@5 99.609 (99.935)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:137/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [137][0/196]	Time 0.120 (0.120)	Data 0.311 (0.311)	Loss 0.1633 (0.1633)	Acc@1 96.094 (96.094)	Acc@5 99.609 (99.609)
Epoch: [137][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.1368 (0.1411)	Acc@1 95.312 (95.108)	Acc@5 100.000 (99.940)
Epoch: [137][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.1393 (0.1455)	Acc@1 95.312 (94.934)	Acc@5 100.000 (99.952)
Epoch: [137][192/196]	Time 0.077 (0.087)	Data 0.000 (0.002)	Loss 0.1866 (0.1514)	Acc@1 94.531 (94.744)	Acc@5 99.609 (99.937)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:138/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [138][0/196]	Time 0.130 (0.130)	Data 0.292 (0.292)	Loss 0.1343 (0.1343)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [138][64/196]	Time 0.094 (0.090)	Data 0.000 (0.005)	Loss 0.1762 (0.1501)	Acc@1 93.750 (94.772)	Acc@5 100.000 (99.934)
Epoch: [138][128/196]	Time 0.083 (0.089)	Data 0.000 (0.003)	Loss 0.2040 (0.1533)	Acc@1 91.797 (94.658)	Acc@5 100.000 (99.927)
Epoch: [138][192/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.1634 (0.1543)	Acc@1 94.141 (94.590)	Acc@5 100.000 (99.923)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:139/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [139][0/196]	Time 0.127 (0.127)	Data 0.315 (0.315)	Loss 0.1508 (0.1508)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [139][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.2231 (0.1542)	Acc@1 92.188 (94.639)	Acc@5 100.000 (99.898)
Epoch: [139][128/196]	Time 0.080 (0.087)	Data 0.000 (0.003)	Loss 0.1766 (0.1524)	Acc@1 92.578 (94.755)	Acc@5 100.000 (99.915)
Epoch: [139][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.1603 (0.1544)	Acc@1 95.312 (94.703)	Acc@5 100.000 (99.915)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:140/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [140][0/196]	Time 0.117 (0.117)	Data 0.304 (0.304)	Loss 0.1443 (0.1443)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [140][64/196]	Time 0.085 (0.087)	Data 0.000 (0.005)	Loss 0.1506 (0.1482)	Acc@1 94.531 (94.988)	Acc@5 100.000 (99.916)
Epoch: [140][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.1234 (0.1489)	Acc@1 95.703 (94.895)	Acc@5 100.000 (99.936)
Epoch: [140][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.2028 (0.1538)	Acc@1 93.359 (94.717)	Acc@5 100.000 (99.925)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.64
Max memory: 51.4381312
 17.412s  j: 141 bis 145
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1080
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 141
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:141/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [141][0/196]	Time 0.166 (0.166)	Data 0.288 (0.288)	Loss 0.1163 (0.1163)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [141][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.1817 (0.1420)	Acc@1 94.141 (95.294)	Acc@5 100.000 (99.952)
Epoch: [141][128/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.1193 (0.1479)	Acc@1 95.703 (94.970)	Acc@5 100.000 (99.912)
Epoch: [141][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.2025 (0.1496)	Acc@1 93.750 (94.819)	Acc@5 99.609 (99.925)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:142/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [142][0/196]	Time 0.132 (0.132)	Data 0.288 (0.288)	Loss 0.1251 (0.1251)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [142][64/196]	Time 0.088 (0.086)	Data 0.000 (0.005)	Loss 0.1468 (0.1527)	Acc@1 94.531 (94.694)	Acc@5 100.000 (99.958)
Epoch: [142][128/196]	Time 0.086 (0.086)	Data 0.000 (0.002)	Loss 0.1375 (0.1553)	Acc@1 94.922 (94.513)	Acc@5 100.000 (99.964)
Epoch: [142][192/196]	Time 0.084 (0.086)	Data 0.000 (0.002)	Loss 0.2139 (0.1569)	Acc@1 92.578 (94.466)	Acc@5 99.609 (99.957)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:143/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [143][0/196]	Time 0.117 (0.117)	Data 0.329 (0.329)	Loss 0.1463 (0.1463)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [143][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.1379 (0.1476)	Acc@1 94.922 (94.802)	Acc@5 99.609 (99.940)
Epoch: [143][128/196]	Time 0.079 (0.087)	Data 0.000 (0.003)	Loss 0.1680 (0.1524)	Acc@1 94.531 (94.549)	Acc@5 100.000 (99.936)
Epoch: [143][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.1195 (0.1550)	Acc@1 95.312 (94.551)	Acc@5 100.000 (99.929)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:144/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [144][0/196]	Time 0.128 (0.128)	Data 0.323 (0.323)	Loss 0.1572 (0.1572)	Acc@1 94.141 (94.141)	Acc@5 99.609 (99.609)
Epoch: [144][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.1690 (0.1588)	Acc@1 94.531 (94.519)	Acc@5 99.609 (99.904)
Epoch: [144][128/196]	Time 0.097 (0.088)	Data 0.000 (0.003)	Loss 0.1185 (0.1560)	Acc@1 94.922 (94.619)	Acc@5 100.000 (99.915)
Epoch: [144][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.1628 (0.1567)	Acc@1 92.969 (94.525)	Acc@5 100.000 (99.921)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:145/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [145][0/196]	Time 0.124 (0.124)	Data 0.283 (0.283)	Loss 0.1564 (0.1564)	Acc@1 95.312 (95.312)	Acc@5 99.609 (99.609)
Epoch: [145][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.1425 (0.1474)	Acc@1 94.141 (94.880)	Acc@5 100.000 (99.952)
Epoch: [145][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1556 (0.1512)	Acc@1 96.484 (94.692)	Acc@5 100.000 (99.927)
Epoch: [145][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.1826 (0.1516)	Acc@1 94.531 (94.679)	Acc@5 100.000 (99.921)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.94
Max memory: 51.4381312
 17.551s  j: 146 bis 150
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8057
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 146
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:146/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [146][0/196]	Time 0.145 (0.145)	Data 0.334 (0.334)	Loss 0.1349 (0.1349)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [146][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.1089 (0.1330)	Acc@1 97.266 (95.294)	Acc@5 100.000 (99.946)
Epoch: [146][128/196]	Time 0.093 (0.088)	Data 0.000 (0.003)	Loss 0.1679 (0.1385)	Acc@1 94.141 (95.203)	Acc@5 100.000 (99.945)
Epoch: [146][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.1276 (0.1449)	Acc@1 94.922 (94.964)	Acc@5 100.000 (99.941)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:147/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [147][0/196]	Time 0.127 (0.127)	Data 0.322 (0.322)	Loss 0.1424 (0.1424)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [147][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.1093 (0.1468)	Acc@1 96.484 (94.928)	Acc@5 99.609 (99.928)
Epoch: [147][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.1811 (0.1485)	Acc@1 93.359 (94.901)	Acc@5 99.609 (99.936)
Epoch: [147][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1498 (0.1512)	Acc@1 94.922 (94.796)	Acc@5 100.000 (99.925)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:148/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [148][0/196]	Time 0.133 (0.133)	Data 0.315 (0.315)	Loss 0.1775 (0.1775)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [148][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.1197 (0.1481)	Acc@1 97.266 (94.862)	Acc@5 100.000 (99.934)
Epoch: [148][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.0854 (0.1476)	Acc@1 96.484 (94.934)	Acc@5 100.000 (99.918)
Epoch: [148][192/196]	Time 0.096 (0.089)	Data 0.000 (0.002)	Loss 0.1392 (0.1516)	Acc@1 94.922 (94.732)	Acc@5 100.000 (99.931)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:149/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [149][0/196]	Time 0.118 (0.118)	Data 0.328 (0.328)	Loss 0.1502 (0.1502)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [149][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.1766 (0.1496)	Acc@1 91.797 (94.748)	Acc@5 100.000 (99.946)
Epoch: [149][128/196]	Time 0.080 (0.089)	Data 0.000 (0.003)	Loss 0.1676 (0.1499)	Acc@1 94.531 (94.761)	Acc@5 100.000 (99.933)
Epoch: [149][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1032 (0.1510)	Acc@1 97.266 (94.740)	Acc@5 100.000 (99.933)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:150/150; Lr: 0.0010000000000000002
batch Size 256
Epoch: [150][0/196]	Time 0.138 (0.138)	Data 0.302 (0.302)	Loss 0.1744 (0.1744)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [150][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.1024 (0.1287)	Acc@1 96.484 (95.655)	Acc@5 100.000 (99.958)
Epoch: [150][128/196]	Time 0.103 (0.088)	Data 0.000 (0.003)	Loss 0.1374 (0.1219)	Acc@1 95.703 (95.985)	Acc@5 100.000 (99.958)
Epoch: [150][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.1428 (0.1166)	Acc@1 94.922 (96.132)	Acc@5 100.000 (99.955)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.35
Max memory: 51.4381312
 17.683s  j: 151 bis 155
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1215
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 151
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:151/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [151][0/196]	Time 0.155 (0.155)	Data 0.288 (0.288)	Loss 0.1040 (0.1040)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [151][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.1229 (0.1041)	Acc@1 96.094 (96.418)	Acc@5 100.000 (99.970)
Epoch: [151][128/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0742 (0.1042)	Acc@1 98.047 (96.460)	Acc@5 100.000 (99.973)
Epoch: [151][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0829 (0.1032)	Acc@1 98.047 (96.567)	Acc@5 100.000 (99.970)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:152/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [152][0/196]	Time 0.122 (0.122)	Data 0.293 (0.293)	Loss 0.1213 (0.1213)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [152][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0823 (0.0991)	Acc@1 97.266 (96.695)	Acc@5 100.000 (99.976)
Epoch: [152][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.1097 (0.0984)	Acc@1 95.703 (96.687)	Acc@5 100.000 (99.982)
Epoch: [152][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.1348 (0.0984)	Acc@1 94.141 (96.685)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:153/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [153][0/196]	Time 0.136 (0.136)	Data 0.283 (0.283)	Loss 0.0838 (0.0838)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [153][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0793 (0.0951)	Acc@1 98.047 (96.845)	Acc@5 100.000 (99.982)
Epoch: [153][128/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.1373 (0.0967)	Acc@1 95.312 (96.845)	Acc@5 100.000 (99.979)
Epoch: [153][192/196]	Time 0.094 (0.087)	Data 0.000 (0.002)	Loss 0.1055 (0.0967)	Acc@1 97.266 (96.857)	Acc@5 100.000 (99.970)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:154/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [154][0/196]	Time 0.121 (0.121)	Data 0.305 (0.305)	Loss 0.0817 (0.0817)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [154][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0557 (0.0912)	Acc@1 98.047 (97.151)	Acc@5 100.000 (99.964)
Epoch: [154][128/196]	Time 0.095 (0.088)	Data 0.000 (0.003)	Loss 0.1013 (0.0930)	Acc@1 96.875 (96.996)	Acc@5 100.000 (99.955)
Epoch: [154][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.1101 (0.0944)	Acc@1 96.484 (96.924)	Acc@5 100.000 (99.960)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:155/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [155][0/196]	Time 0.129 (0.129)	Data 0.291 (0.291)	Loss 0.0780 (0.0780)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [155][64/196]	Time 0.080 (0.089)	Data 0.000 (0.005)	Loss 0.1090 (0.0890)	Acc@1 96.094 (97.109)	Acc@5 100.000 (99.976)
Epoch: [155][128/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0946 (0.0928)	Acc@1 97.656 (96.999)	Acc@5 100.000 (99.979)
Epoch: [155][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.1034 (0.0926)	Acc@1 96.875 (97.047)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.49
Max memory: 51.4381312
 17.561s  j: 156 bis 160
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2062
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 156
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:156/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [156][0/196]	Time 0.155 (0.155)	Data 0.265 (0.265)	Loss 0.0588 (0.0588)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [156][64/196]	Time 0.091 (0.088)	Data 0.000 (0.004)	Loss 0.0909 (0.0883)	Acc@1 95.703 (97.121)	Acc@5 100.000 (99.976)
Epoch: [156][128/196]	Time 0.090 (0.088)	Data 0.000 (0.002)	Loss 0.1203 (0.0899)	Acc@1 96.094 (97.090)	Acc@5 100.000 (99.973)
Epoch: [156][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0748 (0.0910)	Acc@1 97.656 (97.013)	Acc@5 100.000 (99.970)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:157/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [157][0/196]	Time 0.126 (0.126)	Data 0.299 (0.299)	Loss 0.0750 (0.0750)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [157][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.1207 (0.0880)	Acc@1 95.312 (97.121)	Acc@5 100.000 (99.976)
Epoch: [157][128/196]	Time 0.092 (0.089)	Data 0.000 (0.003)	Loss 0.0906 (0.0901)	Acc@1 98.047 (97.057)	Acc@5 100.000 (99.979)
Epoch: [157][192/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.0820 (0.0899)	Acc@1 97.266 (97.065)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:158/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [158][0/196]	Time 0.145 (0.145)	Data 0.280 (0.280)	Loss 0.0857 (0.0857)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [158][64/196]	Time 0.086 (0.091)	Data 0.000 (0.005)	Loss 0.0800 (0.0843)	Acc@1 97.266 (97.392)	Acc@5 100.000 (99.964)
Epoch: [158][128/196]	Time 0.086 (0.090)	Data 0.000 (0.002)	Loss 0.0898 (0.0856)	Acc@1 97.656 (97.347)	Acc@5 100.000 (99.970)
Epoch: [158][192/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.0830 (0.0865)	Acc@1 97.266 (97.270)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:159/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [159][0/196]	Time 0.125 (0.125)	Data 0.278 (0.278)	Loss 0.0703 (0.0703)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [159][64/196]	Time 0.089 (0.090)	Data 0.000 (0.004)	Loss 0.0693 (0.0898)	Acc@1 97.656 (96.989)	Acc@5 100.000 (99.976)
Epoch: [159][128/196]	Time 0.078 (0.089)	Data 0.000 (0.002)	Loss 0.1009 (0.0877)	Acc@1 96.094 (97.093)	Acc@5 100.000 (99.985)
Epoch: [159][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0732 (0.0871)	Acc@1 98.047 (97.154)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:160/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [160][0/196]	Time 0.135 (0.135)	Data 0.264 (0.264)	Loss 0.0929 (0.0929)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [160][64/196]	Time 0.086 (0.089)	Data 0.000 (0.004)	Loss 0.0712 (0.0875)	Acc@1 97.656 (97.109)	Acc@5 100.000 (99.964)
Epoch: [160][128/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.0773 (0.0879)	Acc@1 97.266 (97.081)	Acc@5 100.000 (99.970)
Epoch: [160][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0677 (0.0866)	Acc@1 97.656 (97.116)	Acc@5 100.000 (99.972)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.39
Max memory: 51.4381312
 17.598s  j: 161 bis 165
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6115
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 161
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:161/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [161][0/196]	Time 0.144 (0.144)	Data 0.310 (0.310)	Loss 0.0902 (0.0902)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [161][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0671 (0.0807)	Acc@1 98.438 (97.554)	Acc@5 100.000 (99.988)
Epoch: [161][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.0898 (0.0848)	Acc@1 98.438 (97.320)	Acc@5 100.000 (99.988)
Epoch: [161][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0595 (0.0845)	Acc@1 98.828 (97.272)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:162/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [162][0/196]	Time 0.114 (0.114)	Data 0.334 (0.334)	Loss 0.1061 (0.1061)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [162][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 0.0508 (0.0873)	Acc@1 98.438 (97.218)	Acc@5 100.000 (99.976)
Epoch: [162][128/196]	Time 0.080 (0.087)	Data 0.000 (0.003)	Loss 0.1226 (0.0872)	Acc@1 96.094 (97.169)	Acc@5 100.000 (99.973)
Epoch: [162][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0775 (0.0852)	Acc@1 97.656 (97.249)	Acc@5 99.609 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:163/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [163][0/196]	Time 0.124 (0.124)	Data 0.310 (0.310)	Loss 0.0671 (0.0671)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [163][64/196]	Time 0.086 (0.087)	Data 0.000 (0.005)	Loss 0.0734 (0.0806)	Acc@1 98.047 (97.410)	Acc@5 100.000 (99.988)
Epoch: [163][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.0849 (0.0825)	Acc@1 98.047 (97.329)	Acc@5 100.000 (99.973)
Epoch: [163][192/196]	Time 0.078 (0.086)	Data 0.000 (0.002)	Loss 0.0345 (0.0830)	Acc@1 99.219 (97.318)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:164/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [164][0/196]	Time 0.113 (0.113)	Data 0.314 (0.314)	Loss 0.0790 (0.0790)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [164][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 0.0840 (0.0844)	Acc@1 96.875 (97.242)	Acc@5 100.000 (99.988)
Epoch: [164][128/196]	Time 0.103 (0.087)	Data 0.000 (0.003)	Loss 0.0781 (0.0828)	Acc@1 97.266 (97.305)	Acc@5 100.000 (99.991)
Epoch: [164][192/196]	Time 0.098 (0.086)	Data 0.000 (0.002)	Loss 0.0755 (0.0830)	Acc@1 97.656 (97.353)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:165/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [165][0/196]	Time 0.121 (0.121)	Data 0.312 (0.312)	Loss 0.1402 (0.1402)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [165][64/196]	Time 0.095 (0.089)	Data 0.000 (0.005)	Loss 0.0875 (0.0863)	Acc@1 97.266 (97.169)	Acc@5 100.000 (99.982)
Epoch: [165][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0542 (0.0867)	Acc@1 99.219 (97.181)	Acc@5 100.000 (99.973)
Epoch: [165][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0675 (0.0848)	Acc@1 98.047 (97.237)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.55
Max memory: 51.4381312
 17.506s  j: 166 bis 170
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5913
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 166
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:166/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [166][0/196]	Time 0.170 (0.170)	Data 0.305 (0.305)	Loss 0.0617 (0.0617)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [166][64/196]	Time 0.099 (0.090)	Data 0.000 (0.005)	Loss 0.0391 (0.0791)	Acc@1 99.609 (97.452)	Acc@5 100.000 (99.988)
Epoch: [166][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.1114 (0.0812)	Acc@1 96.484 (97.384)	Acc@5 100.000 (99.988)
Epoch: [166][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.1184 (0.0826)	Acc@1 95.703 (97.302)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:167/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [167][0/196]	Time 0.130 (0.130)	Data 0.308 (0.308)	Loss 0.0513 (0.0513)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [167][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.1011 (0.0794)	Acc@1 95.703 (97.422)	Acc@5 100.000 (99.982)
Epoch: [167][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0709 (0.0797)	Acc@1 98.438 (97.435)	Acc@5 100.000 (99.982)
Epoch: [167][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.0784 (0.0797)	Acc@1 97.266 (97.399)	Acc@5 99.609 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:168/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [168][0/196]	Time 0.128 (0.128)	Data 0.303 (0.303)	Loss 0.0862 (0.0862)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [168][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.1040 (0.0787)	Acc@1 96.484 (97.614)	Acc@5 100.000 (99.964)
Epoch: [168][128/196]	Time 0.090 (0.087)	Data 0.000 (0.003)	Loss 0.0669 (0.0790)	Acc@1 98.828 (97.565)	Acc@5 100.000 (99.979)
Epoch: [168][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0876 (0.0799)	Acc@1 96.875 (97.500)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:169/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [169][0/196]	Time 0.122 (0.122)	Data 0.282 (0.282)	Loss 0.0886 (0.0886)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [169][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.1116 (0.0819)	Acc@1 96.484 (97.344)	Acc@5 100.000 (99.988)
Epoch: [169][128/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0594 (0.0819)	Acc@1 98.438 (97.387)	Acc@5 100.000 (99.970)
Epoch: [169][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0681 (0.0807)	Acc@1 97.656 (97.432)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:170/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [170][0/196]	Time 0.133 (0.133)	Data 0.369 (0.369)	Loss 0.0744 (0.0744)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [170][64/196]	Time 0.078 (0.087)	Data 0.000 (0.006)	Loss 0.0856 (0.0799)	Acc@1 96.875 (97.506)	Acc@5 100.000 (99.988)
Epoch: [170][128/196]	Time 0.079 (0.087)	Data 0.000 (0.003)	Loss 0.0566 (0.0799)	Acc@1 98.828 (97.511)	Acc@5 100.000 (99.991)
Epoch: [170][192/196]	Time 0.078 (0.086)	Data 0.000 (0.002)	Loss 0.0743 (0.0790)	Acc@1 97.266 (97.525)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.51
Max memory: 51.4381312
 17.302s  j: 171 bis 175
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8782
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 171
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:171/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [171][0/196]	Time 0.147 (0.147)	Data 0.325 (0.325)	Loss 0.0867 (0.0867)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [171][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.0887 (0.0767)	Acc@1 97.656 (97.512)	Acc@5 100.000 (99.994)
Epoch: [171][128/196]	Time 0.097 (0.088)	Data 0.000 (0.003)	Loss 0.1179 (0.0776)	Acc@1 94.922 (97.505)	Acc@5 100.000 (99.988)
Epoch: [171][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0772 (0.0772)	Acc@1 97.266 (97.521)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:172/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [172][0/196]	Time 0.137 (0.137)	Data 0.274 (0.274)	Loss 0.0560 (0.0560)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [172][64/196]	Time 0.092 (0.088)	Data 0.000 (0.005)	Loss 0.0663 (0.0784)	Acc@1 98.047 (97.584)	Acc@5 100.000 (99.976)
Epoch: [172][128/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0565 (0.0785)	Acc@1 98.438 (97.499)	Acc@5 100.000 (99.976)
Epoch: [172][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.1091 (0.0783)	Acc@1 96.875 (97.482)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:173/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [173][0/196]	Time 0.110 (0.110)	Data 0.290 (0.290)	Loss 0.0628 (0.0628)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [173][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0547 (0.0708)	Acc@1 99.219 (97.728)	Acc@5 100.000 (99.976)
Epoch: [173][128/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.0701 (0.0735)	Acc@1 97.656 (97.674)	Acc@5 100.000 (99.964)
Epoch: [173][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0887 (0.0755)	Acc@1 97.266 (97.527)	Acc@5 99.609 (99.968)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:174/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [174][0/196]	Time 0.120 (0.120)	Data 0.291 (0.291)	Loss 0.0534 (0.0534)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [174][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.0673 (0.0753)	Acc@1 98.047 (97.632)	Acc@5 100.000 (99.976)
Epoch: [174][128/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0871 (0.0770)	Acc@1 97.266 (97.505)	Acc@5 100.000 (99.976)
Epoch: [174][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0469 (0.0772)	Acc@1 99.219 (97.460)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:175/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [175][0/196]	Time 0.143 (0.143)	Data 0.310 (0.310)	Loss 0.0951 (0.0951)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [175][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.0502 (0.0758)	Acc@1 97.656 (97.476)	Acc@5 100.000 (99.982)
Epoch: [175][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0681 (0.0728)	Acc@1 98.438 (97.674)	Acc@5 100.000 (99.982)
Epoch: [175][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0943 (0.0744)	Acc@1 96.875 (97.638)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.66
Max memory: 51.4381312
 17.519s  j: 176 bis 180
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 58
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 176
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:176/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [176][0/196]	Time 0.141 (0.141)	Data 0.296 (0.296)	Loss 0.0781 (0.0781)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [176][64/196]	Time 0.102 (0.092)	Data 0.000 (0.005)	Loss 0.1009 (0.0754)	Acc@1 96.875 (97.626)	Acc@5 100.000 (99.982)
Epoch: [176][128/196]	Time 0.092 (0.090)	Data 0.000 (0.002)	Loss 0.0837 (0.0771)	Acc@1 96.094 (97.508)	Acc@5 100.000 (99.985)
Epoch: [176][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.0958 (0.0771)	Acc@1 97.266 (97.504)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:177/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [177][0/196]	Time 0.127 (0.127)	Data 0.303 (0.303)	Loss 0.0686 (0.0686)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [177][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.0699 (0.0767)	Acc@1 96.484 (97.566)	Acc@5 100.000 (99.976)
Epoch: [177][128/196]	Time 0.084 (0.090)	Data 0.000 (0.003)	Loss 0.0864 (0.0786)	Acc@1 97.266 (97.441)	Acc@5 100.000 (99.979)
Epoch: [177][192/196]	Time 0.096 (0.089)	Data 0.000 (0.002)	Loss 0.0666 (0.0781)	Acc@1 97.266 (97.498)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:178/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [178][0/196]	Time 0.150 (0.150)	Data 0.294 (0.294)	Loss 0.0557 (0.0557)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [178][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.1023 (0.0757)	Acc@1 96.875 (97.584)	Acc@5 100.000 (99.994)
Epoch: [178][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.1182 (0.0751)	Acc@1 96.484 (97.565)	Acc@5 100.000 (99.985)
Epoch: [178][192/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.0746 (0.0755)	Acc@1 97.266 (97.571)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:179/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [179][0/196]	Time 0.112 (0.112)	Data 0.286 (0.286)	Loss 0.0867 (0.0867)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [179][64/196]	Time 0.086 (0.091)	Data 0.000 (0.005)	Loss 0.0759 (0.0730)	Acc@1 96.484 (97.650)	Acc@5 100.000 (99.976)
Epoch: [179][128/196]	Time 0.083 (0.090)	Data 0.000 (0.002)	Loss 0.0701 (0.0744)	Acc@1 98.438 (97.571)	Acc@5 100.000 (99.979)
Epoch: [179][192/196]	Time 0.083 (0.090)	Data 0.000 (0.002)	Loss 0.0828 (0.0743)	Acc@1 97.266 (97.598)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:180/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [180][0/196]	Time 0.119 (0.119)	Data 0.290 (0.290)	Loss 0.0684 (0.0684)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [180][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0313 (0.0714)	Acc@1 99.219 (97.849)	Acc@5 100.000 (99.976)
Epoch: [180][128/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.0841 (0.0724)	Acc@1 97.656 (97.747)	Acc@5 100.000 (99.979)
Epoch: [180][192/196]	Time 0.080 (0.089)	Data 0.000 (0.002)	Loss 0.1011 (0.0737)	Acc@1 96.094 (97.664)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
Stage: 3
width of Layers: [8, 16, 32]
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 32
stage: 3; tobestage: 3
Num: 23
width: 32
stage: 3; tobestage: 3
Num: 24
width: 32
stage: 3; tobestage: 3
Num: 25
width: 32
stage: 3; tobestage: 3
Num: 26
width: 32
stage: 3; tobestage: 3
Num: 27
width: 32
stage: 3; tobestage: 3
Num: 28
width: 32
stage: 3; tobestage: 3
Num: 29
width: 32
stage: 3; tobestage: 3
Num: 30
width: 32
stage: 3; tobestage: 3
Num: 31
width: 32
stage: 3; tobestage: 3
Num: 32
width: 32
stage: 3; tobestage: 3
Num: 33
altList: ['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']
Residual ListI: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
Residual ListO: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
j: 23
Resiudual I
new width: 32; old width: 16
Residual O
old width1: 32; new width: 64
c:[[[-9.93947387e-02 -8.77043828e-02 -1.69341639e-02]
  [-4.77717593e-02 -6.92748129e-02 -1.37123261e-02]
  [ 5.76269929e-04 -7.01218396e-02 -3.66765223e-02]]

 [[ 4.54461649e-02  6.06414415e-02  2.65946630e-02]
  [ 4.69482355e-02  9.52033103e-02  3.62385400e-02]
  [-2.08627284e-02 -7.62478961e-03 -1.19744111e-02]]

 [[ 2.41907630e-02  8.65234286e-02  6.93233907e-02]
  [ 4.08516973e-02  9.89449024e-02  6.94945082e-02]
  [-2.84880195e-02  5.71541768e-03  1.25883203e-02]]

 [[ 3.05962116e-02  1.26711816e-01  9.57829431e-02]
  [ 6.67589158e-02  1.49125069e-01  1.07962772e-01]
  [-5.66576794e-03  4.56574708e-02  4.47543487e-02]]

 [[-2.29060766e-03 -4.52623963e-02 -6.34641945e-02]
  [ 3.42793874e-02  4.79302816e-02  3.31362411e-02]
  [ 1.32759726e-02  7.13887885e-02  6.55979589e-02]]

 [[ 3.27807479e-02  3.61994132e-02  3.96474749e-02]
  [ 4.98310700e-02  1.08743563e-01  8.40106532e-02]
  [ 9.66061130e-02  1.32690892e-01  9.65919942e-02]]

 [[-2.30258275e-02 -7.95675665e-02 -5.18607050e-02]
  [-5.83662316e-02 -7.73373395e-02 -4.77148853e-02]
  [-3.46078984e-02 -4.20084521e-02 -2.89598741e-02]]

 [[ 4.41512279e-03 -3.20229121e-02 -1.95006225e-02]
  [ 5.01546310e-03  1.18124625e-02  9.88511741e-03]
  [-2.62519829e-02  3.63640040e-02  6.95560593e-03]]

 [[ 5.92628531e-02  9.18866694e-02  2.06387378e-02]
  [ 7.01416805e-02  1.23653978e-01  6.01381660e-02]
  [-2.29833759e-02  4.21917923e-02  1.66304912e-02]]

 [[ 1.38974609e-02  3.84407863e-02 -4.89923358e-03]
  [ 1.36530830e-03  9.07534435e-02  4.83219959e-02]
  [-9.09906439e-03  4.98215891e-02  2.34302226e-02]]

 [[-3.23432423e-02 -5.35831936e-02 -2.73066889e-02]
  [ 3.56860012e-02 -6.94837868e-02 -6.77561238e-02]
  [ 5.63038699e-02  1.63720958e-02 -1.07274121e-02]]

 [[-9.00598019e-02 -1.21614784e-01 -6.93473294e-02]
  [-5.79303540e-02 -1.55782670e-01 -8.97743851e-02]
  [ 3.13215777e-02 -2.96971120e-04 -2.31232755e-02]]

 [[ 1.00392297e-01  7.12121651e-02  3.26303691e-02]
  [-4.29902226e-02 -7.66397268e-02 -1.47988703e-02]
  [-1.19845740e-01 -1.21131174e-01 -4.58325446e-02]]

 [[ 1.36000686e-03 -4.50190641e-02  9.39213485e-03]
  [-4.58584018e-02 -1.09485276e-01 -1.08848020e-01]
  [-5.70173748e-02 -8.45927745e-02 -8.87286142e-02]]

 [[-4.67267819e-02 -6.82441657e-03  2.20123604e-02]
  [ 1.40748890e-02  2.00959947e-02  1.99954901e-02]
  [ 2.88763661e-02  4.60742675e-02  1.28036803e-02]]

 [[-7.09946081e-03 -6.82172850e-02 -4.07130495e-02]
  [-3.31167094e-02 -6.31037727e-02 -7.93448687e-02]
  [-3.73056158e-02 -3.64143960e-02 -3.44316997e-02]]

 [[ 6.12564608e-02  9.16605145e-02  1.82313100e-02]
  [ 6.86774105e-02  1.21783353e-01  5.51896542e-02]
  [-2.33461056e-02  4.34904359e-02  1.74589381e-02]]

 [[ 9.75460932e-02  7.98191056e-02  3.08053587e-02]
  [-4.68314514e-02 -7.48159215e-02 -1.88060272e-02]
  [-1.15959898e-01 -1.21677294e-01 -4.94922660e-02]]

 [[ 9.70357358e-02  7.41854906e-02  3.30608822e-02]
  [-4.83115204e-02 -7.76788294e-02 -2.32642312e-02]
  [-1.17194362e-01 -1.17806435e-01 -4.96782884e-02]]

 [[-8.69185999e-02 -1.19883507e-01 -7.56128356e-02]
  [-5.70081174e-02 -1.59388304e-01 -8.60160664e-02]
  [ 3.58419195e-02 -2.38232710e-03 -2.23557279e-02]]

 [[-1.86307877e-02 -6.65952191e-02 -3.82123850e-02]
  [-3.24787945e-02 -6.38876855e-02 -7.94108883e-02]
  [-3.65232341e-02 -3.44211981e-02 -3.05685978e-02]]

 [[ 3.97189334e-02  5.78064471e-02  3.43949683e-02]
  [ 5.04181907e-02  8.75863656e-02  3.23777720e-02]
  [-2.44447012e-02 -8.40868149e-03 -1.01654232e-02]]

 [[-1.01583116e-01 -9.05112848e-02 -1.64197348e-02]
  [-5.03991097e-02 -7.44738132e-02 -1.81707107e-02]
  [-3.52839055e-03 -7.08427206e-02 -4.37231883e-02]]

 [[ 2.55498346e-02  9.49341729e-02  6.80061877e-02]
  [ 4.90607098e-02  1.05050489e-01  6.78173229e-02]
  [-2.92023867e-02  1.17310788e-03  8.25051498e-03]]

 [[-1.06328167e-01 -8.87089893e-02 -1.84482690e-02]
  [-5.22900000e-02 -7.48588145e-02 -1.66915040e-02]
  [-3.89495725e-03 -7.56249130e-02 -4.09057923e-02]]

 [[ 6.01567216e-02  8.92435089e-02  1.56630948e-02]
  [ 6.41510636e-02  1.16131380e-01  5.54031841e-02]
  [-1.60537753e-02  3.88813317e-02  1.89967379e-02]]

 [[-5.28387353e-02 -1.04443850e-02  2.21155100e-02]
  [ 1.49032595e-02  2.33065709e-02  1.09644542e-02]
  [ 3.09982263e-02  4.34628762e-02  1.59816500e-02]]

 [[ 1.20935328e-02  3.74065377e-02 -4.29003825e-03]
  [-6.02438929e-04  8.24413076e-02  4.91002873e-02]
  [-9.57659446e-03  4.85154577e-02  2.63395868e-02]]

 [[ 3.97351645e-02  3.72048989e-02  3.82561758e-02]
  [ 4.86165807e-02  1.06191501e-01  7.68152624e-02]
  [ 8.86062831e-02  1.31451041e-01  1.00945920e-01]]

 [[-5.00157215e-02 -1.14086578e-02  2.45503578e-02]
  [ 1.31225372e-02  1.85195785e-02  1.20884404e-02]
  [ 3.13367657e-02  4.80573550e-02  1.75321866e-02]]

 [[ 3.03820055e-02  3.74124572e-02  3.65838222e-02]
  [ 4.83251885e-02  1.03943616e-01  8.52080062e-02]
  [ 8.98825973e-02  1.32612109e-01  1.00762643e-01]]

 [[-1.02520362e-01 -8.09214339e-02 -1.61640607e-02]
  [-4.60973047e-02 -6.75091594e-02 -1.45851821e-02]
  [ 6.53749885e-05 -6.99360669e-02 -3.30606923e-02]]]
Traceback (most recent call last):
  File "main.py", line 965, in <module>
    main()
  File "main.py", line 558, in main
    model = model.wider(3, 2, out_size=None, weight_norm=None, random_init=False, addNoise=True)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 883, in wider
    f[m] = f[m] / ct.get(listindices[idx])
TypeError: unsupported operand type(s) for /: 'float' and 'NoneType'
j: 181 bis 185
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6977
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 176
Max memory: 0.1097216
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:176/180; Lr: 0.0010000000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [176][0/196]	Time 0.148 (0.148)	Data 0.320 (0.320)	Loss 0.0625 (0.0625)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [176][64/196]	Time 0.082 (0.090)	Data 0.000 (0.005)	Loss 0.0936 (0.0767)	Acc@1 96.875 (97.518)	Acc@5 100.000 (99.982)
Epoch: [176][128/196]	Time 0.082 (0.089)	Data 0.000 (0.003)	Loss 0.0659 (0.0785)	Acc@1 97.266 (97.402)	Acc@5 100.000 (99.985)
Epoch: [176][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0954 (0.0781)	Acc@1 95.703 (97.476)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:177/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [177][0/196]	Time 0.122 (0.122)	Data 0.312 (0.312)	Loss 0.0645 (0.0645)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [177][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.0619 (0.0728)	Acc@1 98.828 (97.644)	Acc@5 99.609 (99.994)
Epoch: [177][128/196]	Time 0.098 (0.088)	Data 0.000 (0.003)	Loss 0.0718 (0.0774)	Acc@1 98.047 (97.435)	Acc@5 100.000 (99.991)
Epoch: [177][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0530 (0.0775)	Acc@1 98.047 (97.458)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:178/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [178][0/196]	Time 0.143 (0.143)	Data 0.297 (0.297)	Loss 0.0485 (0.0485)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [178][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.1097 (0.0783)	Acc@1 96.484 (97.584)	Acc@5 99.609 (99.970)
Epoch: [178][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0743 (0.0784)	Acc@1 97.266 (97.469)	Acc@5 100.000 (99.970)
Epoch: [178][192/196]	Time 0.093 (0.087)	Data 0.000 (0.002)	Loss 0.0493 (0.0772)	Acc@1 98.438 (97.533)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:179/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [179][0/196]	Time 0.128 (0.128)	Data 0.321 (0.321)	Loss 0.0608 (0.0608)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [179][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0765 (0.0771)	Acc@1 97.656 (97.512)	Acc@5 100.000 (99.976)
Epoch: [179][128/196]	Time 0.096 (0.088)	Data 0.000 (0.003)	Loss 0.0894 (0.0738)	Acc@1 96.484 (97.602)	Acc@5 100.000 (99.979)
Epoch: [179][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0779 (0.0749)	Acc@1 98.047 (97.563)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:180/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [180][0/196]	Time 0.122 (0.122)	Data 0.333 (0.333)	Loss 0.0891 (0.0891)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [180][64/196]	Time 0.086 (0.086)	Data 0.000 (0.005)	Loss 0.0800 (0.0724)	Acc@1 96.875 (97.572)	Acc@5 100.000 (99.994)
Epoch: [180][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.0682 (0.0730)	Acc@1 98.047 (97.635)	Acc@5 99.609 (99.979)
Epoch: [180][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0562 (0.0738)	Acc@1 98.047 (97.587)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
Model: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
[INFO] Storing checkpoint...
  90.49
Max memory: 51.4381312
 17.467s  j: 186 bis 190
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3072
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 181
Max memory: 0.1097216
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:181/185; Lr: 0.0009000000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [181][0/196]	Time 0.189 (0.189)	Data 0.298 (0.298)	Loss 0.0580 (0.0580)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [181][64/196]	Time 0.087 (0.091)	Data 0.000 (0.005)	Loss 0.0718 (0.0698)	Acc@1 98.047 (97.752)	Acc@5 100.000 (99.994)
Epoch: [181][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0865 (0.0713)	Acc@1 96.875 (97.720)	Acc@5 100.000 (99.979)
Epoch: [181][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0764 (0.0734)	Acc@1 97.266 (97.646)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:182/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [182][0/196]	Time 0.140 (0.140)	Data 0.278 (0.278)	Loss 0.0532 (0.0532)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [182][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0650 (0.0700)	Acc@1 97.266 (97.638)	Acc@5 100.000 (99.982)
Epoch: [182][128/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0765 (0.0721)	Acc@1 97.656 (97.629)	Acc@5 100.000 (99.985)
Epoch: [182][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0703 (0.0735)	Acc@1 98.047 (97.614)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:183/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [183][0/196]	Time 0.112 (0.112)	Data 0.310 (0.310)	Loss 0.0475 (0.0475)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [183][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.0864 (0.0689)	Acc@1 96.875 (97.806)	Acc@5 100.000 (99.994)
Epoch: [183][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.0584 (0.0706)	Acc@1 98.047 (97.777)	Acc@5 100.000 (99.988)
Epoch: [183][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0647 (0.0729)	Acc@1 98.047 (97.670)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:184/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [184][0/196]	Time 0.122 (0.122)	Data 0.306 (0.306)	Loss 0.0560 (0.0560)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [184][64/196]	Time 0.095 (0.088)	Data 0.000 (0.005)	Loss 0.0618 (0.0710)	Acc@1 98.828 (97.819)	Acc@5 100.000 (99.964)
Epoch: [184][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0984 (0.0742)	Acc@1 96.875 (97.656)	Acc@5 99.609 (99.961)
Epoch: [184][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0936 (0.0748)	Acc@1 96.094 (97.598)	Acc@5 100.000 (99.968)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:185/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [185][0/196]	Time 0.122 (0.122)	Data 0.327 (0.327)	Loss 0.1031 (0.1031)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [185][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.0657 (0.0767)	Acc@1 97.266 (97.470)	Acc@5 100.000 (99.994)
Epoch: [185][128/196]	Time 0.085 (0.089)	Data 0.000 (0.003)	Loss 0.0904 (0.0746)	Acc@1 96.875 (97.526)	Acc@5 100.000 (99.994)
Epoch: [185][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0610 (0.0746)	Acc@1 98.047 (97.551)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.4
Max memory: 51.4381312
 17.717s  j: 191 bis 195
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2092
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 186
Max memory: 0.1097216
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:186/190; Lr: 0.0008100000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [186][0/196]	Time 0.136 (0.136)	Data 0.297 (0.297)	Loss 0.0573 (0.0573)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [186][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.0700 (0.0685)	Acc@1 98.047 (97.831)	Acc@5 100.000 (99.982)
Epoch: [186][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0691 (0.0675)	Acc@1 97.656 (97.871)	Acc@5 100.000 (99.991)
Epoch: [186][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0652 (0.0703)	Acc@1 97.266 (97.717)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:187/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [187][0/196]	Time 0.137 (0.137)	Data 0.286 (0.286)	Loss 0.0619 (0.0619)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [187][64/196]	Time 0.080 (0.088)	Data 0.000 (0.005)	Loss 0.0527 (0.0719)	Acc@1 98.047 (97.614)	Acc@5 100.000 (99.988)
Epoch: [187][128/196]	Time 0.096 (0.088)	Data 0.000 (0.002)	Loss 0.0472 (0.0712)	Acc@1 98.828 (97.647)	Acc@5 100.000 (99.991)
Epoch: [187][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0671 (0.0711)	Acc@1 98.438 (97.670)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:188/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [188][0/196]	Time 0.122 (0.122)	Data 0.315 (0.315)	Loss 0.0642 (0.0642)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [188][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.0459 (0.0723)	Acc@1 98.828 (97.782)	Acc@5 100.000 (99.970)
Epoch: [188][128/196]	Time 0.095 (0.089)	Data 0.000 (0.003)	Loss 0.0930 (0.0731)	Acc@1 96.094 (97.696)	Acc@5 100.000 (99.973)
Epoch: [188][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.0532 (0.0723)	Acc@1 98.438 (97.705)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:189/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [189][0/196]	Time 0.111 (0.111)	Data 0.301 (0.301)	Loss 0.0784 (0.0784)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [189][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.0617 (0.0702)	Acc@1 98.828 (97.770)	Acc@5 100.000 (99.970)
Epoch: [189][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.0623 (0.0685)	Acc@1 98.828 (97.865)	Acc@5 100.000 (99.976)
Epoch: [189][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.1035 (0.0696)	Acc@1 95.312 (97.847)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0008100000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:190/190; Lr: 0.0008100000000000002
batch Size 256
Epoch: [190][0/196]	Time 0.137 (0.137)	Data 0.274 (0.274)	Loss 0.0362 (0.0362)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [190][64/196]	Time 0.090 (0.089)	Data 0.000 (0.004)	Loss 0.0506 (0.0686)	Acc@1 98.438 (97.758)	Acc@5 100.000 (99.988)
Epoch: [190][128/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0731 (0.0699)	Acc@1 98.047 (97.684)	Acc@5 100.000 (99.982)
Epoch: [190][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0976 (0.0706)	Acc@1 96.484 (97.687)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.34
Max memory: 51.4381312
 17.667s  j: 196 bis 200
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7672
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 191
Max memory: 0.1097216
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:191/195; Lr: 0.0007290000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [191][0/196]	Time 0.146 (0.146)	Data 0.296 (0.296)	Loss 0.0687 (0.0687)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [191][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.0708 (0.0673)	Acc@1 98.047 (97.945)	Acc@5 100.000 (99.988)
Epoch: [191][128/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0727 (0.0676)	Acc@1 96.875 (97.898)	Acc@5 100.000 (99.985)
Epoch: [191][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0488 (0.0681)	Acc@1 98.828 (97.851)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:192/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [192][0/196]	Time 0.130 (0.130)	Data 0.343 (0.343)	Loss 0.1011 (0.1011)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [192][64/196]	Time 0.088 (0.087)	Data 0.000 (0.005)	Loss 0.0438 (0.0716)	Acc@1 99.219 (97.794)	Acc@5 100.000 (99.976)
Epoch: [192][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.0727 (0.0691)	Acc@1 96.875 (97.823)	Acc@5 100.000 (99.979)
Epoch: [192][192/196]	Time 0.097 (0.087)	Data 0.000 (0.002)	Loss 0.0809 (0.0690)	Acc@1 97.266 (97.816)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:193/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [193][0/196]	Time 0.129 (0.129)	Data 0.312 (0.312)	Loss 0.0636 (0.0636)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [193][64/196]	Time 0.091 (0.087)	Data 0.000 (0.005)	Loss 0.0707 (0.0707)	Acc@1 96.875 (97.758)	Acc@5 100.000 (99.982)
Epoch: [193][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.0918 (0.0697)	Acc@1 96.875 (97.744)	Acc@5 100.000 (99.982)
Epoch: [193][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0671 (0.0701)	Acc@1 97.656 (97.733)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0007290000000000002
lr: 0.0007290000000000002
Epoche:194/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [194][0/196]	Time 0.121 (0.121)	Data 0.289 (0.289)	Loss 0.0827 (0.0827)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [194][64/196]	Time 0.095 (0.088)	Data 0.000 (0.005)	Loss 0.0558 (0.0680)	Acc@1 98.828 (97.776)	Acc@5 100.000 (99.982)
Epoch: [194][128/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0744 (0.0683)	Acc@1 97.266 (97.802)	Acc@5 100.000 (99.979)
Epoch: [194][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0730 (0.0696)	Acc@1 97.656 (97.766)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0007290000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:195/195; Lr: 0.0007290000000000002
batch Size 256
Epoch: [195][0/196]	Time 0.117 (0.117)	Data 0.306 (0.306)	Loss 0.0861 (0.0861)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [195][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.1189 (0.0680)	Acc@1 96.094 (97.812)	Acc@5 100.000 (99.994)
Epoch: [195][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.0700 (0.0712)	Acc@1 98.047 (97.735)	Acc@5 100.000 (99.982)
Epoch: [195][192/196]	Time 0.082 (0.086)	Data 0.000 (0.002)	Loss 0.0522 (0.0710)	Acc@1 98.438 (97.723)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.38
Max memory: 51.4381312
 17.330s  j: 201 bis 205
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8622
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 196
Max memory: 0.1097216
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:196/200; Lr: 0.0006561000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [196][0/196]	Time 0.130 (0.130)	Data 0.328 (0.328)	Loss 0.0572 (0.0572)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [196][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.0853 (0.0661)	Acc@1 96.875 (97.873)	Acc@5 100.000 (99.988)
Epoch: [196][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0637 (0.0678)	Acc@1 98.828 (97.799)	Acc@5 100.000 (99.985)
Epoch: [196][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0454 (0.0681)	Acc@1 98.828 (97.790)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:197/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [197][0/196]	Time 0.122 (0.122)	Data 0.328 (0.328)	Loss 0.0782 (0.0782)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [197][64/196]	Time 0.093 (0.088)	Data 0.000 (0.005)	Loss 0.0576 (0.0672)	Acc@1 98.047 (97.903)	Acc@5 100.000 (99.982)
Epoch: [197][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0538 (0.0676)	Acc@1 98.438 (97.868)	Acc@5 100.000 (99.979)
Epoch: [197][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.1181 (0.0673)	Acc@1 96.484 (97.873)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:198/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [198][0/196]	Time 0.119 (0.119)	Data 0.308 (0.308)	Loss 0.0600 (0.0600)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [198][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0426 (0.0675)	Acc@1 98.438 (97.897)	Acc@5 100.000 (100.000)
Epoch: [198][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.0493 (0.0664)	Acc@1 98.438 (97.971)	Acc@5 100.000 (100.000)
Epoch: [198][192/196]	Time 0.081 (0.086)	Data 0.000 (0.002)	Loss 0.0568 (0.0680)	Acc@1 98.438 (97.913)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0006561000000000002
lr: 0.0006561000000000002
Epoche:199/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [199][0/196]	Time 0.126 (0.126)	Data 0.335 (0.335)	Loss 0.0598 (0.0598)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [199][64/196]	Time 0.092 (0.088)	Data 0.000 (0.005)	Loss 0.0977 (0.0659)	Acc@1 96.094 (97.981)	Acc@5 100.000 (99.976)
Epoch: [199][128/196]	Time 0.081 (0.087)	Data 0.000 (0.003)	Loss 0.0627 (0.0646)	Acc@1 97.266 (97.968)	Acc@5 100.000 (99.988)
Epoch: [199][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0698 (0.0659)	Acc@1 98.047 (97.921)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0006561000000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:200/200; Lr: 0.0006561000000000002
batch Size 256
Epoch: [200][0/196]	Time 0.118 (0.118)	Data 0.311 (0.311)	Loss 0.0562 (0.0562)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [200][64/196]	Time 0.080 (0.087)	Data 0.000 (0.005)	Loss 0.0506 (0.0683)	Acc@1 98.047 (97.722)	Acc@5 100.000 (99.970)
Epoch: [200][128/196]	Time 0.081 (0.087)	Data 0.000 (0.003)	Loss 0.0798 (0.0676)	Acc@1 98.047 (97.729)	Acc@5 100.000 (99.979)
Epoch: [200][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0625 (0.0683)	Acc@1 97.656 (97.679)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.33
Max memory: 51.4381312
 17.365s  j: 206 bis 210
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7115
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 201
Max memory: 0.1097216
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:201/205; Lr: 0.0005904900000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [201][0/196]	Time 0.148 (0.148)	Data 0.285 (0.285)	Loss 0.0639 (0.0639)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [201][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.0462 (0.0677)	Acc@1 99.219 (97.891)	Acc@5 100.000 (99.970)
Epoch: [201][128/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0982 (0.0671)	Acc@1 96.875 (97.895)	Acc@5 100.000 (99.979)
Epoch: [201][192/196]	Time 0.078 (0.087)	Data 0.000 (0.002)	Loss 0.0743 (0.0656)	Acc@1 97.266 (97.960)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:202/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [202][0/196]	Time 0.137 (0.137)	Data 0.261 (0.261)	Loss 0.0734 (0.0734)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [202][64/196]	Time 0.085 (0.089)	Data 0.000 (0.004)	Loss 0.0939 (0.0654)	Acc@1 97.266 (97.897)	Acc@5 100.000 (99.976)
Epoch: [202][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0418 (0.0660)	Acc@1 99.219 (97.902)	Acc@5 100.000 (99.979)
Epoch: [202][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0790 (0.0649)	Acc@1 97.656 (97.974)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:203/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [203][0/196]	Time 0.112 (0.112)	Data 0.298 (0.298)	Loss 0.0851 (0.0851)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [203][64/196]	Time 0.079 (0.088)	Data 0.000 (0.005)	Loss 0.0449 (0.0662)	Acc@1 98.828 (97.867)	Acc@5 100.000 (100.000)
Epoch: [203][128/196]	Time 0.096 (0.087)	Data 0.000 (0.003)	Loss 0.0697 (0.0645)	Acc@1 96.875 (97.920)	Acc@5 100.000 (100.000)
Epoch: [203][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0936 (0.0652)	Acc@1 96.875 (97.927)	Acc@5 99.609 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005904900000000002
lr: 0.0005904900000000002
Epoche:204/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [204][0/196]	Time 0.127 (0.127)	Data 0.344 (0.344)	Loss 0.0576 (0.0576)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [204][64/196]	Time 0.085 (0.088)	Data 0.000 (0.006)	Loss 0.0520 (0.0640)	Acc@1 98.047 (97.981)	Acc@5 100.000 (99.988)
Epoch: [204][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.0654 (0.0659)	Acc@1 98.047 (97.917)	Acc@5 100.000 (99.991)
Epoch: [204][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0509 (0.0655)	Acc@1 99.219 (97.946)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0005904900000000002
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:205/205; Lr: 0.0005904900000000002
batch Size 256
Epoch: [205][0/196]	Time 0.135 (0.135)	Data 0.280 (0.280)	Loss 0.0939 (0.0939)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [205][64/196]	Time 0.079 (0.088)	Data 0.000 (0.005)	Loss 0.0927 (0.0650)	Acc@1 98.047 (97.921)	Acc@5 100.000 (99.976)
Epoch: [205][128/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0664 (0.0658)	Acc@1 98.438 (97.974)	Acc@5 100.000 (99.976)
Epoch: [205][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0362 (0.0658)	Acc@1 99.219 (97.982)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.34
Max memory: 51.4381312
 17.619s  j: 211 bis 215
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3041
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 206
Max memory: 0.1097216
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:206/210; Lr: 0.0005314410000000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [206][0/196]	Time 0.142 (0.142)	Data 0.299 (0.299)	Loss 0.0481 (0.0481)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [206][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.1029 (0.0616)	Acc@1 95.312 (98.083)	Acc@5 100.000 (99.994)
Epoch: [206][128/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.0389 (0.0620)	Acc@1 99.219 (98.062)	Acc@5 100.000 (99.991)
Epoch: [206][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0579 (0.0641)	Acc@1 98.047 (97.970)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:207/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [207][0/196]	Time 0.116 (0.116)	Data 0.319 (0.319)	Loss 0.0842 (0.0842)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [207][64/196]	Time 0.081 (0.088)	Data 0.000 (0.005)	Loss 0.0763 (0.0644)	Acc@1 97.266 (98.059)	Acc@5 100.000 (99.976)
Epoch: [207][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0790 (0.0658)	Acc@1 98.047 (97.989)	Acc@5 100.000 (99.976)
Epoch: [207][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0676 (0.0674)	Acc@1 96.875 (97.877)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:208/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [208][0/196]	Time 0.130 (0.130)	Data 0.303 (0.303)	Loss 0.0533 (0.0533)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [208][64/196]	Time 0.086 (0.087)	Data 0.000 (0.005)	Loss 0.0727 (0.0620)	Acc@1 97.656 (98.071)	Acc@5 100.000 (99.994)
Epoch: [208][128/196]	Time 0.084 (0.087)	Data 0.000 (0.003)	Loss 0.0657 (0.0632)	Acc@1 97.656 (98.020)	Acc@5 99.609 (99.988)
Epoch: [208][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0614 (0.0630)	Acc@1 98.438 (98.025)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0005314410000000001
lr: 0.0005314410000000001
Epoche:209/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [209][0/196]	Time 0.109 (0.109)	Data 0.285 (0.285)	Loss 0.0828 (0.0828)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [209][64/196]	Time 0.090 (0.089)	Data 0.000 (0.005)	Loss 0.0699 (0.0650)	Acc@1 97.266 (97.951)	Acc@5 100.000 (99.976)
Epoch: [209][128/196]	Time 0.098 (0.088)	Data 0.000 (0.002)	Loss 0.0351 (0.0629)	Acc@1 99.219 (98.047)	Acc@5 100.000 (99.982)
Epoch: [209][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0452 (0.0624)	Acc@1 98.438 (98.093)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0005314410000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:210/210; Lr: 0.0005314410000000001
batch Size 256
Epoch: [210][0/196]	Time 0.131 (0.131)	Data 0.283 (0.283)	Loss 0.0780 (0.0780)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [210][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.0842 (0.0627)	Acc@1 96.875 (98.041)	Acc@5 100.000 (99.964)
Epoch: [210][128/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0660 (0.0625)	Acc@1 98.047 (98.083)	Acc@5 100.000 (99.970)
Epoch: [210][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0435 (0.0636)	Acc@1 99.219 (98.055)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.37
Max memory: 51.4381312
 17.708s  j: 216 bis 220
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6804
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 211
Max memory: 0.1097216
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:211/215; Lr: 0.0004782969000000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [211][0/196]	Time 0.135 (0.135)	Data 0.287 (0.287)	Loss 0.0582 (0.0582)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [211][64/196]	Time 0.096 (0.088)	Data 0.000 (0.005)	Loss 0.0546 (0.0614)	Acc@1 98.438 (98.005)	Acc@5 100.000 (99.988)
Epoch: [211][128/196]	Time 0.097 (0.088)	Data 0.000 (0.002)	Loss 0.0994 (0.0614)	Acc@1 96.484 (98.032)	Acc@5 100.000 (99.988)
Epoch: [211][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0521 (0.0631)	Acc@1 98.047 (97.946)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:212/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [212][0/196]	Time 0.139 (0.139)	Data 0.271 (0.271)	Loss 0.0620 (0.0620)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [212][64/196]	Time 0.085 (0.088)	Data 0.000 (0.004)	Loss 0.0654 (0.0628)	Acc@1 98.047 (97.981)	Acc@5 100.000 (99.994)
Epoch: [212][128/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0412 (0.0634)	Acc@1 99.219 (98.011)	Acc@5 100.000 (99.985)
Epoch: [212][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0705 (0.0646)	Acc@1 98.828 (97.964)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:213/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [213][0/196]	Time 0.110 (0.110)	Data 0.306 (0.306)	Loss 0.0378 (0.0378)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [213][64/196]	Time 0.083 (0.087)	Data 0.000 (0.005)	Loss 0.0556 (0.0630)	Acc@1 98.047 (98.053)	Acc@5 100.000 (99.994)
Epoch: [213][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.0679 (0.0628)	Acc@1 97.656 (97.998)	Acc@5 100.000 (99.988)
Epoch: [213][192/196]	Time 0.085 (0.086)	Data 0.000 (0.002)	Loss 0.0680 (0.0631)	Acc@1 96.484 (98.000)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004782969000000001
lr: 0.0004782969000000001
Epoche:214/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [214][0/196]	Time 0.129 (0.129)	Data 0.309 (0.309)	Loss 0.0443 (0.0443)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [214][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.0499 (0.0649)	Acc@1 98.828 (98.029)	Acc@5 100.000 (99.982)
Epoch: [214][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0760 (0.0634)	Acc@1 98.047 (97.998)	Acc@5 100.000 (99.982)
Epoch: [214][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0587 (0.0631)	Acc@1 98.828 (98.025)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0004782969000000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:215/215; Lr: 0.0004782969000000001
batch Size 256
Epoch: [215][0/196]	Time 0.124 (0.124)	Data 0.295 (0.295)	Loss 0.0867 (0.0867)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [215][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.0637 (0.0607)	Acc@1 98.047 (98.131)	Acc@5 100.000 (99.994)
Epoch: [215][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.0427 (0.0602)	Acc@1 99.219 (98.156)	Acc@5 100.000 (99.994)
Epoch: [215][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.1083 (0.0627)	Acc@1 96.875 (98.035)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.37
Max memory: 51.4381312
 17.532s  j: 221 bis 225
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8924
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 216
Max memory: 0.1097216
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:216/220; Lr: 0.0004304672100000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [216][0/196]	Time 0.183 (0.183)	Data 0.285 (0.285)	Loss 0.0522 (0.0522)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [216][64/196]	Time 0.084 (0.091)	Data 0.000 (0.005)	Loss 0.0659 (0.0614)	Acc@1 97.656 (98.023)	Acc@5 100.000 (99.982)
Epoch: [216][128/196]	Time 0.102 (0.089)	Data 0.000 (0.002)	Loss 0.0666 (0.0611)	Acc@1 97.266 (98.041)	Acc@5 100.000 (99.985)
Epoch: [216][192/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.0497 (0.0623)	Acc@1 99.219 (98.006)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:217/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [217][0/196]	Time 0.118 (0.118)	Data 0.305 (0.305)	Loss 0.0829 (0.0829)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [217][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.0540 (0.0630)	Acc@1 98.828 (98.005)	Acc@5 100.000 (99.982)
Epoch: [217][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0984 (0.0615)	Acc@1 96.875 (98.059)	Acc@5 100.000 (99.985)
Epoch: [217][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.0823 (0.0615)	Acc@1 97.266 (98.053)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:218/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [218][0/196]	Time 0.138 (0.138)	Data 0.321 (0.321)	Loss 0.0751 (0.0751)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [218][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.0704 (0.0623)	Acc@1 97.266 (97.939)	Acc@5 100.000 (99.994)
Epoch: [218][128/196]	Time 0.092 (0.088)	Data 0.000 (0.003)	Loss 0.0566 (0.0626)	Acc@1 98.438 (97.971)	Acc@5 100.000 (99.991)
Epoch: [218][192/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 0.0446 (0.0631)	Acc@1 98.828 (97.960)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0004304672100000001
lr: 0.0004304672100000001
Epoche:219/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [219][0/196]	Time 0.100 (0.100)	Data 0.327 (0.327)	Loss 0.0583 (0.0583)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [219][64/196]	Time 0.104 (0.088)	Data 0.000 (0.005)	Loss 0.0521 (0.0590)	Acc@1 98.438 (98.083)	Acc@5 100.000 (99.988)
Epoch: [219][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.0708 (0.0594)	Acc@1 98.047 (98.110)	Acc@5 100.000 (99.988)
Epoch: [219][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0588 (0.0600)	Acc@1 97.656 (98.120)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0004304672100000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:220/220; Lr: 0.0004304672100000001
batch Size 256
Epoch: [220][0/196]	Time 0.131 (0.131)	Data 0.303 (0.303)	Loss 0.0368 (0.0368)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [220][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0477 (0.0607)	Acc@1 98.828 (98.071)	Acc@5 100.000 (99.988)
Epoch: [220][128/196]	Time 0.082 (0.088)	Data 0.000 (0.003)	Loss 0.1076 (0.0611)	Acc@1 96.094 (98.071)	Acc@5 100.000 (99.988)
Epoch: [220][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0593 (0.0627)	Acc@1 98.047 (98.047)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.18
Max memory: 51.4381312
 17.603s  j: 226 bis 230
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8389
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 221
Max memory: 0.1097216
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:221/225; Lr: 0.0003874204890000001
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [221][0/196]	Time 0.148 (0.148)	Data 0.323 (0.323)	Loss 0.0572 (0.0572)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [221][64/196]	Time 0.086 (0.091)	Data 0.000 (0.005)	Loss 0.0560 (0.0611)	Acc@1 98.047 (98.131)	Acc@5 100.000 (99.982)
Epoch: [221][128/196]	Time 0.086 (0.090)	Data 0.000 (0.003)	Loss 0.0682 (0.0638)	Acc@1 97.266 (97.971)	Acc@5 100.000 (99.991)
Epoch: [221][192/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.0799 (0.0635)	Acc@1 96.875 (97.990)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:222/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [222][0/196]	Time 0.139 (0.139)	Data 0.309 (0.309)	Loss 0.0629 (0.0629)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [222][64/196]	Time 0.079 (0.087)	Data 0.000 (0.005)	Loss 0.0684 (0.0599)	Acc@1 97.656 (98.203)	Acc@5 100.000 (99.994)
Epoch: [222][128/196]	Time 0.079 (0.087)	Data 0.000 (0.003)	Loss 0.0843 (0.0622)	Acc@1 97.266 (98.083)	Acc@5 100.000 (99.991)
Epoch: [222][192/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.0493 (0.0622)	Acc@1 98.438 (98.071)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:223/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [223][0/196]	Time 0.107 (0.107)	Data 0.322 (0.322)	Loss 0.0362 (0.0362)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [223][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.0605 (0.0613)	Acc@1 98.047 (98.101)	Acc@5 100.000 (99.994)
Epoch: [223][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0413 (0.0619)	Acc@1 99.219 (98.050)	Acc@5 100.000 (99.985)
Epoch: [223][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.0478 (0.0623)	Acc@1 98.438 (98.029)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.0003874204890000001
lr: 0.0003874204890000001
Epoche:224/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [224][0/196]	Time 0.126 (0.126)	Data 0.306 (0.306)	Loss 0.0859 (0.0859)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [224][64/196]	Time 0.093 (0.090)	Data 0.000 (0.005)	Loss 0.0595 (0.0639)	Acc@1 98.438 (98.089)	Acc@5 100.000 (99.988)
Epoch: [224][128/196]	Time 0.085 (0.089)	Data 0.000 (0.003)	Loss 0.0676 (0.0630)	Acc@1 96.875 (98.029)	Acc@5 100.000 (99.988)
Epoch: [224][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0521 (0.0612)	Acc@1 97.656 (98.091)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0003874204890000001
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:225/225; Lr: 0.0003874204890000001
batch Size 256
Epoch: [225][0/196]	Time 0.138 (0.138)	Data 0.293 (0.293)	Loss 0.0689 (0.0689)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [225][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.0552 (0.0611)	Acc@1 99.219 (98.065)	Acc@5 100.000 (99.994)
Epoch: [225][128/196]	Time 0.081 (0.089)	Data 0.000 (0.002)	Loss 0.0414 (0.0619)	Acc@1 99.219 (98.086)	Acc@5 100.000 (99.991)
Epoch: [225][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0803 (0.0622)	Acc@1 96.484 (98.045)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.28
Max memory: 51.4381312
 17.565s  j: 231 bis 235
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 21
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 226
Max memory: 0.1097216
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:226/230; Lr: 0.00034867844010000006
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [226][0/196]	Time 0.151 (0.151)	Data 0.291 (0.291)	Loss 0.0602 (0.0602)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [226][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.0944 (0.0616)	Acc@1 96.094 (98.131)	Acc@5 100.000 (99.970)
Epoch: [226][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0445 (0.0605)	Acc@1 98.438 (98.150)	Acc@5 100.000 (99.982)
Epoch: [226][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0439 (0.0606)	Acc@1 98.828 (98.108)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:227/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [227][0/196]	Time 0.123 (0.123)	Data 0.304 (0.304)	Loss 0.0504 (0.0504)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [227][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.0479 (0.0613)	Acc@1 98.828 (98.047)	Acc@5 100.000 (99.982)
Epoch: [227][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.0353 (0.0608)	Acc@1 98.828 (98.117)	Acc@5 100.000 (99.985)
Epoch: [227][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0612 (0.0608)	Acc@1 98.828 (98.126)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:228/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [228][0/196]	Time 0.125 (0.125)	Data 0.299 (0.299)	Loss 0.0754 (0.0754)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [228][64/196]	Time 0.083 (0.089)	Data 0.000 (0.005)	Loss 0.0689 (0.0607)	Acc@1 97.656 (98.155)	Acc@5 100.000 (99.988)
Epoch: [228][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0655 (0.0606)	Acc@1 98.438 (98.110)	Acc@5 100.000 (99.991)
Epoch: [228][192/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0789 (0.0597)	Acc@1 97.266 (98.154)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00034867844010000006
lr: 0.00034867844010000006
Epoche:229/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [229][0/196]	Time 0.115 (0.115)	Data 0.304 (0.304)	Loss 0.0549 (0.0549)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [229][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.0370 (0.0613)	Acc@1 99.219 (98.023)	Acc@5 100.000 (99.988)
Epoch: [229][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0481 (0.0604)	Acc@1 98.438 (98.080)	Acc@5 100.000 (99.988)
Epoch: [229][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0772 (0.0604)	Acc@1 97.656 (98.130)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00034867844010000006
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:230/230; Lr: 0.00034867844010000006
batch Size 256
Epoch: [230][0/196]	Time 0.126 (0.126)	Data 0.287 (0.287)	Loss 0.0846 (0.0846)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [230][64/196]	Time 0.085 (0.090)	Data 0.000 (0.005)	Loss 0.0605 (0.0607)	Acc@1 98.438 (98.113)	Acc@5 100.000 (99.994)
Epoch: [230][128/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0700 (0.0625)	Acc@1 98.047 (98.026)	Acc@5 100.000 (99.991)
Epoch: [230][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0598 (0.0615)	Acc@1 97.656 (98.075)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.28
Max memory: 51.4381312
 17.556s  j: 236 bis 240
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8691
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 231
Max memory: 0.1097216
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:231/235; Lr: 0.00031381059609000004
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [231][0/196]	Time 0.158 (0.158)	Data 0.309 (0.309)	Loss 0.0690 (0.0690)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [231][64/196]	Time 0.094 (0.091)	Data 0.000 (0.005)	Loss 0.0556 (0.0565)	Acc@1 98.047 (98.281)	Acc@5 100.000 (100.000)
Epoch: [231][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.0344 (0.0577)	Acc@1 100.000 (98.232)	Acc@5 100.000 (99.997)
Epoch: [231][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.0786 (0.0597)	Acc@1 97.656 (98.178)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:232/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [232][0/196]	Time 0.118 (0.118)	Data 0.289 (0.289)	Loss 0.0567 (0.0567)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [232][64/196]	Time 0.089 (0.090)	Data 0.000 (0.005)	Loss 0.0507 (0.0603)	Acc@1 99.609 (98.179)	Acc@5 100.000 (100.000)
Epoch: [232][128/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.0824 (0.0613)	Acc@1 97.656 (98.080)	Acc@5 99.609 (99.988)
Epoch: [232][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.0431 (0.0599)	Acc@1 98.047 (98.118)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:233/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [233][0/196]	Time 0.130 (0.130)	Data 0.295 (0.295)	Loss 0.0732 (0.0732)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [233][64/196]	Time 0.101 (0.091)	Data 0.000 (0.005)	Loss 0.0568 (0.0604)	Acc@1 98.047 (98.173)	Acc@5 100.000 (99.988)
Epoch: [233][128/196]	Time 0.083 (0.090)	Data 0.000 (0.003)	Loss 0.0874 (0.0589)	Acc@1 97.266 (98.165)	Acc@5 100.000 (99.988)
Epoch: [233][192/196]	Time 0.086 (0.090)	Data 0.000 (0.002)	Loss 0.0411 (0.0594)	Acc@1 99.219 (98.156)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00031381059609000004
lr: 0.00031381059609000004
Epoche:234/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [234][0/196]	Time 0.142 (0.142)	Data 0.292 (0.292)	Loss 0.0583 (0.0583)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [234][64/196]	Time 0.088 (0.091)	Data 0.000 (0.005)	Loss 0.0603 (0.0640)	Acc@1 98.828 (98.005)	Acc@5 100.000 (99.988)
Epoch: [234][128/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.0668 (0.0623)	Acc@1 97.266 (97.974)	Acc@5 100.000 (99.988)
Epoch: [234][192/196]	Time 0.086 (0.089)	Data 0.000 (0.002)	Loss 0.0392 (0.0605)	Acc@1 99.219 (98.065)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00031381059609000004
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:235/235; Lr: 0.00031381059609000004
batch Size 256
Epoch: [235][0/196]	Time 0.139 (0.139)	Data 0.286 (0.286)	Loss 0.0552 (0.0552)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [235][64/196]	Time 0.089 (0.091)	Data 0.000 (0.005)	Loss 0.0556 (0.0604)	Acc@1 98.438 (98.197)	Acc@5 100.000 (99.982)
Epoch: [235][128/196]	Time 0.090 (0.090)	Data 0.000 (0.002)	Loss 0.0455 (0.0595)	Acc@1 98.438 (98.195)	Acc@5 100.000 (99.988)
Epoch: [235][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.0382 (0.0590)	Acc@1 99.609 (98.219)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.46
Max memory: 51.4381312
 17.875s  j: 241 bis 245
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6718
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 236
Max memory: 0.1097216
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:236/240; Lr: 0.00028242953648100003
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [236][0/196]	Time 0.143 (0.143)	Data 0.285 (0.285)	Loss 0.0581 (0.0581)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [236][64/196]	Time 0.095 (0.086)	Data 0.000 (0.005)	Loss 0.0946 (0.0633)	Acc@1 97.266 (98.023)	Acc@5 100.000 (99.994)
Epoch: [236][128/196]	Time 0.082 (0.086)	Data 0.000 (0.002)	Loss 0.0302 (0.0605)	Acc@1 99.219 (98.135)	Acc@5 100.000 (99.985)
Epoch: [236][192/196]	Time 0.090 (0.085)	Data 0.000 (0.002)	Loss 0.0915 (0.0615)	Acc@1 96.094 (98.093)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:237/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [237][0/196]	Time 0.111 (0.111)	Data 0.313 (0.313)	Loss 0.0817 (0.0817)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [237][64/196]	Time 0.082 (0.090)	Data 0.000 (0.005)	Loss 0.0661 (0.0625)	Acc@1 97.656 (97.999)	Acc@5 100.000 (99.988)
Epoch: [237][128/196]	Time 0.103 (0.088)	Data 0.000 (0.003)	Loss 0.0347 (0.0602)	Acc@1 99.609 (98.086)	Acc@5 100.000 (99.988)
Epoch: [237][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0566 (0.0596)	Acc@1 98.047 (98.108)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:238/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [238][0/196]	Time 0.126 (0.126)	Data 0.295 (0.295)	Loss 0.0755 (0.0755)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [238][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.0592 (0.0566)	Acc@1 97.656 (98.341)	Acc@5 100.000 (100.000)
Epoch: [238][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0862 (0.0591)	Acc@1 98.047 (98.207)	Acc@5 100.000 (99.988)
Epoch: [238][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0894 (0.0597)	Acc@1 97.656 (98.201)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00028242953648100003
lr: 0.00028242953648100003
Epoche:239/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [239][0/196]	Time 0.117 (0.117)	Data 0.319 (0.319)	Loss 0.0685 (0.0685)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [239][64/196]	Time 0.083 (0.087)	Data 0.000 (0.005)	Loss 0.0702 (0.0612)	Acc@1 97.656 (98.023)	Acc@5 100.000 (99.994)
Epoch: [239][128/196]	Time 0.077 (0.087)	Data 0.000 (0.003)	Loss 0.0722 (0.0611)	Acc@1 97.266 (98.083)	Acc@5 99.609 (99.988)
Epoch: [239][192/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.0942 (0.0603)	Acc@1 96.875 (98.081)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00028242953648100003
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:240/240; Lr: 0.00028242953648100003
batch Size 256
Epoch: [240][0/196]	Time 0.134 (0.134)	Data 0.355 (0.355)	Loss 0.0552 (0.0552)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [240][64/196]	Time 0.086 (0.087)	Data 0.000 (0.006)	Loss 0.0728 (0.0578)	Acc@1 97.266 (98.197)	Acc@5 100.000 (99.994)
Epoch: [240][128/196]	Time 0.091 (0.086)	Data 0.000 (0.003)	Loss 0.0771 (0.0608)	Acc@1 97.656 (98.062)	Acc@5 100.000 (99.991)
Epoch: [240][192/196]	Time 0.083 (0.086)	Data 0.000 (0.002)	Loss 0.0477 (0.0601)	Acc@1 98.047 (98.081)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.37
Max memory: 51.4381312
 17.312s  j: 246 bis 250
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 268
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 241
Max memory: 0.1097216
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:241/245; Lr: 0.00025418658283290005
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [241][0/196]	Time 0.150 (0.150)	Data 0.312 (0.312)	Loss 0.0494 (0.0494)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [241][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.0684 (0.0568)	Acc@1 97.266 (98.179)	Acc@5 100.000 (99.994)
Epoch: [241][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0416 (0.0567)	Acc@1 98.828 (98.171)	Acc@5 100.000 (99.994)
Epoch: [241][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0736 (0.0576)	Acc@1 97.266 (98.174)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:242/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [242][0/196]	Time 0.138 (0.138)	Data 0.271 (0.271)	Loss 0.0773 (0.0773)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [242][64/196]	Time 0.094 (0.089)	Data 0.000 (0.004)	Loss 0.0441 (0.0584)	Acc@1 98.828 (98.245)	Acc@5 100.000 (99.988)
Epoch: [242][128/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.0697 (0.0588)	Acc@1 98.047 (98.168)	Acc@5 100.000 (99.991)
Epoch: [242][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0446 (0.0600)	Acc@1 98.438 (98.108)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:243/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [243][0/196]	Time 0.121 (0.121)	Data 0.299 (0.299)	Loss 0.0600 (0.0600)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [243][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.0576 (0.0568)	Acc@1 98.438 (98.293)	Acc@5 100.000 (99.982)
Epoch: [243][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.0567 (0.0583)	Acc@1 97.656 (98.247)	Acc@5 100.000 (99.985)
Epoch: [243][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0607 (0.0583)	Acc@1 97.656 (98.231)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00025418658283290005
lr: 0.00025418658283290005
Epoche:244/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [244][0/196]	Time 0.135 (0.135)	Data 0.297 (0.297)	Loss 0.0542 (0.0542)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [244][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0732 (0.0605)	Acc@1 97.656 (98.167)	Acc@5 100.000 (99.994)
Epoch: [244][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0530 (0.0599)	Acc@1 98.828 (98.168)	Acc@5 100.000 (99.991)
Epoch: [244][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0473 (0.0604)	Acc@1 98.438 (98.106)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00025418658283290005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:245/245; Lr: 0.00025418658283290005
batch Size 256
Epoch: [245][0/196]	Time 0.131 (0.131)	Data 0.298 (0.298)	Loss 0.0504 (0.0504)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [245][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0506 (0.0566)	Acc@1 99.609 (98.227)	Acc@5 100.000 (99.976)
Epoch: [245][128/196]	Time 0.082 (0.087)	Data 0.000 (0.003)	Loss 0.0488 (0.0578)	Acc@1 98.438 (98.201)	Acc@5 100.000 (99.985)
Epoch: [245][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0655 (0.0587)	Acc@1 97.656 (98.170)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.43
Max memory: 51.4381312
 17.373s  j: 251 bis 255
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8706
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 246
Max memory: 0.1097216
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:246/250; Lr: 0.00022876792454961005
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [246][0/196]	Time 0.149 (0.149)	Data 0.294 (0.294)	Loss 0.0486 (0.0486)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [246][64/196]	Time 0.090 (0.090)	Data 0.000 (0.005)	Loss 0.0810 (0.0535)	Acc@1 97.656 (98.371)	Acc@5 100.000 (99.994)
Epoch: [246][128/196]	Time 0.083 (0.089)	Data 0.000 (0.002)	Loss 0.0705 (0.0554)	Acc@1 97.656 (98.310)	Acc@5 100.000 (99.991)
Epoch: [246][192/196]	Time 0.091 (0.089)	Data 0.000 (0.002)	Loss 0.0483 (0.0574)	Acc@1 99.219 (98.223)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:247/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [247][0/196]	Time 0.123 (0.123)	Data 0.296 (0.296)	Loss 0.0382 (0.0382)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [247][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0533 (0.0548)	Acc@1 97.656 (98.311)	Acc@5 100.000 (99.994)
Epoch: [247][128/196]	Time 0.090 (0.088)	Data 0.000 (0.003)	Loss 0.0601 (0.0573)	Acc@1 97.656 (98.223)	Acc@5 100.000 (99.994)
Epoch: [247][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0623 (0.0582)	Acc@1 98.438 (98.189)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:248/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [248][0/196]	Time 0.128 (0.128)	Data 0.299 (0.299)	Loss 0.0737 (0.0737)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [248][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.0584 (0.0556)	Acc@1 98.828 (98.323)	Acc@5 100.000 (99.988)
Epoch: [248][128/196]	Time 0.092 (0.088)	Data 0.000 (0.003)	Loss 0.0797 (0.0589)	Acc@1 98.047 (98.177)	Acc@5 100.000 (99.991)
Epoch: [248][192/196]	Time 0.091 (0.088)	Data 0.000 (0.002)	Loss 0.0440 (0.0587)	Acc@1 98.828 (98.178)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00022876792454961005
lr: 0.00022876792454961005
Epoche:249/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [249][0/196]	Time 0.131 (0.131)	Data 0.307 (0.307)	Loss 0.0554 (0.0554)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [249][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0866 (0.0577)	Acc@1 98.047 (98.143)	Acc@5 100.000 (99.994)
Epoch: [249][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0562 (0.0572)	Acc@1 99.219 (98.226)	Acc@5 100.000 (99.994)
Epoch: [249][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0613 (0.0568)	Acc@1 98.438 (98.227)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00022876792454961005
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:250/250; Lr: 0.00022876792454961005
batch Size 256
Epoch: [250][0/196]	Time 0.133 (0.133)	Data 0.271 (0.271)	Loss 0.0464 (0.0464)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [250][64/196]	Time 0.090 (0.089)	Data 0.000 (0.004)	Loss 0.0647 (0.0570)	Acc@1 97.656 (98.155)	Acc@5 99.609 (99.982)
Epoch: [250][128/196]	Time 0.088 (0.089)	Data 0.000 (0.002)	Loss 0.0626 (0.0597)	Acc@1 98.828 (98.044)	Acc@5 100.000 (99.985)
Epoch: [250][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0718 (0.0599)	Acc@1 97.656 (98.045)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.36
Max memory: 51.4381312
 17.580s  j: 256 bis 260
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2654
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 251
Max memory: 0.1097216
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:251/255; Lr: 0.00020589113209464906
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [251][0/196]	Time 0.152 (0.152)	Data 0.296 (0.296)	Loss 0.0573 (0.0573)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [251][64/196]	Time 0.094 (0.089)	Data 0.000 (0.005)	Loss 0.0411 (0.0584)	Acc@1 98.828 (98.209)	Acc@5 100.000 (99.994)
Epoch: [251][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0593 (0.0574)	Acc@1 98.047 (98.262)	Acc@5 100.000 (99.994)
Epoch: [251][192/196]	Time 0.094 (0.088)	Data 0.000 (0.002)	Loss 0.0388 (0.0575)	Acc@1 98.438 (98.249)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:252/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [252][0/196]	Time 0.130 (0.130)	Data 0.329 (0.329)	Loss 0.0486 (0.0486)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [252][64/196]	Time 0.087 (0.090)	Data 0.000 (0.005)	Loss 0.0593 (0.0561)	Acc@1 98.047 (98.341)	Acc@5 100.000 (99.994)
Epoch: [252][128/196]	Time 0.084 (0.089)	Data 0.000 (0.003)	Loss 0.0531 (0.0562)	Acc@1 98.828 (98.274)	Acc@5 100.000 (99.988)
Epoch: [252][192/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.0453 (0.0566)	Acc@1 98.438 (98.251)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:253/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [253][0/196]	Time 0.127 (0.127)	Data 0.297 (0.297)	Loss 0.0556 (0.0556)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [253][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0537 (0.0594)	Acc@1 98.828 (98.179)	Acc@5 100.000 (99.994)
Epoch: [253][128/196]	Time 0.089 (0.089)	Data 0.000 (0.003)	Loss 0.0651 (0.0577)	Acc@1 98.047 (98.229)	Acc@5 100.000 (99.988)
Epoch: [253][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.0485 (0.0575)	Acc@1 98.438 (98.217)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00020589113209464906
lr: 0.00020589113209464906
Epoche:254/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [254][0/196]	Time 0.132 (0.132)	Data 0.304 (0.304)	Loss 0.0669 (0.0669)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [254][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0459 (0.0569)	Acc@1 98.828 (98.203)	Acc@5 100.000 (99.994)
Epoch: [254][128/196]	Time 0.088 (0.089)	Data 0.000 (0.003)	Loss 0.0819 (0.0586)	Acc@1 97.266 (98.168)	Acc@5 100.000 (99.991)
Epoch: [254][192/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.0403 (0.0582)	Acc@1 98.438 (98.205)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00020589113209464906
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:255/255; Lr: 0.00020589113209464906
batch Size 256
Epoch: [255][0/196]	Time 0.116 (0.116)	Data 0.288 (0.288)	Loss 0.0980 (0.0980)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [255][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.0375 (0.0595)	Acc@1 99.219 (98.143)	Acc@5 100.000 (99.988)
Epoch: [255][128/196]	Time 0.107 (0.088)	Data 0.000 (0.002)	Loss 0.0388 (0.0589)	Acc@1 99.219 (98.180)	Acc@5 100.000 (99.982)
Epoch: [255][192/196]	Time 0.096 (0.088)	Data 0.000 (0.002)	Loss 0.0769 (0.0579)	Acc@1 98.438 (98.243)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.43
Max memory: 51.4381312
 17.548s  j: 261 bis 265
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8617
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 256
Max memory: 0.1097216
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:256/260; Lr: 0.00018530201888518417
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [256][0/196]	Time 0.144 (0.144)	Data 0.300 (0.300)	Loss 0.0675 (0.0675)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [256][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.0563 (0.0562)	Acc@1 98.047 (98.233)	Acc@5 100.000 (99.994)
Epoch: [256][128/196]	Time 0.096 (0.089)	Data 0.000 (0.003)	Loss 0.0783 (0.0575)	Acc@1 97.656 (98.268)	Acc@5 100.000 (99.991)
Epoch: [256][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0554 (0.0585)	Acc@1 98.438 (98.162)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:257/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [257][0/196]	Time 0.136 (0.136)	Data 0.290 (0.290)	Loss 0.0578 (0.0578)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [257][64/196]	Time 0.088 (0.090)	Data 0.000 (0.005)	Loss 0.0601 (0.0567)	Acc@1 98.438 (98.125)	Acc@5 100.000 (100.000)
Epoch: [257][128/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.0524 (0.0580)	Acc@1 98.438 (98.123)	Acc@5 100.000 (99.994)
Epoch: [257][192/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.0823 (0.0575)	Acc@1 97.656 (98.174)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:258/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [258][0/196]	Time 0.140 (0.140)	Data 0.323 (0.323)	Loss 0.0298 (0.0298)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [258][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0663 (0.0587)	Acc@1 97.656 (98.161)	Acc@5 100.000 (99.988)
Epoch: [258][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.0452 (0.0577)	Acc@1 98.828 (98.177)	Acc@5 100.000 (99.985)
Epoch: [258][192/196]	Time 0.082 (0.089)	Data 0.000 (0.002)	Loss 0.0451 (0.0578)	Acc@1 98.828 (98.182)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00018530201888518417
lr: 0.00018530201888518417
Epoche:259/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [259][0/196]	Time 0.126 (0.126)	Data 0.277 (0.277)	Loss 0.0532 (0.0532)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [259][64/196]	Time 0.089 (0.090)	Data 0.000 (0.004)	Loss 0.0907 (0.0588)	Acc@1 97.266 (98.101)	Acc@5 100.000 (99.988)
Epoch: [259][128/196]	Time 0.081 (0.089)	Data 0.000 (0.002)	Loss 0.0418 (0.0585)	Acc@1 98.828 (98.123)	Acc@5 100.000 (99.985)
Epoch: [259][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0627 (0.0575)	Acc@1 98.828 (98.178)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.00018530201888518417
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:260/260; Lr: 0.00018530201888518417
batch Size 256
Epoch: [260][0/196]	Time 0.101 (0.101)	Data 0.259 (0.259)	Loss 0.0666 (0.0666)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [260][64/196]	Time 0.083 (0.088)	Data 0.000 (0.004)	Loss 0.0874 (0.0586)	Acc@1 97.656 (98.209)	Acc@5 100.000 (99.988)
Epoch: [260][128/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0524 (0.0565)	Acc@1 98.047 (98.271)	Acc@5 100.000 (99.985)
Epoch: [260][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0806 (0.0575)	Acc@1 97.266 (98.229)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.43
Max memory: 51.4381312
 17.551s  j: 266 bis 270
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7739
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 261
Max memory: 0.1097216
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:261/265; Lr: 0.00016677181699666576
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [261][0/196]	Time 0.138 (0.138)	Data 0.323 (0.323)	Loss 0.0654 (0.0654)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [261][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.0586 (0.0569)	Acc@1 98.438 (98.113)	Acc@5 100.000 (99.988)
Epoch: [261][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0351 (0.0568)	Acc@1 99.219 (98.219)	Acc@5 100.000 (99.988)
Epoch: [261][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0547 (0.0571)	Acc@1 98.047 (98.211)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:262/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [262][0/196]	Time 0.121 (0.121)	Data 0.322 (0.322)	Loss 0.0638 (0.0638)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [262][64/196]	Time 0.090 (0.088)	Data 0.000 (0.005)	Loss 0.0356 (0.0578)	Acc@1 98.828 (98.377)	Acc@5 100.000 (99.982)
Epoch: [262][128/196]	Time 0.081 (0.088)	Data 0.000 (0.003)	Loss 0.0808 (0.0578)	Acc@1 96.875 (98.271)	Acc@5 100.000 (99.979)
Epoch: [262][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0497 (0.0583)	Acc@1 98.828 (98.209)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:263/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [263][0/196]	Time 0.127 (0.127)	Data 0.310 (0.310)	Loss 0.0581 (0.0581)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [263][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.0622 (0.0607)	Acc@1 98.047 (98.059)	Acc@5 100.000 (99.994)
Epoch: [263][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.0768 (0.0578)	Acc@1 97.656 (98.198)	Acc@5 100.000 (99.994)
Epoch: [263][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0436 (0.0576)	Acc@1 98.828 (98.197)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.00016677181699666576
lr: 0.00016677181699666576
Epoche:264/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [264][0/196]	Time 0.124 (0.124)	Data 0.311 (0.311)	Loss 0.0369 (0.0369)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [264][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0630 (0.0546)	Acc@1 97.656 (98.275)	Acc@5 100.000 (99.976)
Epoch: [264][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0740 (0.0548)	Acc@1 98.438 (98.274)	Acc@5 100.000 (99.976)
Epoch: [264][192/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0631 (0.0553)	Acc@1 98.047 (98.259)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.00016677181699666576
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:265/265; Lr: 0.00016677181699666576
batch Size 256
Epoch: [265][0/196]	Time 0.143 (0.143)	Data 0.287 (0.287)	Loss 0.0569 (0.0569)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [265][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0562 (0.0554)	Acc@1 97.656 (98.281)	Acc@5 100.000 (100.000)
Epoch: [265][128/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0910 (0.0576)	Acc@1 96.094 (98.177)	Acc@5 100.000 (99.994)
Epoch: [265][192/196]	Time 0.095 (0.088)	Data 0.000 (0.002)	Loss 0.0430 (0.0571)	Acc@1 98.828 (98.221)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.3
Max memory: 51.4381312
 17.664s  j: 271 bis 275
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7472
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 266
Max memory: 0.1097216
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:266/270; Lr: 0.0001500946352969992
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [266][0/196]	Time 0.149 (0.149)	Data 0.270 (0.270)	Loss 0.0594 (0.0594)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [266][64/196]	Time 0.084 (0.089)	Data 0.000 (0.004)	Loss 0.0450 (0.0566)	Acc@1 99.219 (98.143)	Acc@5 100.000 (99.994)
Epoch: [266][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0461 (0.0588)	Acc@1 98.438 (98.086)	Acc@5 100.000 (99.991)
Epoch: [266][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.0527 (0.0567)	Acc@1 97.266 (98.201)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:267/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [267][0/196]	Time 0.124 (0.124)	Data 0.321 (0.321)	Loss 0.0491 (0.0491)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [267][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.0489 (0.0510)	Acc@1 98.438 (98.486)	Acc@5 100.000 (99.988)
Epoch: [267][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0559 (0.0552)	Acc@1 98.047 (98.307)	Acc@5 100.000 (99.991)
Epoch: [267][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.0830 (0.0558)	Acc@1 96.875 (98.304)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:268/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [268][0/196]	Time 0.134 (0.134)	Data 0.378 (0.378)	Loss 0.0449 (0.0449)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [268][64/196]	Time 0.083 (0.090)	Data 0.000 (0.006)	Loss 0.0530 (0.0544)	Acc@1 98.438 (98.413)	Acc@5 100.000 (100.000)
Epoch: [268][128/196]	Time 0.096 (0.089)	Data 0.000 (0.003)	Loss 0.0938 (0.0551)	Acc@1 97.266 (98.347)	Acc@5 99.609 (99.988)
Epoch: [268][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0502 (0.0544)	Acc@1 97.656 (98.352)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001500946352969992
lr: 0.0001500946352969992
Epoche:269/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [269][0/196]	Time 0.120 (0.120)	Data 0.288 (0.288)	Loss 0.0386 (0.0386)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [269][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.0723 (0.0566)	Acc@1 97.266 (98.215)	Acc@5 100.000 (100.000)
Epoch: [269][128/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0566 (0.0557)	Acc@1 98.828 (98.274)	Acc@5 100.000 (99.997)
Epoch: [269][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0478 (0.0558)	Acc@1 98.047 (98.280)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.0001500946352969992
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:270/270; Lr: 0.0001500946352969992
batch Size 256
Epoch: [270][0/196]	Time 0.138 (0.138)	Data 0.293 (0.293)	Loss 0.0697 (0.0697)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [270][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.0486 (0.0555)	Acc@1 99.219 (98.311)	Acc@5 100.000 (99.994)
Epoch: [270][128/196]	Time 0.086 (0.088)	Data 0.000 (0.002)	Loss 0.0694 (0.0561)	Acc@1 98.047 (98.271)	Acc@5 100.000 (99.994)
Epoch: [270][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0724 (0.0567)	Acc@1 98.438 (98.247)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.37
Max memory: 51.4381312
 17.611s  j: 276 bis 280
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9663
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 271
Max memory: 0.1097216
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:271/275; Lr: 0.0001350851717672993
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [271][0/196]	Time 0.165 (0.165)	Data 0.287 (0.287)	Loss 0.0374 (0.0374)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [271][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0478 (0.0561)	Acc@1 98.438 (98.323)	Acc@5 100.000 (99.994)
Epoch: [271][128/196]	Time 0.082 (0.088)	Data 0.000 (0.002)	Loss 0.0484 (0.0551)	Acc@1 97.656 (98.389)	Acc@5 100.000 (99.991)
Epoch: [271][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.0557 (0.0567)	Acc@1 98.438 (98.257)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:272/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [272][0/196]	Time 0.116 (0.116)	Data 0.313 (0.313)	Loss 0.0400 (0.0400)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [272][64/196]	Time 0.080 (0.087)	Data 0.000 (0.005)	Loss 0.0542 (0.0569)	Acc@1 99.219 (98.203)	Acc@5 100.000 (100.000)
Epoch: [272][128/196]	Time 0.090 (0.087)	Data 0.000 (0.003)	Loss 0.0771 (0.0566)	Acc@1 98.047 (98.204)	Acc@5 100.000 (99.994)
Epoch: [272][192/196]	Time 0.088 (0.087)	Data 0.000 (0.002)	Loss 0.0568 (0.0569)	Acc@1 98.047 (98.209)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:273/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [273][0/196]	Time 0.120 (0.120)	Data 0.290 (0.290)	Loss 0.0527 (0.0527)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [273][64/196]	Time 0.083 (0.088)	Data 0.000 (0.005)	Loss 0.0513 (0.0544)	Acc@1 98.047 (98.305)	Acc@5 100.000 (99.988)
Epoch: [273][128/196]	Time 0.098 (0.087)	Data 0.000 (0.002)	Loss 0.0842 (0.0542)	Acc@1 96.484 (98.307)	Acc@5 100.000 (99.988)
Epoch: [273][192/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.0568 (0.0551)	Acc@1 98.438 (98.318)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.0001350851717672993
lr: 0.0001350851717672993
Epoche:274/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [274][0/196]	Time 0.123 (0.123)	Data 0.288 (0.288)	Loss 0.0577 (0.0577)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [274][64/196]	Time 0.086 (0.087)	Data 0.000 (0.005)	Loss 0.0521 (0.0589)	Acc@1 97.656 (98.149)	Acc@5 100.000 (99.976)
Epoch: [274][128/196]	Time 0.089 (0.086)	Data 0.000 (0.002)	Loss 0.0570 (0.0567)	Acc@1 97.266 (98.244)	Acc@5 100.000 (99.985)
Epoch: [274][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.0402 (0.0563)	Acc@1 98.828 (98.263)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
lr: 0.0001350851717672993
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:275/275; Lr: 0.0001350851717672993
batch Size 256
Epoch: [275][0/196]	Time 0.132 (0.132)	Data 0.302 (0.302)	Loss 0.0423 (0.0423)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [275][64/196]	Time 0.099 (0.088)	Data 0.000 (0.005)	Loss 0.0424 (0.0548)	Acc@1 99.609 (98.293)	Acc@5 100.000 (99.988)
Epoch: [275][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0678 (0.0555)	Acc@1 97.656 (98.322)	Acc@5 100.000 (99.991)
Epoch: [275][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.0538 (0.0543)	Acc@1 97.656 (98.371)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.36
Max memory: 51.4381312
 17.514s  j: 281 bis 285
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2135
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 276
Max memory: 0.1097216
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:276/280; Lr: 0.00012157665459056936
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [276][0/196]	Time 0.161 (0.161)	Data 0.293 (0.293)	Loss 0.0596 (0.0596)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [276][64/196]	Time 0.103 (0.089)	Data 0.000 (0.005)	Loss 0.0607 (0.0548)	Acc@1 98.438 (98.353)	Acc@5 100.000 (99.988)
Epoch: [276][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0509 (0.0551)	Acc@1 98.438 (98.328)	Acc@5 100.000 (99.985)
Epoch: [276][192/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.0824 (0.0559)	Acc@1 98.047 (98.274)	Acc@5 99.609 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:277/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [277][0/196]	Time 0.124 (0.124)	Data 0.297 (0.297)	Loss 0.0528 (0.0528)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [277][64/196]	Time 0.096 (0.087)	Data 0.000 (0.005)	Loss 0.0491 (0.0596)	Acc@1 98.047 (98.191)	Acc@5 100.000 (100.000)
Epoch: [277][128/196]	Time 0.086 (0.087)	Data 0.000 (0.003)	Loss 0.0532 (0.0553)	Acc@1 98.438 (98.368)	Acc@5 100.000 (99.997)
Epoch: [277][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0370 (0.0554)	Acc@1 99.219 (98.342)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:278/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [278][0/196]	Time 0.136 (0.136)	Data 0.290 (0.290)	Loss 0.0566 (0.0566)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [278][64/196]	Time 0.091 (0.088)	Data 0.000 (0.005)	Loss 0.0473 (0.0573)	Acc@1 98.438 (98.155)	Acc@5 100.000 (100.000)
Epoch: [278][128/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.0575 (0.0573)	Acc@1 98.828 (98.180)	Acc@5 100.000 (99.997)
Epoch: [278][192/196]	Time 0.092 (0.086)	Data 0.000 (0.002)	Loss 0.0516 (0.0567)	Acc@1 98.438 (98.195)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00012157665459056936
lr: 0.00012157665459056936
Epoche:279/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [279][0/196]	Time 0.141 (0.141)	Data 0.282 (0.282)	Loss 0.0535 (0.0535)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [279][64/196]	Time 0.081 (0.089)	Data 0.000 (0.005)	Loss 0.0460 (0.0537)	Acc@1 98.828 (98.425)	Acc@5 100.000 (99.994)
Epoch: [279][128/196]	Time 0.096 (0.088)	Data 0.000 (0.002)	Loss 0.0573 (0.0548)	Acc@1 97.656 (98.332)	Acc@5 100.000 (99.994)
Epoch: [279][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.0622 (0.0546)	Acc@1 99.219 (98.322)	Acc@5 99.609 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00012157665459056936
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:280/280; Lr: 0.00012157665459056936
batch Size 256
Epoch: [280][0/196]	Time 0.130 (0.130)	Data 0.281 (0.281)	Loss 0.0465 (0.0465)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [280][64/196]	Time 0.087 (0.088)	Data 0.000 (0.005)	Loss 0.0313 (0.0563)	Acc@1 99.219 (98.227)	Acc@5 100.000 (99.976)
Epoch: [280][128/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0696 (0.0560)	Acc@1 98.047 (98.289)	Acc@5 100.000 (99.985)
Epoch: [280][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0432 (0.0572)	Acc@1 98.438 (98.221)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.29
Max memory: 51.4381312
 17.425s  j: 286 bis 290
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1616
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 281
Max memory: 0.1097216
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:281/285; Lr: 0.00010941898913151243
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [281][0/196]	Time 0.155 (0.155)	Data 0.283 (0.283)	Loss 0.0407 (0.0407)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [281][64/196]	Time 0.089 (0.089)	Data 0.000 (0.005)	Loss 0.0343 (0.0564)	Acc@1 99.609 (98.269)	Acc@5 100.000 (99.988)
Epoch: [281][128/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.0633 (0.0567)	Acc@1 97.656 (98.226)	Acc@5 100.000 (99.991)
Epoch: [281][192/196]	Time 0.085 (0.088)	Data 0.000 (0.002)	Loss 0.0831 (0.0558)	Acc@1 96.484 (98.302)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:282/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [282][0/196]	Time 0.139 (0.139)	Data 0.299 (0.299)	Loss 0.0911 (0.0911)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [282][64/196]	Time 0.082 (0.089)	Data 0.000 (0.005)	Loss 0.0870 (0.0591)	Acc@1 98.438 (98.095)	Acc@5 99.609 (99.994)
Epoch: [282][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.0346 (0.0562)	Acc@1 99.609 (98.253)	Acc@5 100.000 (99.991)
Epoch: [282][192/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0610 (0.0563)	Acc@1 98.828 (98.249)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:283/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [283][0/196]	Time 0.125 (0.125)	Data 0.331 (0.331)	Loss 0.0536 (0.0536)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [283][64/196]	Time 0.090 (0.089)	Data 0.000 (0.005)	Loss 0.0547 (0.0546)	Acc@1 98.438 (98.275)	Acc@5 100.000 (100.000)
Epoch: [283][128/196]	Time 0.085 (0.087)	Data 0.000 (0.003)	Loss 0.0703 (0.0562)	Acc@1 98.047 (98.259)	Acc@5 100.000 (100.000)
Epoch: [283][192/196]	Time 0.098 (0.087)	Data 0.000 (0.002)	Loss 0.0596 (0.0550)	Acc@1 96.875 (98.306)	Acc@5 100.000 (99.998)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 0.00010941898913151243
lr: 0.00010941898913151243
Epoche:284/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [284][0/196]	Time 0.124 (0.124)	Data 0.310 (0.310)	Loss 0.0302 (0.0302)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [284][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.0417 (0.0562)	Acc@1 98.828 (98.143)	Acc@5 100.000 (100.000)
Epoch: [284][128/196]	Time 0.089 (0.088)	Data 0.000 (0.003)	Loss 0.0729 (0.0559)	Acc@1 96.484 (98.192)	Acc@5 100.000 (99.994)
Epoch: [284][192/196]	Time 0.079 (0.087)	Data 0.000 (0.002)	Loss 0.0528 (0.0563)	Acc@1 98.438 (98.257)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 0.00010941898913151243
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:285/285; Lr: 0.00010941898913151243
batch Size 256
Epoch: [285][0/196]	Time 0.134 (0.134)	Data 0.298 (0.298)	Loss 0.0479 (0.0479)	Acc@1 98.438 (98.438)	Acc@5 99.609 (99.609)
Epoch: [285][64/196]	Time 0.085 (0.089)	Data 0.000 (0.005)	Loss 0.0649 (0.0522)	Acc@1 96.875 (98.395)	Acc@5 100.000 (99.958)
Epoch: [285][128/196]	Time 0.094 (0.088)	Data 0.000 (0.003)	Loss 0.0710 (0.0552)	Acc@1 97.266 (98.265)	Acc@5 100.000 (99.979)
Epoch: [285][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.0881 (0.0552)	Acc@1 96.094 (98.237)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.4
Max memory: 51.4381312
 17.531s  j: 291 bis 295
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3598
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 286
Max memory: 0.1097216
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:286/290; Lr: 9.847709021836118e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [286][0/196]	Time 0.147 (0.147)	Data 0.297 (0.297)	Loss 0.0551 (0.0551)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [286][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.0745 (0.0570)	Acc@1 97.266 (98.233)	Acc@5 100.000 (99.982)
Epoch: [286][128/196]	Time 0.082 (0.090)	Data 0.000 (0.003)	Loss 0.0700 (0.0565)	Acc@1 98.047 (98.250)	Acc@5 100.000 (99.985)
Epoch: [286][192/196]	Time 0.081 (0.089)	Data 0.000 (0.002)	Loss 0.0436 (0.0561)	Acc@1 98.828 (98.272)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:287/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [287][0/196]	Time 0.123 (0.123)	Data 0.312 (0.312)	Loss 0.0464 (0.0464)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [287][64/196]	Time 0.089 (0.088)	Data 0.000 (0.005)	Loss 0.0580 (0.0562)	Acc@1 99.219 (98.215)	Acc@5 100.000 (99.988)
Epoch: [287][128/196]	Time 0.091 (0.088)	Data 0.000 (0.003)	Loss 0.0586 (0.0562)	Acc@1 98.047 (98.223)	Acc@5 100.000 (99.991)
Epoch: [287][192/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.0989 (0.0554)	Acc@1 96.875 (98.274)	Acc@5 100.000 (99.990)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:288/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [288][0/196]	Time 0.108 (0.108)	Data 0.307 (0.307)	Loss 0.0437 (0.0437)	Acc@1 99.609 (99.609)	Acc@5 100.000 (100.000)
Epoch: [288][64/196]	Time 0.090 (0.087)	Data 0.000 (0.005)	Loss 0.0912 (0.0571)	Acc@1 96.484 (98.239)	Acc@5 100.000 (99.976)
Epoch: [288][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 0.0767 (0.0566)	Acc@1 97.266 (98.189)	Acc@5 99.609 (99.982)
Epoch: [288][192/196]	Time 0.092 (0.087)	Data 0.000 (0.002)	Loss 0.0918 (0.0554)	Acc@1 96.875 (98.249)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 9.847709021836118e-05
lr: 9.847709021836118e-05
Epoche:289/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [289][0/196]	Time 0.120 (0.120)	Data 0.362 (0.362)	Loss 0.0611 (0.0611)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [289][64/196]	Time 0.086 (0.089)	Data 0.000 (0.006)	Loss 0.0911 (0.0535)	Acc@1 96.484 (98.383)	Acc@5 100.000 (99.982)
Epoch: [289][128/196]	Time 0.087 (0.089)	Data 0.000 (0.003)	Loss 0.0587 (0.0548)	Acc@1 98.828 (98.347)	Acc@5 100.000 (99.988)
Epoch: [289][192/196]	Time 0.097 (0.088)	Data 0.000 (0.002)	Loss 0.0613 (0.0561)	Acc@1 97.656 (98.280)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 9.847709021836118e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:290/290; Lr: 9.847709021836118e-05
batch Size 256
Epoch: [290][0/196]	Time 0.133 (0.133)	Data 0.326 (0.326)	Loss 0.0663 (0.0663)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [290][64/196]	Time 0.088 (0.089)	Data 0.000 (0.005)	Loss 0.0567 (0.0582)	Acc@1 98.047 (98.161)	Acc@5 100.000 (99.988)
Epoch: [290][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.0652 (0.0580)	Acc@1 98.438 (98.180)	Acc@5 100.000 (99.988)
Epoch: [290][192/196]	Time 0.092 (0.088)	Data 0.000 (0.002)	Loss 0.0698 (0.0575)	Acc@1 98.047 (98.189)	Acc@5 100.000 (99.986)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.38
Max memory: 51.4381312
 17.637s  j: 296 bis 300
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5305
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 291
Max memory: 0.1097216
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:291/295; Lr: 8.862938119652506e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [291][0/196]	Time 0.153 (0.153)	Data 0.294 (0.294)	Loss 0.0350 (0.0350)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [291][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.0649 (0.0531)	Acc@1 98.047 (98.353)	Acc@5 100.000 (99.994)
Epoch: [291][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0679 (0.0534)	Acc@1 96.875 (98.328)	Acc@5 100.000 (99.997)
Epoch: [291][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.1013 (0.0541)	Acc@1 95.703 (98.316)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:292/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [292][0/196]	Time 0.152 (0.152)	Data 0.271 (0.271)	Loss 0.0714 (0.0714)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [292][64/196]	Time 0.085 (0.089)	Data 0.000 (0.004)	Loss 0.0454 (0.0553)	Acc@1 98.438 (98.329)	Acc@5 100.000 (99.994)
Epoch: [292][128/196]	Time 0.080 (0.087)	Data 0.000 (0.002)	Loss 0.0519 (0.0541)	Acc@1 98.828 (98.344)	Acc@5 100.000 (99.994)
Epoch: [292][192/196]	Time 0.093 (0.087)	Data 0.000 (0.002)	Loss 0.0803 (0.0547)	Acc@1 97.266 (98.284)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:293/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [293][0/196]	Time 0.110 (0.110)	Data 0.315 (0.315)	Loss 0.0445 (0.0445)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [293][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.0681 (0.0595)	Acc@1 98.438 (98.125)	Acc@5 100.000 (99.988)
Epoch: [293][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.0516 (0.0576)	Acc@1 98.438 (98.226)	Acc@5 100.000 (99.994)
Epoch: [293][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.0692 (0.0575)	Acc@1 98.438 (98.235)	Acc@5 100.000 (99.994)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 8.862938119652506e-05
lr: 8.862938119652506e-05
Epoche:294/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [294][0/196]	Time 0.143 (0.143)	Data 0.306 (0.306)	Loss 0.0490 (0.0490)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [294][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.0400 (0.0599)	Acc@1 99.219 (98.143)	Acc@5 100.000 (99.976)
Epoch: [294][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.0497 (0.0585)	Acc@1 98.828 (98.177)	Acc@5 100.000 (99.988)
Epoch: [294][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.0901 (0.0563)	Acc@1 97.266 (98.259)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
lr: 8.862938119652506e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:295/295; Lr: 8.862938119652506e-05
batch Size 256
Epoch: [295][0/196]	Time 0.120 (0.120)	Data 0.297 (0.297)	Loss 0.0576 (0.0576)	Acc@1 98.438 (98.438)	Acc@5 99.609 (99.609)
Epoch: [295][64/196]	Time 0.093 (0.089)	Data 0.000 (0.005)	Loss 0.0567 (0.0536)	Acc@1 98.828 (98.431)	Acc@5 100.000 (99.994)
Epoch: [295][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.0788 (0.0547)	Acc@1 98.047 (98.341)	Acc@5 100.000 (99.994)
Epoch: [295][192/196]	Time 0.089 (0.088)	Data 0.000 (0.002)	Loss 0.0521 (0.0558)	Acc@1 98.438 (98.288)	Acc@5 100.000 (99.992)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.32
Max memory: 51.4381312
 17.655s  j: 301 bis 305
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7389
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 296
Max memory: 0.1097216
lr: 7.976644307687256e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:296/300; Lr: 7.976644307687256e-05
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [296][0/196]	Time 0.143 (0.143)	Data 0.303 (0.303)	Loss 0.0535 (0.0535)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [296][64/196]	Time 0.086 (0.089)	Data 0.000 (0.005)	Loss 0.0632 (0.0562)	Acc@1 97.266 (98.275)	Acc@5 100.000 (99.988)
Epoch: [296][128/196]	Time 0.088 (0.088)	Data 0.000 (0.003)	Loss 0.0361 (0.0544)	Acc@1 99.219 (98.377)	Acc@5 100.000 (99.991)
Epoch: [296][192/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.0440 (0.0555)	Acc@1 98.828 (98.304)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 7.976644307687256e-05
args.lr: 7.976644307687256e-05
lr: 7.976644307687256e-05
Epoche:297/300; Lr: 7.976644307687256e-05
batch Size 256
