Net2Net 1
j: 1 bis 5
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9185
Files already downloaded and verified
width: 8

Arch Num:  [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
Max memory: 0.0573952
lr: 0.1
lr: 0.1
1
Epoche:1/5; Lr: 0.1
batch Size 256
Epoch: [1][0/196]	Time 0.155 (0.155)	Data 0.264 (0.264)	Loss 2.9887 (2.9887)	Acc@1 10.938 (10.938)	Acc@5 46.875 (46.875)
Epoch: [1][64/196]	Time 0.083 (0.090)	Data 0.000 (0.004)	Loss 1.8264 (2.1158)	Acc@1 26.953 (20.956)	Acc@5 86.328 (72.879)
Epoch: [1][128/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 1.7285 (1.9443)	Acc@1 32.812 (26.590)	Acc@5 87.891 (79.463)
Epoch: [1][192/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 1.5865 (1.8506)	Acc@1 38.281 (29.758)	Acc@5 91.406 (82.479)
Max memory in training epoch: 33.3018624
lr: 0.1
lr: 0.1
1
Epoche:2/5; Lr: 0.1
batch Size 256
Epoch: [2][0/196]	Time 0.129 (0.129)	Data 0.316 (0.316)	Loss 1.7251 (1.7251)	Acc@1 38.281 (38.281)	Acc@5 88.281 (88.281)
Epoch: [2][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 1.5001 (1.5134)	Acc@1 45.703 (44.597)	Acc@5 91.406 (91.286)
Epoch: [2][128/196]	Time 0.088 (0.087)	Data 0.000 (0.003)	Loss 1.2255 (1.4533)	Acc@1 55.859 (46.678)	Acc@5 93.750 (92.097)
Epoch: [2][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 1.2678 (1.4016)	Acc@1 56.250 (48.731)	Acc@5 95.312 (92.742)
Max memory in training epoch: 33.3018624
lr: 0.1
lr: 0.1
1
Epoche:3/5; Lr: 0.1
batch Size 256
Epoch: [3][0/196]	Time 0.125 (0.125)	Data 0.287 (0.287)	Loss 1.2304 (1.2304)	Acc@1 58.984 (58.984)	Acc@5 94.531 (94.531)
Epoch: [3][64/196]	Time 0.084 (0.087)	Data 0.000 (0.005)	Loss 1.1893 (1.1608)	Acc@1 57.812 (58.588)	Acc@5 94.141 (95.343)
Epoch: [3][128/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 1.0709 (1.1335)	Acc@1 61.719 (59.557)	Acc@5 94.922 (95.673)
Epoch: [3][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.9114 (1.0947)	Acc@1 66.016 (60.933)	Acc@5 97.656 (95.999)
Max memory in training epoch: 33.3018624
lr: 0.1
lr: 0.1
1
Epoche:4/5; Lr: 0.1
batch Size 256
Epoch: [4][0/196]	Time 0.118 (0.118)	Data 0.296 (0.296)	Loss 0.8649 (0.8649)	Acc@1 67.188 (67.188)	Acc@5 97.656 (97.656)
Epoch: [4][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.8198 (0.9691)	Acc@1 69.922 (65.264)	Acc@5 97.656 (96.839)
Epoch: [4][128/196]	Time 0.087 (0.087)	Data 0.000 (0.003)	Loss 0.8058 (0.9468)	Acc@1 72.656 (66.476)	Acc@5 96.875 (97.093)
Epoch: [4][192/196]	Time 0.086 (0.087)	Data 0.000 (0.002)	Loss 0.8462 (0.9243)	Acc@1 71.875 (67.202)	Acc@5 98.828 (97.298)
Max memory in training epoch: 33.3018624
lr: 0.1
lr: 0.1
1
Epoche:5/5; Lr: 0.1
batch Size 256
Epoch: [5][0/196]	Time 0.133 (0.133)	Data 0.289 (0.289)	Loss 0.8542 (0.8542)	Acc@1 71.875 (71.875)	Acc@5 97.656 (97.656)
Epoch: [5][64/196]	Time 0.084 (0.088)	Data 0.000 (0.005)	Loss 0.7272 (0.8463)	Acc@1 75.000 (70.463)	Acc@5 98.438 (97.861)
Epoch: [5][128/196]	Time 0.090 (0.087)	Data 0.000 (0.002)	Loss 0.8680 (0.8275)	Acc@1 73.047 (71.030)	Acc@5 96.094 (97.902)
Epoch: [5][192/196]	Time 0.089 (0.087)	Data 0.000 (0.002)	Loss 0.8057 (0.8204)	Acc@1 70.312 (71.256)	Acc@5 98.828 (97.905)
Max memory in training epoch: 33.3018624
[INFO] Storing checkpoint...
  67.49
Max memory: 51.3858048
 17.381s  j: 6 bis 10
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1918
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 6
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:6/10; Lr: 0.1
batch Size 256
Epoch: [6][0/196]	Time 0.164 (0.164)	Data 0.269 (0.269)	Loss 0.8159 (0.8159)	Acc@1 73.438 (73.438)	Acc@5 97.266 (97.266)
Epoch: [6][64/196]	Time 0.084 (0.089)	Data 0.000 (0.004)	Loss 0.7485 (0.7447)	Acc@1 75.000 (74.105)	Acc@5 98.047 (98.251)
Epoch: [6][128/196]	Time 0.087 (0.089)	Data 0.000 (0.002)	Loss 0.6995 (0.7378)	Acc@1 77.734 (74.252)	Acc@5 98.828 (98.307)
Epoch: [6][192/196]	Time 0.088 (0.088)	Data 0.000 (0.002)	Loss 0.7106 (0.7388)	Acc@1 76.172 (74.095)	Acc@5 99.219 (98.288)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:7/10; Lr: 0.1
batch Size 256
Epoch: [7][0/196]	Time 0.118 (0.118)	Data 0.313 (0.313)	Loss 0.7106 (0.7106)	Acc@1 76.562 (76.562)	Acc@5 96.875 (96.875)
Epoch: [7][64/196]	Time 0.082 (0.088)	Data 0.000 (0.005)	Loss 0.5996 (0.7137)	Acc@1 81.641 (74.916)	Acc@5 98.828 (98.263)
Epoch: [7][128/196]	Time 0.083 (0.088)	Data 0.000 (0.003)	Loss 0.7000 (0.7029)	Acc@1 76.172 (75.433)	Acc@5 99.219 (98.413)
Epoch: [7][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.6358 (0.7009)	Acc@1 76.172 (75.480)	Acc@5 99.609 (98.464)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:8/10; Lr: 0.1
batch Size 256
Epoch: [8][0/196]	Time 0.137 (0.137)	Data 0.271 (0.271)	Loss 0.7184 (0.7184)	Acc@1 72.656 (72.656)	Acc@5 98.828 (98.828)
Epoch: [8][64/196]	Time 0.085 (0.090)	Data 0.000 (0.004)	Loss 0.6303 (0.6496)	Acc@1 80.078 (77.434)	Acc@5 97.266 (98.738)
Epoch: [8][128/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.6514 (0.6564)	Acc@1 76.562 (77.171)	Acc@5 99.219 (98.707)
Epoch: [8][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.6023 (0.6592)	Acc@1 78.906 (76.965)	Acc@5 99.219 (98.674)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:9/10; Lr: 0.1
batch Size 256
Epoch: [9][0/196]	Time 0.148 (0.148)	Data 0.272 (0.272)	Loss 0.6688 (0.6688)	Acc@1 77.734 (77.734)	Acc@5 98.828 (98.828)
Epoch: [9][64/196]	Time 0.085 (0.090)	Data 0.000 (0.004)	Loss 0.6694 (0.6551)	Acc@1 76.562 (77.542)	Acc@5 99.609 (98.606)
Epoch: [9][128/196]	Time 0.084 (0.089)	Data 0.000 (0.002)	Loss 0.7007 (0.6496)	Acc@1 76.562 (77.659)	Acc@5 99.609 (98.683)
Epoch: [9][192/196]	Time 0.083 (0.088)	Data 0.000 (0.002)	Loss 0.5185 (0.6441)	Acc@1 83.203 (77.827)	Acc@5 99.219 (98.676)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:10/10; Lr: 0.1
batch Size 256
Epoch: [10][0/196]	Time 0.115 (0.115)	Data 0.294 (0.294)	Loss 0.6595 (0.6595)	Acc@1 80.469 (80.469)	Acc@5 98.828 (98.828)
Epoch: [10][64/196]	Time 0.091 (0.090)	Data 0.000 (0.005)	Loss 0.5645 (0.6323)	Acc@1 80.859 (78.401)	Acc@5 100.000 (98.774)
Epoch: [10][128/196]	Time 0.090 (0.089)	Data 0.000 (0.002)	Loss 0.6405 (0.6212)	Acc@1 75.391 (78.464)	Acc@5 98.828 (98.792)
Epoch: [10][192/196]	Time 0.085 (0.089)	Data 0.000 (0.002)	Loss 0.4582 (0.6195)	Acc@1 84.375 (78.479)	Acc@5 100.000 (98.776)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  74.53
Max memory: 51.4381312
 17.746s  j: 11 bis 15
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6365
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 11
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:11/15; Lr: 0.1
batch Size 256
Epoch: [11][0/196]	Time 0.152 (0.152)	Data 0.295 (0.295)	Loss 0.6444 (0.6444)	Acc@1 74.219 (74.219)	Acc@5 99.219 (99.219)
Epoch: [11][64/196]	Time 0.081 (0.089)	Data 0.000 (0.005)	Loss 0.5705 (0.5937)	Acc@1 82.422 (79.519)	Acc@5 98.828 (98.948)
Epoch: [11][128/196]	Time 0.093 (0.088)	Data 0.000 (0.002)	Loss 0.6857 (0.5967)	Acc@1 73.828 (79.394)	Acc@5 98.828 (98.916)
Epoch: [11][192/196]	Time 0.081 (0.087)	Data 0.000 (0.002)	Loss 0.6594 (0.5945)	Acc@1 82.031 (79.602)	Acc@5 98.438 (98.913)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:12/15; Lr: 0.1
batch Size 256
Epoch: [12][0/196]	Time 0.107 (0.107)	Data 0.299 (0.299)	Loss 0.5424 (0.5424)	Acc@1 82.422 (82.422)	Acc@5 98.828 (98.828)
Epoch: [12][64/196]	Time 0.085 (0.088)	Data 0.000 (0.005)	Loss 0.7155 (0.5996)	Acc@1 75.000 (79.099)	Acc@5 98.047 (99.069)
Epoch: [12][128/196]	Time 0.083 (0.087)	Data 0.000 (0.003)	Loss 0.4866 (0.5960)	Acc@1 83.984 (79.473)	Acc@5 98.828 (98.931)
Epoch: [12][192/196]	Time 0.083 (0.087)	Data 0.000 (0.002)	Loss 0.5963 (0.5911)	Acc@1 78.125 (79.534)	Acc@5 99.219 (98.895)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:13/15; Lr: 0.1
batch Size 256
Epoch: [13][0/196]	Time 0.137 (0.137)	Data 0.266 (0.266)	Loss 0.4851 (0.4851)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [13][64/196]	Time 0.083 (0.088)	Data 0.000 (0.004)	Loss 0.6315 (0.5596)	Acc@1 78.906 (80.571)	Acc@5 98.438 (99.050)
Epoch: [13][128/196]	Time 0.094 (0.088)	Data 0.000 (0.002)	Loss 0.6817 (0.5656)	Acc@1 74.609 (80.351)	Acc@5 99.609 (98.986)
Epoch: [13][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.6822 (0.5647)	Acc@1 78.125 (80.418)	Acc@5 98.438 (99.010)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:14/15; Lr: 0.1
batch Size 256
Epoch: [14][0/196]	Time 0.128 (0.128)	Data 0.312 (0.312)	Loss 0.6104 (0.6104)	Acc@1 82.031 (82.031)	Acc@5 98.047 (98.047)
Epoch: [14][64/196]	Time 0.086 (0.088)	Data 0.000 (0.005)	Loss 0.5461 (0.5723)	Acc@1 79.688 (80.270)	Acc@5 99.609 (98.948)
Epoch: [14][128/196]	Time 0.085 (0.088)	Data 0.000 (0.003)	Loss 0.6062 (0.5640)	Acc@1 77.344 (80.587)	Acc@5 98.047 (98.952)
Epoch: [14][192/196]	Time 0.085 (0.087)	Data 0.000 (0.002)	Loss 0.5387 (0.5596)	Acc@1 80.078 (80.706)	Acc@5 98.828 (98.974)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:15/15; Lr: 0.1
batch Size 256
Epoch: [15][0/196]	Time 0.117 (0.117)	Data 0.271 (0.271)	Loss 0.5668 (0.5668)	Acc@1 79.297 (79.297)	Acc@5 98.828 (98.828)
Epoch: [15][64/196]	Time 0.095 (0.089)	Data 0.000 (0.004)	Loss 0.4566 (0.5281)	Acc@1 84.766 (81.562)	Acc@5 98.828 (99.062)
Epoch: [15][128/196]	Time 0.080 (0.088)	Data 0.000 (0.002)	Loss 0.5819 (0.5396)	Acc@1 76.562 (81.301)	Acc@5 99.609 (99.073)
Epoch: [15][192/196]	Time 0.082 (0.087)	Data 0.000 (0.002)	Loss 0.6114 (0.5436)	Acc@1 79.688 (81.212)	Acc@5 98.438 (99.033)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  74.12
Max memory: 51.4381312
 17.469s  j: 16 bis 20
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2987
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 16
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:16/20; Lr: 0.1
batch Size 256
Epoch: [16][0/196]	Time 0.154 (0.154)	Data 0.254 (0.254)	Loss 0.5267 (0.5267)	Acc@1 80.469 (80.469)	Acc@5 99.609 (99.609)
Epoch: [16][64/196]	Time 0.082 (0.088)	Data 0.000 (0.004)	Loss 0.4979 (0.5201)	Acc@1 86.328 (82.019)	Acc@5 99.219 (99.129)
Epoch: [16][128/196]	Time 0.084 (0.087)	Data 0.000 (0.002)	Loss 0.5899 (0.5323)	Acc@1 76.172 (81.589)	Acc@5 98.438 (99.095)
Epoch: [16][192/196]	Time 0.086 (0.088)	Data 0.000 (0.001)	Loss 0.5784 (0.5395)	Acc@1 80.859 (81.458)	Acc@5 98.047 (99.063)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:17/20; Lr: 0.1
batch Size 256
Epoch: [17][0/196]	Time 0.121 (0.121)	Data 0.295 (0.295)	Loss 0.5940 (0.5940)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [17][64/196]	Time 0.091 (0.089)	Data 0.000 (0.005)	Loss 0.5401 (0.5339)	Acc@1 79.297 (81.526)	Acc@5 99.609 (99.062)
Epoch: [17][128/196]	Time 0.087 (0.088)	Data 0.000 (0.003)	Loss 0.4768 (0.5355)	Acc@1 83.594 (81.504)	Acc@5 98.828 (99.076)
Epoch: [17][192/196]	Time 0.084 (0.088)	Data 0.000 (0.002)	Loss 0.5335 (0.5388)	Acc@1 80.859 (81.341)	Acc@5 100.000 (99.085)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:18/20; Lr: 0.1
batch Size 256
Epoch: [18][0/196]	Time 0.134 (0.134)	Data 0.290 (0.290)	Loss 0.5461 (0.5461)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [18][64/196]	Time 0.093 (0.091)	Data 0.000 (0.005)	Loss 0.5358 (0.5303)	Acc@1 83.203 (81.977)	Acc@5 98.438 (99.117)
Epoch: [18][128/196]	Time 0.082 (0.090)	Data 0.000 (0.003)	Loss 0.5173 (0.5270)	Acc@1 82.031 (81.859)	Acc@5 99.219 (99.137)
Epoch: [18][192/196]	Time 0.089 (0.089)	Data 0.000 (0.002)	Loss 0.5814 (0.5248)	Acc@1 78.906 (81.914)	Acc@5 98.828 (99.124)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:19/20; Lr: 0.1
batch Size 256
Epoch: [19][0/196]	Time 0.141 (0.141)	Data 0.298 (0.298)	Loss 0.5078 (0.5078)	Acc@1 81.641 (81.641)	Acc@5 99.219 (99.219)
Epoch: [19][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.4725 (0.5246)	Acc@1 84.766 (81.881)	Acc@5 99.609 (99.135)
Epoch: [19][128/196]	Time 0.084 (0.088)	Data 0.000 (0.003)	Loss 0.5454 (0.5211)	Acc@1 83.203 (82.074)	Acc@5 98.828 (99.104)
Epoch: [19][192/196]	Time 0.081 (0.088)	Data 0.000 (0.002)	Loss 0.5077 (0.5253)	Acc@1 85.156 (81.843)	Acc@5 98.047 (99.077)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:20/20; Lr: 0.1
batch Size 256
Epoch: [20][0/196]	Time 0.126 (0.126)	Data 0.298 (0.298)	Loss 0.4209 (0.4209)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [20][64/196]	Time 0.092 (0.089)	Data 0.000 (0.005)	Loss 0.4975 (0.5106)	Acc@1 82.812 (82.320)	Acc@5 99.219 (99.183)
Epoch: [20][128/196]	Time 0.086 (0.088)	Data 0.000 (0.003)	Loss 0.4975 (0.5120)	Acc@1 83.203 (82.404)	Acc@5 98.438 (99.161)
Epoch: [20][192/196]	Time 0.087 (0.088)	Data 0.000 (0.002)	Loss 0.5492 (0.5115)	Acc@1 81.641 (82.404)	Acc@5 98.438 (99.166)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  76.67
Max memory: 51.4381312
 17.587s  j: 21 bis 25
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4526
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 21
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:21/25; Lr: 0.1
batch Size 256
Epoch: [21][0/196]	Time 0.166 (0.166)	Data 0.353 (0.353)	Loss 0.4870 (0.4870)	Acc@1 83.984 (83.984)	Acc@5 98.828 (98.828)
Epoch: [21][64/196]	Time 0.088 (0.098)	Data 0.000 (0.006)	Loss 0.5546 (0.4864)	Acc@1 80.469 (83.161)	Acc@5 98.828 (99.231)
Epoch: [21][128/196]	Time 0.096 (0.097)	Data 0.000 (0.003)	Loss 0.5537 (0.4959)	Acc@1 82.031 (82.894)	Acc@5 98.828 (99.188)
Epoch: [21][192/196]	Time 0.109 (0.097)	Data 0.000 (0.002)	Loss 0.5161 (0.5036)	Acc@1 80.469 (82.594)	Acc@5 98.047 (99.203)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:22/25; Lr: 0.1
batch Size 256
Epoch: [22][0/196]	Time 0.172 (0.172)	Data 0.352 (0.352)	Loss 0.5559 (0.5559)	Acc@1 78.516 (78.516)	Acc@5 99.219 (99.219)
Epoch: [22][64/196]	Time 0.098 (0.098)	Data 0.000 (0.006)	Loss 0.5988 (0.5129)	Acc@1 79.297 (81.989)	Acc@5 99.609 (99.237)
Epoch: [22][128/196]	Time 0.114 (0.097)	Data 0.000 (0.003)	Loss 0.4789 (0.5074)	Acc@1 82.812 (82.340)	Acc@5 99.609 (99.170)
Epoch: [22][192/196]	Time 0.092 (0.095)	Data 0.000 (0.002)	Loss 0.5228 (0.5028)	Acc@1 82.812 (82.477)	Acc@5 98.828 (99.219)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:23/25; Lr: 0.1
batch Size 256
Epoch: [23][0/196]	Time 0.139 (0.139)	Data 0.284 (0.284)	Loss 0.5528 (0.5528)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [23][64/196]	Time 0.110 (0.093)	Data 0.000 (0.005)	Loss 0.5640 (0.4966)	Acc@1 79.297 (82.812)	Acc@5 99.219 (99.267)
Epoch: [23][128/196]	Time 0.093 (0.095)	Data 0.000 (0.002)	Loss 0.4589 (0.4961)	Acc@1 84.375 (82.876)	Acc@5 100.000 (99.237)
Epoch: [23][192/196]	Time 0.089 (0.094)	Data 0.000 (0.002)	Loss 0.5197 (0.4969)	Acc@1 80.078 (82.853)	Acc@5 99.219 (99.251)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:24/25; Lr: 0.1
batch Size 256
Epoch: [24][0/196]	Time 0.148 (0.148)	Data 0.294 (0.294)	Loss 0.4971 (0.4971)	Acc@1 83.203 (83.203)	Acc@5 100.000 (100.000)
Epoch: [24][64/196]	Time 0.104 (0.102)	Data 0.000 (0.005)	Loss 0.4593 (0.4958)	Acc@1 84.766 (82.975)	Acc@5 100.000 (99.285)
Epoch: [24][128/196]	Time 0.091 (0.100)	Data 0.000 (0.003)	Loss 0.4793 (0.4861)	Acc@1 84.375 (83.218)	Acc@5 99.219 (99.310)
Epoch: [24][192/196]	Time 0.102 (0.099)	Data 0.000 (0.002)	Loss 0.5453 (0.4904)	Acc@1 80.078 (82.995)	Acc@5 98.438 (99.288)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:25/25; Lr: 0.1
batch Size 256
Epoch: [25][0/196]	Time 0.135 (0.135)	Data 0.324 (0.324)	Loss 0.4663 (0.4663)	Acc@1 82.812 (82.812)	Acc@5 98.828 (98.828)
Epoch: [25][64/196]	Time 0.089 (0.098)	Data 0.000 (0.005)	Loss 0.5162 (0.4872)	Acc@1 81.641 (82.782)	Acc@5 99.609 (99.303)
Epoch: [25][128/196]	Time 0.080 (0.092)	Data 0.000 (0.003)	Loss 0.4813 (0.4855)	Acc@1 80.469 (83.064)	Acc@5 99.609 (99.301)
Epoch: [25][192/196]	Time 0.087 (0.092)	Data 0.000 (0.002)	Loss 0.3745 (0.4907)	Acc@1 86.719 (82.991)	Acc@5 100.000 (99.243)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  73.03
Max memory: 51.4381312
 18.517s  j: 26 bis 30
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 421
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 26
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:26/30; Lr: 0.1
batch Size 256
Epoch: [26][0/196]	Time 0.148 (0.148)	Data 0.415 (0.415)	Loss 0.4712 (0.4712)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [26][64/196]	Time 0.092 (0.097)	Data 0.000 (0.007)	Loss 0.5129 (0.4517)	Acc@1 82.812 (84.279)	Acc@5 98.047 (99.387)
Epoch: [26][128/196]	Time 0.090 (0.097)	Data 0.000 (0.003)	Loss 0.4359 (0.4752)	Acc@1 84.375 (83.382)	Acc@5 99.609 (99.282)
Epoch: [26][192/196]	Time 0.086 (0.096)	Data 0.000 (0.002)	Loss 0.5264 (0.4805)	Acc@1 82.422 (83.325)	Acc@5 98.828 (99.273)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:27/30; Lr: 0.1
batch Size 256
Epoch: [27][0/196]	Time 0.164 (0.164)	Data 0.365 (0.365)	Loss 0.4653 (0.4653)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [27][64/196]	Time 0.088 (0.100)	Data 0.000 (0.006)	Loss 0.4219 (0.4815)	Acc@1 85.156 (83.522)	Acc@5 98.828 (99.261)
Epoch: [27][128/196]	Time 0.085 (0.094)	Data 0.000 (0.003)	Loss 0.4659 (0.4781)	Acc@1 86.719 (83.533)	Acc@5 98.047 (99.267)
Epoch: [27][192/196]	Time 0.092 (0.094)	Data 0.000 (0.002)	Loss 0.5533 (0.4836)	Acc@1 80.078 (83.221)	Acc@5 99.219 (99.284)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:28/30; Lr: 0.1
batch Size 256
Epoch: [28][0/196]	Time 0.148 (0.148)	Data 0.340 (0.340)	Loss 0.3883 (0.3883)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [28][64/196]	Time 0.084 (0.097)	Data 0.000 (0.005)	Loss 0.5290 (0.4859)	Acc@1 81.641 (83.161)	Acc@5 99.609 (99.273)
Epoch: [28][128/196]	Time 0.109 (0.095)	Data 0.000 (0.003)	Loss 0.4054 (0.4857)	Acc@1 85.547 (83.370)	Acc@5 100.000 (99.237)
Epoch: [28][192/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 0.4979 (0.4848)	Acc@1 80.469 (83.316)	Acc@5 99.219 (99.271)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:29/30; Lr: 0.1
batch Size 256
Epoch: [29][0/196]	Time 0.152 (0.152)	Data 0.297 (0.297)	Loss 0.4507 (0.4507)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [29][64/196]	Time 0.087 (0.099)	Data 0.000 (0.005)	Loss 0.4156 (0.4807)	Acc@1 86.328 (83.600)	Acc@5 98.828 (99.249)
Epoch: [29][128/196]	Time 0.092 (0.099)	Data 0.000 (0.003)	Loss 0.4611 (0.4759)	Acc@1 84.766 (83.718)	Acc@5 100.000 (99.276)
Epoch: [29][192/196]	Time 0.085 (0.097)	Data 0.000 (0.002)	Loss 0.5173 (0.4745)	Acc@1 83.203 (83.741)	Acc@5 98.828 (99.249)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:30/30; Lr: 0.1
batch Size 256
Epoch: [30][0/196]	Time 0.130 (0.130)	Data 0.319 (0.319)	Loss 0.4442 (0.4442)	Acc@1 86.328 (86.328)	Acc@5 98.828 (98.828)
Epoch: [30][64/196]	Time 0.091 (0.093)	Data 0.000 (0.005)	Loss 0.5374 (0.4625)	Acc@1 83.203 (84.525)	Acc@5 99.219 (99.261)
Epoch: [30][128/196]	Time 0.093 (0.096)	Data 0.000 (0.003)	Loss 0.4627 (0.4664)	Acc@1 84.766 (84.033)	Acc@5 99.219 (99.231)
Epoch: [30][192/196]	Time 0.093 (0.097)	Data 0.000 (0.002)	Loss 0.4254 (0.4666)	Acc@1 84.766 (83.944)	Acc@5 99.609 (99.225)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  75.21
Max memory: 51.4381312
 19.322s  j: 31 bis 35
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9071
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 31
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:31/35; Lr: 0.1
batch Size 256
Epoch: [31][0/196]	Time 0.194 (0.194)	Data 0.334 (0.334)	Loss 0.4478 (0.4478)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [31][64/196]	Time 0.087 (0.100)	Data 0.000 (0.005)	Loss 0.6324 (0.4596)	Acc@1 78.125 (84.267)	Acc@5 99.219 (99.279)
Epoch: [31][128/196]	Time 0.089 (0.100)	Data 0.000 (0.003)	Loss 0.6032 (0.4629)	Acc@1 78.516 (84.136)	Acc@5 97.656 (99.279)
Epoch: [31][192/196]	Time 0.080 (0.097)	Data 0.000 (0.002)	Loss 0.5813 (0.4650)	Acc@1 79.688 (84.067)	Acc@5 98.828 (99.271)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:32/35; Lr: 0.1
batch Size 256
Epoch: [32][0/196]	Time 0.106 (0.106)	Data 0.267 (0.267)	Loss 0.5421 (0.5421)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [32][64/196]	Time 0.094 (0.090)	Data 0.000 (0.004)	Loss 0.4075 (0.4614)	Acc@1 85.156 (84.291)	Acc@5 99.219 (99.369)
Epoch: [32][128/196]	Time 0.094 (0.094)	Data 0.000 (0.002)	Loss 0.5516 (0.4653)	Acc@1 79.297 (84.054)	Acc@5 99.609 (99.316)
Epoch: [32][192/196]	Time 0.105 (0.092)	Data 0.000 (0.002)	Loss 0.4272 (0.4648)	Acc@1 85.938 (83.976)	Acc@5 99.609 (99.271)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:33/35; Lr: 0.1
batch Size 256
Epoch: [33][0/196]	Time 0.156 (0.156)	Data 0.316 (0.316)	Loss 0.4217 (0.4217)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [33][64/196]	Time 0.109 (0.101)	Data 0.000 (0.005)	Loss 0.4394 (0.4581)	Acc@1 86.328 (84.411)	Acc@5 99.219 (99.255)
Epoch: [33][128/196]	Time 0.094 (0.100)	Data 0.000 (0.003)	Loss 0.4218 (0.4621)	Acc@1 85.156 (84.190)	Acc@5 99.219 (99.273)
Epoch: [33][192/196]	Time 0.093 (0.100)	Data 0.000 (0.002)	Loss 0.3734 (0.4612)	Acc@1 87.500 (84.197)	Acc@5 98.438 (99.279)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:34/35; Lr: 0.1
batch Size 256
Epoch: [34][0/196]	Time 0.130 (0.130)	Data 0.361 (0.361)	Loss 0.4601 (0.4601)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [34][64/196]	Time 0.105 (0.100)	Data 0.000 (0.006)	Loss 0.4653 (0.4683)	Acc@1 83.984 (84.081)	Acc@5 99.609 (99.333)
Epoch: [34][128/196]	Time 0.102 (0.094)	Data 0.000 (0.003)	Loss 0.3901 (0.4616)	Acc@1 89.062 (84.151)	Acc@5 99.219 (99.294)
Epoch: [34][192/196]	Time 0.085 (0.093)	Data 0.000 (0.002)	Loss 0.5142 (0.4634)	Acc@1 83.203 (84.007)	Acc@5 99.219 (99.316)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:35/35; Lr: 0.1
batch Size 256
Epoch: [35][0/196]	Time 0.153 (0.153)	Data 0.334 (0.334)	Loss 0.4108 (0.4108)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [35][64/196]	Time 0.106 (0.102)	Data 0.000 (0.005)	Loss 0.4053 (0.4519)	Acc@1 86.328 (84.345)	Acc@5 99.219 (99.279)
Epoch: [35][128/196]	Time 0.121 (0.099)	Data 0.000 (0.003)	Loss 0.4123 (0.4605)	Acc@1 88.672 (84.057)	Acc@5 99.219 (99.273)
Epoch: [35][192/196]	Time 0.083 (0.098)	Data 0.000 (0.002)	Loss 0.4683 (0.4569)	Acc@1 85.547 (84.179)	Acc@5 99.219 (99.275)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  77.15
Max memory: 51.4381312
 19.520s  j: 36 bis 40
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2373
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 36
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:36/40; Lr: 0.1
batch Size 256
Epoch: [36][0/196]	Time 0.172 (0.172)	Data 0.387 (0.387)	Loss 0.5004 (0.5004)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [36][64/196]	Time 0.086 (0.099)	Data 0.000 (0.006)	Loss 0.3659 (0.4265)	Acc@1 85.938 (85.343)	Acc@5 99.609 (99.393)
Epoch: [36][128/196]	Time 0.084 (0.093)	Data 0.000 (0.003)	Loss 0.5267 (0.4398)	Acc@1 82.031 (84.941)	Acc@5 99.609 (99.379)
Epoch: [36][192/196]	Time 0.103 (0.092)	Data 0.000 (0.002)	Loss 0.4412 (0.4465)	Acc@1 83.984 (84.626)	Acc@5 99.609 (99.348)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:37/40; Lr: 0.1
batch Size 256
Epoch: [37][0/196]	Time 0.167 (0.167)	Data 0.372 (0.372)	Loss 0.4920 (0.4920)	Acc@1 81.250 (81.250)	Acc@5 98.828 (98.828)
Epoch: [37][64/196]	Time 0.097 (0.094)	Data 0.000 (0.006)	Loss 0.3457 (0.4463)	Acc@1 87.109 (84.543)	Acc@5 99.609 (99.321)
Epoch: [37][128/196]	Time 0.092 (0.095)	Data 0.000 (0.003)	Loss 0.4913 (0.4534)	Acc@1 83.203 (84.287)	Acc@5 99.609 (99.367)
Epoch: [37][192/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 0.5320 (0.4522)	Acc@1 83.203 (84.375)	Acc@5 98.047 (99.320)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:38/40; Lr: 0.1
batch Size 256
Epoch: [38][0/196]	Time 0.132 (0.132)	Data 0.320 (0.320)	Loss 0.4037 (0.4037)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [38][64/196]	Time 0.109 (0.103)	Data 0.000 (0.005)	Loss 0.4932 (0.4608)	Acc@1 82.031 (84.105)	Acc@5 99.609 (99.381)
Epoch: [38][128/196]	Time 0.098 (0.101)	Data 0.000 (0.003)	Loss 0.3967 (0.4647)	Acc@1 86.328 (84.009)	Acc@5 99.609 (99.361)
Epoch: [38][192/196]	Time 0.080 (0.098)	Data 0.000 (0.002)	Loss 0.4792 (0.4605)	Acc@1 84.375 (84.173)	Acc@5 100.000 (99.344)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:39/40; Lr: 0.1
batch Size 256
Epoch: [39][0/196]	Time 0.120 (0.120)	Data 0.306 (0.306)	Loss 0.4809 (0.4809)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [39][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.5589 (0.4463)	Acc@1 80.859 (84.585)	Acc@5 99.609 (99.369)
Epoch: [39][128/196]	Time 0.103 (0.093)	Data 0.000 (0.003)	Loss 0.4282 (0.4529)	Acc@1 86.719 (84.320)	Acc@5 99.219 (99.316)
Epoch: [39][192/196]	Time 0.095 (0.095)	Data 0.000 (0.002)	Loss 0.4741 (0.4517)	Acc@1 83.203 (84.391)	Acc@5 99.609 (99.352)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:40/40; Lr: 0.1
batch Size 256
Epoch: [40][0/196]	Time 0.144 (0.144)	Data 0.382 (0.382)	Loss 0.4874 (0.4874)	Acc@1 84.766 (84.766)	Acc@5 98.828 (98.828)
Epoch: [40][64/196]	Time 0.111 (0.098)	Data 0.000 (0.006)	Loss 0.5294 (0.4433)	Acc@1 83.203 (84.730)	Acc@5 99.219 (99.333)
Epoch: [40][128/196]	Time 0.098 (0.098)	Data 0.000 (0.003)	Loss 0.3759 (0.4486)	Acc@1 86.719 (84.526)	Acc@5 99.609 (99.279)
Epoch: [40][192/196]	Time 0.087 (0.098)	Data 0.000 (0.002)	Loss 0.5122 (0.4513)	Acc@1 81.250 (84.496)	Acc@5 98.438 (99.279)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  78.64
Max memory: 51.4381312
 19.598s  j: 41 bis 45
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7050
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 41
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:41/45; Lr: 0.1
batch Size 256
Epoch: [41][0/196]	Time 0.138 (0.138)	Data 0.299 (0.299)	Loss 0.3624 (0.3624)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [41][64/196]	Time 0.082 (0.091)	Data 0.000 (0.005)	Loss 0.4625 (0.4264)	Acc@1 83.594 (85.174)	Acc@5 98.828 (99.399)
Epoch: [41][128/196]	Time 0.094 (0.093)	Data 0.000 (0.003)	Loss 0.5276 (0.4336)	Acc@1 83.203 (85.132)	Acc@5 99.609 (99.425)
Epoch: [41][192/196]	Time 0.092 (0.093)	Data 0.000 (0.002)	Loss 0.5125 (0.4367)	Acc@1 84.375 (84.940)	Acc@5 98.828 (99.449)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:42/45; Lr: 0.1
batch Size 256
Epoch: [42][0/196]	Time 0.150 (0.150)	Data 0.350 (0.350)	Loss 0.3329 (0.3329)	Acc@1 90.234 (90.234)	Acc@5 98.438 (98.438)
Epoch: [42][64/196]	Time 0.105 (0.101)	Data 0.000 (0.006)	Loss 0.5090 (0.4262)	Acc@1 81.641 (85.337)	Acc@5 98.438 (99.393)
Epoch: [42][128/196]	Time 0.092 (0.099)	Data 0.000 (0.003)	Loss 0.4623 (0.4375)	Acc@1 85.547 (84.965)	Acc@5 99.219 (99.373)
Epoch: [42][192/196]	Time 0.098 (0.098)	Data 0.000 (0.002)	Loss 0.4050 (0.4403)	Acc@1 85.938 (84.756)	Acc@5 99.609 (99.344)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:43/45; Lr: 0.1
batch Size 256
Epoch: [43][0/196]	Time 0.136 (0.136)	Data 0.367 (0.367)	Loss 0.4672 (0.4672)	Acc@1 85.156 (85.156)	Acc@5 98.047 (98.047)
Epoch: [43][64/196]	Time 0.091 (0.100)	Data 0.000 (0.006)	Loss 0.3987 (0.4343)	Acc@1 86.328 (84.970)	Acc@5 99.609 (99.309)
Epoch: [43][128/196]	Time 0.081 (0.095)	Data 0.000 (0.003)	Loss 0.4919 (0.4396)	Acc@1 83.203 (84.887)	Acc@5 99.609 (99.319)
Epoch: [43][192/196]	Time 0.090 (0.093)	Data 0.000 (0.002)	Loss 0.4628 (0.4408)	Acc@1 85.938 (84.758)	Acc@5 98.828 (99.344)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:44/45; Lr: 0.1
batch Size 256
Epoch: [44][0/196]	Time 0.145 (0.145)	Data 0.376 (0.376)	Loss 0.4477 (0.4477)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [44][64/196]	Time 0.094 (0.101)	Data 0.000 (0.006)	Loss 0.3127 (0.4440)	Acc@1 91.406 (84.525)	Acc@5 100.000 (99.357)
Epoch: [44][128/196]	Time 0.086 (0.100)	Data 0.000 (0.003)	Loss 0.4412 (0.4394)	Acc@1 85.938 (84.859)	Acc@5 98.438 (99.319)
Epoch: [44][192/196]	Time 0.107 (0.099)	Data 0.000 (0.002)	Loss 0.3705 (0.4427)	Acc@1 87.109 (84.709)	Acc@5 98.828 (99.354)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:45/45; Lr: 0.1
batch Size 256
Epoch: [45][0/196]	Time 0.130 (0.130)	Data 0.304 (0.304)	Loss 0.4126 (0.4126)	Acc@1 86.328 (86.328)	Acc@5 99.609 (99.609)
Epoch: [45][64/196]	Time 0.094 (0.099)	Data 0.000 (0.005)	Loss 0.3579 (0.4230)	Acc@1 85.938 (85.186)	Acc@5 100.000 (99.519)
Epoch: [45][128/196]	Time 0.108 (0.099)	Data 0.000 (0.003)	Loss 0.3591 (0.4363)	Acc@1 87.891 (84.941)	Acc@5 99.609 (99.413)
Epoch: [45][192/196]	Time 0.085 (0.098)	Data 0.000 (0.002)	Loss 0.4204 (0.4388)	Acc@1 84.766 (84.841)	Acc@5 99.609 (99.395)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  79.19
Max memory: 51.4381312
 19.484s  j: 46 bis 50
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6924
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 46
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:46/50; Lr: 0.1
batch Size 256
Epoch: [46][0/196]	Time 0.168 (0.168)	Data 0.297 (0.297)	Loss 0.4715 (0.4715)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [46][64/196]	Time 0.099 (0.093)	Data 0.000 (0.005)	Loss 0.4389 (0.4122)	Acc@1 83.984 (85.715)	Acc@5 99.609 (99.459)
Epoch: [46][128/196]	Time 0.106 (0.095)	Data 0.000 (0.002)	Loss 0.4462 (0.4262)	Acc@1 84.766 (85.326)	Acc@5 99.609 (99.416)
Epoch: [46][192/196]	Time 0.087 (0.095)	Data 0.000 (0.002)	Loss 0.5189 (0.4358)	Acc@1 83.594 (85.069)	Acc@5 98.828 (99.336)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:47/50; Lr: 0.1
batch Size 256
Epoch: [47][0/196]	Time 0.159 (0.159)	Data 0.317 (0.317)	Loss 0.5673 (0.5673)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [47][64/196]	Time 0.100 (0.101)	Data 0.000 (0.005)	Loss 0.5410 (0.4343)	Acc@1 79.688 (84.946)	Acc@5 98.828 (99.399)
Epoch: [47][128/196]	Time 0.091 (0.100)	Data 0.000 (0.003)	Loss 0.4805 (0.4327)	Acc@1 81.641 (85.008)	Acc@5 100.000 (99.376)
Epoch: [47][192/196]	Time 0.078 (0.098)	Data 0.000 (0.002)	Loss 0.5747 (0.4383)	Acc@1 78.125 (84.845)	Acc@5 99.609 (99.366)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:48/50; Lr: 0.1
batch Size 256
Epoch: [48][0/196]	Time 0.128 (0.128)	Data 0.298 (0.298)	Loss 0.5099 (0.5099)	Acc@1 83.594 (83.594)	Acc@5 99.609 (99.609)
Epoch: [48][64/196]	Time 0.079 (0.089)	Data 0.000 (0.005)	Loss 0.5401 (0.4408)	Acc@1 82.812 (85.024)	Acc@5 98.438 (99.381)
Epoch: [48][128/196]	Time 0.099 (0.093)	Data 0.000 (0.003)	Loss 0.5759 (0.4367)	Acc@1 79.297 (84.941)	Acc@5 98.047 (99.394)
Epoch: [48][192/196]	Time 0.089 (0.095)	Data 0.000 (0.002)	Loss 0.5847 (0.4395)	Acc@1 78.906 (84.960)	Acc@5 99.609 (99.381)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:49/50; Lr: 0.1
batch Size 256
Epoch: [49][0/196]	Time 0.138 (0.138)	Data 0.361 (0.361)	Loss 0.4103 (0.4103)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [49][64/196]	Time 0.083 (0.100)	Data 0.000 (0.006)	Loss 0.3571 (0.4107)	Acc@1 87.891 (86.070)	Acc@5 99.219 (99.441)
Epoch: [49][128/196]	Time 0.101 (0.098)	Data 0.000 (0.003)	Loss 0.4651 (0.4281)	Acc@1 84.766 (85.329)	Acc@5 99.219 (99.376)
Epoch: [49][192/196]	Time 0.094 (0.098)	Data 0.000 (0.002)	Loss 0.4173 (0.4318)	Acc@1 85.547 (85.146)	Acc@5 99.219 (99.375)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:50/50; Lr: 0.1
batch Size 256
Epoch: [50][0/196]	Time 0.162 (0.162)	Data 0.285 (0.285)	Loss 0.4503 (0.4503)	Acc@1 84.766 (84.766)	Acc@5 98.047 (98.047)
Epoch: [50][64/196]	Time 0.102 (0.100)	Data 0.000 (0.005)	Loss 0.5226 (0.4290)	Acc@1 84.375 (85.547)	Acc@5 99.609 (99.297)
Epoch: [50][128/196]	Time 0.089 (0.094)	Data 0.000 (0.003)	Loss 0.4461 (0.4329)	Acc@1 83.984 (85.180)	Acc@5 99.609 (99.331)
Epoch: [50][192/196]	Time 0.094 (0.092)	Data 0.000 (0.002)	Loss 0.3268 (0.4327)	Acc@1 87.500 (85.203)	Acc@5 99.609 (99.364)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  75.75
Max memory: 51.4381312
 18.504s  j: 51 bis 55
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1577
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 51
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:51/55; Lr: 0.1
batch Size 256
Epoch: [51][0/196]	Time 0.182 (0.182)	Data 0.341 (0.341)	Loss 0.3460 (0.3460)	Acc@1 87.891 (87.891)	Acc@5 100.000 (100.000)
Epoch: [51][64/196]	Time 0.096 (0.099)	Data 0.000 (0.005)	Loss 0.4274 (0.4192)	Acc@1 85.547 (85.703)	Acc@5 99.219 (99.459)
Epoch: [51][128/196]	Time 0.090 (0.098)	Data 0.000 (0.003)	Loss 0.4543 (0.4205)	Acc@1 83.984 (85.656)	Acc@5 99.219 (99.434)
Epoch: [51][192/196]	Time 0.085 (0.097)	Data 0.000 (0.002)	Loss 0.3447 (0.4218)	Acc@1 87.891 (85.490)	Acc@5 100.000 (99.415)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:52/55; Lr: 0.1
batch Size 256
Epoch: [52][0/196]	Time 0.153 (0.153)	Data 0.315 (0.315)	Loss 0.4504 (0.4504)	Acc@1 87.500 (87.500)	Acc@5 98.047 (98.047)
Epoch: [52][64/196]	Time 0.096 (0.102)	Data 0.000 (0.005)	Loss 0.4404 (0.4388)	Acc@1 86.328 (84.742)	Acc@5 98.828 (99.345)
Epoch: [52][128/196]	Time 0.092 (0.096)	Data 0.000 (0.003)	Loss 0.3876 (0.4287)	Acc@1 87.891 (85.174)	Acc@5 99.609 (99.376)
Epoch: [52][192/196]	Time 0.089 (0.094)	Data 0.000 (0.002)	Loss 0.4423 (0.4309)	Acc@1 86.328 (85.132)	Acc@5 100.000 (99.334)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:53/55; Lr: 0.1
batch Size 256
Epoch: [53][0/196]	Time 0.151 (0.151)	Data 0.361 (0.361)	Loss 0.4318 (0.4318)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [53][64/196]	Time 0.095 (0.101)	Data 0.000 (0.006)	Loss 0.4534 (0.4341)	Acc@1 83.594 (84.856)	Acc@5 98.828 (99.399)
Epoch: [53][128/196]	Time 0.101 (0.100)	Data 0.000 (0.003)	Loss 0.5186 (0.4360)	Acc@1 81.641 (84.811)	Acc@5 98.438 (99.334)
Epoch: [53][192/196]	Time 0.094 (0.099)	Data 0.000 (0.002)	Loss 0.3935 (0.4347)	Acc@1 85.547 (84.836)	Acc@5 99.609 (99.354)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:54/55; Lr: 0.1
batch Size 256
Epoch: [54][0/196]	Time 0.115 (0.115)	Data 0.316 (0.316)	Loss 0.4150 (0.4150)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [54][64/196]	Time 0.099 (0.100)	Data 0.000 (0.005)	Loss 0.4126 (0.4219)	Acc@1 84.766 (85.421)	Acc@5 98.828 (99.381)
Epoch: [54][128/196]	Time 0.107 (0.098)	Data 0.000 (0.003)	Loss 0.4155 (0.4286)	Acc@1 85.938 (85.220)	Acc@5 99.609 (99.379)
Epoch: [54][192/196]	Time 0.113 (0.097)	Data 0.000 (0.002)	Loss 0.3751 (0.4295)	Acc@1 87.500 (85.237)	Acc@5 100.000 (99.395)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:55/55; Lr: 0.1
batch Size 256
Epoch: [55][0/196]	Time 0.118 (0.118)	Data 0.268 (0.268)	Loss 0.4216 (0.4216)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [55][64/196]	Time 0.089 (0.088)	Data 0.000 (0.004)	Loss 0.4803 (0.4252)	Acc@1 81.250 (85.331)	Acc@5 99.219 (99.459)
Epoch: [55][128/196]	Time 0.092 (0.090)	Data 0.000 (0.002)	Loss 0.3768 (0.4216)	Acc@1 85.938 (85.411)	Acc@5 99.609 (99.452)
Epoch: [55][192/196]	Time 0.077 (0.091)	Data 0.000 (0.002)	Loss 0.5370 (0.4256)	Acc@1 82.422 (85.298)	Acc@5 97.656 (99.443)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  81.94
Max memory: 51.4381312
 18.163s  j: 56 bis 60
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3884
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 56
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:56/60; Lr: 0.1
batch Size 256
Epoch: [56][0/196]	Time 0.188 (0.188)	Data 0.317 (0.317)	Loss 0.4155 (0.4155)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [56][64/196]	Time 0.099 (0.099)	Data 0.000 (0.005)	Loss 0.4035 (0.4081)	Acc@1 85.547 (85.763)	Acc@5 99.609 (99.417)
Epoch: [56][128/196]	Time 0.101 (0.099)	Data 0.000 (0.003)	Loss 0.5128 (0.4183)	Acc@1 83.594 (85.559)	Acc@5 98.047 (99.361)
Epoch: [56][192/196]	Time 0.089 (0.098)	Data 0.000 (0.002)	Loss 0.5568 (0.4197)	Acc@1 81.250 (85.496)	Acc@5 99.219 (99.391)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:57/60; Lr: 0.1
batch Size 256
Epoch: [57][0/196]	Time 0.144 (0.144)	Data 0.240 (0.240)	Loss 0.3698 (0.3698)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [57][64/196]	Time 0.086 (0.090)	Data 0.000 (0.004)	Loss 0.3912 (0.4219)	Acc@1 87.109 (85.499)	Acc@5 99.609 (99.351)
Epoch: [57][128/196]	Time 0.102 (0.093)	Data 0.000 (0.002)	Loss 0.4779 (0.4279)	Acc@1 82.812 (85.205)	Acc@5 98.828 (99.419)
Epoch: [57][192/196]	Time 0.093 (0.095)	Data 0.000 (0.001)	Loss 0.5205 (0.4239)	Acc@1 80.078 (85.434)	Acc@5 99.219 (99.405)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:58/60; Lr: 0.1
batch Size 256
Epoch: [58][0/196]	Time 0.158 (0.158)	Data 0.352 (0.352)	Loss 0.4636 (0.4636)	Acc@1 83.984 (83.984)	Acc@5 98.438 (98.438)
Epoch: [58][64/196]	Time 0.100 (0.101)	Data 0.000 (0.006)	Loss 0.4769 (0.4265)	Acc@1 80.078 (85.252)	Acc@5 100.000 (99.399)
Epoch: [58][128/196]	Time 0.085 (0.098)	Data 0.000 (0.003)	Loss 0.5232 (0.4299)	Acc@1 83.984 (85.299)	Acc@5 99.609 (99.358)
Epoch: [58][192/196]	Time 0.096 (0.098)	Data 0.000 (0.002)	Loss 0.4143 (0.4292)	Acc@1 85.547 (85.344)	Acc@5 99.219 (99.387)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:59/60; Lr: 0.1
batch Size 256
Epoch: [59][0/196]	Time 0.136 (0.136)	Data 0.340 (0.340)	Loss 0.4579 (0.4579)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [59][64/196]	Time 0.094 (0.102)	Data 0.000 (0.005)	Loss 0.3880 (0.4171)	Acc@1 88.672 (85.733)	Acc@5 99.219 (99.339)
Epoch: [59][128/196]	Time 0.094 (0.097)	Data 0.000 (0.003)	Loss 0.4124 (0.4234)	Acc@1 85.547 (85.344)	Acc@5 99.609 (99.394)
Epoch: [59][192/196]	Time 0.107 (0.094)	Data 0.000 (0.002)	Loss 0.5030 (0.4274)	Acc@1 81.250 (85.193)	Acc@5 98.438 (99.419)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:60/60; Lr: 0.1
batch Size 256
Epoch: [60][0/196]	Time 0.144 (0.144)	Data 0.237 (0.237)	Loss 0.4406 (0.4406)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [60][64/196]	Time 0.092 (0.098)	Data 0.000 (0.004)	Loss 0.4417 (0.4041)	Acc@1 84.375 (85.998)	Acc@5 99.609 (99.555)
Epoch: [60][128/196]	Time 0.102 (0.100)	Data 0.000 (0.002)	Loss 0.4041 (0.4149)	Acc@1 86.719 (85.556)	Acc@5 99.219 (99.416)
Epoch: [60][192/196]	Time 0.094 (0.100)	Data 0.000 (0.001)	Loss 0.5172 (0.4192)	Acc@1 84.375 (85.486)	Acc@5 99.219 (99.405)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  77.2
Max memory: 51.4381312
 19.824s  j: 61 bis 65
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2940
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 61
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:61/65; Lr: 0.1
batch Size 256
Epoch: [61][0/196]	Time 0.203 (0.203)	Data 0.314 (0.314)	Loss 0.4212 (0.4212)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [61][64/196]	Time 0.099 (0.102)	Data 0.000 (0.005)	Loss 0.4405 (0.4050)	Acc@1 84.375 (85.793)	Acc@5 98.828 (99.513)
Epoch: [61][128/196]	Time 0.085 (0.096)	Data 0.000 (0.003)	Loss 0.4299 (0.4122)	Acc@1 82.422 (85.610)	Acc@5 100.000 (99.455)
Epoch: [61][192/196]	Time 0.108 (0.094)	Data 0.000 (0.002)	Loss 0.3726 (0.4164)	Acc@1 85.938 (85.508)	Acc@5 99.609 (99.460)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:62/65; Lr: 0.1
batch Size 256
Epoch: [62][0/196]	Time 0.155 (0.155)	Data 0.369 (0.369)	Loss 0.4819 (0.4819)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [62][64/196]	Time 0.098 (0.103)	Data 0.000 (0.006)	Loss 0.4701 (0.4298)	Acc@1 83.203 (85.216)	Acc@5 98.828 (99.471)
Epoch: [62][128/196]	Time 0.097 (0.100)	Data 0.000 (0.003)	Loss 0.3482 (0.4267)	Acc@1 86.719 (85.238)	Acc@5 99.219 (99.413)
Epoch: [62][192/196]	Time 0.093 (0.100)	Data 0.000 (0.002)	Loss 0.4528 (0.4266)	Acc@1 81.250 (85.272)	Acc@5 99.609 (99.403)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:63/65; Lr: 0.1
batch Size 256
Epoch: [63][0/196]	Time 0.172 (0.172)	Data 0.318 (0.318)	Loss 0.3999 (0.3999)	Acc@1 85.938 (85.938)	Acc@5 98.828 (98.828)
Epoch: [63][64/196]	Time 0.104 (0.102)	Data 0.000 (0.005)	Loss 0.3813 (0.4138)	Acc@1 86.719 (85.841)	Acc@5 98.828 (99.501)
Epoch: [63][128/196]	Time 0.087 (0.101)	Data 0.000 (0.003)	Loss 0.3991 (0.4128)	Acc@1 83.594 (85.786)	Acc@5 99.609 (99.446)
Epoch: [63][192/196]	Time 0.087 (0.100)	Data 0.000 (0.002)	Loss 0.4817 (0.4186)	Acc@1 84.766 (85.614)	Acc@5 99.219 (99.407)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:64/65; Lr: 0.1
batch Size 256
Epoch: [64][0/196]	Time 0.116 (0.116)	Data 0.256 (0.256)	Loss 0.4168 (0.4168)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [64][64/196]	Time 0.085 (0.087)	Data 0.000 (0.004)	Loss 0.5057 (0.4171)	Acc@1 82.812 (85.655)	Acc@5 99.609 (99.435)
Epoch: [64][128/196]	Time 0.100 (0.091)	Data 0.000 (0.002)	Loss 0.3598 (0.4199)	Acc@1 88.672 (85.508)	Acc@5 99.609 (99.391)
Epoch: [64][192/196]	Time 0.091 (0.094)	Data 0.000 (0.002)	Loss 0.3845 (0.4173)	Acc@1 86.328 (85.559)	Acc@5 99.609 (99.423)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:65/65; Lr: 0.1
batch Size 256
Epoch: [65][0/196]	Time 0.149 (0.149)	Data 0.347 (0.347)	Loss 0.2973 (0.2973)	Acc@1 90.625 (90.625)	Acc@5 99.219 (99.219)
Epoch: [65][64/196]	Time 0.085 (0.101)	Data 0.000 (0.006)	Loss 0.3889 (0.4260)	Acc@1 85.547 (85.162)	Acc@5 100.000 (99.357)
Epoch: [65][128/196]	Time 0.092 (0.099)	Data 0.000 (0.003)	Loss 0.4319 (0.4175)	Acc@1 85.938 (85.520)	Acc@5 99.219 (99.419)
Epoch: [65][192/196]	Time 0.091 (0.099)	Data 0.000 (0.002)	Loss 0.4405 (0.4230)	Acc@1 83.203 (85.237)	Acc@5 97.656 (99.389)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  79.44
Max memory: 51.4381312
 19.814s  j: 66 bis 70
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 9918
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 66
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:66/70; Lr: 0.1
batch Size 256
Epoch: [66][0/196]	Time 0.173 (0.173)	Data 0.284 (0.284)	Loss 0.3366 (0.3366)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [66][64/196]	Time 0.087 (0.091)	Data 0.000 (0.005)	Loss 0.4366 (0.3927)	Acc@1 87.109 (86.526)	Acc@5 100.000 (99.399)
Epoch: [66][128/196]	Time 0.089 (0.094)	Data 0.000 (0.002)	Loss 0.3455 (0.4031)	Acc@1 87.109 (86.101)	Acc@5 100.000 (99.422)
Epoch: [66][192/196]	Time 0.091 (0.095)	Data 0.000 (0.002)	Loss 0.4113 (0.4101)	Acc@1 83.203 (85.875)	Acc@5 99.609 (99.433)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:67/70; Lr: 0.1
batch Size 256
Epoch: [67][0/196]	Time 0.162 (0.162)	Data 0.354 (0.354)	Loss 0.4977 (0.4977)	Acc@1 82.422 (82.422)	Acc@5 99.609 (99.609)
Epoch: [67][64/196]	Time 0.099 (0.099)	Data 0.000 (0.006)	Loss 0.4840 (0.4154)	Acc@1 83.203 (85.493)	Acc@5 99.609 (99.513)
Epoch: [67][128/196]	Time 0.098 (0.098)	Data 0.000 (0.003)	Loss 0.5310 (0.4192)	Acc@1 81.250 (85.471)	Acc@5 98.828 (99.497)
Epoch: [67][192/196]	Time 0.105 (0.097)	Data 0.000 (0.002)	Loss 0.5132 (0.4194)	Acc@1 81.641 (85.383)	Acc@5 99.219 (99.447)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:68/70; Lr: 0.1
batch Size 256
Epoch: [68][0/196]	Time 0.173 (0.173)	Data 0.451 (0.451)	Loss 0.4346 (0.4346)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [68][64/196]	Time 0.088 (0.101)	Data 0.000 (0.007)	Loss 0.3888 (0.4179)	Acc@1 85.547 (85.559)	Acc@5 100.000 (99.435)
Epoch: [68][128/196]	Time 0.084 (0.095)	Data 0.000 (0.004)	Loss 0.4475 (0.4091)	Acc@1 85.156 (85.880)	Acc@5 99.609 (99.488)
Epoch: [68][192/196]	Time 0.087 (0.093)	Data 0.000 (0.003)	Loss 0.4378 (0.4134)	Acc@1 83.984 (85.693)	Acc@5 99.219 (99.470)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:69/70; Lr: 0.1
batch Size 256
Epoch: [69][0/196]	Time 0.165 (0.165)	Data 0.412 (0.412)	Loss 0.4010 (0.4010)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [69][64/196]	Time 0.104 (0.098)	Data 0.000 (0.007)	Loss 0.3998 (0.4090)	Acc@1 83.594 (85.835)	Acc@5 100.000 (99.435)
Epoch: [69][128/196]	Time 0.090 (0.098)	Data 0.000 (0.003)	Loss 0.4179 (0.4198)	Acc@1 84.766 (85.456)	Acc@5 100.000 (99.403)
Epoch: [69][192/196]	Time 0.082 (0.095)	Data 0.000 (0.002)	Loss 0.4391 (0.4191)	Acc@1 82.812 (85.567)	Acc@5 99.609 (99.401)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:70/70; Lr: 0.1
batch Size 256
Epoch: [70][0/196]	Time 0.129 (0.129)	Data 0.299 (0.299)	Loss 0.4059 (0.4059)	Acc@1 87.891 (87.891)	Acc@5 98.047 (98.047)
Epoch: [70][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.3625 (0.4091)	Acc@1 87.500 (86.022)	Acc@5 100.000 (99.483)
Epoch: [70][128/196]	Time 0.098 (0.091)	Data 0.000 (0.003)	Loss 0.5172 (0.4086)	Acc@1 80.859 (86.040)	Acc@5 99.219 (99.416)
Epoch: [70][192/196]	Time 0.095 (0.093)	Data 0.000 (0.002)	Loss 0.4239 (0.4153)	Acc@1 85.938 (85.784)	Acc@5 99.219 (99.395)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  78.57
Max memory: 51.4381312
 18.523s  j: 71 bis 75
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5122
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 71
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:71/75; Lr: 0.1
batch Size 256
Epoch: [71][0/196]	Time 0.171 (0.171)	Data 0.388 (0.388)	Loss 0.4047 (0.4047)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [71][64/196]	Time 0.092 (0.101)	Data 0.000 (0.006)	Loss 0.4256 (0.3863)	Acc@1 88.281 (86.683)	Acc@5 98.438 (99.525)
Epoch: [71][128/196]	Time 0.090 (0.099)	Data 0.000 (0.003)	Loss 0.4027 (0.4007)	Acc@1 85.547 (86.152)	Acc@5 99.609 (99.464)
Epoch: [71][192/196]	Time 0.089 (0.098)	Data 0.000 (0.002)	Loss 0.3891 (0.4069)	Acc@1 85.547 (85.935)	Acc@5 99.609 (99.484)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:72/75; Lr: 0.1
batch Size 256
Epoch: [72][0/196]	Time 0.126 (0.126)	Data 0.283 (0.283)	Loss 0.3638 (0.3638)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [72][64/196]	Time 0.087 (0.089)	Data 0.000 (0.005)	Loss 0.4550 (0.4215)	Acc@1 82.031 (85.727)	Acc@5 99.609 (99.417)
Epoch: [72][128/196]	Time 0.092 (0.090)	Data 0.000 (0.002)	Loss 0.5229 (0.4218)	Acc@1 78.906 (85.692)	Acc@5 99.219 (99.403)
Epoch: [72][192/196]	Time 0.094 (0.093)	Data 0.000 (0.002)	Loss 0.4484 (0.4201)	Acc@1 85.547 (85.713)	Acc@5 99.609 (99.385)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:73/75; Lr: 0.1
batch Size 256
Epoch: [73][0/196]	Time 0.141 (0.141)	Data 0.295 (0.295)	Loss 0.4219 (0.4219)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [73][64/196]	Time 0.097 (0.097)	Data 0.000 (0.005)	Loss 0.4208 (0.4153)	Acc@1 86.328 (86.010)	Acc@5 99.609 (99.471)
Epoch: [73][128/196]	Time 0.095 (0.097)	Data 0.000 (0.003)	Loss 0.3826 (0.4065)	Acc@1 87.109 (86.134)	Acc@5 99.219 (99.503)
Epoch: [73][192/196]	Time 0.092 (0.096)	Data 0.000 (0.002)	Loss 0.4334 (0.4108)	Acc@1 85.938 (85.929)	Acc@5 99.609 (99.449)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:74/75; Lr: 0.1
batch Size 256
Epoch: [74][0/196]	Time 0.140 (0.140)	Data 0.336 (0.336)	Loss 0.4051 (0.4051)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [74][64/196]	Time 0.104 (0.100)	Data 0.000 (0.005)	Loss 0.4225 (0.4046)	Acc@1 82.812 (85.980)	Acc@5 99.609 (99.351)
Epoch: [74][128/196]	Time 0.083 (0.097)	Data 0.000 (0.003)	Loss 0.4462 (0.4084)	Acc@1 83.203 (85.756)	Acc@5 99.219 (99.419)
Epoch: [74][192/196]	Time 0.088 (0.094)	Data 0.000 (0.002)	Loss 0.4066 (0.4148)	Acc@1 86.328 (85.689)	Acc@5 99.609 (99.369)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:75/75; Lr: 0.1
batch Size 256
Epoch: [75][0/196]	Time 0.165 (0.165)	Data 0.323 (0.323)	Loss 0.3746 (0.3746)	Acc@1 88.672 (88.672)	Acc@5 99.219 (99.219)
Epoch: [75][64/196]	Time 0.087 (0.100)	Data 0.000 (0.005)	Loss 0.4558 (0.4049)	Acc@1 83.984 (86.130)	Acc@5 100.000 (99.459)
Epoch: [75][128/196]	Time 0.104 (0.099)	Data 0.000 (0.003)	Loss 0.3440 (0.4108)	Acc@1 87.109 (85.835)	Acc@5 99.609 (99.413)
Epoch: [75][192/196]	Time 0.087 (0.098)	Data 0.000 (0.002)	Loss 0.3670 (0.4056)	Acc@1 88.672 (86.023)	Acc@5 99.219 (99.433)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  79.57
Max memory: 51.4381312
 19.612s  j: 76 bis 80
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3508
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 76
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:76/80; Lr: 0.1
batch Size 256
Epoch: [76][0/196]	Time 0.208 (0.208)	Data 0.299 (0.299)	Loss 0.4306 (0.4306)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [76][64/196]	Time 0.098 (0.099)	Data 0.000 (0.005)	Loss 0.3861 (0.3829)	Acc@1 85.547 (86.719)	Acc@5 99.609 (99.465)
Epoch: [76][128/196]	Time 0.097 (0.096)	Data 0.000 (0.003)	Loss 0.3639 (0.3983)	Acc@1 88.281 (86.243)	Acc@5 100.000 (99.385)
Epoch: [76][192/196]	Time 0.089 (0.094)	Data 0.000 (0.002)	Loss 0.4127 (0.4058)	Acc@1 87.500 (85.942)	Acc@5 99.219 (99.395)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:77/80; Lr: 0.1
batch Size 256
Epoch: [77][0/196]	Time 0.129 (0.129)	Data 0.374 (0.374)	Loss 0.3738 (0.3738)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [77][64/196]	Time 0.085 (0.099)	Data 0.000 (0.006)	Loss 0.4228 (0.4099)	Acc@1 84.766 (85.841)	Acc@5 99.609 (99.459)
Epoch: [77][128/196]	Time 0.097 (0.097)	Data 0.000 (0.003)	Loss 0.4664 (0.4123)	Acc@1 82.812 (85.838)	Acc@5 99.219 (99.434)
Epoch: [77][192/196]	Time 0.085 (0.097)	Data 0.000 (0.002)	Loss 0.3254 (0.4162)	Acc@1 88.672 (85.668)	Acc@5 100.000 (99.484)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:78/80; Lr: 0.1
batch Size 256
Epoch: [78][0/196]	Time 0.150 (0.150)	Data 0.399 (0.399)	Loss 0.3782 (0.3782)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [78][64/196]	Time 0.084 (0.100)	Data 0.000 (0.006)	Loss 0.4705 (0.4224)	Acc@1 83.203 (85.355)	Acc@5 99.219 (99.339)
Epoch: [78][128/196]	Time 0.092 (0.098)	Data 0.000 (0.003)	Loss 0.4061 (0.4216)	Acc@1 85.156 (85.377)	Acc@5 100.000 (99.337)
Epoch: [78][192/196]	Time 0.091 (0.098)	Data 0.000 (0.002)	Loss 0.4416 (0.4162)	Acc@1 85.156 (85.486)	Acc@5 99.219 (99.373)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:79/80; Lr: 0.1
batch Size 256
Epoch: [79][0/196]	Time 0.130 (0.130)	Data 0.309 (0.309)	Loss 0.4764 (0.4764)	Acc@1 84.375 (84.375)	Acc@5 98.828 (98.828)
Epoch: [79][64/196]	Time 0.084 (0.091)	Data 0.000 (0.005)	Loss 0.5207 (0.3997)	Acc@1 82.812 (86.238)	Acc@5 99.609 (99.501)
Epoch: [79][128/196]	Time 0.097 (0.092)	Data 0.000 (0.003)	Loss 0.3846 (0.4042)	Acc@1 85.938 (86.080)	Acc@5 99.219 (99.425)
Epoch: [79][192/196]	Time 0.102 (0.095)	Data 0.000 (0.002)	Loss 0.4400 (0.4056)	Acc@1 84.766 (86.031)	Acc@5 98.828 (99.397)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:80/80; Lr: 0.1
batch Size 256
Epoch: [80][0/196]	Time 0.143 (0.143)	Data 0.404 (0.404)	Loss 0.3736 (0.3736)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [80][64/196]	Time 0.093 (0.102)	Data 0.000 (0.006)	Loss 0.5460 (0.4097)	Acc@1 80.859 (85.757)	Acc@5 99.219 (99.489)
Epoch: [80][128/196]	Time 0.089 (0.101)	Data 0.000 (0.003)	Loss 0.4028 (0.4093)	Acc@1 86.719 (85.968)	Acc@5 100.000 (99.479)
Epoch: [80][192/196]	Time 0.080 (0.100)	Data 0.000 (0.002)	Loss 0.3863 (0.4090)	Acc@1 84.766 (85.923)	Acc@5 99.609 (99.470)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  81.57
Max memory: 51.4381312
 20.159s  j: 81 bis 85
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 299
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 81
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:81/85; Lr: 0.1
batch Size 256
Epoch: [81][0/196]	Time 0.139 (0.139)	Data 0.290 (0.290)	Loss 0.3932 (0.3932)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [81][64/196]	Time 0.096 (0.091)	Data 0.000 (0.005)	Loss 0.2765 (0.3974)	Acc@1 89.844 (86.262)	Acc@5 100.000 (99.489)
Epoch: [81][128/196]	Time 0.084 (0.093)	Data 0.000 (0.002)	Loss 0.4303 (0.4009)	Acc@1 83.594 (86.186)	Acc@5 99.609 (99.461)
Epoch: [81][192/196]	Time 0.095 (0.092)	Data 0.000 (0.002)	Loss 0.4358 (0.4053)	Acc@1 84.766 (86.095)	Acc@5 99.609 (99.454)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:82/85; Lr: 0.1
batch Size 256
Epoch: [82][0/196]	Time 0.129 (0.129)	Data 0.416 (0.416)	Loss 0.4001 (0.4001)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [82][64/196]	Time 0.110 (0.100)	Data 0.000 (0.007)	Loss 0.3307 (0.4136)	Acc@1 87.500 (85.727)	Acc@5 99.609 (99.429)
Epoch: [82][128/196]	Time 0.101 (0.100)	Data 0.000 (0.003)	Loss 0.3404 (0.4192)	Acc@1 88.281 (85.520)	Acc@5 99.219 (99.440)
Epoch: [82][192/196]	Time 0.095 (0.100)	Data 0.000 (0.002)	Loss 0.3390 (0.4104)	Acc@1 87.109 (85.788)	Acc@5 99.219 (99.468)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:83/85; Lr: 0.1
batch Size 256
Epoch: [83][0/196]	Time 0.164 (0.164)	Data 0.373 (0.373)	Loss 0.4107 (0.4107)	Acc@1 87.891 (87.891)	Acc@5 99.219 (99.219)
Epoch: [83][64/196]	Time 0.099 (0.104)	Data 0.000 (0.006)	Loss 0.3789 (0.4150)	Acc@1 88.281 (85.709)	Acc@5 100.000 (99.507)
Epoch: [83][128/196]	Time 0.084 (0.098)	Data 0.000 (0.003)	Loss 0.4864 (0.4140)	Acc@1 82.812 (85.732)	Acc@5 99.219 (99.464)
Epoch: [83][192/196]	Time 0.082 (0.095)	Data 0.000 (0.002)	Loss 0.4020 (0.4163)	Acc@1 87.891 (85.638)	Acc@5 99.219 (99.437)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:84/85; Lr: 0.1
batch Size 256
Epoch: [84][0/196]	Time 0.136 (0.136)	Data 0.359 (0.359)	Loss 0.4025 (0.4025)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [84][64/196]	Time 0.100 (0.101)	Data 0.000 (0.006)	Loss 0.3516 (0.4009)	Acc@1 88.281 (86.388)	Acc@5 100.000 (99.489)
Epoch: [84][128/196]	Time 0.090 (0.099)	Data 0.000 (0.003)	Loss 0.3952 (0.4040)	Acc@1 85.156 (86.280)	Acc@5 100.000 (99.470)
Epoch: [84][192/196]	Time 0.094 (0.099)	Data 0.000 (0.002)	Loss 0.3228 (0.4102)	Acc@1 89.062 (86.079)	Acc@5 99.609 (99.452)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:85/85; Lr: 0.1
batch Size 256
Epoch: [85][0/196]	Time 0.141 (0.141)	Data 0.390 (0.390)	Loss 0.4036 (0.4036)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [85][64/196]	Time 0.087 (0.099)	Data 0.000 (0.006)	Loss 0.4146 (0.3865)	Acc@1 83.984 (86.857)	Acc@5 99.219 (99.471)
Epoch: [85][128/196]	Time 0.109 (0.098)	Data 0.000 (0.003)	Loss 0.3512 (0.3991)	Acc@1 89.453 (86.280)	Acc@5 100.000 (99.458)
Epoch: [85][192/196]	Time 0.088 (0.098)	Data 0.000 (0.002)	Loss 0.3522 (0.4057)	Acc@1 89.062 (86.037)	Acc@5 100.000 (99.433)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  70.28
Max memory: 51.4381312
 19.750s  j: 86 bis 90
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1362
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 86
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:86/90; Lr: 0.1
batch Size 256
Epoch: [86][0/196]	Time 0.148 (0.148)	Data 0.267 (0.267)	Loss 0.3838 (0.3838)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [86][64/196]	Time 0.087 (0.096)	Data 0.000 (0.004)	Loss 0.4630 (0.3900)	Acc@1 84.766 (86.508)	Acc@5 99.609 (99.513)
Epoch: [86][128/196]	Time 0.098 (0.097)	Data 0.000 (0.002)	Loss 0.2756 (0.3872)	Acc@1 90.234 (86.607)	Acc@5 99.609 (99.525)
Epoch: [86][192/196]	Time 0.093 (0.098)	Data 0.000 (0.002)	Loss 0.3663 (0.4008)	Acc@1 87.500 (86.243)	Acc@5 99.609 (99.452)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:87/90; Lr: 0.1
batch Size 256
Epoch: [87][0/196]	Time 0.147 (0.147)	Data 0.365 (0.365)	Loss 0.4941 (0.4941)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [87][64/196]	Time 0.114 (0.099)	Data 0.000 (0.006)	Loss 0.3824 (0.4063)	Acc@1 88.281 (86.022)	Acc@5 99.609 (99.447)
Epoch: [87][128/196]	Time 0.085 (0.099)	Data 0.000 (0.003)	Loss 0.2785 (0.4001)	Acc@1 91.797 (86.367)	Acc@5 100.000 (99.458)
Epoch: [87][192/196]	Time 0.097 (0.098)	Data 0.000 (0.002)	Loss 0.3110 (0.4058)	Acc@1 90.234 (86.193)	Acc@5 99.609 (99.449)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:88/90; Lr: 0.1
batch Size 256
Epoch: [88][0/196]	Time 0.115 (0.115)	Data 0.282 (0.282)	Loss 0.4699 (0.4699)	Acc@1 83.984 (83.984)	Acc@5 100.000 (100.000)
Epoch: [88][64/196]	Time 0.079 (0.089)	Data 0.000 (0.005)	Loss 0.4388 (0.4080)	Acc@1 84.766 (85.799)	Acc@5 100.000 (99.525)
Epoch: [88][128/196]	Time 0.096 (0.092)	Data 0.000 (0.002)	Loss 0.3773 (0.4120)	Acc@1 87.500 (85.850)	Acc@5 99.609 (99.509)
Epoch: [88][192/196]	Time 0.093 (0.093)	Data 0.000 (0.002)	Loss 0.3842 (0.4096)	Acc@1 87.500 (85.873)	Acc@5 98.828 (99.506)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:89/90; Lr: 0.1
batch Size 256
Epoch: [89][0/196]	Time 0.140 (0.140)	Data 0.423 (0.423)	Loss 0.3898 (0.3898)	Acc@1 85.547 (85.547)	Acc@5 100.000 (100.000)
Epoch: [89][64/196]	Time 0.089 (0.100)	Data 0.000 (0.007)	Loss 0.3274 (0.4125)	Acc@1 90.234 (85.793)	Acc@5 98.828 (99.417)
Epoch: [89][128/196]	Time 0.090 (0.098)	Data 0.000 (0.004)	Loss 0.3438 (0.4055)	Acc@1 86.328 (86.001)	Acc@5 99.609 (99.431)
Epoch: [89][192/196]	Time 0.087 (0.098)	Data 0.000 (0.002)	Loss 0.4019 (0.4092)	Acc@1 84.375 (85.745)	Acc@5 99.609 (99.466)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:90/90; Lr: 0.1
batch Size 256
Epoch: [90][0/196]	Time 0.144 (0.144)	Data 0.292 (0.292)	Loss 0.4276 (0.4276)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [90][64/196]	Time 0.110 (0.100)	Data 0.000 (0.005)	Loss 0.3448 (0.4033)	Acc@1 87.109 (86.016)	Acc@5 99.609 (99.453)
Epoch: [90][128/196]	Time 0.093 (0.096)	Data 0.000 (0.002)	Loss 0.3383 (0.4040)	Acc@1 88.281 (86.025)	Acc@5 99.609 (99.449)
Epoch: [90][192/196]	Time 0.083 (0.093)	Data 0.000 (0.002)	Loss 0.4245 (0.4106)	Acc@1 85.156 (85.834)	Acc@5 100.000 (99.433)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  75.54
Max memory: 51.4381312
 18.544s  j: 91 bis 95
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3698
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 91
Max memory: 0.1097216
lr: 0.1
lr: 0.1
1
Epoche:91/95; Lr: 0.1
batch Size 256
Epoch: [91][0/196]	Time 0.202 (0.202)	Data 0.323 (0.323)	Loss 0.3862 (0.3862)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [91][64/196]	Time 0.095 (0.101)	Data 0.000 (0.005)	Loss 0.3284 (0.3773)	Acc@1 87.500 (87.055)	Acc@5 100.000 (99.525)
Epoch: [91][128/196]	Time 0.085 (0.099)	Data 0.000 (0.003)	Loss 0.4584 (0.3889)	Acc@1 86.328 (86.740)	Acc@5 98.828 (99.488)
Epoch: [91][192/196]	Time 0.098 (0.098)	Data 0.000 (0.002)	Loss 0.4036 (0.3978)	Acc@1 85.938 (86.286)	Acc@5 99.609 (99.478)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:92/95; Lr: 0.1
batch Size 256
Epoch: [92][0/196]	Time 0.124 (0.124)	Data 0.387 (0.387)	Loss 0.3816 (0.3816)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [92][64/196]	Time 0.092 (0.095)	Data 0.000 (0.006)	Loss 0.3976 (0.4111)	Acc@1 87.500 (85.925)	Acc@5 98.828 (99.333)
Epoch: [92][128/196]	Time 0.092 (0.094)	Data 0.000 (0.003)	Loss 0.4530 (0.4066)	Acc@1 82.812 (85.913)	Acc@5 98.828 (99.388)
Epoch: [92][192/196]	Time 0.088 (0.092)	Data 0.000 (0.002)	Loss 0.3418 (0.4060)	Acc@1 87.500 (85.950)	Acc@5 99.609 (99.425)
Max memory in training epoch: 33.3541888
lr: 0.1
lr: 0.1
1
Epoche:93/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [93][0/196]	Time 0.148 (0.148)	Data 0.295 (0.295)	Loss 0.4467 (0.4467)	Acc@1 86.719 (86.719)	Acc@5 98.438 (98.438)
Epoch: [93][64/196]	Time 0.110 (0.101)	Data 0.000 (0.005)	Loss 0.3413 (0.3234)	Acc@1 88.281 (88.954)	Acc@5 99.609 (99.627)
Epoch: [93][128/196]	Time 0.102 (0.099)	Data 0.000 (0.003)	Loss 0.2755 (0.3005)	Acc@1 91.797 (89.759)	Acc@5 99.609 (99.682)
Epoch: [93][192/196]	Time 0.110 (0.098)	Data 0.000 (0.002)	Loss 0.2788 (0.2889)	Acc@1 88.672 (90.164)	Acc@5 99.609 (99.696)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:94/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [94][0/196]	Time 0.141 (0.141)	Data 0.398 (0.398)	Loss 0.3193 (0.3193)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [94][64/196]	Time 0.083 (0.101)	Data 0.000 (0.006)	Loss 0.2389 (0.2504)	Acc@1 92.188 (91.647)	Acc@5 100.000 (99.802)
Epoch: [94][128/196]	Time 0.109 (0.099)	Data 0.000 (0.003)	Loss 0.2379 (0.2530)	Acc@1 91.406 (91.467)	Acc@5 100.000 (99.800)
Epoch: [94][192/196]	Time 0.099 (0.099)	Data 0.000 (0.002)	Loss 0.2575 (0.2510)	Acc@1 91.406 (91.469)	Acc@5 100.000 (99.802)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:95/95; Lr: 0.010000000000000002
batch Size 256
Epoch: [95][0/196]	Time 0.160 (0.160)	Data 0.343 (0.343)	Loss 0.2121 (0.2121)	Acc@1 91.406 (91.406)	Acc@5 99.609 (99.609)
Epoch: [95][64/196]	Time 0.084 (0.089)	Data 0.000 (0.005)	Loss 0.1765 (0.2355)	Acc@1 94.141 (91.875)	Acc@5 100.000 (99.772)
Epoch: [95][128/196]	Time 0.102 (0.089)	Data 0.000 (0.003)	Loss 0.2576 (0.2310)	Acc@1 93.359 (92.085)	Acc@5 100.000 (99.785)
Epoch: [95][192/196]	Time 0.090 (0.090)	Data 0.000 (0.002)	Loss 0.1612 (0.2352)	Acc@1 96.484 (91.973)	Acc@5 100.000 (99.785)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.23
Max memory: 51.4381312
 18.140s  j: 96 bis 100
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3515
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 96
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:96/100; Lr: 0.010000000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'src.n2n.N2N' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.
  warnings.warn(msg, SourceChangeWarning)
Epoch: [96][0/196]	Time 0.173 (0.173)	Data 0.320 (0.320)	Loss 0.1934 (0.1934)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [96][64/196]	Time 0.097 (0.098)	Data 0.000 (0.005)	Loss 0.1841 (0.2213)	Acc@1 94.141 (92.470)	Acc@5 99.609 (99.868)
Epoch: [96][128/196]	Time 0.100 (0.098)	Data 0.000 (0.003)	Loss 0.2513 (0.2232)	Acc@1 91.016 (92.424)	Acc@5 99.609 (99.815)
Epoch: [96][192/196]	Time 0.083 (0.097)	Data 0.000 (0.002)	Loss 0.1661 (0.2258)	Acc@1 94.141 (92.317)	Acc@5 100.000 (99.810)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:97/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [97][0/196]	Time 0.146 (0.146)	Data 0.300 (0.300)	Loss 0.2472 (0.2472)	Acc@1 92.578 (92.578)	Acc@5 99.219 (99.219)
Epoch: [97][64/196]	Time 0.106 (0.099)	Data 0.000 (0.005)	Loss 0.1916 (0.2134)	Acc@1 94.141 (92.782)	Acc@5 100.000 (99.838)
Epoch: [97][128/196]	Time 0.098 (0.099)	Data 0.000 (0.003)	Loss 0.1832 (0.2154)	Acc@1 94.922 (92.723)	Acc@5 100.000 (99.818)
Epoch: [97][192/196]	Time 0.088 (0.098)	Data 0.000 (0.002)	Loss 0.2046 (0.2185)	Acc@1 92.969 (92.552)	Acc@5 99.609 (99.808)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:98/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [98][0/196]	Time 0.127 (0.127)	Data 0.269 (0.269)	Loss 0.1806 (0.1806)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [98][64/196]	Time 0.092 (0.091)	Data 0.000 (0.004)	Loss 0.1586 (0.2172)	Acc@1 94.531 (92.512)	Acc@5 100.000 (99.856)
Epoch: [98][128/196]	Time 0.091 (0.094)	Data 0.000 (0.002)	Loss 0.1931 (0.2155)	Acc@1 93.359 (92.560)	Acc@5 100.000 (99.843)
Epoch: [98][192/196]	Time 0.108 (0.095)	Data 0.000 (0.002)	Loss 0.2520 (0.2150)	Acc@1 92.578 (92.657)	Acc@5 99.609 (99.822)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:99/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [99][0/196]	Time 0.145 (0.145)	Data 0.351 (0.351)	Loss 0.1541 (0.1541)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [99][64/196]	Time 0.102 (0.102)	Data 0.000 (0.006)	Loss 0.2315 (0.2087)	Acc@1 92.969 (92.843)	Acc@5 100.000 (99.784)
Epoch: [99][128/196]	Time 0.087 (0.100)	Data 0.000 (0.003)	Loss 0.2295 (0.2090)	Acc@1 90.234 (92.781)	Acc@5 100.000 (99.803)
Epoch: [99][192/196]	Time 0.095 (0.097)	Data 0.000 (0.002)	Loss 0.3114 (0.2094)	Acc@1 89.062 (92.807)	Acc@5 100.000 (99.832)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:100/100; Lr: 0.010000000000000002
batch Size 256
Epoch: [100][0/196]	Time 0.124 (0.124)	Data 0.358 (0.358)	Loss 0.2655 (0.2655)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [100][64/196]	Time 0.083 (0.100)	Data 0.000 (0.006)	Loss 0.1539 (0.1955)	Acc@1 94.922 (93.462)	Acc@5 100.000 (99.862)
Epoch: [100][128/196]	Time 0.081 (0.095)	Data 0.000 (0.003)	Loss 0.1518 (0.2006)	Acc@1 96.094 (93.202)	Acc@5 100.000 (99.843)
Epoch: [100][192/196]	Time 0.088 (0.094)	Data 0.000 (0.002)	Loss 0.2275 (0.2021)	Acc@1 91.406 (93.125)	Acc@5 100.000 (99.834)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.48
Max memory: 51.4381312
 18.820s  j: 101 bis 105
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 7566
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 101
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:101/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [101][0/196]	Time 0.198 (0.198)	Data 0.319 (0.319)	Loss 0.2198 (0.2198)	Acc@1 92.969 (92.969)	Acc@5 99.609 (99.609)
Epoch: [101][64/196]	Time 0.099 (0.103)	Data 0.000 (0.005)	Loss 0.1719 (0.1994)	Acc@1 94.141 (93.167)	Acc@5 100.000 (99.832)
Epoch: [101][128/196]	Time 0.089 (0.101)	Data 0.000 (0.003)	Loss 0.2049 (0.2004)	Acc@1 92.188 (93.202)	Acc@5 100.000 (99.855)
Epoch: [101][192/196]	Time 0.093 (0.099)	Data 0.000 (0.002)	Loss 0.2688 (0.2003)	Acc@1 89.453 (93.187)	Acc@5 99.609 (99.842)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:102/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [102][0/196]	Time 0.132 (0.132)	Data 0.250 (0.250)	Loss 0.2509 (0.2509)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [102][64/196]	Time 0.098 (0.103)	Data 0.000 (0.004)	Loss 0.1679 (0.1930)	Acc@1 94.141 (93.281)	Acc@5 99.609 (99.862)
Epoch: [102][128/196]	Time 0.103 (0.097)	Data 0.000 (0.002)	Loss 0.1797 (0.1987)	Acc@1 93.750 (93.141)	Acc@5 100.000 (99.849)
Epoch: [102][192/196]	Time 0.082 (0.095)	Data 0.000 (0.002)	Loss 0.2017 (0.1988)	Acc@1 94.531 (93.163)	Acc@5 99.609 (99.848)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:103/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [103][0/196]	Time 0.162 (0.162)	Data 0.308 (0.308)	Loss 0.2056 (0.2056)	Acc@1 93.750 (93.750)	Acc@5 99.609 (99.609)
Epoch: [103][64/196]	Time 0.096 (0.099)	Data 0.000 (0.005)	Loss 0.2122 (0.1920)	Acc@1 93.750 (93.462)	Acc@5 99.609 (99.826)
Epoch: [103][128/196]	Time 0.111 (0.100)	Data 0.000 (0.003)	Loss 0.1969 (0.1935)	Acc@1 94.531 (93.356)	Acc@5 100.000 (99.833)
Epoch: [103][192/196]	Time 0.102 (0.099)	Data 0.000 (0.002)	Loss 0.2290 (0.1946)	Acc@1 92.578 (93.319)	Acc@5 99.609 (99.848)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:104/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [104][0/196]	Time 0.160 (0.160)	Data 0.327 (0.327)	Loss 0.2243 (0.2243)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [104][64/196]	Time 0.097 (0.099)	Data 0.000 (0.005)	Loss 0.1889 (0.1879)	Acc@1 91.016 (93.552)	Acc@5 100.000 (99.892)
Epoch: [104][128/196]	Time 0.096 (0.098)	Data 0.000 (0.003)	Loss 0.1760 (0.1884)	Acc@1 94.531 (93.465)	Acc@5 99.609 (99.885)
Epoch: [104][192/196]	Time 0.101 (0.098)	Data 0.000 (0.002)	Loss 0.2601 (0.1889)	Acc@1 92.188 (93.446)	Acc@5 100.000 (99.883)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:105/105; Lr: 0.010000000000000002
batch Size 256
Epoch: [105][0/196]	Time 0.126 (0.126)	Data 0.289 (0.289)	Loss 0.1998 (0.1998)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [105][64/196]	Time 0.083 (0.090)	Data 0.000 (0.005)	Loss 0.1433 (0.1888)	Acc@1 94.141 (93.413)	Acc@5 99.609 (99.850)
Epoch: [105][128/196]	Time 0.093 (0.092)	Data 0.000 (0.002)	Loss 0.1994 (0.1885)	Acc@1 92.188 (93.514)	Acc@5 100.000 (99.855)
Epoch: [105][192/196]	Time 0.092 (0.093)	Data 0.000 (0.002)	Loss 0.1977 (0.1884)	Acc@1 92.578 (93.481)	Acc@5 100.000 (99.866)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.63
Max memory: 51.4381312
 18.605s  j: 106 bis 110
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 188
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 106
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:106/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [106][0/196]	Time 0.190 (0.190)	Data 0.382 (0.382)	Loss 0.1462 (0.1462)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [106][64/196]	Time 0.100 (0.102)	Data 0.000 (0.006)	Loss 0.2190 (0.1798)	Acc@1 93.359 (93.876)	Acc@5 99.609 (99.886)
Epoch: [106][128/196]	Time 0.102 (0.101)	Data 0.000 (0.003)	Loss 0.2465 (0.1839)	Acc@1 93.750 (93.750)	Acc@5 100.000 (99.876)
Epoch: [106][192/196]	Time 0.084 (0.100)	Data 0.000 (0.002)	Loss 0.2793 (0.1846)	Acc@1 88.672 (93.598)	Acc@5 99.609 (99.875)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:107/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [107][0/196]	Time 0.138 (0.138)	Data 0.289 (0.289)	Loss 0.1962 (0.1962)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [107][64/196]	Time 0.089 (0.092)	Data 0.000 (0.005)	Loss 0.1760 (0.1829)	Acc@1 92.578 (93.906)	Acc@5 100.000 (99.856)
Epoch: [107][128/196]	Time 0.094 (0.094)	Data 0.000 (0.002)	Loss 0.1803 (0.1860)	Acc@1 94.141 (93.762)	Acc@5 100.000 (99.867)
Epoch: [107][192/196]	Time 0.099 (0.096)	Data 0.000 (0.002)	Loss 0.1673 (0.1845)	Acc@1 94.922 (93.768)	Acc@5 99.609 (99.877)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:108/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [108][0/196]	Time 0.152 (0.152)	Data 0.355 (0.355)	Loss 0.2395 (0.2395)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [108][64/196]	Time 0.100 (0.097)	Data 0.000 (0.006)	Loss 0.1771 (0.1796)	Acc@1 94.531 (93.852)	Acc@5 99.609 (99.868)
Epoch: [108][128/196]	Time 0.095 (0.097)	Data 0.000 (0.003)	Loss 0.1845 (0.1778)	Acc@1 93.359 (93.898)	Acc@5 100.000 (99.879)
Epoch: [108][192/196]	Time 0.099 (0.098)	Data 0.000 (0.002)	Loss 0.1760 (0.1819)	Acc@1 93.750 (93.663)	Acc@5 100.000 (99.887)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:109/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [109][0/196]	Time 0.121 (0.121)	Data 0.365 (0.365)	Loss 0.1983 (0.1983)	Acc@1 91.797 (91.797)	Acc@5 99.609 (99.609)
Epoch: [109][64/196]	Time 0.111 (0.105)	Data 0.000 (0.006)	Loss 0.1598 (0.1764)	Acc@1 94.922 (94.026)	Acc@5 100.000 (99.898)
Epoch: [109][128/196]	Time 0.092 (0.099)	Data 0.000 (0.003)	Loss 0.1566 (0.1782)	Acc@1 94.141 (93.923)	Acc@5 100.000 (99.873)
Epoch: [109][192/196]	Time 0.103 (0.096)	Data 0.000 (0.002)	Loss 0.1716 (0.1805)	Acc@1 94.922 (93.807)	Acc@5 100.000 (99.868)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:110/110; Lr: 0.010000000000000002
batch Size 256
Epoch: [110][0/196]	Time 0.156 (0.156)	Data 0.338 (0.338)	Loss 0.1709 (0.1709)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [110][64/196]	Time 0.095 (0.105)	Data 0.000 (0.005)	Loss 0.1723 (0.1738)	Acc@1 94.531 (94.062)	Acc@5 100.000 (99.892)
Epoch: [110][128/196]	Time 0.102 (0.103)	Data 0.000 (0.003)	Loss 0.1959 (0.1758)	Acc@1 94.141 (94.023)	Acc@5 100.000 (99.912)
Epoch: [110][192/196]	Time 0.095 (0.102)	Data 0.000 (0.002)	Loss 0.1717 (0.1781)	Acc@1 94.531 (93.981)	Acc@5 100.000 (99.891)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.28
Max memory: 51.4381312
 20.309s  j: 111 bis 115
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 3965
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 111
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:111/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [111][0/196]	Time 0.181 (0.181)	Data 0.385 (0.385)	Loss 0.2281 (0.2281)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [111][64/196]	Time 0.083 (0.100)	Data 0.000 (0.006)	Loss 0.1960 (0.1673)	Acc@1 92.188 (94.195)	Acc@5 99.609 (99.898)
Epoch: [111][128/196]	Time 0.085 (0.095)	Data 0.000 (0.003)	Loss 0.1621 (0.1683)	Acc@1 94.141 (94.207)	Acc@5 100.000 (99.873)
Epoch: [111][192/196]	Time 0.095 (0.093)	Data 0.000 (0.002)	Loss 0.1139 (0.1710)	Acc@1 96.094 (94.114)	Acc@5 100.000 (99.885)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:112/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [112][0/196]	Time 0.162 (0.162)	Data 0.365 (0.365)	Loss 0.1465 (0.1465)	Acc@1 94.531 (94.531)	Acc@5 99.609 (99.609)
Epoch: [112][64/196]	Time 0.105 (0.096)	Data 0.000 (0.006)	Loss 0.2056 (0.1756)	Acc@1 94.141 (93.960)	Acc@5 100.000 (99.904)
Epoch: [112][128/196]	Time 0.092 (0.093)	Data 0.000 (0.003)	Loss 0.1892 (0.1710)	Acc@1 93.750 (94.020)	Acc@5 99.609 (99.909)
Epoch: [112][192/196]	Time 0.107 (0.094)	Data 0.000 (0.002)	Loss 0.1170 (0.1717)	Acc@1 95.703 (94.048)	Acc@5 100.000 (99.893)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:113/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [113][0/196]	Time 0.156 (0.156)	Data 0.397 (0.397)	Loss 0.1622 (0.1622)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [113][64/196]	Time 0.091 (0.095)	Data 0.000 (0.006)	Loss 0.1693 (0.1669)	Acc@1 94.922 (94.237)	Acc@5 100.000 (99.904)
Epoch: [113][128/196]	Time 0.110 (0.096)	Data 0.000 (0.003)	Loss 0.2073 (0.1668)	Acc@1 91.406 (94.283)	Acc@5 100.000 (99.906)
Epoch: [113][192/196]	Time 0.091 (0.096)	Data 0.000 (0.002)	Loss 0.1166 (0.1718)	Acc@1 96.094 (94.102)	Acc@5 100.000 (99.885)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:114/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [114][0/196]	Time 0.132 (0.132)	Data 0.273 (0.273)	Loss 0.2205 (0.2205)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [114][64/196]	Time 0.089 (0.088)	Data 0.000 (0.004)	Loss 0.1605 (0.1677)	Acc@1 94.531 (94.195)	Acc@5 100.000 (99.904)
Epoch: [114][128/196]	Time 0.097 (0.092)	Data 0.000 (0.002)	Loss 0.1622 (0.1685)	Acc@1 93.359 (94.222)	Acc@5 100.000 (99.900)
Epoch: [114][192/196]	Time 0.107 (0.093)	Data 0.000 (0.002)	Loss 0.1567 (0.1703)	Acc@1 93.750 (94.151)	Acc@5 100.000 (99.897)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:115/115; Lr: 0.010000000000000002
batch Size 256
Epoch: [115][0/196]	Time 0.160 (0.160)	Data 0.374 (0.374)	Loss 0.1460 (0.1460)	Acc@1 93.750 (93.750)	Acc@5 99.609 (99.609)
Epoch: [115][64/196]	Time 0.097 (0.099)	Data 0.000 (0.006)	Loss 0.1712 (0.1607)	Acc@1 92.969 (94.507)	Acc@5 100.000 (99.904)
Epoch: [115][128/196]	Time 0.096 (0.098)	Data 0.000 (0.003)	Loss 0.2434 (0.1673)	Acc@1 91.406 (94.183)	Acc@5 99.609 (99.900)
Epoch: [115][192/196]	Time 0.100 (0.097)	Data 0.000 (0.002)	Loss 0.1405 (0.1683)	Acc@1 95.312 (94.191)	Acc@5 100.000 (99.901)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.63
Max memory: 51.4381312
 19.528s  j: 116 bis 120
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 2232
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 116
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:116/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [116][0/196]	Time 0.142 (0.142)	Data 0.312 (0.312)	Loss 0.1265 (0.1265)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [116][64/196]	Time 0.084 (0.090)	Data 0.000 (0.005)	Loss 0.1623 (0.1553)	Acc@1 95.312 (94.832)	Acc@5 100.000 (99.916)
Epoch: [116][128/196]	Time 0.108 (0.091)	Data 0.000 (0.003)	Loss 0.1829 (0.1617)	Acc@1 94.922 (94.552)	Acc@5 99.609 (99.918)
Epoch: [116][192/196]	Time 0.099 (0.093)	Data 0.000 (0.002)	Loss 0.2667 (0.1648)	Acc@1 91.797 (94.430)	Acc@5 100.000 (99.913)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:117/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [117][0/196]	Time 0.109 (0.109)	Data 0.292 (0.292)	Loss 0.1555 (0.1555)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [117][64/196]	Time 0.089 (0.094)	Data 0.000 (0.005)	Loss 0.1865 (0.1598)	Acc@1 94.141 (94.525)	Acc@5 100.000 (99.910)
Epoch: [117][128/196]	Time 0.097 (0.096)	Data 0.000 (0.002)	Loss 0.1752 (0.1620)	Acc@1 93.750 (94.371)	Acc@5 100.000 (99.924)
Epoch: [117][192/196]	Time 0.094 (0.096)	Data 0.000 (0.002)	Loss 0.2174 (0.1649)	Acc@1 92.969 (94.313)	Acc@5 100.000 (99.921)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:118/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [118][0/196]	Time 0.136 (0.136)	Data 0.394 (0.394)	Loss 0.1836 (0.1836)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [118][64/196]	Time 0.089 (0.097)	Data 0.000 (0.006)	Loss 0.1547 (0.1584)	Acc@1 94.141 (94.525)	Acc@5 100.000 (99.916)
Epoch: [118][128/196]	Time 0.098 (0.096)	Data 0.002 (0.003)	Loss 0.1593 (0.1597)	Acc@1 94.531 (94.528)	Acc@5 100.000 (99.906)
Epoch: [118][192/196]	Time 0.091 (0.093)	Data 0.000 (0.002)	Loss 0.1407 (0.1644)	Acc@1 95.312 (94.303)	Acc@5 99.609 (99.887)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:119/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [119][0/196]	Time 0.127 (0.127)	Data 0.323 (0.323)	Loss 0.1281 (0.1281)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [119][64/196]	Time 0.104 (0.099)	Data 0.000 (0.005)	Loss 0.1573 (0.1569)	Acc@1 94.922 (94.760)	Acc@5 99.609 (99.874)
Epoch: [119][128/196]	Time 0.091 (0.098)	Data 0.000 (0.003)	Loss 0.1425 (0.1657)	Acc@1 94.531 (94.337)	Acc@5 100.000 (99.888)
Epoch: [119][192/196]	Time 0.093 (0.098)	Data 0.000 (0.002)	Loss 0.1887 (0.1651)	Acc@1 92.188 (94.339)	Acc@5 100.000 (99.895)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:120/120; Lr: 0.010000000000000002
batch Size 256
Epoch: [120][0/196]	Time 0.149 (0.149)	Data 0.359 (0.359)	Loss 0.1415 (0.1415)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [120][64/196]	Time 0.108 (0.098)	Data 0.000 (0.006)	Loss 0.1393 (0.1538)	Acc@1 94.531 (94.748)	Acc@5 100.000 (99.952)
Epoch: [120][128/196]	Time 0.093 (0.098)	Data 0.000 (0.003)	Loss 0.2010 (0.1574)	Acc@1 94.531 (94.722)	Acc@5 100.000 (99.939)
Epoch: [120][192/196]	Time 0.098 (0.097)	Data 0.000 (0.002)	Loss 0.1903 (0.1626)	Acc@1 93.359 (94.464)	Acc@5 100.000 (99.919)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.97
Max memory: 51.4381312
 19.481s  j: 121 bis 125
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6557
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 121
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:121/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [121][0/196]	Time 0.144 (0.144)	Data 0.340 (0.340)	Loss 0.1725 (0.1725)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [121][64/196]	Time 0.094 (0.099)	Data 0.000 (0.005)	Loss 0.1317 (0.1517)	Acc@1 95.703 (94.856)	Acc@5 100.000 (99.904)
Epoch: [121][128/196]	Time 0.097 (0.095)	Data 0.000 (0.003)	Loss 0.1372 (0.1630)	Acc@1 95.312 (94.449)	Acc@5 100.000 (99.891)
Epoch: [121][192/196]	Time 0.080 (0.095)	Data 0.000 (0.002)	Loss 0.2518 (0.1657)	Acc@1 91.016 (94.301)	Acc@5 100.000 (99.905)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:122/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [122][0/196]	Time 0.171 (0.171)	Data 0.335 (0.335)	Loss 0.1494 (0.1494)	Acc@1 94.141 (94.141)	Acc@5 99.609 (99.609)
Epoch: [122][64/196]	Time 0.107 (0.100)	Data 0.000 (0.005)	Loss 0.1656 (0.1585)	Acc@1 94.922 (94.489)	Acc@5 100.000 (99.928)
Epoch: [122][128/196]	Time 0.108 (0.100)	Data 0.000 (0.003)	Loss 0.1407 (0.1597)	Acc@1 95.703 (94.459)	Acc@5 100.000 (99.939)
Epoch: [122][192/196]	Time 0.096 (0.099)	Data 0.000 (0.002)	Loss 0.2782 (0.1661)	Acc@1 91.797 (94.222)	Acc@5 98.438 (99.919)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:123/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [123][0/196]	Time 0.185 (0.185)	Data 0.328 (0.328)	Loss 0.1325 (0.1325)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [123][64/196]	Time 0.086 (0.092)	Data 0.000 (0.005)	Loss 0.2066 (0.1562)	Acc@1 93.359 (94.543)	Acc@5 100.000 (99.910)
Epoch: [123][128/196]	Time 0.092 (0.091)	Data 0.000 (0.003)	Loss 0.1648 (0.1590)	Acc@1 94.141 (94.571)	Acc@5 100.000 (99.906)
Epoch: [123][192/196]	Time 0.088 (0.093)	Data 0.000 (0.002)	Loss 0.1535 (0.1610)	Acc@1 95.312 (94.466)	Acc@5 100.000 (99.915)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:124/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [124][0/196]	Time 0.145 (0.145)	Data 0.419 (0.419)	Loss 0.1363 (0.1363)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [124][64/196]	Time 0.117 (0.099)	Data 0.003 (0.007)	Loss 0.1307 (0.1653)	Acc@1 95.312 (94.213)	Acc@5 100.000 (99.874)
Epoch: [124][128/196]	Time 0.078 (0.098)	Data 0.000 (0.004)	Loss 0.2311 (0.1606)	Acc@1 91.406 (94.359)	Acc@5 99.609 (99.888)
Epoch: [124][192/196]	Time 0.094 (0.097)	Data 0.000 (0.002)	Loss 0.1492 (0.1629)	Acc@1 94.922 (94.284)	Acc@5 100.000 (99.903)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:125/125; Lr: 0.010000000000000002
batch Size 256
Epoch: [125][0/196]	Time 0.146 (0.146)	Data 0.358 (0.358)	Loss 0.1481 (0.1481)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [125][64/196]	Time 0.095 (0.099)	Data 0.000 (0.006)	Loss 0.2076 (0.1627)	Acc@1 92.188 (94.417)	Acc@5 100.000 (99.910)
Epoch: [125][128/196]	Time 0.102 (0.098)	Data 0.000 (0.003)	Loss 0.1740 (0.1589)	Acc@1 92.969 (94.416)	Acc@5 100.000 (99.927)
Epoch: [125][192/196]	Time 0.081 (0.094)	Data 0.000 (0.002)	Loss 0.1053 (0.1609)	Acc@1 96.875 (94.345)	Acc@5 100.000 (99.923)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.77
Max memory: 51.4381312
 18.887s  j: 126 bis 130
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1709
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 126
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:126/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [126][0/196]	Time 0.181 (0.181)	Data 0.366 (0.366)	Loss 0.1194 (0.1194)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [126][64/196]	Time 0.092 (0.102)	Data 0.000 (0.006)	Loss 0.1490 (0.1573)	Acc@1 94.531 (94.537)	Acc@5 100.000 (99.922)
Epoch: [126][128/196]	Time 0.098 (0.101)	Data 0.000 (0.003)	Loss 0.1823 (0.1585)	Acc@1 93.750 (94.449)	Acc@5 99.609 (99.915)
Epoch: [126][192/196]	Time 0.092 (0.101)	Data 0.000 (0.002)	Loss 0.1097 (0.1608)	Acc@1 98.438 (94.412)	Acc@5 100.000 (99.917)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:127/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [127][0/196]	Time 0.176 (0.176)	Data 0.343 (0.343)	Loss 0.1018 (0.1018)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [127][64/196]	Time 0.099 (0.100)	Data 0.000 (0.006)	Loss 0.1536 (0.1561)	Acc@1 94.531 (94.645)	Acc@5 100.000 (99.892)
Epoch: [127][128/196]	Time 0.095 (0.099)	Data 0.000 (0.003)	Loss 0.1572 (0.1528)	Acc@1 94.531 (94.758)	Acc@5 100.000 (99.897)
Epoch: [127][192/196]	Time 0.094 (0.095)	Data 0.000 (0.002)	Loss 0.2021 (0.1563)	Acc@1 92.578 (94.570)	Acc@5 100.000 (99.917)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:128/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [128][0/196]	Time 0.133 (0.133)	Data 0.290 (0.290)	Loss 0.1362 (0.1362)	Acc@1 94.531 (94.531)	Acc@5 99.609 (99.609)
Epoch: [128][64/196]	Time 0.106 (0.096)	Data 0.000 (0.005)	Loss 0.1350 (0.1473)	Acc@1 94.922 (94.886)	Acc@5 100.000 (99.904)
Epoch: [128][128/196]	Time 0.083 (0.097)	Data 0.000 (0.002)	Loss 0.1356 (0.1547)	Acc@1 96.094 (94.622)	Acc@5 99.609 (99.912)
Epoch: [128][192/196]	Time 0.095 (0.097)	Data 0.000 (0.002)	Loss 0.1297 (0.1584)	Acc@1 94.531 (94.477)	Acc@5 100.000 (99.909)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:129/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [129][0/196]	Time 0.147 (0.147)	Data 0.359 (0.359)	Loss 0.1827 (0.1827)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [129][64/196]	Time 0.125 (0.102)	Data 0.000 (0.006)	Loss 0.1515 (0.1569)	Acc@1 95.312 (94.369)	Acc@5 100.000 (99.934)
Epoch: [129][128/196]	Time 0.092 (0.100)	Data 0.000 (0.003)	Loss 0.1635 (0.1594)	Acc@1 94.922 (94.271)	Acc@5 100.000 (99.936)
Epoch: [129][192/196]	Time 0.090 (0.100)	Data 0.000 (0.002)	Loss 0.1592 (0.1597)	Acc@1 94.141 (94.327)	Acc@5 100.000 (99.927)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:130/130; Lr: 0.010000000000000002
batch Size 256
Epoch: [130][0/196]	Time 0.128 (0.128)	Data 0.338 (0.338)	Loss 0.1114 (0.1114)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [130][64/196]	Time 0.084 (0.092)	Data 0.000 (0.005)	Loss 0.1619 (0.1509)	Acc@1 94.141 (94.736)	Acc@5 100.000 (99.958)
Epoch: [130][128/196]	Time 0.106 (0.091)	Data 0.000 (0.003)	Loss 0.1326 (0.1565)	Acc@1 94.531 (94.601)	Acc@5 100.000 (99.939)
Epoch: [130][192/196]	Time 0.088 (0.091)	Data 0.000 (0.002)	Loss 0.1992 (0.1600)	Acc@1 92.969 (94.471)	Acc@5 100.000 (99.931)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.57
Max memory: 51.4381312
 18.259s  j: 131 bis 135
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4183
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:131/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [131][0/196]	Time 0.188 (0.188)	Data 0.410 (0.410)	Loss 0.1535 (0.1535)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [131][64/196]	Time 0.107 (0.101)	Data 0.000 (0.007)	Loss 0.1402 (0.1455)	Acc@1 94.531 (94.928)	Acc@5 99.219 (99.928)
Epoch: [131][128/196]	Time 0.087 (0.100)	Data 0.000 (0.003)	Loss 0.1732 (0.1502)	Acc@1 95.312 (94.816)	Acc@5 99.609 (99.918)
Epoch: [131][192/196]	Time 0.099 (0.100)	Data 0.000 (0.002)	Loss 0.1584 (0.1556)	Acc@1 92.578 (94.598)	Acc@5 100.000 (99.915)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:132/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [132][0/196]	Time 0.139 (0.139)	Data 0.325 (0.325)	Loss 0.1512 (0.1512)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [132][64/196]	Time 0.086 (0.091)	Data 0.000 (0.005)	Loss 0.1690 (0.1467)	Acc@1 93.359 (94.922)	Acc@5 100.000 (99.940)
Epoch: [132][128/196]	Time 0.084 (0.090)	Data 0.000 (0.003)	Loss 0.1471 (0.1518)	Acc@1 94.531 (94.731)	Acc@5 100.000 (99.936)
Epoch: [132][192/196]	Time 0.094 (0.093)	Data 0.000 (0.002)	Loss 0.1604 (0.1568)	Acc@1 94.531 (94.554)	Acc@5 100.000 (99.929)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:133/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [133][0/196]	Time 0.165 (0.165)	Data 0.345 (0.345)	Loss 0.1048 (0.1048)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [133][64/196]	Time 0.103 (0.101)	Data 0.000 (0.006)	Loss 0.1249 (0.1504)	Acc@1 95.703 (94.688)	Acc@5 100.000 (99.898)
Epoch: [133][128/196]	Time 0.098 (0.101)	Data 0.000 (0.003)	Loss 0.2105 (0.1556)	Acc@1 92.969 (94.571)	Acc@5 99.219 (99.900)
Epoch: [133][192/196]	Time 0.102 (0.100)	Data 0.000 (0.002)	Loss 0.2167 (0.1591)	Acc@1 92.578 (94.440)	Acc@5 100.000 (99.899)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:134/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [134][0/196]	Time 0.169 (0.169)	Data 0.373 (0.373)	Loss 0.1242 (0.1242)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [134][64/196]	Time 0.099 (0.100)	Data 0.000 (0.006)	Loss 0.2274 (0.1546)	Acc@1 91.797 (94.627)	Acc@5 100.000 (99.910)
Epoch: [134][128/196]	Time 0.097 (0.100)	Data 0.000 (0.003)	Loss 0.1466 (0.1573)	Acc@1 96.484 (94.519)	Acc@5 100.000 (99.921)
Epoch: [134][192/196]	Time 0.084 (0.096)	Data 0.000 (0.002)	Loss 0.2204 (0.1614)	Acc@1 92.969 (94.418)	Acc@5 99.609 (99.907)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:135/135; Lr: 0.010000000000000002
batch Size 256
Epoch: [135][0/196]	Time 0.131 (0.131)	Data 0.281 (0.281)	Loss 0.1386 (0.1386)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [135][64/196]	Time 0.096 (0.096)	Data 0.000 (0.005)	Loss 0.1617 (0.1549)	Acc@1 93.359 (94.681)	Acc@5 100.000 (99.952)
Epoch: [135][128/196]	Time 0.107 (0.098)	Data 0.000 (0.002)	Loss 0.1995 (0.1558)	Acc@1 93.750 (94.628)	Acc@5 100.000 (99.945)
Epoch: [135][192/196]	Time 0.099 (0.098)	Data 0.000 (0.002)	Loss 0.1533 (0.1584)	Acc@1 94.531 (94.503)	Acc@5 99.609 (99.921)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  88.58
Max memory: 51.4381312
 19.619s  j: 136 bis 140
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6187
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:136/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [136][0/196]	Time 0.177 (0.177)	Data 0.385 (0.385)	Loss 0.1323 (0.1323)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [136][64/196]	Time 0.103 (0.101)	Data 0.000 (0.006)	Loss 0.1700 (0.1497)	Acc@1 93.359 (94.736)	Acc@5 100.000 (99.934)
Epoch: [136][128/196]	Time 0.089 (0.099)	Data 0.000 (0.003)	Loss 0.1591 (0.1544)	Acc@1 94.922 (94.516)	Acc@5 100.000 (99.924)
Epoch: [136][192/196]	Time 0.087 (0.096)	Data 0.000 (0.002)	Loss 0.2280 (0.1575)	Acc@1 92.188 (94.481)	Acc@5 99.609 (99.927)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:137/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [137][0/196]	Time 0.161 (0.161)	Data 0.264 (0.264)	Loss 0.1639 (0.1639)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [137][64/196]	Time 0.102 (0.101)	Data 0.000 (0.004)	Loss 0.1275 (0.1522)	Acc@1 96.094 (94.766)	Acc@5 100.000 (99.946)
Epoch: [137][128/196]	Time 0.097 (0.099)	Data 0.000 (0.002)	Loss 0.1713 (0.1551)	Acc@1 92.578 (94.640)	Acc@5 100.000 (99.936)
Epoch: [137][192/196]	Time 0.088 (0.098)	Data 0.000 (0.002)	Loss 0.1618 (0.1555)	Acc@1 95.312 (94.643)	Acc@5 100.000 (99.923)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:138/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [138][0/196]	Time 0.132 (0.132)	Data 0.369 (0.369)	Loss 0.1470 (0.1470)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [138][64/196]	Time 0.091 (0.099)	Data 0.000 (0.006)	Loss 0.1786 (0.1531)	Acc@1 94.531 (94.537)	Acc@5 100.000 (99.940)
Epoch: [138][128/196]	Time 0.090 (0.099)	Data 0.000 (0.003)	Loss 0.1464 (0.1536)	Acc@1 95.703 (94.583)	Acc@5 100.000 (99.924)
Epoch: [138][192/196]	Time 0.089 (0.098)	Data 0.000 (0.002)	Loss 0.2064 (0.1569)	Acc@1 93.750 (94.469)	Acc@5 100.000 (99.917)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:139/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [139][0/196]	Time 0.140 (0.140)	Data 0.380 (0.380)	Loss 0.1781 (0.1781)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [139][64/196]	Time 0.081 (0.091)	Data 0.000 (0.006)	Loss 0.2169 (0.1581)	Acc@1 92.578 (94.549)	Acc@5 100.000 (99.910)
Epoch: [139][128/196]	Time 0.096 (0.090)	Data 0.000 (0.003)	Loss 0.1525 (0.1564)	Acc@1 95.703 (94.619)	Acc@5 100.000 (99.906)
Epoch: [139][192/196]	Time 0.090 (0.093)	Data 0.000 (0.002)	Loss 0.2052 (0.1614)	Acc@1 93.359 (94.412)	Acc@5 100.000 (99.913)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:140/140; Lr: 0.010000000000000002
batch Size 256
Epoch: [140][0/196]	Time 0.148 (0.148)	Data 0.349 (0.349)	Loss 0.1595 (0.1595)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [140][64/196]	Time 0.112 (0.101)	Data 0.000 (0.006)	Loss 0.1753 (0.1492)	Acc@1 93.750 (94.820)	Acc@5 100.000 (99.946)
Epoch: [140][128/196]	Time 0.103 (0.100)	Data 0.000 (0.003)	Loss 0.1988 (0.1516)	Acc@1 92.969 (94.734)	Acc@5 100.000 (99.936)
Epoch: [140][192/196]	Time 0.090 (0.099)	Data 0.000 (0.002)	Loss 0.2925 (0.1562)	Acc@1 89.844 (94.503)	Acc@5 99.219 (99.929)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.09
Max memory: 51.4381312
 19.863s  j: 141 bis 145
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1427
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 141
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:141/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [141][0/196]	Time 0.158 (0.158)	Data 0.376 (0.376)	Loss 0.1520 (0.1520)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [141][64/196]	Time 0.087 (0.090)	Data 0.000 (0.006)	Loss 0.1379 (0.1498)	Acc@1 94.922 (94.856)	Acc@5 100.000 (99.928)
Epoch: [141][128/196]	Time 0.095 (0.090)	Data 0.000 (0.003)	Loss 0.1533 (0.1526)	Acc@1 94.922 (94.686)	Acc@5 99.219 (99.924)
Epoch: [141][192/196]	Time 0.089 (0.092)	Data 0.000 (0.002)	Loss 0.2571 (0.1571)	Acc@1 90.625 (94.547)	Acc@5 100.000 (99.925)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:142/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [142][0/196]	Time 0.136 (0.136)	Data 0.335 (0.335)	Loss 0.1579 (0.1579)	Acc@1 94.141 (94.141)	Acc@5 99.609 (99.609)
Epoch: [142][64/196]	Time 0.103 (0.101)	Data 0.000 (0.005)	Loss 0.1340 (0.1547)	Acc@1 94.531 (94.615)	Acc@5 100.000 (99.916)
Epoch: [142][128/196]	Time 0.112 (0.099)	Data 0.000 (0.003)	Loss 0.1677 (0.1544)	Acc@1 93.750 (94.689)	Acc@5 100.000 (99.936)
Epoch: [142][192/196]	Time 0.098 (0.098)	Data 0.000 (0.002)	Loss 0.1809 (0.1576)	Acc@1 92.969 (94.606)	Acc@5 100.000 (99.923)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:143/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [143][0/196]	Time 0.127 (0.127)	Data 0.365 (0.365)	Loss 0.0941 (0.0941)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [143][64/196]	Time 0.080 (0.096)	Data 0.000 (0.006)	Loss 0.1813 (0.1645)	Acc@1 94.922 (94.189)	Acc@5 100.000 (99.922)
Epoch: [143][128/196]	Time 0.098 (0.096)	Data 0.000 (0.003)	Loss 0.1582 (0.1626)	Acc@1 93.750 (94.259)	Acc@5 99.609 (99.918)
Epoch: [143][192/196]	Time 0.090 (0.092)	Data 0.000 (0.002)	Loss 0.1114 (0.1608)	Acc@1 95.703 (94.311)	Acc@5 100.000 (99.917)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:144/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [144][0/196]	Time 0.129 (0.129)	Data 0.327 (0.327)	Loss 0.1613 (0.1613)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [144][64/196]	Time 0.094 (0.097)	Data 0.000 (0.005)	Loss 0.1227 (0.1544)	Acc@1 94.531 (94.663)	Acc@5 100.000 (99.964)
Epoch: [144][128/196]	Time 0.100 (0.097)	Data 0.000 (0.003)	Loss 0.1479 (0.1571)	Acc@1 94.531 (94.655)	Acc@5 100.000 (99.961)
Epoch: [144][192/196]	Time 0.105 (0.096)	Data 0.000 (0.002)	Loss 0.2045 (0.1566)	Acc@1 92.578 (94.586)	Acc@5 100.000 (99.957)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:145/145; Lr: 0.010000000000000002
batch Size 256
Epoch: [145][0/196]	Time 0.149 (0.149)	Data 0.404 (0.404)	Loss 0.1687 (0.1687)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [145][64/196]	Time 0.094 (0.096)	Data 0.000 (0.006)	Loss 0.1430 (0.1577)	Acc@1 95.312 (94.303)	Acc@5 99.609 (99.934)
Epoch: [145][128/196]	Time 0.085 (0.095)	Data 0.000 (0.003)	Loss 0.1333 (0.1583)	Acc@1 94.922 (94.383)	Acc@5 99.609 (99.924)
Epoch: [145][192/196]	Time 0.085 (0.095)	Data 0.000 (0.002)	Loss 0.1844 (0.1604)	Acc@1 93.750 (94.365)	Acc@5 99.609 (99.927)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  87.73
Max memory: 51.4381312
 19.139s  j: 146 bis 150
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5569
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 146
Max memory: 0.1097216
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:146/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [146][0/196]	Time 0.153 (0.153)	Data 0.316 (0.316)	Loss 0.0805 (0.0805)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [146][64/196]	Time 0.094 (0.097)	Data 0.000 (0.005)	Loss 0.1307 (0.1437)	Acc@1 95.312 (94.844)	Acc@5 100.000 (99.946)
Epoch: [146][128/196]	Time 0.097 (0.098)	Data 0.000 (0.003)	Loss 0.1512 (0.1503)	Acc@1 92.188 (94.686)	Acc@5 100.000 (99.939)
Epoch: [146][192/196]	Time 0.098 (0.098)	Data 0.000 (0.002)	Loss 0.1833 (0.1531)	Acc@1 94.922 (94.622)	Acc@5 100.000 (99.937)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:147/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [147][0/196]	Time 0.139 (0.139)	Data 0.298 (0.298)	Loss 0.1429 (0.1429)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [147][64/196]	Time 0.103 (0.103)	Data 0.000 (0.005)	Loss 0.0966 (0.1567)	Acc@1 97.656 (94.633)	Acc@5 100.000 (99.934)
Epoch: [147][128/196]	Time 0.098 (0.099)	Data 0.000 (0.003)	Loss 0.1686 (0.1554)	Acc@1 94.531 (94.652)	Acc@5 100.000 (99.912)
Epoch: [147][192/196]	Time 0.092 (0.098)	Data 0.000 (0.002)	Loss 0.1560 (0.1601)	Acc@1 95.703 (94.456)	Acc@5 100.000 (99.909)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:148/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [148][0/196]	Time 0.168 (0.168)	Data 0.368 (0.368)	Loss 0.1830 (0.1830)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [148][64/196]	Time 0.082 (0.090)	Data 0.000 (0.006)	Loss 0.1917 (0.1524)	Acc@1 92.969 (94.543)	Acc@5 100.000 (99.946)
Epoch: [148][128/196]	Time 0.086 (0.089)	Data 0.000 (0.003)	Loss 0.1716 (0.1535)	Acc@1 95.312 (94.468)	Acc@5 99.609 (99.933)
Epoch: [148][192/196]	Time 0.088 (0.091)	Data 0.000 (0.002)	Loss 0.1523 (0.1574)	Acc@1 95.312 (94.355)	Acc@5 100.000 (99.925)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:149/150; Lr: 0.010000000000000002
batch Size 256
Epoch: [149][0/196]	Time 0.126 (0.126)	Data 0.322 (0.322)	Loss 0.1570 (0.1570)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [149][64/196]	Time 0.094 (0.100)	Data 0.000 (0.005)	Loss 0.1128 (0.1510)	Acc@1 96.484 (94.946)	Acc@5 100.000 (99.946)
Epoch: [149][128/196]	Time 0.079 (0.098)	Data 0.000 (0.003)	Loss 0.1375 (0.1536)	Acc@1 96.484 (94.780)	Acc@5 100.000 (99.933)
Epoch: [149][192/196]	Time 0.091 (0.097)	Data 0.000 (0.002)	Loss 0.2568 (0.1584)	Acc@1 90.625 (94.584)	Acc@5 99.609 (99.943)
Max memory in training epoch: 33.3541888
lr: 0.010000000000000002
lr: 0.010000000000000002
1
Epoche:150/150; Lr: 0.0010000000000000002
batch Size 256
Epoch: [150][0/196]	Time 0.143 (0.143)	Data 0.456 (0.456)	Loss 0.1282 (0.1282)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [150][64/196]	Time 0.105 (0.098)	Data 0.000 (0.007)	Loss 0.1036 (0.1365)	Acc@1 97.656 (95.331)	Acc@5 100.000 (99.940)
Epoch: [150][128/196]	Time 0.102 (0.098)	Data 0.000 (0.004)	Loss 0.1262 (0.1301)	Acc@1 96.094 (95.561)	Acc@5 100.000 (99.942)
Epoch: [150][192/196]	Time 0.088 (0.095)	Data 0.000 (0.003)	Loss 0.0716 (0.1243)	Acc@1 98.438 (95.774)	Acc@5 100.000 (99.931)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.9
Max memory: 51.4381312
 19.189s  j: 151 bis 155
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 6415
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 151
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:151/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [151][0/196]	Time 0.165 (0.165)	Data 0.402 (0.402)	Loss 0.1213 (0.1213)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [151][64/196]	Time 0.092 (0.100)	Data 0.000 (0.006)	Loss 0.1303 (0.1112)	Acc@1 93.750 (96.400)	Acc@5 100.000 (99.964)
Epoch: [151][128/196]	Time 0.104 (0.099)	Data 0.000 (0.003)	Loss 0.1069 (0.1096)	Acc@1 96.484 (96.460)	Acc@5 100.000 (99.964)
Epoch: [151][192/196]	Time 0.103 (0.098)	Data 0.000 (0.002)	Loss 0.0891 (0.1104)	Acc@1 96.484 (96.484)	Acc@5 100.000 (99.960)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:152/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [152][0/196]	Time 0.169 (0.169)	Data 0.340 (0.340)	Loss 0.1024 (0.1024)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [152][64/196]	Time 0.111 (0.101)	Data 0.000 (0.005)	Loss 0.1337 (0.1081)	Acc@1 95.312 (96.376)	Acc@5 100.000 (99.928)
Epoch: [152][128/196]	Time 0.097 (0.098)	Data 0.000 (0.003)	Loss 0.0982 (0.1051)	Acc@1 96.094 (96.554)	Acc@5 100.000 (99.942)
Epoch: [152][192/196]	Time 0.087 (0.095)	Data 0.000 (0.002)	Loss 0.1045 (0.1055)	Acc@1 95.703 (96.559)	Acc@5 100.000 (99.933)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:153/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [153][0/196]	Time 0.144 (0.144)	Data 0.289 (0.289)	Loss 0.1102 (0.1102)	Acc@1 96.875 (96.875)	Acc@5 99.609 (99.609)
Epoch: [153][64/196]	Time 0.087 (0.094)	Data 0.000 (0.005)	Loss 0.0639 (0.0983)	Acc@1 98.047 (96.875)	Acc@5 100.000 (99.958)
Epoch: [153][128/196]	Time 0.107 (0.095)	Data 0.000 (0.002)	Loss 0.0863 (0.1009)	Acc@1 97.656 (96.757)	Acc@5 99.609 (99.964)
Epoch: [153][192/196]	Time 0.092 (0.095)	Data 0.000 (0.002)	Loss 0.0835 (0.1006)	Acc@1 97.266 (96.741)	Acc@5 100.000 (99.960)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:154/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [154][0/196]	Time 0.146 (0.146)	Data 0.299 (0.299)	Loss 0.1300 (0.1300)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [154][64/196]	Time 0.098 (0.102)	Data 0.000 (0.005)	Loss 0.0913 (0.1004)	Acc@1 96.484 (96.911)	Acc@5 100.000 (99.970)
Epoch: [154][128/196]	Time 0.096 (0.099)	Data 0.000 (0.003)	Loss 0.1081 (0.0980)	Acc@1 95.703 (96.933)	Acc@5 100.000 (99.976)
Epoch: [154][192/196]	Time 0.103 (0.098)	Data 0.000 (0.002)	Loss 0.1030 (0.0991)	Acc@1 97.266 (96.863)	Acc@5 100.000 (99.962)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:155/155; Lr: 0.0010000000000000002
batch Size 256
Epoch: [155][0/196]	Time 0.181 (0.181)	Data 0.308 (0.308)	Loss 0.0810 (0.0810)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [155][64/196]	Time 0.089 (0.096)	Data 0.000 (0.005)	Loss 0.1488 (0.0988)	Acc@1 94.922 (96.761)	Acc@5 100.000 (99.952)
Epoch: [155][128/196]	Time 0.086 (0.092)	Data 0.000 (0.003)	Loss 0.1247 (0.0960)	Acc@1 96.484 (96.942)	Acc@5 99.609 (99.958)
Epoch: [155][192/196]	Time 0.090 (0.093)	Data 0.000 (0.002)	Loss 0.0936 (0.0978)	Acc@1 97.266 (96.861)	Acc@5 100.000 (99.960)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.1
Max memory: 51.4381312
 18.578s  j: 156 bis 160
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 4685
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 156
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:156/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [156][0/196]	Time 0.223 (0.223)	Data 0.302 (0.302)	Loss 0.1050 (0.1050)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [156][64/196]	Time 0.105 (0.103)	Data 0.000 (0.005)	Loss 0.1486 (0.0959)	Acc@1 95.312 (96.941)	Acc@5 100.000 (99.964)
Epoch: [156][128/196]	Time 0.096 (0.100)	Data 0.000 (0.003)	Loss 0.0861 (0.0944)	Acc@1 97.266 (97.035)	Acc@5 100.000 (99.967)
Epoch: [156][192/196]	Time 0.094 (0.099)	Data 0.000 (0.002)	Loss 0.0531 (0.0960)	Acc@1 98.047 (96.952)	Acc@5 100.000 (99.960)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:157/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [157][0/196]	Time 0.128 (0.128)	Data 0.271 (0.271)	Loss 0.1064 (0.1064)	Acc@1 97.266 (97.266)	Acc@5 99.609 (99.609)
Epoch: [157][64/196]	Time 0.091 (0.093)	Data 0.000 (0.004)	Loss 0.0907 (0.0999)	Acc@1 98.438 (96.815)	Acc@5 100.000 (99.958)
Epoch: [157][128/196]	Time 0.081 (0.091)	Data 0.000 (0.002)	Loss 0.1073 (0.0989)	Acc@1 96.875 (96.863)	Acc@5 100.000 (99.958)
Epoch: [157][192/196]	Time 0.114 (0.093)	Data 0.000 (0.002)	Loss 0.1112 (0.0966)	Acc@1 96.094 (96.938)	Acc@5 100.000 (99.964)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:158/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [158][0/196]	Time 0.154 (0.154)	Data 0.389 (0.389)	Loss 0.0939 (0.0939)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [158][64/196]	Time 0.090 (0.101)	Data 0.000 (0.006)	Loss 0.0908 (0.0912)	Acc@1 97.656 (97.031)	Acc@5 100.000 (99.952)
Epoch: [158][128/196]	Time 0.113 (0.100)	Data 0.000 (0.003)	Loss 0.1361 (0.0922)	Acc@1 95.312 (97.029)	Acc@5 100.000 (99.949)
Epoch: [158][192/196]	Time 0.100 (0.099)	Data 0.000 (0.002)	Loss 0.0735 (0.0925)	Acc@1 98.047 (97.053)	Acc@5 100.000 (99.955)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:159/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [159][0/196]	Time 0.149 (0.149)	Data 0.311 (0.311)	Loss 0.0766 (0.0766)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [159][64/196]	Time 0.104 (0.102)	Data 0.000 (0.005)	Loss 0.0748 (0.0891)	Acc@1 98.047 (97.157)	Acc@5 100.000 (99.982)
Epoch: [159][128/196]	Time 0.103 (0.101)	Data 0.000 (0.003)	Loss 0.0931 (0.0907)	Acc@1 97.656 (97.102)	Acc@5 100.000 (99.970)
Epoch: [159][192/196]	Time 0.080 (0.097)	Data 0.000 (0.002)	Loss 0.0798 (0.0907)	Acc@1 98.047 (97.108)	Acc@5 100.000 (99.966)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:160/160; Lr: 0.0010000000000000002
batch Size 256
Epoch: [160][0/196]	Time 0.123 (0.123)	Data 0.288 (0.288)	Loss 0.0665 (0.0665)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [160][64/196]	Time 0.096 (0.094)	Data 0.000 (0.005)	Loss 0.1127 (0.0910)	Acc@1 96.875 (97.007)	Acc@5 100.000 (100.000)
Epoch: [160][128/196]	Time 0.114 (0.096)	Data 0.000 (0.002)	Loss 0.0810 (0.0905)	Acc@1 96.875 (97.045)	Acc@5 100.000 (99.991)
Epoch: [160][192/196]	Time 0.099 (0.097)	Data 0.000 (0.002)	Loss 0.0854 (0.0915)	Acc@1 98.047 (97.065)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.08
Max memory: 51.4381312
 19.275s  j: 161 bis 165
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 1545
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 161
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:161/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [161][0/196]	Time 0.217 (0.217)	Data 0.351 (0.351)	Loss 0.0752 (0.0752)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [161][64/196]	Time 0.085 (0.102)	Data 0.000 (0.006)	Loss 0.0533 (0.0902)	Acc@1 99.219 (97.091)	Acc@5 100.000 (99.964)
Epoch: [161][128/196]	Time 0.091 (0.097)	Data 0.000 (0.003)	Loss 0.1295 (0.0901)	Acc@1 96.484 (97.039)	Acc@5 99.609 (99.964)
Epoch: [161][192/196]	Time 0.083 (0.094)	Data 0.000 (0.002)	Loss 0.0841 (0.0907)	Acc@1 97.266 (97.059)	Acc@5 100.000 (99.966)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:162/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [162][0/196]	Time 0.125 (0.125)	Data 0.283 (0.283)	Loss 0.1032 (0.1032)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [162][64/196]	Time 0.121 (0.093)	Data 0.000 (0.005)	Loss 0.0927 (0.0900)	Acc@1 97.266 (97.175)	Acc@5 100.000 (99.982)
Epoch: [162][128/196]	Time 0.084 (0.095)	Data 0.000 (0.002)	Loss 0.1008 (0.0906)	Acc@1 97.266 (97.120)	Acc@5 99.219 (99.976)
Epoch: [162][192/196]	Time 0.091 (0.095)	Data 0.000 (0.002)	Loss 0.0968 (0.0904)	Acc@1 97.266 (97.175)	Acc@5 99.609 (99.964)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:163/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [163][0/196]	Time 0.139 (0.139)	Data 0.399 (0.399)	Loss 0.0750 (0.0750)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [163][64/196]	Time 0.104 (0.098)	Data 0.000 (0.006)	Loss 0.1177 (0.0914)	Acc@1 95.312 (97.019)	Acc@5 99.609 (99.946)
Epoch: [163][128/196]	Time 0.096 (0.098)	Data 0.000 (0.003)	Loss 0.1132 (0.0890)	Acc@1 94.922 (97.096)	Acc@5 100.000 (99.964)
Epoch: [163][192/196]	Time 0.090 (0.098)	Data 0.000 (0.002)	Loss 0.0706 (0.0892)	Acc@1 97.656 (97.061)	Acc@5 100.000 (99.970)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:164/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [164][0/196]	Time 0.135 (0.135)	Data 0.363 (0.363)	Loss 0.0981 (0.0981)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [164][64/196]	Time 0.083 (0.096)	Data 0.000 (0.006)	Loss 0.1146 (0.0843)	Acc@1 96.484 (97.260)	Acc@5 100.000 (99.964)
Epoch: [164][128/196]	Time 0.084 (0.091)	Data 0.000 (0.003)	Loss 0.1002 (0.0880)	Acc@1 96.484 (97.105)	Acc@5 100.000 (99.970)
Epoch: [164][192/196]	Time 0.098 (0.092)	Data 0.000 (0.002)	Loss 0.0794 (0.0875)	Acc@1 96.484 (97.146)	Acc@5 100.000 (99.972)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:165/165; Lr: 0.0010000000000000002
batch Size 256
Epoch: [165][0/196]	Time 0.133 (0.133)	Data 0.355 (0.355)	Loss 0.1306 (0.1306)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [165][64/196]	Time 0.103 (0.098)	Data 0.000 (0.006)	Loss 0.1195 (0.0890)	Acc@1 95.312 (97.097)	Acc@5 100.000 (99.988)
Epoch: [165][128/196]	Time 0.084 (0.098)	Data 0.000 (0.003)	Loss 0.0885 (0.0893)	Acc@1 97.266 (97.063)	Acc@5 100.000 (99.982)
Epoch: [165][192/196]	Time 0.096 (0.098)	Data 0.000 (0.002)	Loss 0.1051 (0.0874)	Acc@1 95.703 (97.185)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.13
Max memory: 51.4381312
 19.611s  j: 166 bis 170
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5812
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 166
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:166/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [166][0/196]	Time 0.188 (0.188)	Data 0.363 (0.363)	Loss 0.0732 (0.0732)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [166][64/196]	Time 0.083 (0.102)	Data 0.000 (0.006)	Loss 0.1183 (0.0847)	Acc@1 94.531 (97.320)	Acc@5 100.000 (99.970)
Epoch: [166][128/196]	Time 0.103 (0.095)	Data 0.000 (0.003)	Loss 0.0707 (0.0848)	Acc@1 98.438 (97.278)	Acc@5 100.000 (99.970)
Epoch: [166][192/196]	Time 0.098 (0.095)	Data 0.000 (0.002)	Loss 0.0923 (0.0852)	Acc@1 97.266 (97.251)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:167/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [167][0/196]	Time 0.152 (0.152)	Data 0.366 (0.366)	Loss 0.0720 (0.0720)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [167][64/196]	Time 0.112 (0.102)	Data 0.000 (0.006)	Loss 0.0858 (0.0844)	Acc@1 96.484 (97.115)	Acc@5 100.000 (99.958)
Epoch: [167][128/196]	Time 0.096 (0.099)	Data 0.000 (0.003)	Loss 0.0714 (0.0848)	Acc@1 98.047 (97.205)	Acc@5 100.000 (99.967)
Epoch: [167][192/196]	Time 0.090 (0.098)	Data 0.000 (0.002)	Loss 0.0585 (0.0862)	Acc@1 99.219 (97.175)	Acc@5 100.000 (99.972)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:168/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [168][0/196]	Time 0.201 (0.201)	Data 0.321 (0.321)	Loss 0.0816 (0.0816)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [168][64/196]	Time 0.107 (0.103)	Data 0.000 (0.005)	Loss 0.1005 (0.0785)	Acc@1 96.484 (97.416)	Acc@5 100.000 (99.982)
Epoch: [168][128/196]	Time 0.104 (0.100)	Data 0.000 (0.003)	Loss 0.1177 (0.0842)	Acc@1 94.531 (97.223)	Acc@5 100.000 (99.985)
Epoch: [168][192/196]	Time 0.087 (0.098)	Data 0.000 (0.002)	Loss 0.1242 (0.0849)	Acc@1 96.484 (97.233)	Acc@5 99.609 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:169/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [169][0/196]	Time 0.142 (0.142)	Data 0.270 (0.270)	Loss 0.0663 (0.0663)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [169][64/196]	Time 0.094 (0.091)	Data 0.000 (0.004)	Loss 0.1406 (0.0797)	Acc@1 96.094 (97.380)	Acc@5 100.000 (99.976)
Epoch: [169][128/196]	Time 0.103 (0.095)	Data 0.000 (0.002)	Loss 0.0679 (0.0801)	Acc@1 98.828 (97.399)	Acc@5 100.000 (99.982)
Epoch: [169][192/196]	Time 0.099 (0.097)	Data 0.000 (0.002)	Loss 0.1001 (0.0832)	Acc@1 96.094 (97.336)	Acc@5 100.000 (99.970)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:170/170; Lr: 0.0010000000000000002
batch Size 256
Epoch: [170][0/196]	Time 0.160 (0.160)	Data 0.338 (0.338)	Loss 0.0935 (0.0935)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [170][64/196]	Time 0.092 (0.099)	Data 0.000 (0.005)	Loss 0.0732 (0.0825)	Acc@1 96.484 (97.181)	Acc@5 100.000 (99.988)
Epoch: [170][128/196]	Time 0.099 (0.099)	Data 0.000 (0.003)	Loss 0.0904 (0.0834)	Acc@1 96.094 (97.217)	Acc@5 100.000 (99.985)
Epoch: [170][192/196]	Time 0.091 (0.096)	Data 0.000 (0.002)	Loss 0.0725 (0.0838)	Acc@1 97.656 (97.177)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.03
Max memory: 51.4381312
 19.228s  j: 171 bis 175
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5012
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 171
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:171/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [171][0/196]	Time 0.138 (0.138)	Data 0.311 (0.311)	Loss 0.0794 (0.0794)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [171][64/196]	Time 0.099 (0.092)	Data 0.000 (0.005)	Loss 0.0816 (0.0844)	Acc@1 97.266 (97.266)	Acc@5 100.000 (99.994)
Epoch: [171][128/196]	Time 0.090 (0.095)	Data 0.000 (0.003)	Loss 0.0942 (0.0810)	Acc@1 96.875 (97.481)	Acc@5 100.000 (99.991)
Epoch: [171][192/196]	Time 0.091 (0.095)	Data 0.000 (0.002)	Loss 0.1405 (0.0824)	Acc@1 94.922 (97.383)	Acc@5 100.000 (99.984)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:172/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [172][0/196]	Time 0.141 (0.141)	Data 0.374 (0.374)	Loss 0.0866 (0.0866)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [172][64/196]	Time 0.096 (0.101)	Data 0.000 (0.006)	Loss 0.0841 (0.0811)	Acc@1 98.047 (97.350)	Acc@5 100.000 (99.982)
Epoch: [172][128/196]	Time 0.096 (0.100)	Data 0.000 (0.003)	Loss 0.0746 (0.0807)	Acc@1 97.266 (97.344)	Acc@5 100.000 (99.982)
Epoch: [172][192/196]	Time 0.092 (0.098)	Data 0.000 (0.002)	Loss 0.0691 (0.0812)	Acc@1 98.828 (97.363)	Acc@5 100.000 (99.980)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:173/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [173][0/196]	Time 0.139 (0.139)	Data 0.344 (0.344)	Loss 0.0589 (0.0589)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [173][64/196]	Time 0.088 (0.102)	Data 0.000 (0.006)	Loss 0.0711 (0.0788)	Acc@1 98.047 (97.578)	Acc@5 100.000 (99.982)
Epoch: [173][128/196]	Time 0.085 (0.095)	Data 0.000 (0.003)	Loss 0.0916 (0.0824)	Acc@1 96.875 (97.432)	Acc@5 100.000 (99.967)
Epoch: [173][192/196]	Time 0.090 (0.094)	Data 0.000 (0.002)	Loss 0.0621 (0.0832)	Acc@1 99.219 (97.365)	Acc@5 100.000 (99.970)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:174/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [174][0/196]	Time 0.134 (0.134)	Data 0.331 (0.331)	Loss 0.0599 (0.0599)	Acc@1 98.828 (98.828)	Acc@5 100.000 (100.000)
Epoch: [174][64/196]	Time 0.110 (0.099)	Data 0.000 (0.005)	Loss 0.0445 (0.0812)	Acc@1 98.438 (97.482)	Acc@5 100.000 (99.970)
Epoch: [174][128/196]	Time 0.101 (0.098)	Data 0.000 (0.003)	Loss 0.0780 (0.0812)	Acc@1 97.656 (97.450)	Acc@5 100.000 (99.961)
Epoch: [174][192/196]	Time 0.094 (0.098)	Data 0.000 (0.002)	Loss 0.0544 (0.0822)	Acc@1 98.828 (97.393)	Acc@5 100.000 (99.957)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:175/175; Lr: 0.0010000000000000002
batch Size 256
Epoch: [175][0/196]	Time 0.118 (0.118)	Data 0.268 (0.268)	Loss 0.0747 (0.0747)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [175][64/196]	Time 0.109 (0.095)	Data 0.000 (0.004)	Loss 0.0783 (0.0831)	Acc@1 96.875 (97.212)	Acc@5 100.000 (99.982)
Epoch: [175][128/196]	Time 0.090 (0.097)	Data 0.000 (0.002)	Loss 0.1028 (0.0796)	Acc@1 96.484 (97.429)	Acc@5 100.000 (99.985)
Epoch: [175][192/196]	Time 0.092 (0.096)	Data 0.000 (0.002)	Loss 0.0835 (0.0800)	Acc@1 97.266 (97.456)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  90.02
Max memory: 51.4381312
 19.238s  j: 176 bis 180
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 8378
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 176
Max memory: 0.1097216
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:176/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [176][0/196]	Time 0.212 (0.212)	Data 0.295 (0.295)	Loss 0.0952 (0.0952)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [176][64/196]	Time 0.111 (0.098)	Data 0.000 (0.005)	Loss 0.0509 (0.0792)	Acc@1 98.828 (97.512)	Acc@5 100.000 (99.952)
Epoch: [176][128/196]	Time 0.085 (0.097)	Data 0.000 (0.002)	Loss 0.0356 (0.0798)	Acc@1 99.219 (97.502)	Acc@5 100.000 (99.967)
Epoch: [176][192/196]	Time 0.092 (0.098)	Data 0.000 (0.002)	Loss 0.0891 (0.0787)	Acc@1 97.266 (97.509)	Acc@5 100.000 (99.968)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:177/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [177][0/196]	Time 0.139 (0.139)	Data 0.366 (0.366)	Loss 0.0923 (0.0923)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [177][64/196]	Time 0.093 (0.101)	Data 0.000 (0.006)	Loss 0.1084 (0.0786)	Acc@1 95.703 (97.440)	Acc@5 100.000 (99.976)
Epoch: [177][128/196]	Time 0.103 (0.099)	Data 0.000 (0.003)	Loss 0.0477 (0.0779)	Acc@1 99.219 (97.475)	Acc@5 100.000 (99.976)
Epoch: [177][192/196]	Time 0.093 (0.097)	Data 0.000 (0.002)	Loss 0.0646 (0.0797)	Acc@1 98.438 (97.450)	Acc@5 100.000 (99.972)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:178/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [178][0/196]	Time 0.148 (0.148)	Data 0.301 (0.301)	Loss 0.0719 (0.0719)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [178][64/196]	Time 0.103 (0.099)	Data 0.000 (0.005)	Loss 0.1084 (0.0795)	Acc@1 95.703 (97.560)	Acc@5 100.000 (99.964)
Epoch: [178][128/196]	Time 0.108 (0.098)	Data 0.000 (0.003)	Loss 0.0817 (0.0801)	Acc@1 97.656 (97.484)	Acc@5 100.000 (99.973)
Epoch: [178][192/196]	Time 0.102 (0.098)	Data 0.000 (0.002)	Loss 0.0586 (0.0795)	Acc@1 98.438 (97.513)	Acc@5 100.000 (99.974)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:179/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [179][0/196]	Time 0.141 (0.141)	Data 0.427 (0.427)	Loss 0.0732 (0.0732)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [179][64/196]	Time 0.096 (0.097)	Data 0.000 (0.007)	Loss 0.0598 (0.0780)	Acc@1 97.656 (97.602)	Acc@5 100.000 (99.970)
Epoch: [179][128/196]	Time 0.089 (0.097)	Data 0.000 (0.004)	Loss 0.0990 (0.0763)	Acc@1 96.484 (97.629)	Acc@5 100.000 (99.979)
Epoch: [179][192/196]	Time 0.101 (0.096)	Data 0.000 (0.002)	Loss 0.0585 (0.0768)	Acc@1 98.047 (97.579)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
lr: 0.0010000000000000002
1
Epoche:180/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [180][0/196]	Time 0.175 (0.175)	Data 0.319 (0.319)	Loss 0.0807 (0.0807)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [180][64/196]	Time 0.090 (0.093)	Data 0.000 (0.005)	Loss 0.0832 (0.0792)	Acc@1 96.484 (97.536)	Acc@5 99.609 (99.964)
Epoch: [180][128/196]	Time 0.098 (0.093)	Data 0.000 (0.003)	Loss 0.0611 (0.0796)	Acc@1 98.047 (97.469)	Acc@5 100.000 (99.979)
Epoch: [180][192/196]	Time 0.093 (0.094)	Data 0.000 (0.002)	Loss 0.0476 (0.0783)	Acc@1 99.609 (97.531)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
Stage: 3
width of Layers: [8, 16, 32]
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 8
stage: 3; tobestage: 1
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 16
stage: 3; tobestage: 2
width: 32
stage: 3; tobestage: 3
Num: 23
width: 32
stage: 3; tobestage: 3
Num: 24
width: 32
stage: 3; tobestage: 3
Num: 25
width: 32
stage: 3; tobestage: 3
Num: 26
width: 32
stage: 3; tobestage: 3
Num: 27
width: 32
stage: 3; tobestage: 3
Num: 28
width: 32
stage: 3; tobestage: 3
Num: 29
width: 32
stage: 3; tobestage: 3
Num: 30
width: 32
stage: 3; tobestage: 3
Num: 31
width: 32
stage: 3; tobestage: 3
Num: 32
width: 32
stage: 3; tobestage: 3
Num: 33
altList: ['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.conv2.weight', 'module.bn2.weight', 'module.bn2.bias', 'module.conv3.weight', 'module.bn3.weight', 'module.bn3.bias', 'module.conv4.weight', 'module.bn4.weight', 'module.bn4.bias', 'module.conv5.weight', 'module.bn5.weight', 'module.bn5.bias', 'module.conv6.weight', 'module.bn6.weight', 'module.bn6.bias', 'module.conv7.weight', 'module.bn7.weight', 'module.bn7.bias', 'module.conv8.weight', 'module.bn8.weight', 'module.bn8.bias', 'module.conv9.weight', 'module.bn9.weight', 'module.bn9.bias', 'module.conv10.weight', 'module.bn10.weight', 'module.bn10.bias', 'module.conv11.weight', 'module.bn11.weight', 'module.bn11.bias', 'module.conv12.weight', 'module.bn12.weight', 'module.bn12.bias', 'module.conv13.weight', 'module.bn13.weight', 'module.bn13.bias', 'module.conv14.weight', 'module.bn14.weight', 'module.bn14.bias', 'module.conv15.weight', 'module.bn15.weight', 'module.bn15.bias', 'module.conv16.weight', 'module.bn16.weight', 'module.bn16.bias', 'module.conv17.weight', 'module.bn17.weight', 'module.bn17.bias', 'module.conv18.weight', 'module.bn18.weight', 'module.bn18.bias', 'module.conv19.weight', 'module.bn19.weight', 'module.bn19.bias', 'module.conv20.weight', 'module.bn20.weight', 'module.bn20.bias', 'module.conv21.weight', 'module.bn21.weight', 'module.bn21.bias', 'module.conv22.weight', 'module.bn22.weight', 'module.bn22.bias', 'module.conv23.weight', 'module.bn23.weight', 'module.bn23.bias', 'module.conv24.weight', 'module.bn24.weight', 'module.bn24.bias', 'module.conv25.weight', 'module.bn25.weight', 'module.bn25.bias', 'module.conv26.weight', 'module.bn26.weight', 'module.bn26.bias', 'module.conv27.weight', 'module.bn27.weight', 'module.bn27.bias', 'module.conv28.weight', 'module.bn28.weight', 'module.bn28.bias', 'module.conv29.weight', 'module.bn29.weight', 'module.bn29.bias', 'module.conv30.weight', 'module.bn30.weight', 'module.bn30.bias', 'module.conv31.weight', 'module.bn31.weight', 'module.bn31.bias', 'module.conv32.weight', 'module.bn32.weight', 'module.bn32.bias', 'module.conv33.weight', 'module.bn33.weight', 'module.bn33.bias', 'module.fc34.weight', 'module.fc34.bias']
Residual ListI: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
Residual ListO: [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]
j: 23
Resiudual I
new width: 32; old width: 16
Residual O
old width1: 32; new width: 64
c:[[[ 2.75830925e-02 -3.03258561e-02  1.94839351e-02]
  [-4.79108766e-02 -5.77015802e-02 -5.94859049e-02]
  [-1.48576144e-02 -4.67501916e-02 -3.14016603e-02]]

 [[ 6.87119644e-03  3.17204781e-02  5.31681776e-02]
  [ 1.49481418e-03 -1.06332190e-02 -1.64042171e-02]
  [-2.27584667e-03 -1.26682743e-02 -3.13698612e-02]]

 [[-2.69309729e-02  1.88893005e-02  1.02128759e-02]
  [ 6.10381030e-02  7.77558535e-02  9.55155864e-03]
  [ 3.93438861e-02  1.67966560e-02 -8.07691831e-03]]

 [[-2.25527249e-02 -4.85861190e-02 -1.38298785e-02]
  [-4.57283072e-02 -6.33312762e-02  2.51525268e-02]
  [ 2.59962082e-02  3.98113504e-02  5.40203452e-02]]

 [[-3.15660611e-02 -3.99923325e-02 -4.42070281e-03]
  [-7.65905306e-02 -6.25507981e-02 -3.27351950e-02]
  [-6.75168708e-02 -1.05114892e-01 -3.47722471e-02]]

 [[-5.91655262e-02 -1.15730830e-01  9.87561978e-03]
  [-7.07707480e-02 -1.02653213e-01  5.32799587e-02]
  [-4.44479249e-02  1.54755227e-02  8.68223011e-02]]

 [[-7.55960941e-02 -3.33708972e-02 -1.77502465e-02]
  [-5.37277237e-02 -5.62337749e-02 -4.62733470e-02]
  [-1.42469294e-02 -4.48215567e-02 -3.21516097e-02]]

 [[ 3.04527185e-03  3.38302851e-02  5.63464239e-02]
  [ 3.43868472e-02  2.26175357e-02  1.43625261e-03]
  [ 3.54971811e-02 -1.26395365e-02 -4.68121581e-02]]

 [[ 2.92571299e-02  2.79347133e-02 -2.92578549e-03]
  [ 4.97691073e-02 -6.17973739e-04 -2.67337058e-02]
  [ 1.81280600e-03 -3.03876512e-02 -7.70907402e-02]]

 [[ 3.52747403e-02  1.32580744e-02  5.36759049e-02]
  [ 1.89098176e-02 -7.68237095e-03 -6.00271719e-03]
  [ 3.37128676e-02  2.72092540e-02 -7.13883806e-03]]

 [[ 4.55470756e-02 -4.11806591e-02  4.41070311e-02]
  [ 3.32112834e-02 -7.61991949e-04 -8.05197004e-03]
  [ 3.54017243e-02 -3.86478715e-02 -3.30919363e-02]]

 [[ 3.31349787e-03  1.02904551e-01  9.33102593e-02]
  [ 4.57287841e-02  6.86505958e-02  2.20338702e-02]
  [ 7.74776340e-02  2.53463034e-02 -2.69641709e-02]]

 [[-2.55651325e-02  3.65982577e-02 -4.34184596e-02]
  [-2.34157462e-02  1.08843846e-02 -5.11559881e-02]
  [-1.51539957e-02 -4.33582999e-02 -1.38375357e-01]]

 [[-1.05547175e-01 -5.21983169e-02 -1.41301248e-02]
  [-7.65351579e-02  7.64374435e-02  1.28931895e-01]
  [ 4.33730073e-02  2.15098917e-01  2.40117893e-01]]

 [[-1.85010416e-04 -4.51871119e-02 -1.75836310e-02]
  [ 5.73301408e-03  6.69854321e-03  2.17356309e-02]
  [ 3.24553549e-02 -1.71033889e-02 -3.04047647e-03]]

 [[-1.01312600e-01 -6.01755828e-02 -3.59096266e-02]
  [-8.96110460e-02 -5.80073930e-02 -6.81479722e-02]
  [-7.57511556e-02 -5.53870834e-02  9.43033863e-03]]

 [[-2.86350679e-02 -4.15891632e-02 -6.50104601e-03]
  [-7.90336579e-02 -6.92715123e-02 -2.26886533e-02]
  [-7.96354562e-02 -9.80969742e-02 -4.05280404e-02]]

 [[ 3.05718947e-02  3.12347952e-02 -2.88804946e-03]
  [ 5.32353446e-02 -1.47646945e-03 -2.44056173e-02]
  [-2.56652548e-03 -3.30532975e-02 -7.72119686e-02]]

 [[ 2.69816257e-02  3.01959645e-02 -3.08408891e-03]
  [ 5.32721095e-02 -4.69060615e-03 -2.23859940e-02]
  [-1.02979236e-03 -3.40014957e-02 -7.98165500e-02]]

 [[-4.18660836e-03  1.01437211e-01  9.57543999e-02]
  [ 5.03422953e-02  6.89171776e-02  1.93877257e-02]
  [ 7.15530068e-02  2.94133034e-02 -2.79330201e-02]]

 [[ 5.18931122e-03  3.01660113e-02  6.24043494e-02]
  [ 2.92959865e-02  2.15512272e-02  4.50327713e-03]
  [ 3.21033671e-02 -9.62422229e-03 -4.72945049e-02]]

 [[-1.08078986e-01 -4.75613922e-02 -1.41464137e-02]
  [-7.99893737e-02  7.72088170e-02  1.35382220e-01]
  [ 4.64090295e-02  2.16198072e-01  2.37644404e-01]]

 [[-2.47996412e-02 -4.16595601e-02 -9.38132871e-03]
  [-5.40320165e-02 -6.99612051e-02  2.28259396e-02]
  [ 2.38476153e-02  3.85511965e-02  5.27168140e-02]]

 [[-1.29249865e-05 -4.27273773e-02 -1.92560535e-02]
  [ 6.98032370e-03  4.73905355e-03  2.12608147e-02]
  [ 2.79488433e-02 -1.66810714e-02  3.25428788e-04]]

 [[ 4.85119186e-02 -4.44168113e-02  3.88605185e-02]
  [ 3.12448069e-02  7.44984346e-03 -8.88411980e-03]
  [ 3.19960751e-02 -4.12405841e-02 -3.06902174e-02]]

 [[-1.03323676e-01 -6.19716085e-02 -3.72346789e-02]
  [-9.50722247e-02 -5.86142018e-02 -6.51004836e-02]
  [-7.25127235e-02 -5.24110533e-02  1.04743568e-02]]

 [[ 2.55986489e-02  8.86524003e-03  5.25075272e-02]
  [ 1.43979443e-02 -1.64268930e-02 -7.20230490e-03]
  [ 3.19004282e-02  2.39423495e-02 -9.75914113e-03]]

 [[-4.80806492e-02 -1.13748245e-01  6.01780694e-03]
  [-7.04010725e-02 -9.82010961e-02  5.26780114e-02]
  [-4.38577905e-02  1.58932582e-02  8.35414007e-02]]

 [[-2.41032355e-02  3.62356976e-02 -4.43776883e-02]
  [-1.95346624e-02  5.56616578e-03 -4.93493713e-02]
  [-1.51708750e-02 -4.78011221e-02 -1.37681052e-01]]

 [[-2.74873301e-02 -3.65400761e-02 -9.66241676e-03]
  [-7.77672902e-02 -6.83502927e-02 -2.47506350e-02]
  [-7.12320507e-02 -1.03736840e-01 -4.05624323e-02]]

 [[ 2.89922804e-02  2.38019526e-02 -2.79403129e-03]
  [ 5.22931106e-02  5.35746367e-05 -2.52516996e-02]
  [ 2.10848683e-03 -3.46052684e-02 -8.01600814e-02]]

 [[-2.49486808e-02 -4.08902429e-02 -6.64274860e-03]
  [-7.76753947e-02 -6.06916547e-02 -2.47118659e-02]
  [-7.41501302e-02 -1.04833953e-01 -4.21361886e-02]]]
Traceback (most recent call last):
  File "main.py", line 965, in <module>
    main()
  File "main.py", line 558, in main
    model = model.wider(3, 2, out_size=None, weight_norm=None, random_init=False, addNoise=True)
  File "/home/jessica.buehler/MA_Source/src/n2n.py", line 883, in wider
    f[m] = f[m] / ct.get(listindices[idx])
TypeError: unsupported operand type(s) for /: 'float' and 'NoneType'
j: 181 bis 185
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 5883
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 176
Max memory: 0.1097216
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:176/180; Lr: 0.0010000000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [176][0/196]	Time 0.155 (0.155)	Data 0.330 (0.330)	Loss 0.0753 (0.0753)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [176][64/196]	Time 0.109 (0.101)	Data 0.000 (0.005)	Loss 0.0902 (0.0789)	Acc@1 97.266 (97.428)	Acc@5 100.000 (99.976)
Epoch: [176][128/196]	Time 0.108 (0.099)	Data 0.000 (0.003)	Loss 0.0598 (0.0779)	Acc@1 98.047 (97.496)	Acc@5 100.000 (99.973)
Epoch: [176][192/196]	Time 0.088 (0.098)	Data 0.000 (0.002)	Loss 0.0564 (0.0792)	Acc@1 98.438 (97.430)	Acc@5 100.000 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:177/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [177][0/196]	Time 0.148 (0.148)	Data 0.297 (0.297)	Loss 0.0650 (0.0650)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [177][64/196]	Time 0.091 (0.100)	Data 0.000 (0.005)	Loss 0.0664 (0.0820)	Acc@1 97.656 (97.236)	Acc@5 100.000 (99.976)
Epoch: [177][128/196]	Time 0.106 (0.094)	Data 0.000 (0.003)	Loss 0.0507 (0.0789)	Acc@1 98.438 (97.481)	Acc@5 100.000 (99.967)
Epoch: [177][192/196]	Time 0.099 (0.094)	Data 0.000 (0.002)	Loss 0.0721 (0.0788)	Acc@1 98.047 (97.470)	Acc@5 100.000 (99.970)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:178/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [178][0/196]	Time 0.168 (0.168)	Data 0.327 (0.327)	Loss 0.0670 (0.0670)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [178][64/196]	Time 0.095 (0.101)	Data 0.000 (0.005)	Loss 0.0973 (0.0824)	Acc@1 96.484 (97.260)	Acc@5 100.000 (99.982)
Epoch: [178][128/196]	Time 0.087 (0.099)	Data 0.000 (0.003)	Loss 0.0707 (0.0808)	Acc@1 98.047 (97.378)	Acc@5 100.000 (99.982)
Epoch: [178][192/196]	Time 0.095 (0.098)	Data 0.000 (0.002)	Loss 0.0894 (0.0812)	Acc@1 96.484 (97.385)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0010000000000000002
lr: 0.0010000000000000002
Epoche:179/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [179][0/196]	Time 0.138 (0.138)	Data 0.324 (0.324)	Loss 0.0838 (0.0838)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [179][64/196]	Time 0.086 (0.095)	Data 0.000 (0.005)	Loss 0.0915 (0.0747)	Acc@1 95.703 (97.638)	Acc@5 100.000 (100.000)
Epoch: [179][128/196]	Time 0.081 (0.097)	Data 0.000 (0.003)	Loss 0.0708 (0.0752)	Acc@1 97.656 (97.590)	Acc@5 100.000 (99.982)
Epoch: [179][192/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 0.1009 (0.0755)	Acc@1 96.875 (97.628)	Acc@5 99.609 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0010000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:180/180; Lr: 0.0010000000000000002
batch Size 256
Epoch: [180][0/196]	Time 0.131 (0.131)	Data 0.289 (0.289)	Loss 0.0726 (0.0726)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [180][64/196]	Time 0.086 (0.090)	Data 0.000 (0.005)	Loss 0.0740 (0.0748)	Acc@1 97.656 (97.620)	Acc@5 100.000 (99.982)
Epoch: [180][128/196]	Time 0.102 (0.095)	Data 0.000 (0.002)	Loss 0.0510 (0.0759)	Acc@1 98.438 (97.605)	Acc@5 100.000 (99.979)
Epoch: [180][192/196]	Time 0.098 (0.097)	Data 0.000 (0.002)	Loss 0.0696 (0.0773)	Acc@1 97.266 (97.529)	Acc@5 100.000 (99.982)
Max memory in training epoch: 33.3541888
Model: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
[INFO] Storing checkpoint...
  90.16
Max memory: 51.4381312
 19.443s  j: 186 bis 190
no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
random number: 70
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 181
Max memory: 0.1097216
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:181/185; Lr: 0.0009000000000000002
batch Size 256
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Epoch: [181][0/196]	Time 0.185 (0.185)	Data 0.341 (0.341)	Loss 0.0583 (0.0583)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [181][64/196]	Time 0.084 (0.099)	Data 0.000 (0.005)	Loss 0.0915 (0.0774)	Acc@1 96.484 (97.542)	Acc@5 100.000 (99.982)
Epoch: [181][128/196]	Time 0.089 (0.097)	Data 0.000 (0.003)	Loss 0.0740 (0.0766)	Acc@1 98.047 (97.574)	Acc@5 100.000 (99.976)
Epoch: [181][192/196]	Time 0.082 (0.095)	Data 0.000 (0.002)	Loss 0.0639 (0.0767)	Acc@1 98.438 (97.569)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:182/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [182][0/196]	Time 0.128 (0.128)	Data 0.264 (0.264)	Loss 0.0680 (0.0680)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [182][64/196]	Time 0.106 (0.090)	Data 0.000 (0.004)	Loss 0.0819 (0.0785)	Acc@1 96.094 (97.464)	Acc@5 100.000 (99.982)
Epoch: [182][128/196]	Time 0.095 (0.093)	Data 0.000 (0.002)	Loss 0.0624 (0.0799)	Acc@1 97.656 (97.353)	Acc@5 100.000 (99.982)
Epoch: [182][192/196]	Time 0.097 (0.095)	Data 0.000 (0.002)	Loss 0.0782 (0.0785)	Acc@1 97.266 (97.452)	Acc@5 100.000 (99.972)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:183/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [183][0/196]	Time 0.132 (0.132)	Data 0.307 (0.307)	Loss 0.0917 (0.0917)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [183][64/196]	Time 0.093 (0.096)	Data 0.000 (0.005)	Loss 0.0965 (0.0755)	Acc@1 95.312 (97.560)	Acc@5 100.000 (99.982)
Epoch: [183][128/196]	Time 0.078 (0.095)	Data 0.000 (0.003)	Loss 0.0758 (0.0746)	Acc@1 98.438 (97.659)	Acc@5 100.000 (99.979)
Epoch: [183][192/196]	Time 0.081 (0.094)	Data 0.000 (0.002)	Loss 0.0857 (0.0747)	Acc@1 97.656 (97.628)	Acc@5 99.609 (99.976)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0009000000000000002
lr: 0.0009000000000000002
Epoche:184/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [184][0/196]	Time 0.143 (0.143)	Data 0.402 (0.402)	Loss 0.0938 (0.0938)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [184][64/196]	Time 0.089 (0.097)	Data 0.000 (0.006)	Loss 0.0823 (0.0787)	Acc@1 98.047 (97.476)	Acc@5 100.000 (99.988)
Epoch: [184][128/196]	Time 0.085 (0.093)	Data 0.000 (0.003)	Loss 0.0607 (0.0783)	Acc@1 98.438 (97.496)	Acc@5 100.000 (99.982)
Epoch: [184][192/196]	Time 0.090 (0.092)	Data 0.000 (0.002)	Loss 0.0692 (0.0768)	Acc@1 97.656 (97.543)	Acc@5 100.000 (99.988)
Max memory in training epoch: 33.3541888
lr: 0.0009000000000000002
args.lr: 0.0008100000000000002
lr: 0.0008100000000000002
Epoche:185/185; Lr: 0.0009000000000000002
batch Size 256
Epoch: [185][0/196]	Time 0.166 (0.166)	Data 0.368 (0.368)	Loss 0.0725 (0.0725)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [185][64/196]	Time 0.091 (0.098)	Data 0.000 (0.006)	Loss 0.0621 (0.0720)	Acc@1 97.656 (97.740)	Acc@5 100.000 (99.988)
Epoch: [185][128/196]	Time 0.094 (0.097)	Data 0.000 (0.003)	Loss 0.0600 (0.0743)	Acc@1 98.438 (97.668)	Acc@5 100.000 (99.973)
Epoch: [185][192/196]	Time 0.099 (0.098)	Data 0.000 (0.002)	Loss 0.0708 (0.0754)	Acc@1 98.438 (97.616)	Acc@5 100.000 (99.978)
Max memory in training epoch: 33.3541888
[INFO] Storing checkpoint...
  89.94
Max memory: 51.4381312
 19.618s  