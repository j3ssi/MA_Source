no display found. Using non-interactive Agg backend
[3, 3, 3]
[4, 8, 16]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 120; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/room3x1/model.nn; checkpoint: ./output/experimente4/room3x1; saveModell: True; LR: 0.1
random number: 8378
Files already downloaded and verified

width: 4
module: Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
conv gefunden
(i,j): (3, 0
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
conv gefunden
(i,j): (4, 0
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
conv gefunden
(i,j): (5, 0
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=16, out_features=10, bias=True)
width: 8
module: Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 4
conv gefunden
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
conv gefunden
(i,j): (7, 0
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
conv gefunden
(i,j): (8, 0
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
conv gefunden
(i,j): (9, 0
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=16, out_features=10, bias=True)
width: 16
module: Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 4
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 8
conv gefunden
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 8
conv gefunden
(i,j): (11, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (12, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (13, 0
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=16, out_features=10, bias=True)
stagesI: {4: [(3, 0), (4, 0), (5, 0), (6, 0), (7, 0)], 8: [(8, 0), (9, 0), (10, 0), (11, 0)], 16: [(12, 0), (13, 0), (15, None)]}
stagesO: {4: [(0, None), (3, 0), (4, 0), (5, 0)], 8: [(6, 0), (7, 0), (8, 0), (9, 0)], 16: [(10, 0), (11, 0), (12, 0), (13, 0)]}
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (7): Sequential(
      (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (11): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): AdaptiveAvgPool2d(output_size=(1, 1))
    (15): Linear(in_features=16, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
deeper epoch: 60
Epoche: [1/120]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [1][0/196]	Time 0.194 (0.194)	Data 0.556 (0.556)	Loss 2.5038 (2.5038)	Acc@1 8.984 (8.984)	Acc@5 50.391 (50.391)
Epoch: [1][64/196]	Time 0.056 (0.074)	Data 0.000 (0.009)	Loss 1.7923 (2.0766)	Acc@1 25.391 (21.202)	Acc@5 86.328 (72.704)
Epoch: [1][128/196]	Time 0.067 (0.072)	Data 0.000 (0.005)	Loss 1.6857 (1.9210)	Acc@1 39.062 (26.717)	Acc@5 89.062 (79.545)
Epoch: [1][192/196]	Time 0.062 (0.070)	Data 0.000 (0.003)	Loss 1.6718 (1.8306)	Acc@1 38.672 (30.374)	Acc@5 87.891 (82.760)
after train
n1: 1 for:
wAcc: 30.52
test acc: 30.52
Epoche: [2/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [2][0/196]	Time 0.106 (0.106)	Data 0.766 (0.766)	Loss 1.6214 (1.6214)	Acc@1 35.938 (35.938)	Acc@5 89.844 (89.844)
Epoch: [2][64/196]	Time 0.094 (0.076)	Data 0.000 (0.013)	Loss 1.5046 (1.5503)	Acc@1 47.266 (41.611)	Acc@5 93.359 (91.184)
Epoch: [2][128/196]	Time 0.080 (0.073)	Data 0.000 (0.007)	Loss 1.4298 (1.5215)	Acc@1 48.047 (43.081)	Acc@5 94.531 (91.488)
Epoch: [2][192/196]	Time 0.043 (0.075)	Data 0.000 (0.005)	Loss 1.4138 (1.4899)	Acc@1 47.266 (44.319)	Acc@5 93.359 (91.971)
after train
n1: 2 for:
wAcc: 30.519999999999996
test acc: 46.81
Epoche: [3/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [3][0/196]	Time 0.085 (0.085)	Data 0.724 (0.724)	Loss 1.4388 (1.4388)	Acc@1 45.703 (45.703)	Acc@5 93.750 (93.750)
Epoch: [3][64/196]	Time 0.103 (0.093)	Data 0.000 (0.012)	Loss 1.3438 (1.3666)	Acc@1 51.953 (49.465)	Acc@5 92.969 (93.612)
Epoch: [3][128/196]	Time 0.047 (0.090)	Data 0.000 (0.006)	Loss 1.3782 (1.3468)	Acc@1 45.703 (50.318)	Acc@5 96.094 (93.753)
Epoch: [3][192/196]	Time 0.075 (0.091)	Data 0.000 (0.004)	Loss 1.3545 (1.3352)	Acc@1 50.391 (50.988)	Acc@5 90.625 (93.827)
after train
n1: 3 for:
wAcc: 38.665
test acc: 44.16
Epoche: [4/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [4][0/196]	Time 0.052 (0.052)	Data 1.062 (1.062)	Loss 1.1696 (1.1696)	Acc@1 56.641 (56.641)	Acc@5 97.266 (97.266)
Epoch: [4][64/196]	Time 0.057 (0.103)	Data 0.000 (0.017)	Loss 1.2186 (1.2707)	Acc@1 57.031 (54.056)	Acc@5 94.141 (94.201)
Epoch: [4][128/196]	Time 0.096 (0.108)	Data 0.000 (0.009)	Loss 1.2113 (1.2624)	Acc@1 60.547 (54.512)	Acc@5 94.141 (94.559)
Epoch: [4][192/196]	Time 0.065 (0.109)	Data 0.000 (0.006)	Loss 1.2242 (1.2484)	Acc@1 57.031 (55.034)	Acc@5 94.922 (94.655)
after train
n1: 4 for:
wAcc: 39.8856
test acc: 46.76
Epoche: [5/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [5][0/196]	Time 0.095 (0.095)	Data 0.907 (0.907)	Loss 1.2798 (1.2798)	Acc@1 50.391 (50.391)	Acc@5 94.922 (94.922)
Epoch: [5][64/196]	Time 0.093 (0.110)	Data 0.000 (0.014)	Loss 1.1183 (1.2116)	Acc@1 57.031 (56.106)	Acc@5 96.875 (95.108)
Epoch: [5][128/196]	Time 0.089 (0.112)	Data 0.000 (0.007)	Loss 1.3000 (1.1910)	Acc@1 51.562 (56.983)	Acc@5 95.703 (95.346)
Epoch: [5][192/196]	Time 0.154 (0.112)	Data 0.000 (0.005)	Loss 1.1487 (1.1800)	Acc@1 58.594 (57.426)	Acc@5 94.141 (95.395)
after train
n1: 5 for:
wAcc: 41.37777777777778
test acc: 52.07
Epoche: [6/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [6][0/196]	Time 0.092 (0.092)	Data 0.539 (0.539)	Loss 1.2990 (1.2990)	Acc@1 53.125 (53.125)	Acc@5 94.531 (94.531)
Epoch: [6][64/196]	Time 0.122 (0.117)	Data 0.000 (0.009)	Loss 1.0942 (1.1432)	Acc@1 62.109 (59.237)	Acc@5 95.703 (95.697)
Epoch: [6][128/196]	Time 0.084 (0.110)	Data 0.000 (0.005)	Loss 1.0932 (1.1347)	Acc@1 61.719 (59.311)	Acc@5 96.875 (95.770)
Epoch: [6][192/196]	Time 0.138 (0.108)	Data 0.000 (0.003)	Loss 1.0687 (1.1373)	Acc@1 62.891 (59.102)	Acc@5 96.484 (95.727)
after train
n1: 6 for:
wAcc: 43.67593502707206
test acc: 56.92
Epoche: [7/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [7][0/196]	Time 0.126 (0.126)	Data 0.918 (0.918)	Loss 1.0747 (1.0747)	Acc@1 58.203 (58.203)	Acc@5 96.875 (96.875)
Epoch: [7][64/196]	Time 0.087 (0.115)	Data 0.000 (0.015)	Loss 1.0647 (1.1172)	Acc@1 65.625 (60.132)	Acc@5 96.094 (95.679)
Epoch: [7][128/196]	Time 0.074 (0.118)	Data 0.000 (0.007)	Loss 1.1221 (1.1064)	Acc@1 62.109 (60.650)	Acc@5 94.141 (95.867)
Epoch: [7][192/196]	Time 0.101 (0.114)	Data 0.000 (0.005)	Loss 1.1430 (1.1013)	Acc@1 60.938 (60.838)	Acc@5 94.922 (95.875)
after train
n1: 7 for:
wAcc: 46.171533203124994
test acc: 48.04
Epoche: [8/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [8][0/196]	Time 0.078 (0.078)	Data 1.002 (1.002)	Loss 1.0160 (1.0160)	Acc@1 61.719 (61.719)	Acc@5 96.875 (96.875)
Epoch: [8][64/196]	Time 0.158 (0.106)	Data 0.000 (0.018)	Loss 1.1063 (1.0830)	Acc@1 58.594 (61.100)	Acc@5 94.922 (96.178)
Epoch: [8][128/196]	Time 0.158 (0.113)	Data 0.000 (0.009)	Loss 1.0705 (1.0749)	Acc@1 62.500 (61.576)	Acc@5 95.312 (96.121)
Epoch: [8][192/196]	Time 0.046 (0.106)	Data 0.000 (0.006)	Loss 0.9533 (1.0780)	Acc@1 66.406 (61.605)	Acc@5 98.828 (96.084)
after train
n1: 8 for:
wAcc: 45.71087740689936
test acc: 46.37
Epoche: [9/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [9][0/196]	Time 0.099 (0.099)	Data 0.657 (0.657)	Loss 1.0815 (1.0815)	Acc@1 62.500 (62.500)	Acc@5 95.703 (95.703)
Epoch: [9][64/196]	Time 0.143 (0.117)	Data 0.000 (0.013)	Loss 1.1616 (1.0772)	Acc@1 59.766 (61.310)	Acc@5 95.703 (96.436)
Epoch: [9][128/196]	Time 0.127 (0.116)	Data 0.000 (0.007)	Loss 0.9998 (1.0676)	Acc@1 65.625 (61.813)	Acc@5 96.094 (96.272)
Epoch: [9][192/196]	Time 0.090 (0.109)	Data 0.000 (0.005)	Loss 1.1386 (1.0641)	Acc@1 58.203 (61.848)	Acc@5 96.094 (96.284)
after train
n1: 9 for:
wAcc: 45.157476992000014
test acc: 50.49
Epoche: [10/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [10][0/196]	Time 0.074 (0.074)	Data 0.689 (0.689)	Loss 1.0438 (1.0438)	Acc@1 64.062 (64.062)	Acc@5 96.875 (96.875)
Epoch: [10][64/196]	Time 0.150 (0.108)	Data 0.000 (0.012)	Loss 1.0532 (1.0441)	Acc@1 65.625 (62.632)	Acc@5 95.703 (96.460)
Epoch: [10][128/196]	Time 0.127 (0.116)	Data 0.000 (0.006)	Loss 1.0231 (1.0460)	Acc@1 64.844 (62.682)	Acc@5 95.312 (96.351)
Epoch: [10][192/196]	Time 0.094 (0.113)	Data 0.000 (0.004)	Loss 0.9246 (1.0476)	Acc@1 69.922 (62.668)	Acc@5 97.266 (96.335)
after train
n1: 10 for:
wAcc: 45.579535759565736
test acc: 49.29
Epoche: [11/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [11][0/196]	Time 0.148 (0.148)	Data 1.311 (1.311)	Loss 0.9196 (0.9196)	Acc@1 67.578 (67.578)	Acc@5 98.828 (98.828)
Epoch: [11][64/196]	Time 0.100 (0.102)	Data 0.000 (0.021)	Loss 1.0694 (1.0373)	Acc@1 64.062 (62.945)	Acc@5 95.703 (96.316)
Epoch: [11][128/196]	Time 0.165 (0.111)	Data 0.000 (0.011)	Loss 0.9844 (1.0413)	Acc@1 66.016 (62.909)	Acc@5 97.656 (96.297)
Epoch: [11][192/196]	Time 0.151 (0.112)	Data 0.000 (0.008)	Loss 0.9284 (1.0375)	Acc@1 65.234 (62.963)	Acc@5 98.047 (96.448)
after train
n1: 11 for:
wAcc: 45.68389888819826
test acc: 61.37
Epoche: [12/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [12][0/196]	Time 0.073 (0.073)	Data 1.177 (1.177)	Loss 0.8923 (0.8923)	Acc@1 67.188 (67.188)	Acc@5 96.484 (96.484)
Epoch: [12][64/196]	Time 0.073 (0.110)	Data 0.000 (0.018)	Loss 1.0567 (1.0177)	Acc@1 63.281 (63.840)	Acc@5 97.266 (96.635)
Epoch: [12][128/196]	Time 0.076 (0.112)	Data 0.000 (0.009)	Loss 1.0692 (1.0156)	Acc@1 62.891 (63.793)	Acc@5 94.141 (96.696)
Epoch: [12][192/196]	Time 0.083 (0.113)	Data 0.000 (0.006)	Loss 0.9420 (1.0148)	Acc@1 65.625 (63.840)	Acc@5 97.266 (96.644)
after train
n1: 12 for:
wAcc: 47.63144498018815
test acc: 46.72
Epoche: [13/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [13][0/196]	Time 0.112 (0.112)	Data 1.238 (1.238)	Loss 1.0967 (1.0967)	Acc@1 60.547 (60.547)	Acc@5 95.312 (95.312)
Epoch: [13][64/196]	Time 0.083 (0.116)	Data 0.000 (0.019)	Loss 1.1626 (1.0107)	Acc@1 58.203 (64.363)	Acc@5 95.703 (96.719)
Epoch: [13][128/196]	Time 0.108 (0.114)	Data 0.000 (0.010)	Loss 1.1079 (1.0078)	Acc@1 63.281 (64.332)	Acc@5 96.484 (96.715)
Epoch: [13][192/196]	Time 0.098 (0.113)	Data 0.000 (0.007)	Loss 0.9471 (1.0051)	Acc@1 68.359 (64.407)	Acc@5 96.875 (96.802)
after train
n1: 13 for:
wAcc: 46.96141509883985
test acc: 56.7
Epoche: [14/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [14][0/196]	Time 0.089 (0.089)	Data 1.195 (1.195)	Loss 1.0303 (1.0303)	Acc@1 64.453 (64.453)	Acc@5 95.312 (95.312)
Epoch: [14][64/196]	Time 0.149 (0.118)	Data 0.000 (0.021)	Loss 0.9519 (1.0089)	Acc@1 67.578 (63.960)	Acc@5 98.828 (96.719)
Epoch: [14][128/196]	Time 0.154 (0.110)	Data 0.000 (0.011)	Loss 1.0072 (0.9918)	Acc@1 65.234 (64.753)	Acc@5 95.703 (96.784)
Epoch: [14][192/196]	Time 0.077 (0.109)	Data 0.000 (0.007)	Loss 1.0843 (0.9913)	Acc@1 64.844 (64.771)	Acc@5 98.047 (96.788)
after train
n1: 14 for:
wAcc: 47.81613839249417
test acc: 42.4
Epoche: [15/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [15][0/196]	Time 0.219 (0.219)	Data 0.929 (0.929)	Loss 1.1255 (1.1255)	Acc@1 62.500 (62.500)	Acc@5 94.141 (94.141)
Epoch: [15][64/196]	Time 0.067 (0.109)	Data 0.000 (0.015)	Loss 0.9132 (1.0022)	Acc@1 67.969 (64.585)	Acc@5 97.266 (96.629)
Epoch: [15][128/196]	Time 0.102 (0.114)	Data 0.000 (0.008)	Loss 1.0656 (0.9948)	Acc@1 64.062 (64.807)	Acc@5 95.703 (96.760)
Epoch: [15][192/196]	Time 0.100 (0.111)	Data 0.000 (0.005)	Loss 1.0616 (0.9934)	Acc@1 63.672 (64.860)	Acc@5 94.531 (96.812)
after train
n1: 15 for:
wAcc: 46.68753323457373
test acc: 45.84
Epoche: [16/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [16][0/196]	Time 0.106 (0.106)	Data 0.997 (0.997)	Loss 0.9825 (0.9825)	Acc@1 66.406 (66.406)	Acc@5 96.484 (96.484)
Epoch: [16][64/196]	Time 0.118 (0.109)	Data 0.000 (0.018)	Loss 0.9774 (0.9770)	Acc@1 63.672 (65.475)	Acc@5 97.656 (96.863)
Epoch: [16][128/196]	Time 0.135 (0.112)	Data 0.000 (0.009)	Loss 0.9938 (0.9701)	Acc@1 66.016 (65.789)	Acc@5 97.656 (96.990)
Epoch: [16][192/196]	Time 0.148 (0.110)	Data 0.000 (0.006)	Loss 1.1037 (0.9757)	Acc@1 63.672 (65.489)	Acc@5 95.312 (96.972)
after train
n1: 16 for:
wAcc: 46.234273223167776
test acc: 56.88
Epoche: [17/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [17][0/196]	Time 0.089 (0.089)	Data 0.827 (0.827)	Loss 0.9627 (0.9627)	Acc@1 66.016 (66.016)	Acc@5 96.875 (96.875)
Epoch: [17][64/196]	Time 0.102 (0.109)	Data 0.000 (0.013)	Loss 1.0385 (0.9716)	Acc@1 60.938 (65.349)	Acc@5 96.484 (96.791)
Epoch: [17][128/196]	Time 0.079 (0.102)	Data 0.000 (0.007)	Loss 1.0966 (0.9722)	Acc@1 63.672 (65.398)	Acc@5 95.312 (96.908)
Epoch: [17][192/196]	Time 0.144 (0.103)	Data 0.000 (0.005)	Loss 0.9027 (0.9691)	Acc@1 68.359 (65.524)	Acc@5 97.656 (96.980)
after train
n1: 17 for:
wAcc: 47.10982022302145
test acc: 49.56
Epoche: [18/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [18][0/196]	Time 0.114 (0.114)	Data 1.073 (1.073)	Loss 1.0663 (1.0663)	Acc@1 60.938 (60.938)	Acc@5 94.531 (94.531)
Epoch: [18][64/196]	Time 0.090 (0.114)	Data 0.000 (0.017)	Loss 0.8927 (0.9623)	Acc@1 71.094 (66.100)	Acc@5 96.484 (97.133)
Epoch: [18][128/196]	Time 0.096 (0.119)	Data 0.000 (0.009)	Loss 0.9834 (0.9699)	Acc@1 66.406 (65.686)	Acc@5 96.484 (97.017)
Epoch: [18][192/196]	Time 0.115 (0.118)	Data 0.000 (0.006)	Loss 1.0057 (0.9619)	Acc@1 64.453 (65.870)	Acc@5 97.266 (97.045)
after train
n1: 18 for:
wAcc: 47.03688049881656
test acc: 51.41
Epoche: [19/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [19][0/196]	Time 0.100 (0.100)	Data 0.873 (0.873)	Loss 0.9691 (0.9691)	Acc@1 63.672 (63.672)	Acc@5 97.266 (97.266)
Epoch: [19][64/196]	Time 0.081 (0.112)	Data 0.000 (0.015)	Loss 0.9280 (0.9749)	Acc@1 66.406 (65.126)	Acc@5 97.266 (96.857)
Epoch: [19][128/196]	Time 0.091 (0.110)	Data 0.000 (0.008)	Loss 0.9288 (0.9616)	Acc@1 65.625 (65.870)	Acc@5 98.438 (96.966)
Epoch: [19][192/196]	Time 0.097 (0.109)	Data 0.000 (0.005)	Loss 0.8895 (0.9538)	Acc@1 68.750 (66.178)	Acc@5 97.656 (97.069)
after train
n1: 19 for:
wAcc: 47.168132802835565
test acc: 59.05
Epoche: [20/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [20][0/196]	Time 0.099 (0.099)	Data 1.034 (1.034)	Loss 0.8895 (0.8895)	Acc@1 66.797 (66.797)	Acc@5 99.609 (99.609)
Epoch: [20][64/196]	Time 0.126 (0.110)	Data 0.000 (0.017)	Loss 0.9997 (0.9432)	Acc@1 64.062 (66.677)	Acc@5 96.875 (97.446)
Epoch: [20][128/196]	Time 0.073 (0.111)	Data 0.000 (0.009)	Loss 0.9587 (0.9424)	Acc@1 67.969 (66.870)	Acc@5 96.875 (97.238)
Epoch: [20][192/196]	Time 0.082 (0.111)	Data 0.000 (0.006)	Loss 0.8874 (0.9447)	Acc@1 71.484 (66.694)	Acc@5 98.047 (97.235)
after train
n1: 20 for:
wAcc: 48.00629184994092
test acc: 59.06
Epoche: [21/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [21][0/196]	Time 0.079 (0.079)	Data 0.827 (0.827)	Loss 0.9038 (0.9038)	Acc@1 66.016 (66.016)	Acc@5 98.047 (98.047)
Epoch: [21][64/196]	Time 0.160 (0.112)	Data 0.000 (0.013)	Loss 1.0073 (0.9406)	Acc@1 61.719 (66.538)	Acc@5 96.875 (97.368)
Epoch: [21][128/196]	Time 0.179 (0.107)	Data 0.000 (0.007)	Loss 0.9483 (0.9458)	Acc@1 66.406 (66.452)	Acc@5 97.266 (97.141)
Epoch: [21][192/196]	Time 0.097 (0.113)	Data 0.000 (0.005)	Loss 0.9223 (0.9437)	Acc@1 68.359 (66.505)	Acc@5 96.875 (97.120)
after train
n1: 21 for:
wAcc: 48.69988559471024
test acc: 59.4
Epoche: [22/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [22][0/196]	Time 0.140 (0.140)	Data 0.843 (0.843)	Loss 0.9515 (0.9515)	Acc@1 71.484 (71.484)	Acc@5 96.484 (96.484)
Epoch: [22][64/196]	Time 0.102 (0.107)	Data 0.000 (0.013)	Loss 0.8698 (0.9438)	Acc@1 68.359 (66.869)	Acc@5 97.656 (97.031)
Epoch: [22][128/196]	Time 0.097 (0.104)	Data 0.000 (0.007)	Loss 0.9228 (0.9302)	Acc@1 68.359 (67.166)	Acc@5 96.875 (97.217)
Epoch: [22][192/196]	Time 0.056 (0.106)	Data 0.000 (0.005)	Loss 0.8994 (0.9324)	Acc@1 67.188 (67.060)	Acc@5 98.047 (97.158)
after train
n1: 22 for:
wAcc: 49.31048756463538
test acc: 52.76
Epoche: [23/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [23][0/196]	Time 0.103 (0.103)	Data 1.081 (1.081)	Loss 0.9316 (0.9316)	Acc@1 63.672 (63.672)	Acc@5 98.047 (98.047)
Epoch: [23][64/196]	Time 0.133 (0.127)	Data 0.000 (0.017)	Loss 0.8663 (0.9301)	Acc@1 68.750 (67.151)	Acc@5 98.438 (97.163)
Epoch: [23][128/196]	Time 0.075 (0.114)	Data 0.000 (0.009)	Loss 0.9574 (0.9274)	Acc@1 66.406 (67.303)	Acc@5 98.438 (97.272)
Epoch: [23][192/196]	Time 0.072 (0.110)	Data 0.000 (0.006)	Loss 0.8760 (0.9318)	Acc@1 65.234 (67.188)	Acc@5 97.266 (97.183)
after train
n1: 23 for:
wAcc: 49.27448389640757
test acc: 56.73
Epoche: [24/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [24][0/196]	Time 0.080 (0.080)	Data 1.152 (1.152)	Loss 0.9116 (0.9116)	Acc@1 67.969 (67.969)	Acc@5 97.656 (97.656)
Epoch: [24][64/196]	Time 0.093 (0.110)	Data 0.000 (0.018)	Loss 0.9243 (0.9296)	Acc@1 67.578 (67.548)	Acc@5 96.484 (97.314)
Epoch: [24][128/196]	Time 0.085 (0.109)	Data 0.000 (0.009)	Loss 0.9015 (0.9301)	Acc@1 69.922 (67.551)	Acc@5 98.047 (97.275)
Epoch: [24][192/196]	Time 0.157 (0.109)	Data 0.000 (0.006)	Loss 0.8515 (0.9285)	Acc@1 69.922 (67.617)	Acc@5 97.656 (97.270)
after train
n1: 24 for:
wAcc: 49.56849263042078
test acc: 50.29
Epoche: [25/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [25][0/196]	Time 0.074 (0.074)	Data 1.011 (1.011)	Loss 0.9672 (0.9672)	Acc@1 63.672 (63.672)	Acc@5 98.047 (98.047)
Epoch: [25][64/196]	Time 0.076 (0.108)	Data 0.000 (0.016)	Loss 0.8975 (0.9371)	Acc@1 69.922 (67.067)	Acc@5 97.656 (97.326)
Epoch: [25][128/196]	Time 0.161 (0.113)	Data 0.000 (0.008)	Loss 0.9631 (0.9220)	Acc@1 66.406 (67.560)	Acc@5 96.875 (97.366)
Epoch: [25][192/196]	Time 0.082 (0.112)	Data 0.000 (0.006)	Loss 0.8916 (0.9202)	Acc@1 69.141 (67.661)	Acc@5 96.484 (97.324)
after train
n1: 25 for:
wAcc: 49.32814262984194
test acc: 58.49
Epoche: [26/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [26][0/196]	Time 0.119 (0.119)	Data 1.273 (1.273)	Loss 0.8254 (0.8254)	Acc@1 74.609 (74.609)	Acc@5 97.266 (97.266)
Epoch: [26][64/196]	Time 0.134 (0.106)	Data 0.000 (0.020)	Loss 0.9627 (0.9277)	Acc@1 65.625 (67.296)	Acc@5 95.703 (97.163)
Epoch: [26][128/196]	Time 0.123 (0.106)	Data 0.000 (0.010)	Loss 0.9020 (0.9193)	Acc@1 65.625 (67.378)	Acc@5 98.828 (97.341)
Epoch: [26][192/196]	Time 0.085 (0.107)	Data 0.000 (0.007)	Loss 0.9297 (0.9201)	Acc@1 66.016 (67.432)	Acc@5 97.266 (97.260)
after train
n1: 26 for:
wAcc: 49.735261454446665
test acc: 58.84
Epoche: [27/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [27][0/196]	Time 0.062 (0.062)	Data 1.052 (1.052)	Loss 0.9615 (0.9615)	Acc@1 63.281 (63.281)	Acc@5 97.266 (97.266)
Epoch: [27][64/196]	Time 0.102 (0.102)	Data 0.000 (0.017)	Loss 0.8741 (0.9080)	Acc@1 69.531 (68.029)	Acc@5 98.438 (97.206)
Epoch: [27][128/196]	Time 0.109 (0.101)	Data 0.000 (0.009)	Loss 0.9124 (0.9131)	Acc@1 67.188 (67.866)	Acc@5 96.875 (97.157)
Epoch: [27][192/196]	Time 0.115 (0.101)	Data 0.000 (0.006)	Loss 0.9336 (0.9140)	Acc@1 67.188 (67.876)	Acc@5 97.656 (97.201)
after train
n1: 27 for:
wAcc: 50.1139659110795
test acc: 57.4
Epoche: [28/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [28][0/196]	Time 0.106 (0.106)	Data 1.222 (1.222)	Loss 0.8622 (0.8622)	Acc@1 69.141 (69.141)	Acc@5 96.875 (96.875)
Epoch: [28][64/196]	Time 0.066 (0.105)	Data 0.000 (0.019)	Loss 0.8292 (0.8975)	Acc@1 66.797 (68.275)	Acc@5 98.828 (97.446)
Epoch: [28][128/196]	Time 0.045 (0.107)	Data 0.000 (0.010)	Loss 0.8724 (0.9046)	Acc@1 68.750 (68.329)	Acc@5 98.438 (97.359)
Epoch: [28][192/196]	Time 0.094 (0.105)	Data 0.000 (0.007)	Loss 0.8785 (0.9017)	Acc@1 69.922 (68.333)	Acc@5 97.266 (97.369)
after train
n1: 28 for:
wAcc: 50.34597891217998
test acc: 43.13
Epoche: [29/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [29][0/196]	Time 0.185 (0.185)	Data 0.672 (0.672)	Loss 0.9296 (0.9296)	Acc@1 67.578 (67.578)	Acc@5 97.266 (97.266)
Epoch: [29][64/196]	Time 0.113 (0.119)	Data 0.000 (0.011)	Loss 0.9512 (0.9060)	Acc@1 66.406 (68.696)	Acc@5 97.656 (97.398)
Epoch: [29][128/196]	Time 0.106 (0.118)	Data 0.000 (0.006)	Loss 0.7902 (0.9043)	Acc@1 71.875 (68.632)	Acc@5 99.219 (97.366)
Epoch: [29][192/196]	Time 0.059 (0.116)	Data 0.000 (0.004)	Loss 0.8459 (0.9089)	Acc@1 69.922 (68.390)	Acc@5 95.703 (97.401)
after train
n1: 29 for:
wAcc: 49.60029736836494
test acc: 63.22
Epoche: [30/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [30][0/196]	Time 0.060 (0.060)	Data 1.229 (1.229)	Loss 0.9191 (0.9191)	Acc@1 63.281 (63.281)	Acc@5 98.047 (98.047)
Epoch: [30][64/196]	Time 0.109 (0.105)	Data 0.000 (0.021)	Loss 0.8902 (0.9146)	Acc@1 68.750 (67.975)	Acc@5 97.266 (97.422)
Epoch: [30][128/196]	Time 0.140 (0.109)	Data 0.000 (0.011)	Loss 0.9947 (0.9082)	Acc@1 63.281 (68.384)	Acc@5 97.266 (97.387)
Epoch: [30][192/196]	Time 0.050 (0.107)	Data 0.000 (0.007)	Loss 0.8365 (0.9079)	Acc@1 72.266 (68.416)	Acc@5 97.266 (97.347)
after train
n1: 30 for:
wAcc: 50.249258347024266
test acc: 56.83
Epoche: [31/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [31][0/196]	Time 0.140 (0.140)	Data 0.982 (0.982)	Loss 0.7503 (0.7503)	Acc@1 74.219 (74.219)	Acc@5 98.438 (98.438)
Epoch: [31][64/196]	Time 0.137 (0.109)	Data 0.000 (0.015)	Loss 0.9007 (0.8880)	Acc@1 69.141 (68.726)	Acc@5 98.047 (97.440)
Epoch: [31][128/196]	Time 0.131 (0.109)	Data 0.000 (0.008)	Loss 0.8489 (0.8898)	Acc@1 67.578 (68.586)	Acc@5 97.656 (97.481)
Epoch: [31][192/196]	Time 0.079 (0.109)	Data 0.000 (0.005)	Loss 0.8583 (0.8862)	Acc@1 67.969 (68.849)	Acc@5 97.656 (97.535)
after train
n1: 30 for:
wAcc: 53.0287300921228
test acc: 53.75
Epoche: [32/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [32][0/196]	Time 0.097 (0.097)	Data 1.339 (1.339)	Loss 0.9209 (0.9209)	Acc@1 73.828 (73.828)	Acc@5 97.266 (97.266)
Epoch: [32][64/196]	Time 0.057 (0.118)	Data 0.000 (0.024)	Loss 0.7804 (0.8866)	Acc@1 71.484 (68.990)	Acc@5 98.047 (97.584)
Epoch: [32][128/196]	Time 0.159 (0.117)	Data 0.000 (0.012)	Loss 0.9624 (0.8881)	Acc@1 67.188 (68.944)	Acc@5 97.656 (97.574)
Epoch: [32][192/196]	Time 0.115 (0.113)	Data 0.000 (0.008)	Loss 0.8940 (0.8937)	Acc@1 69.141 (68.746)	Acc@5 96.875 (97.500)
after train
n1: 30 for:
wAcc: 52.69217550787276
test acc: 57.88
Epoche: [33/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [33][0/196]	Time 0.101 (0.101)	Data 0.740 (0.740)	Loss 0.9028 (0.9028)	Acc@1 69.141 (69.141)	Acc@5 97.656 (97.656)
Epoch: [33][64/196]	Time 0.070 (0.103)	Data 0.000 (0.013)	Loss 0.9979 (0.8893)	Acc@1 62.109 (68.870)	Acc@5 97.266 (97.572)
Epoch: [33][128/196]	Time 0.109 (0.105)	Data 0.000 (0.007)	Loss 0.8830 (0.8926)	Acc@1 69.531 (68.614)	Acc@5 97.266 (97.481)
Epoch: [33][192/196]	Time 0.073 (0.108)	Data 0.000 (0.005)	Loss 0.9147 (0.8933)	Acc@1 69.141 (68.795)	Acc@5 97.266 (97.407)
after train
n1: 30 for:
wAcc: 53.402733911035135
test acc: 55.74
Epoche: [34/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [34][0/196]	Time 0.174 (0.174)	Data 0.898 (0.898)	Loss 0.9140 (0.9140)	Acc@1 66.406 (66.406)	Acc@5 97.656 (97.656)
Epoch: [34][64/196]	Time 0.089 (0.109)	Data 0.000 (0.014)	Loss 0.8058 (0.8844)	Acc@1 72.656 (69.075)	Acc@5 97.656 (97.434)
Epoch: [34][128/196]	Time 0.109 (0.111)	Data 0.000 (0.007)	Loss 0.9914 (0.8860)	Acc@1 65.234 (69.101)	Acc@5 97.266 (97.447)
Epoch: [34][192/196]	Time 0.086 (0.104)	Data 0.000 (0.005)	Loss 1.0468 (0.8847)	Acc@1 61.719 (69.145)	Acc@5 96.094 (97.436)
after train
n1: 30 for:
wAcc: 54.32114714057342
test acc: 57.79
Epoche: [35/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [35][0/196]	Time 0.157 (0.157)	Data 1.301 (1.301)	Loss 0.9152 (0.9152)	Acc@1 66.797 (66.797)	Acc@5 97.266 (97.266)
Epoch: [35][64/196]	Time 0.130 (0.111)	Data 0.000 (0.021)	Loss 0.8310 (0.8870)	Acc@1 69.141 (68.846)	Acc@5 96.094 (97.428)
Epoch: [35][128/196]	Time 0.187 (0.119)	Data 0.000 (0.011)	Loss 1.0111 (0.8812)	Acc@1 65.234 (68.986)	Acc@5 96.875 (97.453)
Epoch: [35][192/196]	Time 0.108 (0.115)	Data 0.000 (0.007)	Loss 0.8680 (0.8833)	Acc@1 68.359 (69.013)	Acc@5 96.484 (97.448)
after train
n1: 30 for:
wAcc: 55.24606765187426
test acc: 40.1
Epoche: [36/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [36][0/196]	Time 0.145 (0.145)	Data 1.153 (1.153)	Loss 0.8594 (0.8594)	Acc@1 67.188 (67.188)	Acc@5 97.266 (97.266)
Epoch: [36][64/196]	Time 0.116 (0.107)	Data 0.000 (0.018)	Loss 0.7223 (0.8775)	Acc@1 75.781 (68.690)	Acc@5 96.484 (97.656)
Epoch: [36][128/196]	Time 0.066 (0.114)	Data 0.000 (0.009)	Loss 0.9107 (0.8832)	Acc@1 69.922 (68.783)	Acc@5 97.656 (97.511)
Epoch: [36][192/196]	Time 0.085 (0.110)	Data 0.000 (0.006)	Loss 0.9355 (0.8862)	Acc@1 66.406 (68.855)	Acc@5 98.438 (97.482)
after train
n1: 30 for:
wAcc: 52.98519536859751
test acc: 65.73
Epoche: [37/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [37][0/196]	Time 0.098 (0.098)	Data 0.665 (0.665)	Loss 0.9537 (0.9537)	Acc@1 65.234 (65.234)	Acc@5 97.266 (97.266)
Epoch: [37][64/196]	Time 0.074 (0.096)	Data 0.000 (0.012)	Loss 0.7525 (0.8785)	Acc@1 70.312 (69.231)	Acc@5 99.219 (97.536)
Epoch: [37][128/196]	Time 0.123 (0.106)	Data 0.000 (0.006)	Loss 0.9475 (0.8724)	Acc@1 67.969 (69.386)	Acc@5 97.656 (97.553)
Epoch: [37][192/196]	Time 0.138 (0.107)	Data 0.000 (0.004)	Loss 0.8510 (0.8726)	Acc@1 68.750 (69.491)	Acc@5 98.047 (97.458)
after train
n1: 30 for:
wAcc: 53.56602302809232
test acc: 55.19
Epoche: [38/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [38][0/196]	Time 0.125 (0.125)	Data 0.924 (0.924)	Loss 0.9107 (0.9107)	Acc@1 66.016 (66.016)	Acc@5 96.484 (96.484)
Epoch: [38][64/196]	Time 0.095 (0.118)	Data 0.000 (0.014)	Loss 0.8982 (0.8804)	Acc@1 68.750 (69.129)	Acc@5 98.438 (97.656)
Epoch: [38][128/196]	Time 0.043 (0.122)	Data 0.000 (0.008)	Loss 0.9321 (0.8714)	Acc@1 62.891 (69.471)	Acc@5 97.656 (97.562)
Epoch: [38][192/196]	Time 0.080 (0.115)	Data 0.000 (0.005)	Loss 0.8833 (0.8765)	Acc@1 70.312 (69.339)	Acc@5 96.484 (97.573)
after train
n1: 30 for:
wAcc: 54.26638935179817
test acc: 57.79
Epoche: [39/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [39][0/196]	Time 0.098 (0.098)	Data 1.190 (1.190)	Loss 0.8743 (0.8743)	Acc@1 69.531 (69.531)	Acc@5 96.484 (96.484)
Epoch: [39][64/196]	Time 0.087 (0.109)	Data 0.000 (0.019)	Loss 0.8652 (0.8672)	Acc@1 65.234 (69.345)	Acc@5 97.656 (97.680)
Epoch: [39][128/196]	Time 0.055 (0.109)	Data 0.000 (0.010)	Loss 0.8396 (0.8770)	Acc@1 70.312 (68.938)	Acc@5 96.484 (97.453)
Epoch: [39][192/196]	Time 0.064 (0.107)	Data 0.000 (0.007)	Loss 0.9828 (0.8800)	Acc@1 64.062 (69.078)	Acc@5 97.266 (97.399)
after train
n1: 30 for:
wAcc: 54.32024520234549
test acc: 60.64
Epoche: [40/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [40][0/196]	Time 0.107 (0.107)	Data 0.786 (0.786)	Loss 0.7578 (0.7578)	Acc@1 72.266 (72.266)	Acc@5 98.047 (98.047)
Epoch: [40][64/196]	Time 0.074 (0.110)	Data 0.000 (0.013)	Loss 0.7859 (0.8686)	Acc@1 74.219 (69.591)	Acc@5 98.438 (97.746)
Epoch: [40][128/196]	Time 0.112 (0.112)	Data 0.000 (0.007)	Loss 0.8172 (0.8687)	Acc@1 70.312 (69.831)	Acc@5 97.656 (97.523)
Epoch: [40][192/196]	Time 0.115 (0.112)	Data 0.000 (0.004)	Loss 0.9017 (0.8671)	Acc@1 69.922 (69.766)	Acc@5 97.656 (97.553)
after train
n1: 30 for:
wAcc: 56.47427492981768
test acc: 64.29
Epoche: [41/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [41][0/196]	Time 0.113 (0.113)	Data 0.985 (0.985)	Loss 0.9794 (0.9794)	Acc@1 68.750 (68.750)	Acc@5 96.875 (96.875)
Epoch: [41][64/196]	Time 0.115 (0.108)	Data 0.000 (0.015)	Loss 0.8844 (0.8691)	Acc@1 68.750 (69.832)	Acc@5 97.656 (97.464)
Epoch: [41][128/196]	Time 0.088 (0.108)	Data 0.000 (0.008)	Loss 0.7967 (0.8690)	Acc@1 68.750 (69.892)	Acc@5 98.047 (97.493)
Epoch: [41][192/196]	Time 0.091 (0.109)	Data 0.000 (0.005)	Loss 0.9925 (0.8676)	Acc@1 65.234 (69.837)	Acc@5 97.266 (97.523)
after train
n1: 30 for:
wAcc: 54.86068844331745
test acc: 48.92
Epoche: [42/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [42][0/196]	Time 0.145 (0.145)	Data 1.082 (1.082)	Loss 0.9028 (0.9028)	Acc@1 69.922 (69.922)	Acc@5 96.484 (96.484)
Epoch: [42][64/196]	Time 0.213 (0.115)	Data 0.000 (0.017)	Loss 0.8973 (0.8784)	Acc@1 69.531 (69.399)	Acc@5 99.219 (97.554)
Epoch: [42][128/196]	Time 0.052 (0.111)	Data 0.000 (0.009)	Loss 0.7716 (0.8757)	Acc@1 73.047 (69.534)	Acc@5 97.656 (97.550)
Epoch: [42][192/196]	Time 0.085 (0.114)	Data 0.000 (0.006)	Loss 0.9279 (0.8717)	Acc@1 65.625 (69.547)	Acc@5 98.047 (97.579)
after train
n1: 30 for:
wAcc: 55.92014256245228
test acc: 48.87
Epoche: [43/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [43][0/196]	Time 0.172 (0.172)	Data 1.309 (1.309)	Loss 0.9160 (0.9160)	Acc@1 68.359 (68.359)	Acc@5 98.828 (98.828)
Epoch: [43][64/196]	Time 0.074 (0.113)	Data 0.000 (0.020)	Loss 0.7551 (0.8679)	Acc@1 74.219 (69.724)	Acc@5 98.828 (97.668)
Epoch: [43][128/196]	Time 0.105 (0.106)	Data 0.000 (0.011)	Loss 0.9126 (0.8717)	Acc@1 68.359 (69.658)	Acc@5 96.094 (97.605)
Epoch: [43][192/196]	Time 0.083 (0.110)	Data 0.000 (0.007)	Loss 1.0554 (0.8797)	Acc@1 63.281 (69.349)	Acc@5 96.484 (97.500)
after train
n1: 30 for:
wAcc: 53.398064386623524
test acc: 58.36
Epoche: [44/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [44][0/196]	Time 0.061 (0.061)	Data 0.719 (0.719)	Loss 0.9241 (0.9241)	Acc@1 66.016 (66.016)	Acc@5 98.438 (98.438)
Epoch: [44][64/196]	Time 0.067 (0.106)	Data 0.000 (0.011)	Loss 0.8386 (0.8584)	Acc@1 71.875 (70.523)	Acc@5 96.094 (97.680)
Epoch: [44][128/196]	Time 0.093 (0.103)	Data 0.000 (0.006)	Loss 0.8780 (0.8587)	Acc@1 67.578 (70.246)	Acc@5 98.438 (97.629)
Epoch: [44][192/196]	Time 0.159 (0.108)	Data 0.000 (0.004)	Loss 0.7243 (0.8607)	Acc@1 75.391 (70.155)	Acc@5 97.656 (97.636)
after train
n1: 30 for:
wAcc: 54.21548102182153
test acc: 61.94
Epoche: [45/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [45][0/196]	Time 0.076 (0.076)	Data 1.309 (1.309)	Loss 0.8928 (0.8928)	Acc@1 68.750 (68.750)	Acc@5 96.875 (96.875)
Epoch: [45][64/196]	Time 0.148 (0.122)	Data 0.000 (0.020)	Loss 0.7446 (0.8606)	Acc@1 75.781 (69.615)	Acc@5 99.219 (97.710)
Epoch: [45][128/196]	Time 0.127 (0.110)	Data 0.000 (0.010)	Loss 0.8677 (0.8663)	Acc@1 68.750 (69.531)	Acc@5 98.828 (97.644)
Epoch: [45][192/196]	Time 0.072 (0.113)	Data 0.000 (0.007)	Loss 0.9116 (0.8667)	Acc@1 71.094 (69.551)	Acc@5 96.094 (97.600)
after train
n1: 30 for:
wAcc: 56.309796676891665
test acc: 56.32
Epoche: [46/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [46][0/196]	Time 0.072 (0.072)	Data 1.236 (1.236)	Loss 0.7884 (0.7884)	Acc@1 69.922 (69.922)	Acc@5 98.047 (98.047)
Epoch: [46][64/196]	Time 0.140 (0.106)	Data 0.002 (0.020)	Loss 0.8448 (0.8613)	Acc@1 74.609 (70.258)	Acc@5 98.828 (97.584)
Epoch: [46][128/196]	Time 0.096 (0.108)	Data 0.000 (0.010)	Loss 0.8074 (0.8691)	Acc@1 67.578 (69.707)	Acc@5 98.438 (97.514)
Epoch: [46][192/196]	Time 0.069 (0.109)	Data 0.000 (0.007)	Loss 0.8327 (0.8626)	Acc@1 73.828 (69.922)	Acc@5 98.828 (97.583)
after train
n1: 30 for:
wAcc: 55.252264356783705
test acc: 46.54
Epoche: [47/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [47][0/196]	Time 0.142 (0.142)	Data 0.934 (0.934)	Loss 0.7320 (0.7320)	Acc@1 73.047 (73.047)	Acc@5 99.219 (99.219)
Epoch: [47][64/196]	Time 0.094 (0.119)	Data 0.000 (0.015)	Loss 0.8090 (0.8725)	Acc@1 71.875 (69.964)	Acc@5 98.438 (97.554)
Epoch: [47][128/196]	Time 0.102 (0.112)	Data 0.000 (0.008)	Loss 0.8217 (0.8716)	Acc@1 68.750 (69.919)	Acc@5 96.875 (97.453)
Epoch: [47][192/196]	Time 0.045 (0.111)	Data 0.000 (0.005)	Loss 0.9745 (0.8651)	Acc@1 65.625 (70.230)	Acc@5 95.312 (97.545)
after train
n1: 30 for:
wAcc: 54.9576216662777
test acc: 63.38
Epoche: [48/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [48][0/196]	Time 0.108 (0.108)	Data 0.759 (0.759)	Loss 0.8934 (0.8934)	Acc@1 69.531 (69.531)	Acc@5 96.875 (96.875)
Epoch: [48][64/196]	Time 0.077 (0.111)	Data 0.000 (0.013)	Loss 0.8017 (0.8750)	Acc@1 69.531 (69.261)	Acc@5 97.266 (97.446)
Epoch: [48][128/196]	Time 0.067 (0.106)	Data 0.000 (0.007)	Loss 0.8255 (0.8658)	Acc@1 72.656 (69.834)	Acc@5 96.484 (97.508)
Epoch: [48][192/196]	Time 0.090 (0.110)	Data 0.000 (0.005)	Loss 0.9209 (0.8582)	Acc@1 67.188 (70.171)	Acc@5 98.047 (97.622)
after train
n1: 30 for:
wAcc: 56.60545121095054
test acc: 58.17
Epoche: [49/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [49][0/196]	Time 0.058 (0.058)	Data 1.089 (1.089)	Loss 0.8007 (0.8007)	Acc@1 68.750 (68.750)	Acc@5 98.828 (98.828)
Epoch: [49][64/196]	Time 0.148 (0.117)	Data 0.000 (0.017)	Loss 0.7019 (0.8711)	Acc@1 76.953 (69.615)	Acc@5 97.656 (97.278)
Epoch: [49][128/196]	Time 0.127 (0.109)	Data 0.000 (0.009)	Loss 0.7761 (0.8659)	Acc@1 74.609 (69.967)	Acc@5 98.438 (97.469)
Epoch: [49][192/196]	Time 0.122 (0.112)	Data 0.000 (0.006)	Loss 0.8181 (0.8610)	Acc@1 71.875 (70.045)	Acc@5 99.219 (97.565)
after train
n1: 30 for:
wAcc: 56.70783545807455
test acc: 60.97
Epoche: [50/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [50][0/196]	Time 0.176 (0.176)	Data 1.083 (1.083)	Loss 0.7881 (0.7881)	Acc@1 73.438 (73.438)	Acc@5 97.656 (97.656)
Epoch: [50][64/196]	Time 0.078 (0.117)	Data 0.000 (0.017)	Loss 0.7398 (0.8669)	Acc@1 75.391 (69.766)	Acc@5 97.656 (97.662)
Epoch: [50][128/196]	Time 0.068 (0.109)	Data 0.000 (0.009)	Loss 0.8238 (0.8540)	Acc@1 71.875 (70.061)	Acc@5 97.266 (97.711)
Epoch: [50][192/196]	Time 0.085 (0.113)	Data 0.000 (0.006)	Loss 0.8308 (0.8562)	Acc@1 72.266 (69.977)	Acc@5 97.266 (97.731)
after train
n1: 30 for:
wAcc: 57.03196474508072
test acc: 57.91
Epoche: [51/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [51][0/196]	Time 0.147 (0.147)	Data 0.957 (0.957)	Loss 1.0095 (1.0095)	Acc@1 64.062 (64.062)	Acc@5 96.484 (96.484)
Epoch: [51][64/196]	Time 0.091 (0.112)	Data 0.000 (0.017)	Loss 0.8524 (0.8577)	Acc@1 68.359 (70.535)	Acc@5 97.266 (97.602)
Epoch: [51][128/196]	Time 0.116 (0.116)	Data 0.000 (0.009)	Loss 0.7561 (0.8628)	Acc@1 75.391 (70.340)	Acc@5 98.438 (97.493)
Epoch: [51][192/196]	Time 0.108 (0.113)	Data 0.000 (0.006)	Loss 0.9519 (0.8542)	Acc@1 67.578 (70.434)	Acc@5 96.484 (97.616)
after train
n1: 30 for:
wAcc: 56.12872344078897
test acc: 58.76
Epoche: [52/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [52][0/196]	Time 0.132 (0.132)	Data 0.850 (0.850)	Loss 0.8019 (0.8019)	Acc@1 72.266 (72.266)	Acc@5 97.266 (97.266)
Epoch: [52][64/196]	Time 0.136 (0.113)	Data 0.000 (0.014)	Loss 0.7775 (0.8751)	Acc@1 73.047 (69.645)	Acc@5 97.656 (97.596)
Epoch: [52][128/196]	Time 0.085 (0.114)	Data 0.000 (0.007)	Loss 0.9383 (0.8589)	Acc@1 68.750 (70.149)	Acc@5 97.656 (97.741)
Epoch: [52][192/196]	Time 0.116 (0.112)	Data 0.000 (0.005)	Loss 0.7970 (0.8574)	Acc@1 72.266 (70.155)	Acc@5 99.219 (97.774)
after train
n1: 30 for:
wAcc: 56.87239260105698
test acc: 56.64
Epoche: [53/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [53][0/196]	Time 0.155 (0.155)	Data 0.852 (0.852)	Loss 0.8874 (0.8874)	Acc@1 66.797 (66.797)	Acc@5 98.828 (98.828)
Epoch: [53][64/196]	Time 0.126 (0.099)	Data 0.000 (0.013)	Loss 0.9546 (0.8549)	Acc@1 69.922 (70.312)	Acc@5 95.703 (97.614)
Epoch: [53][128/196]	Time 0.048 (0.106)	Data 0.000 (0.007)	Loss 0.8314 (0.8558)	Acc@1 69.922 (70.207)	Acc@5 98.047 (97.665)
Epoch: [53][192/196]	Time 0.147 (0.109)	Data 0.000 (0.005)	Loss 0.9461 (0.8515)	Acc@1 66.797 (70.377)	Acc@5 95.312 (97.709)
after train
n1: 30 for:
wAcc: 55.92642310137665
test acc: 57.45
Epoche: [54/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [54][0/196]	Time 0.083 (0.083)	Data 0.891 (0.891)	Loss 1.0057 (1.0057)	Acc@1 65.234 (65.234)	Acc@5 96.094 (96.094)
Epoch: [54][64/196]	Time 0.137 (0.108)	Data 0.000 (0.014)	Loss 0.9022 (0.8662)	Acc@1 66.797 (69.459)	Acc@5 95.703 (97.644)
Epoch: [54][128/196]	Time 0.147 (0.107)	Data 0.000 (0.007)	Loss 0.9229 (0.8613)	Acc@1 68.750 (69.901)	Acc@5 98.438 (97.620)
Epoch: [54][192/196]	Time 0.130 (0.105)	Data 0.000 (0.005)	Loss 0.9038 (0.8613)	Acc@1 67.969 (69.995)	Acc@5 95.703 (97.596)
after train
n1: 30 for:
wAcc: 57.21012315455074
test acc: 57.8
Epoche: [55/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [55][0/196]	Time 0.102 (0.102)	Data 0.902 (0.902)	Loss 0.8575 (0.8575)	Acc@1 73.828 (73.828)	Acc@5 95.312 (95.312)
Epoch: [55][64/196]	Time 0.108 (0.121)	Data 0.000 (0.015)	Loss 0.8962 (0.8386)	Acc@1 71.484 (70.691)	Acc@5 97.266 (97.855)
Epoch: [55][128/196]	Time 0.081 (0.117)	Data 0.000 (0.008)	Loss 0.8553 (0.8498)	Acc@1 70.312 (70.203)	Acc@5 98.438 (97.729)
Epoch: [55][192/196]	Time 0.088 (0.117)	Data 0.000 (0.005)	Loss 0.7884 (0.8534)	Acc@1 71.875 (70.110)	Acc@5 98.438 (97.735)
after train
n1: 30 for:
wAcc: 57.2987762702599
test acc: 48.76
Epoche: [56/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [56][0/196]	Time 0.126 (0.126)	Data 0.874 (0.874)	Loss 0.8392 (0.8392)	Acc@1 69.922 (69.922)	Acc@5 97.266 (97.266)
Epoch: [56][64/196]	Time 0.191 (0.114)	Data 0.000 (0.014)	Loss 0.8304 (0.8504)	Acc@1 71.875 (70.066)	Acc@5 97.266 (97.662)
Epoch: [56][128/196]	Time 0.143 (0.115)	Data 0.000 (0.007)	Loss 0.8309 (0.8521)	Acc@1 73.828 (70.197)	Acc@5 96.484 (97.717)
Epoch: [56][192/196]	Time 0.037 (0.110)	Data 0.000 (0.005)	Loss 1.0132 (0.8540)	Acc@1 67.969 (70.165)	Acc@5 96.875 (97.717)
after train
n1: 30 for:
wAcc: 56.539718836200436
test acc: 44.69
Epoche: [57/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [57][0/196]	Time 0.086 (0.086)	Data 1.113 (1.113)	Loss 0.8361 (0.8361)	Acc@1 69.922 (69.922)	Acc@5 96.484 (96.484)
Epoch: [57][64/196]	Time 0.060 (0.105)	Data 0.000 (0.018)	Loss 0.7793 (0.8392)	Acc@1 71.875 (70.691)	Acc@5 98.047 (97.548)
Epoch: [57][128/196]	Time 0.162 (0.102)	Data 0.000 (0.009)	Loss 0.8153 (0.8416)	Acc@1 70.312 (70.673)	Acc@5 98.047 (97.635)
Epoch: [57][192/196]	Time 0.036 (0.105)	Data 0.000 (0.006)	Loss 1.0755 (0.8496)	Acc@1 65.625 (70.470)	Acc@5 95.703 (97.666)
after train
n1: 30 for:
wAcc: 53.71232742491168
test acc: 56.65
Epoche: [58/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [58][0/196]	Time 0.119 (0.119)	Data 0.930 (0.930)	Loss 0.9066 (0.9066)	Acc@1 67.188 (67.188)	Acc@5 96.094 (96.094)
Epoch: [58][64/196]	Time 0.129 (0.109)	Data 0.000 (0.015)	Loss 0.7940 (0.8594)	Acc@1 70.703 (70.066)	Acc@5 98.438 (97.764)
Epoch: [58][128/196]	Time 0.150 (0.111)	Data 0.000 (0.008)	Loss 0.8697 (0.8456)	Acc@1 72.656 (70.670)	Acc@5 97.656 (97.744)
Epoch: [58][192/196]	Time 0.186 (0.113)	Data 0.000 (0.005)	Loss 0.8403 (0.8456)	Acc@1 68.750 (70.630)	Acc@5 99.219 (97.697)
after train
n1: 30 for:
wAcc: 56.80609637283088
test acc: 47.14
Epoche: [59/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [59][0/196]	Time 0.053 (0.053)	Data 0.948 (0.948)	Loss 0.8306 (0.8306)	Acc@1 72.656 (72.656)	Acc@5 97.656 (97.656)
Epoch: [59][64/196]	Time 0.107 (0.112)	Data 0.000 (0.015)	Loss 0.8908 (0.8313)	Acc@1 66.406 (70.763)	Acc@5 96.484 (97.819)
Epoch: [59][128/196]	Time 0.089 (0.111)	Data 0.000 (0.008)	Loss 0.7873 (0.8385)	Acc@1 72.656 (70.791)	Acc@5 98.047 (97.720)
Epoch: [59][192/196]	Time 0.107 (0.109)	Data 0.000 (0.005)	Loss 0.7955 (0.8436)	Acc@1 73.438 (70.517)	Acc@5 97.656 (97.725)
after train
n1: 30 for:
wAcc: 55.25872890122083
test acc: 64.65
Epoche: [60/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [60][0/196]	Time 0.106 (0.106)	Data 1.184 (1.184)	Loss 0.8657 (0.8657)	Acc@1 69.141 (69.141)	Acc@5 98.047 (98.047)
Epoch: [60][64/196]	Time 0.162 (0.115)	Data 0.000 (0.018)	Loss 0.8788 (0.8435)	Acc@1 67.188 (70.589)	Acc@5 97.656 (97.686)
Epoch: [60][128/196]	Time 0.104 (0.113)	Data 0.000 (0.009)	Loss 0.7517 (0.8491)	Acc@1 74.219 (70.488)	Acc@5 96.484 (97.671)
Epoch: [60][192/196]	Time 0.076 (0.111)	Data 0.000 (0.006)	Loss 0.7688 (0.8515)	Acc@1 74.609 (70.250)	Acc@5 98.047 (97.670)
after train
n1: 30 for:
wAcc: 55.41936776289838
test acc: 31.74


now deeper1
i: 3
j: 0; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 3; i0=: 4; i1=: 4
i: 4
j: 0; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 4; i0=: 4; i1=: 4
i: 5
j: 0; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 5; i0=: 4; i1=: 4
i: 6
j: 0; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)]
seq[j]: Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 5; 
seq: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
i: 6; i0=: 8; i1=: 4
skip: 7
i: 8
j: 0; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 8; i0=: 8; i1=: 8
i: 9
j: 0; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 9; i0=: 8; i1=: 8
i: 10
j: 0; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)]
seq[j]: Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 5; 
seq: [Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
i: 10; i0=: 16; i1=: 8
skip: 11
i: 12
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 12; i0=: 16; i1=: 16
i: 13
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 13; i0=: 16; i1=: 16
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (7): Sequential(
      (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (11): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): AdaptiveAvgPool2d(output_size=(1, 1))
    (15): Linear(in_features=16, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Nums: [[1, 1, 1], [2, 1, 1], [2, 1, 1]]
num: 10; numofstages: 3, listofBlocks: [3, 3, 3], layers in blocj: 1
model.para: <generator object Module.named_parameters at 0x7fbbe1b2bca8>
Epoche: [61/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [61][0/196]	Time 0.138 (0.138)	Data 1.013 (1.013)	Loss 2.5458 (2.5458)	Acc@1 9.375 (9.375)	Acc@5 50.391 (50.391)
Epoch: [61][64/196]	Time 0.228 (0.201)	Data 0.000 (0.016)	Loss 2.0239 (2.1390)	Acc@1 23.047 (18.353)	Acc@5 80.078 (73.059)
Epoch: [61][128/196]	Time 0.134 (0.200)	Data 0.000 (0.008)	Loss 1.7495 (2.0286)	Acc@1 35.156 (21.430)	Acc@5 88.672 (77.426)
Epoch: [61][192/196]	Time 0.270 (0.199)	Data 0.000 (0.006)	Loss 1.7911 (1.9605)	Acc@1 26.172 (23.678)	Acc@5 86.719 (79.936)
after train
n1: 30 for:
wAcc: 54.4887058483182
test acc: 28.39
Epoche: [62/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [62][0/196]	Time 0.172 (0.172)	Data 0.737 (0.737)	Loss 1.8560 (1.8560)	Acc@1 31.641 (31.641)	Acc@5 85.156 (85.156)
Epoch: [62][64/196]	Time 0.163 (0.195)	Data 0.000 (0.012)	Loss 1.6334 (1.7595)	Acc@1 37.500 (32.254)	Acc@5 90.625 (87.224)
Epoch: [62][128/196]	Time 0.195 (0.186)	Data 0.000 (0.006)	Loss 1.5885 (1.7179)	Acc@1 38.672 (34.075)	Acc@5 92.188 (88.248)
Epoch: [62][192/196]	Time 0.077 (0.185)	Data 0.000 (0.004)	Loss 1.5295 (1.6822)	Acc@1 40.625 (35.879)	Acc@5 92.578 (88.779)
after train
n1: 30 for:
wAcc: 52.49555664173331
test acc: 40.53
Epoche: [63/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [63][0/196]	Time 0.181 (0.181)	Data 0.928 (0.928)	Loss 1.5967 (1.5967)	Acc@1 40.625 (40.625)	Acc@5 91.406 (91.406)
Epoch: [63][64/196]	Time 0.273 (0.195)	Data 0.000 (0.015)	Loss 1.6083 (1.5504)	Acc@1 40.625 (41.400)	Acc@5 89.844 (90.865)
Epoch: [63][128/196]	Time 0.113 (0.190)	Data 0.000 (0.008)	Loss 1.4781 (1.5268)	Acc@1 44.531 (42.878)	Acc@5 93.359 (91.164)
Epoch: [63][192/196]	Time 0.196 (0.188)	Data 0.000 (0.005)	Loss 1.3253 (1.5082)	Acc@1 50.781 (43.835)	Acc@5 93.750 (91.548)
after train
n1: 30 for:
wAcc: 52.01993643784045
test acc: 37.48
Epoche: [64/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [64][0/196]	Time 0.257 (0.257)	Data 0.992 (0.992)	Loss 1.3986 (1.3986)	Acc@1 49.609 (49.609)	Acc@5 93.359 (93.359)
Epoch: [64][64/196]	Time 0.135 (0.188)	Data 0.000 (0.016)	Loss 1.2497 (1.4029)	Acc@1 53.516 (48.528)	Acc@5 95.703 (93.035)
Epoch: [64][128/196]	Time 0.215 (0.194)	Data 0.000 (0.008)	Loss 1.4958 (1.3938)	Acc@1 44.141 (48.780)	Acc@5 92.578 (93.150)
Epoch: [64][192/196]	Time 0.207 (0.193)	Data 0.000 (0.006)	Loss 1.4105 (1.3766)	Acc@1 44.141 (49.603)	Acc@5 90.234 (93.270)
after train
n1: 30 for:
wAcc: 48.52458207486862
test acc: 48.35
Epoche: [65/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [65][0/196]	Time 0.213 (0.213)	Data 1.164 (1.164)	Loss 1.2865 (1.2865)	Acc@1 51.953 (51.953)	Acc@5 95.703 (95.703)
Epoch: [65][64/196]	Time 0.206 (0.179)	Data 0.000 (0.018)	Loss 1.2432 (1.3061)	Acc@1 53.125 (51.989)	Acc@5 94.922 (94.111)
Epoch: [65][128/196]	Time 0.188 (0.179)	Data 0.000 (0.010)	Loss 1.2548 (1.2879)	Acc@1 53.125 (52.740)	Acc@5 97.266 (94.204)
Epoch: [65][192/196]	Time 0.198 (0.183)	Data 0.000 (0.006)	Loss 1.1671 (1.2725)	Acc@1 57.422 (53.627)	Acc@5 94.141 (94.341)
after train
n1: 30 for:
wAcc: 52.21843142733575
test acc: 46.59
Epoche: [66/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [66][0/196]	Time 0.310 (0.310)	Data 1.097 (1.097)	Loss 1.1943 (1.1943)	Acc@1 54.688 (54.688)	Acc@5 94.531 (94.531)
Epoch: [66][64/196]	Time 0.165 (0.181)	Data 0.000 (0.017)	Loss 1.1970 (1.2023)	Acc@1 59.375 (56.268)	Acc@5 95.312 (94.940)
Epoch: [66][128/196]	Time 0.185 (0.191)	Data 0.000 (0.009)	Loss 1.2248 (1.1867)	Acc@1 53.125 (56.874)	Acc@5 95.703 (95.176)
Epoch: [66][192/196]	Time 0.149 (0.193)	Data 0.000 (0.006)	Loss 1.0575 (1.1842)	Acc@1 61.328 (57.031)	Acc@5 98.047 (95.189)
after train
n1: 30 for:
wAcc: 50.33162800578015
test acc: 46.16
Epoche: [67/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [67][0/196]	Time 0.163 (0.163)	Data 1.025 (1.025)	Loss 1.1672 (1.1672)	Acc@1 56.641 (56.641)	Acc@5 96.094 (96.094)
Epoch: [67][64/196]	Time 0.242 (0.186)	Data 0.000 (0.016)	Loss 1.0336 (1.1498)	Acc@1 60.156 (58.239)	Acc@5 98.047 (95.559)
Epoch: [67][128/196]	Time 0.240 (0.191)	Data 0.000 (0.008)	Loss 1.2109 (1.1447)	Acc@1 57.422 (58.612)	Acc@5 94.922 (95.467)
Epoch: [67][192/196]	Time 0.112 (0.189)	Data 0.000 (0.006)	Loss 1.1213 (1.1330)	Acc@1 61.328 (59.233)	Acc@5 94.531 (95.495)
after train
n1: 30 for:
wAcc: 50.438350763916254
test acc: 54.81
Epoche: [68/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [68][0/196]	Time 0.355 (0.355)	Data 0.999 (0.999)	Loss 1.0050 (1.0050)	Acc@1 67.578 (67.578)	Acc@5 98.438 (98.438)
Epoch: [68][64/196]	Time 0.248 (0.194)	Data 0.000 (0.016)	Loss 1.1204 (1.1030)	Acc@1 61.328 (60.427)	Acc@5 93.750 (95.925)
Epoch: [68][128/196]	Time 0.110 (0.194)	Data 0.000 (0.008)	Loss 1.1606 (1.0936)	Acc@1 58.984 (60.653)	Acc@5 95.312 (95.900)
Epoch: [68][192/196]	Time 0.164 (0.187)	Data 0.000 (0.006)	Loss 1.0432 (1.0864)	Acc@1 61.719 (60.923)	Acc@5 95.703 (95.995)
after train
n1: 30 for:
wAcc: 51.132393088257544
test acc: 52.73
Epoche: [69/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [69][0/196]	Time 0.226 (0.226)	Data 0.825 (0.825)	Loss 1.1167 (1.1167)	Acc@1 59.375 (59.375)	Acc@5 95.312 (95.312)
Epoch: [69][64/196]	Time 0.171 (0.172)	Data 0.000 (0.013)	Loss 1.0661 (1.0748)	Acc@1 64.453 (61.340)	Acc@5 96.484 (96.220)
Epoch: [69][128/196]	Time 0.105 (0.182)	Data 0.000 (0.007)	Loss 1.0766 (1.0633)	Acc@1 62.109 (61.867)	Acc@5 95.312 (96.191)
Epoch: [69][192/196]	Time 0.180 (0.186)	Data 0.000 (0.005)	Loss 1.0673 (1.0526)	Acc@1 58.984 (62.115)	Acc@5 95.703 (96.318)
after train
n1: 30 for:
wAcc: 51.76311418585501
test acc: 62.83
Epoche: [70/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [70][0/196]	Time 0.270 (0.270)	Data 0.892 (0.892)	Loss 1.0237 (1.0237)	Acc@1 62.891 (62.891)	Acc@5 98.438 (98.438)
Epoch: [70][64/196]	Time 0.247 (0.224)	Data 0.000 (0.015)	Loss 1.0821 (1.0324)	Acc@1 58.594 (63.053)	Acc@5 97.656 (96.400)
Epoch: [70][128/196]	Time 0.252 (0.218)	Data 0.000 (0.008)	Loss 1.1352 (1.0289)	Acc@1 62.500 (63.203)	Acc@5 94.141 (96.406)
Epoch: [70][192/196]	Time 0.195 (0.207)	Data 0.000 (0.005)	Loss 1.0199 (1.0195)	Acc@1 63.281 (63.522)	Acc@5 95.312 (96.509)
after train
n1: 30 for:
wAcc: 50.25519568420196
test acc: 54.81
Epoche: [71/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [71][0/196]	Time 0.260 (0.260)	Data 1.063 (1.063)	Loss 1.0378 (1.0378)	Acc@1 59.766 (59.766)	Acc@5 95.312 (95.312)
Epoch: [71][64/196]	Time 0.192 (0.209)	Data 0.000 (0.017)	Loss 1.0148 (1.0180)	Acc@1 65.234 (63.558)	Acc@5 96.484 (96.490)
Epoch: [71][128/196]	Time 0.134 (0.203)	Data 0.000 (0.009)	Loss 0.9822 (1.0069)	Acc@1 64.062 (63.863)	Acc@5 97.656 (96.602)
Epoch: [71][192/196]	Time 0.180 (0.187)	Data 0.000 (0.006)	Loss 1.0680 (0.9995)	Acc@1 64.062 (64.170)	Acc@5 95.703 (96.628)
after train
n1: 30 for:
wAcc: 50.54182594929453
test acc: 56.89
Epoche: [72/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [72][0/196]	Time 0.180 (0.180)	Data 0.976 (0.976)	Loss 0.9953 (0.9953)	Acc@1 63.281 (63.281)	Acc@5 97.266 (97.266)
Epoch: [72][64/196]	Time 0.230 (0.185)	Data 0.000 (0.015)	Loss 0.9545 (0.9739)	Acc@1 63.672 (65.060)	Acc@5 95.703 (96.881)
Epoch: [72][128/196]	Time 0.113 (0.194)	Data 0.000 (0.008)	Loss 1.0161 (0.9761)	Acc@1 59.766 (64.983)	Acc@5 97.266 (96.899)
Epoch: [72][192/196]	Time 0.101 (0.189)	Data 0.000 (0.005)	Loss 0.9940 (0.9705)	Acc@1 64.062 (65.180)	Acc@5 95.312 (96.901)
after train
n1: 30 for:
wAcc: 52.32327474370441
test acc: 63.0
Epoche: [73/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [73][0/196]	Time 0.223 (0.223)	Data 0.904 (0.904)	Loss 0.9752 (0.9752)	Acc@1 65.625 (65.625)	Acc@5 98.047 (98.047)
Epoch: [73][64/196]	Time 0.207 (0.181)	Data 0.000 (0.014)	Loss 0.9835 (0.9653)	Acc@1 62.500 (65.559)	Acc@5 96.875 (96.797)
Epoch: [73][128/196]	Time 0.223 (0.189)	Data 0.000 (0.007)	Loss 0.9550 (0.9552)	Acc@1 65.234 (66.052)	Acc@5 96.875 (96.954)
Epoch: [73][192/196]	Time 0.124 (0.184)	Data 0.000 (0.005)	Loss 0.9185 (0.9494)	Acc@1 68.359 (66.180)	Acc@5 98.438 (97.035)
after train
n1: 30 for:
wAcc: 53.529626102911195
test acc: 61.55
Epoche: [74/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [74][0/196]	Time 0.273 (0.273)	Data 0.852 (0.852)	Loss 0.8389 (0.8389)	Acc@1 69.531 (69.531)	Acc@5 97.266 (97.266)
Epoch: [74][64/196]	Time 0.183 (0.188)	Data 0.000 (0.014)	Loss 1.0395 (0.9491)	Acc@1 62.109 (66.292)	Acc@5 98.047 (96.917)
Epoch: [74][128/196]	Time 0.156 (0.184)	Data 0.005 (0.007)	Loss 0.8269 (0.9470)	Acc@1 69.531 (66.173)	Acc@5 98.828 (97.023)
Epoch: [74][192/196]	Time 0.150 (0.180)	Data 0.000 (0.005)	Loss 0.8714 (0.9404)	Acc@1 67.969 (66.307)	Acc@5 97.656 (97.055)
after train
n1: 30 for:
wAcc: 53.234633628437514
test acc: 49.19
Epoche: [75/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [75][0/196]	Time 0.307 (0.307)	Data 0.788 (0.788)	Loss 0.9096 (0.9096)	Acc@1 67.188 (67.188)	Acc@5 98.828 (98.828)
Epoch: [75][64/196]	Time 0.084 (0.186)	Data 0.000 (0.012)	Loss 0.9179 (0.9274)	Acc@1 69.531 (67.218)	Acc@5 96.484 (97.362)
Epoch: [75][128/196]	Time 0.180 (0.182)	Data 0.000 (0.007)	Loss 0.9156 (0.9202)	Acc@1 67.578 (67.390)	Acc@5 96.094 (97.193)
Epoch: [75][192/196]	Time 0.182 (0.174)	Data 0.000 (0.005)	Loss 0.8777 (0.9229)	Acc@1 67.578 (67.153)	Acc@5 97.656 (97.175)
after train
n1: 30 for:
wAcc: 51.55987749354126
test acc: 65.76
Epoche: [76/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [76][0/196]	Time 0.201 (0.201)	Data 0.672 (0.672)	Loss 0.8693 (0.8693)	Acc@1 68.359 (68.359)	Acc@5 97.266 (97.266)
Epoch: [76][64/196]	Time 0.133 (0.163)	Data 0.000 (0.011)	Loss 0.8368 (0.9207)	Acc@1 68.750 (67.392)	Acc@5 98.047 (97.151)
Epoch: [76][128/196]	Time 0.206 (0.166)	Data 0.000 (0.006)	Loss 0.9956 (0.9164)	Acc@1 67.578 (67.475)	Acc@5 95.703 (97.205)
Epoch: [76][192/196]	Time 0.112 (0.157)	Data 0.000 (0.004)	Loss 0.9481 (0.9121)	Acc@1 63.672 (67.440)	Acc@5 97.266 (97.256)
after train
n1: 30 for:
wAcc: 54.91043105341262
test acc: 64.75
Epoche: [77/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [77][0/196]	Time 0.195 (0.195)	Data 0.769 (0.769)	Loss 0.9950 (0.9950)	Acc@1 65.625 (65.625)	Acc@5 96.484 (96.484)
Epoch: [77][64/196]	Time 0.199 (0.168)	Data 0.000 (0.012)	Loss 0.9607 (0.9158)	Acc@1 68.359 (67.380)	Acc@5 96.094 (97.290)
Epoch: [77][128/196]	Time 0.165 (0.165)	Data 0.000 (0.006)	Loss 0.9391 (0.9008)	Acc@1 65.625 (67.799)	Acc@5 97.656 (97.344)
Epoch: [77][192/196]	Time 0.224 (0.170)	Data 0.000 (0.004)	Loss 0.9787 (0.8973)	Acc@1 61.328 (67.951)	Acc@5 96.484 (97.310)
after train
n1: 30 for:
wAcc: 54.792076239956835
test acc: 67.13
Epoche: [78/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [78][0/196]	Time 0.192 (0.192)	Data 0.687 (0.687)	Loss 0.9036 (0.9036)	Acc@1 66.406 (66.406)	Acc@5 96.094 (96.094)
Epoch: [78][64/196]	Time 0.210 (0.170)	Data 0.000 (0.013)	Loss 0.9439 (0.8938)	Acc@1 62.891 (68.540)	Acc@5 97.656 (97.332)
Epoch: [78][128/196]	Time 0.165 (0.168)	Data 0.000 (0.007)	Loss 0.9529 (0.8878)	Acc@1 64.844 (68.547)	Acc@5 96.094 (97.353)
Epoch: [78][192/196]	Time 0.284 (0.165)	Data 0.000 (0.005)	Loss 0.8554 (0.8858)	Acc@1 67.969 (68.647)	Acc@5 98.047 (97.428)
after train
n1: 30 for:
wAcc: 55.992843681530125
test acc: 65.78
Epoche: [79/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [79][0/196]	Time 0.162 (0.162)	Data 0.652 (0.652)	Loss 0.7941 (0.7941)	Acc@1 73.828 (73.828)	Acc@5 98.828 (98.828)
Epoch: [79][64/196]	Time 0.085 (0.180)	Data 0.000 (0.010)	Loss 0.9060 (0.8810)	Acc@1 67.578 (69.002)	Acc@5 97.266 (97.356)
Epoch: [79][128/196]	Time 0.160 (0.166)	Data 0.000 (0.005)	Loss 0.8187 (0.8743)	Acc@1 70.312 (69.113)	Acc@5 98.047 (97.571)
Epoch: [79][192/196]	Time 0.088 (0.165)	Data 0.000 (0.004)	Loss 0.9482 (0.8730)	Acc@1 65.625 (69.058)	Acc@5 96.094 (97.563)
after train
n1: 30 for:
wAcc: 56.181914756268064
test acc: 64.33
Epoche: [80/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [80][0/196]	Time 0.244 (0.244)	Data 0.701 (0.701)	Loss 0.9319 (0.9319)	Acc@1 63.281 (63.281)	Acc@5 98.438 (98.438)
Epoch: [80][64/196]	Time 0.159 (0.166)	Data 0.000 (0.011)	Loss 0.8796 (0.8782)	Acc@1 66.406 (68.918)	Acc@5 97.656 (97.314)
Epoch: [80][128/196]	Time 0.146 (0.164)	Data 0.000 (0.006)	Loss 0.8060 (0.8673)	Acc@1 68.359 (69.023)	Acc@5 97.656 (97.584)
Epoch: [80][192/196]	Time 0.207 (0.157)	Data 0.000 (0.004)	Loss 0.8901 (0.8678)	Acc@1 67.969 (69.133)	Acc@5 98.047 (97.569)
after train
n1: 30 for:
wAcc: 56.830474998875
test acc: 63.88
Epoche: [81/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [81][0/196]	Time 0.209 (0.209)	Data 0.846 (0.846)	Loss 0.8463 (0.8463)	Acc@1 69.922 (69.922)	Acc@5 97.266 (97.266)
Epoch: [81][64/196]	Time 0.197 (0.176)	Data 0.000 (0.013)	Loss 0.9098 (0.8739)	Acc@1 66.406 (68.990)	Acc@5 95.312 (97.434)
Epoch: [81][128/196]	Time 0.078 (0.172)	Data 0.000 (0.007)	Loss 0.7627 (0.8650)	Acc@1 75.391 (69.244)	Acc@5 97.266 (97.562)
Epoch: [81][192/196]	Time 0.135 (0.166)	Data 0.000 (0.005)	Loss 0.8434 (0.8615)	Acc@1 67.188 (69.448)	Acc@5 98.438 (97.557)
after train
n1: 30 for:
wAcc: 56.978812562108715
test acc: 61.61
Epoche: [82/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [82][0/196]	Time 0.200 (0.200)	Data 0.865 (0.865)	Loss 0.9281 (0.9281)	Acc@1 68.750 (68.750)	Acc@5 97.266 (97.266)
Epoch: [82][64/196]	Time 0.135 (0.177)	Data 0.000 (0.014)	Loss 0.9341 (0.8457)	Acc@1 69.141 (70.054)	Acc@5 95.312 (97.831)
Epoch: [82][128/196]	Time 0.115 (0.175)	Data 0.000 (0.007)	Loss 0.9019 (0.8462)	Acc@1 71.875 (70.131)	Acc@5 97.656 (97.677)
Epoch: [82][192/196]	Time 0.099 (0.175)	Data 0.000 (0.005)	Loss 0.9430 (0.8432)	Acc@1 69.531 (70.236)	Acc@5 96.484 (97.646)
after train
n1: 30 for:
wAcc: 57.394693709791056
test acc: 59.52
Epoche: [83/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [83][0/196]	Time 0.136 (0.136)	Data 0.935 (0.935)	Loss 0.9109 (0.9109)	Acc@1 67.188 (67.188)	Acc@5 96.484 (96.484)
Epoch: [83][64/196]	Time 0.231 (0.189)	Data 0.000 (0.015)	Loss 0.9272 (0.8556)	Acc@1 66.406 (69.597)	Acc@5 97.656 (97.668)
Epoch: [83][128/196]	Time 0.169 (0.173)	Data 0.000 (0.008)	Loss 0.7339 (0.8415)	Acc@1 73.438 (70.101)	Acc@5 97.656 (97.786)
Epoch: [83][192/196]	Time 0.139 (0.167)	Data 0.000 (0.005)	Loss 0.8167 (0.8384)	Acc@1 68.359 (70.395)	Acc@5 97.656 (97.753)
after train
n1: 30 for:
wAcc: 57.582406789678274
test acc: 61.02
Epoche: [84/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [84][0/196]	Time 0.209 (0.209)	Data 0.832 (0.832)	Loss 0.7417 (0.7417)	Acc@1 72.266 (72.266)	Acc@5 99.219 (99.219)
Epoch: [84][64/196]	Time 0.169 (0.193)	Data 0.000 (0.013)	Loss 0.9740 (0.8313)	Acc@1 67.969 (70.938)	Acc@5 97.656 (97.746)
Epoch: [84][128/196]	Time 0.108 (0.171)	Data 0.001 (0.007)	Loss 0.9364 (0.8275)	Acc@1 65.625 (70.836)	Acc@5 96.484 (97.717)
Epoch: [84][192/196]	Time 0.205 (0.170)	Data 0.000 (0.005)	Loss 0.8803 (0.8261)	Acc@1 71.875 (70.729)	Acc@5 98.438 (97.778)
after train
n1: 30 for:
wAcc: 56.497350519319816
test acc: 66.34
Epoche: [85/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [85][0/196]	Time 0.323 (0.323)	Data 1.009 (1.009)	Loss 0.7995 (0.7995)	Acc@1 73.047 (73.047)	Acc@5 98.438 (98.438)
Epoch: [85][64/196]	Time 0.200 (0.178)	Data 0.000 (0.016)	Loss 0.7666 (0.8147)	Acc@1 73.047 (71.388)	Acc@5 99.219 (97.812)
Epoch: [85][128/196]	Time 0.117 (0.172)	Data 0.000 (0.008)	Loss 0.8291 (0.8220)	Acc@1 69.922 (71.127)	Acc@5 98.828 (97.841)
Epoch: [85][192/196]	Time 0.083 (0.166)	Data 0.000 (0.006)	Loss 0.8740 (0.8217)	Acc@1 69.531 (70.999)	Acc@5 96.484 (97.772)
after train
n1: 30 for:
wAcc: 56.543994625255976
test acc: 61.01
Epoche: [86/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [86][0/196]	Time 0.326 (0.326)	Data 0.619 (0.619)	Loss 0.8524 (0.8524)	Acc@1 70.312 (70.312)	Acc@5 97.266 (97.266)
Epoch: [86][64/196]	Time 0.145 (0.171)	Data 0.000 (0.010)	Loss 0.7520 (0.7941)	Acc@1 71.875 (71.995)	Acc@5 97.266 (98.005)
Epoch: [86][128/196]	Time 0.152 (0.155)	Data 0.000 (0.005)	Loss 0.8151 (0.8059)	Acc@1 75.000 (71.569)	Acc@5 98.047 (97.932)
Epoch: [86][192/196]	Time 0.251 (0.163)	Data 0.000 (0.004)	Loss 0.8603 (0.8106)	Acc@1 72.266 (71.440)	Acc@5 96.094 (97.923)
after train
n1: 30 for:
wAcc: 58.56108022889705
test acc: 67.45
Epoche: [87/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [87][0/196]	Time 0.136 (0.136)	Data 0.836 (0.836)	Loss 0.7677 (0.7677)	Acc@1 70.312 (70.312)	Acc@5 97.266 (97.266)
Epoch: [87][64/196]	Time 0.127 (0.167)	Data 0.000 (0.013)	Loss 0.8235 (0.7854)	Acc@1 69.531 (72.091)	Acc@5 98.828 (97.969)
Epoch: [87][128/196]	Time 0.144 (0.160)	Data 0.000 (0.007)	Loss 0.8168 (0.7947)	Acc@1 70.703 (72.008)	Acc@5 97.656 (97.880)
Epoch: [87][192/196]	Time 0.134 (0.160)	Data 0.000 (0.005)	Loss 0.8016 (0.8014)	Acc@1 71.875 (71.701)	Acc@5 97.266 (97.923)
after train
n1: 30 for:
wAcc: 57.759778514426756
test acc: 67.99
Epoche: [88/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [88][0/196]	Time 0.250 (0.250)	Data 0.915 (0.915)	Loss 0.8558 (0.8558)	Acc@1 71.484 (71.484)	Acc@5 97.266 (97.266)
Epoch: [88][64/196]	Time 0.134 (0.168)	Data 0.000 (0.014)	Loss 0.8382 (0.7835)	Acc@1 71.875 (72.524)	Acc@5 97.266 (98.029)
Epoch: [88][128/196]	Time 0.076 (0.168)	Data 0.000 (0.007)	Loss 0.7278 (0.7865)	Acc@1 74.219 (72.347)	Acc@5 98.047 (97.986)
Epoch: [88][192/196]	Time 0.183 (0.166)	Data 0.000 (0.005)	Loss 0.8040 (0.7891)	Acc@1 71.484 (72.239)	Acc@5 98.438 (97.950)
after train
n1: 30 for:
wAcc: 60.95106567114211
test acc: 67.11
Epoche: [89/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [89][0/196]	Time 0.247 (0.247)	Data 0.637 (0.637)	Loss 0.9067 (0.9067)	Acc@1 69.531 (69.531)	Acc@5 97.656 (97.656)
Epoch: [89][64/196]	Time 0.126 (0.174)	Data 0.000 (0.010)	Loss 0.7782 (0.8021)	Acc@1 78.125 (72.085)	Acc@5 97.266 (98.023)
Epoch: [89][128/196]	Time 0.157 (0.164)	Data 0.000 (0.005)	Loss 0.8718 (0.7899)	Acc@1 66.797 (72.363)	Acc@5 98.047 (97.986)
Epoch: [89][192/196]	Time 0.182 (0.165)	Data 0.000 (0.004)	Loss 0.8258 (0.7933)	Acc@1 71.484 (72.235)	Acc@5 98.438 (97.960)
after train
n1: 30 for:
wAcc: 56.59089542413939
test acc: 58.47
Epoche: [90/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [90][0/196]	Time 0.262 (0.262)	Data 0.895 (0.895)	Loss 0.7692 (0.7692)	Acc@1 71.094 (71.094)	Acc@5 98.438 (98.438)
Epoch: [90][64/196]	Time 0.205 (0.156)	Data 0.000 (0.014)	Loss 0.7204 (0.7608)	Acc@1 76.172 (73.161)	Acc@5 97.656 (98.275)
Epoch: [90][128/196]	Time 0.132 (0.157)	Data 0.000 (0.007)	Loss 0.6893 (0.7678)	Acc@1 73.047 (72.953)	Acc@5 98.828 (98.244)
Epoch: [90][192/196]	Time 0.270 (0.160)	Data 0.000 (0.005)	Loss 0.7563 (0.7713)	Acc@1 75.000 (73.010)	Acc@5 98.047 (98.162)
after train
n1: 30 for:
wAcc: 56.22784676065694
test acc: 69.55
Epoche: [91/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [91][0/196]	Time 0.172 (0.172)	Data 0.619 (0.619)	Loss 0.7489 (0.7489)	Acc@1 75.391 (75.391)	Acc@5 98.438 (98.438)
Epoch: [91][64/196]	Time 0.180 (0.147)	Data 0.000 (0.010)	Loss 0.7871 (0.7708)	Acc@1 71.875 (72.909)	Acc@5 98.047 (98.221)
Epoch: [91][128/196]	Time 0.200 (0.158)	Data 0.000 (0.005)	Loss 0.8610 (0.7739)	Acc@1 69.531 (72.895)	Acc@5 96.875 (98.044)
Epoch: [91][192/196]	Time 0.169 (0.162)	Data 0.000 (0.004)	Loss 0.6242 (0.7695)	Acc@1 79.297 (73.122)	Acc@5 99.219 (98.099)
after train
n1: 30 for:
wAcc: 58.842317822962976
test acc: 69.69
Epoche: [92/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [92][0/196]	Time 0.260 (0.260)	Data 0.792 (0.792)	Loss 0.8537 (0.8537)	Acc@1 67.188 (67.188)	Acc@5 96.094 (96.094)
Epoch: [92][64/196]	Time 0.177 (0.159)	Data 0.000 (0.013)	Loss 0.7279 (0.7685)	Acc@1 72.656 (72.668)	Acc@5 98.828 (97.993)
Epoch: [92][128/196]	Time 0.196 (0.163)	Data 0.000 (0.007)	Loss 0.6840 (0.7616)	Acc@1 76.953 (73.095)	Acc@5 98.828 (98.101)
Epoch: [92][192/196]	Time 0.209 (0.167)	Data 0.000 (0.005)	Loss 0.7133 (0.7628)	Acc@1 75.000 (73.138)	Acc@5 99.609 (98.124)
after train
n1: 30 for:
wAcc: 59.10125553640671
test acc: 70.5
Epoche: [93/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [93][0/196]	Time 0.211 (0.211)	Data 0.594 (0.594)	Loss 0.8238 (0.8238)	Acc@1 70.703 (70.703)	Acc@5 97.266 (97.266)
Epoch: [93][64/196]	Time 0.088 (0.154)	Data 0.000 (0.009)	Loss 0.7267 (0.7365)	Acc@1 74.219 (73.786)	Acc@5 97.266 (98.341)
Epoch: [93][128/196]	Time 0.136 (0.156)	Data 0.000 (0.005)	Loss 0.7793 (0.7516)	Acc@1 68.750 (73.616)	Acc@5 98.438 (98.253)
Epoch: [93][192/196]	Time 0.104 (0.160)	Data 0.000 (0.004)	Loss 0.6069 (0.7536)	Acc@1 76.562 (73.559)	Acc@5 98.828 (98.182)
after train
n1: 30 for:
wAcc: 61.408042532256175
test acc: 70.55
Epoche: [94/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [94][0/196]	Time 0.166 (0.166)	Data 0.830 (0.830)	Loss 0.7771 (0.7771)	Acc@1 72.656 (72.656)	Acc@5 97.656 (97.656)
Epoch: [94][64/196]	Time 0.159 (0.174)	Data 0.000 (0.013)	Loss 0.8085 (0.7412)	Acc@1 74.609 (74.213)	Acc@5 99.609 (98.155)
Epoch: [94][128/196]	Time 0.129 (0.170)	Data 0.000 (0.007)	Loss 0.8144 (0.7439)	Acc@1 73.438 (74.098)	Acc@5 98.438 (98.159)
Epoch: [94][192/196]	Time 0.172 (0.174)	Data 0.000 (0.005)	Loss 0.7836 (0.7452)	Acc@1 73.828 (74.065)	Acc@5 97.266 (98.221)
after train
n1: 30 for:
wAcc: 61.74341789910503
test acc: 71.25
Epoche: [95/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [95][0/196]	Time 0.165 (0.165)	Data 0.624 (0.624)	Loss 0.6865 (0.6865)	Acc@1 76.953 (76.953)	Acc@5 97.656 (97.656)
Epoch: [95][64/196]	Time 0.128 (0.169)	Data 0.000 (0.010)	Loss 0.7558 (0.7410)	Acc@1 70.312 (73.756)	Acc@5 98.047 (98.119)
Epoch: [95][128/196]	Time 0.218 (0.163)	Data 0.000 (0.005)	Loss 0.7698 (0.7363)	Acc@1 71.875 (73.986)	Acc@5 98.438 (98.177)
Epoch: [95][192/196]	Time 0.154 (0.163)	Data 0.000 (0.004)	Loss 0.7637 (0.7408)	Acc@1 71.875 (73.891)	Acc@5 97.656 (98.205)
after train
n1: 30 for:
wAcc: 62.29458430696767
test acc: 67.83
Epoche: [96/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [96][0/196]	Time 0.233 (0.233)	Data 0.603 (0.603)	Loss 0.7589 (0.7589)	Acc@1 74.219 (74.219)	Acc@5 98.047 (98.047)
Epoch: [96][64/196]	Time 0.215 (0.155)	Data 0.000 (0.010)	Loss 0.6541 (0.7296)	Acc@1 75.000 (74.591)	Acc@5 99.609 (98.347)
Epoch: [96][128/196]	Time 0.098 (0.157)	Data 0.000 (0.005)	Loss 0.7875 (0.7351)	Acc@1 72.656 (74.391)	Acc@5 97.266 (98.286)
Epoch: [96][192/196]	Time 0.169 (0.162)	Data 0.000 (0.003)	Loss 0.8373 (0.7326)	Acc@1 69.141 (74.537)	Acc@5 98.438 (98.267)
after train
n1: 30 for:
wAcc: 63.902165370217794
test acc: 68.36
Epoche: [97/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [97][0/196]	Time 0.266 (0.266)	Data 0.832 (0.832)	Loss 0.6601 (0.6601)	Acc@1 71.094 (71.094)	Acc@5 99.219 (99.219)
Epoch: [97][64/196]	Time 0.154 (0.183)	Data 0.000 (0.013)	Loss 0.8121 (0.7232)	Acc@1 72.266 (74.766)	Acc@5 96.875 (98.263)
Epoch: [97][128/196]	Time 0.256 (0.169)	Data 0.000 (0.007)	Loss 0.8036 (0.7328)	Acc@1 71.484 (74.485)	Acc@5 96.875 (98.235)
Epoch: [97][192/196]	Time 0.164 (0.167)	Data 0.000 (0.005)	Loss 0.6577 (0.7284)	Acc@1 75.781 (74.559)	Acc@5 99.219 (98.243)
after train
n1: 30 for:
wAcc: 63.889079565332025
test acc: 69.18
Epoche: [98/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [98][0/196]	Time 0.235 (0.235)	Data 0.933 (0.933)	Loss 0.7808 (0.7808)	Acc@1 71.484 (71.484)	Acc@5 97.656 (97.656)
Epoch: [98][64/196]	Time 0.152 (0.163)	Data 0.000 (0.015)	Loss 0.7898 (0.7095)	Acc@1 71.875 (75.066)	Acc@5 98.438 (98.287)
Epoch: [98][128/196]	Time 0.131 (0.156)	Data 0.000 (0.008)	Loss 0.7971 (0.7212)	Acc@1 68.750 (74.655)	Acc@5 98.047 (98.307)
Epoch: [98][192/196]	Time 0.137 (0.158)	Data 0.000 (0.005)	Loss 0.7304 (0.7223)	Acc@1 72.656 (74.638)	Acc@5 99.219 (98.300)
after train
n1: 30 for:
wAcc: 65.690500998948
test acc: 68.08
Epoche: [99/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [99][0/196]	Time 0.206 (0.206)	Data 1.107 (1.107)	Loss 0.7325 (0.7325)	Acc@1 72.656 (72.656)	Acc@5 98.438 (98.438)
Epoch: [99][64/196]	Time 0.116 (0.166)	Data 0.000 (0.017)	Loss 0.6437 (0.7105)	Acc@1 76.953 (75.060)	Acc@5 97.656 (98.341)
Epoch: [99][128/196]	Time 0.121 (0.166)	Data 0.000 (0.009)	Loss 0.7691 (0.7171)	Acc@1 72.266 (74.915)	Acc@5 98.047 (98.332)
Epoch: [99][192/196]	Time 0.133 (0.160)	Data 0.000 (0.006)	Loss 0.6418 (0.7212)	Acc@1 79.297 (74.968)	Acc@5 98.438 (98.280)
after train
n1: 30 for:
wAcc: 64.68527853573408
test acc: 67.4
Epoche: [100/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [100][0/196]	Time 0.245 (0.245)	Data 0.698 (0.698)	Loss 0.7558 (0.7558)	Acc@1 74.609 (74.609)	Acc@5 98.438 (98.438)
Epoch: [100][64/196]	Time 0.165 (0.169)	Data 0.000 (0.011)	Loss 0.8356 (0.7156)	Acc@1 73.047 (75.114)	Acc@5 97.656 (98.245)
Epoch: [100][128/196]	Time 0.095 (0.169)	Data 0.000 (0.006)	Loss 0.7576 (0.7231)	Acc@1 72.656 (74.788)	Acc@5 98.438 (98.265)
Epoch: [100][192/196]	Time 0.158 (0.173)	Data 0.000 (0.004)	Loss 0.7139 (0.7191)	Acc@1 77.344 (74.945)	Acc@5 98.047 (98.298)
after train
n1: 30 for:
wAcc: 65.16110989507457
test acc: 71.31
Epoche: [101/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [101][0/196]	Time 0.227 (0.227)	Data 0.756 (0.756)	Loss 0.7455 (0.7455)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [101][64/196]	Time 0.166 (0.169)	Data 0.000 (0.012)	Loss 0.6263 (0.7044)	Acc@1 77.734 (75.102)	Acc@5 98.047 (98.528)
Epoch: [101][128/196]	Time 0.095 (0.156)	Data 0.000 (0.006)	Loss 0.7055 (0.7092)	Acc@1 74.609 (75.073)	Acc@5 98.438 (98.335)
Epoch: [101][192/196]	Time 0.093 (0.156)	Data 0.000 (0.004)	Loss 0.7350 (0.7113)	Acc@1 73.828 (75.121)	Acc@5 98.438 (98.332)
after train
n1: 30 for:
wAcc: 66.44108359724338
test acc: 64.88
Epoche: [102/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [102][0/196]	Time 0.164 (0.164)	Data 0.567 (0.567)	Loss 0.6737 (0.6737)	Acc@1 75.000 (75.000)	Acc@5 98.828 (98.828)
Epoch: [102][64/196]	Time 0.301 (0.166)	Data 0.000 (0.009)	Loss 0.8464 (0.7052)	Acc@1 69.531 (75.174)	Acc@5 97.266 (98.389)
Epoch: [102][128/196]	Time 0.171 (0.162)	Data 0.000 (0.005)	Loss 0.7350 (0.7026)	Acc@1 73.828 (75.176)	Acc@5 97.266 (98.374)
Epoch: [102][192/196]	Time 0.197 (0.163)	Data 0.000 (0.003)	Loss 0.7901 (0.7098)	Acc@1 72.656 (74.862)	Acc@5 97.656 (98.359)
after train
n1: 30 for:
wAcc: 66.13075426845124
test acc: 72.67
Epoche: [103/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [103][0/196]	Time 0.240 (0.240)	Data 0.566 (0.566)	Loss 0.6890 (0.6890)	Acc@1 76.172 (76.172)	Acc@5 98.828 (98.828)
Epoch: [103][64/196]	Time 0.112 (0.162)	Data 0.000 (0.009)	Loss 0.5561 (0.6913)	Acc@1 83.203 (75.793)	Acc@5 99.609 (98.462)
Epoch: [103][128/196]	Time 0.081 (0.163)	Data 0.000 (0.005)	Loss 0.6290 (0.6966)	Acc@1 78.125 (75.733)	Acc@5 98.828 (98.459)
Epoch: [103][192/196]	Time 0.151 (0.164)	Data 0.000 (0.003)	Loss 0.6963 (0.7020)	Acc@1 75.781 (75.445)	Acc@5 99.219 (98.450)
after train
n1: 30 for:
wAcc: 64.765860242319
test acc: 72.14
Epoche: [104/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [104][0/196]	Time 0.181 (0.181)	Data 1.022 (1.022)	Loss 0.6759 (0.6759)	Acc@1 75.391 (75.391)	Acc@5 98.438 (98.438)
Epoch: [104][64/196]	Time 0.132 (0.168)	Data 0.000 (0.016)	Loss 0.6547 (0.6973)	Acc@1 75.000 (75.445)	Acc@5 97.656 (98.498)
Epoch: [104][128/196]	Time 0.169 (0.170)	Data 0.000 (0.008)	Loss 0.8098 (0.6962)	Acc@1 74.219 (75.612)	Acc@5 96.875 (98.468)
Epoch: [104][192/196]	Time 0.077 (0.164)	Data 0.000 (0.006)	Loss 0.7299 (0.6961)	Acc@1 74.609 (75.619)	Acc@5 97.656 (98.423)
after train
n1: 30 for:
wAcc: 67.63699619794264
test acc: 69.99
Epoche: [105/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [105][0/196]	Time 0.129 (0.129)	Data 0.711 (0.711)	Loss 0.7172 (0.7172)	Acc@1 75.000 (75.000)	Acc@5 97.656 (97.656)
Epoch: [105][64/196]	Time 0.192 (0.170)	Data 0.000 (0.011)	Loss 0.6621 (0.6869)	Acc@1 75.000 (76.022)	Acc@5 97.656 (98.389)
Epoch: [105][128/196]	Time 0.203 (0.171)	Data 0.000 (0.006)	Loss 0.6765 (0.6859)	Acc@1 75.391 (76.081)	Acc@5 98.828 (98.431)
Epoch: [105][192/196]	Time 0.172 (0.173)	Data 0.000 (0.004)	Loss 0.7444 (0.6905)	Acc@1 74.219 (75.939)	Acc@5 97.656 (98.411)
after train
n1: 30 for:
wAcc: 67.64279572203422
test acc: 72.15
Epoche: [106/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [106][0/196]	Time 0.301 (0.301)	Data 0.636 (0.636)	Loss 0.8019 (0.8019)	Acc@1 72.656 (72.656)	Acc@5 97.656 (97.656)
Epoch: [106][64/196]	Time 0.092 (0.163)	Data 0.000 (0.010)	Loss 0.7352 (0.6966)	Acc@1 75.391 (75.583)	Acc@5 97.656 (98.341)
Epoch: [106][128/196]	Time 0.104 (0.160)	Data 0.000 (0.005)	Loss 0.7685 (0.6830)	Acc@1 73.047 (76.005)	Acc@5 97.266 (98.392)
Epoch: [106][192/196]	Time 0.087 (0.159)	Data 0.000 (0.004)	Loss 0.7510 (0.6842)	Acc@1 70.703 (75.791)	Acc@5 99.219 (98.494)
after train
n1: 30 for:
wAcc: 68.27763960104436
test acc: 69.36
Epoche: [107/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [107][0/196]	Time 0.232 (0.232)	Data 0.899 (0.899)	Loss 0.7506 (0.7506)	Acc@1 72.656 (72.656)	Acc@5 98.828 (98.828)
Epoch: [107][64/196]	Time 0.181 (0.170)	Data 0.000 (0.014)	Loss 0.8399 (0.6901)	Acc@1 68.750 (75.709)	Acc@5 97.656 (98.474)
Epoch: [107][128/196]	Time 0.174 (0.167)	Data 0.000 (0.007)	Loss 0.7338 (0.6883)	Acc@1 75.781 (75.836)	Acc@5 97.266 (98.413)
Epoch: [107][192/196]	Time 0.144 (0.166)	Data 0.000 (0.005)	Loss 0.7113 (0.6885)	Acc@1 75.000 (75.939)	Acc@5 98.438 (98.421)
after train
n1: 30 for:
wAcc: 68.15231120192486
test acc: 71.64
Epoche: [108/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [108][0/196]	Time 0.233 (0.233)	Data 0.754 (0.754)	Loss 0.7869 (0.7869)	Acc@1 71.484 (71.484)	Acc@5 96.875 (96.875)
Epoch: [108][64/196]	Time 0.142 (0.163)	Data 0.000 (0.016)	Loss 0.5914 (0.6850)	Acc@1 80.078 (75.907)	Acc@5 99.219 (98.359)
Epoch: [108][128/196]	Time 0.172 (0.175)	Data 0.000 (0.008)	Loss 0.6337 (0.6808)	Acc@1 78.125 (76.154)	Acc@5 98.828 (98.447)
Epoch: [108][192/196]	Time 0.166 (0.177)	Data 0.000 (0.006)	Loss 0.5407 (0.6774)	Acc@1 79.297 (76.330)	Acc@5 99.609 (98.496)
after train
n1: 30 for:
wAcc: 68.16770912444359
test acc: 75.07
Epoche: [109/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [109][0/196]	Time 0.275 (0.275)	Data 0.975 (0.975)	Loss 0.8099 (0.8099)	Acc@1 72.656 (72.656)	Acc@5 98.828 (98.828)
Epoch: [109][64/196]	Time 0.267 (0.174)	Data 0.000 (0.015)	Loss 0.6296 (0.6942)	Acc@1 76.172 (75.619)	Acc@5 99.609 (98.450)
Epoch: [109][128/196]	Time 0.261 (0.174)	Data 0.000 (0.008)	Loss 0.6664 (0.6823)	Acc@1 78.516 (76.133)	Acc@5 99.219 (98.492)
Epoch: [109][192/196]	Time 0.098 (0.176)	Data 0.000 (0.005)	Loss 0.6600 (0.6808)	Acc@1 77.344 (76.210)	Acc@5 98.047 (98.484)
after train
n1: 30 for:
wAcc: 68.54796551242985
test acc: 73.58
Epoche: [110/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [110][0/196]	Time 0.266 (0.266)	Data 0.831 (0.831)	Loss 0.6576 (0.6576)	Acc@1 78.906 (78.906)	Acc@5 97.266 (97.266)
Epoch: [110][64/196]	Time 0.178 (0.163)	Data 0.000 (0.013)	Loss 0.6826 (0.6727)	Acc@1 77.344 (76.556)	Acc@5 98.828 (98.528)
Epoch: [110][128/196]	Time 0.262 (0.171)	Data 0.010 (0.007)	Loss 0.7668 (0.6750)	Acc@1 73.047 (76.526)	Acc@5 98.438 (98.480)
Epoch: [110][192/196]	Time 0.199 (0.165)	Data 0.000 (0.005)	Loss 0.6928 (0.6743)	Acc@1 75.391 (76.524)	Acc@5 99.219 (98.492)
after train
n1: 30 for:
wAcc: 68.54445816378326
test acc: 69.56
Epoche: [111/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [111][0/196]	Time 0.108 (0.108)	Data 0.782 (0.782)	Loss 0.6945 (0.6945)	Acc@1 75.391 (75.391)	Acc@5 97.656 (97.656)
Epoch: [111][64/196]	Time 0.111 (0.152)	Data 0.000 (0.012)	Loss 0.7229 (0.6767)	Acc@1 72.656 (76.286)	Acc@5 98.438 (98.299)
Epoch: [111][128/196]	Time 0.137 (0.164)	Data 0.000 (0.007)	Loss 0.6060 (0.6704)	Acc@1 78.516 (76.605)	Acc@5 98.438 (98.462)
Epoch: [111][192/196]	Time 0.205 (0.161)	Data 0.000 (0.004)	Loss 0.7016 (0.6732)	Acc@1 78.516 (76.597)	Acc@5 98.047 (98.478)
after train
n1: 30 for:
wAcc: 68.30784333728857
test acc: 73.11
Epoche: [112/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [112][0/196]	Time 0.218 (0.218)	Data 0.864 (0.864)	Loss 0.5874 (0.5874)	Acc@1 78.516 (78.516)	Acc@5 99.609 (99.609)
Epoch: [112][64/196]	Time 0.154 (0.162)	Data 0.000 (0.015)	Loss 0.6460 (0.6638)	Acc@1 77.734 (76.767)	Acc@5 99.609 (98.564)
Epoch: [112][128/196]	Time 0.243 (0.180)	Data 0.000 (0.008)	Loss 0.7236 (0.6712)	Acc@1 73.438 (76.411)	Acc@5 98.828 (98.540)
Epoch: [112][192/196]	Time 0.094 (0.178)	Data 0.000 (0.005)	Loss 0.8262 (0.6680)	Acc@1 75.000 (76.482)	Acc@5 96.484 (98.551)
after train
n1: 30 for:
wAcc: 68.83450223203756
test acc: 67.63
Epoche: [113/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [113][0/196]	Time 0.213 (0.213)	Data 0.815 (0.815)	Loss 0.5434 (0.5434)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [113][64/196]	Time 0.132 (0.189)	Data 0.000 (0.014)	Loss 0.6379 (0.6564)	Acc@1 77.344 (77.037)	Acc@5 98.438 (98.552)
Epoch: [113][128/196]	Time 0.225 (0.173)	Data 0.000 (0.007)	Loss 0.6594 (0.6645)	Acc@1 78.125 (76.902)	Acc@5 99.219 (98.519)
Epoch: [113][192/196]	Time 0.077 (0.171)	Data 0.000 (0.005)	Loss 0.6140 (0.6642)	Acc@1 77.734 (76.860)	Acc@5 98.828 (98.537)
after train
n1: 30 for:
wAcc: 69.52585989514816
test acc: 72.15
Epoche: [114/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [114][0/196]	Time 0.155 (0.155)	Data 0.919 (0.919)	Loss 0.7571 (0.7571)	Acc@1 71.875 (71.875)	Acc@5 98.047 (98.047)
Epoch: [114][64/196]	Time 0.232 (0.171)	Data 0.000 (0.014)	Loss 0.7101 (0.6681)	Acc@1 75.391 (76.659)	Acc@5 97.266 (98.528)
Epoch: [114][128/196]	Time 0.248 (0.177)	Data 0.000 (0.008)	Loss 0.8452 (0.6629)	Acc@1 70.703 (76.729)	Acc@5 97.266 (98.562)
Epoch: [114][192/196]	Time 0.195 (0.175)	Data 0.000 (0.005)	Loss 0.6204 (0.6680)	Acc@1 77.734 (76.542)	Acc@5 100.000 (98.512)
after train
n1: 30 for:
wAcc: 68.92464615664672
test acc: 71.43
Epoche: [115/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [115][0/196]	Time 0.252 (0.252)	Data 0.840 (0.840)	Loss 0.6267 (0.6267)	Acc@1 75.391 (75.391)	Acc@5 98.438 (98.438)
Epoch: [115][64/196]	Time 0.223 (0.162)	Data 0.000 (0.013)	Loss 0.5371 (0.6623)	Acc@1 81.250 (76.713)	Acc@5 98.828 (98.588)
Epoch: [115][128/196]	Time 0.201 (0.168)	Data 0.000 (0.007)	Loss 0.6524 (0.6556)	Acc@1 75.391 (76.771)	Acc@5 100.000 (98.631)
Epoch: [115][192/196]	Time 0.181 (0.161)	Data 0.000 (0.005)	Loss 0.6338 (0.6596)	Acc@1 76.562 (76.716)	Acc@5 98.828 (98.616)
after train
n1: 30 for:
wAcc: 70.01725831712038
test acc: 74.66
Epoche: [116/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [116][0/196]	Time 0.240 (0.240)	Data 0.914 (0.914)	Loss 0.6144 (0.6144)	Acc@1 83.594 (83.594)	Acc@5 96.875 (96.875)
Epoch: [116][64/196]	Time 0.168 (0.183)	Data 0.000 (0.015)	Loss 0.6539 (0.6548)	Acc@1 77.734 (77.344)	Acc@5 97.266 (98.534)
Epoch: [116][128/196]	Time 0.120 (0.183)	Data 0.000 (0.008)	Loss 0.5809 (0.6551)	Acc@1 78.516 (77.162)	Acc@5 98.828 (98.504)
Epoch: [116][192/196]	Time 0.102 (0.172)	Data 0.000 (0.005)	Loss 0.6290 (0.6532)	Acc@1 78.125 (77.202)	Acc@5 98.828 (98.561)
after train
n1: 30 for:
wAcc: 70.39485327950766
test acc: 70.47
Epoche: [117/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [117][0/196]	Time 0.219 (0.219)	Data 0.678 (0.678)	Loss 0.6275 (0.6275)	Acc@1 80.859 (80.859)	Acc@5 99.219 (99.219)
Epoch: [117][64/196]	Time 0.158 (0.183)	Data 0.000 (0.011)	Loss 0.7544 (0.6528)	Acc@1 75.781 (77.662)	Acc@5 98.438 (98.654)
Epoch: [117][128/196]	Time 0.143 (0.185)	Data 0.000 (0.006)	Loss 0.5433 (0.6530)	Acc@1 79.688 (77.238)	Acc@5 99.609 (98.598)
Epoch: [117][192/196]	Time 0.109 (0.181)	Data 0.000 (0.004)	Loss 0.7347 (0.6525)	Acc@1 74.609 (77.137)	Acc@5 98.828 (98.640)
after train
n1: 30 for:
wAcc: 70.27248728464953
test acc: 74.86
Epoche: [118/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [118][0/196]	Time 0.259 (0.259)	Data 1.178 (1.178)	Loss 0.6693 (0.6693)	Acc@1 74.609 (74.609)	Acc@5 98.828 (98.828)
Epoch: [118][64/196]	Time 0.194 (0.201)	Data 0.000 (0.018)	Loss 0.6824 (0.6566)	Acc@1 74.219 (76.917)	Acc@5 98.438 (98.486)
Epoch: [118][128/196]	Time 0.091 (0.188)	Data 0.000 (0.009)	Loss 0.6083 (0.6583)	Acc@1 80.859 (76.805)	Acc@5 97.266 (98.583)
Epoch: [118][192/196]	Time 0.101 (0.179)	Data 0.000 (0.006)	Loss 0.5849 (0.6550)	Acc@1 80.078 (77.099)	Acc@5 99.609 (98.585)
after train
n1: 30 for:
wAcc: 69.31944399235138
test acc: 72.47
Epoche: [119/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [119][0/196]	Time 0.281 (0.281)	Data 0.881 (0.881)	Loss 0.5397 (0.5397)	Acc@1 80.859 (80.859)	Acc@5 98.828 (98.828)
Epoch: [119][64/196]	Time 0.070 (0.221)	Data 0.000 (0.014)	Loss 0.6039 (0.6463)	Acc@1 74.609 (77.776)	Acc@5 99.609 (98.636)
Epoch: [119][128/196]	Time 0.209 (0.210)	Data 0.000 (0.007)	Loss 0.6019 (0.6508)	Acc@1 80.078 (77.531)	Acc@5 98.438 (98.640)
Epoch: [119][192/196]	Time 0.201 (0.192)	Data 0.000 (0.005)	Loss 0.7689 (0.6506)	Acc@1 69.922 (77.419)	Acc@5 99.219 (98.666)
after train
n1: 30 for:
wAcc: 71.12444772451575
test acc: 72.75
Epoche: [120/120]; Lr: 0.1
batch Size 256
befor train
Epoch: [120][0/196]	Time 0.223 (0.223)	Data 0.847 (0.847)	Loss 0.5883 (0.5883)	Acc@1 79.688 (79.688)	Acc@5 99.219 (99.219)
Epoch: [120][64/196]	Time 0.209 (0.176)	Data 0.000 (0.013)	Loss 0.6755 (0.6411)	Acc@1 75.000 (77.554)	Acc@5 99.609 (98.612)
Epoch: [120][128/196]	Time 0.078 (0.158)	Data 0.000 (0.007)	Loss 0.6277 (0.6396)	Acc@1 77.344 (77.722)	Acc@5 99.609 (98.540)
Epoch: [120][192/196]	Time 0.076 (0.146)	Data 0.000 (0.005)	Loss 0.6562 (0.6449)	Acc@1 80.078 (77.502)	Acc@5 98.438 (98.561)
after train
n1: 30 for:
wAcc: 71.24956068288358
test acc: 71.21
[INFO] Storing checkpoint...
Max memory: 18.4709632
Traceback (most recent call last):
  File "main.py", line 935, in <module>
    main()
  File "main.py", line 601, in main
    print(' {:5.3f}s'.format(ende - start), end='  ')
UnboundLocalError: local variable 'ende' referenced before assignment
