no display found. Using non-interactive Agg backend
[3, 3, 3]
[4, 8, 16]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 120; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/room2x3/model.nn; checkpoint: ./output/experimente4/room233; saveModell: True; LR: 0.1
random number: 8653
Files already downloaded and verified
==> Resuming from checkpoint..
Start epoch: 121
First Lr: 0.1
Startepoche: 121
deeper epoch: 0
Epoche: [121/240]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.
  warnings.warn("Setting attributes on ParameterList is not supported.")
Epoch: [121][0/196]	Time 0.244 (0.244)	Data 0.421 (0.421)	Loss 0.5654 (0.5654)	Acc@1 79.297 (79.297)	Acc@5 99.609 (99.609)
Epoch: [121][64/196]	Time 0.178 (0.181)	Data 0.000 (0.007)	Loss 0.4782 (0.5977)	Acc@1 85.547 (79.345)	Acc@5 98.828 (98.846)
Epoch: [121][128/196]	Time 0.157 (0.168)	Data 0.000 (0.004)	Loss 0.5737 (0.5943)	Acc@1 80.078 (79.324)	Acc@5 99.219 (98.834)
Epoch: [121][192/196]	Time 0.173 (0.170)	Data 0.000 (0.002)	Loss 0.5267 (0.5993)	Acc@1 84.766 (79.092)	Acc@5 98.828 (98.816)
after train
test acc: 77.66


now deeper1
deep2: True
len param: 12
Block: 0
Block: 1
Block: 2
Block: 3
Block: 4
Block: 0
Block: 1
Block: 2
Block: 3
Block: 4
Block: 0
Block: 1
Block: 2
Block: 3
Block: 4
archNums: [[1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [2, 1, 1, 1, 1]]
len paramList: 15
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=16, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
)
args.layersInBlock: 1
module: Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
Size of Weight: torch.Size([4, 3, 3, 3])
module: BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
Size of Weight: torch.Size([4])
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
Size of Weight: torch.Size([4, 4, 3, 3])
Size of Weight: torch.Size([4])
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Size of Weight: torch.Size([8, 4, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Sequential: [Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
module: Sequential(
  (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 4, 1, 1])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
Size of Weight: torch.Size([8, 8, 3, 3])
Size of Weight: torch.Size([8])
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Size of Weight: torch.Size([16, 8, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 8, 1, 1])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
Size of Weight: torch.Size([16, 16, 3, 3])
Size of Weight: torch.Size([16])
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=16, out_features=10, bias=True)
Size of Weight: torch.Size([10, 16])
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=16, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
)
Epoche: [122/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [122][0/196]	Time 0.255 (0.255)	Data 0.284 (0.284)	Loss 2.2275 (2.2275)	Acc@1 33.594 (33.594)	Acc@5 83.984 (83.984)
Epoch: [122][64/196]	Time 0.272 (0.255)	Data 0.000 (0.005)	Loss 2.2732 (2.5675)	Acc@1 15.625 (14.117)	Acc@5 57.422 (56.076)
Epoch: [122][128/196]	Time 0.146 (0.246)	Data 0.000 (0.003)	Loss 2.2508 (2.4093)	Acc@1 13.672 (14.332)	Acc@5 61.328 (58.903)
Epoch: [122][192/196]	Time 0.403 (0.274)	Data 0.000 (0.002)	Loss 2.1261 (2.3353)	Acc@1 18.750 (15.437)	Acc@5 71.484 (61.164)
after train
test acc: 20.97
Epoche: [123/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [123][0/196]	Time 0.367 (0.367)	Data 0.490 (0.490)	Loss 2.0887 (2.0887)	Acc@1 23.828 (23.828)	Acc@5 73.438 (73.438)
Epoch: [123][64/196]	Time 0.339 (0.347)	Data 0.000 (0.008)	Loss 2.0493 (2.0949)	Acc@1 21.484 (20.607)	Acc@5 75.781 (73.209)
Epoch: [123][128/196]	Time 0.278 (0.353)	Data 0.000 (0.004)	Loss 2.0678 (2.0680)	Acc@1 21.875 (21.366)	Acc@5 76.172 (74.800)
Epoch: [123][192/196]	Time 0.511 (0.392)	Data 0.000 (0.003)	Loss 2.0023 (2.0492)	Acc@1 21.484 (21.918)	Acc@5 76.953 (75.767)
after train
test acc: 24.6
Epoche: [124/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [124][0/196]	Time 0.447 (0.447)	Data 0.513 (0.513)	Loss 2.0034 (2.0034)	Acc@1 22.656 (22.656)	Acc@5 78.516 (78.516)
Epoch: [124][64/196]	Time 0.373 (0.476)	Data 0.000 (0.009)	Loss 1.9805 (1.9839)	Acc@1 20.703 (24.694)	Acc@5 78.906 (79.219)
Epoch: [124][128/196]	Time 0.439 (0.479)	Data 0.000 (0.005)	Loss 1.9099 (1.9634)	Acc@1 31.250 (25.657)	Acc@5 79.297 (80.124)
Epoch: [124][192/196]	Time 0.384 (0.471)	Data 0.000 (0.003)	Loss 1.9051 (1.9502)	Acc@1 27.344 (26.204)	Acc@5 81.641 (80.697)
after train
test acc: 26.87
Epoche: [125/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [125][0/196]	Time 0.427 (0.427)	Data 0.447 (0.447)	Loss 1.9248 (1.9248)	Acc@1 24.609 (24.609)	Acc@5 82.031 (82.031)
Epoch: [125][64/196]	Time 0.498 (0.485)	Data 0.000 (0.008)	Loss 1.9358 (1.8953)	Acc@1 25.000 (28.744)	Acc@5 81.641 (82.891)
Epoch: [125][128/196]	Time 0.256 (0.478)	Data 0.000 (0.004)	Loss 1.8169 (1.8827)	Acc@1 32.031 (28.961)	Acc@5 86.719 (83.309)
Epoch: [125][192/196]	Time 0.525 (0.475)	Data 0.000 (0.003)	Loss 1.8685 (1.8728)	Acc@1 30.469 (29.524)	Acc@5 83.203 (83.565)
after train
test acc: 30.19
Epoche: [126/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [126][0/196]	Time 0.490 (0.490)	Data 0.438 (0.438)	Loss 1.7591 (1.7591)	Acc@1 36.328 (36.328)	Acc@5 84.766 (84.766)
Epoch: [126][64/196]	Time 0.746 (0.492)	Data 0.000 (0.008)	Loss 1.8993 (1.8321)	Acc@1 29.297 (31.923)	Acc@5 84.375 (84.838)
Epoch: [126][128/196]	Time 0.255 (0.490)	Data 0.000 (0.004)	Loss 1.6557 (1.8265)	Acc@1 37.891 (32.295)	Acc@5 90.625 (84.826)
Epoch: [126][192/196]	Time 0.611 (0.483)	Data 0.000 (0.003)	Loss 1.7709 (1.8196)	Acc@1 32.031 (32.333)	Acc@5 85.547 (84.982)
after train
test acc: 21.21
Epoche: [127/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [127][0/196]	Time 0.622 (0.622)	Data 0.478 (0.478)	Loss 1.8053 (1.8053)	Acc@1 33.984 (33.984)	Acc@5 86.328 (86.328)
Epoch: [127][64/196]	Time 0.538 (0.519)	Data 0.000 (0.008)	Loss 1.8576 (1.7901)	Acc@1 30.469 (33.720)	Acc@5 84.766 (85.457)
Epoch: [127][128/196]	Time 0.523 (0.495)	Data 0.000 (0.005)	Loss 1.7415 (1.7854)	Acc@1 33.984 (33.573)	Acc@5 85.547 (85.644)
Epoch: [127][192/196]	Time 0.440 (0.496)	Data 0.000 (0.004)	Loss 1.7664 (1.7719)	Acc@1 36.328 (34.142)	Acc@5 84.766 (86.029)
after train
test acc: 25.6
Epoche: [128/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [128][0/196]	Time 0.530 (0.530)	Data 0.655 (0.655)	Loss 1.6793 (1.6793)	Acc@1 39.453 (39.453)	Acc@5 88.281 (88.281)
Epoch: [128][64/196]	Time 0.303 (0.488)	Data 0.000 (0.012)	Loss 1.6990 (1.7439)	Acc@1 34.375 (35.132)	Acc@5 89.062 (86.761)
Epoch: [128][128/196]	Time 0.388 (0.492)	Data 0.000 (0.007)	Loss 1.7469 (1.7386)	Acc@1 35.547 (35.265)	Acc@5 86.328 (86.903)
Epoch: [128][192/196]	Time 0.663 (0.489)	Data 0.000 (0.005)	Loss 1.7513 (1.7311)	Acc@1 34.766 (35.604)	Acc@5 86.719 (87.059)
after train
test acc: 23.83
Epoche: [129/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [129][0/196]	Time 0.750 (0.750)	Data 0.560 (0.560)	Loss 1.6829 (1.6829)	Acc@1 36.719 (36.719)	Acc@5 89.062 (89.062)
Epoch: [129][64/196]	Time 0.603 (0.509)	Data 0.000 (0.010)	Loss 1.6177 (1.7045)	Acc@1 39.453 (36.905)	Acc@5 89.453 (87.999)
Epoch: [129][128/196]	Time 0.572 (0.494)	Data 0.000 (0.005)	Loss 1.6356 (1.6983)	Acc@1 39.062 (37.394)	Acc@5 89.453 (88.039)
Epoch: [129][192/196]	Time 0.417 (0.489)	Data 0.000 (0.004)	Loss 1.6396 (1.6934)	Acc@1 41.797 (37.460)	Acc@5 89.062 (88.083)
after train
test acc: 28.0
Epoche: [130/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [130][0/196]	Time 0.488 (0.488)	Data 0.721 (0.721)	Loss 1.7077 (1.7077)	Acc@1 38.281 (38.281)	Acc@5 85.938 (85.938)
Epoch: [130][64/196]	Time 0.542 (0.508)	Data 0.000 (0.013)	Loss 1.7154 (1.6784)	Acc@1 37.109 (38.281)	Acc@5 85.547 (88.474)
Epoch: [130][128/196]	Time 0.453 (0.506)	Data 0.000 (0.007)	Loss 1.7285 (1.6645)	Acc@1 36.719 (38.705)	Acc@5 86.719 (88.605)
Epoch: [130][192/196]	Time 0.630 (0.500)	Data 0.000 (0.006)	Loss 1.6206 (1.6571)	Acc@1 41.797 (38.967)	Acc@5 89.844 (88.700)
after train
test acc: 26.98
Epoche: [131/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [131][0/196]	Time 0.431 (0.431)	Data 0.509 (0.509)	Loss 1.6882 (1.6882)	Acc@1 38.672 (38.672)	Acc@5 87.891 (87.891)
Epoch: [131][64/196]	Time 0.580 (0.481)	Data 0.000 (0.009)	Loss 1.7135 (1.6390)	Acc@1 33.984 (39.345)	Acc@5 89.453 (88.888)
Epoch: [131][128/196]	Time 0.473 (0.492)	Data 0.000 (0.005)	Loss 1.6613 (1.6253)	Acc@1 38.672 (40.035)	Acc@5 89.453 (89.250)
Epoch: [131][192/196]	Time 0.479 (0.483)	Data 0.000 (0.004)	Loss 1.5325 (1.6172)	Acc@1 43.359 (40.283)	Acc@5 91.797 (89.475)
after train
test acc: 17.89
Epoche: [132/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [132][0/196]	Time 0.471 (0.471)	Data 0.527 (0.527)	Loss 1.7825 (1.7825)	Acc@1 37.891 (37.891)	Acc@5 83.984 (83.984)
Epoch: [132][64/196]	Time 0.309 (0.501)	Data 0.000 (0.009)	Loss 1.5492 (1.6095)	Acc@1 37.891 (40.469)	Acc@5 92.188 (89.633)
Epoch: [132][128/196]	Time 0.563 (0.497)	Data 0.000 (0.006)	Loss 1.5410 (1.5854)	Acc@1 42.188 (41.473)	Acc@5 92.578 (90.013)
Epoch: [132][192/196]	Time 0.361 (0.491)	Data 0.000 (0.005)	Loss 1.4422 (1.5771)	Acc@1 46.094 (41.918)	Acc@5 90.625 (90.188)
after train
test acc: 38.33
Epoche: [133/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [133][0/196]	Time 0.580 (0.580)	Data 0.665 (0.665)	Loss 1.5512 (1.5512)	Acc@1 44.141 (44.141)	Acc@5 92.188 (92.188)
Epoch: [133][64/196]	Time 0.570 (0.499)	Data 0.000 (0.011)	Loss 1.4523 (1.5307)	Acc@1 46.875 (43.221)	Acc@5 92.188 (91.244)
Epoch: [133][128/196]	Time 0.498 (0.497)	Data 0.000 (0.007)	Loss 1.3704 (1.5266)	Acc@1 50.781 (43.402)	Acc@5 94.141 (91.337)
Epoch: [133][192/196]	Time 0.488 (0.487)	Data 0.000 (0.005)	Loss 1.4431 (1.5148)	Acc@1 48.828 (44.021)	Acc@5 90.625 (91.412)
after train
test acc: 33.85
Epoche: [134/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [134][0/196]	Time 0.848 (0.848)	Data 0.570 (0.570)	Loss 1.4691 (1.4691)	Acc@1 46.484 (46.484)	Acc@5 91.016 (91.016)
Epoch: [134][64/196]	Time 0.402 (0.485)	Data 0.000 (0.010)	Loss 1.4563 (1.4724)	Acc@1 44.531 (45.901)	Acc@5 93.359 (91.743)
Epoch: [134][128/196]	Time 0.346 (0.498)	Data 0.000 (0.006)	Loss 1.4051 (1.4617)	Acc@1 48.828 (46.421)	Acc@5 92.969 (92.039)
Epoch: [134][192/196]	Time 0.402 (0.501)	Data 0.000 (0.004)	Loss 1.4262 (1.4556)	Acc@1 46.484 (46.665)	Acc@5 92.969 (92.001)
after train
test acc: 28.99
Epoche: [135/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [135][0/196]	Time 0.565 (0.565)	Data 0.573 (0.573)	Loss 1.4710 (1.4710)	Acc@1 44.141 (44.141)	Acc@5 92.969 (92.969)
Epoch: [135][64/196]	Time 0.534 (0.532)	Data 0.000 (0.011)	Loss 1.4107 (1.4013)	Acc@1 47.266 (49.020)	Acc@5 92.188 (92.764)
Epoch: [135][128/196]	Time 0.599 (0.500)	Data 0.000 (0.006)	Loss 1.3546 (1.3932)	Acc@1 50.391 (49.043)	Acc@5 93.359 (92.929)
Epoch: [135][192/196]	Time 0.599 (0.500)	Data 0.000 (0.004)	Loss 1.3780 (1.3776)	Acc@1 50.781 (49.613)	Acc@5 92.188 (93.013)
after train
test acc: 27.11
Epoche: [136/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [136][0/196]	Time 0.607 (0.607)	Data 0.442 (0.442)	Loss 1.2523 (1.2523)	Acc@1 54.688 (54.688)	Acc@5 95.312 (95.312)
Epoch: [136][64/196]	Time 0.401 (0.479)	Data 0.000 (0.008)	Loss 1.3408 (1.3249)	Acc@1 53.125 (52.151)	Acc@5 92.969 (93.822)
Epoch: [136][128/196]	Time 0.324 (0.494)	Data 0.000 (0.005)	Loss 1.4701 (1.3331)	Acc@1 45.703 (52.192)	Acc@5 91.797 (93.529)
Epoch: [136][192/196]	Time 0.419 (0.501)	Data 0.000 (0.004)	Loss 1.2794 (1.3207)	Acc@1 49.219 (52.384)	Acc@5 96.094 (93.728)
after train
test acc: 34.89
Epoche: [137/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [137][0/196]	Time 0.433 (0.433)	Data 0.609 (0.609)	Loss 1.1937 (1.1937)	Acc@1 58.203 (58.203)	Acc@5 96.094 (96.094)
Epoch: [137][64/196]	Time 0.565 (0.503)	Data 0.000 (0.011)	Loss 1.1880 (1.2804)	Acc@1 56.250 (53.438)	Acc@5 94.141 (94.495)
Epoch: [137][128/196]	Time 0.535 (0.503)	Data 0.000 (0.006)	Loss 1.2693 (1.2650)	Acc@1 50.391 (54.221)	Acc@5 96.875 (94.446)
Epoch: [137][192/196]	Time 0.592 (0.492)	Data 0.000 (0.004)	Loss 1.3649 (1.2541)	Acc@1 50.781 (54.748)	Acc@5 94.141 (94.523)
after train
test acc: 35.35
Epoche: [138/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [138][0/196]	Time 0.589 (0.589)	Data 0.660 (0.660)	Loss 1.0991 (1.0991)	Acc@1 60.938 (60.938)	Acc@5 96.094 (96.094)
Epoch: [138][64/196]	Time 0.658 (0.505)	Data 0.000 (0.012)	Loss 1.3192 (1.2198)	Acc@1 50.391 (56.286)	Acc@5 95.703 (94.754)
Epoch: [138][128/196]	Time 0.441 (0.506)	Data 0.000 (0.007)	Loss 1.2865 (1.2235)	Acc@1 54.297 (56.077)	Acc@5 94.141 (94.716)
Epoch: [138][192/196]	Time 0.572 (0.496)	Data 0.000 (0.005)	Loss 1.2517 (1.2167)	Acc@1 56.641 (56.469)	Acc@5 93.750 (94.742)
after train
test acc: 47.43
Epoche: [139/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [139][0/196]	Time 0.445 (0.445)	Data 0.546 (0.546)	Loss 1.2420 (1.2420)	Acc@1 57.031 (57.031)	Acc@5 94.922 (94.922)
Epoch: [139][64/196]	Time 0.509 (0.476)	Data 0.000 (0.010)	Loss 1.2313 (1.1920)	Acc@1 55.859 (56.851)	Acc@5 95.703 (95.216)
Epoch: [139][128/196]	Time 0.372 (0.486)	Data 0.000 (0.006)	Loss 1.2252 (1.1859)	Acc@1 59.766 (57.364)	Acc@5 94.531 (95.246)
Epoch: [139][192/196]	Time 0.424 (0.490)	Data 0.000 (0.004)	Loss 1.0801 (1.1735)	Acc@1 63.281 (57.762)	Acc@5 96.094 (95.308)
after train
test acc: 45.54
Epoche: [140/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [140][0/196]	Time 0.556 (0.556)	Data 0.589 (0.589)	Loss 1.1203 (1.1203)	Acc@1 59.766 (59.766)	Acc@5 96.484 (96.484)
Epoch: [140][64/196]	Time 0.516 (0.489)	Data 0.005 (0.011)	Loss 1.1733 (1.1702)	Acc@1 57.812 (58.233)	Acc@5 94.531 (95.300)
Epoch: [140][128/196]	Time 0.430 (0.481)	Data 0.000 (0.006)	Loss 1.1837 (1.1507)	Acc@1 57.812 (58.939)	Acc@5 94.922 (95.406)
Epoch: [140][192/196]	Time 0.430 (0.484)	Data 0.000 (0.005)	Loss 1.1421 (1.1360)	Acc@1 60.156 (59.450)	Acc@5 94.531 (95.480)
after train
test acc: 54.91
Epoche: [141/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [141][0/196]	Time 0.540 (0.540)	Data 0.680 (0.680)	Loss 0.8997 (0.8997)	Acc@1 66.797 (66.797)	Acc@5 96.484 (96.484)
Epoch: [141][64/196]	Time 0.405 (0.499)	Data 0.000 (0.011)	Loss 1.0337 (1.0970)	Acc@1 60.156 (60.865)	Acc@5 96.484 (96.004)
Epoch: [141][128/196]	Time 0.593 (0.495)	Data 0.005 (0.007)	Loss 1.0674 (1.0946)	Acc@1 62.109 (61.262)	Acc@5 94.141 (95.827)
Epoch: [141][192/196]	Time 0.550 (0.494)	Data 0.000 (0.005)	Loss 0.9676 (1.0933)	Acc@1 66.016 (61.294)	Acc@5 95.703 (95.829)
after train
test acc: 53.94
Epoche: [142/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [142][0/196]	Time 0.388 (0.388)	Data 0.956 (0.956)	Loss 0.9988 (0.9988)	Acc@1 64.453 (64.453)	Acc@5 96.484 (96.484)
Epoch: [142][64/196]	Time 0.528 (0.501)	Data 0.000 (0.016)	Loss 1.1722 (1.0488)	Acc@1 58.594 (62.728)	Acc@5 93.750 (96.388)
Epoch: [142][128/196]	Time 0.431 (0.511)	Data 0.000 (0.009)	Loss 0.9853 (1.0572)	Acc@1 63.281 (62.309)	Acc@5 96.484 (96.275)
Epoch: [142][192/196]	Time 0.399 (0.505)	Data 0.000 (0.006)	Loss 1.0109 (1.0493)	Acc@1 64.062 (62.686)	Acc@5 98.047 (96.292)
after train
test acc: 38.8
Epoche: [143/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [143][0/196]	Time 0.606 (0.606)	Data 0.650 (0.650)	Loss 0.9266 (0.9266)	Acc@1 67.969 (67.969)	Acc@5 98.438 (98.438)
Epoch: [143][64/196]	Time 0.397 (0.502)	Data 0.000 (0.011)	Loss 1.0148 (1.0221)	Acc@1 68.750 (63.858)	Acc@5 95.703 (96.593)
Epoch: [143][128/196]	Time 0.460 (0.499)	Data 0.000 (0.006)	Loss 0.9531 (1.0125)	Acc@1 66.797 (64.211)	Acc@5 97.266 (96.690)
Epoch: [143][192/196]	Time 0.604 (0.499)	Data 0.000 (0.004)	Loss 1.1373 (1.0126)	Acc@1 57.031 (64.309)	Acc@5 96.875 (96.626)
after train
test acc: 47.24
Epoche: [144/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [144][0/196]	Time 0.620 (0.620)	Data 0.602 (0.602)	Loss 1.1428 (1.1428)	Acc@1 58.984 (58.984)	Acc@5 96.484 (96.484)
Epoch: [144][64/196]	Time 0.382 (0.486)	Data 0.000 (0.010)	Loss 1.0133 (0.9889)	Acc@1 63.281 (64.982)	Acc@5 96.094 (96.695)
Epoch: [144][128/196]	Time 0.765 (0.493)	Data 0.006 (0.006)	Loss 0.9055 (0.9827)	Acc@1 67.969 (65.086)	Acc@5 96.484 (96.721)
Epoch: [144][192/196]	Time 0.635 (0.490)	Data 0.000 (0.004)	Loss 0.8467 (0.9827)	Acc@1 69.922 (65.279)	Acc@5 98.828 (96.735)
after train
test acc: 58.93
Epoche: [145/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [145][0/196]	Time 0.526 (0.526)	Data 0.771 (0.771)	Loss 1.0367 (1.0367)	Acc@1 62.109 (62.109)	Acc@5 97.656 (97.656)
Epoch: [145][64/196]	Time 0.460 (0.515)	Data 0.000 (0.013)	Loss 1.0533 (0.9887)	Acc@1 58.594 (64.970)	Acc@5 97.266 (97.007)
Epoch: [145][128/196]	Time 0.707 (0.523)	Data 0.006 (0.008)	Loss 0.9140 (0.9668)	Acc@1 67.969 (65.907)	Acc@5 97.266 (97.045)
Epoch: [145][192/196]	Time 0.190 (0.501)	Data 0.000 (0.005)	Loss 1.0454 (0.9607)	Acc@1 62.891 (66.157)	Acc@5 96.484 (97.000)
after train
test acc: 36.24
Epoche: [146/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [146][0/196]	Time 0.571 (0.571)	Data 0.651 (0.651)	Loss 0.9799 (0.9799)	Acc@1 67.188 (67.188)	Acc@5 96.094 (96.094)
Epoch: [146][64/196]	Time 0.543 (0.462)	Data 0.000 (0.011)	Loss 1.0285 (0.9468)	Acc@1 62.891 (66.719)	Acc@5 94.922 (96.965)
Epoch: [146][128/196]	Time 0.469 (0.478)	Data 0.008 (0.006)	Loss 1.0067 (0.9406)	Acc@1 66.406 (66.994)	Acc@5 97.656 (96.999)
Epoch: [146][192/196]	Time 0.556 (0.486)	Data 0.000 (0.005)	Loss 0.9373 (0.9380)	Acc@1 67.578 (66.914)	Acc@5 98.047 (97.069)
after train
test acc: 45.06
Epoche: [147/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [147][0/196]	Time 0.543 (0.543)	Data 0.649 (0.649)	Loss 0.9513 (0.9513)	Acc@1 66.797 (66.797)	Acc@5 97.656 (97.656)
Epoch: [147][64/196]	Time 0.566 (0.498)	Data 0.000 (0.011)	Loss 0.8550 (0.9336)	Acc@1 71.875 (67.266)	Acc@5 97.656 (96.965)
Epoch: [147][128/196]	Time 0.302 (0.493)	Data 0.000 (0.007)	Loss 0.9739 (0.9292)	Acc@1 67.188 (67.360)	Acc@5 96.484 (97.108)
Epoch: [147][192/196]	Time 0.463 (0.496)	Data 0.000 (0.005)	Loss 0.9344 (0.9213)	Acc@1 67.578 (67.619)	Acc@5 98.047 (97.219)
after train
test acc: 31.31
Epoche: [148/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [148][0/196]	Time 0.617 (0.617)	Data 0.604 (0.604)	Loss 0.9496 (0.9496)	Acc@1 66.797 (66.797)	Acc@5 95.703 (95.703)
Epoch: [148][64/196]	Time 0.384 (0.486)	Data 0.000 (0.011)	Loss 0.8664 (0.9295)	Acc@1 66.797 (67.626)	Acc@5 97.656 (97.133)
Epoch: [148][128/196]	Time 0.462 (0.499)	Data 0.000 (0.006)	Loss 1.0368 (0.9153)	Acc@1 62.109 (68.029)	Acc@5 95.312 (97.166)
Epoch: [148][192/196]	Time 0.343 (0.487)	Data 0.000 (0.005)	Loss 0.8802 (0.9108)	Acc@1 67.969 (68.137)	Acc@5 95.312 (97.138)
after train
test acc: 64.02
Epoche: [149/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [149][0/196]	Time 0.578 (0.578)	Data 0.538 (0.538)	Loss 0.8552 (0.8552)	Acc@1 71.484 (71.484)	Acc@5 98.047 (98.047)
Epoch: [149][64/196]	Time 0.375 (0.498)	Data 0.000 (0.010)	Loss 0.7881 (0.8997)	Acc@1 71.875 (68.492)	Acc@5 98.828 (97.248)
Epoch: [149][128/196]	Time 0.417 (0.488)	Data 0.000 (0.006)	Loss 1.0779 (0.8919)	Acc@1 62.891 (68.699)	Acc@5 96.094 (97.332)
Epoch: [149][192/196]	Time 0.661 (0.485)	Data 0.000 (0.004)	Loss 0.8994 (0.8945)	Acc@1 69.531 (68.683)	Acc@5 94.922 (97.276)
after train
test acc: 52.7
Epoche: [150/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [150][0/196]	Time 0.597 (0.597)	Data 0.486 (0.486)	Loss 0.8656 (0.8656)	Acc@1 72.656 (72.656)	Acc@5 96.875 (96.875)
Epoch: [150][64/196]	Time 0.341 (0.520)	Data 0.000 (0.009)	Loss 0.8544 (0.8991)	Acc@1 71.094 (68.546)	Acc@5 98.047 (97.248)
Epoch: [150][128/196]	Time 0.364 (0.501)	Data 0.000 (0.005)	Loss 0.7866 (0.8907)	Acc@1 76.562 (68.874)	Acc@5 98.047 (97.247)
Epoch: [150][192/196]	Time 0.702 (0.499)	Data 0.000 (0.004)	Loss 0.9182 (0.8871)	Acc@1 68.359 (68.908)	Acc@5 97.266 (97.314)
after train
test acc: 37.3
Epoche: [151/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [151][0/196]	Time 0.589 (0.589)	Data 0.772 (0.772)	Loss 0.8864 (0.8864)	Acc@1 68.359 (68.359)	Acc@5 96.875 (96.875)
Epoch: [151][64/196]	Time 0.356 (0.479)	Data 0.000 (0.013)	Loss 0.9114 (0.8951)	Acc@1 69.141 (68.750)	Acc@5 97.266 (97.157)
Epoch: [151][128/196]	Time 0.520 (0.489)	Data 0.000 (0.007)	Loss 0.7749 (0.8860)	Acc@1 73.047 (69.004)	Acc@5 99.219 (97.326)
Epoch: [151][192/196]	Time 0.491 (0.498)	Data 0.000 (0.005)	Loss 0.9360 (0.8839)	Acc@1 64.844 (69.108)	Acc@5 95.703 (97.290)
after train
test acc: 37.81
Epoche: [152/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [152][0/196]	Time 0.552 (0.552)	Data 0.785 (0.785)	Loss 0.8388 (0.8388)	Acc@1 71.484 (71.484)	Acc@5 98.047 (98.047)
Epoch: [152][64/196]	Time 0.688 (0.498)	Data 0.000 (0.014)	Loss 0.9927 (0.8708)	Acc@1 63.281 (69.573)	Acc@5 96.484 (97.410)
Epoch: [152][128/196]	Time 0.292 (0.487)	Data 0.000 (0.008)	Loss 0.9102 (0.8657)	Acc@1 69.531 (69.737)	Acc@5 96.875 (97.396)
Epoch: [152][192/196]	Time 0.474 (0.489)	Data 0.000 (0.006)	Loss 0.9395 (0.8689)	Acc@1 67.188 (69.606)	Acc@5 96.484 (97.448)
after train
test acc: 15.17
Epoche: [153/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [153][0/196]	Time 0.626 (0.626)	Data 0.654 (0.654)	Loss 0.9509 (0.9509)	Acc@1 64.844 (64.844)	Acc@5 96.875 (96.875)
Epoch: [153][64/196]	Time 0.568 (0.500)	Data 0.000 (0.012)	Loss 0.7581 (0.8898)	Acc@1 73.047 (68.918)	Acc@5 98.828 (97.422)
Epoch: [153][128/196]	Time 0.686 (0.502)	Data 0.000 (0.007)	Loss 0.8424 (0.8692)	Acc@1 69.531 (69.661)	Acc@5 97.656 (97.541)
Epoch: [153][192/196]	Time 0.767 (0.498)	Data 0.000 (0.005)	Loss 0.8877 (0.8665)	Acc@1 68.359 (69.768)	Acc@5 98.047 (97.555)
after train
test acc: 33.81
Epoche: [154/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [154][0/196]	Time 0.790 (0.790)	Data 0.820 (0.820)	Loss 0.8290 (0.8290)	Acc@1 67.969 (67.969)	Acc@5 98.828 (98.828)
Epoch: [154][64/196]	Time 0.437 (0.500)	Data 0.000 (0.014)	Loss 0.8664 (0.8715)	Acc@1 69.141 (69.327)	Acc@5 98.438 (97.542)
Epoch: [154][128/196]	Time 0.536 (0.500)	Data 0.010 (0.008)	Loss 0.8449 (0.8687)	Acc@1 70.312 (69.565)	Acc@5 96.484 (97.496)
Epoch: [154][192/196]	Time 0.453 (0.498)	Data 0.000 (0.005)	Loss 0.9666 (0.8641)	Acc@1 66.797 (69.841)	Acc@5 97.266 (97.509)
after train
test acc: 37.24
Epoche: [155/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [155][0/196]	Time 0.487 (0.487)	Data 0.632 (0.632)	Loss 0.8937 (0.8937)	Acc@1 67.188 (67.188)	Acc@5 96.484 (96.484)
Epoch: [155][64/196]	Time 0.507 (0.518)	Data 0.000 (0.012)	Loss 0.8537 (0.8655)	Acc@1 69.141 (69.994)	Acc@5 97.266 (97.464)
Epoch: [155][128/196]	Time 0.751 (0.508)	Data 0.000 (0.008)	Loss 0.9678 (0.8556)	Acc@1 68.750 (70.361)	Acc@5 96.875 (97.499)
Epoch: [155][192/196]	Time 0.586 (0.497)	Data 0.000 (0.006)	Loss 0.7921 (0.8524)	Acc@1 74.609 (70.448)	Acc@5 98.047 (97.529)
after train
test acc: 61.96
Epoche: [156/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [156][0/196]	Time 0.534 (0.534)	Data 0.568 (0.568)	Loss 0.7891 (0.7891)	Acc@1 73.828 (73.828)	Acc@5 98.828 (98.828)
Epoch: [156][64/196]	Time 0.456 (0.516)	Data 0.000 (0.010)	Loss 0.7803 (0.8406)	Acc@1 69.531 (70.397)	Acc@5 97.656 (97.590)
Epoch: [156][128/196]	Time 0.386 (0.505)	Data 0.000 (0.005)	Loss 0.8736 (0.8399)	Acc@1 69.531 (70.700)	Acc@5 96.875 (97.674)
Epoch: [156][192/196]	Time 0.414 (0.490)	Data 0.000 (0.004)	Loss 0.8723 (0.8421)	Acc@1 69.531 (70.699)	Acc@5 98.047 (97.630)
after train
test acc: 57.22
Epoche: [157/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [157][0/196]	Time 0.839 (0.839)	Data 0.486 (0.486)	Loss 0.7409 (0.7409)	Acc@1 76.172 (76.172)	Acc@5 98.438 (98.438)
Epoch: [157][64/196]	Time 0.436 (0.498)	Data 0.000 (0.009)	Loss 0.9044 (0.8341)	Acc@1 68.359 (70.841)	Acc@5 97.266 (97.710)
Epoch: [157][128/196]	Time 0.605 (0.515)	Data 0.000 (0.005)	Loss 0.7829 (0.8328)	Acc@1 74.219 (70.909)	Acc@5 98.438 (97.668)
Epoch: [157][192/196]	Time 0.371 (0.496)	Data 0.000 (0.004)	Loss 0.7544 (0.8371)	Acc@1 73.438 (70.788)	Acc@5 96.875 (97.559)
after train
test acc: 66.34
Epoche: [158/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [158][0/196]	Time 0.619 (0.619)	Data 0.815 (0.815)	Loss 0.9051 (0.9051)	Acc@1 67.188 (67.188)	Acc@5 96.875 (96.875)
Epoch: [158][64/196]	Time 0.401 (0.515)	Data 0.000 (0.014)	Loss 0.8954 (0.8420)	Acc@1 69.531 (70.895)	Acc@5 97.266 (97.524)
Epoch: [158][128/196]	Time 0.457 (0.510)	Data 0.000 (0.008)	Loss 0.9099 (0.8401)	Acc@1 71.094 (70.812)	Acc@5 96.875 (97.650)
Epoch: [158][192/196]	Time 0.577 (0.490)	Data 0.000 (0.006)	Loss 0.8280 (0.8358)	Acc@1 71.094 (70.966)	Acc@5 98.047 (97.634)
after train
test acc: 53.41
Epoche: [159/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [159][0/196]	Time 0.610 (0.610)	Data 0.488 (0.488)	Loss 0.7602 (0.7602)	Acc@1 73.047 (73.047)	Acc@5 99.219 (99.219)
Epoch: [159][64/196]	Time 0.531 (0.485)	Data 0.000 (0.009)	Loss 0.6871 (0.8410)	Acc@1 76.953 (70.397)	Acc@5 99.219 (97.524)
Epoch: [159][128/196]	Time 0.481 (0.499)	Data 0.000 (0.005)	Loss 0.7773 (0.8334)	Acc@1 75.781 (70.764)	Acc@5 98.047 (97.596)
Epoch: [159][192/196]	Time 0.526 (0.486)	Data 0.000 (0.004)	Loss 0.8219 (0.8340)	Acc@1 71.094 (70.835)	Acc@5 97.656 (97.618)
after train
test acc: 37.06
Epoche: [160/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [160][0/196]	Time 0.550 (0.550)	Data 0.717 (0.717)	Loss 0.8355 (0.8355)	Acc@1 69.922 (69.922)	Acc@5 96.094 (96.094)
Epoch: [160][64/196]	Time 0.651 (0.498)	Data 0.000 (0.013)	Loss 0.7891 (0.8431)	Acc@1 73.828 (70.871)	Acc@5 96.875 (97.458)
Epoch: [160][128/196]	Time 0.440 (0.483)	Data 0.000 (0.007)	Loss 0.7380 (0.8340)	Acc@1 71.484 (71.009)	Acc@5 98.047 (97.608)
Epoch: [160][192/196]	Time 0.501 (0.488)	Data 0.000 (0.005)	Loss 0.8079 (0.8357)	Acc@1 69.922 (70.901)	Acc@5 97.656 (97.652)
after train
test acc: 44.63
Epoche: [161/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [161][0/196]	Time 0.442 (0.442)	Data 0.673 (0.673)	Loss 0.8380 (0.8380)	Acc@1 69.531 (69.531)	Acc@5 98.828 (98.828)
Epoch: [161][64/196]	Time 0.348 (0.500)	Data 0.000 (0.012)	Loss 0.8075 (0.8255)	Acc@1 71.094 (71.316)	Acc@5 98.047 (97.788)
Epoch: [161][128/196]	Time 0.614 (0.509)	Data 0.000 (0.007)	Loss 0.8620 (0.8297)	Acc@1 70.703 (71.139)	Acc@5 97.266 (97.708)
Epoch: [161][192/196]	Time 0.571 (0.502)	Data 0.000 (0.005)	Loss 0.8293 (0.8258)	Acc@1 71.875 (71.266)	Acc@5 98.828 (97.719)
after train
test acc: 59.71
Epoche: [162/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [162][0/196]	Time 0.675 (0.675)	Data 0.592 (0.592)	Loss 0.8445 (0.8445)	Acc@1 71.094 (71.094)	Acc@5 96.094 (96.094)
Epoch: [162][64/196]	Time 0.500 (0.489)	Data 0.000 (0.011)	Loss 0.7711 (0.8149)	Acc@1 74.219 (71.322)	Acc@5 98.438 (97.746)
Epoch: [162][128/196]	Time 0.597 (0.510)	Data 0.000 (0.007)	Loss 0.8286 (0.8108)	Acc@1 71.875 (71.578)	Acc@5 97.656 (97.786)
Epoch: [162][192/196]	Time 0.523 (0.505)	Data 0.000 (0.005)	Loss 0.8724 (0.8138)	Acc@1 69.531 (71.505)	Acc@5 97.656 (97.780)
after train
test acc: 42.76
Epoche: [163/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [163][0/196]	Time 0.652 (0.652)	Data 0.833 (0.833)	Loss 0.8401 (0.8401)	Acc@1 70.703 (70.703)	Acc@5 98.047 (98.047)
Epoch: [163][64/196]	Time 0.531 (0.486)	Data 0.000 (0.015)	Loss 0.9102 (0.8052)	Acc@1 69.531 (71.863)	Acc@5 96.484 (97.825)
Epoch: [163][128/196]	Time 0.522 (0.486)	Data 0.000 (0.008)	Loss 0.7801 (0.8050)	Acc@1 71.875 (72.039)	Acc@5 98.047 (97.771)
Epoch: [163][192/196]	Time 0.471 (0.470)	Data 0.000 (0.006)	Loss 0.7934 (0.8056)	Acc@1 74.219 (72.098)	Acc@5 99.219 (97.808)
after train
test acc: 66.47
Epoche: [164/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [164][0/196]	Time 0.508 (0.508)	Data 0.606 (0.606)	Loss 0.8555 (0.8555)	Acc@1 67.969 (67.969)	Acc@5 98.047 (98.047)
Epoch: [164][64/196]	Time 0.390 (0.501)	Data 0.000 (0.011)	Loss 0.8570 (0.8098)	Acc@1 70.312 (71.689)	Acc@5 99.219 (97.728)
Epoch: [164][128/196]	Time 0.517 (0.489)	Data 0.000 (0.006)	Loss 0.7678 (0.7999)	Acc@1 72.656 (72.069)	Acc@5 98.047 (97.835)
Epoch: [164][192/196]	Time 0.329 (0.486)	Data 0.000 (0.004)	Loss 0.8562 (0.8073)	Acc@1 70.703 (71.816)	Acc@5 97.656 (97.875)
after train
test acc: 36.68
Epoche: [165/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [165][0/196]	Time 0.638 (0.638)	Data 0.834 (0.834)	Loss 0.7360 (0.7360)	Acc@1 74.219 (74.219)	Acc@5 97.656 (97.656)
Epoch: [165][64/196]	Time 0.454 (0.492)	Data 0.000 (0.015)	Loss 0.8156 (0.7888)	Acc@1 69.141 (72.644)	Acc@5 97.656 (97.812)
Epoch: [165][128/196]	Time 0.320 (0.490)	Data 0.000 (0.008)	Loss 0.6541 (0.7902)	Acc@1 75.391 (72.456)	Acc@5 99.609 (97.820)
Epoch: [165][192/196]	Time 0.352 (0.484)	Data 0.000 (0.006)	Loss 0.7650 (0.7985)	Acc@1 75.000 (72.264)	Acc@5 97.656 (97.818)
after train
test acc: 62.58
Epoche: [166/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [166][0/196]	Time 0.729 (0.729)	Data 0.746 (0.746)	Loss 0.7831 (0.7831)	Acc@1 71.094 (71.094)	Acc@5 98.828 (98.828)
Epoch: [166][64/196]	Time 0.711 (0.499)	Data 0.002 (0.013)	Loss 0.8789 (0.8116)	Acc@1 72.656 (71.484)	Acc@5 97.656 (97.710)
Epoch: [166][128/196]	Time 0.590 (0.507)	Data 0.000 (0.008)	Loss 0.8308 (0.8107)	Acc@1 72.656 (71.621)	Acc@5 98.438 (97.750)
Epoch: [166][192/196]	Time 0.584 (0.506)	Data 0.000 (0.005)	Loss 0.8135 (0.8066)	Acc@1 74.219 (71.932)	Acc@5 98.828 (97.764)
after train
test acc: 58.62
Epoche: [167/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [167][0/196]	Time 0.616 (0.616)	Data 0.550 (0.550)	Loss 0.8446 (0.8446)	Acc@1 71.094 (71.094)	Acc@5 96.484 (96.484)
Epoch: [167][64/196]	Time 0.332 (0.477)	Data 0.000 (0.010)	Loss 0.8660 (0.8334)	Acc@1 69.922 (71.130)	Acc@5 97.266 (97.590)
Epoch: [167][128/196]	Time 0.362 (0.480)	Data 0.000 (0.006)	Loss 0.7525 (0.8307)	Acc@1 74.219 (71.360)	Acc@5 98.047 (97.617)
Epoch: [167][192/196]	Time 0.612 (0.486)	Data 0.000 (0.004)	Loss 0.8588 (0.8165)	Acc@1 68.359 (71.703)	Acc@5 97.266 (97.713)
after train
test acc: 66.49
Epoche: [168/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [168][0/196]	Time 0.558 (0.558)	Data 0.804 (0.804)	Loss 0.8703 (0.8703)	Acc@1 70.312 (70.312)	Acc@5 98.047 (98.047)
Epoch: [168][64/196]	Time 0.708 (0.511)	Data 0.000 (0.014)	Loss 0.9027 (0.8005)	Acc@1 67.969 (71.905)	Acc@5 97.266 (97.788)
Epoch: [168][128/196]	Time 0.709 (0.513)	Data 0.000 (0.008)	Loss 0.7949 (0.7970)	Acc@1 72.266 (72.217)	Acc@5 98.828 (97.874)
Epoch: [168][192/196]	Time 0.581 (0.500)	Data 0.000 (0.006)	Loss 0.7479 (0.7990)	Acc@1 73.047 (72.083)	Acc@5 99.609 (97.828)
after train
test acc: 67.51
Epoche: [169/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [169][0/196]	Time 0.605 (0.605)	Data 0.573 (0.573)	Loss 0.9274 (0.9274)	Acc@1 69.141 (69.141)	Acc@5 96.484 (96.484)
Epoch: [169][64/196]	Time 0.577 (0.497)	Data 0.000 (0.011)	Loss 0.7929 (0.7995)	Acc@1 71.875 (72.302)	Acc@5 97.266 (97.614)
Epoch: [169][128/196]	Time 0.597 (0.493)	Data 0.000 (0.006)	Loss 0.7733 (0.8034)	Acc@1 70.703 (72.275)	Acc@5 97.656 (97.638)
Epoch: [169][192/196]	Time 0.521 (0.494)	Data 0.000 (0.005)	Loss 0.7666 (0.8000)	Acc@1 75.781 (72.361)	Acc@5 98.047 (97.648)
after train
test acc: 61.04
Epoche: [170/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [170][0/196]	Time 0.658 (0.658)	Data 0.545 (0.545)	Loss 0.7497 (0.7497)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [170][64/196]	Time 0.468 (0.483)	Data 0.000 (0.010)	Loss 0.8427 (0.7957)	Acc@1 72.266 (72.151)	Acc@5 97.656 (97.692)
Epoch: [170][128/196]	Time 0.437 (0.479)	Data 0.000 (0.006)	Loss 0.8227 (0.7986)	Acc@1 69.922 (72.257)	Acc@5 96.875 (97.726)
Epoch: [170][192/196]	Time 0.580 (0.483)	Data 0.000 (0.004)	Loss 0.8506 (0.7984)	Acc@1 69.922 (72.314)	Acc@5 96.484 (97.784)
after train
test acc: 44.09
Epoche: [171/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [171][0/196]	Time 0.810 (0.810)	Data 0.558 (0.558)	Loss 0.7718 (0.7718)	Acc@1 70.312 (70.312)	Acc@5 98.047 (98.047)
Epoch: [171][64/196]	Time 0.319 (0.496)	Data 0.000 (0.010)	Loss 0.7664 (0.7882)	Acc@1 70.312 (72.290)	Acc@5 98.438 (98.059)
Epoch: [171][128/196]	Time 0.613 (0.506)	Data 0.000 (0.005)	Loss 0.8535 (0.7867)	Acc@1 72.656 (72.496)	Acc@5 98.047 (97.971)
Epoch: [171][192/196]	Time 0.371 (0.500)	Data 0.000 (0.004)	Loss 0.7877 (0.7894)	Acc@1 69.922 (72.355)	Acc@5 99.219 (97.936)
after train
test acc: 66.58
Epoche: [172/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [172][0/196]	Time 0.697 (0.697)	Data 0.737 (0.737)	Loss 0.7449 (0.7449)	Acc@1 72.656 (72.656)	Acc@5 98.438 (98.438)
Epoch: [172][64/196]	Time 0.423 (0.502)	Data 0.000 (0.012)	Loss 0.7762 (0.7840)	Acc@1 73.828 (72.963)	Acc@5 98.047 (97.855)
Epoch: [172][128/196]	Time 0.572 (0.494)	Data 0.005 (0.007)	Loss 0.7433 (0.7884)	Acc@1 73.047 (72.747)	Acc@5 98.438 (97.865)
Epoch: [172][192/196]	Time 0.643 (0.481)	Data 0.000 (0.005)	Loss 0.6564 (0.7933)	Acc@1 77.344 (72.513)	Acc@5 98.438 (97.840)
after train
test acc: 59.68
Epoche: [173/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [173][0/196]	Time 0.622 (0.622)	Data 0.584 (0.584)	Loss 0.7154 (0.7154)	Acc@1 74.219 (74.219)	Acc@5 99.219 (99.219)
Epoch: [173][64/196]	Time 0.388 (0.500)	Data 0.000 (0.010)	Loss 0.8764 (0.7854)	Acc@1 70.312 (72.410)	Acc@5 96.875 (98.011)
Epoch: [173][128/196]	Time 0.461 (0.493)	Data 0.000 (0.006)	Loss 0.8097 (0.7798)	Acc@1 73.828 (72.841)	Acc@5 97.656 (97.980)
Epoch: [173][192/196]	Time 0.382 (0.496)	Data 0.000 (0.004)	Loss 0.7468 (0.7828)	Acc@1 75.391 (72.731)	Acc@5 98.828 (98.004)
after train
test acc: 42.89
Epoche: [174/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [174][0/196]	Time 0.602 (0.602)	Data 0.495 (0.495)	Loss 0.7377 (0.7377)	Acc@1 75.781 (75.781)	Acc@5 98.047 (98.047)
Epoch: [174][64/196]	Time 0.321 (0.500)	Data 0.016 (0.009)	Loss 0.6704 (0.7713)	Acc@1 74.219 (73.023)	Acc@5 98.438 (97.963)
Epoch: [174][128/196]	Time 0.494 (0.493)	Data 0.000 (0.005)	Loss 0.7881 (0.7844)	Acc@1 72.266 (72.638)	Acc@5 98.438 (97.889)
Epoch: [174][192/196]	Time 0.545 (0.497)	Data 0.000 (0.004)	Loss 0.7713 (0.7895)	Acc@1 71.094 (72.509)	Acc@5 98.438 (97.832)
after train
test acc: 47.68
Epoche: [175/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [175][0/196]	Time 0.489 (0.489)	Data 0.772 (0.772)	Loss 0.7980 (0.7980)	Acc@1 68.359 (68.359)	Acc@5 98.828 (98.828)
Epoch: [175][64/196]	Time 0.437 (0.500)	Data 0.000 (0.013)	Loss 0.7555 (0.7634)	Acc@1 75.781 (73.576)	Acc@5 98.047 (98.095)
Epoch: [175][128/196]	Time 0.506 (0.500)	Data 0.000 (0.007)	Loss 0.6612 (0.7713)	Acc@1 75.000 (73.319)	Acc@5 98.828 (97.959)
Epoch: [175][192/196]	Time 0.687 (0.496)	Data 0.000 (0.005)	Loss 0.8053 (0.7797)	Acc@1 75.391 (72.970)	Acc@5 96.484 (97.917)
after train
test acc: 32.96
Epoche: [176/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [176][0/196]	Time 0.437 (0.437)	Data 0.586 (0.586)	Loss 0.7243 (0.7243)	Acc@1 76.172 (76.172)	Acc@5 97.266 (97.266)
Epoch: [176][64/196]	Time 0.477 (0.456)	Data 0.000 (0.011)	Loss 0.6925 (0.7893)	Acc@1 78.125 (72.500)	Acc@5 99.609 (97.831)
Epoch: [176][128/196]	Time 0.540 (0.478)	Data 0.000 (0.006)	Loss 0.8064 (0.7867)	Acc@1 73.047 (72.708)	Acc@5 96.094 (97.892)
Epoch: [176][192/196]	Time 0.550 (0.489)	Data 0.000 (0.005)	Loss 0.8181 (0.7878)	Acc@1 69.922 (72.606)	Acc@5 97.656 (97.911)
after train
test acc: 41.6
Epoche: [177/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [177][0/196]	Time 0.714 (0.714)	Data 0.766 (0.766)	Loss 0.8773 (0.8773)	Acc@1 66.797 (66.797)	Acc@5 96.875 (96.875)
Epoch: [177][64/196]	Time 0.502 (0.504)	Data 0.000 (0.013)	Loss 0.7767 (0.7829)	Acc@1 73.047 (72.500)	Acc@5 98.438 (97.975)
Epoch: [177][128/196]	Time 0.408 (0.523)	Data 0.000 (0.007)	Loss 0.7420 (0.7781)	Acc@1 75.781 (72.835)	Acc@5 97.656 (97.977)
Epoch: [177][192/196]	Time 0.558 (0.519)	Data 0.000 (0.005)	Loss 0.7757 (0.7790)	Acc@1 73.828 (72.934)	Acc@5 97.266 (97.944)
after train
test acc: 65.05
Epoche: [178/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [178][0/196]	Time 0.744 (0.744)	Data 0.490 (0.490)	Loss 0.8724 (0.8724)	Acc@1 69.531 (69.531)	Acc@5 95.703 (95.703)
Epoch: [178][64/196]	Time 0.506 (0.470)	Data 0.000 (0.009)	Loss 0.6917 (0.7890)	Acc@1 76.172 (72.506)	Acc@5 97.266 (97.867)
Epoch: [178][128/196]	Time 0.427 (0.493)	Data 0.000 (0.006)	Loss 0.8762 (0.7823)	Acc@1 70.312 (72.947)	Acc@5 96.094 (97.889)
Epoch: [178][192/196]	Time 0.501 (0.502)	Data 0.000 (0.004)	Loss 0.8404 (0.7849)	Acc@1 72.266 (72.751)	Acc@5 96.094 (97.897)
after train
test acc: 50.58
Epoche: [179/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [179][0/196]	Time 0.426 (0.426)	Data 0.532 (0.532)	Loss 0.7982 (0.7982)	Acc@1 69.922 (69.922)	Acc@5 97.656 (97.656)
Epoch: [179][64/196]	Time 0.528 (0.510)	Data 0.000 (0.010)	Loss 0.7595 (0.7817)	Acc@1 71.875 (72.770)	Acc@5 97.266 (97.945)
Epoch: [179][128/196]	Time 0.485 (0.496)	Data 0.005 (0.006)	Loss 0.7682 (0.7718)	Acc@1 74.609 (73.156)	Acc@5 96.484 (97.992)
Epoch: [179][192/196]	Time 0.473 (0.493)	Data 0.000 (0.004)	Loss 0.8602 (0.7810)	Acc@1 71.484 (72.851)	Acc@5 97.656 (98.019)
after train
test acc: 60.88
Epoche: [180/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [180][0/196]	Time 0.722 (0.722)	Data 0.689 (0.689)	Loss 0.8070 (0.8070)	Acc@1 70.312 (70.312)	Acc@5 97.266 (97.266)
Epoch: [180][64/196]	Time 0.504 (0.513)	Data 0.000 (0.012)	Loss 0.8819 (0.7769)	Acc@1 70.312 (72.596)	Acc@5 96.875 (97.975)
Epoch: [180][128/196]	Time 0.511 (0.504)	Data 0.000 (0.007)	Loss 0.8194 (0.7788)	Acc@1 71.484 (72.747)	Acc@5 98.438 (97.998)
Epoch: [180][192/196]	Time 0.505 (0.501)	Data 0.000 (0.005)	Loss 0.8129 (0.7782)	Acc@1 70.703 (72.790)	Acc@5 98.047 (97.944)
after train
test acc: 66.35
Epoche: [181/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [181][0/196]	Time 0.579 (0.579)	Data 0.640 (0.640)	Loss 0.7297 (0.7297)	Acc@1 78.125 (78.125)	Acc@5 97.656 (97.656)
Epoch: [181][64/196]	Time 0.744 (0.525)	Data 0.000 (0.012)	Loss 0.7184 (0.7741)	Acc@1 74.219 (73.209)	Acc@5 97.656 (98.095)
Epoch: [181][128/196]	Time 0.611 (0.517)	Data 0.000 (0.006)	Loss 0.6616 (0.7745)	Acc@1 76.953 (73.174)	Acc@5 98.438 (98.050)
Epoch: [181][192/196]	Time 0.402 (0.517)	Data 0.000 (0.005)	Loss 0.7723 (0.7738)	Acc@1 75.000 (73.085)	Acc@5 98.438 (98.021)
after train
test acc: 62.32
Epoche: [182/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [182][0/196]	Time 0.509 (0.509)	Data 0.889 (0.889)	Loss 0.8467 (0.8467)	Acc@1 69.922 (69.922)	Acc@5 98.438 (98.438)
Epoch: [182][64/196]	Time 0.537 (0.481)	Data 0.000 (0.016)	Loss 0.7744 (0.7779)	Acc@1 71.484 (72.843)	Acc@5 98.828 (98.041)
Epoch: [182][128/196]	Time 0.437 (0.474)	Data 0.000 (0.009)	Loss 0.7534 (0.7682)	Acc@1 74.609 (73.174)	Acc@5 97.266 (98.089)
Epoch: [182][192/196]	Time 0.432 (0.478)	Data 0.000 (0.006)	Loss 0.7183 (0.7738)	Acc@1 74.219 (73.035)	Acc@5 98.047 (98.023)
after train
test acc: 50.78
Epoche: [183/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [183][0/196]	Time 0.394 (0.394)	Data 0.721 (0.721)	Loss 0.8157 (0.8157)	Acc@1 70.703 (70.703)	Acc@5 96.875 (96.875)
Epoch: [183][64/196]	Time 0.529 (0.508)	Data 0.000 (0.013)	Loss 0.8926 (0.7772)	Acc@1 68.750 (73.359)	Acc@5 97.656 (97.819)
Epoch: [183][128/196]	Time 0.402 (0.488)	Data 0.000 (0.007)	Loss 0.6962 (0.7690)	Acc@1 76.172 (73.459)	Acc@5 99.219 (97.959)
Epoch: [183][192/196]	Time 0.382 (0.491)	Data 0.000 (0.005)	Loss 0.9316 (0.7671)	Acc@1 69.531 (73.535)	Acc@5 97.656 (97.968)
after train
test acc: 35.04
Epoche: [184/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [184][0/196]	Time 0.348 (0.348)	Data 0.666 (0.666)	Loss 0.8468 (0.8468)	Acc@1 69.141 (69.141)	Acc@5 98.047 (98.047)
Epoch: [184][64/196]	Time 0.483 (0.474)	Data 0.025 (0.012)	Loss 0.7534 (0.7698)	Acc@1 75.391 (73.275)	Acc@5 98.438 (97.933)
Epoch: [184][128/196]	Time 0.436 (0.474)	Data 0.000 (0.007)	Loss 0.7030 (0.7704)	Acc@1 76.562 (73.216)	Acc@5 98.438 (97.932)
Epoch: [184][192/196]	Time 0.520 (0.485)	Data 0.000 (0.005)	Loss 0.7538 (0.7709)	Acc@1 75.000 (73.239)	Acc@5 97.266 (97.936)
after train
test acc: 66.95
Epoche: [185/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [185][0/196]	Time 0.538 (0.538)	Data 0.559 (0.559)	Loss 0.7419 (0.7419)	Acc@1 75.781 (75.781)	Acc@5 98.828 (98.828)
Epoch: [185][64/196]	Time 0.330 (0.501)	Data 0.000 (0.009)	Loss 0.9120 (0.7773)	Acc@1 69.141 (72.915)	Acc@5 98.047 (98.029)
Epoch: [185][128/196]	Time 0.522 (0.491)	Data 0.000 (0.006)	Loss 0.8151 (0.7768)	Acc@1 74.219 (72.883)	Acc@5 98.438 (98.026)
Epoch: [185][192/196]	Time 0.541 (0.492)	Data 0.000 (0.004)	Loss 0.7281 (0.7786)	Acc@1 75.391 (72.919)	Acc@5 98.047 (97.962)
after train
test acc: 42.2
Epoche: [186/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [186][0/196]	Time 0.490 (0.490)	Data 0.730 (0.730)	Loss 0.6524 (0.6524)	Acc@1 75.391 (75.391)	Acc@5 98.828 (98.828)
Epoch: [186][64/196]	Time 0.840 (0.498)	Data 0.000 (0.012)	Loss 0.7009 (0.7687)	Acc@1 75.781 (73.221)	Acc@5 99.219 (97.963)
Epoch: [186][128/196]	Time 0.580 (0.483)	Data 0.000 (0.007)	Loss 0.7689 (0.7698)	Acc@1 73.047 (73.268)	Acc@5 98.438 (97.974)
Epoch: [186][192/196]	Time 0.489 (0.481)	Data 0.000 (0.005)	Loss 0.6650 (0.7752)	Acc@1 79.297 (73.211)	Acc@5 99.219 (97.905)
after train
test acc: 44.66
Epoche: [187/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [187][0/196]	Time 0.747 (0.747)	Data 0.593 (0.593)	Loss 0.8496 (0.8496)	Acc@1 71.094 (71.094)	Acc@5 96.484 (96.484)
Epoch: [187][64/196]	Time 0.418 (0.490)	Data 0.000 (0.010)	Loss 0.7922 (0.7696)	Acc@1 70.312 (73.257)	Acc@5 98.047 (98.119)
Epoch: [187][128/196]	Time 0.424 (0.483)	Data 0.000 (0.006)	Loss 0.8131 (0.7622)	Acc@1 70.703 (73.431)	Acc@5 97.266 (98.038)
Epoch: [187][192/196]	Time 0.385 (0.497)	Data 0.000 (0.005)	Loss 0.8236 (0.7663)	Acc@1 70.312 (73.336)	Acc@5 99.609 (97.960)
after train
test acc: 59.73
Epoche: [188/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [188][0/196]	Time 0.299 (0.299)	Data 0.728 (0.728)	Loss 0.7976 (0.7976)	Acc@1 75.000 (75.000)	Acc@5 98.047 (98.047)
Epoch: [188][64/196]	Time 0.386 (0.499)	Data 0.000 (0.013)	Loss 0.7961 (0.7765)	Acc@1 73.047 (73.233)	Acc@5 98.047 (97.891)
Epoch: [188][128/196]	Time 0.288 (0.498)	Data 0.001 (0.007)	Loss 0.7428 (0.7665)	Acc@1 74.609 (73.410)	Acc@5 96.484 (97.917)
Epoch: [188][192/196]	Time 0.518 (0.493)	Data 0.000 (0.005)	Loss 0.8177 (0.7666)	Acc@1 70.703 (73.395)	Acc@5 98.438 (97.954)
after train
test acc: 58.61
Epoche: [189/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [189][0/196]	Time 0.530 (0.530)	Data 0.632 (0.632)	Loss 0.6838 (0.6838)	Acc@1 76.172 (76.172)	Acc@5 96.484 (96.484)
Epoch: [189][64/196]	Time 0.389 (0.492)	Data 0.000 (0.011)	Loss 0.9236 (0.7737)	Acc@1 64.844 (73.221)	Acc@5 97.656 (97.873)
Epoch: [189][128/196]	Time 0.470 (0.493)	Data 0.000 (0.006)	Loss 0.6201 (0.7708)	Acc@1 79.688 (73.244)	Acc@5 97.656 (97.917)
Epoch: [189][192/196]	Time 0.406 (0.500)	Data 0.000 (0.004)	Loss 0.7408 (0.7670)	Acc@1 76.953 (73.431)	Acc@5 98.047 (97.887)
after train
test acc: 34.13
Epoche: [190/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [190][0/196]	Time 0.624 (0.624)	Data 0.724 (0.724)	Loss 0.8456 (0.8456)	Acc@1 70.312 (70.312)	Acc@5 96.875 (96.875)
Epoch: [190][64/196]	Time 0.309 (0.510)	Data 0.000 (0.013)	Loss 0.7118 (0.7650)	Acc@1 76.172 (73.582)	Acc@5 98.047 (98.113)
Epoch: [190][128/196]	Time 0.505 (0.500)	Data 0.005 (0.007)	Loss 0.7190 (0.7716)	Acc@1 73.438 (73.286)	Acc@5 99.219 (98.086)
Epoch: [190][192/196]	Time 0.557 (0.500)	Data 0.000 (0.005)	Loss 0.7768 (0.7652)	Acc@1 73.828 (73.502)	Acc@5 97.266 (98.095)
after train
test acc: 55.16
Epoche: [191/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [191][0/196]	Time 0.572 (0.572)	Data 0.523 (0.523)	Loss 0.7945 (0.7945)	Acc@1 74.609 (74.609)	Acc@5 98.047 (98.047)
Epoch: [191][64/196]	Time 0.440 (0.512)	Data 0.000 (0.009)	Loss 0.7623 (0.7554)	Acc@1 74.219 (74.177)	Acc@5 97.266 (98.017)
Epoch: [191][128/196]	Time 0.462 (0.508)	Data 0.000 (0.006)	Loss 0.7199 (0.7526)	Acc@1 72.266 (74.037)	Acc@5 98.828 (98.050)
Epoch: [191][192/196]	Time 0.581 (0.499)	Data 0.000 (0.004)	Loss 0.8242 (0.7562)	Acc@1 69.531 (73.909)	Acc@5 97.266 (98.097)
after train
test acc: 51.63
Epoche: [192/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [192][0/196]	Time 0.563 (0.563)	Data 0.702 (0.702)	Loss 0.6410 (0.6410)	Acc@1 80.469 (80.469)	Acc@5 98.047 (98.047)
Epoch: [192][64/196]	Time 0.501 (0.507)	Data 0.000 (0.012)	Loss 0.7505 (0.7625)	Acc@1 73.047 (73.257)	Acc@5 98.438 (98.035)
Epoch: [192][128/196]	Time 0.289 (0.484)	Data 0.000 (0.007)	Loss 0.7122 (0.7580)	Acc@1 73.047 (73.501)	Acc@5 98.047 (98.095)
Epoch: [192][192/196]	Time 0.395 (0.483)	Data 0.000 (0.005)	Loss 0.7823 (0.7574)	Acc@1 71.875 (73.520)	Acc@5 97.266 (98.071)
after train
test acc: 58.38
Epoche: [193/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [193][0/196]	Time 0.658 (0.658)	Data 0.648 (0.648)	Loss 0.6888 (0.6888)	Acc@1 72.266 (72.266)	Acc@5 100.000 (100.000)
Epoch: [193][64/196]	Time 0.675 (0.528)	Data 0.000 (0.011)	Loss 0.7635 (0.7632)	Acc@1 71.094 (73.407)	Acc@5 98.438 (98.161)
Epoch: [193][128/196]	Time 0.519 (0.511)	Data 0.000 (0.006)	Loss 0.7328 (0.7625)	Acc@1 72.266 (73.474)	Acc@5 99.609 (98.089)
Epoch: [193][192/196]	Time 0.309 (0.505)	Data 0.000 (0.005)	Loss 0.6039 (0.7632)	Acc@1 77.344 (73.488)	Acc@5 98.047 (97.998)
after train
test acc: 59.89
Epoche: [194/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [194][0/196]	Time 0.850 (0.850)	Data 0.622 (0.622)	Loss 0.8087 (0.8087)	Acc@1 71.484 (71.484)	Acc@5 96.875 (96.875)
Epoch: [194][64/196]	Time 0.485 (0.522)	Data 0.000 (0.011)	Loss 0.6689 (0.7529)	Acc@1 78.125 (73.942)	Acc@5 100.000 (97.963)
Epoch: [194][128/196]	Time 0.533 (0.504)	Data 0.000 (0.006)	Loss 0.6473 (0.7560)	Acc@1 79.297 (73.913)	Acc@5 97.656 (98.074)
Epoch: [194][192/196]	Time 0.448 (0.493)	Data 0.000 (0.005)	Loss 0.8073 (0.7534)	Acc@1 71.875 (73.893)	Acc@5 98.047 (98.091)
after train
test acc: 55.5
Epoche: [195/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [195][0/196]	Time 0.412 (0.412)	Data 0.844 (0.844)	Loss 0.6691 (0.6691)	Acc@1 76.953 (76.953)	Acc@5 97.656 (97.656)
Epoch: [195][64/196]	Time 0.496 (0.495)	Data 0.007 (0.014)	Loss 0.7973 (0.7725)	Acc@1 74.219 (72.855)	Acc@5 98.438 (98.089)
Epoch: [195][128/196]	Time 0.510 (0.484)	Data 0.000 (0.008)	Loss 0.6908 (0.7540)	Acc@1 76.562 (73.528)	Acc@5 98.828 (98.101)
Epoch: [195][192/196]	Time 0.619 (0.491)	Data 0.000 (0.006)	Loss 0.8566 (0.7541)	Acc@1 72.266 (73.680)	Acc@5 96.875 (98.091)
after train
test acc: 43.71
Epoche: [196/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [196][0/196]	Time 0.330 (0.330)	Data 0.686 (0.686)	Loss 0.8068 (0.8068)	Acc@1 71.484 (71.484)	Acc@5 98.828 (98.828)
Epoch: [196][64/196]	Time 0.288 (0.475)	Data 0.000 (0.012)	Loss 0.6986 (0.7473)	Acc@1 76.953 (74.309)	Acc@5 97.656 (98.113)
Epoch: [196][128/196]	Time 0.392 (0.464)	Data 0.000 (0.007)	Loss 0.6914 (0.7600)	Acc@1 76.562 (73.746)	Acc@5 99.609 (98.047)
Epoch: [196][192/196]	Time 0.507 (0.474)	Data 0.000 (0.005)	Loss 0.6092 (0.7570)	Acc@1 78.516 (73.767)	Acc@5 99.219 (98.087)
after train
test acc: 62.88
Epoche: [197/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [197][0/196]	Time 0.581 (0.581)	Data 0.603 (0.603)	Loss 0.7873 (0.7873)	Acc@1 76.953 (76.953)	Acc@5 97.266 (97.266)
Epoch: [197][64/196]	Time 0.487 (0.508)	Data 0.000 (0.010)	Loss 0.7543 (0.7543)	Acc@1 72.656 (73.984)	Acc@5 97.656 (98.083)
Epoch: [197][128/196]	Time 0.287 (0.483)	Data 0.000 (0.006)	Loss 0.6765 (0.7552)	Acc@1 73.047 (73.895)	Acc@5 99.609 (98.014)
Epoch: [197][192/196]	Time 0.422 (0.484)	Data 0.000 (0.004)	Loss 0.7744 (0.7540)	Acc@1 71.094 (73.946)	Acc@5 96.094 (98.019)
after train
test acc: 61.97
Epoche: [198/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [198][0/196]	Time 0.658 (0.658)	Data 0.542 (0.542)	Loss 0.6922 (0.6922)	Acc@1 76.172 (76.172)	Acc@5 99.219 (99.219)
Epoch: [198][64/196]	Time 0.316 (0.501)	Data 0.000 (0.010)	Loss 0.6944 (0.7543)	Acc@1 78.906 (73.930)	Acc@5 98.047 (97.825)
Epoch: [198][128/196]	Time 0.387 (0.480)	Data 0.000 (0.006)	Loss 0.7885 (0.7495)	Acc@1 74.219 (74.079)	Acc@5 98.047 (98.008)
Epoch: [198][192/196]	Time 0.757 (0.486)	Data 0.000 (0.004)	Loss 0.7096 (0.7504)	Acc@1 76.172 (73.946)	Acc@5 97.266 (98.004)
after train
test acc: 59.27
Epoche: [199/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [199][0/196]	Time 0.644 (0.644)	Data 0.775 (0.775)	Loss 0.6651 (0.6651)	Acc@1 76.562 (76.562)	Acc@5 98.047 (98.047)
Epoch: [199][64/196]	Time 0.534 (0.498)	Data 0.000 (0.013)	Loss 0.9410 (0.7569)	Acc@1 67.188 (73.492)	Acc@5 97.656 (98.341)
Epoch: [199][128/196]	Time 0.570 (0.491)	Data 0.000 (0.007)	Loss 0.8745 (0.7597)	Acc@1 70.312 (73.649)	Acc@5 96.875 (98.092)
Epoch: [199][192/196]	Time 0.427 (0.488)	Data 0.000 (0.006)	Loss 0.7800 (0.7591)	Acc@1 71.094 (73.705)	Acc@5 98.828 (98.077)
after train
test acc: 57.77
Epoche: [200/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [200][0/196]	Time 0.613 (0.613)	Data 0.453 (0.453)	Loss 0.7313 (0.7313)	Acc@1 73.047 (73.047)	Acc@5 98.438 (98.438)
Epoch: [200][64/196]	Time 0.558 (0.511)	Data 0.000 (0.008)	Loss 0.8329 (0.7470)	Acc@1 69.141 (74.062)	Acc@5 97.266 (97.951)
Epoch: [200][128/196]	Time 0.509 (0.493)	Data 0.000 (0.005)	Loss 0.7682 (0.7522)	Acc@1 73.047 (73.952)	Acc@5 98.828 (98.080)
Epoch: [200][192/196]	Time 0.695 (0.487)	Data 0.000 (0.004)	Loss 0.7344 (0.7481)	Acc@1 75.781 (74.168)	Acc@5 97.266 (98.041)
after train
test acc: 53.5
Epoche: [201/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [201][0/196]	Time 0.490 (0.490)	Data 0.655 (0.655)	Loss 0.5852 (0.5852)	Acc@1 80.469 (80.469)	Acc@5 99.609 (99.609)
Epoch: [201][64/196]	Time 0.589 (0.468)	Data 0.000 (0.012)	Loss 0.6489 (0.7583)	Acc@1 76.953 (73.972)	Acc@5 98.828 (98.167)
Epoch: [201][128/196]	Time 0.301 (0.421)	Data 0.000 (0.006)	Loss 0.7333 (0.7595)	Acc@1 76.562 (73.916)	Acc@5 99.219 (98.038)
Epoch: [201][192/196]	Time 0.271 (0.394)	Data 0.000 (0.005)	Loss 0.7010 (0.7560)	Acc@1 75.391 (73.996)	Acc@5 98.047 (98.063)
after train
test acc: 50.96
Epoche: [202/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [202][0/196]	Time 0.385 (0.385)	Data 0.508 (0.508)	Loss 0.7605 (0.7605)	Acc@1 75.000 (75.000)	Acc@5 97.266 (97.266)
Epoch: [202][64/196]	Time 0.315 (0.287)	Data 0.000 (0.009)	Loss 0.7320 (0.7545)	Acc@1 74.609 (74.105)	Acc@5 97.656 (98.233)
Epoch: [202][128/196]	Time 0.258 (0.285)	Data 0.000 (0.005)	Loss 0.6288 (0.7540)	Acc@1 78.906 (73.910)	Acc@5 99.609 (98.126)
Epoch: [202][192/196]	Time 0.289 (0.278)	Data 0.000 (0.003)	Loss 0.6722 (0.7505)	Acc@1 76.953 (73.911)	Acc@5 98.828 (98.146)
after train
test acc: 58.92
Epoche: [203/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [203][0/196]	Time 0.363 (0.363)	Data 0.527 (0.527)	Loss 0.6438 (0.6438)	Acc@1 78.125 (78.125)	Acc@5 99.219 (99.219)
Epoch: [203][64/196]	Time 0.324 (0.288)	Data 0.000 (0.009)	Loss 0.6799 (0.7324)	Acc@1 78.906 (74.742)	Acc@5 98.047 (98.113)
Epoch: [203][128/196]	Time 0.372 (0.294)	Data 0.000 (0.005)	Loss 0.6764 (0.7485)	Acc@1 76.562 (74.055)	Acc@5 98.047 (98.144)
Epoch: [203][192/196]	Time 0.316 (0.281)	Data 0.000 (0.003)	Loss 0.7317 (0.7483)	Acc@1 72.656 (74.079)	Acc@5 99.219 (98.160)
after train
test acc: 65.76
Epoche: [204/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [204][0/196]	Time 0.371 (0.371)	Data 0.501 (0.501)	Loss 0.6861 (0.6861)	Acc@1 76.172 (76.172)	Acc@5 99.219 (99.219)
Epoch: [204][64/196]	Time 0.219 (0.288)	Data 0.000 (0.009)	Loss 0.7944 (0.7500)	Acc@1 72.266 (73.642)	Acc@5 97.266 (98.095)
Epoch: [204][128/196]	Time 0.324 (0.288)	Data 0.000 (0.005)	Loss 0.7569 (0.7445)	Acc@1 73.828 (73.928)	Acc@5 98.047 (98.113)
Epoch: [204][192/196]	Time 0.367 (0.281)	Data 0.000 (0.004)	Loss 0.7711 (0.7517)	Acc@1 71.875 (73.852)	Acc@5 98.438 (98.041)
after train
test acc: 39.5
Epoche: [205/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [205][0/196]	Time 0.398 (0.398)	Data 0.796 (0.796)	Loss 0.7577 (0.7577)	Acc@1 73.047 (73.047)	Acc@5 99.219 (99.219)
Epoch: [205][64/196]	Time 0.356 (0.296)	Data 0.000 (0.014)	Loss 0.7306 (0.7409)	Acc@1 72.266 (74.357)	Acc@5 98.438 (98.125)
Epoch: [205][128/196]	Time 0.292 (0.293)	Data 0.000 (0.008)	Loss 0.7466 (0.7355)	Acc@1 75.000 (74.434)	Acc@5 98.438 (98.174)
Epoch: [205][192/196]	Time 0.283 (0.281)	Data 0.000 (0.005)	Loss 0.7227 (0.7412)	Acc@1 74.219 (74.364)	Acc@5 99.219 (98.162)
after train
test acc: 54.32
Epoche: [206/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [206][0/196]	Time 0.272 (0.272)	Data 0.632 (0.632)	Loss 0.8517 (0.8517)	Acc@1 72.266 (72.266)	Acc@5 96.094 (96.094)
Epoch: [206][64/196]	Time 0.216 (0.278)	Data 0.000 (0.010)	Loss 0.8491 (0.7648)	Acc@1 71.094 (73.672)	Acc@5 97.266 (97.981)
Epoch: [206][128/196]	Time 0.230 (0.277)	Data 0.000 (0.006)	Loss 0.8856 (0.7511)	Acc@1 70.703 (74.167)	Acc@5 96.875 (98.065)
Epoch: [206][192/196]	Time 0.334 (0.274)	Data 0.000 (0.004)	Loss 0.7002 (0.7506)	Acc@1 76.562 (74.176)	Acc@5 98.828 (98.029)
after train
test acc: 56.62
Epoche: [207/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [207][0/196]	Time 0.326 (0.326)	Data 0.626 (0.626)	Loss 0.7022 (0.7022)	Acc@1 74.219 (74.219)	Acc@5 98.047 (98.047)
Epoch: [207][64/196]	Time 0.272 (0.291)	Data 0.000 (0.011)	Loss 0.8889 (0.7442)	Acc@1 69.922 (74.099)	Acc@5 98.828 (98.347)
Epoch: [207][128/196]	Time 0.313 (0.290)	Data 0.000 (0.006)	Loss 0.7266 (0.7498)	Acc@1 73.438 (73.931)	Acc@5 98.047 (98.135)
Epoch: [207][192/196]	Time 0.349 (0.287)	Data 0.000 (0.004)	Loss 0.8229 (0.7497)	Acc@1 74.219 (73.990)	Acc@5 98.047 (98.128)
after train
test acc: 60.98
Epoche: [208/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [208][0/196]	Time 0.352 (0.352)	Data 0.475 (0.475)	Loss 0.8147 (0.8147)	Acc@1 71.094 (71.094)	Acc@5 97.656 (97.656)
Epoch: [208][64/196]	Time 0.227 (0.278)	Data 0.000 (0.009)	Loss 0.7126 (0.7503)	Acc@1 75.781 (74.050)	Acc@5 98.828 (98.275)
Epoch: [208][128/196]	Time 0.273 (0.284)	Data 0.000 (0.005)	Loss 0.6661 (0.7528)	Acc@1 76.562 (73.970)	Acc@5 99.219 (98.141)
Epoch: [208][192/196]	Time 0.342 (0.277)	Data 0.000 (0.004)	Loss 0.7333 (0.7512)	Acc@1 73.438 (73.861)	Acc@5 98.438 (98.128)
after train
test acc: 26.74
Epoche: [209/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [209][0/196]	Time 0.312 (0.312)	Data 0.502 (0.502)	Loss 0.6210 (0.6210)	Acc@1 78.906 (78.906)	Acc@5 98.438 (98.438)
Epoch: [209][64/196]	Time 0.327 (0.299)	Data 0.000 (0.009)	Loss 0.7192 (0.7601)	Acc@1 76.953 (73.438)	Acc@5 98.047 (98.191)
Epoch: [209][128/196]	Time 0.399 (0.293)	Data 0.000 (0.005)	Loss 0.7572 (0.7500)	Acc@1 75.000 (73.983)	Acc@5 98.438 (98.110)
Epoch: [209][192/196]	Time 0.279 (0.286)	Data 0.000 (0.004)	Loss 0.6992 (0.7458)	Acc@1 77.734 (74.101)	Acc@5 98.828 (98.195)
after train
test acc: 29.44
Epoche: [210/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [210][0/196]	Time 0.336 (0.336)	Data 0.708 (0.708)	Loss 0.7511 (0.7511)	Acc@1 73.438 (73.438)	Acc@5 98.047 (98.047)
Epoch: [210][64/196]	Time 0.256 (0.294)	Data 0.000 (0.012)	Loss 0.8061 (0.7502)	Acc@1 69.141 (73.690)	Acc@5 97.656 (98.035)
Epoch: [210][128/196]	Time 0.150 (0.294)	Data 0.000 (0.007)	Loss 0.8057 (0.7486)	Acc@1 73.828 (74.055)	Acc@5 98.438 (98.047)
Epoch: [210][192/196]	Time 0.287 (0.292)	Data 0.000 (0.005)	Loss 0.7441 (0.7488)	Acc@1 75.781 (74.006)	Acc@5 97.266 (98.061)
after train
test acc: 34.78
Epoche: [211/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [211][0/196]	Time 0.338 (0.338)	Data 0.505 (0.505)	Loss 0.7315 (0.7315)	Acc@1 71.875 (71.875)	Acc@5 99.219 (99.219)
Epoch: [211][64/196]	Time 0.401 (0.285)	Data 0.000 (0.008)	Loss 0.7710 (0.7504)	Acc@1 70.703 (73.588)	Acc@5 97.266 (98.203)
Epoch: [211][128/196]	Time 0.279 (0.287)	Data 0.000 (0.005)	Loss 0.7563 (0.7500)	Acc@1 75.391 (73.940)	Acc@5 99.219 (98.174)
Epoch: [211][192/196]	Time 0.317 (0.286)	Data 0.000 (0.003)	Loss 0.6969 (0.7498)	Acc@1 75.391 (73.970)	Acc@5 98.438 (98.203)
after train
test acc: 59.35
Epoche: [212/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [212][0/196]	Time 0.279 (0.279)	Data 0.552 (0.552)	Loss 0.6582 (0.6582)	Acc@1 76.953 (76.953)	Acc@5 98.828 (98.828)
Epoch: [212][64/196]	Time 0.290 (0.299)	Data 0.000 (0.010)	Loss 0.6113 (0.7472)	Acc@1 78.906 (73.612)	Acc@5 99.609 (98.155)
Epoch: [212][128/196]	Time 0.216 (0.289)	Data 0.000 (0.005)	Loss 0.7421 (0.7453)	Acc@1 73.438 (73.840)	Acc@5 98.438 (98.156)
Epoch: [212][192/196]	Time 0.269 (0.288)	Data 0.000 (0.004)	Loss 0.7579 (0.7446)	Acc@1 72.656 (74.008)	Acc@5 98.828 (98.187)
after train
test acc: 60.9
Epoche: [213/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [213][0/196]	Time 0.311 (0.311)	Data 0.729 (0.729)	Loss 0.6608 (0.6608)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [213][64/196]	Time 0.369 (0.268)	Data 0.000 (0.012)	Loss 0.8450 (0.7450)	Acc@1 70.703 (74.105)	Acc@5 96.875 (98.137)
Epoch: [213][128/196]	Time 0.275 (0.280)	Data 0.000 (0.006)	Loss 0.6927 (0.7467)	Acc@1 73.047 (74.076)	Acc@5 98.828 (98.089)
Epoch: [213][192/196]	Time 0.266 (0.269)	Data 0.000 (0.005)	Loss 0.7945 (0.7450)	Acc@1 70.312 (74.154)	Acc@5 99.609 (98.126)
after train
test acc: 58.22
Epoche: [214/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [214][0/196]	Time 0.477 (0.477)	Data 0.529 (0.529)	Loss 0.7914 (0.7914)	Acc@1 72.266 (72.266)	Acc@5 98.047 (98.047)
Epoch: [214][64/196]	Time 0.335 (0.309)	Data 0.000 (0.009)	Loss 0.6968 (0.7375)	Acc@1 78.906 (74.856)	Acc@5 98.047 (98.107)
Epoch: [214][128/196]	Time 0.179 (0.295)	Data 0.006 (0.006)	Loss 0.7330 (0.7328)	Acc@1 74.219 (74.812)	Acc@5 98.828 (98.153)
Epoch: [214][192/196]	Time 0.244 (0.293)	Data 0.000 (0.004)	Loss 0.8233 (0.7368)	Acc@1 74.219 (74.632)	Acc@5 96.484 (98.136)
after train
test acc: 53.72
Epoche: [215/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [215][0/196]	Time 0.319 (0.319)	Data 0.488 (0.488)	Loss 0.7063 (0.7063)	Acc@1 76.172 (76.172)	Acc@5 98.438 (98.438)
Epoch: [215][64/196]	Time 0.230 (0.293)	Data 0.000 (0.008)	Loss 0.7313 (0.7460)	Acc@1 74.609 (74.201)	Acc@5 98.438 (98.149)
Epoch: [215][128/196]	Time 0.325 (0.289)	Data 0.000 (0.005)	Loss 0.7439 (0.7433)	Acc@1 73.828 (74.410)	Acc@5 98.438 (98.156)
Epoch: [215][192/196]	Time 0.168 (0.285)	Data 0.000 (0.003)	Loss 0.7826 (0.7432)	Acc@1 73.438 (74.318)	Acc@5 97.656 (98.158)
after train
test acc: 63.27
Epoche: [216/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [216][0/196]	Time 0.355 (0.355)	Data 0.544 (0.544)	Loss 0.6100 (0.6100)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [216][64/196]	Time 0.280 (0.293)	Data 0.000 (0.009)	Loss 0.7637 (0.7361)	Acc@1 72.656 (74.573)	Acc@5 98.047 (98.119)
Epoch: [216][128/196]	Time 0.375 (0.296)	Data 0.000 (0.005)	Loss 0.7157 (0.7364)	Acc@1 76.562 (74.364)	Acc@5 98.438 (98.110)
Epoch: [216][192/196]	Time 0.331 (0.293)	Data 0.000 (0.004)	Loss 0.6903 (0.7391)	Acc@1 74.219 (74.360)	Acc@5 98.828 (98.138)
after train
test acc: 68.57
Epoche: [217/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [217][0/196]	Time 0.302 (0.302)	Data 0.465 (0.465)	Loss 0.7083 (0.7083)	Acc@1 75.000 (75.000)	Acc@5 97.656 (97.656)
Epoch: [217][64/196]	Time 0.445 (0.281)	Data 0.005 (0.008)	Loss 0.6776 (0.7394)	Acc@1 76.172 (73.828)	Acc@5 98.047 (98.275)
Epoch: [217][128/196]	Time 0.233 (0.277)	Data 0.002 (0.005)	Loss 0.6393 (0.7412)	Acc@1 77.734 (74.134)	Acc@5 98.828 (98.195)
Epoch: [217][192/196]	Time 0.212 (0.274)	Data 0.000 (0.003)	Loss 0.8491 (0.7452)	Acc@1 68.359 (73.982)	Acc@5 98.828 (98.118)
after train
test acc: 39.08
Epoche: [218/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [218][0/196]	Time 0.323 (0.323)	Data 0.480 (0.480)	Loss 0.7537 (0.7537)	Acc@1 73.828 (73.828)	Acc@5 97.656 (97.656)
Epoch: [218][64/196]	Time 0.232 (0.301)	Data 0.000 (0.009)	Loss 0.7761 (0.7507)	Acc@1 73.047 (74.026)	Acc@5 98.047 (98.185)
Epoch: [218][128/196]	Time 0.260 (0.287)	Data 0.000 (0.005)	Loss 0.8182 (0.7502)	Acc@1 69.922 (74.101)	Acc@5 98.438 (98.138)
Epoch: [218][192/196]	Time 0.369 (0.279)	Data 0.000 (0.004)	Loss 0.6046 (0.7458)	Acc@1 80.469 (74.134)	Acc@5 99.219 (98.154)
after train
test acc: 60.85
Epoche: [219/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [219][0/196]	Time 0.416 (0.416)	Data 0.534 (0.534)	Loss 0.7620 (0.7620)	Acc@1 75.781 (75.781)	Acc@5 97.266 (97.266)
Epoch: [219][64/196]	Time 0.359 (0.292)	Data 0.000 (0.009)	Loss 0.6454 (0.7420)	Acc@1 77.734 (74.333)	Acc@5 97.656 (98.209)
Epoch: [219][128/196]	Time 0.255 (0.293)	Data 0.000 (0.005)	Loss 0.7414 (0.7409)	Acc@1 73.438 (74.322)	Acc@5 97.266 (98.141)
Epoch: [219][192/196]	Time 0.153 (0.278)	Data 0.000 (0.004)	Loss 0.6951 (0.7392)	Acc@1 76.953 (74.344)	Acc@5 97.266 (98.191)
after train
test acc: 23.52
Epoche: [220/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [220][0/196]	Time 0.373 (0.373)	Data 0.588 (0.588)	Loss 0.7410 (0.7410)	Acc@1 75.781 (75.781)	Acc@5 98.438 (98.438)
Epoch: [220][64/196]	Time 0.263 (0.285)	Data 0.000 (0.010)	Loss 0.7332 (0.7469)	Acc@1 71.484 (74.447)	Acc@5 98.828 (98.059)
Epoch: [220][128/196]	Time 0.243 (0.286)	Data 0.000 (0.005)	Loss 0.6025 (0.7416)	Acc@1 78.125 (74.470)	Acc@5 100.000 (98.177)
Epoch: [220][192/196]	Time 0.216 (0.282)	Data 0.000 (0.004)	Loss 0.7419 (0.7375)	Acc@1 75.391 (74.496)	Acc@5 96.875 (98.221)
after train
test acc: 61.22
Epoche: [221/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [221][0/196]	Time 0.395 (0.395)	Data 0.604 (0.604)	Loss 0.7343 (0.7343)	Acc@1 73.047 (73.047)	Acc@5 99.219 (99.219)
Epoch: [221][64/196]	Time 0.317 (0.280)	Data 0.000 (0.010)	Loss 0.5652 (0.7269)	Acc@1 78.906 (74.525)	Acc@5 99.219 (98.275)
Epoch: [221][128/196]	Time 0.207 (0.286)	Data 0.000 (0.006)	Loss 0.7005 (0.7334)	Acc@1 76.172 (74.479)	Acc@5 97.656 (98.244)
Epoch: [221][192/196]	Time 0.235 (0.284)	Data 0.000 (0.004)	Loss 0.6521 (0.7338)	Acc@1 80.078 (74.553)	Acc@5 99.219 (98.207)
after train
test acc: 58.06
Epoche: [222/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [222][0/196]	Time 0.463 (0.463)	Data 0.555 (0.555)	Loss 0.6111 (0.6111)	Acc@1 77.734 (77.734)	Acc@5 98.438 (98.438)
Epoch: [222][64/196]	Time 0.277 (0.289)	Data 0.000 (0.009)	Loss 0.8527 (0.7437)	Acc@1 71.484 (74.357)	Acc@5 97.656 (98.107)
Epoch: [222][128/196]	Time 0.222 (0.290)	Data 0.000 (0.005)	Loss 0.8509 (0.7343)	Acc@1 71.094 (74.503)	Acc@5 99.219 (98.256)
Epoch: [222][192/196]	Time 0.210 (0.283)	Data 0.000 (0.004)	Loss 0.7444 (0.7380)	Acc@1 73.828 (74.508)	Acc@5 96.875 (98.176)
after train
test acc: 56.94
Epoche: [223/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [223][0/196]	Time 0.327 (0.327)	Data 0.517 (0.517)	Loss 0.7741 (0.7741)	Acc@1 75.781 (75.781)	Acc@5 96.875 (96.875)
Epoch: [223][64/196]	Time 0.208 (0.277)	Data 0.000 (0.009)	Loss 0.6196 (0.7408)	Acc@1 80.859 (74.561)	Acc@5 98.828 (97.999)
Epoch: [223][128/196]	Time 0.212 (0.276)	Data 0.000 (0.005)	Loss 0.6975 (0.7324)	Acc@1 75.391 (74.815)	Acc@5 97.656 (98.138)
Epoch: [223][192/196]	Time 0.176 (0.272)	Data 0.000 (0.004)	Loss 0.6726 (0.7381)	Acc@1 75.781 (74.561)	Acc@5 97.656 (98.116)
after train
test acc: 65.33
Epoche: [224/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [224][0/196]	Time 0.319 (0.319)	Data 0.694 (0.694)	Loss 0.7489 (0.7489)	Acc@1 73.828 (73.828)	Acc@5 98.438 (98.438)
Epoch: [224][64/196]	Time 0.238 (0.293)	Data 0.000 (0.011)	Loss 0.7585 (0.7240)	Acc@1 67.969 (75.108)	Acc@5 98.438 (98.155)
Epoch: [224][128/196]	Time 0.177 (0.294)	Data 0.000 (0.006)	Loss 0.6894 (0.7296)	Acc@1 78.125 (74.712)	Acc@5 98.047 (98.195)
Epoch: [224][192/196]	Time 0.421 (0.289)	Data 0.000 (0.005)	Loss 0.8085 (0.7319)	Acc@1 71.875 (74.678)	Acc@5 96.875 (98.134)
after train
test acc: 62.04
Epoche: [225/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [225][0/196]	Time 0.382 (0.382)	Data 0.482 (0.482)	Loss 0.6630 (0.6630)	Acc@1 75.781 (75.781)	Acc@5 99.219 (99.219)
Epoch: [225][64/196]	Time 0.198 (0.298)	Data 0.000 (0.009)	Loss 0.7642 (0.7330)	Acc@1 72.266 (74.706)	Acc@5 98.438 (98.191)
Epoch: [225][128/196]	Time 0.233 (0.289)	Data 0.000 (0.005)	Loss 0.7806 (0.7339)	Acc@1 73.438 (74.534)	Acc@5 97.656 (98.238)
Epoch: [225][192/196]	Time 0.297 (0.286)	Data 0.000 (0.004)	Loss 0.6512 (0.7337)	Acc@1 82.422 (74.534)	Acc@5 97.656 (98.219)
after train
test acc: 55.53
Epoche: [226/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [226][0/196]	Time 0.326 (0.326)	Data 0.555 (0.555)	Loss 0.7753 (0.7753)	Acc@1 75.391 (75.391)	Acc@5 96.875 (96.875)
Epoch: [226][64/196]	Time 0.231 (0.280)	Data 0.000 (0.009)	Loss 0.7327 (0.7533)	Acc@1 73.828 (73.732)	Acc@5 98.438 (98.161)
Epoch: [226][128/196]	Time 0.311 (0.279)	Data 0.000 (0.005)	Loss 0.7781 (0.7484)	Acc@1 71.875 (74.061)	Acc@5 98.047 (98.107)
Epoch: [226][192/196]	Time 0.274 (0.276)	Data 0.000 (0.004)	Loss 0.7035 (0.7423)	Acc@1 75.781 (74.180)	Acc@5 98.828 (98.164)
after train
test acc: 61.65
Epoche: [227/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [227][0/196]	Time 0.396 (0.396)	Data 0.394 (0.394)	Loss 0.6583 (0.6583)	Acc@1 76.953 (76.953)	Acc@5 98.828 (98.828)
Epoch: [227][64/196]	Time 0.268 (0.286)	Data 0.000 (0.007)	Loss 0.7488 (0.7172)	Acc@1 75.781 (75.210)	Acc@5 98.047 (98.287)
Epoch: [227][128/196]	Time 0.267 (0.290)	Data 0.000 (0.004)	Loss 0.7567 (0.7243)	Acc@1 75.781 (74.958)	Acc@5 98.047 (98.262)
Epoch: [227][192/196]	Time 0.223 (0.281)	Data 0.000 (0.003)	Loss 0.7423 (0.7344)	Acc@1 72.266 (74.617)	Acc@5 97.656 (98.221)
after train
test acc: 56.02
Epoche: [228/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [228][0/196]	Time 0.448 (0.448)	Data 0.433 (0.433)	Loss 0.6395 (0.6395)	Acc@1 78.516 (78.516)	Acc@5 99.219 (99.219)
Epoch: [228][64/196]	Time 0.338 (0.293)	Data 0.000 (0.008)	Loss 0.7110 (0.7302)	Acc@1 75.000 (74.814)	Acc@5 97.266 (98.209)
Epoch: [228][128/196]	Time 0.347 (0.291)	Data 0.000 (0.004)	Loss 0.8056 (0.7291)	Acc@1 75.391 (74.818)	Acc@5 99.219 (98.268)
Epoch: [228][192/196]	Time 0.222 (0.282)	Data 0.000 (0.003)	Loss 0.7986 (0.7346)	Acc@1 70.312 (74.518)	Acc@5 96.875 (98.182)
after train
test acc: 59.83
Epoche: [229/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [229][0/196]	Time 0.324 (0.324)	Data 0.531 (0.531)	Loss 0.6913 (0.6913)	Acc@1 75.000 (75.000)	Acc@5 98.438 (98.438)
Epoch: [229][64/196]	Time 0.243 (0.299)	Data 0.000 (0.009)	Loss 0.7760 (0.7223)	Acc@1 72.266 (74.940)	Acc@5 98.047 (98.377)
Epoch: [229][128/196]	Time 0.253 (0.294)	Data 0.000 (0.005)	Loss 0.6704 (0.7350)	Acc@1 76.172 (74.609)	Acc@5 98.438 (98.304)
Epoch: [229][192/196]	Time 0.313 (0.283)	Data 0.000 (0.004)	Loss 0.6322 (0.7348)	Acc@1 80.469 (74.502)	Acc@5 98.438 (98.219)
after train
test acc: 48.65
Epoche: [230/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [230][0/196]	Time 0.375 (0.375)	Data 0.516 (0.516)	Loss 0.6995 (0.6995)	Acc@1 75.781 (75.781)	Acc@5 98.047 (98.047)
Epoch: [230][64/196]	Time 0.294 (0.292)	Data 0.000 (0.009)	Loss 0.6219 (0.7293)	Acc@1 76.953 (74.706)	Acc@5 99.219 (98.125)
Epoch: [230][128/196]	Time 0.281 (0.292)	Data 0.000 (0.005)	Loss 0.7935 (0.7253)	Acc@1 72.656 (74.900)	Acc@5 98.438 (98.095)
Epoch: [230][192/196]	Time 0.267 (0.284)	Data 0.000 (0.004)	Loss 0.7596 (0.7235)	Acc@1 71.875 (74.960)	Acc@5 98.047 (98.185)
after train
test acc: 68.14
Epoche: [231/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [231][0/196]	Time 0.385 (0.385)	Data 0.502 (0.502)	Loss 0.7202 (0.7202)	Acc@1 76.562 (76.562)	Acc@5 98.828 (98.828)
Epoch: [231][64/196]	Time 0.298 (0.280)	Data 0.005 (0.008)	Loss 0.7441 (0.7460)	Acc@1 71.484 (74.243)	Acc@5 99.219 (98.035)
Epoch: [231][128/196]	Time 0.295 (0.284)	Data 0.000 (0.005)	Loss 0.7228 (0.7441)	Acc@1 75.781 (74.137)	Acc@5 97.656 (98.047)
Epoch: [231][192/196]	Time 0.362 (0.280)	Data 0.000 (0.003)	Loss 0.7708 (0.7421)	Acc@1 72.266 (74.225)	Acc@5 99.219 (98.075)
after train
test acc: 31.9
Epoche: [232/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [232][0/196]	Time 0.446 (0.446)	Data 0.425 (0.425)	Loss 0.8006 (0.8006)	Acc@1 73.438 (73.438)	Acc@5 97.656 (97.656)
Epoch: [232][64/196]	Time 0.271 (0.295)	Data 0.000 (0.008)	Loss 0.6795 (0.7250)	Acc@1 74.219 (74.369)	Acc@5 98.828 (98.233)
Epoch: [232][128/196]	Time 0.232 (0.291)	Data 0.000 (0.004)	Loss 0.8062 (0.7229)	Acc@1 71.094 (74.797)	Acc@5 98.438 (98.229)
Epoch: [232][192/196]	Time 0.288 (0.283)	Data 0.016 (0.003)	Loss 0.6692 (0.7250)	Acc@1 76.562 (74.895)	Acc@5 98.828 (98.162)
after train
test acc: 55.02
Epoche: [233/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [233][0/196]	Time 0.260 (0.260)	Data 0.557 (0.557)	Loss 0.6959 (0.6959)	Acc@1 74.219 (74.219)	Acc@5 98.438 (98.438)
Epoch: [233][64/196]	Time 0.198 (0.288)	Data 0.000 (0.009)	Loss 0.6464 (0.7230)	Acc@1 79.297 (75.180)	Acc@5 96.875 (98.137)
Epoch: [233][128/196]	Time 0.234 (0.295)	Data 0.000 (0.005)	Loss 0.7940 (0.7326)	Acc@1 74.219 (74.679)	Acc@5 97.656 (98.141)
Epoch: [233][192/196]	Time 0.336 (0.289)	Data 0.000 (0.004)	Loss 0.6203 (0.7354)	Acc@1 77.344 (74.543)	Acc@5 99.609 (98.185)
after train
test acc: 50.38
Epoche: [234/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [234][0/196]	Time 0.308 (0.308)	Data 0.544 (0.544)	Loss 0.7241 (0.7241)	Acc@1 74.219 (74.219)	Acc@5 98.047 (98.047)
Epoch: [234][64/196]	Time 0.339 (0.292)	Data 0.000 (0.009)	Loss 0.7635 (0.7188)	Acc@1 73.438 (74.675)	Acc@5 97.266 (98.419)
Epoch: [234][128/196]	Time 0.285 (0.287)	Data 0.002 (0.005)	Loss 0.8164 (0.7335)	Acc@1 72.656 (74.110)	Acc@5 97.656 (98.359)
Epoch: [234][192/196]	Time 0.245 (0.277)	Data 0.000 (0.004)	Loss 0.7404 (0.7351)	Acc@1 75.000 (74.265)	Acc@5 98.828 (98.340)
after train
test acc: 56.93
Epoche: [235/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [235][0/196]	Time 0.210 (0.210)	Data 0.501 (0.501)	Loss 0.7366 (0.7366)	Acc@1 73.438 (73.438)	Acc@5 98.438 (98.438)
Epoch: [235][64/196]	Time 0.324 (0.285)	Data 0.000 (0.009)	Loss 0.7561 (0.7229)	Acc@1 74.219 (74.928)	Acc@5 98.438 (98.209)
Epoch: [235][128/196]	Time 0.312 (0.296)	Data 0.000 (0.005)	Loss 0.7844 (0.7246)	Acc@1 71.875 (74.737)	Acc@5 98.047 (98.259)
Epoch: [235][192/196]	Time 0.215 (0.289)	Data 0.000 (0.004)	Loss 0.6020 (0.7339)	Acc@1 79.688 (74.476)	Acc@5 98.828 (98.114)
after train
test acc: 18.47
Epoche: [236/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [236][0/196]	Time 0.305 (0.305)	Data 0.539 (0.539)	Loss 0.7259 (0.7259)	Acc@1 73.438 (73.438)	Acc@5 96.875 (96.875)
Epoch: [236][64/196]	Time 0.279 (0.283)	Data 0.000 (0.009)	Loss 0.6375 (0.7184)	Acc@1 76.953 (75.096)	Acc@5 99.219 (98.185)
Epoch: [236][128/196]	Time 0.338 (0.281)	Data 0.000 (0.005)	Loss 0.7764 (0.7212)	Acc@1 73.047 (74.936)	Acc@5 99.219 (98.195)
Epoch: [236][192/196]	Time 0.185 (0.283)	Data 0.000 (0.004)	Loss 0.6355 (0.7240)	Acc@1 75.391 (74.974)	Acc@5 98.828 (98.193)
after train
test acc: 70.62
Epoche: [237/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [237][0/196]	Time 0.306 (0.306)	Data 0.734 (0.734)	Loss 0.7517 (0.7517)	Acc@1 72.656 (72.656)	Acc@5 98.047 (98.047)
Epoch: [237][64/196]	Time 0.289 (0.282)	Data 0.001 (0.012)	Loss 0.7064 (0.7386)	Acc@1 76.562 (74.363)	Acc@5 98.047 (98.125)
Epoch: [237][128/196]	Time 0.291 (0.287)	Data 0.000 (0.006)	Loss 0.6183 (0.7365)	Acc@1 75.391 (74.485)	Acc@5 99.219 (98.165)
Epoch: [237][192/196]	Time 0.369 (0.286)	Data 0.000 (0.005)	Loss 0.6841 (0.7354)	Acc@1 76.562 (74.431)	Acc@5 97.656 (98.197)
after train
test acc: 48.13
Epoche: [238/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [238][0/196]	Time 0.405 (0.405)	Data 0.498 (0.498)	Loss 0.6417 (0.6417)	Acc@1 77.344 (77.344)	Acc@5 98.438 (98.438)
Epoch: [238][64/196]	Time 0.274 (0.289)	Data 0.000 (0.008)	Loss 0.7261 (0.7338)	Acc@1 73.438 (74.483)	Acc@5 98.828 (98.269)
Epoch: [238][128/196]	Time 0.217 (0.286)	Data 0.000 (0.005)	Loss 0.7740 (0.7363)	Acc@1 75.000 (74.449)	Acc@5 98.047 (98.201)
Epoch: [238][192/196]	Time 0.148 (0.268)	Data 0.000 (0.004)	Loss 0.8011 (0.7360)	Acc@1 72.266 (74.472)	Acc@5 96.484 (98.187)
after train
test acc: 42.69
Epoche: [239/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [239][0/196]	Time 0.330 (0.330)	Data 0.463 (0.463)	Loss 0.7085 (0.7085)	Acc@1 74.219 (74.219)	Acc@5 98.438 (98.438)
Epoch: [239][64/196]	Time 0.165 (0.208)	Data 0.000 (0.008)	Loss 0.6794 (0.7330)	Acc@1 73.828 (74.567)	Acc@5 98.438 (98.179)
Epoch: [239][128/196]	Time 0.166 (0.197)	Data 0.000 (0.005)	Loss 0.7048 (0.7331)	Acc@1 75.781 (74.558)	Acc@5 97.656 (98.295)
Epoch: [239][192/196]	Time 0.140 (0.193)	Data 0.000 (0.003)	Loss 0.7476 (0.7350)	Acc@1 75.781 (74.409)	Acc@5 97.266 (98.308)
after train
test acc: 64.5
Epoche: [240/240]; Lr: 0.1
batch Size 256
befor train
Epoch: [240][0/196]	Time 0.282 (0.282)	Data 0.391 (0.391)	Loss 0.7244 (0.7244)	Acc@1 75.000 (75.000)	Acc@5 98.438 (98.438)
Epoch: [240][64/196]	Time 0.208 (0.188)	Data 0.000 (0.006)	Loss 0.7241 (0.7272)	Acc@1 77.344 (74.976)	Acc@5 97.266 (98.203)
Epoch: [240][128/196]	Time 0.157 (0.194)	Data 0.000 (0.004)	Loss 0.8633 (0.7260)	Acc@1 69.531 (74.955)	Acc@5 98.047 (98.120)
Epoch: [240][192/196]	Time 0.147 (0.192)	Data 0.000 (0.003)	Loss 0.7240 (0.7299)	Acc@1 72.266 (74.737)	Acc@5 98.438 (98.138)
after train
test acc: 44.7
[INFO] Storing checkpoint...
Max memory: 29.5321088
