no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 2;widthofFirstLayer: 16; Epochen: 180; reset: False; start epoche: 1; test: True pathtoModell: None; checkpoint: ./output/experimente4/widerRnd_WLr1; saveModell: False; LR: 0.1
random number: 272
Files already downloaded and verified
width: 8

Arch Num:  [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
conv0: Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0
bn1: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
Relu; i: 2
i : 2; block: 0
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 2; block: 0
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 2
i : 3; block: 1
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 3; block: 1
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 3
i : 4; block: 2
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 4; block: 2
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 4
i : 5; block: 3
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 5; block: 3
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 5
i : 6; block: 4
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 6; block: 4
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 6
i : 7; block: 0
Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 7; block: 0
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
i : 7; block: 0
Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 2 if 2
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 2
seq: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 7
seq1: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 8
i : 8; block: 1
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 8; block: 1
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 8
i : 9; block: 2
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 9; block: 2
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 9
i : 10; block: 3
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 10; block: 3
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 10
i : 11; block: 4
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 11; block: 4
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 11
i : 12; block: 0
Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 12; block: 0
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
i : 12; block: 0
Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 2 if 2
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 2
seq: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 12
seq1: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 13
i : 13; block: 1
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 13; block: 1
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 13
i : 14; block: 2
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 14; block: 2
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 14
i : 15; block: 3
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 15; block: 3
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 15
i : 16; block: 4
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 16; block: 4
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 16
avgpoll: AdaptiveAvgPool2d(output_size=(1, 1))
linear: Linear(in_features=32, out_features=10, bias=True)
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 4
Startepoche: 1
deeper epoch: 0
Epoche: [1/180]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [1][0/196]	Time 0.151 (0.151)	Data 0.207 (0.207)	Loss 2.5715 (2.5715)	Acc@1 9.375 (9.375)	Acc@5 43.750 (43.750)
Epoch: [1][64/196]	Time 0.071 (0.075)	Data 0.000 (0.003)	Loss 1.7900 (2.0236)	Acc@1 30.078 (23.624)	Acc@5 84.375 (75.805)
Epoch: [1][128/196]	Time 0.067 (0.074)	Data 0.000 (0.002)	Loss 1.6017 (1.8504)	Acc@1 41.406 (29.966)	Acc@5 90.234 (82.034)
Epoch: [1][192/196]	Time 0.073 (0.074)	Data 0.000 (0.001)	Loss 1.3718 (1.7317)	Acc@1 52.344 (34.733)	Acc@5 94.531 (85.255)
after train
n1: 1 for:
wAcc: 37.8
test acc: 37.8
Epoche: [2/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [2][0/196]	Time 0.117 (0.117)	Data 0.217 (0.217)	Loss 1.4351 (1.4351)	Acc@1 47.266 (47.266)	Acc@5 92.578 (92.578)
Epoch: [2][64/196]	Time 0.090 (0.075)	Data 0.000 (0.004)	Loss 1.3122 (1.3453)	Acc@1 50.781 (50.294)	Acc@5 93.359 (93.612)
Epoch: [2][128/196]	Time 0.066 (0.074)	Data 0.000 (0.002)	Loss 1.1212 (1.3074)	Acc@1 59.766 (51.983)	Acc@5 96.484 (94.104)
Epoch: [2][192/196]	Time 0.069 (0.074)	Data 0.000 (0.001)	Loss 1.0630 (1.2614)	Acc@1 60.938 (53.841)	Acc@5 94.531 (94.505)
after train
n1: 2 for:
wAcc: 37.8
test acc: 56.07
Epoche: [3/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [3][0/196]	Time 0.097 (0.097)	Data 0.236 (0.236)	Loss 1.0645 (1.0645)	Acc@1 60.938 (60.938)	Acc@5 96.875 (96.875)
Epoch: [3][64/196]	Time 0.066 (0.073)	Data 0.000 (0.004)	Loss 1.0130 (1.1041)	Acc@1 60.938 (59.826)	Acc@5 96.875 (96.142)
Epoch: [3][128/196]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 1.0477 (1.0803)	Acc@1 62.109 (60.695)	Acc@5 96.484 (96.245)
Epoch: [3][192/196]	Time 0.070 (0.073)	Data 0.000 (0.001)	Loss 0.8642 (1.0525)	Acc@1 67.969 (61.769)	Acc@5 97.656 (96.474)
after train
n1: 3 for:
wAcc: 46.935
test acc: 58.99
Epoche: [4/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [4][0/196]	Time 0.105 (0.105)	Data 0.228 (0.228)	Loss 1.0198 (1.0198)	Acc@1 64.062 (64.062)	Acc@5 97.656 (97.656)
Epoch: [4][64/196]	Time 0.069 (0.074)	Data 0.000 (0.004)	Loss 0.9694 (0.9392)	Acc@1 66.016 (66.316)	Acc@5 96.875 (97.133)
Epoch: [4][128/196]	Time 0.072 (0.074)	Data 0.000 (0.002)	Loss 0.7719 (0.9308)	Acc@1 71.484 (66.530)	Acc@5 99.609 (97.363)
Epoch: [4][192/196]	Time 0.075 (0.074)	Data 0.000 (0.001)	Loss 1.0068 (0.9241)	Acc@1 66.016 (66.989)	Acc@5 94.922 (97.369)
after train
n1: 4 for:
wAcc: 50.6608
test acc: 67.8
Epoche: [5/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [5][0/196]	Time 0.129 (0.129)	Data 0.235 (0.235)	Loss 0.9020 (0.9020)	Acc@1 68.750 (68.750)	Acc@5 97.266 (97.266)
Epoch: [5][64/196]	Time 0.205 (0.127)	Data 0.000 (0.004)	Loss 0.7118 (0.8484)	Acc@1 71.484 (69.940)	Acc@5 98.828 (97.770)
Epoch: [5][128/196]	Time 0.220 (0.164)	Data 0.000 (0.002)	Loss 0.8635 (0.8406)	Acc@1 71.094 (70.325)	Acc@5 98.047 (97.862)
Epoch: [5][192/196]	Time 0.196 (0.180)	Data 0.000 (0.001)	Loss 0.7981 (0.8334)	Acc@1 70.312 (70.527)	Acc@5 98.438 (97.844)
after train
n1: 5 for:
wAcc: 55.21555555555557
test acc: 62.37
Epoche: [6/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [6][0/196]	Time 0.241 (0.241)	Data 0.269 (0.269)	Loss 0.7626 (0.7626)	Acc@1 73.438 (73.438)	Acc@5 98.047 (98.047)
Epoch: [6][64/196]	Time 0.206 (0.214)	Data 0.000 (0.004)	Loss 0.7061 (0.7843)	Acc@1 77.734 (72.596)	Acc@5 97.656 (98.149)
Epoch: [6][128/196]	Time 0.220 (0.217)	Data 0.000 (0.002)	Loss 0.8753 (0.7768)	Acc@1 67.969 (72.732)	Acc@5 97.656 (98.129)
Epoch: [6][192/196]	Time 0.222 (0.217)	Data 0.000 (0.002)	Loss 0.7660 (0.7692)	Acc@1 71.875 (72.998)	Acc@5 98.828 (98.170)
after train
n1: 6 for:
wAcc: 55.9337026239067
test acc: 62.54
Epoche: [7/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [7][0/196]	Time 0.242 (0.242)	Data 0.202 (0.202)	Loss 0.6906 (0.6906)	Acc@1 75.000 (75.000)	Acc@5 98.438 (98.438)
Epoch: [7][64/196]	Time 0.549 (0.373)	Data 0.000 (0.003)	Loss 0.7885 (0.7289)	Acc@1 72.266 (74.555)	Acc@5 96.875 (98.221)
Epoch: [7][128/196]	Time 0.538 (0.391)	Data 0.000 (0.002)	Loss 0.7505 (0.7239)	Acc@1 75.000 (74.758)	Acc@5 97.656 (98.268)
Epoch: [7][192/196]	Time 0.535 (0.396)	Data 0.000 (0.001)	Loss 0.7221 (0.7229)	Acc@1 73.047 (74.785)	Acc@5 99.609 (98.304)
after train
n1: 7 for:
wAcc: 56.490693359374994
test acc: 68.22
Epoche: [8/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [8][0/196]	Time 0.321 (0.321)	Data 0.234 (0.234)	Loss 0.6591 (0.6591)	Acc@1 74.219 (74.219)	Acc@5 98.828 (98.828)
Epoch: [8][64/196]	Time 0.381 (0.406)	Data 0.000 (0.004)	Loss 0.7223 (0.7010)	Acc@1 74.609 (75.673)	Acc@5 97.656 (98.450)
Epoch: [8][128/196]	Time 0.234 (0.407)	Data 0.000 (0.002)	Loss 0.6820 (0.6891)	Acc@1 73.047 (75.996)	Acc@5 98.828 (98.374)
Epoch: [8][192/196]	Time 0.338 (0.407)	Data 0.000 (0.001)	Loss 0.6068 (0.6888)	Acc@1 78.125 (76.176)	Acc@5 99.219 (98.391)
after train
n1: 8 for:
wAcc: 58.15453843418178
test acc: 68.86
Epoche: [9/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [9][0/196]	Time 0.590 (0.590)	Data 0.224 (0.224)	Loss 0.6114 (0.6114)	Acc@1 80.078 (80.078)	Acc@5 98.828 (98.828)
Epoch: [9][64/196]	Time 0.551 (0.405)	Data 0.000 (0.004)	Loss 0.4803 (0.6836)	Acc@1 85.547 (76.358)	Acc@5 99.609 (98.431)
Epoch: [9][128/196]	Time 0.537 (0.406)	Data 0.000 (0.002)	Loss 0.6014 (0.6717)	Acc@1 79.688 (76.738)	Acc@5 98.828 (98.553)
Epoch: [9][192/196]	Time 0.512 (0.403)	Data 0.000 (0.001)	Loss 0.6273 (0.6676)	Acc@1 78.906 (76.820)	Acc@5 98.438 (98.512)
after train
n1: 9 for:
wAcc: 59.366070016000016
test acc: 70.15
Epoche: [10/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [10][0/196]	Time 0.580 (0.580)	Data 0.241 (0.241)	Loss 0.6119 (0.6119)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [10][64/196]	Time 0.525 (0.407)	Data 0.000 (0.004)	Loss 0.6259 (0.6305)	Acc@1 76.172 (78.029)	Acc@5 98.828 (98.606)
Epoch: [10][128/196]	Time 0.555 (0.408)	Data 0.000 (0.002)	Loss 0.6915 (0.6291)	Acc@1 77.734 (78.189)	Acc@5 97.656 (98.659)
Epoch: [10][192/196]	Time 0.538 (0.407)	Data 0.000 (0.001)	Loss 0.6785 (0.6318)	Acc@1 78.125 (78.048)	Acc@5 98.828 (98.701)
after train
n1: 10 for:
wAcc: 60.44138254071217
test acc: 65.65
Epoche: [11/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [11][0/196]	Time 0.602 (0.602)	Data 0.250 (0.250)	Loss 0.6035 (0.6035)	Acc@1 80.078 (80.078)	Acc@5 97.656 (97.656)
Epoch: [11][64/196]	Time 0.557 (0.409)	Data 0.000 (0.004)	Loss 0.7229 (0.6261)	Acc@1 75.391 (78.462)	Acc@5 98.047 (98.714)
Epoch: [11][128/196]	Time 0.545 (0.407)	Data 0.000 (0.002)	Loss 0.6510 (0.6242)	Acc@1 78.906 (78.561)	Acc@5 98.047 (98.722)
Epoch: [11][192/196]	Time 0.539 (0.407)	Data 0.000 (0.002)	Loss 0.5930 (0.6268)	Acc@1 78.906 (78.469)	Acc@5 99.219 (98.723)
after train
n1: 11 for:
wAcc: 60.466119205223094
test acc: 64.75
Epoche: [12/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [12][0/196]	Time 0.309 (0.309)	Data 0.233 (0.233)	Loss 0.5644 (0.5644)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [12][64/196]	Time 0.304 (0.406)	Data 0.000 (0.004)	Loss 0.6256 (0.6161)	Acc@1 77.344 (78.696)	Acc@5 98.828 (98.738)
Epoch: [12][128/196]	Time 0.542 (0.405)	Data 0.000 (0.002)	Loss 0.7867 (0.6162)	Acc@1 75.000 (78.658)	Acc@5 97.266 (98.707)
Epoch: [12][192/196]	Time 0.550 (0.406)	Data 0.000 (0.001)	Loss 0.7509 (0.6115)	Acc@1 76.172 (78.884)	Acc@5 97.656 (98.751)
after train
n1: 12 for:
wAcc: 60.3822522766937
test acc: 67.94
Epoche: [13/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [13][0/196]	Time 0.599 (0.599)	Data 0.259 (0.259)	Loss 0.6531 (0.6531)	Acc@1 76.953 (76.953)	Acc@5 97.656 (97.656)
Epoch: [13][64/196]	Time 0.557 (0.403)	Data 0.000 (0.004)	Loss 0.6365 (0.6072)	Acc@1 79.688 (78.930)	Acc@5 99.219 (98.792)
Epoch: [13][128/196]	Time 0.539 (0.406)	Data 0.000 (0.002)	Loss 0.5468 (0.6048)	Acc@1 82.812 (79.155)	Acc@5 97.266 (98.716)
Epoch: [13][192/196]	Time 0.466 (0.403)	Data 0.000 (0.002)	Loss 0.6830 (0.6010)	Acc@1 77.344 (79.356)	Acc@5 99.609 (98.739)
after train
n1: 13 for:
wAcc: 60.80274571355455
test acc: 72.29
Epoche: [14/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [14][0/196]	Time 0.612 (0.612)	Data 0.263 (0.263)	Loss 0.6664 (0.6664)	Acc@1 76.172 (76.172)	Acc@5 98.438 (98.438)
Epoch: [14][64/196]	Time 0.567 (0.409)	Data 0.000 (0.004)	Loss 0.6027 (0.5804)	Acc@1 77.734 (80.132)	Acc@5 99.219 (98.876)
Epoch: [14][128/196]	Time 0.534 (0.410)	Data 0.000 (0.002)	Loss 0.5325 (0.5839)	Acc@1 83.203 (79.993)	Acc@5 97.656 (98.874)
Epoch: [14][192/196]	Time 0.545 (0.406)	Data 0.000 (0.002)	Loss 0.6521 (0.5884)	Acc@1 76.172 (79.720)	Acc@5 98.828 (98.846)
after train
n1: 14 for:
wAcc: 61.7125195409656
test acc: 66.85
Epoche: [15/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [15][0/196]	Time 0.598 (0.598)	Data 0.303 (0.303)	Loss 0.5831 (0.5831)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [15][64/196]	Time 0.539 (0.412)	Data 0.000 (0.005)	Loss 0.7491 (0.5975)	Acc@1 75.391 (79.321)	Acc@5 98.047 (98.876)
Epoch: [15][128/196]	Time 0.551 (0.404)	Data 0.000 (0.003)	Loss 0.5093 (0.5849)	Acc@1 81.250 (79.739)	Acc@5 99.609 (98.837)
Epoch: [15][192/196]	Time 0.540 (0.404)	Data 0.000 (0.002)	Loss 0.5655 (0.5791)	Acc@1 80.469 (79.928)	Acc@5 98.438 (98.873)
after train
n1: 15 for:
wAcc: 61.7375242227248
test acc: 74.92
Epoche: [16/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [16][0/196]	Time 0.351 (0.351)	Data 0.241 (0.241)	Loss 0.5167 (0.5167)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [16][64/196]	Time 0.342 (0.405)	Data 0.000 (0.004)	Loss 0.5807 (0.5797)	Acc@1 81.250 (80.397)	Acc@5 99.219 (98.858)
Epoch: [16][128/196]	Time 0.565 (0.404)	Data 0.000 (0.002)	Loss 0.5840 (0.5741)	Acc@1 79.688 (80.184)	Acc@5 96.875 (98.913)
Epoch: [16][192/196]	Time 0.521 (0.400)	Data 0.000 (0.001)	Loss 0.5595 (0.5724)	Acc@1 78.906 (80.220)	Acc@5 99.219 (98.956)
after train
n1: 16 for:
wAcc: 62.7227358143585
test acc: 69.73
Epoche: [17/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [17][0/196]	Time 0.586 (0.586)	Data 0.258 (0.258)	Loss 0.6664 (0.6664)	Acc@1 77.734 (77.734)	Acc@5 96.875 (96.875)
Epoch: [17][64/196]	Time 0.538 (0.407)	Data 0.000 (0.004)	Loss 0.4761 (0.5497)	Acc@1 85.156 (81.214)	Acc@5 98.828 (98.798)
Epoch: [17][128/196]	Time 0.535 (0.407)	Data 0.000 (0.002)	Loss 0.6165 (0.5515)	Acc@1 77.344 (80.908)	Acc@5 100.000 (98.871)
Epoch: [17][192/196]	Time 0.529 (0.405)	Data 0.000 (0.002)	Loss 0.5480 (0.5585)	Acc@1 78.906 (80.645)	Acc@5 99.609 (98.917)
after train
n1: 17 for:
wAcc: 62.930956833067036
test acc: 66.64
Epoche: [18/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [18][0/196]	Time 0.581 (0.581)	Data 0.237 (0.237)	Loss 0.4963 (0.4963)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [18][64/196]	Time 0.522 (0.404)	Data 0.000 (0.004)	Loss 0.5468 (0.5532)	Acc@1 80.469 (81.124)	Acc@5 98.828 (98.906)
Epoch: [18][128/196]	Time 0.563 (0.404)	Data 0.000 (0.002)	Loss 0.5277 (0.5508)	Acc@1 82.422 (81.180)	Acc@5 99.219 (98.970)
Epoch: [18][192/196]	Time 0.519 (0.404)	Data 0.000 (0.001)	Loss 0.4957 (0.5535)	Acc@1 82.812 (81.052)	Acc@5 98.828 (98.950)
after train
n1: 18 for:
wAcc: 62.785207668331125
test acc: 75.29
Epoche: [19/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [19][0/196]	Time 0.583 (0.583)	Data 0.255 (0.255)	Loss 0.4206 (0.4206)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [19][64/196]	Time 0.557 (0.400)	Data 0.000 (0.004)	Loss 0.4807 (0.5597)	Acc@1 82.031 (80.667)	Acc@5 100.000 (98.978)
Epoch: [19][128/196]	Time 0.567 (0.405)	Data 0.000 (0.002)	Loss 0.5888 (0.5494)	Acc@1 78.125 (81.120)	Acc@5 99.219 (99.007)
Epoch: [19][192/196]	Time 0.544 (0.403)	Data 0.000 (0.002)	Loss 0.4909 (0.5472)	Acc@1 83.203 (81.228)	Acc@5 98.828 (98.974)
after train
n1: 19 for:
wAcc: 63.54448633785645
test acc: 74.94
Epoche: [20/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [20][0/196]	Time 0.552 (0.552)	Data 0.217 (0.217)	Loss 0.6138 (0.6138)	Acc@1 78.516 (78.516)	Acc@5 98.828 (98.828)
Epoch: [20][64/196]	Time 0.214 (0.407)	Data 0.000 (0.004)	Loss 0.5311 (0.5326)	Acc@1 81.250 (81.929)	Acc@5 98.828 (99.032)
Epoch: [20][128/196]	Time 0.082 (0.406)	Data 0.000 (0.002)	Loss 0.6267 (0.5379)	Acc@1 77.734 (81.765)	Acc@5 98.047 (99.031)
Epoch: [20][192/196]	Time 0.071 (0.406)	Data 0.000 (0.001)	Loss 0.5742 (0.5392)	Acc@1 80.078 (81.689)	Acc@5 98.828 (99.055)
after train
n1: 20 for:
wAcc: 64.13782411319151
test acc: 78.91
Epoche: [21/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [21][0/196]	Time 0.597 (0.597)	Data 0.294 (0.294)	Loss 0.6269 (0.6269)	Acc@1 79.688 (79.688)	Acc@5 98.828 (98.828)
Epoch: [21][64/196]	Time 0.562 (0.404)	Data 0.000 (0.005)	Loss 0.6335 (0.5321)	Acc@1 81.250 (81.773)	Acc@5 98.047 (99.002)
Epoch: [21][128/196]	Time 0.066 (0.242)	Data 0.000 (0.003)	Loss 0.5371 (0.5278)	Acc@1 82.031 (81.928)	Acc@5 99.219 (99.046)
Epoch: [21][192/196]	Time 0.072 (0.186)	Data 0.000 (0.002)	Loss 0.6585 (0.5302)	Acc@1 78.125 (81.792)	Acc@5 98.047 (99.043)
after train
n1: 21 for:
wAcc: 64.99609293915103
test acc: 70.62
Epoche: [22/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [22][0/196]	Time 0.112 (0.112)	Data 0.242 (0.242)	Loss 0.4708 (0.4708)	Acc@1 81.641 (81.641)	Acc@5 98.438 (98.438)
Epoch: [22][64/196]	Time 0.237 (0.090)	Data 0.000 (0.004)	Loss 0.5334 (0.5322)	Acc@1 80.078 (81.731)	Acc@5 98.828 (99.099)
Epoch: [22][128/196]	Time 0.122 (0.156)	Data 0.000 (0.002)	Loss 0.7116 (0.5379)	Acc@1 77.344 (81.465)	Acc@5 98.047 (99.098)
Epoch: [22][192/196]	Time 0.244 (0.180)	Data 0.000 (0.001)	Loss 0.5048 (0.5335)	Acc@1 80.859 (81.614)	Acc@5 99.219 (99.051)
after train
n1: 22 for:
wAcc: 64.99689769305948
test acc: 73.0
Epoche: [23/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [23][0/196]	Time 0.229 (0.229)	Data 0.283 (0.283)	Loss 0.5213 (0.5213)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [23][64/196]	Time 0.236 (0.225)	Data 0.000 (0.005)	Loss 0.4683 (0.5237)	Acc@1 83.984 (82.212)	Acc@5 99.219 (99.087)
Epoch: [23][128/196]	Time 0.242 (0.220)	Data 0.000 (0.002)	Loss 0.5843 (0.5134)	Acc@1 82.031 (82.510)	Acc@5 98.438 (99.140)
Epoch: [23][192/196]	Time 0.226 (0.218)	Data 0.000 (0.002)	Loss 0.5372 (0.5194)	Acc@1 82.422 (82.302)	Acc@5 99.219 (99.132)
after train
n1: 23 for:
wAcc: 65.20598347263604
test acc: 71.76
Epoche: [24/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [24][0/196]	Time 0.261 (0.261)	Data 0.252 (0.252)	Loss 0.6074 (0.6074)	Acc@1 81.641 (81.641)	Acc@5 99.219 (99.219)
Epoch: [24][64/196]	Time 0.479 (0.277)	Data 0.000 (0.004)	Loss 0.4899 (0.5208)	Acc@1 82.812 (81.977)	Acc@5 98.828 (99.111)
Epoch: [24][128/196]	Time 0.516 (0.308)	Data 0.000 (0.002)	Loss 0.4881 (0.5181)	Acc@1 84.375 (82.004)	Acc@5 98.047 (99.095)
Epoch: [24][192/196]	Time 0.142 (0.311)	Data 0.000 (0.002)	Loss 0.6737 (0.5195)	Acc@1 78.125 (82.187)	Acc@5 98.047 (99.067)
after train
n1: 24 for:
wAcc: 65.29123945247778
test acc: 71.6
Epoche: [25/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [25][0/196]	Time 0.456 (0.456)	Data 0.311 (0.311)	Loss 0.5484 (0.5484)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [25][64/196]	Time 0.510 (0.341)	Data 0.000 (0.005)	Loss 0.5178 (0.5274)	Acc@1 84.766 (81.905)	Acc@5 99.219 (99.044)
Epoch: [25][128/196]	Time 0.076 (0.334)	Data 0.000 (0.003)	Loss 0.4346 (0.5206)	Acc@1 85.156 (82.049)	Acc@5 99.609 (99.101)
Epoch: [25][192/196]	Time 0.136 (0.322)	Data 0.000 (0.002)	Loss 0.6373 (0.5148)	Acc@1 75.781 (82.203)	Acc@5 98.828 (99.128)
after train
n1: 25 for:
wAcc: 65.35853939927289
test acc: 77.07
Epoche: [26/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [26][0/196]	Time 0.399 (0.399)	Data 0.277 (0.277)	Loss 0.4319 (0.4319)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [26][64/196]	Time 0.082 (0.166)	Data 0.000 (0.005)	Loss 0.5631 (0.5017)	Acc@1 78.516 (82.482)	Acc@5 99.219 (99.207)
Epoch: [26][128/196]	Time 0.233 (0.138)	Data 0.000 (0.002)	Loss 0.4833 (0.5077)	Acc@1 81.641 (82.543)	Acc@5 98.828 (99.101)
Epoch: [26][192/196]	Time 0.229 (0.163)	Data 0.000 (0.002)	Loss 0.5416 (0.5076)	Acc@1 81.641 (82.505)	Acc@5 99.609 (99.124)
after train
n1: 26 for:
wAcc: 65.8274163007396
test acc: 73.63
Epoche: [27/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [27][0/196]	Time 0.289 (0.289)	Data 0.234 (0.234)	Loss 0.5133 (0.5133)	Acc@1 82.422 (82.422)	Acc@5 98.828 (98.828)
Epoch: [27][64/196]	Time 0.121 (0.216)	Data 0.000 (0.004)	Loss 0.6490 (0.5035)	Acc@1 78.516 (82.560)	Acc@5 98.047 (98.984)
Epoch: [27][128/196]	Time 0.233 (0.225)	Data 0.000 (0.002)	Loss 0.5412 (0.4983)	Acc@1 81.641 (82.828)	Acc@5 99.219 (99.101)
Epoch: [27][192/196]	Time 0.215 (0.220)	Data 0.000 (0.001)	Loss 0.5668 (0.5060)	Acc@1 78.125 (82.604)	Acc@5 99.219 (99.095)
after train
n1: 27 for:
wAcc: 65.99010985000483
test acc: 76.94
Epoche: [28/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [28][0/196]	Time 0.244 (0.244)	Data 0.273 (0.273)	Loss 0.5802 (0.5802)	Acc@1 79.688 (79.688)	Acc@5 98.047 (98.047)
Epoch: [28][64/196]	Time 0.230 (0.215)	Data 0.000 (0.004)	Loss 0.4946 (0.5051)	Acc@1 84.375 (82.837)	Acc@5 99.219 (99.111)
Epoch: [28][128/196]	Time 0.506 (0.246)	Data 0.000 (0.002)	Loss 0.4752 (0.5060)	Acc@1 85.938 (82.610)	Acc@5 98.828 (99.143)
Epoch: [28][192/196]	Time 0.503 (0.276)	Data 0.000 (0.002)	Loss 0.5520 (0.5021)	Acc@1 79.688 (82.701)	Acc@5 98.828 (99.190)
after train
n1: 28 for:
wAcc: 66.36447778517255
test acc: 67.44
Epoche: [29/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [29][0/196]	Time 0.559 (0.559)	Data 0.292 (0.292)	Loss 0.4484 (0.4484)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [29][64/196]	Time 0.522 (0.326)	Data 0.000 (0.005)	Loss 0.4939 (0.5086)	Acc@1 82.812 (82.512)	Acc@5 98.828 (99.177)
Epoch: [29][128/196]	Time 0.081 (0.208)	Data 0.000 (0.003)	Loss 0.4939 (0.5086)	Acc@1 83.984 (82.413)	Acc@5 99.219 (99.170)
Epoch: [29][192/196]	Time 0.067 (0.174)	Data 0.000 (0.002)	Loss 0.6561 (0.5074)	Acc@1 78.125 (82.543)	Acc@5 98.828 (99.140)
after train
n1: 29 for:
wAcc: 66.06135305737101
test acc: 76.85
Epoche: [30/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [30][0/196]	Time 0.103 (0.103)	Data 0.260 (0.260)	Loss 0.4977 (0.4977)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [30][64/196]	Time 0.083 (0.077)	Data 0.000 (0.004)	Loss 0.4363 (0.4952)	Acc@1 82.422 (82.446)	Acc@5 100.000 (99.177)
Epoch: [30][128/196]	Time 0.242 (0.093)	Data 0.000 (0.002)	Loss 0.3498 (0.4992)	Acc@1 89.453 (82.616)	Acc@5 99.609 (99.134)
Epoch: [30][192/196]	Time 0.231 (0.135)	Data 0.000 (0.002)	Loss 0.4965 (0.5011)	Acc@1 82.812 (82.578)	Acc@5 98.047 (99.146)
after train
n1: 30 for:
wAcc: 66.40867849519876
test acc: 69.61
Epoche: [31/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [31][0/196]	Time 0.292 (0.292)	Data 0.287 (0.287)	Loss 0.5139 (0.5139)	Acc@1 83.984 (83.984)	Acc@5 100.000 (100.000)
Epoch: [31][64/196]	Time 0.239 (0.218)	Data 0.000 (0.005)	Loss 0.4970 (0.4977)	Acc@1 83.984 (82.975)	Acc@5 99.609 (99.177)
Epoch: [31][128/196]	Time 0.074 (0.223)	Data 0.000 (0.002)	Loss 0.5124 (0.4936)	Acc@1 82.031 (83.121)	Acc@5 98.438 (99.210)
Epoch: [31][192/196]	Time 0.240 (0.223)	Data 0.000 (0.002)	Loss 0.5500 (0.4983)	Acc@1 80.859 (82.930)	Acc@5 98.047 (99.188)
after train
n1: 30 for:
wAcc: 69.2563550173044
test acc: 74.04
Epoche: [32/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [32][0/196]	Time 0.128 (0.128)	Data 0.228 (0.228)	Loss 0.4806 (0.4806)	Acc@1 84.766 (84.766)	Acc@5 98.828 (98.828)
Epoch: [32][64/196]	Time 0.274 (0.221)	Data 0.000 (0.004)	Loss 0.4481 (0.4855)	Acc@1 84.766 (83.095)	Acc@5 99.609 (99.123)
Epoch: [32][128/196]	Time 0.483 (0.262)	Data 0.000 (0.002)	Loss 0.6104 (0.4853)	Acc@1 78.516 (83.082)	Acc@5 99.609 (99.173)
Epoch: [32][192/196]	Time 0.158 (0.277)	Data 0.000 (0.001)	Loss 0.5719 (0.4902)	Acc@1 80.078 (83.031)	Acc@5 98.438 (99.148)
after train
n1: 30 for:
wAcc: 69.98709702140185
test acc: 76.34
Epoche: [33/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [33][0/196]	Time 0.498 (0.498)	Data 0.288 (0.288)	Loss 0.4591 (0.4591)	Acc@1 82.031 (82.031)	Acc@5 100.000 (100.000)
Epoch: [33][64/196]	Time 0.391 (0.331)	Data 0.000 (0.005)	Loss 0.5308 (0.4779)	Acc@1 83.594 (83.648)	Acc@5 100.000 (99.135)
Epoch: [33][128/196]	Time 0.484 (0.332)	Data 0.000 (0.002)	Loss 0.4136 (0.4834)	Acc@1 86.328 (83.527)	Acc@5 99.219 (99.176)
Epoch: [33][192/196]	Time 0.119 (0.321)	Data 0.000 (0.002)	Loss 0.4342 (0.4856)	Acc@1 85.547 (83.531)	Acc@5 99.609 (99.158)
after train
n1: 30 for:
wAcc: 71.67054904900863
test acc: 77.58
Epoche: [34/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [34][0/196]	Time 0.508 (0.508)	Data 0.238 (0.238)	Loss 0.4339 (0.4339)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [34][64/196]	Time 0.482 (0.334)	Data 0.000 (0.004)	Loss 0.4965 (0.4830)	Acc@1 81.250 (83.305)	Acc@5 99.219 (99.243)
Epoch: [34][128/196]	Time 0.471 (0.332)	Data 0.000 (0.002)	Loss 0.5315 (0.4908)	Acc@1 78.125 (82.909)	Acc@5 98.828 (99.249)
Epoch: [34][192/196]	Time 0.136 (0.319)	Data 0.000 (0.001)	Loss 0.4588 (0.4948)	Acc@1 82.422 (82.863)	Acc@5 99.609 (99.194)
after train
n1: 30 for:
wAcc: 71.26683469324351
test acc: 73.34
Epoche: [35/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [35][0/196]	Time 0.506 (0.506)	Data 0.390 (0.390)	Loss 0.3314 (0.3314)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [35][64/196]	Time 0.510 (0.325)	Data 0.000 (0.006)	Loss 0.4689 (0.4712)	Acc@1 82.031 (83.744)	Acc@5 100.000 (99.285)
Epoch: [35][128/196]	Time 0.397 (0.331)	Data 0.000 (0.003)	Loss 0.4468 (0.4776)	Acc@1 82.812 (83.533)	Acc@5 100.000 (99.264)
Epoch: [35][192/196]	Time 0.143 (0.319)	Data 0.000 (0.002)	Loss 0.4601 (0.4856)	Acc@1 83.594 (83.298)	Acc@5 98.438 (99.201)
after train
n1: 30 for:
wAcc: 71.42516275841074
test acc: 75.06
Epoche: [36/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [36][0/196]	Time 0.412 (0.412)	Data 0.337 (0.337)	Loss 0.5323 (0.5323)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [36][64/196]	Time 0.461 (0.322)	Data 0.000 (0.005)	Loss 0.5315 (0.4778)	Acc@1 82.031 (83.588)	Acc@5 98.828 (99.171)
Epoch: [36][128/196]	Time 0.428 (0.327)	Data 0.000 (0.003)	Loss 0.5394 (0.4757)	Acc@1 81.641 (83.591)	Acc@5 100.000 (99.198)
Epoch: [36][192/196]	Time 0.156 (0.318)	Data 0.000 (0.002)	Loss 0.4621 (0.4793)	Acc@1 83.984 (83.478)	Acc@5 100.000 (99.223)
after train
n1: 30 for:
wAcc: 72.48077803204016
test acc: 74.9
Epoche: [37/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [37][0/196]	Time 0.508 (0.508)	Data 0.320 (0.320)	Loss 0.4533 (0.4533)	Acc@1 85.938 (85.938)	Acc@5 98.828 (98.828)
Epoch: [37][64/196]	Time 0.508 (0.330)	Data 0.000 (0.005)	Loss 0.5063 (0.4670)	Acc@1 81.641 (83.900)	Acc@5 98.047 (99.297)
Epoch: [37][128/196]	Time 0.158 (0.329)	Data 0.000 (0.003)	Loss 0.4940 (0.4822)	Acc@1 84.766 (83.448)	Acc@5 99.609 (99.198)
Epoch: [37][192/196]	Time 0.155 (0.318)	Data 0.000 (0.002)	Loss 0.4926 (0.4822)	Acc@1 83.203 (83.462)	Acc@5 99.219 (99.196)
after train
n1: 30 for:
wAcc: 72.72937626531821
test acc: 78.89
Epoche: [38/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [38][0/196]	Time 0.462 (0.462)	Data 0.274 (0.274)	Loss 0.5075 (0.5075)	Acc@1 82.812 (82.812)	Acc@5 98.828 (98.828)
Epoch: [38][64/196]	Time 0.502 (0.328)	Data 0.000 (0.004)	Loss 0.5511 (0.4895)	Acc@1 81.641 (83.191)	Acc@5 98.438 (99.267)
Epoch: [38][128/196]	Time 0.491 (0.327)	Data 0.000 (0.002)	Loss 0.3976 (0.4819)	Acc@1 85.547 (83.479)	Acc@5 100.000 (99.255)
Epoch: [38][192/196]	Time 0.126 (0.318)	Data 0.000 (0.002)	Loss 0.4407 (0.4810)	Acc@1 85.547 (83.470)	Acc@5 100.000 (99.267)
after train
n1: 30 for:
wAcc: 73.31332026994752
test acc: 61.05
Epoche: [39/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [39][0/196]	Time 0.359 (0.359)	Data 0.261 (0.261)	Loss 0.5542 (0.5542)	Acc@1 78.516 (78.516)	Acc@5 99.219 (99.219)
Epoch: [39][64/196]	Time 0.482 (0.323)	Data 0.000 (0.004)	Loss 0.5238 (0.4839)	Acc@1 82.031 (83.239)	Acc@5 98.828 (99.189)
Epoch: [39][128/196]	Time 0.514 (0.324)	Data 0.000 (0.002)	Loss 0.4294 (0.4849)	Acc@1 84.766 (83.194)	Acc@5 99.609 (99.188)
Epoch: [39][192/196]	Time 0.071 (0.284)	Data 0.000 (0.002)	Loss 0.4332 (0.4779)	Acc@1 86.328 (83.547)	Acc@5 99.609 (99.243)
after train
n1: 30 for:
wAcc: 71.87161130945451
test acc: 73.8
Epoche: [40/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [40][0/196]	Time 0.114 (0.114)	Data 0.234 (0.234)	Loss 0.4955 (0.4955)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [40][64/196]	Time 0.073 (0.074)	Data 0.000 (0.004)	Loss 0.4531 (0.4814)	Acc@1 80.469 (83.510)	Acc@5 98.828 (99.153)
Epoch: [40][128/196]	Time 0.081 (0.074)	Data 0.000 (0.002)	Loss 0.4482 (0.4784)	Acc@1 84.375 (83.391)	Acc@5 99.609 (99.198)
Epoch: [40][192/196]	Time 0.068 (0.073)	Data 0.000 (0.001)	Loss 0.3941 (0.4720)	Acc@1 86.719 (83.600)	Acc@5 100.000 (99.239)
after train
n1: 30 for:
wAcc: 71.86591808151948
test acc: 78.54
Epoche: [41/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [41][0/196]	Time 0.108 (0.108)	Data 0.238 (0.238)	Loss 0.3573 (0.3573)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [41][64/196]	Time 0.075 (0.075)	Data 0.000 (0.004)	Loss 0.3964 (0.4614)	Acc@1 86.719 (84.014)	Acc@5 99.219 (99.135)
Epoch: [41][128/196]	Time 0.223 (0.087)	Data 0.000 (0.002)	Loss 0.5189 (0.4736)	Acc@1 80.859 (83.660)	Acc@5 99.609 (99.137)
Epoch: [41][192/196]	Time 0.236 (0.131)	Data 0.000 (0.001)	Loss 0.5438 (0.4720)	Acc@1 82.422 (83.796)	Acc@5 100.000 (99.180)
after train
n1: 30 for:
wAcc: 72.75765537934893
test acc: 71.33
Epoche: [42/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [42][0/196]	Time 0.233 (0.233)	Data 0.281 (0.281)	Loss 0.4337 (0.4337)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [42][64/196]	Time 0.154 (0.224)	Data 0.000 (0.005)	Loss 0.4996 (0.4687)	Acc@1 82.422 (83.876)	Acc@5 99.609 (99.123)
Epoch: [42][128/196]	Time 0.211 (0.227)	Data 0.000 (0.002)	Loss 0.5153 (0.4709)	Acc@1 85.547 (83.875)	Acc@5 98.438 (99.170)
Epoch: [42][192/196]	Time 0.217 (0.223)	Data 0.000 (0.002)	Loss 0.5422 (0.4734)	Acc@1 82.812 (83.679)	Acc@5 98.438 (99.199)
after train
n1: 30 for:
wAcc: 73.29439135468797
test acc: 75.15
Epoche: [43/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [43][0/196]	Time 0.241 (0.241)	Data 0.278 (0.278)	Loss 0.4710 (0.4710)	Acc@1 83.203 (83.203)	Acc@5 98.828 (98.828)
Epoch: [43][64/196]	Time 0.257 (0.214)	Data 0.000 (0.005)	Loss 0.4771 (0.4748)	Acc@1 83.203 (83.798)	Acc@5 99.219 (99.243)
Epoch: [43][128/196]	Time 0.230 (0.205)	Data 0.000 (0.002)	Loss 0.4464 (0.4702)	Acc@1 83.984 (83.872)	Acc@5 100.000 (99.297)
Epoch: [43][192/196]	Time 0.183 (0.195)	Data 0.000 (0.002)	Loss 0.5919 (0.4696)	Acc@1 82.031 (83.942)	Acc@5 98.047 (99.259)
after train
n1: 30 for:
wAcc: 72.6276931700808
test acc: 74.18
Epoche: [44/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [44][0/196]	Time 0.229 (0.229)	Data 0.231 (0.231)	Loss 0.4331 (0.4331)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [44][64/196]	Time 0.200 (0.178)	Data 0.000 (0.004)	Loss 0.5021 (0.4689)	Acc@1 80.859 (83.618)	Acc@5 99.219 (99.303)
Epoch: [44][128/196]	Time 0.168 (0.177)	Data 0.000 (0.002)	Loss 0.4547 (0.4633)	Acc@1 84.766 (83.963)	Acc@5 99.609 (99.273)
Epoch: [44][192/196]	Time 0.175 (0.178)	Data 0.000 (0.001)	Loss 0.4309 (0.4690)	Acc@1 85.156 (83.750)	Acc@5 100.000 (99.292)
after train
n1: 30 for:
wAcc: 73.89445376476792
test acc: 77.6
Epoche: [45/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [45][0/196]	Time 0.228 (0.228)	Data 0.276 (0.276)	Loss 0.3997 (0.3997)	Acc@1 88.672 (88.672)	Acc@5 98.438 (98.438)
Epoch: [45][64/196]	Time 0.166 (0.169)	Data 0.000 (0.004)	Loss 0.4464 (0.4580)	Acc@1 83.984 (84.201)	Acc@5 99.609 (99.423)
Epoch: [45][128/196]	Time 0.172 (0.168)	Data 0.000 (0.002)	Loss 0.4546 (0.4606)	Acc@1 86.328 (84.196)	Acc@5 98.828 (99.334)
Epoch: [45][192/196]	Time 0.201 (0.167)	Data 0.000 (0.002)	Loss 0.4012 (0.4655)	Acc@1 87.891 (84.031)	Acc@5 99.609 (99.281)
after train
n1: 30 for:
wAcc: 73.38324678172435
test acc: 75.94
Epoche: [46/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [46][0/196]	Time 0.186 (0.186)	Data 0.254 (0.254)	Loss 0.4480 (0.4480)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [46][64/196]	Time 0.160 (0.166)	Data 0.000 (0.004)	Loss 0.4353 (0.4605)	Acc@1 83.594 (83.960)	Acc@5 100.000 (99.339)
Epoch: [46][128/196]	Time 0.196 (0.166)	Data 0.000 (0.002)	Loss 0.4766 (0.4673)	Acc@1 83.203 (83.785)	Acc@5 99.609 (99.307)
Epoch: [46][192/196]	Time 0.087 (0.167)	Data 0.000 (0.002)	Loss 0.4821 (0.4683)	Acc@1 82.422 (83.861)	Acc@5 99.609 (99.273)
after train
n1: 30 for:
wAcc: 73.10150339037769
test acc: 72.57
Epoche: [47/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [47][0/196]	Time 0.204 (0.204)	Data 0.274 (0.274)	Loss 0.6822 (0.6822)	Acc@1 80.078 (80.078)	Acc@5 98.438 (98.438)
Epoch: [47][64/196]	Time 0.174 (0.164)	Data 0.000 (0.004)	Loss 0.4628 (0.4681)	Acc@1 87.500 (84.038)	Acc@5 98.047 (99.303)
Epoch: [47][128/196]	Time 0.119 (0.169)	Data 0.000 (0.002)	Loss 0.4507 (0.4649)	Acc@1 82.812 (84.136)	Acc@5 100.000 (99.304)
Epoch: [47][192/196]	Time 0.205 (0.172)	Data 0.000 (0.002)	Loss 0.6012 (0.4657)	Acc@1 79.297 (84.053)	Acc@5 99.609 (99.326)
after train
n1: 30 for:
wAcc: 74.31767031921423
test acc: 77.56
Epoche: [48/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [48][0/196]	Time 0.208 (0.208)	Data 0.234 (0.234)	Loss 0.4774 (0.4774)	Acc@1 82.422 (82.422)	Acc@5 100.000 (100.000)
Epoch: [48][64/196]	Time 0.160 (0.175)	Data 0.000 (0.004)	Loss 0.3424 (0.4523)	Acc@1 87.891 (84.423)	Acc@5 99.609 (99.291)
Epoch: [48][128/196]	Time 0.192 (0.173)	Data 0.000 (0.002)	Loss 0.5345 (0.4620)	Acc@1 81.250 (84.084)	Acc@5 99.219 (99.237)
Epoch: [48][192/196]	Time 0.201 (0.172)	Data 0.000 (0.001)	Loss 0.4723 (0.4626)	Acc@1 81.250 (84.118)	Acc@5 99.609 (99.239)
after train
n1: 30 for:
wAcc: 74.47625633422993
test acc: 81.03
Epoche: [49/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [49][0/196]	Time 0.141 (0.141)	Data 0.326 (0.326)	Loss 0.4534 (0.4534)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [49][64/196]	Time 0.184 (0.174)	Data 0.000 (0.005)	Loss 0.4978 (0.4603)	Acc@1 78.125 (83.774)	Acc@5 100.000 (99.261)
Epoch: [49][128/196]	Time 0.204 (0.170)	Data 0.000 (0.003)	Loss 0.4482 (0.4575)	Acc@1 84.375 (84.142)	Acc@5 99.219 (99.273)
Epoch: [49][192/196]	Time 0.145 (0.169)	Data 0.000 (0.002)	Loss 0.4880 (0.4632)	Acc@1 82.031 (83.918)	Acc@5 98.828 (99.243)
after train
n1: 30 for:
wAcc: 75.47298788846949
test acc: 74.27
Epoche: [50/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [50][0/196]	Time 0.198 (0.198)	Data 0.247 (0.247)	Loss 0.3845 (0.3845)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [50][64/196]	Time 0.170 (0.166)	Data 0.000 (0.004)	Loss 0.5816 (0.4614)	Acc@1 80.469 (83.930)	Acc@5 99.219 (99.273)
Epoch: [50][128/196]	Time 0.188 (0.166)	Data 0.000 (0.002)	Loss 0.4974 (0.4640)	Acc@1 82.812 (83.978)	Acc@5 98.438 (99.249)
Epoch: [50][192/196]	Time 0.196 (0.167)	Data 0.000 (0.002)	Loss 0.4493 (0.4608)	Acc@1 82.812 (84.169)	Acc@5 100.000 (99.279)
after train
n1: 30 for:
wAcc: 74.19696045708895
test acc: 77.07
Epoche: [51/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [51][0/196]	Time 0.213 (0.213)	Data 0.235 (0.235)	Loss 0.4146 (0.4146)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [51][64/196]	Time 0.147 (0.168)	Data 0.000 (0.004)	Loss 0.5391 (0.4526)	Acc@1 79.688 (84.315)	Acc@5 99.219 (99.357)
Epoch: [51][128/196]	Time 0.137 (0.166)	Data 0.000 (0.002)	Loss 0.5379 (0.4564)	Acc@1 83.203 (84.124)	Acc@5 99.609 (99.325)
Epoch: [51][192/196]	Time 0.143 (0.164)	Data 0.000 (0.001)	Loss 0.4457 (0.4598)	Acc@1 85.547 (83.976)	Acc@5 99.219 (99.336)
after train
n1: 30 for:
wAcc: 74.72637435319233
test acc: 74.08
Epoche: [52/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [52][0/196]	Time 0.215 (0.215)	Data 0.270 (0.270)	Loss 0.3333 (0.3333)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [52][64/196]	Time 0.173 (0.165)	Data 0.000 (0.004)	Loss 0.5272 (0.4505)	Acc@1 82.422 (84.369)	Acc@5 100.000 (99.417)
Epoch: [52][128/196]	Time 0.166 (0.167)	Data 0.000 (0.002)	Loss 0.4287 (0.4536)	Acc@1 84.375 (84.472)	Acc@5 99.219 (99.331)
Epoch: [52][192/196]	Time 0.199 (0.167)	Data 0.000 (0.002)	Loss 0.5017 (0.4598)	Acc@1 84.766 (84.223)	Acc@5 99.219 (99.300)
after train
n1: 30 for:
wAcc: 74.5054164510374
test acc: 77.38
Epoche: [53/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [53][0/196]	Time 0.165 (0.165)	Data 0.288 (0.288)	Loss 0.4609 (0.4609)	Acc@1 84.375 (84.375)	Acc@5 98.828 (98.828)
Epoch: [53][64/196]	Time 0.157 (0.159)	Data 0.000 (0.005)	Loss 0.4519 (0.4467)	Acc@1 84.375 (84.597)	Acc@5 100.000 (99.279)
Epoch: [53][128/196]	Time 0.114 (0.165)	Data 0.000 (0.002)	Loss 0.4249 (0.4620)	Acc@1 85.938 (84.112)	Acc@5 99.609 (99.279)
Epoch: [53][192/196]	Time 0.135 (0.168)	Data 0.000 (0.002)	Loss 0.4816 (0.4587)	Acc@1 83.203 (84.146)	Acc@5 99.219 (99.302)
after train
n1: 30 for:
wAcc: 74.66774360503746
test acc: 74.95
Epoche: [54/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [54][0/196]	Time 0.217 (0.217)	Data 0.242 (0.242)	Loss 0.4723 (0.4723)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [54][64/196]	Time 0.084 (0.168)	Data 0.000 (0.004)	Loss 0.4860 (0.4557)	Acc@1 81.641 (84.327)	Acc@5 99.609 (99.393)
Epoch: [54][128/196]	Time 0.109 (0.171)	Data 0.000 (0.002)	Loss 0.4301 (0.4623)	Acc@1 86.328 (83.930)	Acc@5 98.828 (99.343)
Epoch: [54][192/196]	Time 0.168 (0.171)	Data 0.000 (0.001)	Loss 0.4803 (0.4655)	Acc@1 84.375 (83.863)	Acc@5 98.438 (99.306)
after train
n1: 30 for:
wAcc: 75.47670541315378
test acc: 71.93
Epoche: [55/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [55][0/196]	Time 0.220 (0.220)	Data 0.261 (0.261)	Loss 0.4887 (0.4887)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [55][64/196]	Time 0.133 (0.171)	Data 0.000 (0.004)	Loss 0.4598 (0.4535)	Acc@1 82.422 (84.375)	Acc@5 99.609 (99.303)
Epoch: [55][128/196]	Time 0.176 (0.172)	Data 0.000 (0.002)	Loss 0.4261 (0.4518)	Acc@1 85.938 (84.496)	Acc@5 98.438 (99.304)
Epoch: [55][192/196]	Time 0.212 (0.169)	Data 0.000 (0.002)	Loss 0.5455 (0.4565)	Acc@1 80.859 (84.347)	Acc@5 98.828 (99.265)
after train
n1: 30 for:
wAcc: 74.75059395216371
test acc: 80.2
Epoche: [56/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [56][0/196]	Time 0.152 (0.152)	Data 0.237 (0.237)	Loss 0.4571 (0.4571)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [56][64/196]	Time 0.185 (0.163)	Data 0.000 (0.004)	Loss 0.5068 (0.4559)	Acc@1 80.469 (84.477)	Acc@5 98.047 (99.357)
Epoch: [56][128/196]	Time 0.182 (0.163)	Data 0.000 (0.002)	Loss 0.4219 (0.4503)	Acc@1 85.938 (84.699)	Acc@5 98.828 (99.334)
Epoch: [56][192/196]	Time 0.201 (0.164)	Data 0.000 (0.001)	Loss 0.6029 (0.4567)	Acc@1 80.859 (84.436)	Acc@5 99.609 (99.316)
after train
n1: 30 for:
wAcc: 75.58066729036912
test acc: 78.49
Epoche: [57/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [57][0/196]	Time 0.215 (0.215)	Data 0.235 (0.235)	Loss 0.3350 (0.3350)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [57][64/196]	Time 0.171 (0.166)	Data 0.000 (0.004)	Loss 0.4661 (0.4522)	Acc@1 84.766 (84.441)	Acc@5 99.219 (99.231)
Epoch: [57][128/196]	Time 0.203 (0.166)	Data 0.000 (0.002)	Loss 0.3645 (0.4590)	Acc@1 87.500 (84.218)	Acc@5 99.609 (99.225)
Epoch: [57][192/196]	Time 0.211 (0.166)	Data 0.000 (0.001)	Loss 0.5392 (0.4523)	Acc@1 80.469 (84.500)	Acc@5 98.828 (99.263)
after train
n1: 30 for:
wAcc: 74.39503138105373
test acc: 77.62
Epoche: [58/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [58][0/196]	Time 0.180 (0.180)	Data 0.290 (0.290)	Loss 0.4742 (0.4742)	Acc@1 82.812 (82.812)	Acc@5 99.609 (99.609)
Epoch: [58][64/196]	Time 0.172 (0.164)	Data 0.000 (0.005)	Loss 0.3774 (0.4388)	Acc@1 88.281 (84.934)	Acc@5 99.219 (99.411)
Epoch: [58][128/196]	Time 0.195 (0.164)	Data 0.000 (0.002)	Loss 0.3851 (0.4472)	Acc@1 87.891 (84.717)	Acc@5 98.828 (99.325)
Epoch: [58][192/196]	Time 0.198 (0.163)	Data 0.000 (0.002)	Loss 0.4998 (0.4508)	Acc@1 82.422 (84.490)	Acc@5 98.828 (99.306)
after train
n1: 30 for:
wAcc: 75.96341812625454
test acc: 79.4
Epoche: [59/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [59][0/196]	Time 0.189 (0.189)	Data 0.248 (0.248)	Loss 0.4473 (0.4473)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [59][64/196]	Time 0.164 (0.165)	Data 0.000 (0.004)	Loss 0.5419 (0.4563)	Acc@1 80.078 (84.237)	Acc@5 98.828 (99.261)
Epoch: [59][128/196]	Time 0.091 (0.167)	Data 0.000 (0.002)	Loss 0.4709 (0.4545)	Acc@1 81.641 (84.445)	Acc@5 99.219 (99.240)
Epoch: [59][192/196]	Time 0.142 (0.170)	Data 0.000 (0.002)	Loss 0.4372 (0.4555)	Acc@1 84.375 (84.355)	Acc@5 98.438 (99.267)
after train
n1: 30 for:
wAcc: 75.1385074114123
test acc: 77.3
Epoche: [60/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [60][0/196]	Time 0.192 (0.192)	Data 0.258 (0.258)	Loss 0.4019 (0.4019)	Acc@1 87.891 (87.891)	Acc@5 99.219 (99.219)
Epoch: [60][64/196]	Time 0.083 (0.167)	Data 0.000 (0.004)	Loss 0.4988 (0.4442)	Acc@1 85.547 (84.778)	Acc@5 98.438 (99.273)
Epoch: [60][128/196]	Time 0.086 (0.170)	Data 0.000 (0.002)	Loss 0.4304 (0.4533)	Acc@1 87.109 (84.390)	Acc@5 99.219 (99.310)
Epoch: [60][192/196]	Time 0.176 (0.169)	Data 0.000 (0.002)	Loss 0.4157 (0.4504)	Acc@1 85.547 (84.448)	Acc@5 99.219 (99.328)
after train
n1: 30 for:
wAcc: 75.91836624474601
test acc: 72.68
Epoche: [61/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [61][0/196]	Time 0.190 (0.190)	Data 0.354 (0.354)	Loss 0.4756 (0.4756)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [61][64/196]	Time 0.095 (0.176)	Data 0.000 (0.006)	Loss 0.3986 (0.4338)	Acc@1 86.328 (84.958)	Acc@5 100.000 (99.411)
Epoch: [61][128/196]	Time 0.157 (0.171)	Data 0.000 (0.003)	Loss 0.2986 (0.4471)	Acc@1 90.234 (84.532)	Acc@5 100.000 (99.319)
Epoch: [61][192/196]	Time 0.183 (0.168)	Data 0.000 (0.002)	Loss 0.4566 (0.4499)	Acc@1 85.156 (84.480)	Acc@5 99.609 (99.304)
after train
n1: 30 for:
wAcc: 76.04193097190499
test acc: 75.26
Epoche: [62/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [62][0/196]	Time 0.269 (0.269)	Data 0.245 (0.245)	Loss 0.5059 (0.5059)	Acc@1 83.984 (83.984)	Acc@5 98.828 (98.828)
Epoch: [62][64/196]	Time 0.204 (0.169)	Data 0.000 (0.004)	Loss 0.3685 (0.4511)	Acc@1 87.109 (84.603)	Acc@5 100.000 (99.321)
Epoch: [62][128/196]	Time 0.184 (0.167)	Data 0.000 (0.002)	Loss 0.4968 (0.4488)	Acc@1 84.375 (84.714)	Acc@5 98.828 (99.273)
Epoch: [62][192/196]	Time 0.189 (0.168)	Data 0.000 (0.002)	Loss 0.5091 (0.4529)	Acc@1 82.812 (84.535)	Acc@5 99.609 (99.318)
after train
n1: 30 for:
wAcc: 76.17074014340847
test acc: 81.26
Epoche: [63/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [63][0/196]	Time 0.199 (0.199)	Data 0.254 (0.254)	Loss 0.4848 (0.4848)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [63][64/196]	Time 0.202 (0.165)	Data 0.000 (0.004)	Loss 0.4879 (0.4525)	Acc@1 84.375 (84.453)	Acc@5 99.219 (99.321)
Epoch: [63][128/196]	Time 0.166 (0.163)	Data 0.000 (0.002)	Loss 0.4652 (0.4548)	Acc@1 82.812 (84.345)	Acc@5 99.609 (99.364)
Epoch: [63][192/196]	Time 0.171 (0.165)	Data 0.000 (0.002)	Loss 0.4754 (0.4511)	Acc@1 83.203 (84.529)	Acc@5 99.609 (99.330)
after train
n1: 30 for:
wAcc: 75.88613848628505
test acc: 75.66
Epoche: [64/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [64][0/196]	Time 0.209 (0.209)	Data 0.249 (0.249)	Loss 0.4452 (0.4452)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [64][64/196]	Time 0.171 (0.168)	Data 0.000 (0.004)	Loss 0.4236 (0.4393)	Acc@1 84.766 (84.802)	Acc@5 99.219 (99.375)
Epoch: [64][128/196]	Time 0.214 (0.167)	Data 0.000 (0.002)	Loss 0.4664 (0.4515)	Acc@1 84.375 (84.293)	Acc@5 98.828 (99.364)
Epoch: [64][192/196]	Time 0.194 (0.167)	Data 0.000 (0.002)	Loss 0.4032 (0.4479)	Acc@1 87.109 (84.466)	Acc@5 99.609 (99.350)
after train
n1: 30 for:
wAcc: 76.12019478498256
test acc: 69.65
Epoche: [65/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [65][0/196]	Time 0.216 (0.216)	Data 0.317 (0.317)	Loss 0.3711 (0.3711)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [65][64/196]	Time 0.186 (0.167)	Data 0.000 (0.005)	Loss 0.4054 (0.4474)	Acc@1 85.938 (84.651)	Acc@5 99.219 (99.309)
Epoch: [65][128/196]	Time 0.196 (0.166)	Data 0.000 (0.003)	Loss 0.5426 (0.4451)	Acc@1 80.469 (84.648)	Acc@5 99.219 (99.328)
Epoch: [65][192/196]	Time 0.145 (0.169)	Data 0.000 (0.002)	Loss 0.4707 (0.4495)	Acc@1 85.156 (84.600)	Acc@5 99.219 (99.304)
after train
n1: 30 for:
wAcc: 75.67963301421192
test acc: 78.83
Epoche: [66/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [66][0/196]	Time 0.203 (0.203)	Data 0.237 (0.237)	Loss 0.4958 (0.4958)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [66][64/196]	Time 0.180 (0.168)	Data 0.000 (0.004)	Loss 0.4114 (0.4324)	Acc@1 87.891 (85.168)	Acc@5 98.828 (99.399)
Epoch: [66][128/196]	Time 0.146 (0.171)	Data 0.000 (0.002)	Loss 0.4295 (0.4397)	Acc@1 85.156 (84.875)	Acc@5 99.219 (99.349)
Epoch: [66][192/196]	Time 0.082 (0.173)	Data 0.000 (0.001)	Loss 0.4533 (0.4466)	Acc@1 82.812 (84.622)	Acc@5 100.000 (99.387)
after train
n1: 30 for:
wAcc: 76.45968311056524
test acc: 78.53
Epoche: [67/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [67][0/196]	Time 0.198 (0.198)	Data 0.277 (0.277)	Loss 0.5375 (0.5375)	Acc@1 82.812 (82.812)	Acc@5 98.828 (98.828)
Epoch: [67][64/196]	Time 0.073 (0.174)	Data 0.000 (0.004)	Loss 0.5594 (0.4486)	Acc@1 81.250 (84.754)	Acc@5 99.219 (99.243)
Epoch: [67][128/196]	Time 0.147 (0.175)	Data 0.000 (0.002)	Loss 0.4813 (0.4486)	Acc@1 85.547 (84.575)	Acc@5 99.219 (99.285)
Epoch: [67][192/196]	Time 0.187 (0.174)	Data 0.000 (0.002)	Loss 0.3658 (0.4508)	Acc@1 87.500 (84.543)	Acc@5 99.609 (99.296)
after train
n1: 30 for:
wAcc: 74.01427376092796
test acc: 76.99
Epoche: [68/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [68][0/196]	Time 0.182 (0.182)	Data 0.294 (0.294)	Loss 0.4892 (0.4892)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [68][64/196]	Time 0.162 (0.174)	Data 0.000 (0.005)	Loss 0.4245 (0.4312)	Acc@1 83.984 (84.748)	Acc@5 99.609 (99.399)
Epoch: [68][128/196]	Time 0.190 (0.173)	Data 0.000 (0.003)	Loss 0.4322 (0.4372)	Acc@1 85.938 (84.844)	Acc@5 98.828 (99.358)
Epoch: [68][192/196]	Time 0.195 (0.171)	Data 0.000 (0.002)	Loss 0.5057 (0.4431)	Acc@1 84.766 (84.598)	Acc@5 98.828 (99.371)
after train
n1: 30 for:
wAcc: 76.04941595377987
test acc: 76.17
Epoche: [69/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [69][0/196]	Time 0.166 (0.166)	Data 0.334 (0.334)	Loss 0.3108 (0.3108)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [69][64/196]	Time 0.169 (0.170)	Data 0.000 (0.005)	Loss 0.3191 (0.4283)	Acc@1 91.016 (85.006)	Acc@5 100.000 (99.441)
Epoch: [69][128/196]	Time 0.180 (0.168)	Data 0.000 (0.003)	Loss 0.4589 (0.4441)	Acc@1 81.641 (84.563)	Acc@5 98.438 (99.322)
Epoch: [69][192/196]	Time 0.158 (0.166)	Data 0.000 (0.002)	Loss 0.4360 (0.4404)	Acc@1 85.156 (84.656)	Acc@5 98.438 (99.330)
after train
n1: 30 for:
wAcc: 76.74241735099648
test acc: 68.01
Epoche: [70/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [70][0/196]	Time 0.212 (0.212)	Data 0.291 (0.291)	Loss 0.4840 (0.4840)	Acc@1 83.984 (83.984)	Acc@5 98.438 (98.438)
Epoch: [70][64/196]	Time 0.190 (0.166)	Data 0.000 (0.005)	Loss 0.3525 (0.4566)	Acc@1 89.453 (84.273)	Acc@5 99.609 (99.381)
Epoch: [70][128/196]	Time 0.198 (0.163)	Data 0.000 (0.002)	Loss 0.4612 (0.4531)	Acc@1 84.375 (84.381)	Acc@5 100.000 (99.361)
Epoch: [70][192/196]	Time 0.192 (0.163)	Data 0.000 (0.002)	Loss 0.5123 (0.4509)	Acc@1 84.766 (84.466)	Acc@5 99.219 (99.338)
after train
n1: 30 for:
wAcc: 75.13674675869458
test acc: 54.28
Epoche: [71/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [71][0/196]	Time 0.197 (0.197)	Data 0.271 (0.271)	Loss 0.4272 (0.4272)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [71][64/196]	Time 0.197 (0.169)	Data 0.000 (0.004)	Loss 0.4307 (0.4285)	Acc@1 83.984 (85.198)	Acc@5 98.828 (99.387)
Epoch: [71][128/196]	Time 0.196 (0.168)	Data 0.000 (0.002)	Loss 0.4429 (0.4440)	Acc@1 87.109 (84.611)	Acc@5 99.609 (99.313)
Epoch: [71][192/196]	Time 0.195 (0.166)	Data 0.000 (0.002)	Loss 0.3981 (0.4480)	Acc@1 83.984 (84.594)	Acc@5 100.000 (99.322)
after train
n1: 30 for:
wAcc: 74.3433753422855
test acc: 76.43
Epoche: [72/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [72][0/196]	Time 0.193 (0.193)	Data 0.205 (0.205)	Loss 0.3814 (0.3814)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [72][64/196]	Time 0.176 (0.164)	Data 0.000 (0.003)	Loss 0.3970 (0.4219)	Acc@1 84.766 (85.493)	Acc@5 99.609 (99.351)
Epoch: [72][128/196]	Time 0.196 (0.164)	Data 0.000 (0.002)	Loss 0.3840 (0.4304)	Acc@1 89.062 (85.193)	Acc@5 99.219 (99.337)
Epoch: [72][192/196]	Time 0.079 (0.164)	Data 0.000 (0.001)	Loss 0.4494 (0.4335)	Acc@1 85.156 (85.071)	Acc@5 99.219 (99.304)
after train
n1: 30 for:
wAcc: 74.33777157741888
test acc: 66.75
Epoche: [73/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [73][0/196]	Time 0.200 (0.200)	Data 0.273 (0.273)	Loss 0.4882 (0.4882)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [73][64/196]	Time 0.146 (0.163)	Data 0.000 (0.004)	Loss 0.4409 (0.4466)	Acc@1 84.766 (84.531)	Acc@5 99.609 (99.393)
Epoch: [73][128/196]	Time 0.138 (0.167)	Data 0.000 (0.002)	Loss 0.4224 (0.4460)	Acc@1 85.938 (84.623)	Acc@5 99.219 (99.328)
Epoch: [73][192/196]	Time 0.143 (0.169)	Data 0.000 (0.002)	Loss 0.4687 (0.4486)	Acc@1 83.203 (84.583)	Acc@5 99.219 (99.306)
after train
n1: 30 for:
wAcc: 74.3426384530336
test acc: 73.7
Epoche: [74/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [74][0/196]	Time 0.159 (0.159)	Data 0.215 (0.215)	Loss 0.3241 (0.3241)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [74][64/196]	Time 0.121 (0.170)	Data 0.000 (0.004)	Loss 0.5135 (0.4504)	Acc@1 83.203 (84.573)	Acc@5 98.828 (99.315)
Epoch: [74][128/196]	Time 0.193 (0.170)	Data 0.000 (0.002)	Loss 0.4555 (0.4392)	Acc@1 84.375 (84.911)	Acc@5 99.609 (99.316)
Epoch: [74][192/196]	Time 0.190 (0.167)	Data 0.000 (0.001)	Loss 0.4228 (0.4399)	Acc@1 83.203 (84.794)	Acc@5 99.609 (99.348)
after train
n1: 30 for:
wAcc: 74.06120572265336
test acc: 71.55
Epoche: [75/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [75][0/196]	Time 0.151 (0.151)	Data 0.237 (0.237)	Loss 0.3702 (0.3702)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [75][64/196]	Time 0.144 (0.162)	Data 0.000 (0.004)	Loss 0.4723 (0.4223)	Acc@1 81.250 (85.403)	Acc@5 99.609 (99.363)
Epoch: [75][128/196]	Time 0.190 (0.163)	Data 0.000 (0.002)	Loss 0.4916 (0.4324)	Acc@1 83.203 (85.135)	Acc@5 99.219 (99.334)
Epoch: [75][192/196]	Time 0.200 (0.165)	Data 0.000 (0.001)	Loss 0.4395 (0.4400)	Acc@1 85.547 (84.996)	Acc@5 99.609 (99.342)
after train
n1: 30 for:
wAcc: 73.41202000231547
test acc: 77.27
Epoche: [76/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [76][0/196]	Time 0.186 (0.186)	Data 0.248 (0.248)	Loss 0.4132 (0.4132)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [76][64/196]	Time 0.168 (0.165)	Data 0.000 (0.004)	Loss 0.4971 (0.4399)	Acc@1 84.766 (84.856)	Acc@5 99.219 (99.387)
Epoch: [76][128/196]	Time 0.196 (0.165)	Data 0.000 (0.002)	Loss 0.4605 (0.4405)	Acc@1 84.766 (84.811)	Acc@5 99.219 (99.376)
Epoch: [76][192/196]	Time 0.191 (0.164)	Data 0.000 (0.002)	Loss 0.4158 (0.4436)	Acc@1 85.547 (84.610)	Acc@5 99.609 (99.405)
after train
n1: 30 for:
wAcc: 74.38228410829213
test acc: 78.97
Epoche: [77/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [77][0/196]	Time 0.164 (0.164)	Data 0.278 (0.278)	Loss 0.4495 (0.4495)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [77][64/196]	Time 0.197 (0.164)	Data 0.000 (0.005)	Loss 0.4722 (0.4427)	Acc@1 83.984 (84.784)	Acc@5 99.219 (99.471)
Epoch: [77][128/196]	Time 0.166 (0.164)	Data 0.000 (0.002)	Loss 0.4366 (0.4416)	Acc@1 86.719 (84.769)	Acc@5 100.000 (99.406)
Epoch: [77][192/196]	Time 0.192 (0.164)	Data 0.000 (0.002)	Loss 0.4536 (0.4438)	Acc@1 83.594 (84.703)	Acc@5 99.609 (99.375)
after train
n1: 30 for:
wAcc: 75.17989438235782
test acc: 75.66
Epoche: [78/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [78][0/196]	Time 0.190 (0.190)	Data 0.247 (0.247)	Loss 0.4693 (0.4693)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [78][64/196]	Time 0.161 (0.166)	Data 0.000 (0.004)	Loss 0.4265 (0.4434)	Acc@1 87.109 (84.814)	Acc@5 99.609 (99.399)
Epoch: [78][128/196]	Time 0.172 (0.166)	Data 0.000 (0.002)	Loss 0.4343 (0.4375)	Acc@1 84.375 (85.020)	Acc@5 100.000 (99.373)
Epoch: [78][192/196]	Time 0.163 (0.167)	Data 0.000 (0.002)	Loss 0.3813 (0.4421)	Acc@1 87.109 (84.798)	Acc@5 100.000 (99.320)
after train
n1: 30 for:
wAcc: 74.23363281137257
test acc: 70.75
Epoche: [79/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [79][0/196]	Time 0.236 (0.236)	Data 0.208 (0.208)	Loss 0.4420 (0.4420)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [79][64/196]	Time 0.096 (0.164)	Data 0.000 (0.003)	Loss 0.4832 (0.4340)	Acc@1 82.422 (84.808)	Acc@5 98.828 (99.417)
Epoch: [79][128/196]	Time 0.146 (0.168)	Data 0.000 (0.002)	Loss 0.4474 (0.4382)	Acc@1 87.109 (84.978)	Acc@5 99.219 (99.343)
Epoch: [79][192/196]	Time 0.171 (0.169)	Data 0.000 (0.001)	Loss 0.4675 (0.4346)	Acc@1 85.156 (85.106)	Acc@5 99.219 (99.366)
after train
n1: 30 for:
wAcc: 74.41365466769325
test acc: 75.37
Epoche: [80/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [80][0/196]	Time 0.169 (0.169)	Data 0.311 (0.311)	Loss 0.4567 (0.4567)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [80][64/196]	Time 0.123 (0.173)	Data 0.000 (0.005)	Loss 0.4382 (0.4355)	Acc@1 83.984 (85.264)	Acc@5 99.219 (99.333)
Epoch: [80][128/196]	Time 0.146 (0.169)	Data 0.000 (0.003)	Loss 0.4925 (0.4391)	Acc@1 83.594 (84.908)	Acc@5 98.828 (99.382)
Epoch: [80][192/196]	Time 0.184 (0.167)	Data 0.000 (0.002)	Loss 0.4232 (0.4347)	Acc@1 86.328 (84.996)	Acc@5 99.219 (99.362)
after train
n1: 30 for:
wAcc: 74.04311531039542
test acc: 62.02
Epoche: [81/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [81][0/196]	Time 0.214 (0.214)	Data 0.242 (0.242)	Loss 0.3573 (0.3573)	Acc@1 87.891 (87.891)	Acc@5 100.000 (100.000)
Epoch: [81][64/196]	Time 0.160 (0.161)	Data 0.000 (0.004)	Loss 0.4819 (0.4385)	Acc@1 80.859 (84.910)	Acc@5 100.000 (99.345)
Epoch: [81][128/196]	Time 0.212 (0.164)	Data 0.000 (0.002)	Loss 0.4362 (0.4396)	Acc@1 83.203 (84.875)	Acc@5 100.000 (99.355)
Epoch: [81][192/196]	Time 0.197 (0.165)	Data 0.000 (0.001)	Loss 0.5699 (0.4337)	Acc@1 80.859 (84.968)	Acc@5 99.609 (99.401)
after train
n1: 30 for:
wAcc: 73.74448359056187
test acc: 75.21
Epoche: [82/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [82][0/196]	Time 0.222 (0.222)	Data 0.294 (0.294)	Loss 0.4771 (0.4771)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [82][64/196]	Time 0.182 (0.168)	Data 0.000 (0.005)	Loss 0.4073 (0.4386)	Acc@1 86.328 (84.880)	Acc@5 99.219 (99.339)
Epoch: [82][128/196]	Time 0.161 (0.165)	Data 0.000 (0.003)	Loss 0.3893 (0.4297)	Acc@1 87.109 (85.147)	Acc@5 99.609 (99.376)
Epoch: [82][192/196]	Time 0.194 (0.165)	Data 0.000 (0.002)	Loss 0.3786 (0.4378)	Acc@1 85.547 (84.883)	Acc@5 99.219 (99.352)
after train
n1: 30 for:
wAcc: 73.48774845223178
test acc: 79.47
Epoche: [83/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [83][0/196]	Time 0.212 (0.212)	Data 0.221 (0.221)	Loss 0.4013 (0.4013)	Acc@1 87.891 (87.891)	Acc@5 98.828 (98.828)
Epoch: [83][64/196]	Time 0.177 (0.162)	Data 0.000 (0.004)	Loss 0.5459 (0.4306)	Acc@1 81.641 (85.186)	Acc@5 99.219 (99.315)
Epoch: [83][128/196]	Time 0.197 (0.161)	Data 0.000 (0.002)	Loss 0.5312 (0.4362)	Acc@1 82.422 (84.887)	Acc@5 98.438 (99.382)
Epoch: [83][192/196]	Time 0.164 (0.161)	Data 0.000 (0.001)	Loss 0.4116 (0.4326)	Acc@1 86.719 (84.994)	Acc@5 100.000 (99.381)
after train
n1: 30 for:
wAcc: 73.43712426211741
test acc: 79.23
Epoche: [84/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [84][0/196]	Time 0.221 (0.221)	Data 0.237 (0.237)	Loss 0.4664 (0.4664)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [84][64/196]	Time 0.189 (0.163)	Data 0.000 (0.004)	Loss 0.4555 (0.4410)	Acc@1 83.594 (84.730)	Acc@5 100.000 (99.399)
Epoch: [84][128/196]	Time 0.077 (0.167)	Data 0.000 (0.002)	Loss 0.4447 (0.4417)	Acc@1 82.812 (84.787)	Acc@5 99.609 (99.340)
Epoch: [84][192/196]	Time 0.121 (0.169)	Data 0.000 (0.001)	Loss 0.5466 (0.4386)	Acc@1 82.812 (84.875)	Acc@5 98.828 (99.308)
after train
n1: 30 for:
wAcc: 75.00638225908945
test acc: 76.52
Epoche: [85/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [85][0/196]	Time 0.190 (0.190)	Data 0.263 (0.263)	Loss 0.4912 (0.4912)	Acc@1 82.812 (82.812)	Acc@5 100.000 (100.000)
Epoch: [85][64/196]	Time 0.093 (0.167)	Data 0.000 (0.004)	Loss 0.4670 (0.4295)	Acc@1 84.766 (85.234)	Acc@5 98.828 (99.399)
Epoch: [85][128/196]	Time 0.088 (0.169)	Data 0.000 (0.002)	Loss 0.4232 (0.4362)	Acc@1 84.375 (85.093)	Acc@5 98.438 (99.355)
Epoch: [85][192/196]	Time 0.194 (0.169)	Data 0.000 (0.002)	Loss 0.5845 (0.4388)	Acc@1 76.953 (85.013)	Acc@5 100.000 (99.379)
after train
n1: 30 for:
wAcc: 74.85683475368212
test acc: 79.44
Epoche: [86/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [86][0/196]	Time 0.121 (0.121)	Data 0.238 (0.238)	Loss 0.4416 (0.4416)	Acc@1 82.422 (82.422)	Acc@5 100.000 (100.000)
Epoch: [86][64/196]	Time 0.087 (0.171)	Data 0.000 (0.004)	Loss 0.4563 (0.4414)	Acc@1 84.375 (84.838)	Acc@5 99.219 (99.417)
Epoch: [86][128/196]	Time 0.175 (0.166)	Data 0.000 (0.002)	Loss 0.4124 (0.4307)	Acc@1 85.156 (85.205)	Acc@5 98.828 (99.376)
Epoch: [86][192/196]	Time 0.170 (0.165)	Data 0.000 (0.001)	Loss 0.5076 (0.4334)	Acc@1 81.250 (85.152)	Acc@5 98.828 (99.362)
after train
n1: 30 for:
wAcc: 75.0267542792884
test acc: 72.77
Epoche: [87/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [87][0/196]	Time 0.206 (0.206)	Data 0.253 (0.253)	Loss 0.4324 (0.4324)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [87][64/196]	Time 0.181 (0.165)	Data 0.000 (0.004)	Loss 0.4967 (0.4237)	Acc@1 82.422 (85.547)	Acc@5 99.609 (99.399)
Epoch: [87][128/196]	Time 0.202 (0.164)	Data 0.000 (0.002)	Loss 0.3537 (0.4268)	Acc@1 89.062 (85.326)	Acc@5 99.609 (99.406)
Epoch: [87][192/196]	Time 0.172 (0.163)	Data 0.000 (0.002)	Loss 0.5042 (0.4318)	Acc@1 82.422 (85.132)	Acc@5 98.828 (99.379)
after train
n1: 30 for:
wAcc: 75.1384768009041
test acc: 76.61
Epoche: [88/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [88][0/196]	Time 0.200 (0.200)	Data 0.231 (0.231)	Loss 0.4576 (0.4576)	Acc@1 82.031 (82.031)	Acc@5 99.609 (99.609)
Epoch: [88][64/196]	Time 0.165 (0.161)	Data 0.000 (0.004)	Loss 0.3507 (0.4162)	Acc@1 88.672 (85.805)	Acc@5 99.219 (99.333)
Epoch: [88][128/196]	Time 0.176 (0.161)	Data 0.000 (0.002)	Loss 0.3941 (0.4296)	Acc@1 85.547 (85.359)	Acc@5 99.609 (99.322)
Epoch: [88][192/196]	Time 0.164 (0.162)	Data 0.000 (0.001)	Loss 0.3334 (0.4299)	Acc@1 89.062 (85.264)	Acc@5 100.000 (99.332)
after train
n1: 30 for:
wAcc: 74.92983451128083
test acc: 74.44
Epoche: [89/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [89][0/196]	Time 0.205 (0.205)	Data 0.266 (0.266)	Loss 0.4164 (0.4164)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [89][64/196]	Time 0.198 (0.162)	Data 0.000 (0.004)	Loss 0.4423 (0.4373)	Acc@1 87.109 (84.910)	Acc@5 99.219 (99.351)
Epoch: [89][128/196]	Time 0.175 (0.163)	Data 0.000 (0.002)	Loss 0.4349 (0.4324)	Acc@1 84.766 (85.177)	Acc@5 99.219 (99.413)
Epoch: [89][192/196]	Time 0.091 (0.164)	Data 0.000 (0.002)	Loss 0.3387 (0.4347)	Acc@1 87.891 (85.055)	Acc@5 99.219 (99.411)
after train
n1: 30 for:
wAcc: 74.23035789028431
test acc: 73.23
Epoche: [90/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [90][0/196]	Time 0.214 (0.214)	Data 0.238 (0.238)	Loss 0.5007 (0.5007)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [90][64/196]	Time 0.171 (0.164)	Data 0.000 (0.004)	Loss 0.4142 (0.4265)	Acc@1 87.891 (85.379)	Acc@5 99.609 (99.453)
Epoch: [90][128/196]	Time 0.149 (0.167)	Data 0.000 (0.002)	Loss 0.4377 (0.4269)	Acc@1 85.547 (85.308)	Acc@5 99.219 (99.437)
Epoch: [90][192/196]	Time 0.111 (0.169)	Data 0.000 (0.001)	Loss 0.5105 (0.4322)	Acc@1 82.422 (85.051)	Acc@5 98.047 (99.411)
after train
n1: 30 for:
wAcc: 74.53878748924298
test acc: 80.96
IndexL: 0
Module= Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); indexL: 1
indexConv: 1
modulelist[indexConv]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
indexConv: 2
modulelist[indexConv]: ReLU(inplace=True)
indexConv: 3
modulelist[indexConv]: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); indexConv: 3; index: 3
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 3, 3, 3); new shape: (8, 3, 3, 3)
new shape: (16, 3, 3, 3)
module after: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 3, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 3
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 3
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 3
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 3
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 3
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 3
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 3
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 4
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 4
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 4
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 4
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 4
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 4
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 4
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 4
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 5
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 5
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 5
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 5
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 5
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 5
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 5
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 5
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 6
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 6
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 6
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 6
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 6
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 6
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 6
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 6
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 7
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 7
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 7
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 7
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 7
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 7
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 7
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 7
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 8
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (16, 8, 3, 3); new shape w2: (16, 8, 3, 3)
new shape: (16, 16, 3, 3)
module1 after: Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
IndexL: 8
Module= Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i: 0 indexL: 8
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 8
module1: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 8
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 16, 3, 3); new shape: (16, 16, 3, 3)
new shape: (32, 16, 3, 3)
module after: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 16, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 8
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 8
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 8
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 9
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 8, 1, 1); new shape w2: (16, 8, 1, 1)
new shape: (16, 16, 1, 1)
module1 after: Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
IndexL: 9
Module= Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 9
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 9
Module= Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 10
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 16, 1, 1); new shape: (16, 16, 1, 1)
new shape: (32, 16, 1, 1)
module after: Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
size of weight after: torch.Size([32, 16, 1, 1])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 10
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 10
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 10
module1: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 10
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 10
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 10
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 10
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 11
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 11
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 11
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 11
module1: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 11
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 11
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 11
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 11
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 12
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 12
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 12
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 12
module1: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 12
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 12
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 12
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 12
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 13
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 13
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 13
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 13
module1: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 13
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 13
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 13
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 13
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 14
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (32, 16, 3, 3); new shape w2: (32, 16, 3, 3)
new shape: (32, 32, 3, 3)
module1 after: Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
IndexL: 14
Module= Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i: 0 indexL: 14
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 14
module1: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 14
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 32, 3, 3); new shape: (32, 32, 3, 3)
new shape: (64, 32, 3, 3)
module after: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 32, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 14
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 14
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 14
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 15
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 16, 1, 1); new shape w2: (32, 16, 1, 1)
new shape: (32, 32, 1, 1)
module1 after: Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
IndexL: 15
Module= Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 15
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 15
Module= Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 16
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 32, 1, 1); new shape: (32, 32, 1, 1)
new shape: (64, 32, 1, 1)
module after: Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
size of weight after: torch.Size([64, 32, 1, 1])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 16
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 16
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 16
module1: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 16
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 16
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 16
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 16
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 17
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 17
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 17
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 17
module1: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 17
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 17
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 17
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 17
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 18
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 18
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 18
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 18
module1: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 18
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 18
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 18
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 18
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 19
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 19
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 19
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 19
module1: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 19
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 19
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 19
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 19
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (10, 32); new shape w2: (10, 32)
new shape: (10, 64)
module after: Linear(in_features=64, out_features=10, bias=True)
size of weight after: torch.Size([10, 64])
self: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
layer: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0
layer: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))

>new Layer:  [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
Sequential: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))

>new Layer:  [Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))]
>new Layer: [Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))

>new Layer:  [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))

>new Layer:  [Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))]
>new Layer: [Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

Linear:  Linear(in_features=64, out_features=10, bias=True)
 Modell: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Epoche: [91/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [91][0/196]	Time 0.286 (0.286)	Data 0.216 (0.216)	Loss 97.9005 (97.9005)	Acc@1 12.891 (12.891)	Acc@5 44.531 (44.531)
Epoch: [91][64/196]	Time 0.245 (0.238)	Data 0.000 (0.004)	Loss 3.6137 (37.7632)	Acc@1 10.938 (11.148)	Acc@5 57.031 (52.518)
Epoch: [91][128/196]	Time 0.246 (0.238)	Data 0.000 (0.002)	Loss 2.4220 (20.3481)	Acc@1 16.797 (13.094)	Acc@5 63.281 (56.662)
Epoch: [91][192/196]	Time 0.197 (0.239)	Data 0.000 (0.001)	Loss 2.3032 (14.3484)	Acc@1 14.844 (14.892)	Acc@5 67.188 (60.261)
after train
n1: 30 for:
wAcc: 75.82042860758804
test acc: 20.14
Epoche: [92/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [92][0/196]	Time 0.305 (0.305)	Data 0.237 (0.237)	Loss 2.1938 (2.1938)	Acc@1 20.703 (20.703)	Acc@5 69.531 (69.531)
Epoch: [92][64/196]	Time 0.238 (0.236)	Data 0.000 (0.004)	Loss 2.1475 (2.1717)	Acc@1 21.484 (20.445)	Acc@5 72.266 (70.523)
Epoch: [92][128/196]	Time 0.232 (0.237)	Data 0.000 (0.002)	Loss 2.1350 (2.1432)	Acc@1 23.438 (21.360)	Acc@5 77.344 (72.317)
Epoch: [92][192/196]	Time 0.257 (0.238)	Data 0.000 (0.001)	Loss 2.0474 (2.1176)	Acc@1 25.781 (21.877)	Acc@5 73.828 (73.670)
after train
n1: 30 for:
wAcc: 71.41859817040914
test acc: 22.9
Epoche: [93/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [93][0/196]	Time 0.260 (0.260)	Data 0.238 (0.238)	Loss 2.0173 (2.0173)	Acc@1 25.000 (25.000)	Acc@5 78.516 (78.516)
Epoch: [93][64/196]	Time 0.233 (0.241)	Data 0.000 (0.004)	Loss 2.1217 (2.0223)	Acc@1 19.141 (24.267)	Acc@5 75.781 (77.782)
Epoch: [93][128/196]	Time 0.241 (0.236)	Data 0.000 (0.002)	Loss 1.9760 (2.0162)	Acc@1 23.438 (24.391)	Acc@5 77.734 (78.134)
Epoch: [93][192/196]	Time 0.255 (0.237)	Data 0.000 (0.001)	Loss 1.9972 (2.0123)	Acc@1 23.828 (24.373)	Acc@5 77.344 (78.370)
after train
n1: 30 for:
wAcc: 67.41955107135279
test acc: 24.73
Epoche: [94/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [94][0/196]	Time 0.253 (0.253)	Data 0.320 (0.320)	Loss 2.0496 (2.0496)	Acc@1 20.312 (20.312)	Acc@5 77.734 (77.734)
Epoch: [94][64/196]	Time 0.249 (0.233)	Data 0.000 (0.005)	Loss 2.0981 (2.0025)	Acc@1 20.703 (25.138)	Acc@5 76.953 (78.480)
Epoch: [94][128/196]	Time 0.284 (0.235)	Data 0.000 (0.003)	Loss 1.9138 (2.0020)	Acc@1 25.391 (24.785)	Acc@5 81.641 (78.546)
Epoch: [94][192/196]	Time 0.318 (0.236)	Data 0.000 (0.002)	Loss 1.9662 (1.9978)	Acc@1 24.609 (24.832)	Acc@5 79.688 (78.607)
after train
n1: 30 for:
wAcc: 65.99246158159424
test acc: 24.65
Epoche: [95/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [95][0/196]	Time 0.313 (0.313)	Data 0.326 (0.326)	Loss 1.9448 (1.9448)	Acc@1 25.000 (25.000)	Acc@5 83.203 (83.203)
Epoch: [95][64/196]	Time 0.233 (0.237)	Data 0.000 (0.005)	Loss 1.9604 (1.9853)	Acc@1 22.656 (25.138)	Acc@5 81.641 (79.639)
Epoch: [95][128/196]	Time 0.244 (0.234)	Data 0.000 (0.003)	Loss 1.9364 (1.9922)	Acc@1 26.172 (24.833)	Acc@5 80.078 (79.300)
Epoch: [95][192/196]	Time 0.263 (0.234)	Data 0.000 (0.002)	Loss 1.9794 (1.9862)	Acc@1 21.484 (25.004)	Acc@5 76.953 (79.451)
after train
n1: 30 for:
wAcc: 63.28183752851206
test acc: 25.05
Epoche: [96/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [96][0/196]	Time 0.270 (0.270)	Data 0.347 (0.347)	Loss 1.9190 (1.9190)	Acc@1 25.781 (25.781)	Acc@5 81.250 (81.250)
Epoch: [96][64/196]	Time 0.257 (0.229)	Data 0.000 (0.006)	Loss 1.9768 (1.9755)	Acc@1 21.094 (24.958)	Acc@5 73.438 (79.567)
Epoch: [96][128/196]	Time 0.280 (0.232)	Data 0.000 (0.003)	Loss 2.0585 (1.9800)	Acc@1 19.531 (25.112)	Acc@5 79.297 (79.733)
Epoch: [96][192/196]	Time 0.214 (0.231)	Data 0.000 (0.002)	Loss 1.9538 (1.9783)	Acc@1 25.781 (25.065)	Acc@5 82.031 (79.712)
after train
n1: 30 for:
wAcc: 60.59264256722815
test acc: 25.08
Epoche: [97/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [97][0/196]	Time 0.292 (0.292)	Data 0.226 (0.226)	Loss 2.0005 (2.0005)	Acc@1 27.734 (27.734)	Acc@5 79.688 (79.688)
Epoch: [97][64/196]	Time 0.103 (0.220)	Data 0.000 (0.004)	Loss 2.0223 (1.9777)	Acc@1 19.922 (24.928)	Acc@5 77.734 (79.675)
Epoch: [97][128/196]	Time 0.253 (0.225)	Data 0.000 (0.002)	Loss 1.9960 (1.9735)	Acc@1 24.219 (25.064)	Acc@5 79.297 (79.848)
Epoch: [97][192/196]	Time 0.256 (0.226)	Data 0.000 (0.001)	Loss 1.9662 (1.9732)	Acc@1 23.047 (25.051)	Acc@5 78.516 (80.042)
after train
n1: 30 for:
wAcc: 58.18296386014522
test acc: 25.55
Epoche: [98/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [98][0/196]	Time 0.308 (0.308)	Data 0.313 (0.313)	Loss 1.9901 (1.9901)	Acc@1 23.047 (23.047)	Acc@5 78.125 (78.125)
Epoch: [98][64/196]	Time 0.270 (0.232)	Data 0.000 (0.005)	Loss 2.0378 (1.9685)	Acc@1 24.609 (24.886)	Acc@5 77.344 (79.808)
Epoch: [98][128/196]	Time 0.100 (0.223)	Data 0.000 (0.003)	Loss 1.9596 (1.9704)	Acc@1 22.266 (24.964)	Acc@5 80.859 (79.896)
Epoch: [98][192/196]	Time 0.291 (0.225)	Data 0.000 (0.002)	Loss 1.9347 (1.9663)	Acc@1 25.781 (25.318)	Acc@5 82.031 (80.234)
after train
n1: 30 for:
wAcc: 54.89798904593684
test acc: 25.5
Epoche: [99/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [99][0/196]	Time 0.281 (0.281)	Data 0.279 (0.279)	Loss 1.9759 (1.9759)	Acc@1 28.516 (28.516)	Acc@5 78.516 (78.516)
Epoch: [99][64/196]	Time 0.252 (0.233)	Data 0.000 (0.005)	Loss 1.9231 (1.9582)	Acc@1 29.688 (25.409)	Acc@5 81.250 (80.072)
Epoch: [99][128/196]	Time 0.255 (0.232)	Data 0.000 (0.002)	Loss 1.9631 (1.9561)	Acc@1 21.875 (25.488)	Acc@5 81.250 (80.251)
Epoch: [99][192/196]	Time 0.280 (0.227)	Data 0.000 (0.002)	Loss 1.8676 (1.9559)	Acc@1 26.953 (25.640)	Acc@5 83.984 (80.339)
after train
n1: 30 for:
wAcc: 51.016514410414985
test acc: 25.64
Epoche: [100/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [100][0/196]	Time 0.292 (0.292)	Data 0.232 (0.232)	Loss 1.9446 (1.9446)	Acc@1 27.344 (27.344)	Acc@5 82.422 (82.422)
Epoch: [100][64/196]	Time 0.267 (0.234)	Data 0.000 (0.004)	Loss 1.9472 (1.9581)	Acc@1 26.172 (25.457)	Acc@5 83.203 (81.142)
Epoch: [100][128/196]	Time 0.224 (0.233)	Data 0.000 (0.002)	Loss 2.0321 (1.9519)	Acc@1 28.125 (25.908)	Acc@5 77.734 (80.884)
Epoch: [100][192/196]	Time 0.207 (0.231)	Data 0.000 (0.001)	Loss 1.9231 (1.9542)	Acc@1 28.516 (25.804)	Acc@5 82.422 (80.710)
after train
n1: 30 for:
wAcc: 52.581358425254386
test acc: 25.99
Epoche: [101/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [101][0/196]	Time 0.288 (0.288)	Data 0.236 (0.236)	Loss 1.9825 (1.9825)	Acc@1 26.562 (26.562)	Acc@5 80.859 (80.859)
Epoch: [101][64/196]	Time 0.097 (0.192)	Data 0.000 (0.004)	Loss 1.9407 (1.9555)	Acc@1 23.828 (25.727)	Acc@5 81.250 (80.529)
Epoch: [101][128/196]	Time 0.098 (0.146)	Data 0.000 (0.002)	Loss 1.9039 (1.9514)	Acc@1 26.953 (25.781)	Acc@5 81.250 (80.396)
Epoch: [101][192/196]	Time 0.095 (0.130)	Data 0.000 (0.001)	Loss 2.0074 (1.9477)	Acc@1 29.688 (26.135)	Acc@5 79.688 (80.600)
after train
n1: 30 for:
wAcc: 49.46643103983617
test acc: 26.21
Epoche: [102/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [102][0/196]	Time 0.160 (0.160)	Data 0.231 (0.231)	Loss 1.8974 (1.8974)	Acc@1 27.734 (27.734)	Acc@5 86.328 (86.328)
Epoch: [102][64/196]	Time 0.102 (0.099)	Data 0.000 (0.004)	Loss 1.9706 (1.9442)	Acc@1 25.391 (25.877)	Acc@5 78.906 (80.962)
Epoch: [102][128/196]	Time 0.097 (0.099)	Data 0.000 (0.002)	Loss 1.9950 (1.9477)	Acc@1 26.562 (25.911)	Acc@5 76.562 (80.811)
Epoch: [102][192/196]	Time 0.098 (0.099)	Data 0.000 (0.001)	Loss 1.8870 (1.9421)	Acc@1 25.781 (26.212)	Acc@5 85.938 (81.007)
after train
n1: 30 for:
wAcc: 48.97071895687859
test acc: 25.87
Epoche: [103/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [103][0/196]	Time 0.146 (0.146)	Data 0.234 (0.234)	Loss 1.9317 (1.9317)	Acc@1 31.250 (31.250)	Acc@5 81.641 (81.641)
Epoch: [103][64/196]	Time 0.100 (0.100)	Data 0.000 (0.004)	Loss 1.9372 (1.9370)	Acc@1 27.734 (26.214)	Acc@5 79.297 (80.811)
Epoch: [103][128/196]	Time 0.100 (0.099)	Data 0.000 (0.002)	Loss 1.9376 (1.9344)	Acc@1 26.953 (26.320)	Acc@5 79.297 (80.932)
Epoch: [103][192/196]	Time 0.103 (0.099)	Data 0.000 (0.001)	Loss 1.9748 (1.9374)	Acc@1 27.734 (26.109)	Acc@5 77.344 (80.938)
after train
n1: 30 for:
wAcc: 47.16954264384641
test acc: 26.67
Epoche: [104/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [104][0/196]	Time 0.156 (0.156)	Data 0.207 (0.207)	Loss 2.0193 (2.0193)	Acc@1 25.000 (25.000)	Acc@5 78.516 (78.516)
Epoch: [104][64/196]	Time 0.095 (0.099)	Data 0.000 (0.003)	Loss 1.9344 (1.9290)	Acc@1 24.219 (26.550)	Acc@5 76.953 (81.058)
Epoch: [104][128/196]	Time 0.099 (0.099)	Data 0.000 (0.002)	Loss 1.9121 (1.9322)	Acc@1 23.828 (26.199)	Acc@5 82.812 (81.229)
Epoch: [104][192/196]	Time 0.097 (0.099)	Data 0.000 (0.001)	Loss 1.9610 (1.9324)	Acc@1 28.125 (26.204)	Acc@5 78.906 (81.347)
after train
n1: 30 for:
wAcc: 46.67388361296325
test acc: 26.46
Epoche: [105/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [105][0/196]	Time 0.121 (0.121)	Data 0.207 (0.207)	Loss 1.9811 (1.9811)	Acc@1 28.125 (28.125)	Acc@5 78.906 (78.906)
Epoch: [105][64/196]	Time 0.096 (0.100)	Data 0.000 (0.003)	Loss 1.9454 (1.9380)	Acc@1 25.391 (26.184)	Acc@5 79.297 (80.938)
Epoch: [105][128/196]	Time 0.097 (0.099)	Data 0.000 (0.002)	Loss 1.9578 (1.9347)	Acc@1 29.297 (25.936)	Acc@5 79.688 (81.189)
Epoch: [105][192/196]	Time 0.091 (0.099)	Data 0.000 (0.001)	Loss 1.9850 (1.9295)	Acc@1 25.000 (26.277)	Acc@5 79.297 (81.232)
after train
n1: 30 for:
wAcc: 45.61551673685922
test acc: 26.82
Epoche: [106/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [106][0/196]	Time 0.141 (0.141)	Data 0.234 (0.234)	Loss 1.9691 (1.9691)	Acc@1 24.609 (24.609)	Acc@5 78.516 (78.516)
Epoch: [106][64/196]	Time 0.099 (0.099)	Data 0.000 (0.004)	Loss 2.0140 (1.9188)	Acc@1 21.484 (26.827)	Acc@5 76.953 (81.310)
Epoch: [106][128/196]	Time 0.102 (0.099)	Data 0.000 (0.002)	Loss 1.8329 (1.9212)	Acc@1 32.812 (26.699)	Acc@5 82.422 (81.429)
Epoch: [106][192/196]	Time 0.097 (0.099)	Data 0.000 (0.001)	Loss 1.9185 (1.9229)	Acc@1 27.344 (26.637)	Acc@5 80.469 (81.392)
after train
n1: 30 for:
wAcc: 43.92440399936199
test acc: 26.95
Epoche: [107/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [107][0/196]	Time 0.134 (0.134)	Data 0.211 (0.211)	Loss 1.9492 (1.9492)	Acc@1 23.047 (23.047)	Acc@5 81.641 (81.641)
Epoch: [107][64/196]	Time 0.096 (0.099)	Data 0.000 (0.003)	Loss 1.9324 (1.9130)	Acc@1 25.781 (27.284)	Acc@5 78.906 (82.200)
Epoch: [107][128/196]	Time 0.107 (0.099)	Data 0.000 (0.002)	Loss 2.0167 (1.9162)	Acc@1 24.609 (26.944)	Acc@5 78.906 (81.674)
Epoch: [107][192/196]	Time 0.096 (0.099)	Data 0.000 (0.001)	Loss 1.9622 (1.9160)	Acc@1 26.562 (27.042)	Acc@5 78.125 (81.752)
after train
n1: 30 for:
wAcc: 42.119483914630734
test acc: 26.93
Epoche: [108/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [108][0/196]	Time 0.166 (0.166)	Data 0.235 (0.235)	Loss 1.7456 (1.7456)	Acc@1 32.422 (32.422)	Acc@5 88.281 (88.281)
Epoch: [108][64/196]	Time 0.093 (0.100)	Data 0.000 (0.004)	Loss 1.9428 (1.9084)	Acc@1 30.078 (27.266)	Acc@5 81.641 (81.839)
Epoch: [108][128/196]	Time 0.099 (0.099)	Data 0.000 (0.002)	Loss 1.9833 (1.9103)	Acc@1 21.875 (26.947)	Acc@5 80.859 (81.904)
Epoch: [108][192/196]	Time 0.095 (0.099)	Data 0.000 (0.001)	Loss 1.9658 (1.9103)	Acc@1 25.781 (27.060)	Acc@5 82.031 (81.920)
after train
n1: 30 for:
wAcc: 41.80739160492331
test acc: 27.28
Epoche: [109/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [109][0/196]	Time 0.133 (0.133)	Data 0.246 (0.246)	Loss 1.9029 (1.9029)	Acc@1 24.219 (24.219)	Acc@5 83.203 (83.203)
Epoch: [109][64/196]	Time 0.100 (0.099)	Data 0.000 (0.004)	Loss 1.9049 (1.9035)	Acc@1 29.297 (26.983)	Acc@5 82.812 (82.133)
Epoch: [109][128/196]	Time 0.097 (0.099)	Data 0.000 (0.002)	Loss 1.8395 (1.9103)	Acc@1 31.641 (26.826)	Acc@5 87.109 (82.149)
Epoch: [109][192/196]	Time 0.093 (0.098)	Data 0.000 (0.001)	Loss 1.8897 (1.9088)	Acc@1 26.172 (26.911)	Acc@5 86.328 (82.130)
after train
n1: 30 for:
wAcc: 38.94024374444494
test acc: 26.85
Epoche: [110/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [110][0/196]	Time 0.155 (0.155)	Data 0.241 (0.241)	Loss 1.8891 (1.8891)	Acc@1 28.906 (28.906)	Acc@5 82.422 (82.422)
Epoch: [110][64/196]	Time 0.100 (0.100)	Data 0.000 (0.004)	Loss 1.9537 (1.9088)	Acc@1 23.828 (27.073)	Acc@5 78.516 (82.284)
Epoch: [110][128/196]	Time 0.105 (0.099)	Data 0.000 (0.002)	Loss 1.8867 (1.9074)	Acc@1 26.172 (26.992)	Acc@5 81.250 (82.104)
Epoch: [110][192/196]	Time 0.105 (0.099)	Data 0.000 (0.001)	Loss 1.8714 (1.9054)	Acc@1 28.516 (27.259)	Acc@5 87.891 (82.151)
after train
n1: 30 for:
wAcc: 40.06699495903101
test acc: 27.07
Epoche: [111/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [111][0/196]	Time 0.155 (0.155)	Data 0.211 (0.211)	Loss 1.9615 (1.9615)	Acc@1 23.438 (23.438)	Acc@5 82.422 (82.422)
Epoch: [111][64/196]	Time 0.092 (0.100)	Data 0.000 (0.003)	Loss 1.8443 (1.9040)	Acc@1 26.562 (27.019)	Acc@5 85.547 (82.374)
Epoch: [111][128/196]	Time 0.100 (0.099)	Data 0.000 (0.002)	Loss 1.8871 (1.9017)	Acc@1 25.391 (27.453)	Acc@5 78.516 (82.367)
Epoch: [111][192/196]	Time 0.099 (0.099)	Data 0.000 (0.001)	Loss 1.9150 (1.9024)	Acc@1 24.609 (27.330)	Acc@5 82.812 (82.272)
after train
n1: 30 for:
wAcc: 39.84431138907741
test acc: 27.12
Epoche: [112/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [112][0/196]	Time 0.146 (0.146)	Data 0.211 (0.211)	Loss 1.8360 (1.8360)	Acc@1 27.734 (27.734)	Acc@5 85.156 (85.156)
Epoch: [112][64/196]	Time 0.098 (0.100)	Data 0.000 (0.003)	Loss 1.9574 (1.9046)	Acc@1 25.000 (26.671)	Acc@5 80.859 (82.007)
Epoch: [112][128/196]	Time 0.103 (0.099)	Data 0.000 (0.002)	Loss 1.8830 (1.8970)	Acc@1 29.688 (27.435)	Acc@5 82.422 (82.171)
Epoch: [112][192/196]	Time 0.091 (0.098)	Data 0.000 (0.001)	Loss 1.8925 (1.8977)	Acc@1 27.734 (27.453)	Acc@5 82.812 (82.266)
after train
n1: 30 for:
wAcc: 38.98869329991475
test acc: 27.53
Epoche: [113/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [113][0/196]	Time 0.143 (0.143)	Data 0.241 (0.241)	Loss 1.9288 (1.9288)	Acc@1 25.391 (25.391)	Acc@5 82.422 (82.422)
Epoch: [113][64/196]	Time 0.106 (0.099)	Data 0.000 (0.004)	Loss 1.8993 (1.8966)	Acc@1 25.781 (27.296)	Acc@5 80.078 (82.392)
Epoch: [113][128/196]	Time 0.092 (0.099)	Data 0.000 (0.002)	Loss 1.9386 (1.8944)	Acc@1 25.391 (27.410)	Acc@5 81.250 (82.431)
Epoch: [113][192/196]	Time 0.102 (0.099)	Data 0.000 (0.001)	Loss 1.8800 (1.8917)	Acc@1 27.344 (27.451)	Acc@5 84.375 (82.464)
after train
n1: 30 for:
wAcc: 37.85766094430807
test acc: 27.4
Epoche: [114/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [114][0/196]	Time 0.163 (0.163)	Data 0.231 (0.231)	Loss 1.9019 (1.9019)	Acc@1 23.828 (23.828)	Acc@5 79.297 (79.297)
Epoch: [114][64/196]	Time 0.096 (0.099)	Data 0.000 (0.004)	Loss 1.9675 (1.8923)	Acc@1 25.391 (27.368)	Acc@5 80.078 (82.163)
Epoch: [114][128/196]	Time 0.099 (0.099)	Data 0.000 (0.002)	Loss 1.7958 (1.8919)	Acc@1 30.078 (27.495)	Acc@5 85.938 (82.180)
Epoch: [114][192/196]	Time 0.096 (0.099)	Data 0.000 (0.001)	Loss 1.8238 (1.8900)	Acc@1 28.906 (27.700)	Acc@5 86.719 (82.446)
after train
n1: 30 for:
wAcc: 37.605092888598804
test acc: 27.96
Epoche: [115/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [115][0/196]	Time 0.145 (0.145)	Data 0.233 (0.233)	Loss 1.9132 (1.9132)	Acc@1 28.906 (28.906)	Acc@5 82.031 (82.031)
Epoch: [115][64/196]	Time 0.100 (0.100)	Data 0.000 (0.004)	Loss 1.9335 (1.8825)	Acc@1 27.344 (27.873)	Acc@5 80.078 (82.536)
Epoch: [115][128/196]	Time 0.096 (0.100)	Data 0.000 (0.002)	Loss 1.8156 (1.8818)	Acc@1 30.469 (27.698)	Acc@5 85.156 (82.658)
Epoch: [115][192/196]	Time 0.097 (0.099)	Data 0.000 (0.001)	Loss 1.8247 (1.8844)	Acc@1 31.250 (27.722)	Acc@5 86.719 (82.744)
after train
n1: 30 for:
wAcc: 36.01860324445957
test acc: 27.3
Epoche: [116/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [116][0/196]	Time 0.153 (0.153)	Data 0.240 (0.240)	Loss 1.8308 (1.8308)	Acc@1 32.812 (32.812)	Acc@5 84.766 (84.766)
Epoch: [116][64/196]	Time 0.097 (0.100)	Data 0.000 (0.004)	Loss 1.9110 (1.8794)	Acc@1 25.391 (28.293)	Acc@5 81.250 (82.812)
Epoch: [116][128/196]	Time 0.095 (0.099)	Data 0.000 (0.002)	Loss 1.8913 (1.8786)	Acc@1 21.094 (28.264)	Acc@5 81.250 (82.864)
Epoch: [116][192/196]	Time 0.095 (0.099)	Data 0.000 (0.001)	Loss 1.8294 (1.8795)	Acc@1 30.469 (28.269)	Acc@5 85.156 (82.784)
after train
n1: 30 for:
wAcc: 36.01122909237178
test acc: 28.07
Epoche: [117/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [117][0/196]	Time 0.147 (0.147)	Data 0.204 (0.204)	Loss 1.8526 (1.8526)	Acc@1 27.734 (27.734)	Acc@5 86.719 (86.719)
Epoch: [117][64/196]	Time 0.093 (0.099)	Data 0.000 (0.003)	Loss 1.8365 (1.8771)	Acc@1 30.469 (27.806)	Acc@5 84.766 (83.450)
Epoch: [117][128/196]	Time 0.102 (0.099)	Data 0.000 (0.002)	Loss 1.8527 (1.8728)	Acc@1 28.516 (28.237)	Acc@5 83.594 (83.291)
Epoch: [117][192/196]	Time 0.095 (0.099)	Data 0.000 (0.001)	Loss 1.8383 (1.8760)	Acc@1 26.953 (28.267)	Acc@5 82.422 (83.244)
after train
n1: 30 for:
wAcc: 35.18519315235646
test acc: 28.43
Epoche: [118/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [118][0/196]	Time 0.161 (0.161)	Data 0.214 (0.214)	Loss 1.8833 (1.8833)	Acc@1 28.516 (28.516)	Acc@5 84.375 (84.375)
Epoch: [118][64/196]	Time 0.103 (0.100)	Data 0.000 (0.004)	Loss 1.8874 (1.8706)	Acc@1 27.344 (28.636)	Acc@5 81.641 (83.275)
Epoch: [118][128/196]	Time 0.099 (0.099)	Data 0.000 (0.002)	Loss 1.9392 (1.8714)	Acc@1 22.266 (28.261)	Acc@5 82.422 (83.364)
Epoch: [118][192/196]	Time 0.093 (0.099)	Data 0.000 (0.001)	Loss 1.8929 (1.8718)	Acc@1 30.078 (28.192)	Acc@5 82.422 (83.242)
after train
n1: 30 for:
wAcc: 34.57445475503728
test acc: 28.16
Epoche: [119/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [119][0/196]	Time 0.167 (0.167)	Data 0.210 (0.210)	Loss 1.9291 (1.9291)	Acc@1 26.172 (26.172)	Acc@5 82.031 (82.031)
Epoch: [119][64/196]	Time 0.097 (0.101)	Data 0.000 (0.003)	Loss 1.8810 (1.8740)	Acc@1 26.562 (28.474)	Acc@5 83.984 (83.023)
Epoch: [119][128/196]	Time 0.098 (0.100)	Data 0.000 (0.002)	Loss 1.8174 (1.8689)	Acc@1 29.688 (28.491)	Acc@5 83.203 (83.342)
Epoch: [119][192/196]	Time 0.092 (0.100)	Data 0.000 (0.001)	Loss 1.8181 (1.8670)	Acc@1 32.422 (28.534)	Acc@5 83.594 (83.462)
after train
n1: 30 for:
wAcc: 35.27807980187748
test acc: 28.35
Epoche: [120/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [120][0/196]	Time 0.136 (0.136)	Data 0.235 (0.235)	Loss 1.8139 (1.8139)	Acc@1 28.906 (28.906)	Acc@5 85.547 (85.547)
Epoch: [120][64/196]	Time 0.095 (0.100)	Data 0.000 (0.004)	Loss 1.8101 (1.8651)	Acc@1 28.906 (28.792)	Acc@5 84.375 (83.215)
Epoch: [120][128/196]	Time 0.102 (0.099)	Data 0.000 (0.002)	Loss 1.8828 (1.8650)	Acc@1 30.078 (28.625)	Acc@5 84.766 (83.385)
Epoch: [120][192/196]	Time 0.094 (0.099)	Data 0.000 (0.001)	Loss 1.9043 (1.8677)	Acc@1 25.781 (28.467)	Acc@5 81.641 (83.422)
after train
n1: 30 for:
wAcc: 26.038872999919352
test acc: 29.25
Epoche: [121/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [121][0/196]	Time 0.159 (0.159)	Data 0.230 (0.230)	Loss 1.8367 (1.8367)	Acc@1 31.641 (31.641)	Acc@5 85.156 (85.156)
Epoch: [121][64/196]	Time 0.103 (0.100)	Data 0.000 (0.004)	Loss 1.8164 (1.8613)	Acc@1 29.297 (29.099)	Acc@5 86.328 (83.624)
Epoch: [121][128/196]	Time 0.100 (0.100)	Data 0.000 (0.002)	Loss 1.8987 (1.8605)	Acc@1 22.266 (29.061)	Acc@5 82.422 (83.470)
Epoch: [121][192/196]	Time 0.099 (0.100)	Data 0.000 (0.001)	Loss 1.7524 (1.8614)	Acc@1 32.031 (28.951)	Acc@5 85.938 (83.462)
after train
n1: 30 for:
wAcc: 26.645032381785978
test acc: 29.63
Epoche: [122/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [122][0/196]	Time 0.136 (0.136)	Data 0.207 (0.207)	Loss 1.7689 (1.7689)	Acc@1 33.984 (33.984)	Acc@5 85.156 (85.156)
Epoch: [122][64/196]	Time 0.098 (0.100)	Data 0.000 (0.003)	Loss 1.8563 (1.8591)	Acc@1 23.047 (29.081)	Acc@5 84.766 (83.456)
Epoch: [122][128/196]	Time 0.097 (0.099)	Data 0.000 (0.002)	Loss 1.8505 (1.8574)	Acc@1 28.516 (29.012)	Acc@5 81.250 (83.751)
Epoch: [122][192/196]	Time 0.104 (0.099)	Data 0.000 (0.001)	Loss 1.8954 (1.8560)	Acc@1 28.125 (29.076)	Acc@5 77.344 (83.699)
after train
n1: 30 for:
wAcc: 27.10215858755433
test acc: 29.65
Epoche: [123/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [123][0/196]	Time 0.154 (0.154)	Data 0.203 (0.203)	Loss 1.8379 (1.8379)	Acc@1 30.859 (30.859)	Acc@5 83.594 (83.594)
Epoch: [123][64/196]	Time 0.101 (0.099)	Data 0.000 (0.003)	Loss 1.8474 (1.8542)	Acc@1 29.688 (28.714)	Acc@5 86.328 (83.972)
Epoch: [123][128/196]	Time 0.095 (0.099)	Data 0.000 (0.002)	Loss 1.8115 (1.8500)	Acc@1 30.078 (29.067)	Acc@5 84.375 (83.887)
Epoch: [123][192/196]	Time 0.095 (0.099)	Data 0.000 (0.001)	Loss 1.8937 (1.8534)	Acc@1 27.734 (28.949)	Acc@5 81.250 (83.701)
after train
n1: 30 for:
wAcc: 27.254970528293974
test acc: 29.28
Epoche: [124/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [124][0/196]	Time 0.163 (0.163)	Data 0.213 (0.213)	Loss 1.9112 (1.9112)	Acc@1 26.562 (26.562)	Acc@5 81.250 (81.250)
Epoch: [124][64/196]	Time 0.098 (0.102)	Data 0.000 (0.004)	Loss 1.8264 (1.8563)	Acc@1 35.156 (29.087)	Acc@5 84.766 (83.413)
Epoch: [124][128/196]	Time 0.101 (0.101)	Data 0.000 (0.002)	Loss 1.8272 (1.8469)	Acc@1 28.516 (29.297)	Acc@5 83.984 (83.803)
Epoch: [124][192/196]	Time 0.094 (0.100)	Data 0.000 (0.001)	Loss 1.8703 (1.8467)	Acc@1 25.391 (29.368)	Acc@5 82.031 (83.940)
after train
n1: 30 for:
wAcc: 27.443442213881866
test acc: 29.65
Epoche: [125/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [125][0/196]	Time 0.145 (0.145)	Data 0.226 (0.226)	Loss 1.9468 (1.9468)	Acc@1 27.734 (27.734)	Acc@5 82.812 (82.812)
Epoch: [125][64/196]	Time 0.095 (0.099)	Data 0.000 (0.004)	Loss 1.8963 (1.8479)	Acc@1 30.859 (29.549)	Acc@5 81.250 (84.056)
Epoch: [125][128/196]	Time 0.097 (0.099)	Data 0.000 (0.002)	Loss 1.8421 (1.8490)	Acc@1 29.688 (29.264)	Acc@5 85.156 (83.875)
Epoch: [125][192/196]	Time 0.097 (0.099)	Data 0.000 (0.001)	Loss 1.8514 (1.8470)	Acc@1 28.125 (29.271)	Acc@5 83.984 (83.833)
after train
n1: 30 for:
wAcc: 27.59013762744548
test acc: 29.76
Epoche: [126/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [126][0/196]	Time 0.157 (0.157)	Data 0.242 (0.242)	Loss 1.9014 (1.9014)	Acc@1 28.516 (28.516)	Acc@5 79.688 (79.688)
Epoch: [126][64/196]	Time 0.100 (0.100)	Data 0.000 (0.004)	Loss 1.8715 (1.8430)	Acc@1 30.859 (29.597)	Acc@5 79.297 (84.044)
Epoch: [126][128/196]	Time 0.096 (0.099)	Data 0.000 (0.002)	Loss 1.8327 (1.8427)	Acc@1 32.031 (29.421)	Acc@5 81.641 (84.075)
Epoch: [126][192/196]	Time 0.096 (0.099)	Data 0.000 (0.001)	Loss 1.9028 (1.8418)	Acc@1 30.078 (29.441)	Acc@5 82.812 (84.179)
after train
n1: 30 for:
wAcc: 27.79807268015962
test acc: 30.03
Epoche: [127/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [127][0/196]	Time 0.148 (0.148)	Data 0.237 (0.237)	Loss 1.7880 (1.7880)	Acc@1 30.078 (30.078)	Acc@5 84.766 (84.766)
Epoch: [127][64/196]	Time 0.096 (0.099)	Data 0.000 (0.004)	Loss 1.7867 (1.8474)	Acc@1 32.812 (28.996)	Acc@5 85.156 (83.894)
Epoch: [127][128/196]	Time 0.099 (0.099)	Data 0.000 (0.002)	Loss 1.8589 (1.8431)	Acc@1 29.688 (29.457)	Acc@5 82.031 (84.060)
Epoch: [127][192/196]	Time 0.096 (0.099)	Data 0.000 (0.001)	Loss 1.8049 (1.8392)	Acc@1 35.547 (29.600)	Acc@5 87.109 (84.100)
after train
n1: 30 for:
wAcc: 27.93483991325492
test acc: 30.76
Epoche: [128/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [128][0/196]	Time 0.160 (0.160)	Data 0.206 (0.206)	Loss 1.8252 (1.8252)	Acc@1 29.688 (29.688)	Acc@5 83.984 (83.984)
Epoch: [128][64/196]	Time 0.095 (0.100)	Data 0.000 (0.003)	Loss 1.8293 (1.8438)	Acc@1 30.469 (29.694)	Acc@5 82.812 (83.978)
Epoch: [128][128/196]	Time 0.109 (0.099)	Data 0.000 (0.002)	Loss 1.7925 (1.8383)	Acc@1 30.078 (29.784)	Acc@5 86.719 (84.269)
Epoch: [128][192/196]	Time 0.093 (0.099)	Data 0.000 (0.001)	Loss 1.8109 (1.8365)	Acc@1 30.469 (30.082)	Acc@5 85.156 (84.260)
after train
n1: 30 for:
wAcc: 28.137346923962166
test acc: 30.72
Epoche: [129/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [129][0/196]	Time 0.150 (0.150)	Data 0.236 (0.236)	Loss 1.8331 (1.8331)	Acc@1 29.688 (29.688)	Acc@5 83.594 (83.594)
Epoch: [129][64/196]	Time 0.096 (0.099)	Data 0.000 (0.004)	Loss 1.8686 (1.8374)	Acc@1 27.734 (29.675)	Acc@5 85.547 (84.099)
Epoch: [129][128/196]	Time 0.098 (0.099)	Data 0.000 (0.002)	Loss 1.8946 (1.8338)	Acc@1 26.562 (29.703)	Acc@5 84.375 (84.387)
Epoch: [129][192/196]	Time 0.099 (0.099)	Data 0.000 (0.001)	Loss 1.8263 (1.8334)	Acc@1 31.250 (29.862)	Acc@5 84.375 (84.347)
after train
n1: 30 for:
wAcc: 28.3545662480964
test acc: 31.33
Epoche: [130/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [130][0/196]	Time 0.141 (0.141)	Data 0.243 (0.243)	Loss 1.8679 (1.8679)	Acc@1 25.781 (25.781)	Acc@5 80.859 (80.859)
Epoch: [130][64/196]	Time 0.095 (0.099)	Data 0.000 (0.004)	Loss 1.9021 (1.8155)	Acc@1 28.516 (30.156)	Acc@5 82.422 (85.006)
Epoch: [130][128/196]	Time 0.099 (0.099)	Data 0.000 (0.002)	Loss 1.8192 (1.8253)	Acc@1 32.812 (30.121)	Acc@5 81.641 (84.599)
Epoch: [130][192/196]	Time 0.102 (0.099)	Data 0.000 (0.001)	Loss 1.8153 (1.8285)	Acc@1 28.906 (30.064)	Acc@5 85.938 (84.535)
after train
n1: 30 for:
wAcc: 28.57833325855459
test acc: 30.95
Epoche: [131/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [131][0/196]	Time 0.125 (0.125)	Data 0.211 (0.211)	Loss 1.8411 (1.8411)	Acc@1 26.172 (26.172)	Acc@5 85.156 (85.156)
Epoch: [131][64/196]	Time 0.103 (0.100)	Data 0.000 (0.003)	Loss 1.8346 (1.8305)	Acc@1 25.000 (29.772)	Acc@5 85.156 (84.736)
Epoch: [131][128/196]	Time 0.100 (0.099)	Data 0.000 (0.002)	Loss 1.8394 (1.8281)	Acc@1 30.469 (29.600)	Acc@5 83.203 (84.626)
Epoch: [131][192/196]	Time 0.099 (0.099)	Data 0.000 (0.001)	Loss 1.8158 (1.8277)	Acc@1 23.047 (29.831)	Acc@5 82.812 (84.476)
after train
n1: 30 for:
wAcc: 28.68219308660461
test acc: 31.37
Epoche: [132/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [132][0/196]	Time 0.156 (0.156)	Data 0.239 (0.239)	Loss 1.8545 (1.8545)	Acc@1 28.125 (28.125)	Acc@5 80.859 (80.859)
Epoch: [132][64/196]	Time 0.100 (0.100)	Data 0.000 (0.004)	Loss 1.8300 (1.8202)	Acc@1 27.734 (30.847)	Acc@5 86.328 (84.856)
Epoch: [132][128/196]	Time 0.105 (0.099)	Data 0.000 (0.002)	Loss 1.7759 (1.8232)	Acc@1 28.516 (30.563)	Acc@5 84.766 (84.439)
Epoch: [132][192/196]	Time 0.096 (0.099)	Data 0.000 (0.001)	Loss 1.8948 (1.8215)	Acc@1 26.562 (30.540)	Acc@5 82.422 (84.440)
after train
n1: 30 for:
wAcc: 28.97124923003736
test acc: 32.02
Epoche: [133/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [133][0/196]	Time 0.147 (0.147)	Data 0.207 (0.207)	Loss 1.8180 (1.8180)	Acc@1 27.344 (27.344)	Acc@5 87.109 (87.109)
Epoch: [133][64/196]	Time 0.092 (0.099)	Data 0.000 (0.003)	Loss 1.8404 (1.8189)	Acc@1 31.250 (30.685)	Acc@5 83.203 (84.724)
Epoch: [133][128/196]	Time 0.099 (0.099)	Data 0.000 (0.002)	Loss 1.8830 (1.8199)	Acc@1 31.250 (30.696)	Acc@5 82.812 (84.738)
Epoch: [133][192/196]	Time 0.099 (0.099)	Data 0.000 (0.001)	Loss 1.7635 (1.8169)	Acc@1 31.641 (30.805)	Acc@5 87.891 (84.737)
after train
n1: 30 for:
wAcc: 29.137584901078448
test acc: 32.0
Epoche: [134/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [134][0/196]	Time 0.131 (0.131)	Data 0.210 (0.210)	Loss 1.8094 (1.8094)	Acc@1 35.938 (35.938)	Acc@5 85.156 (85.156)
Epoch: [134][64/196]	Time 0.095 (0.099)	Data 0.000 (0.003)	Loss 1.7649 (1.8087)	Acc@1 31.250 (31.040)	Acc@5 85.938 (84.862)
Epoch: [134][128/196]	Time 0.100 (0.098)	Data 0.000 (0.002)	Loss 1.7716 (1.8178)	Acc@1 34.375 (30.705)	Acc@5 86.328 (84.708)
Epoch: [134][192/196]	Time 0.095 (0.099)	Data 0.000 (0.001)	Loss 1.8274 (1.8174)	Acc@1 30.078 (30.837)	Acc@5 88.281 (84.705)
after train
n1: 30 for:
wAcc: 29.374299003551812
test acc: 32.15
Epoche: [135/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [135][0/196]	Time 0.147 (0.147)	Data 0.246 (0.246)	Loss 1.7919 (1.7919)	Acc@1 29.688 (29.688)	Acc@5 87.891 (87.891)
Epoch: [135][64/196]	Time 0.101 (0.099)	Data 0.000 (0.004)	Loss 1.8325 (1.8227)	Acc@1 31.641 (30.319)	Acc@5 86.719 (84.519)
Epoch: [135][128/196]	Time 0.106 (0.099)	Data 0.000 (0.002)	Loss 1.8126 (1.8175)	Acc@1 24.609 (30.417)	Acc@5 86.328 (84.623)
Epoch: [135][192/196]	Time 0.097 (0.098)	Data 0.000 (0.002)	Loss 1.8435 (1.8140)	Acc@1 29.297 (30.586)	Acc@5 80.859 (84.731)
after train
n1: 30 for:
wAcc: 29.572169489635208
test acc: 32.88
Epoche: [136/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [136][0/196]	Time 0.158 (0.158)	Data 0.234 (0.234)	Loss 1.7963 (1.7963)	Acc@1 32.422 (32.422)	Acc@5 88.281 (88.281)
Epoch: [136][64/196]	Time 0.096 (0.100)	Data 0.000 (0.004)	Loss 1.8567 (1.8083)	Acc@1 26.953 (31.238)	Acc@5 83.984 (84.928)
Epoch: [136][128/196]	Time 0.100 (0.099)	Data 0.000 (0.002)	Loss 1.7462 (1.8088)	Acc@1 33.594 (31.477)	Acc@5 89.453 (84.847)
Epoch: [136][192/196]	Time 0.094 (0.099)	Data 0.000 (0.001)	Loss 1.7938 (1.8087)	Acc@1 32.031 (31.382)	Acc@5 85.156 (84.893)
after train
n1: 30 for:
wAcc: 29.78268667851389
test acc: 32.74
Epoche: [137/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [137][0/196]	Time 0.136 (0.136)	Data 0.238 (0.238)	Loss 1.8569 (1.8569)	Acc@1 30.078 (30.078)	Acc@5 83.984 (83.984)
Epoch: [137][64/196]	Time 0.094 (0.098)	Data 0.000 (0.004)	Loss 1.8571 (1.8108)	Acc@1 31.250 (31.935)	Acc@5 83.984 (84.754)
Epoch: [137][128/196]	Time 0.098 (0.098)	Data 0.000 (0.002)	Loss 1.8068 (1.8100)	Acc@1 29.297 (31.326)	Acc@5 85.938 (84.920)
Epoch: [137][192/196]	Time 0.103 (0.099)	Data 0.000 (0.001)	Loss 1.8555 (1.8061)	Acc@1 27.344 (31.408)	Acc@5 83.984 (85.162)
after train
n1: 30 for:
wAcc: 30.024077631386728
test acc: 32.12
Epoche: [138/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [138][0/196]	Time 0.151 (0.151)	Data 0.250 (0.250)	Loss 1.9320 (1.9320)	Acc@1 26.172 (26.172)	Acc@5 80.078 (80.078)
Epoch: [138][64/196]	Time 0.098 (0.100)	Data 0.000 (0.004)	Loss 1.7414 (1.8049)	Acc@1 33.594 (31.514)	Acc@5 85.156 (84.802)
Epoch: [138][128/196]	Time 0.092 (0.099)	Data 0.000 (0.002)	Loss 1.7550 (1.8034)	Acc@1 29.688 (31.504)	Acc@5 82.812 (84.969)
Epoch: [138][192/196]	Time 0.093 (0.099)	Data 0.000 (0.002)	Loss 1.7972 (1.8039)	Acc@1 29.688 (31.380)	Acc@5 88.281 (85.039)
after train
n1: 30 for:
wAcc: 30.097136959747314
test acc: 33.2
Epoche: [139/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [139][0/196]	Time 0.151 (0.151)	Data 0.225 (0.225)	Loss 1.8200 (1.8200)	Acc@1 31.250 (31.250)	Acc@5 80.859 (80.859)
Epoch: [139][64/196]	Time 0.102 (0.099)	Data 0.000 (0.004)	Loss 1.8587 (1.7999)	Acc@1 30.078 (31.496)	Acc@5 82.812 (85.246)
Epoch: [139][128/196]	Time 0.099 (0.098)	Data 0.000 (0.002)	Loss 1.7844 (1.7976)	Acc@1 35.938 (31.786)	Acc@5 82.031 (85.405)
Epoch: [139][192/196]	Time 0.099 (0.099)	Data 0.000 (0.001)	Loss 1.8157 (1.7990)	Acc@1 31.250 (31.740)	Acc@5 85.938 (85.286)
after train
n1: 30 for:
wAcc: 30.329125214615125
test acc: 33.13
Epoche: [140/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [140][0/196]	Time 0.125 (0.125)	Data 0.231 (0.231)	Loss 1.6923 (1.6923)	Acc@1 35.156 (35.156)	Acc@5 87.891 (87.891)
Epoch: [140][64/196]	Time 0.095 (0.100)	Data 0.000 (0.004)	Loss 1.8296 (1.7966)	Acc@1 26.562 (31.899)	Acc@5 87.109 (85.084)
Epoch: [140][128/196]	Time 0.102 (0.099)	Data 0.000 (0.002)	Loss 1.7699 (1.7980)	Acc@1 31.641 (31.507)	Acc@5 86.719 (85.120)
Epoch: [140][192/196]	Time 0.101 (0.099)	Data 0.000 (0.001)	Loss 1.8411 (1.7983)	Acc@1 26.953 (31.716)	Acc@5 84.375 (85.231)
after train
n1: 30 for:
wAcc: 30.517054891534364
test acc: 33.21
Epoche: [141/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [141][0/196]	Time 0.145 (0.145)	Data 0.239 (0.239)	Loss 1.7843 (1.7843)	Acc@1 32.422 (32.422)	Acc@5 86.328 (86.328)
Epoch: [141][64/196]	Time 0.092 (0.100)	Data 0.000 (0.004)	Loss 1.8604 (1.7908)	Acc@1 30.469 (32.079)	Acc@5 80.469 (85.331)
Epoch: [141][128/196]	Time 0.098 (0.099)	Data 0.000 (0.002)	Loss 1.8805 (1.7929)	Acc@1 23.047 (32.107)	Acc@5 82.422 (85.377)
Epoch: [141][192/196]	Time 0.099 (0.099)	Data 0.000 (0.001)	Loss 1.7224 (1.7935)	Acc@1 33.984 (32.076)	Acc@5 87.891 (85.286)
after train
n1: 30 for:
wAcc: 30.750063524098522
test acc: 33.52
Epoche: [142/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [142][0/196]	Time 0.157 (0.157)	Data 0.234 (0.234)	Loss 1.7951 (1.7951)	Acc@1 37.891 (37.891)	Acc@5 84.375 (84.375)
Epoch: [142][64/196]	Time 0.093 (0.100)	Data 0.000 (0.004)	Loss 1.8195 (1.7925)	Acc@1 31.250 (31.869)	Acc@5 86.719 (85.216)
Epoch: [142][128/196]	Time 0.097 (0.099)	Data 0.000 (0.002)	Loss 1.7872 (1.7904)	Acc@1 28.516 (31.901)	Acc@5 89.062 (85.423)
Epoch: [142][192/196]	Time 0.095 (0.099)	Data 0.000 (0.001)	Loss 1.7337 (1.7903)	Acc@1 29.688 (31.847)	Acc@5 88.281 (85.407)
after train
n1: 30 for:
wAcc: 30.90997610074736
test acc: 33.6
Epoche: [143/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [143][0/196]	Time 0.153 (0.153)	Data 0.232 (0.232)	Loss 1.7652 (1.7652)	Acc@1 33.594 (33.594)	Acc@5 88.281 (88.281)
Epoch: [143][64/196]	Time 0.098 (0.100)	Data 0.000 (0.004)	Loss 1.7411 (1.7985)	Acc@1 34.766 (31.701)	Acc@5 89.844 (85.114)
Epoch: [143][128/196]	Time 0.099 (0.099)	Data 0.000 (0.002)	Loss 1.7090 (1.7888)	Acc@1 35.938 (32.104)	Acc@5 87.500 (85.395)
Epoch: [143][192/196]	Time 0.099 (0.099)	Data 0.000 (0.001)	Loss 1.7289 (1.7899)	Acc@1 29.688 (32.078)	Acc@5 85.938 (85.415)
after train
n1: 30 for:
wAcc: 31.16448050178744
test acc: 33.5
Epoche: [144/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [144][0/196]	Time 0.158 (0.158)	Data 0.239 (0.239)	Loss 1.7421 (1.7421)	Acc@1 32.031 (32.031)	Acc@5 85.547 (85.547)
Epoch: [144][64/196]	Time 0.095 (0.100)	Data 0.000 (0.004)	Loss 1.7776 (1.7868)	Acc@1 33.984 (31.899)	Acc@5 86.719 (85.751)
Epoch: [144][128/196]	Time 0.102 (0.100)	Data 0.000 (0.002)	Loss 1.7308 (1.7863)	Acc@1 33.594 (32.037)	Acc@5 89.062 (85.807)
Epoch: [144][192/196]	Time 0.092 (0.099)	Data 0.000 (0.001)	Loss 1.7918 (1.7871)	Acc@1 33.594 (32.108)	Acc@5 85.547 (85.589)
after train
n1: 30 for:
wAcc: 31.21974855131114
test acc: 33.65
Epoche: [145/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [145][0/196]	Time 0.130 (0.130)	Data 0.214 (0.214)	Loss 1.7750 (1.7750)	Acc@1 28.906 (28.906)	Acc@5 87.500 (87.500)
Epoch: [145][64/196]	Time 0.098 (0.100)	Data 0.000 (0.004)	Loss 1.8826 (1.7794)	Acc@1 32.031 (32.073)	Acc@5 83.203 (85.871)
Epoch: [145][128/196]	Time 0.101 (0.099)	Data 0.000 (0.002)	Loss 1.6933 (1.7858)	Acc@1 34.375 (32.440)	Acc@5 85.547 (85.559)
Epoch: [145][192/196]	Time 0.100 (0.099)	Data 0.000 (0.001)	Loss 1.7544 (1.7841)	Acc@1 32.812 (32.515)	Acc@5 88.672 (85.660)
after train
n1: 30 for:
wAcc: 31.48785136643264
test acc: 33.37
Epoche: [146/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [146][0/196]	Time 0.143 (0.143)	Data 0.238 (0.238)	Loss 1.7900 (1.7900)	Acc@1 32.812 (32.812)	Acc@5 85.156 (85.156)
Epoch: [146][64/196]	Time 0.103 (0.099)	Data 0.000 (0.004)	Loss 1.8140 (1.7896)	Acc@1 30.469 (32.145)	Acc@5 83.984 (85.577)
Epoch: [146][128/196]	Time 0.100 (0.100)	Data 0.000 (0.002)	Loss 1.7590 (1.7818)	Acc@1 31.250 (32.277)	Acc@5 86.328 (85.692)
Epoch: [146][192/196]	Time 0.093 (0.099)	Data 0.000 (0.001)	Loss 1.7742 (1.7804)	Acc@1 33.984 (32.385)	Acc@5 87.500 (85.660)
after train
n1: 30 for:
wAcc: 31.66132247114122
test acc: 33.45
Epoche: [147/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [147][0/196]	Time 0.159 (0.159)	Data 0.216 (0.216)	Loss 1.7755 (1.7755)	Acc@1 30.859 (30.859)	Acc@5 88.672 (88.672)
Epoch: [147][64/196]	Time 0.095 (0.099)	Data 0.000 (0.004)	Loss 1.6720 (1.7722)	Acc@1 33.984 (32.602)	Acc@5 89.844 (85.613)
Epoch: [147][128/196]	Time 0.096 (0.099)	Data 0.000 (0.002)	Loss 1.7508 (1.7769)	Acc@1 36.719 (32.740)	Acc@5 89.062 (85.565)
Epoch: [147][192/196]	Time 0.099 (0.099)	Data 0.000 (0.001)	Loss 1.8624 (1.7760)	Acc@1 29.297 (32.727)	Acc@5 84.375 (85.674)
after train
n1: 30 for:
wAcc: 31.737689400934585
test acc: 33.85
Epoche: [148/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [148][0/196]	Time 0.165 (0.165)	Data 0.205 (0.205)	Loss 1.7788 (1.7788)	Acc@1 34.375 (34.375)	Acc@5 87.109 (87.109)
Epoch: [148][64/196]	Time 0.100 (0.100)	Data 0.000 (0.003)	Loss 1.8109 (1.7726)	Acc@1 28.125 (33.263)	Acc@5 84.375 (85.944)
Epoch: [148][128/196]	Time 0.098 (0.099)	Data 0.000 (0.002)	Loss 1.6882 (1.7748)	Acc@1 37.891 (32.979)	Acc@5 88.281 (85.980)
Epoch: [148][192/196]	Time 0.103 (0.099)	Data 0.000 (0.001)	Loss 1.7889 (1.7753)	Acc@1 30.078 (32.983)	Acc@5 87.109 (85.800)
after train
n1: 30 for:
wAcc: 31.901434199976244
test acc: 34.08
Epoche: [149/180]; Lr: 0.010000000000000002
batch Size 256
befor train
Epoch: [149][0/196]	Time 0.154 (0.154)	Data 0.235 (0.235)	Loss 1.8447 (1.8447)	Acc@1 33.203 (33.203)	Acc@5 86.719 (86.719)
Epoch: [149][64/196]	Time 0.093 (0.099)	Data 0.000 (0.004)	Loss 1.7503 (1.7786)	Acc@1 36.328 (32.644)	Acc@5 88.281 (85.727)
Epoch: [149][128/196]	Time 0.095 (0.099)	Data 0.000 (0.002)	Loss 1.6735 (1.7758)	Acc@1 35.156 (32.691)	Acc@5 89.062 (85.765)
Epoch: [149][192/196]	Time 0.099 (0.099)	Data 0.000 (0.001)	Loss 1.7826 (1.7723)	Acc@1 32.812 (32.675)	Acc@5 84.766 (85.792)
after train
n1: 30 for:
wAcc: 32.17209223375447
test acc: 34.82
Epoche: [150/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [150][0/196]	Time 0.156 (0.156)	Data 0.233 (0.233)	Loss 1.7481 (1.7481)	Acc@1 32.031 (32.031)	Acc@5 85.547 (85.547)
Epoch: [150][64/196]	Time 0.101 (0.101)	Data 0.000 (0.004)	Loss 1.8300 (1.7767)	Acc@1 28.516 (32.831)	Acc@5 85.156 (85.505)
Epoch: [150][128/196]	Time 0.101 (0.099)	Data 0.000 (0.002)	Loss 1.8020 (1.7685)	Acc@1 24.609 (33.224)	Acc@5 86.719 (85.786)
Epoch: [150][192/196]	Time 0.097 (0.099)	Data 0.000 (0.001)	Loss 1.7870 (1.7689)	Acc@1 32.422 (33.345)	Acc@5 85.547 (85.960)
after train
n1: 30 for:
wAcc: 32.397858384619404
test acc: 34.43
Epoche: [151/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [151][0/196]	Time 0.155 (0.155)	Data 0.236 (0.236)	Loss 1.7180 (1.7180)	Acc@1 34.375 (34.375)	Acc@5 88.281 (88.281)
Epoch: [151][64/196]	Time 0.095 (0.100)	Data 0.000 (0.004)	Loss 1.8070 (1.7644)	Acc@1 26.562 (33.071)	Acc@5 84.766 (86.184)
Epoch: [151][128/196]	Time 0.103 (0.099)	Data 0.000 (0.002)	Loss 1.8482 (1.7688)	Acc@1 29.297 (32.855)	Acc@5 85.938 (86.050)
Epoch: [151][192/196]	Time 0.097 (0.099)	Data 0.000 (0.001)	Loss 1.7521 (1.7674)	Acc@1 31.641 (33.033)	Acc@5 90.234 (86.065)
after train
n1: 30 for:
wAcc: 32.531855526433986
test acc: 34.85
Epoche: [152/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [152][0/196]	Time 0.150 (0.150)	Data 0.203 (0.203)	Loss 1.8030 (1.8030)	Acc@1 29.688 (29.688)	Acc@5 87.109 (87.109)
Epoch: [152][64/196]	Time 0.096 (0.099)	Data 0.000 (0.003)	Loss 1.7388 (1.7597)	Acc@1 29.297 (33.810)	Acc@5 86.719 (86.418)
Epoch: [152][128/196]	Time 0.102 (0.099)	Data 0.000 (0.002)	Loss 1.8128 (1.7622)	Acc@1 33.594 (33.246)	Acc@5 84.766 (86.162)
Epoch: [152][192/196]	Time 0.098 (0.099)	Data 0.000 (0.001)	Loss 1.7995 (1.7691)	Acc@1 29.688 (33.068)	Acc@5 83.203 (85.905)
after train
n1: 30 for:
wAcc: 32.627925458226095
test acc: 34.49
Epoche: [153/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [153][0/196]	Time 0.154 (0.154)	Data 0.240 (0.240)	Loss 1.7148 (1.7148)	Acc@1 36.719 (36.719)	Acc@5 91.406 (91.406)
Epoch: [153][64/196]	Time 0.099 (0.099)	Data 0.000 (0.004)	Loss 1.7691 (1.7713)	Acc@1 32.422 (32.806)	Acc@5 88.281 (85.865)
Epoch: [153][128/196]	Time 0.097 (0.099)	Data 0.000 (0.002)	Loss 1.7874 (1.7715)	Acc@1 30.469 (32.809)	Acc@5 86.719 (85.726)
Epoch: [153][192/196]	Time 0.097 (0.099)	Data 0.000 (0.001)	Loss 1.6541 (1.7688)	Acc@1 35.547 (32.980)	Acc@5 88.672 (85.871)
after train
n1: 30 for:
wAcc: 32.80154707581075
test acc: 34.68
Epoche: [154/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [154][0/196]	Time 0.153 (0.153)	Data 0.239 (0.239)	Loss 1.8006 (1.8006)	Acc@1 34.375 (34.375)	Acc@5 84.766 (84.766)
Epoch: [154][64/196]	Time 0.096 (0.101)	Data 0.000 (0.004)	Loss 1.8116 (1.7728)	Acc@1 35.938 (33.648)	Acc@5 85.938 (85.709)
Epoch: [154][128/196]	Time 0.097 (0.100)	Data 0.000 (0.002)	Loss 1.6465 (1.7699)	Acc@1 39.062 (33.324)	Acc@5 89.844 (85.856)
Epoch: [154][192/196]	Time 0.101 (0.099)	Data 0.000 (0.001)	Loss 1.8360 (1.7688)	Acc@1 32.422 (33.278)	Acc@5 84.375 (85.881)
after train
n1: 30 for:
wAcc: 32.93863935834549
test acc: 34.76
Epoche: [155/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [155][0/196]	Time 0.154 (0.154)	Data 0.203 (0.203)	Loss 1.7370 (1.7370)	Acc@1 33.203 (33.203)	Acc@5 89.453 (89.453)
Epoch: [155][64/196]	Time 0.097 (0.099)	Data 0.000 (0.003)	Loss 1.7614 (1.7783)	Acc@1 33.594 (33.107)	Acc@5 88.281 (85.859)
Epoch: [155][128/196]	Time 0.102 (0.099)	Data 0.000 (0.002)	Loss 1.8293 (1.7710)	Acc@1 33.203 (33.291)	Acc@5 82.422 (85.895)
Epoch: [155][192/196]	Time 0.101 (0.099)	Data 0.000 (0.001)	Loss 1.7601 (1.7660)	Acc@1 35.938 (33.458)	Acc@5 85.156 (86.065)
after train
n1: 30 for:
wAcc: 33.095178116972335
test acc: 34.67
Epoche: [156/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [156][0/196]	Time 0.164 (0.164)	Data 0.210 (0.210)	Loss 1.8141 (1.8141)	Acc@1 33.984 (33.984)	Acc@5 86.719 (86.719)
Epoch: [156][64/196]	Time 0.103 (0.100)	Data 0.000 (0.003)	Loss 1.6687 (1.7696)	Acc@1 35.938 (33.492)	Acc@5 87.891 (85.992)
Epoch: [156][128/196]	Time 0.103 (0.099)	Data 0.000 (0.002)	Loss 1.7601 (1.7717)	Acc@1 33.984 (33.473)	Acc@5 83.203 (85.892)
Epoch: [156][192/196]	Time 0.094 (0.099)	Data 0.000 (0.001)	Loss 1.7563 (1.7679)	Acc@1 33.203 (33.410)	Acc@5 84.375 (85.988)
after train
n1: 30 for:
wAcc: 33.30230946556791
test acc: 34.7
Epoche: [157/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [157][0/196]	Time 0.152 (0.152)	Data 0.239 (0.239)	Loss 1.7948 (1.7948)	Acc@1 28.516 (28.516)	Acc@5 84.766 (84.766)
Epoch: [157][64/196]	Time 0.095 (0.099)	Data 0.000 (0.004)	Loss 1.7718 (1.7650)	Acc@1 30.078 (33.245)	Acc@5 84.375 (85.992)
Epoch: [157][128/196]	Time 0.095 (0.099)	Data 0.000 (0.002)	Loss 1.7437 (1.7668)	Acc@1 39.062 (33.309)	Acc@5 87.500 (85.922)
Epoch: [157][192/196]	Time 0.097 (0.099)	Data 0.000 (0.001)	Loss 1.7821 (1.7684)	Acc@1 33.203 (33.331)	Acc@5 84.766 (85.946)
after train
n1: 30 for:
wAcc: 33.38670058614479
test acc: 34.61
Epoche: [158/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [158][0/196]	Time 0.164 (0.164)	Data 0.235 (0.235)	Loss 1.7098 (1.7098)	Acc@1 36.719 (36.719)	Acc@5 86.328 (86.328)
Epoch: [158][64/196]	Time 0.100 (0.100)	Data 0.000 (0.004)	Loss 1.7136 (1.7753)	Acc@1 28.516 (33.071)	Acc@5 86.719 (85.523)
Epoch: [158][128/196]	Time 0.097 (0.100)	Data 0.000 (0.002)	Loss 1.7637 (1.7663)	Acc@1 33.203 (33.500)	Acc@5 86.719 (85.813)
Epoch: [158][192/196]	Time 0.096 (0.099)	Data 0.000 (0.001)	Loss 1.6941 (1.7654)	Acc@1 35.938 (33.517)	Acc@5 89.844 (85.911)
after train
n1: 30 for:
wAcc: 33.55380567889233
test acc: 34.95
Epoche: [159/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [159][0/196]	Time 0.155 (0.155)	Data 0.203 (0.203)	Loss 1.7711 (1.7711)	Acc@1 33.984 (33.984)	Acc@5 87.109 (87.109)
Epoch: [159][64/196]	Time 0.114 (0.099)	Data 0.000 (0.003)	Loss 1.6765 (1.7703)	Acc@1 37.500 (32.981)	Acc@5 88.281 (86.118)
Epoch: [159][128/196]	Time 0.095 (0.100)	Data 0.000 (0.002)	Loss 1.8417 (1.7709)	Acc@1 30.469 (33.127)	Acc@5 87.500 (86.162)
Epoch: [159][192/196]	Time 0.096 (0.099)	Data 0.000 (0.001)	Loss 1.7302 (1.7700)	Acc@1 35.938 (33.264)	Acc@5 87.109 (85.889)
after train
n1: 30 for:
wAcc: 33.58894934011471
test acc: 34.95
Epoche: [160/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [160][0/196]	Time 0.129 (0.129)	Data 0.229 (0.229)	Loss 1.7500 (1.7500)	Acc@1 33.203 (33.203)	Acc@5 88.281 (88.281)
Epoch: [160][64/196]	Time 0.100 (0.099)	Data 0.000 (0.004)	Loss 1.8493 (1.7671)	Acc@1 33.594 (33.209)	Acc@5 84.375 (86.082)
Epoch: [160][128/196]	Time 0.093 (0.099)	Data 0.000 (0.002)	Loss 1.7933 (1.7653)	Acc@1 33.984 (33.215)	Acc@5 85.547 (86.310)
Epoch: [160][192/196]	Time 0.091 (0.099)	Data 0.000 (0.001)	Loss 1.7298 (1.7656)	Acc@1 32.031 (33.229)	Acc@5 87.891 (86.170)
after train
n1: 30 for:
wAcc: 33.73747491414935
test acc: 35.03
Epoche: [161/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [161][0/196]	Time 0.131 (0.131)	Data 0.221 (0.221)	Loss 1.7489 (1.7489)	Acc@1 36.719 (36.719)	Acc@5 87.500 (87.500)
Epoch: [161][64/196]	Time 0.095 (0.099)	Data 0.000 (0.004)	Loss 1.7270 (1.7752)	Acc@1 32.422 (32.800)	Acc@5 89.062 (85.511)
Epoch: [161][128/196]	Time 0.098 (0.099)	Data 0.000 (0.002)	Loss 1.6741 (1.7664)	Acc@1 37.109 (33.224)	Acc@5 88.672 (85.819)
Epoch: [161][192/196]	Time 0.095 (0.099)	Data 0.000 (0.001)	Loss 1.7409 (1.7660)	Acc@1 36.328 (33.365)	Acc@5 87.500 (85.938)
after train
n1: 30 for:
wAcc: 33.91482864157342
test acc: 34.85
Epoche: [162/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [162][0/196]	Time 0.116 (0.116)	Data 0.242 (0.242)	Loss 1.7638 (1.7638)	Acc@1 33.594 (33.594)	Acc@5 87.891 (87.891)
Epoch: [162][64/196]	Time 0.100 (0.101)	Data 0.000 (0.004)	Loss 1.7998 (1.7683)	Acc@1 36.328 (33.185)	Acc@5 83.984 (85.865)
Epoch: [162][128/196]	Time 0.096 (0.100)	Data 0.000 (0.002)	Loss 1.7746 (1.7644)	Acc@1 34.375 (33.467)	Acc@5 84.375 (86.001)
Epoch: [162][192/196]	Time 0.097 (0.099)	Data 0.000 (0.001)	Loss 1.7361 (1.7646)	Acc@1 35.156 (33.450)	Acc@5 85.547 (86.138)
after train
n1: 30 for:
wAcc: 33.97227104645607
test acc: 34.78
Epoche: [163/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [163][0/196]	Time 0.152 (0.152)	Data 0.236 (0.236)	Loss 1.8523 (1.8523)	Acc@1 29.688 (29.688)	Acc@5 83.594 (83.594)
Epoch: [163][64/196]	Time 0.101 (0.100)	Data 0.000 (0.004)	Loss 1.7819 (1.7635)	Acc@1 36.719 (33.395)	Acc@5 86.328 (86.190)
Epoch: [163][128/196]	Time 0.098 (0.100)	Data 0.000 (0.002)	Loss 1.8962 (1.7642)	Acc@1 27.344 (33.352)	Acc@5 81.641 (86.149)
Epoch: [163][192/196]	Time 0.093 (0.099)	Data 0.000 (0.001)	Loss 1.6948 (1.7625)	Acc@1 34.375 (33.410)	Acc@5 89.062 (86.055)
after train
n1: 30 for:
wAcc: 34.04606682543246
test acc: 34.77
Epoche: [164/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [164][0/196]	Time 0.140 (0.140)	Data 0.209 (0.209)	Loss 1.8258 (1.8258)	Acc@1 31.250 (31.250)	Acc@5 84.766 (84.766)
Epoch: [164][64/196]	Time 0.101 (0.100)	Data 0.000 (0.003)	Loss 1.7367 (1.7611)	Acc@1 34.375 (33.762)	Acc@5 88.672 (86.298)
Epoch: [164][128/196]	Time 0.098 (0.100)	Data 0.000 (0.002)	Loss 1.7726 (1.7609)	Acc@1 33.984 (33.539)	Acc@5 86.328 (86.222)
Epoch: [164][192/196]	Time 0.094 (0.099)	Data 0.000 (0.001)	Loss 1.7078 (1.7649)	Acc@1 33.984 (33.456)	Acc@5 86.719 (85.996)
after train
n1: 30 for:
wAcc: 34.19830212832091
test acc: 34.61
Epoche: [165/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [165][0/196]	Time 0.148 (0.148)	Data 0.205 (0.205)	Loss 1.7638 (1.7638)	Acc@1 29.297 (29.297)	Acc@5 86.328 (86.328)
Epoch: [165][64/196]	Time 0.099 (0.100)	Data 0.000 (0.003)	Loss 1.8110 (1.7632)	Acc@1 33.594 (33.840)	Acc@5 84.375 (86.088)
Epoch: [165][128/196]	Time 0.097 (0.099)	Data 0.000 (0.002)	Loss 1.7553 (1.7612)	Acc@1 35.547 (33.824)	Acc@5 87.500 (86.228)
Epoch: [165][192/196]	Time 0.097 (0.099)	Data 0.000 (0.001)	Loss 1.7232 (1.7632)	Acc@1 37.109 (33.598)	Acc@5 87.500 (86.170)
after train
n1: 30 for:
wAcc: 34.20462466331847
test acc: 34.61
Epoche: [166/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [166][0/196]	Time 0.132 (0.132)	Data 0.214 (0.214)	Loss 1.7855 (1.7855)	Acc@1 32.422 (32.422)	Acc@5 84.375 (84.375)
Epoch: [166][64/196]	Time 0.095 (0.099)	Data 0.000 (0.004)	Loss 1.6758 (1.7584)	Acc@1 40.234 (34.062)	Acc@5 90.234 (85.925)
Epoch: [166][128/196]	Time 0.096 (0.098)	Data 0.000 (0.002)	Loss 1.7313 (1.7610)	Acc@1 33.594 (33.688)	Acc@5 89.062 (86.086)
Epoch: [166][192/196]	Time 0.093 (0.098)	Data 0.000 (0.001)	Loss 1.8141 (1.7639)	Acc@1 29.688 (33.444)	Acc@5 81.250 (85.976)
after train
n1: 30 for:
wAcc: 34.141149745355705
test acc: 34.58
Epoche: [167/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [167][0/196]	Time 0.142 (0.142)	Data 0.255 (0.255)	Loss 1.7103 (1.7103)	Acc@1 33.594 (33.594)	Acc@5 88.281 (88.281)
Epoch: [167][64/196]	Time 0.096 (0.100)	Data 0.000 (0.004)	Loss 1.7146 (1.7696)	Acc@1 34.375 (33.359)	Acc@5 87.891 (85.457)
Epoch: [167][128/196]	Time 0.098 (0.100)	Data 0.000 (0.002)	Loss 1.6658 (1.7609)	Acc@1 41.406 (33.573)	Acc@5 89.844 (85.892)
Epoch: [167][192/196]	Time 0.101 (0.100)	Data 0.000 (0.002)	Loss 1.7999 (1.7636)	Acc@1 32.031 (33.432)	Acc@5 83.984 (85.988)
after train
n1: 30 for:
wAcc: 34.32558914683253
test acc: 34.56
Epoche: [168/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [168][0/196]	Time 0.172 (0.172)	Data 0.206 (0.206)	Loss 1.7405 (1.7405)	Acc@1 35.156 (35.156)	Acc@5 84.766 (84.766)
Epoch: [168][64/196]	Time 0.099 (0.100)	Data 0.000 (0.003)	Loss 1.7363 (1.7614)	Acc@1 35.547 (33.546)	Acc@5 86.328 (86.178)
Epoch: [168][128/196]	Time 0.100 (0.099)	Data 0.000 (0.002)	Loss 1.7920 (1.7619)	Acc@1 32.422 (33.515)	Acc@5 85.938 (86.222)
Epoch: [168][192/196]	Time 0.093 (0.099)	Data 0.000 (0.001)	Loss 1.7990 (1.7634)	Acc@1 29.688 (33.474)	Acc@5 84.375 (86.182)
after train
n1: 30 for:
wAcc: 34.33059311867504
test acc: 34.82
Epoche: [169/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [169][0/196]	Time 0.142 (0.142)	Data 0.206 (0.206)	Loss 1.6554 (1.6554)	Acc@1 38.672 (38.672)	Acc@5 89.844 (89.844)
Epoch: [169][64/196]	Time 0.094 (0.098)	Data 0.000 (0.003)	Loss 1.7034 (1.7600)	Acc@1 33.984 (33.395)	Acc@5 88.672 (86.022)
Epoch: [169][128/196]	Time 0.096 (0.099)	Data 0.000 (0.002)	Loss 1.7142 (1.7632)	Acc@1 33.594 (33.170)	Acc@5 85.547 (86.040)
Epoch: [169][192/196]	Time 0.102 (0.098)	Data 0.000 (0.001)	Loss 1.8700 (1.7625)	Acc@1 30.859 (33.412)	Acc@5 86.328 (86.099)
after train
n1: 30 for:
wAcc: 34.3737326807593
test acc: 34.86
Epoche: [170/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [170][0/196]	Time 0.152 (0.152)	Data 0.237 (0.237)	Loss 1.7739 (1.7739)	Acc@1 37.500 (37.500)	Acc@5 84.375 (84.375)
Epoch: [170][64/196]	Time 0.099 (0.100)	Data 0.000 (0.004)	Loss 1.7597 (1.7625)	Acc@1 32.422 (33.588)	Acc@5 86.719 (86.046)
Epoch: [170][128/196]	Time 0.098 (0.099)	Data 0.000 (0.002)	Loss 1.7326 (1.7658)	Acc@1 35.156 (33.642)	Acc@5 87.500 (86.101)
Epoch: [170][192/196]	Time 0.100 (0.099)	Data 0.000 (0.001)	Loss 1.7285 (1.7627)	Acc@1 35.938 (33.630)	Acc@5 85.547 (86.203)
after train
n1: 30 for:
wAcc: 34.44991884861692
test acc: 35.29
Epoche: [171/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [171][0/196]	Time 0.148 (0.148)	Data 0.205 (0.205)	Loss 1.7381 (1.7381)	Acc@1 35.156 (35.156)	Acc@5 91.016 (91.016)
Epoch: [171][64/196]	Time 0.097 (0.100)	Data 0.000 (0.003)	Loss 1.7855 (1.7634)	Acc@1 37.109 (33.600)	Acc@5 85.156 (86.040)
Epoch: [171][128/196]	Time 0.097 (0.099)	Data 0.000 (0.002)	Loss 1.7745 (1.7636)	Acc@1 30.859 (33.612)	Acc@5 85.938 (86.022)
Epoch: [171][192/196]	Time 0.098 (0.099)	Data 0.000 (0.001)	Loss 1.7877 (1.7637)	Acc@1 35.938 (33.458)	Acc@5 86.328 (86.025)
after train
n1: 30 for:
wAcc: 34.51568255715656
test acc: 35.01
Epoche: [172/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [172][0/196]	Time 0.151 (0.151)	Data 0.258 (0.258)	Loss 1.6325 (1.6325)	Acc@1 39.844 (39.844)	Acc@5 87.891 (87.891)
Epoch: [172][64/196]	Time 0.095 (0.100)	Data 0.000 (0.004)	Loss 1.8165 (1.7637)	Acc@1 31.250 (33.564)	Acc@5 82.422 (85.835)
Epoch: [172][128/196]	Time 0.094 (0.100)	Data 0.000 (0.002)	Loss 1.8175 (1.7629)	Acc@1 35.547 (33.451)	Acc@5 81.250 (86.119)
Epoch: [172][192/196]	Time 0.095 (0.099)	Data 0.000 (0.002)	Loss 1.8097 (1.7637)	Acc@1 30.859 (33.533)	Acc@5 86.328 (86.122)
after train
n1: 30 for:
wAcc: 34.533117849357645
test acc: 35.11
Epoche: [173/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [173][0/196]	Time 0.154 (0.154)	Data 0.230 (0.230)	Loss 1.7234 (1.7234)	Acc@1 39.062 (39.062)	Acc@5 88.281 (88.281)
Epoch: [173][64/196]	Time 0.100 (0.100)	Data 0.000 (0.004)	Loss 1.7638 (1.7627)	Acc@1 36.328 (33.149)	Acc@5 85.156 (86.136)
Epoch: [173][128/196]	Time 0.100 (0.100)	Data 0.000 (0.002)	Loss 1.7929 (1.7637)	Acc@1 35.156 (33.306)	Acc@5 86.328 (85.953)
Epoch: [173][192/196]	Time 0.095 (0.099)	Data 0.000 (0.001)	Loss 1.7851 (1.7594)	Acc@1 32.422 (33.677)	Acc@5 85.547 (86.176)
after train
n1: 30 for:
wAcc: 34.59202028621133
test acc: 34.83
Epoche: [174/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [174][0/196]	Time 0.157 (0.157)	Data 0.240 (0.240)	Loss 1.7166 (1.7166)	Acc@1 37.109 (37.109)	Acc@5 87.891 (87.891)
Epoch: [174][64/196]	Time 0.096 (0.100)	Data 0.000 (0.004)	Loss 1.8299 (1.7655)	Acc@1 29.297 (33.546)	Acc@5 82.422 (86.070)
Epoch: [174][128/196]	Time 0.094 (0.099)	Data 0.000 (0.002)	Loss 1.7624 (1.7600)	Acc@1 33.203 (33.812)	Acc@5 88.281 (86.255)
Epoch: [174][192/196]	Time 0.100 (0.099)	Data 0.000 (0.001)	Loss 1.6647 (1.7595)	Acc@1 38.672 (33.827)	Acc@5 88.672 (86.172)
after train
n1: 30 for:
wAcc: 34.566896580105166
test acc: 35.05
Epoche: [175/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [175][0/196]	Time 0.161 (0.161)	Data 0.205 (0.205)	Loss 1.7142 (1.7142)	Acc@1 32.031 (32.031)	Acc@5 87.109 (87.109)
Epoch: [175][64/196]	Time 0.100 (0.100)	Data 0.000 (0.003)	Loss 1.7620 (1.7584)	Acc@1 35.547 (33.612)	Acc@5 84.766 (86.112)
Epoch: [175][128/196]	Time 0.103 (0.099)	Data 0.000 (0.002)	Loss 1.7552 (1.7602)	Acc@1 34.375 (33.521)	Acc@5 85.547 (85.931)
Epoch: [175][192/196]	Time 0.098 (0.099)	Data 0.000 (0.001)	Loss 1.7398 (1.7617)	Acc@1 35.547 (33.470)	Acc@5 85.938 (86.025)
after train
n1: 30 for:
wAcc: 34.60962946725846
test acc: 34.99
Epoche: [176/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [176][0/196]	Time 0.155 (0.155)	Data 0.216 (0.216)	Loss 1.7244 (1.7244)	Acc@1 37.109 (37.109)	Acc@5 86.328 (86.328)
Epoch: [176][64/196]	Time 0.094 (0.099)	Data 0.000 (0.004)	Loss 1.7818 (1.7608)	Acc@1 39.062 (33.347)	Acc@5 81.250 (85.980)
Epoch: [176][128/196]	Time 0.096 (0.099)	Data 0.000 (0.002)	Loss 1.7914 (1.7583)	Acc@1 30.078 (33.682)	Acc@5 87.891 (86.295)
Epoch: [176][192/196]	Time 0.097 (0.098)	Data 0.000 (0.001)	Loss 1.7599 (1.7583)	Acc@1 33.594 (33.527)	Acc@5 86.719 (86.217)
after train
n1: 30 for:
wAcc: 34.691994124526055
test acc: 35.24
Epoche: [177/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [177][0/196]	Time 0.154 (0.154)	Data 0.236 (0.236)	Loss 1.7366 (1.7366)	Acc@1 31.641 (31.641)	Acc@5 83.594 (83.594)
Epoch: [177][64/196]	Time 0.096 (0.099)	Data 0.000 (0.004)	Loss 1.7536 (1.7503)	Acc@1 31.641 (33.846)	Acc@5 86.328 (86.623)
Epoch: [177][128/196]	Time 0.098 (0.099)	Data 0.000 (0.002)	Loss 1.6905 (1.7585)	Acc@1 38.281 (33.682)	Acc@5 88.281 (86.216)
Epoch: [177][192/196]	Time 0.093 (0.099)	Data 0.000 (0.001)	Loss 1.8068 (1.7623)	Acc@1 31.641 (33.448)	Acc@5 88.672 (86.059)
after train
n1: 30 for:
wAcc: 34.76059850046444
test acc: 34.94
Epoche: [178/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [178][0/196]	Time 0.158 (0.158)	Data 0.236 (0.236)	Loss 1.6375 (1.6375)	Acc@1 37.500 (37.500)	Acc@5 90.625 (90.625)
Epoch: [178][64/196]	Time 0.106 (0.100)	Data 0.000 (0.004)	Loss 1.8740 (1.7610)	Acc@1 33.203 (34.129)	Acc@5 85.547 (86.034)
Epoch: [178][128/196]	Time 0.095 (0.099)	Data 0.000 (0.002)	Loss 1.7746 (1.7622)	Acc@1 29.297 (33.718)	Acc@5 81.250 (86.168)
Epoch: [178][192/196]	Time 0.096 (0.099)	Data 0.000 (0.001)	Loss 1.8141 (1.7587)	Acc@1 30.469 (33.760)	Acc@5 84.766 (86.223)
after train
n1: 30 for:
wAcc: 34.87914834311681
test acc: 35.2
Epoche: [179/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [179][0/196]	Time 0.162 (0.162)	Data 0.234 (0.234)	Loss 1.7012 (1.7012)	Acc@1 39.453 (39.453)	Acc@5 87.500 (87.500)
Epoch: [179][64/196]	Time 0.098 (0.100)	Data 0.000 (0.004)	Loss 1.6988 (1.7510)	Acc@1 33.594 (33.990)	Acc@5 87.109 (86.382)
Epoch: [179][128/196]	Time 0.100 (0.099)	Data 0.000 (0.002)	Loss 1.7763 (1.7576)	Acc@1 31.641 (33.830)	Acc@5 85.156 (86.316)
Epoch: [179][192/196]	Time 0.095 (0.099)	Data 0.000 (0.001)	Loss 1.7493 (1.7584)	Acc@1 32.031 (33.855)	Acc@5 84.375 (86.180)
after train
n1: 30 for:
wAcc: 34.84346944268777
test acc: 35.04
Epoche: [180/180]; Lr: 0.0010000000000000002
batch Size 256
befor train
Epoch: [180][0/196]	Time 0.119 (0.119)	Data 0.234 (0.234)	Loss 1.7452 (1.7452)	Acc@1 33.594 (33.594)	Acc@5 89.844 (89.844)
Epoch: [180][64/196]	Time 0.096 (0.098)	Data 0.000 (0.004)	Loss 1.7145 (1.7547)	Acc@1 32.812 (34.393)	Acc@5 87.891 (86.214)
Epoch: [180][128/196]	Time 0.091 (0.098)	Data 0.000 (0.002)	Loss 1.7948 (1.7610)	Acc@1 29.688 (33.736)	Acc@5 83.594 (86.065)
Epoch: [180][192/196]	Time 0.096 (0.098)	Data 0.000 (0.001)	Loss 1.7702 (1.7593)	Acc@1 34.766 (33.808)	Acc@5 83.203 (86.180)
after train
n1: 30 for:
wAcc: 34.91686468752414
test acc: 34.93
Max memory: 101.188096
 19.585s  