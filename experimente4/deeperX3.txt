no display found. Using non-interactive Agg backend
[5, 5, 5]
[16, 32, 64]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 270; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/deeperX3/model.nn; checkpoint: ./output/experimente4/deeper; saveModell: True; LR: 0.1
random number: 1195
Files already downloaded and verified

width: 16
module: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (3, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (4, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (5, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (6, 0
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (7, 0
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=64, out_features=10, bias=True)
width: 32
module: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 16
conv gefunden
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
conv gefunden
(i,j): (9, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (10, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (11, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (12, 0
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (13, 0
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=64, out_features=10, bias=True)
width: 64
module: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
module: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
module: ReLU(inplace=True)
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 16
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
)
width Module: 32
conv gefunden
module: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 32
conv gefunden
(i,j): (15, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (16, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (17, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (18, 0
module: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
width Module: 64
conv gefunden
(i,j): (19, 0
module: AdaptiveAvgPool2d(output_size=(1, 1))
module: Linear(in_features=64, out_features=10, bias=True)
stagesI: {16: [(3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0)], 32: [(10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 0)], 64: [(16, 0), (17, 0), (18, 0), (19, 0), (21, None)]}
stagesO: {16: [(0, None), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0)], 32: [(8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0)], 64: [(14, 0), (15, 0), (16, 0), (17, 0), (18, 0), (19, 0)]}
Modell Erstellung
N2N(
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (15): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (16): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (15): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (16): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
deeper epoch: 90
Epoche: [1/270]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.
  warnings.warn("Setting attributes on ParameterList is not supported.")
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [1][0/196]	Time 0.142 (0.142)	Data 0.322 (0.322)	Loss 2.5610 (2.5610)	Acc@1 7.812 (7.812)	Acc@5 46.484 (46.484)
Epoch: [1][64/196]	Time 0.065 (0.073)	Data 0.000 (0.005)	Loss 1.7804 (1.9688)	Acc@1 35.938 (26.376)	Acc@5 88.281 (79.483)
Epoch: [1][128/196]	Time 0.069 (0.075)	Data 0.000 (0.003)	Loss 1.5101 (1.8069)	Acc@1 44.531 (32.168)	Acc@5 91.406 (84.490)
Epoch: [1][192/196]	Time 0.069 (0.075)	Data 0.000 (0.002)	Loss 1.4032 (1.7000)	Acc@1 48.438 (36.435)	Acc@5 92.578 (86.990)
after train
n1: 1 for:
wAcc: 41.12
test acc: 41.12
Epoche: [2/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [2][0/196]	Time 0.111 (0.111)	Data 0.420 (0.420)	Loss 1.4860 (1.4860)	Acc@1 48.047 (48.047)	Acc@5 91.406 (91.406)
Epoch: [2][64/196]	Time 0.069 (0.082)	Data 0.000 (0.007)	Loss 1.1509 (1.3539)	Acc@1 57.422 (50.751)	Acc@5 94.531 (93.618)
Epoch: [2][128/196]	Time 0.092 (0.080)	Data 0.000 (0.004)	Loss 1.1876 (1.2947)	Acc@1 58.984 (53.004)	Acc@5 94.922 (94.165)
Epoch: [2][192/196]	Time 0.095 (0.085)	Data 0.000 (0.002)	Loss 1.0277 (1.2414)	Acc@1 63.281 (55.052)	Acc@5 95.312 (94.764)
after train
n1: 2 for:
wAcc: 41.12
test acc: 56.42
Epoche: [3/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [3][0/196]	Time 0.116 (0.116)	Data 0.237 (0.237)	Loss 0.9831 (0.9831)	Acc@1 63.672 (63.672)	Acc@5 97.266 (97.266)
Epoch: [3][64/196]	Time 0.086 (0.081)	Data 0.000 (0.004)	Loss 1.0738 (1.0620)	Acc@1 60.938 (61.460)	Acc@5 94.141 (96.454)
Epoch: [3][128/196]	Time 0.087 (0.084)	Data 0.000 (0.002)	Loss 1.0818 (1.0282)	Acc@1 60.156 (62.839)	Acc@5 95.703 (96.642)
Epoch: [3][192/196]	Time 0.082 (0.082)	Data 0.000 (0.001)	Loss 0.9787 (1.0023)	Acc@1 62.109 (63.913)	Acc@5 97.266 (96.826)
after train
n1: 3 for:
wAcc: 48.769999999999996
test acc: 53.84
Epoche: [4/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [4][0/196]	Time 0.093 (0.093)	Data 0.260 (0.260)	Loss 0.9239 (0.9239)	Acc@1 68.359 (68.359)	Acc@5 97.266 (97.266)
Epoch: [4][64/196]	Time 0.082 (0.074)	Data 0.000 (0.004)	Loss 0.9507 (0.8959)	Acc@1 65.625 (68.269)	Acc@5 97.266 (97.362)
Epoch: [4][128/196]	Time 0.070 (0.074)	Data 0.000 (0.002)	Loss 0.7891 (0.8786)	Acc@1 72.656 (69.004)	Acc@5 98.828 (97.472)
Epoch: [4][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.6972 (0.8603)	Acc@1 76.953 (69.790)	Acc@5 98.828 (97.642)
after train
n1: 4 for:
wAcc: 49.879999999999995
test acc: 65.99
Epoche: [5/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [5][0/196]	Time 0.118 (0.118)	Data 0.292 (0.292)	Loss 0.8168 (0.8168)	Acc@1 71.094 (71.094)	Acc@5 97.656 (97.656)
Epoch: [5][64/196]	Time 0.068 (0.074)	Data 0.000 (0.005)	Loss 0.7454 (0.7804)	Acc@1 74.219 (72.157)	Acc@5 98.828 (98.107)
Epoch: [5][128/196]	Time 0.070 (0.072)	Data 0.000 (0.003)	Loss 0.7750 (0.7776)	Acc@1 72.266 (72.529)	Acc@5 98.828 (98.086)
Epoch: [5][192/196]	Time 0.084 (0.074)	Data 0.000 (0.002)	Loss 0.8054 (0.7648)	Acc@1 72.656 (73.061)	Acc@5 97.656 (98.172)
after train
n1: 5 for:
wAcc: 54.50333333333334
test acc: 57.32
Epoche: [6/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [6][0/196]	Time 0.111 (0.111)	Data 0.406 (0.406)	Loss 0.7006 (0.7006)	Acc@1 76.562 (76.562)	Acc@5 99.609 (99.609)
Epoch: [6][64/196]	Time 0.070 (0.073)	Data 0.000 (0.006)	Loss 0.7110 (0.7139)	Acc@1 73.828 (75.331)	Acc@5 98.438 (98.359)
Epoch: [6][128/196]	Time 0.068 (0.071)	Data 0.000 (0.003)	Loss 0.6470 (0.7128)	Acc@1 76.953 (75.136)	Acc@5 98.047 (98.359)
Epoch: [6][192/196]	Time 0.068 (0.071)	Data 0.000 (0.002)	Loss 0.7285 (0.7077)	Acc@1 75.781 (75.348)	Acc@5 98.438 (98.375)
after train
n1: 6 for:
wAcc: 54.27139525197834
test acc: 72.6
Epoche: [7/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [7][0/196]	Time 0.092 (0.092)	Data 0.284 (0.284)	Loss 0.7050 (0.7050)	Acc@1 75.781 (75.781)	Acc@5 98.828 (98.828)
Epoch: [7][64/196]	Time 0.070 (0.072)	Data 0.000 (0.005)	Loss 0.7704 (0.6629)	Acc@1 74.609 (77.458)	Acc@5 98.438 (98.660)
Epoch: [7][128/196]	Time 0.089 (0.073)	Data 0.000 (0.002)	Loss 0.5603 (0.6621)	Acc@1 83.203 (77.150)	Acc@5 99.219 (98.680)
Epoch: [7][192/196]	Time 0.068 (0.074)	Data 0.000 (0.002)	Loss 0.6068 (0.6649)	Acc@1 76.562 (77.038)	Acc@5 99.219 (98.654)
after train
n1: 7 for:
wAcc: 58.07666015624999
test acc: 67.17
Epoche: [8/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [8][0/196]	Time 0.102 (0.102)	Data 0.340 (0.340)	Loss 0.6196 (0.6196)	Acc@1 78.516 (78.516)	Acc@5 99.219 (99.219)
Epoch: [8][64/196]	Time 0.086 (0.086)	Data 0.000 (0.005)	Loss 0.6538 (0.6254)	Acc@1 77.734 (78.293)	Acc@5 98.438 (98.816)
Epoch: [8][128/196]	Time 0.085 (0.086)	Data 0.000 (0.003)	Loss 0.5826 (0.6311)	Acc@1 81.250 (78.155)	Acc@5 99.219 (98.798)
Epoch: [8][192/196]	Time 0.068 (0.082)	Data 0.000 (0.002)	Loss 0.6632 (0.6324)	Acc@1 78.906 (78.172)	Acc@5 98.828 (98.800)
after train
n1: 8 for:
wAcc: 59.13014750461481
test acc: 72.81
Epoche: [9/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [9][0/196]	Time 0.113 (0.113)	Data 0.317 (0.317)	Loss 0.6348 (0.6348)	Acc@1 76.953 (76.953)	Acc@5 97.656 (97.656)
Epoch: [9][64/196]	Time 0.071 (0.072)	Data 0.000 (0.005)	Loss 0.6495 (0.6067)	Acc@1 77.734 (79.062)	Acc@5 98.047 (98.798)
Epoch: [9][128/196]	Time 0.094 (0.075)	Data 0.000 (0.003)	Loss 0.6577 (0.6079)	Acc@1 77.734 (78.706)	Acc@5 98.438 (98.804)
Epoch: [9][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.5980 (0.6046)	Acc@1 79.688 (78.902)	Acc@5 98.438 (98.822)
after train
n1: 9 for:
wAcc: 60.98744896000002
test acc: 72.32
Epoche: [10/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [10][0/196]	Time 0.087 (0.087)	Data 0.289 (0.289)	Loss 0.6258 (0.6258)	Acc@1 76.562 (76.562)	Acc@5 99.609 (99.609)
Epoch: [10][64/196]	Time 0.069 (0.076)	Data 0.000 (0.005)	Loss 0.5995 (0.5787)	Acc@1 78.906 (80.000)	Acc@5 99.609 (98.960)
Epoch: [10][128/196]	Time 0.068 (0.073)	Data 0.000 (0.002)	Loss 0.6542 (0.5847)	Acc@1 80.469 (79.812)	Acc@5 98.047 (98.934)
Epoch: [10][192/196]	Time 0.095 (0.076)	Data 0.000 (0.002)	Loss 0.5968 (0.5872)	Acc@1 79.297 (79.787)	Acc@5 99.609 (98.903)
after train
n1: 10 for:
wAcc: 62.16685438370057
test acc: 72.29
Epoche: [11/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [11][0/196]	Time 0.117 (0.117)	Data 0.295 (0.295)	Loss 0.5182 (0.5182)	Acc@1 81.641 (81.641)	Acc@5 99.219 (99.219)
Epoch: [11][64/196]	Time 0.070 (0.076)	Data 0.000 (0.005)	Loss 0.6037 (0.5672)	Acc@1 78.516 (80.024)	Acc@5 98.438 (98.924)
Epoch: [11][128/196]	Time 0.070 (0.073)	Data 0.000 (0.003)	Loss 0.5502 (0.5683)	Acc@1 77.734 (80.214)	Acc@5 100.000 (98.995)
Epoch: [11][192/196]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.5132 (0.5689)	Acc@1 83.203 (80.287)	Acc@5 100.000 (99.004)
after train
n1: 11 for:
wAcc: 63.01684033136146
test acc: 76.24
Epoche: [12/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [12][0/196]	Time 0.110 (0.110)	Data 0.281 (0.281)	Loss 0.5599 (0.5599)	Acc@1 78.516 (78.516)	Acc@5 99.219 (99.219)
Epoch: [12][64/196]	Time 0.085 (0.079)	Data 0.000 (0.005)	Loss 0.5458 (0.5451)	Acc@1 82.422 (81.310)	Acc@5 98.828 (98.960)
Epoch: [12][128/196]	Time 0.071 (0.079)	Data 0.000 (0.002)	Loss 0.5639 (0.5477)	Acc@1 81.641 (81.232)	Acc@5 99.219 (99.040)
Epoch: [12][192/196]	Time 0.092 (0.078)	Data 0.000 (0.002)	Loss 0.5749 (0.5496)	Acc@1 78.516 (81.189)	Acc@5 99.219 (99.039)
after train
n1: 12 for:
wAcc: 64.26680161730543
test acc: 73.54
Epoche: [13/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [13][0/196]	Time 0.131 (0.131)	Data 0.293 (0.293)	Loss 0.5944 (0.5944)	Acc@1 78.906 (78.906)	Acc@5 99.219 (99.219)
Epoch: [13][64/196]	Time 0.069 (0.072)	Data 0.000 (0.005)	Loss 0.4868 (0.5517)	Acc@1 82.812 (81.088)	Acc@5 99.609 (99.008)
Epoch: [13][128/196]	Time 0.069 (0.071)	Data 0.000 (0.003)	Loss 0.5849 (0.5442)	Acc@1 78.906 (81.311)	Acc@5 100.000 (99.131)
Epoch: [13][192/196]	Time 0.068 (0.070)	Data 0.000 (0.002)	Loss 0.3762 (0.5394)	Acc@1 87.109 (81.521)	Acc@5 98.828 (99.132)
after train
n1: 13 for:
wAcc: 64.82248354745488
test acc: 74.34
Epoche: [14/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [14][0/196]	Time 0.085 (0.085)	Data 0.356 (0.356)	Loss 0.5359 (0.5359)	Acc@1 78.516 (78.516)	Acc@5 100.000 (100.000)
Epoch: [14][64/196]	Time 0.068 (0.075)	Data 0.000 (0.006)	Loss 0.5775 (0.5131)	Acc@1 79.688 (82.254)	Acc@5 98.828 (99.129)
Epoch: [14][128/196]	Time 0.071 (0.074)	Data 0.000 (0.003)	Loss 0.4299 (0.5171)	Acc@1 84.375 (82.270)	Acc@5 99.609 (99.095)
Epoch: [14][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.5733 (0.5242)	Acc@1 80.469 (81.952)	Acc@5 99.219 (99.071)
after train
n1: 14 for:
wAcc: 65.37479809865248
test acc: 72.91
Epoche: [15/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [15][0/196]	Time 0.107 (0.107)	Data 0.271 (0.271)	Loss 0.5622 (0.5622)	Acc@1 77.344 (77.344)	Acc@5 99.609 (99.609)
Epoch: [15][64/196]	Time 0.096 (0.091)	Data 0.000 (0.004)	Loss 0.4897 (0.5379)	Acc@1 84.766 (81.052)	Acc@5 98.438 (99.020)
Epoch: [15][128/196]	Time 0.072 (0.089)	Data 0.000 (0.002)	Loss 0.4029 (0.5260)	Acc@1 83.984 (81.668)	Acc@5 99.219 (99.076)
Epoch: [15][192/196]	Time 0.069 (0.083)	Data 0.000 (0.002)	Loss 0.5689 (0.5232)	Acc@1 81.250 (81.841)	Acc@5 99.219 (99.083)
after train
n1: 15 for:
wAcc: 65.64158509396587
test acc: 74.46
Epoche: [16/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [16][0/196]	Time 0.103 (0.103)	Data 0.284 (0.284)	Loss 0.4210 (0.4210)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [16][64/196]	Time 0.071 (0.071)	Data 0.000 (0.005)	Loss 0.5517 (0.4943)	Acc@1 80.469 (83.035)	Acc@5 98.438 (99.237)
Epoch: [16][128/196]	Time 0.071 (0.071)	Data 0.000 (0.002)	Loss 0.4820 (0.5030)	Acc@1 85.938 (82.607)	Acc@5 99.609 (99.185)
Epoch: [16][192/196]	Time 0.096 (0.077)	Data 0.000 (0.002)	Loss 0.4671 (0.5112)	Acc@1 81.641 (82.375)	Acc@5 100.000 (99.124)
after train
n1: 16 for:
wAcc: 66.05252980680035
test acc: 78.03
Epoche: [17/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [17][0/196]	Time 0.110 (0.110)	Data 0.282 (0.282)	Loss 0.4724 (0.4724)	Acc@1 83.594 (83.594)	Acc@5 98.828 (98.828)
Epoch: [17][64/196]	Time 0.070 (0.082)	Data 0.000 (0.005)	Loss 0.5318 (0.4970)	Acc@1 82.031 (82.957)	Acc@5 98.047 (99.201)
Epoch: [17][128/196]	Time 0.070 (0.076)	Data 0.000 (0.002)	Loss 0.4286 (0.4977)	Acc@1 87.109 (83.009)	Acc@5 99.609 (99.222)
Epoch: [17][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.5045 (0.5014)	Acc@1 82.422 (82.863)	Acc@5 98.828 (99.188)
after train
n1: 17 for:
wAcc: 66.78968092376716
test acc: 76.89
Epoche: [18/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [18][0/196]	Time 0.097 (0.097)	Data 0.258 (0.258)	Loss 0.4397 (0.4397)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [18][64/196]	Time 0.094 (0.085)	Data 0.000 (0.004)	Loss 0.4361 (0.5004)	Acc@1 85.938 (82.662)	Acc@5 98.828 (99.111)
Epoch: [18][128/196]	Time 0.096 (0.090)	Data 0.000 (0.002)	Loss 0.4884 (0.5019)	Acc@1 80.859 (82.613)	Acc@5 99.219 (99.125)
Epoch: [18][192/196]	Time 0.096 (0.092)	Data 0.000 (0.002)	Loss 0.4327 (0.4979)	Acc@1 87.891 (82.776)	Acc@5 99.219 (99.146)
after train
n1: 18 for:
wAcc: 67.27156062154029
test acc: 79.46
Epoche: [19/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [19][0/196]	Time 0.111 (0.111)	Data 0.316 (0.316)	Loss 0.4682 (0.4682)	Acc@1 82.422 (82.422)	Acc@5 98.828 (98.828)
Epoch: [19][64/196]	Time 0.068 (0.071)	Data 0.000 (0.005)	Loss 0.4840 (0.4814)	Acc@1 84.375 (83.504)	Acc@5 98.828 (99.153)
Epoch: [19][128/196]	Time 0.069 (0.071)	Data 0.000 (0.003)	Loss 0.4364 (0.4816)	Acc@1 85.156 (83.388)	Acc@5 99.609 (99.225)
Epoch: [19][192/196]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.4755 (0.4813)	Acc@1 81.250 (83.379)	Acc@5 99.609 (99.247)
after train
n1: 19 for:
wAcc: 67.93173479584645
test acc: 76.5
Epoche: [20/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [20][0/196]	Time 0.109 (0.109)	Data 0.309 (0.309)	Loss 0.5019 (0.5019)	Acc@1 82.422 (82.422)	Acc@5 99.219 (99.219)
Epoch: [20][64/196]	Time 0.069 (0.082)	Data 0.000 (0.005)	Loss 0.4595 (0.4767)	Acc@1 84.375 (83.642)	Acc@5 99.609 (99.285)
Epoch: [20][128/196]	Time 0.095 (0.082)	Data 0.000 (0.003)	Loss 0.4801 (0.4768)	Acc@1 83.984 (83.542)	Acc@5 99.219 (99.273)
Epoch: [20][192/196]	Time 0.094 (0.087)	Data 0.000 (0.002)	Loss 0.6192 (0.4802)	Acc@1 79.688 (83.519)	Acc@5 98.438 (99.231)
after train
n1: 20 for:
wAcc: 68.20097972949992
test acc: 70.44
Epoche: [21/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [21][0/196]	Time 0.110 (0.110)	Data 0.306 (0.306)	Loss 0.5736 (0.5736)	Acc@1 82.031 (82.031)	Acc@5 98.438 (98.438)
Epoch: [21][64/196]	Time 0.068 (0.074)	Data 0.000 (0.005)	Loss 0.4654 (0.4515)	Acc@1 83.594 (84.351)	Acc@5 98.828 (99.327)
Epoch: [21][128/196]	Time 0.084 (0.074)	Data 0.000 (0.003)	Loss 0.5837 (0.4702)	Acc@1 78.906 (83.697)	Acc@5 99.609 (99.279)
Epoch: [21][192/196]	Time 0.083 (0.077)	Data 0.000 (0.002)	Loss 0.5163 (0.4755)	Acc@1 81.641 (83.541)	Acc@5 99.609 (99.288)
after train
n1: 21 for:
wAcc: 67.88459257893436
test acc: 73.8
Epoche: [22/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [22][0/196]	Time 0.143 (0.143)	Data 0.302 (0.302)	Loss 0.4085 (0.4085)	Acc@1 86.328 (86.328)	Acc@5 99.219 (99.219)
Epoch: [22][64/196]	Time 0.082 (0.074)	Data 0.000 (0.005)	Loss 0.4870 (0.4568)	Acc@1 85.938 (84.423)	Acc@5 99.219 (99.453)
Epoch: [22][128/196]	Time 0.070 (0.076)	Data 0.000 (0.003)	Loss 0.5558 (0.4650)	Acc@1 80.859 (83.957)	Acc@5 99.219 (99.361)
Epoch: [22][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.5385 (0.4703)	Acc@1 82.422 (83.879)	Acc@5 99.219 (99.281)
after train
n1: 22 for:
wAcc: 67.92553378210847
test acc: 76.5
Epoche: [23/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [23][0/196]	Time 0.101 (0.101)	Data 0.345 (0.345)	Loss 0.4888 (0.4888)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [23][64/196]	Time 0.071 (0.078)	Data 0.000 (0.006)	Loss 0.3746 (0.4649)	Acc@1 86.719 (83.870)	Acc@5 99.609 (99.375)
Epoch: [23][128/196]	Time 0.082 (0.076)	Data 0.000 (0.003)	Loss 0.5057 (0.4629)	Acc@1 82.422 (83.912)	Acc@5 99.219 (99.410)
Epoch: [23][192/196]	Time 0.086 (0.079)	Data 0.000 (0.002)	Loss 0.3701 (0.4625)	Acc@1 88.672 (84.003)	Acc@5 99.609 (99.364)
after train
n1: 23 for:
wAcc: 68.19304561929852
test acc: 77.66
Epoche: [24/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [24][0/196]	Time 0.127 (0.127)	Data 0.279 (0.279)	Loss 0.4659 (0.4659)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [24][64/196]	Time 0.094 (0.089)	Data 0.000 (0.005)	Loss 0.5313 (0.4510)	Acc@1 82.812 (84.483)	Acc@5 98.828 (99.291)
Epoch: [24][128/196]	Time 0.071 (0.089)	Data 0.000 (0.002)	Loss 0.3825 (0.4549)	Acc@1 88.672 (84.408)	Acc@5 100.000 (99.291)
Epoch: [24][192/196]	Time 0.068 (0.084)	Data 0.000 (0.002)	Loss 0.4957 (0.4569)	Acc@1 85.156 (84.306)	Acc@5 98.828 (99.306)
after train
n1: 24 for:
wAcc: 68.51833517064705
test acc: 78.97
Epoche: [25/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [25][0/196]	Time 0.095 (0.095)	Data 0.244 (0.244)	Loss 0.4135 (0.4135)	Acc@1 86.328 (86.328)	Acc@5 99.219 (99.219)
Epoch: [25][64/196]	Time 0.069 (0.072)	Data 0.000 (0.004)	Loss 0.4604 (0.4536)	Acc@1 83.203 (84.399)	Acc@5 98.828 (99.369)
Epoch: [25][128/196]	Time 0.089 (0.071)	Data 0.000 (0.002)	Loss 0.5482 (0.4504)	Acc@1 79.297 (84.433)	Acc@5 99.219 (99.325)
Epoch: [25][192/196]	Time 0.096 (0.079)	Data 0.000 (0.002)	Loss 0.4202 (0.4515)	Acc@1 85.938 (84.464)	Acc@5 100.000 (99.322)
after train
n1: 25 for:
wAcc: 68.9017610837355
test acc: 73.21
Epoche: [26/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [26][0/196]	Time 0.103 (0.103)	Data 0.291 (0.291)	Loss 0.3979 (0.3979)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [26][64/196]	Time 0.073 (0.072)	Data 0.000 (0.005)	Loss 0.4643 (0.4516)	Acc@1 85.547 (84.471)	Acc@5 99.219 (99.369)
Epoch: [26][128/196]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 0.3584 (0.4505)	Acc@1 88.281 (84.439)	Acc@5 100.000 (99.385)
Epoch: [26][192/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.3959 (0.4518)	Acc@1 85.156 (84.472)	Acc@5 99.609 (99.381)
after train
n1: 26 for:
wAcc: 68.80893707885002
test acc: 76.68
Epoche: [27/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [27][0/196]	Time 0.092 (0.092)	Data 0.284 (0.284)	Loss 0.3899 (0.3899)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [27][64/196]	Time 0.070 (0.073)	Data 0.000 (0.005)	Loss 0.3993 (0.4420)	Acc@1 85.156 (84.675)	Acc@5 100.000 (99.297)
Epoch: [27][128/196]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.4457 (0.4489)	Acc@1 83.984 (84.581)	Acc@5 100.000 (99.334)
Epoch: [27][192/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.4512 (0.4509)	Acc@1 87.109 (84.498)	Acc@5 99.609 (99.324)
after train
n1: 27 for:
wAcc: 68.98302397885766
test acc: 77.95
Epoche: [28/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [28][0/196]	Time 0.144 (0.144)	Data 0.272 (0.272)	Loss 0.4784 (0.4784)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [28][64/196]	Time 0.071 (0.076)	Data 0.000 (0.004)	Loss 0.4592 (0.4270)	Acc@1 83.203 (85.054)	Acc@5 99.219 (99.441)
Epoch: [28][128/196]	Time 0.071 (0.073)	Data 0.000 (0.002)	Loss 0.4108 (0.4293)	Acc@1 86.328 (85.099)	Acc@5 99.609 (99.403)
Epoch: [28][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.4081 (0.4354)	Acc@1 85.938 (84.911)	Acc@5 98.438 (99.387)
after train
n1: 28 for:
wAcc: 69.22599890916311
test acc: 75.1
Epoche: [29/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [29][0/196]	Time 0.115 (0.115)	Data 0.271 (0.271)	Loss 0.4137 (0.4137)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [29][64/196]	Time 0.087 (0.082)	Data 0.000 (0.004)	Loss 0.5514 (0.4384)	Acc@1 81.250 (84.754)	Acc@5 97.656 (99.387)
Epoch: [29][128/196]	Time 0.076 (0.077)	Data 0.000 (0.002)	Loss 0.3949 (0.4363)	Acc@1 85.156 (84.747)	Acc@5 98.828 (99.388)
Epoch: [29][192/196]	Time 0.084 (0.078)	Data 0.000 (0.002)	Loss 0.4486 (0.4376)	Acc@1 85.938 (84.743)	Acc@5 100.000 (99.362)
after train
n1: 29 for:
wAcc: 69.25163534033867
test acc: 79.07
Epoche: [30/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [30][0/196]	Time 0.088 (0.088)	Data 0.361 (0.361)	Loss 0.3547 (0.3547)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [30][64/196]	Time 0.071 (0.074)	Data 0.000 (0.006)	Loss 0.3867 (0.4230)	Acc@1 86.719 (85.571)	Acc@5 100.000 (99.489)
Epoch: [30][128/196]	Time 0.072 (0.072)	Data 0.000 (0.003)	Loss 0.4578 (0.4291)	Acc@1 85.156 (85.296)	Acc@5 98.438 (99.440)
Epoch: [30][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.5522 (0.4345)	Acc@1 81.250 (85.138)	Acc@5 99.219 (99.423)
after train
n1: 30 for:
wAcc: 69.53446221345162
test acc: 76.67
Epoche: [31/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [31][0/196]	Time 0.104 (0.104)	Data 0.274 (0.274)	Loss 0.4528 (0.4528)	Acc@1 84.766 (84.766)	Acc@5 98.438 (98.438)
Epoch: [31][64/196]	Time 0.070 (0.073)	Data 0.000 (0.004)	Loss 0.4393 (0.4352)	Acc@1 83.984 (84.934)	Acc@5 99.609 (99.381)
Epoch: [31][128/196]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.4791 (0.4307)	Acc@1 82.812 (85.090)	Acc@5 99.219 (99.416)
Epoch: [31][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.4285 (0.4365)	Acc@1 85.938 (84.883)	Acc@5 99.219 (99.391)
after train
n1: 30 for:
wAcc: 72.20661131581986
test acc: 82.16
Epoche: [32/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [32][0/196]	Time 0.092 (0.092)	Data 0.359 (0.359)	Loss 0.4112 (0.4112)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [32][64/196]	Time 0.074 (0.086)	Data 0.000 (0.006)	Loss 0.4318 (0.4297)	Acc@1 86.719 (84.934)	Acc@5 99.219 (99.429)
Epoch: [32][128/196]	Time 0.068 (0.078)	Data 0.000 (0.003)	Loss 0.4098 (0.4309)	Acc@1 86.328 (85.132)	Acc@5 99.609 (99.410)
Epoch: [32][192/196]	Time 0.069 (0.076)	Data 0.000 (0.002)	Loss 0.4286 (0.4336)	Acc@1 85.938 (84.994)	Acc@5 99.219 (99.425)
after train
n1: 30 for:
wAcc: 72.47579660678987
test acc: 77.78
Epoche: [33/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [33][0/196]	Time 0.088 (0.088)	Data 0.365 (0.365)	Loss 0.3932 (0.3932)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [33][64/196]	Time 0.070 (0.072)	Data 0.000 (0.006)	Loss 0.4918 (0.4291)	Acc@1 82.812 (85.036)	Acc@5 99.219 (99.363)
Epoch: [33][128/196]	Time 0.084 (0.075)	Data 0.000 (0.003)	Loss 0.3947 (0.4335)	Acc@1 85.938 (85.041)	Acc@5 99.219 (99.307)
Epoch: [33][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.3320 (0.4329)	Acc@1 87.109 (85.013)	Acc@5 100.000 (99.387)
after train
n1: 30 for:
wAcc: 74.57442619782105
test acc: 80.56
Epoche: [34/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [34][0/196]	Time 0.114 (0.114)	Data 0.248 (0.248)	Loss 0.3502 (0.3502)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [34][64/196]	Time 0.086 (0.080)	Data 0.000 (0.004)	Loss 0.4537 (0.4350)	Acc@1 85.547 (85.234)	Acc@5 99.219 (99.381)
Epoch: [34][128/196]	Time 0.097 (0.088)	Data 0.000 (0.002)	Loss 0.4749 (0.4258)	Acc@1 83.203 (85.444)	Acc@5 99.219 (99.416)
Epoch: [34][192/196]	Time 0.069 (0.084)	Data 0.000 (0.002)	Loss 0.4057 (0.4291)	Acc@1 85.547 (85.318)	Acc@5 100.000 (99.399)
after train
n1: 30 for:
wAcc: 73.70724354827841
test acc: 79.14
Epoche: [35/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [35][0/196]	Time 0.116 (0.116)	Data 0.290 (0.290)	Loss 0.3227 (0.3227)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [35][64/196]	Time 0.069 (0.075)	Data 0.000 (0.005)	Loss 0.4394 (0.4312)	Acc@1 83.984 (85.012)	Acc@5 98.438 (99.393)
Epoch: [35][128/196]	Time 0.073 (0.072)	Data 0.000 (0.002)	Loss 0.4152 (0.4267)	Acc@1 86.719 (85.341)	Acc@5 99.609 (99.422)
Epoch: [35][192/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.5123 (0.4284)	Acc@1 83.203 (85.288)	Acc@5 98.438 (99.409)
after train
n1: 30 for:
wAcc: 76.26664455919035
test acc: 79.18
Epoche: [36/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [36][0/196]	Time 0.134 (0.134)	Data 0.290 (0.290)	Loss 0.4101 (0.4101)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [36][64/196]	Time 0.089 (0.073)	Data 0.000 (0.005)	Loss 0.2982 (0.4188)	Acc@1 90.234 (85.529)	Acc@5 100.000 (99.489)
Epoch: [36][128/196]	Time 0.070 (0.076)	Data 0.000 (0.003)	Loss 0.4577 (0.4160)	Acc@1 83.594 (85.568)	Acc@5 98.828 (99.431)
Epoch: [36][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.4142 (0.4207)	Acc@1 87.891 (85.494)	Acc@5 99.609 (99.447)
after train
n1: 30 for:
wAcc: 75.66963371889742
test acc: 72.9
Epoche: [37/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [37][0/196]	Time 0.101 (0.101)	Data 0.265 (0.265)	Loss 0.3881 (0.3881)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [37][64/196]	Time 0.069 (0.071)	Data 0.000 (0.004)	Loss 0.4046 (0.4149)	Acc@1 85.547 (85.583)	Acc@5 99.219 (99.375)
Epoch: [37][128/196]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 0.4762 (0.4246)	Acc@1 83.594 (85.241)	Acc@5 99.219 (99.376)
Epoch: [37][192/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.3884 (0.4180)	Acc@1 85.938 (85.547)	Acc@5 99.609 (99.425)
after train
n1: 30 for:
wAcc: 76.30627485536701
test acc: 81.0
Epoche: [38/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [38][0/196]	Time 0.099 (0.099)	Data 0.230 (0.230)	Loss 0.5062 (0.5062)	Acc@1 80.078 (80.078)	Acc@5 99.609 (99.609)
Epoch: [38][64/196]	Time 0.082 (0.078)	Data 0.000 (0.004)	Loss 0.4248 (0.4227)	Acc@1 86.328 (85.397)	Acc@5 99.219 (99.447)
Epoch: [38][128/196]	Time 0.086 (0.081)	Data 0.000 (0.002)	Loss 0.4919 (0.4187)	Acc@1 82.031 (85.653)	Acc@5 99.219 (99.470)
Epoch: [38][192/196]	Time 0.069 (0.079)	Data 0.000 (0.001)	Loss 0.3959 (0.4183)	Acc@1 87.891 (85.646)	Acc@5 98.438 (99.460)
after train
n1: 30 for:
wAcc: 76.53826066939108
test acc: 78.74
Epoche: [39/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [39][0/196]	Time 0.090 (0.090)	Data 0.348 (0.348)	Loss 0.3276 (0.3276)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [39][64/196]	Time 0.084 (0.075)	Data 0.000 (0.006)	Loss 0.3005 (0.4225)	Acc@1 91.016 (85.577)	Acc@5 100.000 (99.423)
Epoch: [39][128/196]	Time 0.087 (0.080)	Data 0.000 (0.003)	Loss 0.4414 (0.4196)	Acc@1 83.594 (85.423)	Acc@5 99.609 (99.446)
Epoch: [39][192/196]	Time 0.090 (0.080)	Data 0.000 (0.002)	Loss 0.3636 (0.4213)	Acc@1 85.938 (85.423)	Acc@5 100.000 (99.417)
after train
n1: 30 for:
wAcc: 76.67597152142277
test acc: 82.04
Epoche: [40/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [40][0/196]	Time 0.105 (0.105)	Data 0.259 (0.259)	Loss 0.4169 (0.4169)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [40][64/196]	Time 0.071 (0.074)	Data 0.000 (0.004)	Loss 0.3925 (0.4079)	Acc@1 86.328 (85.895)	Acc@5 100.000 (99.495)
Epoch: [40][128/196]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.4059 (0.4115)	Acc@1 87.109 (85.868)	Acc@5 99.219 (99.452)
Epoch: [40][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.3959 (0.4142)	Acc@1 85.938 (85.770)	Acc@5 100.000 (99.431)
after train
n1: 30 for:
wAcc: 77.59305602598891
test acc: 79.53
Epoche: [41/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [41][0/196]	Time 0.105 (0.105)	Data 0.295 (0.295)	Loss 0.4005 (0.4005)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [41][64/196]	Time 0.076 (0.071)	Data 0.000 (0.005)	Loss 0.3712 (0.3959)	Acc@1 87.891 (86.022)	Acc@5 100.000 (99.567)
Epoch: [41][128/196]	Time 0.068 (0.073)	Data 0.000 (0.003)	Loss 0.4940 (0.4149)	Acc@1 82.031 (85.511)	Acc@5 99.609 (99.455)
Epoch: [41][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.3959 (0.4141)	Acc@1 87.500 (85.577)	Acc@5 99.609 (99.454)
after train
n1: 30 for:
wAcc: 77.32770394878855
test acc: 75.49
Epoche: [42/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [42][0/196]	Time 0.120 (0.120)	Data 0.258 (0.258)	Loss 0.3764 (0.3764)	Acc@1 85.547 (85.547)	Acc@5 100.000 (100.000)
Epoch: [42][64/196]	Time 0.081 (0.074)	Data 0.000 (0.004)	Loss 0.4328 (0.4172)	Acc@1 82.812 (85.403)	Acc@5 98.438 (99.525)
Epoch: [42][128/196]	Time 0.086 (0.079)	Data 0.000 (0.002)	Loss 0.4077 (0.4134)	Acc@1 85.547 (85.674)	Acc@5 99.609 (99.455)
Epoch: [42][192/196]	Time 0.084 (0.081)	Data 0.000 (0.002)	Loss 0.3620 (0.4129)	Acc@1 87.500 (85.780)	Acc@5 99.609 (99.447)
after train
n1: 30 for:
wAcc: 77.32479164949977
test acc: 74.33
Epoche: [43/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [43][0/196]	Time 0.128 (0.128)	Data 0.292 (0.292)	Loss 0.4647 (0.4647)	Acc@1 84.375 (84.375)	Acc@5 98.828 (98.828)
Epoch: [43][64/196]	Time 0.071 (0.074)	Data 0.000 (0.005)	Loss 0.3937 (0.4086)	Acc@1 85.547 (85.968)	Acc@5 99.609 (99.525)
Epoch: [43][128/196]	Time 0.087 (0.079)	Data 0.000 (0.003)	Loss 0.4407 (0.4138)	Acc@1 84.375 (85.765)	Acc@5 100.000 (99.470)
Epoch: [43][192/196]	Time 0.086 (0.082)	Data 0.000 (0.002)	Loss 0.3832 (0.4142)	Acc@1 86.719 (85.741)	Acc@5 99.219 (99.466)
after train
n1: 30 for:
wAcc: 76.92485625815854
test acc: 77.07
Epoche: [44/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [44][0/196]	Time 0.127 (0.127)	Data 0.284 (0.284)	Loss 0.4617 (0.4617)	Acc@1 83.594 (83.594)	Acc@5 99.609 (99.609)
Epoch: [44][64/196]	Time 0.070 (0.077)	Data 0.000 (0.005)	Loss 0.4801 (0.4276)	Acc@1 85.547 (85.180)	Acc@5 98.828 (99.309)
Epoch: [44][128/196]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.3747 (0.4117)	Acc@1 86.719 (85.844)	Acc@5 99.609 (99.419)
Epoch: [44][192/196]	Time 0.068 (0.074)	Data 0.000 (0.002)	Loss 0.4050 (0.4091)	Acc@1 83.203 (85.877)	Acc@5 100.000 (99.458)
after train
n1: 30 for:
wAcc: 77.15829078426192
test acc: 80.32
Epoche: [45/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [45][0/196]	Time 0.125 (0.125)	Data 0.300 (0.300)	Loss 0.5164 (0.5164)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [45][64/196]	Time 0.095 (0.084)	Data 0.000 (0.005)	Loss 0.3712 (0.3970)	Acc@1 85.938 (86.058)	Acc@5 99.609 (99.501)
Epoch: [45][128/196]	Time 0.097 (0.090)	Data 0.000 (0.003)	Loss 0.3382 (0.3941)	Acc@1 90.234 (86.316)	Acc@5 100.000 (99.497)
Epoch: [45][192/196]	Time 0.069 (0.088)	Data 0.000 (0.002)	Loss 0.3840 (0.4002)	Acc@1 85.938 (86.095)	Acc@5 99.219 (99.480)
after train
n1: 30 for:
wAcc: 77.87835678334423
test acc: 79.17
Epoche: [46/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [46][0/196]	Time 0.104 (0.104)	Data 0.296 (0.296)	Loss 0.3535 (0.3535)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [46][64/196]	Time 0.070 (0.073)	Data 0.000 (0.005)	Loss 0.3704 (0.4127)	Acc@1 86.328 (85.889)	Acc@5 100.000 (99.459)
Epoch: [46][128/196]	Time 0.070 (0.072)	Data 0.000 (0.003)	Loss 0.2750 (0.4059)	Acc@1 92.188 (86.047)	Acc@5 99.609 (99.482)
Epoch: [46][192/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.3910 (0.4069)	Acc@1 87.500 (85.980)	Acc@5 99.609 (99.482)
after train
n1: 30 for:
wAcc: 77.7968884285167
test acc: 78.26
Epoche: [47/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [47][0/196]	Time 0.129 (0.129)	Data 0.293 (0.293)	Loss 0.4137 (0.4137)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [47][64/196]	Time 0.097 (0.097)	Data 0.000 (0.005)	Loss 0.5102 (0.4083)	Acc@1 81.641 (85.859)	Acc@5 99.609 (99.459)
Epoch: [47][128/196]	Time 0.070 (0.092)	Data 0.000 (0.003)	Loss 0.3866 (0.4072)	Acc@1 86.719 (85.907)	Acc@5 100.000 (99.485)
Epoch: [47][192/196]	Time 0.091 (0.087)	Data 0.000 (0.002)	Loss 0.3554 (0.4030)	Acc@1 87.109 (86.065)	Acc@5 100.000 (99.484)
after train
n1: 30 for:
wAcc: 78.19828979653317
test acc: 82.94
Epoche: [48/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [48][0/196]	Time 0.088 (0.088)	Data 0.305 (0.305)	Loss 0.3841 (0.3841)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [48][64/196]	Time 0.071 (0.075)	Data 0.000 (0.005)	Loss 0.2812 (0.3821)	Acc@1 91.406 (86.635)	Acc@5 100.000 (99.573)
Epoch: [48][128/196]	Time 0.071 (0.073)	Data 0.000 (0.003)	Loss 0.3579 (0.3919)	Acc@1 88.281 (86.364)	Acc@5 100.000 (99.528)
Epoch: [48][192/196]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.4034 (0.3998)	Acc@1 83.984 (86.103)	Acc@5 99.609 (99.510)
after train
n1: 30 for:
wAcc: 78.07630437441453
test acc: 78.03
Epoche: [49/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [49][0/196]	Time 0.088 (0.088)	Data 0.364 (0.364)	Loss 0.3705 (0.3705)	Acc@1 86.328 (86.328)	Acc@5 98.828 (98.828)
Epoch: [49][64/196]	Time 0.072 (0.075)	Data 0.000 (0.006)	Loss 0.3685 (0.3793)	Acc@1 89.844 (87.073)	Acc@5 98.828 (99.513)
Epoch: [49][128/196]	Time 0.070 (0.073)	Data 0.000 (0.003)	Loss 0.3191 (0.4064)	Acc@1 87.500 (85.968)	Acc@5 99.609 (99.419)
Epoch: [49][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.4507 (0.4067)	Acc@1 85.547 (85.992)	Acc@5 99.219 (99.439)
after train
n1: 30 for:
wAcc: 77.19727395852796
test acc: 79.06
Epoche: [50/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [50][0/196]	Time 0.091 (0.091)	Data 0.305 (0.305)	Loss 0.4248 (0.4248)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [50][64/196]	Time 0.070 (0.075)	Data 0.000 (0.005)	Loss 0.3677 (0.3769)	Acc@1 88.672 (86.875)	Acc@5 99.609 (99.537)
Epoch: [50][128/196]	Time 0.076 (0.076)	Data 0.000 (0.003)	Loss 0.4024 (0.3903)	Acc@1 87.109 (86.555)	Acc@5 98.047 (99.494)
Epoch: [50][192/196]	Time 0.083 (0.076)	Data 0.000 (0.002)	Loss 0.3249 (0.3960)	Acc@1 89.062 (86.454)	Acc@5 99.609 (99.472)
after train
n1: 30 for:
wAcc: 77.80317666450753
test acc: 82.76
Epoche: [51/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [51][0/196]	Time 0.112 (0.112)	Data 0.290 (0.290)	Loss 0.4109 (0.4109)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [51][64/196]	Time 0.088 (0.088)	Data 0.000 (0.005)	Loss 0.4674 (0.4081)	Acc@1 87.891 (85.913)	Acc@5 98.438 (99.417)
Epoch: [51][128/196]	Time 0.087 (0.087)	Data 0.000 (0.002)	Loss 0.4282 (0.3969)	Acc@1 85.156 (86.374)	Acc@5 99.609 (99.461)
Epoch: [51][192/196]	Time 0.069 (0.085)	Data 0.000 (0.002)	Loss 0.3661 (0.3994)	Acc@1 88.281 (86.217)	Acc@5 99.609 (99.478)
after train
n1: 30 for:
wAcc: 78.51328792296617
test acc: 78.45
Epoche: [52/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [52][0/196]	Time 0.107 (0.107)	Data 0.311 (0.311)	Loss 0.4180 (0.4180)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [52][64/196]	Time 0.085 (0.079)	Data 0.000 (0.005)	Loss 0.4031 (0.3887)	Acc@1 88.281 (86.556)	Acc@5 99.219 (99.549)
Epoch: [52][128/196]	Time 0.087 (0.083)	Data 0.000 (0.003)	Loss 0.4580 (0.3892)	Acc@1 83.984 (86.613)	Acc@5 98.828 (99.516)
Epoch: [52][192/196]	Time 0.085 (0.084)	Data 0.000 (0.002)	Loss 0.3325 (0.3923)	Acc@1 88.672 (86.508)	Acc@5 99.609 (99.494)
after train
n1: 30 for:
wAcc: 78.6768962375637
test acc: 69.07
Epoche: [53/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [53][0/196]	Time 0.093 (0.093)	Data 0.321 (0.321)	Loss 0.3185 (0.3185)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [53][64/196]	Time 0.071 (0.078)	Data 0.000 (0.005)	Loss 0.5120 (0.3867)	Acc@1 82.031 (86.358)	Acc@5 98.828 (99.543)
Epoch: [53][128/196]	Time 0.070 (0.075)	Data 0.000 (0.003)	Loss 0.3809 (0.4002)	Acc@1 85.938 (86.098)	Acc@5 99.609 (99.506)
Epoch: [53][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.4475 (0.4017)	Acc@1 83.984 (86.053)	Acc@5 99.219 (99.484)
after train
n1: 30 for:
wAcc: 78.24647212028974
test acc: 76.86
Epoche: [54/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [54][0/196]	Time 0.108 (0.108)	Data 0.288 (0.288)	Loss 0.3048 (0.3048)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [54][64/196]	Time 0.089 (0.074)	Data 0.000 (0.005)	Loss 0.3967 (0.3910)	Acc@1 87.500 (86.611)	Acc@5 100.000 (99.555)
Epoch: [54][128/196]	Time 0.096 (0.084)	Data 0.000 (0.003)	Loss 0.3584 (0.3949)	Acc@1 87.891 (86.367)	Acc@5 99.219 (99.482)
Epoch: [54][192/196]	Time 0.069 (0.086)	Data 0.000 (0.002)	Loss 0.3672 (0.3943)	Acc@1 87.109 (86.391)	Acc@5 98.828 (99.460)
after train
n1: 30 for:
wAcc: 77.32434773635828
test acc: 80.16
Epoche: [55/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [55][0/196]	Time 0.092 (0.092)	Data 0.283 (0.283)	Loss 0.3893 (0.3893)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [55][64/196]	Time 0.072 (0.073)	Data 0.000 (0.005)	Loss 0.3047 (0.3847)	Acc@1 91.016 (86.785)	Acc@5 98.828 (99.573)
Epoch: [55][128/196]	Time 0.089 (0.074)	Data 0.000 (0.002)	Loss 0.4456 (0.3953)	Acc@1 84.766 (86.449)	Acc@5 100.000 (99.512)
Epoch: [55][192/196]	Time 0.068 (0.076)	Data 0.000 (0.002)	Loss 0.4578 (0.3972)	Acc@1 85.156 (86.273)	Acc@5 99.609 (99.488)
after train
n1: 30 for:
wAcc: 78.00892164732295
test acc: 79.65
Epoche: [56/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [56][0/196]	Time 0.122 (0.122)	Data 0.253 (0.253)	Loss 0.3493 (0.3493)	Acc@1 89.062 (89.062)	Acc@5 99.609 (99.609)
Epoch: [56][64/196]	Time 0.089 (0.078)	Data 0.000 (0.004)	Loss 0.3553 (0.3878)	Acc@1 88.281 (86.701)	Acc@5 99.609 (99.531)
Epoch: [56][128/196]	Time 0.068 (0.080)	Data 0.000 (0.002)	Loss 0.4173 (0.3912)	Acc@1 87.500 (86.622)	Acc@5 99.609 (99.497)
Epoch: [56][192/196]	Time 0.069 (0.077)	Data 0.000 (0.002)	Loss 0.4147 (0.3928)	Acc@1 85.938 (86.518)	Acc@5 99.219 (99.480)
after train
n1: 30 for:
wAcc: 78.29839084777481
test acc: 80.78
Epoche: [57/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [57][0/196]	Time 0.086 (0.086)	Data 0.369 (0.369)	Loss 0.4019 (0.4019)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [57][64/196]	Time 0.069 (0.071)	Data 0.000 (0.006)	Loss 0.3328 (0.3823)	Acc@1 88.281 (86.785)	Acc@5 99.609 (99.525)
Epoch: [57][128/196]	Time 0.084 (0.071)	Data 0.000 (0.003)	Loss 0.3657 (0.3898)	Acc@1 85.547 (86.540)	Acc@5 99.219 (99.494)
Epoch: [57][192/196]	Time 0.085 (0.076)	Data 0.000 (0.002)	Loss 0.3731 (0.3902)	Acc@1 86.719 (86.531)	Acc@5 99.219 (99.520)
after train
n1: 30 for:
wAcc: 78.04649422590508
test acc: 81.01
Epoche: [58/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [58][0/196]	Time 0.108 (0.108)	Data 0.252 (0.252)	Loss 0.3970 (0.3970)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [58][64/196]	Time 0.070 (0.077)	Data 0.000 (0.004)	Loss 0.5066 (0.3972)	Acc@1 81.641 (86.076)	Acc@5 100.000 (99.483)
Epoch: [58][128/196]	Time 0.083 (0.076)	Data 0.000 (0.002)	Loss 0.3978 (0.3920)	Acc@1 86.719 (86.401)	Acc@5 99.219 (99.506)
Epoch: [58][192/196]	Time 0.085 (0.079)	Data 0.000 (0.002)	Loss 0.4037 (0.3893)	Acc@1 86.719 (86.454)	Acc@5 99.219 (99.506)
after train
n1: 30 for:
wAcc: 78.81159752906882
test acc: 79.95
Epoche: [59/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [59][0/196]	Time 0.109 (0.109)	Data 0.389 (0.389)	Loss 0.3239 (0.3239)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [59][64/196]	Time 0.070 (0.081)	Data 0.000 (0.006)	Loss 0.4673 (0.3877)	Acc@1 84.375 (86.623)	Acc@5 99.609 (99.537)
Epoch: [59][128/196]	Time 0.082 (0.077)	Data 0.000 (0.003)	Loss 0.3492 (0.3913)	Acc@1 89.062 (86.419)	Acc@5 99.219 (99.509)
Epoch: [59][192/196]	Time 0.070 (0.076)	Data 0.000 (0.002)	Loss 0.3610 (0.3908)	Acc@1 88.281 (86.454)	Acc@5 99.219 (99.482)
after train
n1: 30 for:
wAcc: 78.53809511239108
test acc: 76.66
Epoche: [60/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [60][0/196]	Time 0.089 (0.089)	Data 0.319 (0.319)	Loss 0.4198 (0.4198)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [60][64/196]	Time 0.084 (0.076)	Data 0.000 (0.005)	Loss 0.4271 (0.3956)	Acc@1 85.547 (86.262)	Acc@5 99.609 (99.447)
Epoch: [60][128/196]	Time 0.086 (0.081)	Data 0.000 (0.003)	Loss 0.4310 (0.3893)	Acc@1 85.156 (86.534)	Acc@5 99.219 (99.509)
Epoch: [60][192/196]	Time 0.095 (0.084)	Data 0.000 (0.002)	Loss 0.3707 (0.3919)	Acc@1 87.500 (86.512)	Acc@5 99.219 (99.484)
after train
n1: 30 for:
wAcc: 79.21057063504884
test acc: 80.55
Epoche: [61/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [61][0/196]	Time 0.094 (0.094)	Data 0.246 (0.246)	Loss 0.4084 (0.4084)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [61][64/196]	Time 0.071 (0.072)	Data 0.000 (0.004)	Loss 0.3602 (0.3801)	Acc@1 87.109 (86.550)	Acc@5 100.000 (99.525)
Epoch: [61][128/196]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 0.4158 (0.3817)	Acc@1 83.984 (86.667)	Acc@5 99.609 (99.531)
Epoch: [61][192/196]	Time 0.068 (0.071)	Data 0.000 (0.002)	Loss 0.3564 (0.3835)	Acc@1 88.672 (86.571)	Acc@5 99.609 (99.537)
after train
n1: 30 for:
wAcc: 78.66380581206363
test acc: 73.82
Epoche: [62/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [62][0/196]	Time 0.117 (0.117)	Data 0.296 (0.296)	Loss 0.4057 (0.4057)	Acc@1 86.328 (86.328)	Acc@5 99.219 (99.219)
Epoch: [62][64/196]	Time 0.071 (0.072)	Data 0.000 (0.005)	Loss 0.3460 (0.4052)	Acc@1 87.109 (86.172)	Acc@5 100.000 (99.417)
Epoch: [62][128/196]	Time 0.071 (0.071)	Data 0.000 (0.003)	Loss 0.4478 (0.3915)	Acc@1 85.938 (86.552)	Acc@5 98.438 (99.473)
Epoch: [62][192/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.4128 (0.3918)	Acc@1 84.766 (86.419)	Acc@5 99.219 (99.478)
after train
n1: 30 for:
wAcc: 78.75318334042065
test acc: 75.97
Epoche: [63/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [63][0/196]	Time 0.128 (0.128)	Data 0.275 (0.275)	Loss 0.5141 (0.5141)	Acc@1 80.469 (80.469)	Acc@5 98.828 (98.828)
Epoch: [63][64/196]	Time 0.070 (0.072)	Data 0.000 (0.004)	Loss 0.3896 (0.3834)	Acc@1 86.328 (86.815)	Acc@5 99.219 (99.513)
Epoch: [63][128/196]	Time 0.068 (0.071)	Data 0.000 (0.002)	Loss 0.4277 (0.3811)	Acc@1 83.984 (86.801)	Acc@5 99.609 (99.528)
Epoch: [63][192/196]	Time 0.092 (0.073)	Data 0.000 (0.002)	Loss 0.4388 (0.3858)	Acc@1 84.375 (86.648)	Acc@5 99.609 (99.514)
after train
n1: 30 for:
wAcc: 78.3683457136247
test acc: 75.8
Epoche: [64/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [64][0/196]	Time 0.123 (0.123)	Data 0.267 (0.267)	Loss 0.3871 (0.3871)	Acc@1 87.891 (87.891)	Acc@5 99.219 (99.219)
Epoch: [64][64/196]	Time 0.071 (0.072)	Data 0.000 (0.004)	Loss 0.3559 (0.3751)	Acc@1 88.672 (87.212)	Acc@5 99.609 (99.549)
Epoch: [64][128/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.3830 (0.3811)	Acc@1 90.234 (87.012)	Acc@5 99.219 (99.561)
Epoch: [64][192/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.3950 (0.3863)	Acc@1 87.500 (86.840)	Acc@5 99.609 (99.506)
after train
n1: 30 for:
wAcc: 78.20842845245478
test acc: 67.45
Epoche: [65/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [65][0/196]	Time 0.102 (0.102)	Data 0.294 (0.294)	Loss 0.3199 (0.3199)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [65][64/196]	Time 0.070 (0.073)	Data 0.000 (0.005)	Loss 0.3605 (0.3584)	Acc@1 86.328 (87.434)	Acc@5 99.609 (99.657)
Epoch: [65][128/196]	Time 0.071 (0.071)	Data 0.000 (0.003)	Loss 0.3513 (0.3762)	Acc@1 87.109 (86.852)	Acc@5 99.609 (99.555)
Epoch: [65][192/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.4007 (0.3836)	Acc@1 85.938 (86.670)	Acc@5 99.609 (99.537)
after train
n1: 30 for:
wAcc: 76.60648971474632
test acc: 80.18
Epoche: [66/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [66][0/196]	Time 0.120 (0.120)	Data 0.249 (0.249)	Loss 0.3309 (0.3309)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [66][64/196]	Time 0.071 (0.072)	Data 0.000 (0.004)	Loss 0.4403 (0.3781)	Acc@1 83.203 (87.206)	Acc@5 99.219 (99.573)
Epoch: [66][128/196]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.3906 (0.3809)	Acc@1 86.328 (86.928)	Acc@5 100.000 (99.591)
Epoch: [66][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.3798 (0.3807)	Acc@1 87.500 (86.889)	Acc@5 98.438 (99.577)
after train
n1: 30 for:
wAcc: 78.00798737907554
test acc: 76.67
Epoche: [67/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [67][0/196]	Time 0.090 (0.090)	Data 0.330 (0.330)	Loss 0.3576 (0.3576)	Acc@1 89.453 (89.453)	Acc@5 100.000 (100.000)
Epoch: [67][64/196]	Time 0.072 (0.076)	Data 0.000 (0.005)	Loss 0.3899 (0.3687)	Acc@1 85.156 (87.464)	Acc@5 99.609 (99.501)
Epoch: [67][128/196]	Time 0.094 (0.078)	Data 0.000 (0.003)	Loss 0.3700 (0.3761)	Acc@1 88.672 (87.246)	Acc@5 98.438 (99.461)
Epoch: [67][192/196]	Time 0.069 (0.076)	Data 0.000 (0.002)	Loss 0.3886 (0.3778)	Acc@1 86.328 (87.109)	Acc@5 100.000 (99.500)
after train
n1: 30 for:
wAcc: 77.59495649331456
test acc: 77.14
Epoche: [68/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [68][0/196]	Time 0.109 (0.109)	Data 0.282 (0.282)	Loss 0.3127 (0.3127)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [68][64/196]	Time 0.071 (0.073)	Data 0.000 (0.005)	Loss 0.2856 (0.3689)	Acc@1 90.625 (87.296)	Acc@5 99.609 (99.627)
Epoch: [68][128/196]	Time 0.091 (0.075)	Data 0.000 (0.002)	Loss 0.3481 (0.3656)	Acc@1 86.328 (87.433)	Acc@5 99.609 (99.582)
Epoch: [68][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.4003 (0.3735)	Acc@1 84.766 (87.172)	Acc@5 99.609 (99.555)
after train
n1: 30 for:
wAcc: 78.04265760038948
test acc: 79.48
Epoche: [69/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [69][0/196]	Time 0.100 (0.100)	Data 0.366 (0.366)	Loss 0.4557 (0.4557)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [69][64/196]	Time 0.071 (0.072)	Data 0.000 (0.006)	Loss 0.3684 (0.3746)	Acc@1 88.281 (86.989)	Acc@5 99.219 (99.585)
Epoch: [69][128/196]	Time 0.071 (0.072)	Data 0.000 (0.003)	Loss 0.4691 (0.3733)	Acc@1 85.547 (87.070)	Acc@5 98.438 (99.576)
Epoch: [69][192/196]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 0.4549 (0.3808)	Acc@1 81.641 (86.761)	Acc@5 100.000 (99.551)
after train
n1: 30 for:
wAcc: 77.77253985942654
test acc: 76.32
Epoche: [70/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [70][0/196]	Time 0.125 (0.125)	Data 0.289 (0.289)	Loss 0.4510 (0.4510)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [70][64/196]	Time 0.071 (0.076)	Data 0.000 (0.005)	Loss 0.3511 (0.3659)	Acc@1 88.281 (87.476)	Acc@5 100.000 (99.597)
Epoch: [70][128/196]	Time 0.071 (0.073)	Data 0.000 (0.002)	Loss 0.3830 (0.3700)	Acc@1 88.281 (87.373)	Acc@5 100.000 (99.591)
Epoch: [70][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.3544 (0.3798)	Acc@1 89.453 (87.026)	Acc@5 98.828 (99.514)
after train
n1: 30 for:
wAcc: 77.09479891916989
test acc: 80.75
Epoche: [71/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [71][0/196]	Time 0.134 (0.134)	Data 0.260 (0.260)	Loss 0.4180 (0.4180)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [71][64/196]	Time 0.070 (0.074)	Data 0.000 (0.004)	Loss 0.3265 (0.3628)	Acc@1 87.891 (87.764)	Acc@5 100.000 (99.483)
Epoch: [71][128/196]	Time 0.086 (0.078)	Data 0.000 (0.002)	Loss 0.3655 (0.3731)	Acc@1 88.672 (87.391)	Acc@5 100.000 (99.522)
Epoch: [71][192/196]	Time 0.069 (0.079)	Data 0.000 (0.002)	Loss 0.4082 (0.3769)	Acc@1 85.547 (87.182)	Acc@5 99.609 (99.504)
after train
n1: 30 for:
wAcc: 77.1629269373378
test acc: 76.55
Epoche: [72/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [72][0/196]	Time 0.115 (0.115)	Data 0.405 (0.405)	Loss 0.3786 (0.3786)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [72][64/196]	Time 0.081 (0.074)	Data 0.000 (0.006)	Loss 0.3580 (0.3698)	Acc@1 85.938 (87.043)	Acc@5 99.609 (99.519)
Epoch: [72][128/196]	Time 0.082 (0.077)	Data 0.000 (0.003)	Loss 0.3507 (0.3665)	Acc@1 86.719 (87.234)	Acc@5 100.000 (99.528)
Epoch: [72][192/196]	Time 0.085 (0.080)	Data 0.000 (0.002)	Loss 0.3826 (0.3774)	Acc@1 85.156 (86.903)	Acc@5 99.609 (99.520)
after train
n1: 30 for:
wAcc: 77.51948193080676
test acc: 80.38
Epoche: [73/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [73][0/196]	Time 0.117 (0.117)	Data 0.270 (0.270)	Loss 0.3339 (0.3339)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [73][64/196]	Time 0.085 (0.087)	Data 0.000 (0.004)	Loss 0.4645 (0.3699)	Acc@1 84.766 (87.422)	Acc@5 99.609 (99.531)
Epoch: [73][128/196]	Time 0.085 (0.086)	Data 0.000 (0.002)	Loss 0.3615 (0.3808)	Acc@1 87.500 (86.840)	Acc@5 100.000 (99.488)
Epoch: [73][192/196]	Time 0.085 (0.086)	Data 0.000 (0.002)	Loss 0.4128 (0.3766)	Acc@1 86.328 (86.984)	Acc@5 99.219 (99.508)
after train
n1: 30 for:
wAcc: 78.17385654469743
test acc: 83.17
Epoche: [74/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [74][0/196]	Time 0.115 (0.115)	Data 0.283 (0.283)	Loss 0.3063 (0.3063)	Acc@1 90.234 (90.234)	Acc@5 100.000 (100.000)
Epoch: [74][64/196]	Time 0.070 (0.073)	Data 0.000 (0.005)	Loss 0.3951 (0.3853)	Acc@1 87.500 (86.544)	Acc@5 98.828 (99.543)
Epoch: [74][128/196]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.3511 (0.3821)	Acc@1 86.719 (86.794)	Acc@5 99.219 (99.546)
Epoch: [74][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.5457 (0.3813)	Acc@1 80.469 (86.737)	Acc@5 99.219 (99.545)
after train
n1: 30 for:
wAcc: 78.32994258969406
test acc: 79.12
Epoche: [75/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [75][0/196]	Time 0.117 (0.117)	Data 0.418 (0.418)	Loss 0.2926 (0.2926)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [75][64/196]	Time 0.069 (0.072)	Data 0.000 (0.007)	Loss 0.3618 (0.3715)	Acc@1 85.156 (86.839)	Acc@5 99.219 (99.591)
Epoch: [75][128/196]	Time 0.071 (0.071)	Data 0.000 (0.003)	Loss 0.4595 (0.3812)	Acc@1 87.891 (86.601)	Acc@5 99.219 (99.522)
Epoch: [75][192/196]	Time 0.094 (0.075)	Data 0.000 (0.002)	Loss 0.4137 (0.3769)	Acc@1 87.109 (86.856)	Acc@5 99.609 (99.516)
after train
n1: 30 for:
wAcc: 78.24936301842918
test acc: 72.87
Epoche: [76/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [76][0/196]	Time 0.106 (0.106)	Data 0.298 (0.298)	Loss 0.4813 (0.4813)	Acc@1 83.984 (83.984)	Acc@5 98.828 (98.828)
Epoch: [76][64/196]	Time 0.071 (0.071)	Data 0.000 (0.005)	Loss 0.3502 (0.3688)	Acc@1 87.109 (87.254)	Acc@5 99.609 (99.585)
Epoch: [76][128/196]	Time 0.071 (0.075)	Data 0.000 (0.003)	Loss 0.3757 (0.3691)	Acc@1 87.500 (87.270)	Acc@5 99.219 (99.573)
Epoch: [76][192/196]	Time 0.068 (0.074)	Data 0.000 (0.002)	Loss 0.3332 (0.3734)	Acc@1 86.719 (87.051)	Acc@5 99.609 (99.575)
after train
n1: 30 for:
wAcc: 78.57885542771774
test acc: 81.31
Epoche: [77/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [77][0/196]	Time 0.112 (0.112)	Data 0.248 (0.248)	Loss 0.4075 (0.4075)	Acc@1 85.938 (85.938)	Acc@5 98.828 (98.828)
Epoch: [77][64/196]	Time 0.071 (0.070)	Data 0.000 (0.004)	Loss 0.3321 (0.3809)	Acc@1 87.109 (86.875)	Acc@5 99.609 (99.483)
Epoch: [77][128/196]	Time 0.074 (0.073)	Data 0.000 (0.002)	Loss 0.3802 (0.3809)	Acc@1 85.156 (86.955)	Acc@5 99.609 (99.525)
Epoch: [77][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.3588 (0.3795)	Acc@1 86.719 (86.877)	Acc@5 99.609 (99.528)
after train
n1: 30 for:
wAcc: 78.04526105728611
test acc: 81.94
Epoche: [78/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [78][0/196]	Time 0.093 (0.093)	Data 0.351 (0.351)	Loss 0.3629 (0.3629)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [78][64/196]	Time 0.068 (0.074)	Data 0.000 (0.006)	Loss 0.4085 (0.3700)	Acc@1 84.766 (87.218)	Acc@5 98.828 (99.513)
Epoch: [78][128/196]	Time 0.082 (0.072)	Data 0.000 (0.003)	Loss 0.3273 (0.3629)	Acc@1 89.062 (87.436)	Acc@5 99.609 (99.558)
Epoch: [78][192/196]	Time 0.068 (0.072)	Data 0.000 (0.002)	Loss 0.4457 (0.3663)	Acc@1 83.203 (87.316)	Acc@5 99.609 (99.551)
after train
n1: 30 for:
wAcc: 78.4454329414214
test acc: 80.51
Epoche: [79/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [79][0/196]	Time 0.092 (0.092)	Data 0.354 (0.354)	Loss 0.3406 (0.3406)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [79][64/196]	Time 0.070 (0.072)	Data 0.000 (0.006)	Loss 0.3211 (0.3655)	Acc@1 89.844 (87.194)	Acc@5 99.219 (99.597)
Epoch: [79][128/196]	Time 0.086 (0.075)	Data 0.000 (0.003)	Loss 0.3291 (0.3742)	Acc@1 87.109 (86.900)	Acc@5 98.828 (99.567)
Epoch: [79][192/196]	Time 0.085 (0.079)	Data 0.000 (0.002)	Loss 0.4137 (0.3728)	Acc@1 86.328 (87.004)	Acc@5 99.219 (99.575)
after train
n1: 30 for:
wAcc: 79.11350857796722
test acc: 79.47
Epoche: [80/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [80][0/196]	Time 0.125 (0.125)	Data 0.264 (0.264)	Loss 0.4601 (0.4601)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [80][64/196]	Time 0.081 (0.075)	Data 0.000 (0.004)	Loss 0.3984 (0.3760)	Acc@1 83.984 (87.055)	Acc@5 98.828 (99.465)
Epoch: [80][128/196]	Time 0.070 (0.075)	Data 0.000 (0.002)	Loss 0.5118 (0.3787)	Acc@1 82.422 (87.031)	Acc@5 98.047 (99.488)
Epoch: [80][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.3621 (0.3767)	Acc@1 86.328 (87.077)	Acc@5 100.000 (99.500)
after train
n1: 30 for:
wAcc: 78.51344771283298
test acc: 79.45
Epoche: [81/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [81][0/196]	Time 0.117 (0.117)	Data 0.260 (0.260)	Loss 0.3165 (0.3165)	Acc@1 91.016 (91.016)	Acc@5 99.609 (99.609)
Epoch: [81][64/196]	Time 0.069 (0.083)	Data 0.000 (0.004)	Loss 0.4860 (0.3764)	Acc@1 83.984 (87.139)	Acc@5 99.219 (99.537)
Epoch: [81][128/196]	Time 0.069 (0.076)	Data 0.000 (0.002)	Loss 0.3513 (0.3747)	Acc@1 88.281 (87.137)	Acc@5 99.219 (99.528)
Epoch: [81][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.3491 (0.3719)	Acc@1 87.500 (87.176)	Acc@5 99.609 (99.547)
after train
n1: 30 for:
wAcc: 77.21788303409873
test acc: 79.4
Epoche: [82/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [82][0/196]	Time 0.107 (0.107)	Data 0.313 (0.313)	Loss 0.3787 (0.3787)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [82][64/196]	Time 0.092 (0.095)	Data 0.000 (0.005)	Loss 0.3237 (0.3776)	Acc@1 90.625 (86.713)	Acc@5 99.609 (99.603)
Epoch: [82][128/196]	Time 0.081 (0.084)	Data 0.000 (0.003)	Loss 0.4077 (0.3791)	Acc@1 87.500 (86.855)	Acc@5 98.828 (99.519)
Epoch: [82][192/196]	Time 0.069 (0.079)	Data 0.000 (0.002)	Loss 0.3864 (0.3780)	Acc@1 86.719 (86.887)	Acc@5 98.438 (99.528)
after train
n1: 30 for:
wAcc: 78.48479930475665
test acc: 75.53
Epoche: [83/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [83][0/196]	Time 0.092 (0.092)	Data 0.300 (0.300)	Loss 0.3973 (0.3973)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [83][64/196]	Time 0.089 (0.079)	Data 0.000 (0.005)	Loss 0.3212 (0.3572)	Acc@1 89.062 (87.873)	Acc@5 100.000 (99.615)
Epoch: [83][128/196]	Time 0.070 (0.081)	Data 0.000 (0.003)	Loss 0.3015 (0.3663)	Acc@1 88.281 (87.421)	Acc@5 98.438 (99.555)
Epoch: [83][192/196]	Time 0.085 (0.080)	Data 0.000 (0.002)	Loss 0.3588 (0.3724)	Acc@1 87.891 (87.227)	Acc@5 99.609 (99.520)
after train
n1: 30 for:
wAcc: 78.77122023044818
test acc: 83.9
Epoche: [84/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [84][0/196]	Time 0.109 (0.109)	Data 0.293 (0.293)	Loss 0.3087 (0.3087)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [84][64/196]	Time 0.097 (0.089)	Data 0.000 (0.005)	Loss 0.4289 (0.3683)	Acc@1 85.156 (87.446)	Acc@5 99.609 (99.567)
Epoch: [84][128/196]	Time 0.070 (0.088)	Data 0.000 (0.003)	Loss 0.4147 (0.3698)	Acc@1 83.984 (87.224)	Acc@5 98.828 (99.588)
Epoch: [84][192/196]	Time 0.069 (0.082)	Data 0.000 (0.002)	Loss 0.3479 (0.3743)	Acc@1 89.453 (87.014)	Acc@5 99.609 (99.559)
after train
n1: 30 for:
wAcc: 79.02838285364473
test acc: 78.37
Epoche: [85/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [85][0/196]	Time 0.128 (0.128)	Data 0.272 (0.272)	Loss 0.3491 (0.3491)	Acc@1 87.109 (87.109)	Acc@5 99.219 (99.219)
Epoch: [85][64/196]	Time 0.068 (0.072)	Data 0.000 (0.004)	Loss 0.3610 (0.3561)	Acc@1 87.500 (87.782)	Acc@5 99.609 (99.579)
Epoch: [85][128/196]	Time 0.071 (0.071)	Data 0.000 (0.002)	Loss 0.3192 (0.3643)	Acc@1 87.891 (87.488)	Acc@5 99.219 (99.576)
Epoch: [85][192/196]	Time 0.081 (0.071)	Data 0.000 (0.002)	Loss 0.4342 (0.3639)	Acc@1 85.547 (87.565)	Acc@5 99.609 (99.557)
after train
n1: 30 for:
wAcc: 79.14926110019086
test acc: 76.91
Epoche: [86/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [86][0/196]	Time 0.131 (0.131)	Data 0.237 (0.237)	Loss 0.3353 (0.3353)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [86][64/196]	Time 0.094 (0.078)	Data 0.000 (0.004)	Loss 0.2641 (0.3663)	Acc@1 89.453 (87.380)	Acc@5 100.000 (99.549)
Epoch: [86][128/196]	Time 0.097 (0.087)	Data 0.000 (0.002)	Loss 0.3222 (0.3738)	Acc@1 89.453 (87.143)	Acc@5 100.000 (99.552)
Epoch: [86][192/196]	Time 0.070 (0.085)	Data 0.000 (0.002)	Loss 0.3000 (0.3734)	Acc@1 91.797 (87.109)	Acc@5 99.609 (99.512)
after train
n1: 30 for:
wAcc: 79.03804180027994
test acc: 80.5
Epoche: [87/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [87][0/196]	Time 0.113 (0.113)	Data 0.259 (0.259)	Loss 0.3672 (0.3672)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [87][64/196]	Time 0.096 (0.091)	Data 0.000 (0.004)	Loss 0.3518 (0.3743)	Acc@1 87.891 (87.043)	Acc@5 100.000 (99.501)
Epoch: [87][128/196]	Time 0.070 (0.081)	Data 0.000 (0.002)	Loss 0.3734 (0.3685)	Acc@1 87.891 (87.200)	Acc@5 98.828 (99.519)
Epoch: [87][192/196]	Time 0.069 (0.077)	Data 0.000 (0.002)	Loss 0.3950 (0.3662)	Acc@1 87.109 (87.330)	Acc@5 98.047 (99.528)
after train
n1: 30 for:
wAcc: 78.97912643345535
test acc: 83.9
Epoche: [88/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [88][0/196]	Time 0.112 (0.112)	Data 0.265 (0.265)	Loss 0.3145 (0.3145)	Acc@1 91.797 (91.797)	Acc@5 99.219 (99.219)
Epoch: [88][64/196]	Time 0.087 (0.085)	Data 0.000 (0.004)	Loss 0.4119 (0.3619)	Acc@1 84.375 (87.416)	Acc@5 99.609 (99.465)
Epoch: [88][128/196]	Time 0.090 (0.084)	Data 0.000 (0.002)	Loss 0.3729 (0.3720)	Acc@1 86.719 (87.125)	Acc@5 100.000 (99.500)
Epoch: [88][192/196]	Time 0.069 (0.082)	Data 0.000 (0.002)	Loss 0.4433 (0.3734)	Acc@1 83.594 (87.150)	Acc@5 99.609 (99.520)
after train
n1: 30 for:
wAcc: 78.82099462409674
test acc: 78.95
Epoche: [89/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [89][0/196]	Time 0.089 (0.089)	Data 0.229 (0.229)	Loss 0.2745 (0.2745)	Acc@1 91.797 (91.797)	Acc@5 99.219 (99.219)
Epoch: [89][64/196]	Time 0.089 (0.074)	Data 0.000 (0.004)	Loss 0.3053 (0.3558)	Acc@1 88.672 (87.692)	Acc@5 99.609 (99.537)
Epoch: [89][128/196]	Time 0.073 (0.082)	Data 0.000 (0.002)	Loss 0.4077 (0.3662)	Acc@1 86.719 (87.364)	Acc@5 99.609 (99.552)
Epoch: [89][192/196]	Time 0.069 (0.078)	Data 0.000 (0.001)	Loss 0.4966 (0.3675)	Acc@1 82.422 (87.405)	Acc@5 99.219 (99.541)
after train
n1: 30 for:
wAcc: 79.39166200924934
test acc: 79.63
Epoche: [90/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [90][0/196]	Time 0.091 (0.091)	Data 0.357 (0.357)	Loss 0.3161 (0.3161)	Acc@1 91.797 (91.797)	Acc@5 99.219 (99.219)
Epoch: [90][64/196]	Time 0.072 (0.074)	Data 0.000 (0.006)	Loss 0.3367 (0.3794)	Acc@1 89.844 (87.218)	Acc@5 99.609 (99.531)
Epoch: [90][128/196]	Time 0.069 (0.072)	Data 0.000 (0.003)	Loss 0.3938 (0.3764)	Acc@1 83.594 (87.070)	Acc@5 100.000 (99.555)
Epoch: [90][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.3752 (0.3771)	Acc@1 83.594 (87.008)	Acc@5 99.609 (99.516)
after train
n1: 30 for:
wAcc: 78.43413937356905
test acc: 81.49


now deeper1
i: 3
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 3; i0=: 16; i1=: 16
i: 4
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 4; i0=: 16; i1=: 16
i: 5
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 5; i0=: 16; i1=: 16
i: 6
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 6; i0=: 16; i1=: 16
i: 7
j: 0; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 7; i0=: 16; i1=: 16
i: 8
j: 0; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)]
seq[j]: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 5; 
seq: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
i: 8; i0=: 32; i1=: 16
skip: 9
i: 10
j: 0; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 10; i0=: 32; i1=: 32
i: 11
j: 0; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 11; i0=: 32; i1=: 32
i: 12
j: 0; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 12; i0=: 32; i1=: 32
i: 13
j: 0; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 13; i0=: 32; i1=: 32
i: 14
j: 0; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)]
seq[j]: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 5; 
seq: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
i: 14; i0=: 64; i1=: 32
skip: 15
i: 16
j: 0; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 16; i0=: 64; i1=: 64
i: 17
j: 0; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 17; i0=: 64; i1=: 64
i: 18
j: 0; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 18; i0=: 64; i1=: 64
i: 19
j: 0; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
j: 1; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
j: 2; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True)]
seq[j]: ReLU(inplace=True)
j: 3; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
seq[j]: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
j: 4; 
seq: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
seq[j]: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
i: 19; i0=: 64; i1=: 64
N2N(
  (paramList): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (15): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (16): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (paramList1): ParameterList(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (5): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (6): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (7): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (8): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (9): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (10): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (11): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (12): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (13): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (14): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (15): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
      (16): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]
  )
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Nums: [[1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [2, 1, 1, 1, 1]]
num: 10; numofstages: 3, listofBlocks: [5, 5, 5], layers in blocj: 1
model.para: <generator object Module.named_parameters at 0x7fbab9f46e08>
Epoche: [91/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [91][0/196]	Time 0.216 (0.216)	Data 0.296 (0.296)	Loss 2.7064 (2.7064)	Acc@1 12.500 (12.500)	Acc@5 51.562 (51.562)
Epoch: [91][64/196]	Time 0.173 (0.173)	Data 0.000 (0.005)	Loss 2.2032 (2.3055)	Acc@1 14.062 (13.083)	Acc@5 57.422 (57.704)
Epoch: [91][128/196]	Time 0.171 (0.173)	Data 0.000 (0.003)	Loss 2.0183 (2.2228)	Acc@1 21.875 (15.286)	Acc@5 75.391 (64.199)
Epoch: [91][192/196]	Time 0.172 (0.173)	Data 0.000 (0.002)	Loss 1.9063 (2.1367)	Acc@1 25.781 (17.430)	Acc@5 83.594 (69.554)
after train
n1: 30 for:
wAcc: 78.94209902012076
test acc: 22.06
Epoche: [92/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [92][0/196]	Time 0.210 (0.210)	Data 0.334 (0.334)	Loss 1.8391 (1.8391)	Acc@1 27.734 (27.734)	Acc@5 85.156 (85.156)
Epoch: [92][64/196]	Time 0.126 (0.128)	Data 0.000 (0.005)	Loss 1.8366 (1.8684)	Acc@1 27.734 (25.805)	Acc@5 81.250 (82.981)
Epoch: [92][128/196]	Time 0.125 (0.127)	Data 0.000 (0.003)	Loss 1.8539 (1.8447)	Acc@1 28.125 (26.323)	Acc@5 82.422 (83.839)
Epoch: [92][192/196]	Time 0.125 (0.127)	Data 0.000 (0.002)	Loss 1.7782 (1.8293)	Acc@1 32.031 (27.087)	Acc@5 82.422 (84.231)
after train
n1: 30 for:
wAcc: 75.24771071538166
test acc: 25.63
Epoche: [93/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [93][0/196]	Time 0.223 (0.223)	Data 0.307 (0.307)	Loss 1.7372 (1.7372)	Acc@1 31.641 (31.641)	Acc@5 87.109 (87.109)
Epoch: [93][64/196]	Time 0.174 (0.173)	Data 0.000 (0.005)	Loss 1.7626 (1.7512)	Acc@1 32.812 (31.328)	Acc@5 85.938 (86.376)
Epoch: [93][128/196]	Time 0.127 (0.157)	Data 0.000 (0.003)	Loss 1.7057 (1.7385)	Acc@1 32.422 (31.722)	Acc@5 84.375 (86.940)
Epoch: [93][192/196]	Time 0.154 (0.155)	Data 0.000 (0.002)	Loss 1.6622 (1.7293)	Acc@1 35.938 (32.155)	Acc@5 87.891 (87.346)
after train
n1: 30 for:
wAcc: 70.83947908560447
test acc: 32.36
Epoche: [94/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [94][0/196]	Time 0.163 (0.163)	Data 0.373 (0.373)	Loss 1.6756 (1.6756)	Acc@1 33.203 (33.203)	Acc@5 88.281 (88.281)
Epoch: [94][64/196]	Time 0.126 (0.127)	Data 0.000 (0.006)	Loss 1.6463 (1.6799)	Acc@1 37.891 (34.970)	Acc@5 90.234 (88.618)
Epoch: [94][128/196]	Time 0.126 (0.132)	Data 0.000 (0.003)	Loss 1.6500 (1.6635)	Acc@1 39.062 (35.723)	Acc@5 92.578 (89.168)
Epoch: [94][192/196]	Time 0.173 (0.142)	Data 0.000 (0.002)	Loss 1.6117 (1.6427)	Acc@1 38.281 (36.858)	Acc@5 88.672 (89.506)
after train
n1: 30 for:
wAcc: 70.1972006715259
test acc: 40.07
Epoche: [95/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [95][0/196]	Time 0.171 (0.171)	Data 0.302 (0.302)	Loss 1.6480 (1.6480)	Acc@1 37.109 (37.109)	Acc@5 88.672 (88.672)
Epoch: [95][64/196]	Time 0.126 (0.133)	Data 0.000 (0.005)	Loss 1.6149 (1.5850)	Acc@1 42.188 (39.940)	Acc@5 88.281 (90.709)
Epoch: [95][128/196]	Time 0.152 (0.137)	Data 0.000 (0.003)	Loss 1.4992 (1.5685)	Acc@1 43.359 (40.689)	Acc@5 88.672 (90.970)
Epoch: [95][192/196]	Time 0.153 (0.139)	Data 0.000 (0.002)	Loss 1.5005 (1.5514)	Acc@1 42.578 (41.366)	Acc@5 92.969 (91.392)
after train
n1: 30 for:
wAcc: 67.74609923969835
test acc: 39.47
Epoche: [96/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [96][0/196]	Time 0.185 (0.185)	Data 0.287 (0.287)	Loss 1.4979 (1.4979)	Acc@1 44.922 (44.922)	Acc@5 90.625 (90.625)
Epoch: [96][64/196]	Time 0.125 (0.127)	Data 0.000 (0.005)	Loss 1.4026 (1.4961)	Acc@1 46.094 (43.990)	Acc@5 92.578 (92.049)
Epoch: [96][128/196]	Time 0.126 (0.127)	Data 0.000 (0.003)	Loss 1.3575 (1.4764)	Acc@1 46.094 (44.734)	Acc@5 93.750 (92.527)
Epoch: [96][192/196]	Time 0.126 (0.126)	Data 0.000 (0.002)	Loss 1.5310 (1.4568)	Acc@1 42.188 (45.592)	Acc@5 89.844 (92.728)
after train
n1: 30 for:
wAcc: 65.98977870452522
test acc: 38.85
Epoche: [97/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [97][0/196]	Time 0.214 (0.214)	Data 0.278 (0.278)	Loss 1.4039 (1.4039)	Acc@1 47.656 (47.656)	Acc@5 93.750 (93.750)
Epoch: [97][64/196]	Time 0.129 (0.151)	Data 0.000 (0.005)	Loss 1.3941 (1.3992)	Acc@1 47.266 (47.849)	Acc@5 93.359 (93.684)
Epoch: [97][128/196]	Time 0.126 (0.139)	Data 0.000 (0.002)	Loss 1.3231 (1.3882)	Acc@1 52.734 (48.241)	Acc@5 93.359 (93.617)
Epoch: [97][192/196]	Time 0.125 (0.135)	Data 0.000 (0.002)	Loss 1.3451 (1.3704)	Acc@1 50.000 (49.024)	Acc@5 94.531 (93.752)
after train
n1: 30 for:
wAcc: 64.57709928366559
test acc: 48.65
Epoche: [98/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [98][0/196]	Time 0.178 (0.178)	Data 0.279 (0.279)	Loss 1.3246 (1.3246)	Acc@1 52.734 (52.734)	Acc@5 95.312 (95.312)
Epoch: [98][64/196]	Time 0.126 (0.128)	Data 0.000 (0.005)	Loss 1.2083 (1.3359)	Acc@1 55.078 (50.535)	Acc@5 96.094 (94.014)
Epoch: [98][128/196]	Time 0.149 (0.128)	Data 0.000 (0.002)	Loss 1.3665 (1.3207)	Acc@1 48.828 (51.420)	Acc@5 93.750 (94.004)
Epoch: [98][192/196]	Time 0.153 (0.136)	Data 0.000 (0.002)	Loss 1.3112 (1.3058)	Acc@1 53.906 (52.064)	Acc@5 95.312 (94.169)
after train
n1: 30 for:
wAcc: 63.092729970283386
test acc: 52.77
Epoche: [99/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [99][0/196]	Time 0.181 (0.181)	Data 0.385 (0.385)	Loss 1.1879 (1.1879)	Acc@1 55.859 (55.859)	Acc@5 95.703 (95.703)
Epoch: [99][64/196]	Time 0.126 (0.129)	Data 0.000 (0.006)	Loss 1.2407 (1.2627)	Acc@1 55.859 (53.756)	Acc@5 95.703 (94.724)
Epoch: [99][128/196]	Time 0.127 (0.128)	Data 0.000 (0.003)	Loss 1.1947 (1.2558)	Acc@1 56.641 (53.988)	Acc@5 93.359 (94.671)
Epoch: [99][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 1.2283 (1.2422)	Acc@1 52.734 (54.509)	Acc@5 97.266 (94.792)
after train
n1: 30 for:
wAcc: 63.06715509014158
test acc: 46.99
Epoche: [100/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [100][0/196]	Time 0.193 (0.193)	Data 0.278 (0.278)	Loss 1.2433 (1.2433)	Acc@1 52.344 (52.344)	Acc@5 96.484 (96.484)
Epoch: [100][64/196]	Time 0.149 (0.128)	Data 0.000 (0.005)	Loss 1.3532 (1.2159)	Acc@1 49.219 (55.541)	Acc@5 92.969 (95.162)
Epoch: [100][128/196]	Time 0.128 (0.128)	Data 0.000 (0.002)	Loss 1.2054 (1.2071)	Acc@1 55.859 (56.017)	Acc@5 95.312 (95.149)
Epoch: [100][192/196]	Time 0.149 (0.129)	Data 0.000 (0.002)	Loss 1.1641 (1.1958)	Acc@1 58.984 (56.535)	Acc@5 96.484 (95.240)
after train
n1: 30 for:
wAcc: 61.42276073745413
test acc: 49.1
Epoche: [101/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [101][0/196]	Time 0.191 (0.191)	Data 0.248 (0.248)	Loss 1.1829 (1.1829)	Acc@1 61.328 (61.328)	Acc@5 95.703 (95.703)
Epoch: [101][64/196]	Time 0.153 (0.155)	Data 0.000 (0.004)	Loss 1.2674 (1.1620)	Acc@1 54.688 (57.632)	Acc@5 94.922 (95.625)
Epoch: [101][128/196]	Time 0.156 (0.154)	Data 0.000 (0.002)	Loss 1.1008 (1.1510)	Acc@1 59.766 (58.109)	Acc@5 96.094 (95.800)
Epoch: [101][192/196]	Time 0.153 (0.154)	Data 0.000 (0.002)	Loss 1.1649 (1.1466)	Acc@1 55.859 (58.387)	Acc@5 96.484 (95.776)
after train
n1: 30 for:
wAcc: 61.181414679923265
test acc: 48.9
Epoche: [102/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [102][0/196]	Time 0.180 (0.180)	Data 0.368 (0.368)	Loss 1.1349 (1.1349)	Acc@1 57.422 (57.422)	Acc@5 96.875 (96.875)
Epoch: [102][64/196]	Time 0.154 (0.155)	Data 0.000 (0.006)	Loss 1.0790 (1.0952)	Acc@1 59.766 (59.886)	Acc@5 95.312 (96.310)
Epoch: [102][128/196]	Time 0.152 (0.154)	Data 0.000 (0.003)	Loss 1.0397 (1.0937)	Acc@1 64.062 (60.489)	Acc@5 96.094 (96.148)
Epoch: [102][192/196]	Time 0.154 (0.154)	Data 0.000 (0.002)	Loss 1.0768 (1.0889)	Acc@1 61.328 (60.579)	Acc@5 97.266 (96.195)
after train
n1: 30 for:
wAcc: 60.79239209044245
test acc: 53.63
Epoche: [103/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [103][0/196]	Time 0.204 (0.204)	Data 0.282 (0.282)	Loss 1.0932 (1.0932)	Acc@1 62.891 (62.891)	Acc@5 96.875 (96.875)
Epoch: [103][64/196]	Time 0.126 (0.129)	Data 0.000 (0.005)	Loss 1.1768 (1.0754)	Acc@1 58.984 (61.160)	Acc@5 97.266 (96.460)
Epoch: [103][128/196]	Time 0.126 (0.131)	Data 0.000 (0.002)	Loss 1.0237 (1.0609)	Acc@1 60.547 (61.695)	Acc@5 95.703 (96.490)
Epoch: [103][192/196]	Time 0.125 (0.130)	Data 0.000 (0.002)	Loss 0.9229 (1.0511)	Acc@1 62.500 (62.016)	Acc@5 98.047 (96.484)
after train
n1: 30 for:
wAcc: 59.74482797132201
test acc: 59.75
Epoche: [104/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [104][0/196]	Time 0.177 (0.177)	Data 0.288 (0.288)	Loss 1.0540 (1.0540)	Acc@1 60.156 (60.156)	Acc@5 96.094 (96.094)
Epoch: [104][64/196]	Time 0.125 (0.127)	Data 0.000 (0.005)	Loss 0.9989 (1.0045)	Acc@1 62.500 (63.816)	Acc@5 96.875 (96.911)
Epoch: [104][128/196]	Time 0.150 (0.129)	Data 0.000 (0.003)	Loss 0.9845 (1.0082)	Acc@1 64.844 (63.729)	Acc@5 98.047 (96.893)
Epoch: [104][192/196]	Time 0.153 (0.137)	Data 0.000 (0.002)	Loss 0.9529 (1.0078)	Acc@1 64.844 (63.761)	Acc@5 99.219 (96.849)
after train
n1: 30 for:
wAcc: 58.8416519178233
test acc: 55.1
Epoche: [105/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [105][0/196]	Time 0.182 (0.182)	Data 0.364 (0.364)	Loss 1.0306 (1.0306)	Acc@1 62.891 (62.891)	Acc@5 96.484 (96.484)
Epoch: [105][64/196]	Time 0.126 (0.129)	Data 0.000 (0.006)	Loss 0.9667 (0.9740)	Acc@1 64.453 (64.657)	Acc@5 96.094 (97.157)
Epoch: [105][128/196]	Time 0.126 (0.128)	Data 0.000 (0.003)	Loss 1.0034 (0.9706)	Acc@1 64.453 (64.798)	Acc@5 95.312 (97.096)
Epoch: [105][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.9735 (0.9708)	Acc@1 65.234 (64.801)	Acc@5 97.656 (97.071)
after train
n1: 30 for:
wAcc: 59.82035456302947
test acc: 55.62
Epoche: [106/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [106][0/196]	Time 0.168 (0.168)	Data 0.290 (0.290)	Loss 0.9718 (0.9718)	Acc@1 63.281 (63.281)	Acc@5 96.875 (96.875)
Epoch: [106][64/196]	Time 0.156 (0.152)	Data 0.000 (0.005)	Loss 0.9648 (0.9598)	Acc@1 64.453 (65.216)	Acc@5 98.438 (97.302)
Epoch: [106][128/196]	Time 0.124 (0.142)	Data 0.000 (0.003)	Loss 0.9278 (0.9346)	Acc@1 68.750 (66.137)	Acc@5 95.703 (97.411)
Epoch: [106][192/196]	Time 0.126 (0.137)	Data 0.000 (0.002)	Loss 0.9611 (0.9283)	Acc@1 67.969 (66.540)	Acc@5 97.266 (97.373)
after train
n1: 30 for:
wAcc: 59.64043772712286
test acc: 60.45
Epoche: [107/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [107][0/196]	Time 0.187 (0.187)	Data 0.290 (0.290)	Loss 0.8379 (0.8379)	Acc@1 70.703 (70.703)	Acc@5 98.828 (98.828)
Epoch: [107][64/196]	Time 0.148 (0.131)	Data 0.000 (0.005)	Loss 0.9879 (0.8985)	Acc@1 62.109 (67.806)	Acc@5 96.875 (97.500)
Epoch: [107][128/196]	Time 0.126 (0.140)	Data 0.000 (0.003)	Loss 0.9560 (0.8987)	Acc@1 65.234 (67.832)	Acc@5 97.266 (97.520)
Epoch: [107][192/196]	Time 0.126 (0.135)	Data 0.000 (0.002)	Loss 0.8751 (0.8974)	Acc@1 70.703 (67.882)	Acc@5 97.656 (97.458)
after train
n1: 30 for:
wAcc: 59.48594452432207
test acc: 47.75
Epoche: [108/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [108][0/196]	Time 0.166 (0.166)	Data 0.310 (0.310)	Loss 0.9014 (0.9014)	Acc@1 71.484 (71.484)	Acc@5 94.922 (94.922)
Epoch: [108][64/196]	Time 0.149 (0.130)	Data 0.000 (0.005)	Loss 0.8837 (0.8957)	Acc@1 68.750 (67.921)	Acc@5 96.484 (97.428)
Epoch: [108][128/196]	Time 0.156 (0.142)	Data 0.000 (0.003)	Loss 0.8183 (0.8780)	Acc@1 72.266 (68.680)	Acc@5 97.656 (97.596)
Epoch: [108][192/196]	Time 0.126 (0.138)	Data 0.000 (0.002)	Loss 0.7570 (0.8708)	Acc@1 75.000 (68.736)	Acc@5 98.047 (97.705)
after train
n1: 30 for:
wAcc: 58.57844279354286
test acc: 67.2
Epoche: [109/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [109][0/196]	Time 0.167 (0.167)	Data 0.267 (0.267)	Loss 0.7956 (0.7956)	Acc@1 70.312 (70.312)	Acc@5 98.438 (98.438)
Epoch: [109][64/196]	Time 0.127 (0.129)	Data 0.000 (0.004)	Loss 0.7627 (0.8524)	Acc@1 71.484 (69.387)	Acc@5 98.438 (97.812)
Epoch: [109][128/196]	Time 0.155 (0.133)	Data 0.000 (0.002)	Loss 0.7743 (0.8410)	Acc@1 72.266 (69.979)	Acc@5 97.656 (97.853)
Epoch: [109][192/196]	Time 0.155 (0.140)	Data 0.000 (0.002)	Loss 0.8516 (0.8370)	Acc@1 66.016 (70.191)	Acc@5 99.219 (97.895)
after train
n1: 30 for:
wAcc: 59.131781059588775
test acc: 66.19
Epoche: [110/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [110][0/196]	Time 0.158 (0.158)	Data 0.365 (0.365)	Loss 0.8044 (0.8044)	Acc@1 71.484 (71.484)	Acc@5 98.047 (98.047)
Epoch: [110][64/196]	Time 0.127 (0.126)	Data 0.000 (0.006)	Loss 0.8127 (0.8076)	Acc@1 70.703 (71.334)	Acc@5 98.047 (98.083)
Epoch: [110][128/196]	Time 0.125 (0.126)	Data 0.000 (0.003)	Loss 0.7833 (0.8089)	Acc@1 70.703 (71.285)	Acc@5 98.047 (98.068)
Epoch: [110][192/196]	Time 0.125 (0.126)	Data 0.000 (0.002)	Loss 0.7943 (0.8068)	Acc@1 74.219 (71.327)	Acc@5 98.047 (98.099)
after train
n1: 30 for:
wAcc: 59.57992194562416
test acc: 68.6
Epoche: [111/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [111][0/196]	Time 0.174 (0.174)	Data 0.300 (0.300)	Loss 0.7955 (0.7955)	Acc@1 70.703 (70.703)	Acc@5 97.266 (97.266)
Epoch: [111][64/196]	Time 0.125 (0.129)	Data 0.000 (0.005)	Loss 0.7542 (0.7952)	Acc@1 72.266 (71.520)	Acc@5 98.438 (98.071)
Epoch: [111][128/196]	Time 0.126 (0.128)	Data 0.000 (0.003)	Loss 0.6844 (0.7861)	Acc@1 76.562 (71.860)	Acc@5 98.047 (98.153)
Epoch: [111][192/196]	Time 0.171 (0.133)	Data 0.000 (0.002)	Loss 0.7032 (0.7816)	Acc@1 73.438 (71.994)	Acc@5 98.828 (98.187)
after train
n1: 30 for:
wAcc: 59.602409238731184
test acc: 66.9
Epoche: [112/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [112][0/196]	Time 0.199 (0.199)	Data 0.275 (0.275)	Loss 0.7222 (0.7222)	Acc@1 74.609 (74.609)	Acc@5 98.828 (98.828)
Epoch: [112][64/196]	Time 0.148 (0.134)	Data 0.000 (0.005)	Loss 0.7232 (0.7612)	Acc@1 77.734 (73.269)	Acc@5 98.438 (98.251)
Epoch: [112][128/196]	Time 0.127 (0.141)	Data 0.000 (0.002)	Loss 0.7160 (0.7526)	Acc@1 73.047 (73.368)	Acc@5 98.828 (98.268)
Epoch: [112][192/196]	Time 0.155 (0.142)	Data 0.000 (0.002)	Loss 0.8185 (0.7522)	Acc@1 71.875 (73.350)	Acc@5 97.266 (98.276)
after train
n1: 30 for:
wAcc: 61.28320178003307
test acc: 56.37
Epoche: [113/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [113][0/196]	Time 0.179 (0.179)	Data 0.280 (0.280)	Loss 0.8102 (0.8102)	Acc@1 69.922 (69.922)	Acc@5 98.828 (98.828)
Epoch: [113][64/196]	Time 0.126 (0.138)	Data 0.000 (0.005)	Loss 0.6310 (0.7350)	Acc@1 79.688 (73.528)	Acc@5 98.438 (98.293)
Epoch: [113][128/196]	Time 0.149 (0.133)	Data 0.000 (0.002)	Loss 0.8209 (0.7331)	Acc@1 72.656 (73.795)	Acc@5 96.875 (98.395)
Epoch: [113][192/196]	Time 0.153 (0.140)	Data 0.000 (0.002)	Loss 0.7120 (0.7311)	Acc@1 75.781 (73.861)	Acc@5 99.609 (98.442)
after train
n1: 30 for:
wAcc: 60.166795608477585
test acc: 66.78
Epoche: [114/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [114][0/196]	Time 0.172 (0.172)	Data 0.286 (0.286)	Loss 0.7881 (0.7881)	Acc@1 70.703 (70.703)	Acc@5 98.438 (98.438)
Epoch: [114][64/196]	Time 0.154 (0.139)	Data 0.000 (0.005)	Loss 0.6516 (0.7213)	Acc@1 77.344 (74.850)	Acc@5 99.609 (98.407)
Epoch: [114][128/196]	Time 0.156 (0.146)	Data 0.000 (0.003)	Loss 0.6975 (0.7077)	Acc@1 76.562 (75.160)	Acc@5 98.438 (98.510)
Epoch: [114][192/196]	Time 0.125 (0.143)	Data 0.000 (0.002)	Loss 0.6656 (0.7065)	Acc@1 75.391 (75.172)	Acc@5 98.047 (98.516)
after train
n1: 30 for:
wAcc: 60.382394082743076
test acc: 70.37
Epoche: [115/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [115][0/196]	Time 0.199 (0.199)	Data 0.298 (0.298)	Loss 0.8238 (0.8238)	Acc@1 70.703 (70.703)	Acc@5 97.656 (97.656)
Epoch: [115][64/196]	Time 0.126 (0.146)	Data 0.000 (0.005)	Loss 0.7516 (0.6921)	Acc@1 74.609 (75.361)	Acc@5 98.047 (98.684)
Epoch: [115][128/196]	Time 0.127 (0.136)	Data 0.000 (0.003)	Loss 0.7612 (0.6832)	Acc@1 73.828 (75.736)	Acc@5 98.047 (98.640)
Epoch: [115][192/196]	Time 0.127 (0.133)	Data 0.000 (0.002)	Loss 0.6834 (0.6827)	Acc@1 76.562 (75.838)	Acc@5 98.047 (98.646)
after train
n1: 30 for:
wAcc: 61.54573174532626
test acc: 71.58
Epoche: [116/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [116][0/196]	Time 0.172 (0.172)	Data 0.316 (0.316)	Loss 0.6415 (0.6415)	Acc@1 78.516 (78.516)	Acc@5 98.828 (98.828)
Epoch: [116][64/196]	Time 0.125 (0.130)	Data 0.000 (0.005)	Loss 0.7051 (0.6639)	Acc@1 75.391 (76.364)	Acc@5 98.438 (98.780)
Epoch: [116][128/196]	Time 0.126 (0.129)	Data 0.000 (0.003)	Loss 0.6836 (0.6622)	Acc@1 75.000 (76.650)	Acc@5 97.656 (98.659)
Epoch: [116][192/196]	Time 0.128 (0.129)	Data 0.000 (0.002)	Loss 0.6204 (0.6595)	Acc@1 77.344 (76.803)	Acc@5 99.219 (98.672)
after train
n1: 30 for:
wAcc: 62.68461318541502
test acc: 65.16
Epoche: [117/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [117][0/196]	Time 0.161 (0.161)	Data 0.346 (0.346)	Loss 0.7349 (0.7349)	Acc@1 74.609 (74.609)	Acc@5 98.438 (98.438)
Epoch: [117][64/196]	Time 0.128 (0.129)	Data 0.000 (0.006)	Loss 0.6461 (0.6554)	Acc@1 76.172 (76.635)	Acc@5 99.219 (98.606)
Epoch: [117][128/196]	Time 0.127 (0.128)	Data 0.000 (0.003)	Loss 0.6863 (0.6435)	Acc@1 71.484 (77.029)	Acc@5 99.609 (98.737)
Epoch: [117][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.7450 (0.6435)	Acc@1 74.609 (77.192)	Acc@5 97.656 (98.719)
after train
n1: 30 for:
wAcc: 62.12873585219708
test acc: 71.85
Epoche: [118/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [118][0/196]	Time 0.169 (0.169)	Data 0.320 (0.320)	Loss 0.5937 (0.5937)	Acc@1 77.344 (77.344)	Acc@5 98.828 (98.828)
Epoch: [118][64/196]	Time 0.127 (0.141)	Data 0.000 (0.005)	Loss 0.5995 (0.6223)	Acc@1 76.562 (78.233)	Acc@5 98.047 (98.774)
Epoch: [118][128/196]	Time 0.125 (0.134)	Data 0.000 (0.003)	Loss 0.6153 (0.6253)	Acc@1 79.297 (78.083)	Acc@5 98.828 (98.731)
Epoch: [118][192/196]	Time 0.132 (0.132)	Data 0.000 (0.002)	Loss 0.7435 (0.6240)	Acc@1 69.922 (78.072)	Acc@5 97.266 (98.808)
after train
n1: 30 for:
wAcc: 62.85421604323857
test acc: 68.98
Epoche: [119/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [119][0/196]	Time 0.157 (0.157)	Data 0.336 (0.336)	Loss 0.5597 (0.5597)	Acc@1 77.344 (77.344)	Acc@5 99.609 (99.609)
Epoch: [119][64/196]	Time 0.127 (0.128)	Data 0.000 (0.005)	Loss 0.6093 (0.5970)	Acc@1 79.688 (78.924)	Acc@5 97.656 (98.828)
Epoch: [119][128/196]	Time 0.128 (0.128)	Data 0.000 (0.003)	Loss 0.6303 (0.6041)	Acc@1 76.172 (78.743)	Acc@5 99.219 (98.895)
Epoch: [119][192/196]	Time 0.131 (0.128)	Data 0.000 (0.002)	Loss 0.5938 (0.6102)	Acc@1 80.078 (78.544)	Acc@5 98.828 (98.863)
after train
n1: 30 for:
wAcc: 63.51831240788857
test acc: 77.3
Epoche: [120/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [120][0/196]	Time 0.182 (0.182)	Data 0.244 (0.244)	Loss 0.5115 (0.5115)	Acc@1 82.812 (82.812)	Acc@5 98.828 (98.828)
Epoch: [120][64/196]	Time 0.162 (0.132)	Data 0.000 (0.004)	Loss 0.6045 (0.5769)	Acc@1 79.297 (79.597)	Acc@5 99.219 (99.002)
Epoch: [120][128/196]	Time 0.152 (0.144)	Data 0.000 (0.002)	Loss 0.6079 (0.5878)	Acc@1 80.078 (79.115)	Acc@5 99.609 (99.001)
Epoch: [120][192/196]	Time 0.153 (0.147)	Data 0.000 (0.002)	Loss 0.6914 (0.5911)	Acc@1 74.609 (79.141)	Acc@5 98.828 (98.982)
after train
n1: 30 for:
wAcc: 55.81616019591673
test acc: 74.58
Epoche: [121/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [121][0/196]	Time 0.183 (0.183)	Data 0.358 (0.358)	Loss 0.6128 (0.6128)	Acc@1 78.516 (78.516)	Acc@5 98.828 (98.828)
Epoch: [121][64/196]	Time 0.155 (0.142)	Data 0.000 (0.006)	Loss 0.5554 (0.5686)	Acc@1 81.641 (80.150)	Acc@5 99.219 (99.050)
Epoch: [121][128/196]	Time 0.153 (0.148)	Data 0.000 (0.003)	Loss 0.5950 (0.5741)	Acc@1 78.906 (79.951)	Acc@5 99.219 (99.067)
Epoch: [121][192/196]	Time 0.155 (0.150)	Data 0.000 (0.002)	Loss 0.4927 (0.5719)	Acc@1 82.812 (79.866)	Acc@5 99.609 (99.099)
after train
n1: 30 for:
wAcc: 57.54281526521482
test acc: 76.62
Epoche: [122/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [122][0/196]	Time 0.168 (0.168)	Data 0.326 (0.326)	Loss 0.6090 (0.6090)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [122][64/196]	Time 0.127 (0.128)	Data 0.000 (0.005)	Loss 0.5799 (0.5572)	Acc@1 79.297 (80.649)	Acc@5 98.047 (99.002)
Epoch: [122][128/196]	Time 0.171 (0.135)	Data 0.000 (0.003)	Loss 0.5896 (0.5624)	Acc@1 74.219 (80.345)	Acc@5 100.000 (98.973)
Epoch: [122][192/196]	Time 0.170 (0.147)	Data 0.000 (0.002)	Loss 0.5669 (0.5601)	Acc@1 78.516 (80.349)	Acc@5 99.219 (98.996)
after train
n1: 30 for:
wAcc: 59.74650065738129
test acc: 75.97
Epoche: [123/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [123][0/196]	Time 0.175 (0.175)	Data 0.279 (0.279)	Loss 0.5072 (0.5072)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [123][64/196]	Time 0.153 (0.153)	Data 0.000 (0.005)	Loss 0.5759 (0.5627)	Acc@1 80.859 (80.409)	Acc@5 98.438 (99.111)
Epoch: [123][128/196]	Time 0.126 (0.148)	Data 0.000 (0.002)	Loss 0.4968 (0.5560)	Acc@1 83.984 (80.557)	Acc@5 99.219 (99.070)
Epoch: [123][192/196]	Time 0.152 (0.148)	Data 0.000 (0.002)	Loss 0.5080 (0.5576)	Acc@1 83.984 (80.534)	Acc@5 98.828 (99.049)
after train
n1: 30 for:
wAcc: 61.90774764066737
test acc: 73.27
Epoche: [124/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [124][0/196]	Time 0.196 (0.196)	Data 0.267 (0.267)	Loss 0.4632 (0.4632)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [124][64/196]	Time 0.126 (0.130)	Data 0.000 (0.004)	Loss 0.6105 (0.5302)	Acc@1 78.516 (80.944)	Acc@5 99.609 (99.219)
Epoch: [124][128/196]	Time 0.126 (0.128)	Data 0.000 (0.002)	Loss 0.5070 (0.5333)	Acc@1 79.688 (81.111)	Acc@5 98.438 (99.167)
Epoch: [124][192/196]	Time 0.126 (0.128)	Data 0.000 (0.002)	Loss 0.5247 (0.5358)	Acc@1 82.812 (81.019)	Acc@5 98.828 (99.150)
after train
n1: 30 for:
wAcc: 62.5540592456334
test acc: 69.15
Epoche: [125/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [125][0/196]	Time 0.166 (0.166)	Data 0.312 (0.312)	Loss 0.5347 (0.5347)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [125][64/196]	Time 0.150 (0.125)	Data 0.000 (0.005)	Loss 0.6093 (0.5167)	Acc@1 80.469 (82.188)	Acc@5 98.438 (99.177)
Epoch: [125][128/196]	Time 0.151 (0.138)	Data 0.000 (0.003)	Loss 0.4789 (0.5220)	Acc@1 85.156 (81.859)	Acc@5 99.219 (99.152)
Epoch: [125][192/196]	Time 0.126 (0.139)	Data 0.000 (0.002)	Loss 0.5317 (0.5296)	Acc@1 78.906 (81.568)	Acc@5 99.609 (99.118)
after train
n1: 30 for:
wAcc: 62.88997564494064
test acc: 65.79
Epoche: [126/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [126][0/196]	Time 0.173 (0.173)	Data 0.258 (0.258)	Loss 0.5315 (0.5315)	Acc@1 82.031 (82.031)	Acc@5 99.609 (99.609)
Epoch: [126][64/196]	Time 0.125 (0.141)	Data 0.000 (0.004)	Loss 0.4752 (0.5039)	Acc@1 81.250 (82.181)	Acc@5 100.000 (99.231)
Epoch: [126][128/196]	Time 0.126 (0.134)	Data 0.000 (0.002)	Loss 0.6286 (0.5081)	Acc@1 78.906 (82.213)	Acc@5 98.828 (99.149)
Epoch: [126][192/196]	Time 0.127 (0.131)	Data 0.000 (0.002)	Loss 0.4820 (0.5086)	Acc@1 85.547 (82.161)	Acc@5 99.609 (99.148)
after train
n1: 30 for:
wAcc: 64.49377725140897
test acc: 72.42
Epoche: [127/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [127][0/196]	Time 0.194 (0.194)	Data 0.239 (0.239)	Loss 0.4079 (0.4079)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [127][64/196]	Time 0.174 (0.173)	Data 0.000 (0.004)	Loss 0.4935 (0.4965)	Acc@1 81.641 (82.758)	Acc@5 100.000 (99.243)
Epoch: [127][128/196]	Time 0.171 (0.173)	Data 0.000 (0.002)	Loss 0.5336 (0.5013)	Acc@1 82.422 (82.540)	Acc@5 99.219 (99.243)
Epoch: [127][192/196]	Time 0.171 (0.173)	Data 0.000 (0.002)	Loss 0.6188 (0.5032)	Acc@1 78.516 (82.406)	Acc@5 98.438 (99.243)
after train
n1: 30 for:
wAcc: 65.60074007683633
test acc: 74.99
Epoche: [128/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [128][0/196]	Time 0.161 (0.161)	Data 0.337 (0.337)	Loss 0.5116 (0.5116)	Acc@1 82.422 (82.422)	Acc@5 99.609 (99.609)
Epoch: [128][64/196]	Time 0.148 (0.129)	Data 0.000 (0.005)	Loss 0.5245 (0.4970)	Acc@1 80.859 (82.692)	Acc@5 99.219 (99.207)
Epoch: [128][128/196]	Time 0.153 (0.138)	Data 0.000 (0.003)	Loss 0.5590 (0.5018)	Acc@1 82.031 (82.443)	Acc@5 98.438 (99.273)
Epoch: [128][192/196]	Time 0.153 (0.143)	Data 0.000 (0.002)	Loss 0.4456 (0.5005)	Acc@1 83.984 (82.458)	Acc@5 100.000 (99.253)
after train
n1: 30 for:
wAcc: 65.3709329806925
test acc: 78.34
Epoche: [129/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [129][0/196]	Time 0.187 (0.187)	Data 0.248 (0.248)	Loss 0.4055 (0.4055)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [129][64/196]	Time 0.153 (0.147)	Data 0.000 (0.004)	Loss 0.4872 (0.4911)	Acc@1 83.203 (82.500)	Acc@5 99.219 (99.327)
Epoch: [129][128/196]	Time 0.155 (0.151)	Data 0.000 (0.002)	Loss 0.4447 (0.4848)	Acc@1 87.109 (82.903)	Acc@5 99.219 (99.285)
Epoch: [129][192/196]	Time 0.126 (0.144)	Data 0.000 (0.002)	Loss 0.5106 (0.4829)	Acc@1 82.422 (83.031)	Acc@5 99.219 (99.312)
after train
n1: 30 for:
wAcc: 66.51267186772068
test acc: 76.33
Epoche: [130/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [130][0/196]	Time 0.163 (0.163)	Data 0.354 (0.354)	Loss 0.4036 (0.4036)	Acc@1 89.062 (89.062)	Acc@5 99.219 (99.219)
Epoch: [130][64/196]	Time 0.125 (0.129)	Data 0.000 (0.006)	Loss 0.4456 (0.4760)	Acc@1 83.594 (83.251)	Acc@5 99.609 (99.285)
Epoch: [130][128/196]	Time 0.125 (0.127)	Data 0.000 (0.003)	Loss 0.5202 (0.4704)	Acc@1 82.031 (83.560)	Acc@5 98.828 (99.337)
Epoch: [130][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.4370 (0.4732)	Acc@1 83.984 (83.426)	Acc@5 99.219 (99.304)
after train
n1: 30 for:
wAcc: 67.11713556480626
test acc: 78.15
Epoche: [131/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [131][0/196]	Time 0.176 (0.176)	Data 0.401 (0.401)	Loss 0.4423 (0.4423)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [131][64/196]	Time 0.127 (0.129)	Data 0.000 (0.006)	Loss 0.4837 (0.4728)	Acc@1 83.984 (83.185)	Acc@5 99.219 (99.471)
Epoch: [131][128/196]	Time 0.127 (0.128)	Data 0.000 (0.003)	Loss 0.5423 (0.4615)	Acc@1 80.078 (83.672)	Acc@5 99.609 (99.440)
Epoch: [131][192/196]	Time 0.126 (0.128)	Data 0.000 (0.002)	Loss 0.4308 (0.4611)	Acc@1 85.156 (83.802)	Acc@5 99.609 (99.399)
after train
n1: 30 for:
wAcc: 68.51270943606167
test acc: 70.78
Epoche: [132/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [132][0/196]	Time 0.184 (0.184)	Data 0.286 (0.286)	Loss 0.4790 (0.4790)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [132][64/196]	Time 0.126 (0.129)	Data 0.000 (0.005)	Loss 0.3949 (0.4665)	Acc@1 85.938 (83.978)	Acc@5 98.828 (99.303)
Epoch: [132][128/196]	Time 0.126 (0.137)	Data 0.000 (0.003)	Loss 0.4347 (0.4626)	Acc@1 83.594 (84.000)	Acc@5 99.219 (99.355)
Epoch: [132][192/196]	Time 0.148 (0.138)	Data 0.000 (0.002)	Loss 0.4402 (0.4618)	Acc@1 84.375 (83.907)	Acc@5 99.219 (99.340)
after train
n1: 30 for:
wAcc: 69.54370297696498
test acc: 79.17
Epoche: [133/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [133][0/196]	Time 0.169 (0.169)	Data 0.267 (0.267)	Loss 0.4030 (0.4030)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [133][64/196]	Time 0.128 (0.129)	Data 0.000 (0.004)	Loss 0.4034 (0.4467)	Acc@1 86.719 (84.483)	Acc@5 99.609 (99.399)
Epoch: [133][128/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.4516 (0.4559)	Acc@1 84.375 (84.257)	Acc@5 100.000 (99.376)
Epoch: [133][192/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.4509 (0.4536)	Acc@1 84.766 (84.262)	Acc@5 99.609 (99.389)
after train
n1: 30 for:
wAcc: 69.49254315662637
test acc: 73.21
Epoche: [134/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [134][0/196]	Time 0.192 (0.192)	Data 0.315 (0.315)	Loss 0.4173 (0.4173)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [134][64/196]	Time 0.126 (0.138)	Data 0.000 (0.005)	Loss 0.3841 (0.4595)	Acc@1 87.109 (83.930)	Acc@5 98.828 (99.345)
Epoch: [134][128/196]	Time 0.126 (0.132)	Data 0.000 (0.003)	Loss 0.4457 (0.4557)	Acc@1 85.547 (83.884)	Acc@5 99.609 (99.376)
Epoch: [134][192/196]	Time 0.126 (0.130)	Data 0.000 (0.002)	Loss 0.5792 (0.4510)	Acc@1 79.688 (84.108)	Acc@5 98.828 (99.427)
after train
n1: 30 for:
wAcc: 69.80755109177167
test acc: 80.13
Epoche: [135/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [135][0/196]	Time 0.205 (0.205)	Data 0.278 (0.278)	Loss 0.4095 (0.4095)	Acc@1 85.547 (85.547)	Acc@5 100.000 (100.000)
Epoch: [135][64/196]	Time 0.147 (0.130)	Data 0.000 (0.005)	Loss 0.4670 (0.4293)	Acc@1 84.375 (85.132)	Acc@5 99.219 (99.423)
Epoch: [135][128/196]	Time 0.153 (0.141)	Data 0.000 (0.002)	Loss 0.4939 (0.4285)	Acc@1 83.203 (85.056)	Acc@5 99.219 (99.458)
Epoch: [135][192/196]	Time 0.153 (0.145)	Data 0.000 (0.002)	Loss 0.3539 (0.4342)	Acc@1 89.453 (84.905)	Acc@5 99.609 (99.401)
after train
n1: 30 for:
wAcc: 71.17174785894711
test acc: 76.04
Epoche: [136/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [136][0/196]	Time 0.193 (0.193)	Data 0.412 (0.412)	Loss 0.4155 (0.4155)	Acc@1 83.984 (83.984)	Acc@5 100.000 (100.000)
Epoch: [136][64/196]	Time 0.127 (0.147)	Data 0.000 (0.007)	Loss 0.4621 (0.4259)	Acc@1 83.984 (85.162)	Acc@5 99.609 (99.453)
Epoch: [136][128/196]	Time 0.173 (0.153)	Data 0.000 (0.004)	Loss 0.5013 (0.4261)	Acc@1 83.984 (84.947)	Acc@5 99.609 (99.464)
Epoch: [136][192/196]	Time 0.175 (0.159)	Data 0.000 (0.002)	Loss 0.4343 (0.4307)	Acc@1 83.594 (84.824)	Acc@5 98.828 (99.433)
after train
n1: 30 for:
wAcc: 69.64989686525578
test acc: 79.35
Epoche: [137/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [137][0/196]	Time 0.165 (0.165)	Data 0.245 (0.245)	Loss 0.3720 (0.3720)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [137][64/196]	Time 0.126 (0.129)	Data 0.000 (0.004)	Loss 0.4201 (0.4134)	Acc@1 83.984 (85.511)	Acc@5 99.609 (99.435)
Epoch: [137][128/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.4286 (0.4178)	Acc@1 85.156 (85.332)	Acc@5 99.609 (99.443)
Epoch: [137][192/196]	Time 0.124 (0.127)	Data 0.000 (0.002)	Loss 0.4144 (0.4189)	Acc@1 87.500 (85.249)	Acc@5 100.000 (99.462)
after train
n1: 30 for:
wAcc: 73.08743225909797
test acc: 80.37
Epoche: [138/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [138][0/196]	Time 0.167 (0.167)	Data 0.274 (0.274)	Loss 0.4527 (0.4527)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [138][64/196]	Time 0.125 (0.128)	Data 0.000 (0.005)	Loss 0.3650 (0.4040)	Acc@1 87.109 (85.769)	Acc@5 99.219 (99.561)
Epoch: [138][128/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.4062 (0.4134)	Acc@1 84.375 (85.523)	Acc@5 100.000 (99.494)
Epoch: [138][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.3941 (0.4105)	Acc@1 87.500 (85.729)	Acc@5 99.609 (99.478)
after train
n1: 30 for:
wAcc: 73.41126816634082
test acc: 79.73
Epoche: [139/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [139][0/196]	Time 0.197 (0.197)	Data 0.289 (0.289)	Loss 0.4390 (0.4390)	Acc@1 83.594 (83.594)	Acc@5 99.609 (99.609)
Epoch: [139][64/196]	Time 0.125 (0.129)	Data 0.000 (0.005)	Loss 0.4177 (0.4021)	Acc@1 84.375 (85.745)	Acc@5 99.609 (99.495)
Epoch: [139][128/196]	Time 0.128 (0.128)	Data 0.000 (0.003)	Loss 0.3854 (0.4078)	Acc@1 86.328 (85.559)	Acc@5 99.219 (99.491)
Epoch: [139][192/196]	Time 0.125 (0.128)	Data 0.000 (0.002)	Loss 0.3731 (0.4155)	Acc@1 86.328 (85.357)	Acc@5 99.609 (99.484)
after train
n1: 30 for:
wAcc: 74.16732163759683
test acc: 80.49
Epoche: [140/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [140][0/196]	Time 0.184 (0.184)	Data 0.307 (0.307)	Loss 0.3549 (0.3549)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [140][64/196]	Time 0.130 (0.128)	Data 0.000 (0.005)	Loss 0.4468 (0.3952)	Acc@1 83.594 (86.382)	Acc@5 99.609 (99.507)
Epoch: [140][128/196]	Time 0.162 (0.131)	Data 0.000 (0.003)	Loss 0.3623 (0.3948)	Acc@1 87.109 (86.307)	Acc@5 99.609 (99.525)
Epoch: [140][192/196]	Time 0.171 (0.145)	Data 0.000 (0.002)	Loss 0.4079 (0.4031)	Acc@1 87.109 (86.018)	Acc@5 99.219 (99.494)
after train
n1: 30 for:
wAcc: 74.32948172334217
test acc: 80.88
Epoche: [141/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [141][0/196]	Time 0.189 (0.189)	Data 0.282 (0.282)	Loss 0.3227 (0.3227)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [141][64/196]	Time 0.124 (0.127)	Data 0.000 (0.005)	Loss 0.4331 (0.4003)	Acc@1 85.156 (85.799)	Acc@5 99.609 (99.555)
Epoch: [141][128/196]	Time 0.125 (0.131)	Data 0.000 (0.002)	Loss 0.3436 (0.3955)	Acc@1 87.109 (86.101)	Acc@5 99.609 (99.555)
Epoch: [141][192/196]	Time 0.126 (0.132)	Data 0.000 (0.002)	Loss 0.3512 (0.3956)	Acc@1 88.281 (86.142)	Acc@5 100.000 (99.559)
after train
n1: 30 for:
wAcc: 73.22986260793921
test acc: 79.82
Epoche: [142/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [142][0/196]	Time 0.184 (0.184)	Data 0.263 (0.263)	Loss 0.3497 (0.3497)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [142][64/196]	Time 0.125 (0.129)	Data 0.000 (0.004)	Loss 0.3869 (0.3795)	Acc@1 84.375 (86.695)	Acc@5 100.000 (99.549)
Epoch: [142][128/196]	Time 0.128 (0.128)	Data 0.000 (0.002)	Loss 0.3031 (0.3814)	Acc@1 86.719 (86.607)	Acc@5 100.000 (99.543)
Epoch: [142][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.3541 (0.3881)	Acc@1 86.719 (86.381)	Acc@5 100.000 (99.563)
after train
n1: 30 for:
wAcc: 75.15991857316453
test acc: 74.12
Epoche: [143/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [143][0/196]	Time 0.164 (0.164)	Data 0.274 (0.274)	Loss 0.3545 (0.3545)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [143][64/196]	Time 0.127 (0.132)	Data 0.000 (0.005)	Loss 0.3475 (0.3845)	Acc@1 88.672 (86.316)	Acc@5 99.609 (99.537)
Epoch: [143][128/196]	Time 0.129 (0.131)	Data 0.000 (0.002)	Loss 0.3332 (0.3892)	Acc@1 88.672 (86.262)	Acc@5 99.609 (99.512)
Epoch: [143][192/196]	Time 0.127 (0.130)	Data 0.000 (0.002)	Loss 0.4458 (0.3904)	Acc@1 86.328 (86.184)	Acc@5 98.047 (99.514)
after train
n1: 30 for:
wAcc: 75.6118030428173
test acc: 79.23
Epoche: [144/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [144][0/196]	Time 0.180 (0.180)	Data 0.298 (0.298)	Loss 0.3144 (0.3144)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [144][64/196]	Time 0.127 (0.132)	Data 0.000 (0.005)	Loss 0.5334 (0.3801)	Acc@1 83.594 (86.737)	Acc@5 98.438 (99.579)
Epoch: [144][128/196]	Time 0.152 (0.139)	Data 0.000 (0.003)	Loss 0.3633 (0.3754)	Acc@1 89.453 (86.849)	Acc@5 98.828 (99.540)
Epoch: [144][192/196]	Time 0.151 (0.144)	Data 0.000 (0.002)	Loss 0.4013 (0.3761)	Acc@1 85.156 (86.755)	Acc@5 100.000 (99.549)
after train
n1: 30 for:
wAcc: 76.02015458883496
test acc: 81.58
Epoche: [145/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [145][0/196]	Time 0.190 (0.190)	Data 0.301 (0.301)	Loss 0.3279 (0.3279)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [145][64/196]	Time 0.153 (0.135)	Data 0.000 (0.005)	Loss 0.3722 (0.3897)	Acc@1 86.719 (86.412)	Acc@5 99.609 (99.513)
Epoch: [145][128/196]	Time 0.153 (0.144)	Data 0.000 (0.003)	Loss 0.3952 (0.3780)	Acc@1 87.109 (86.779)	Acc@5 98.828 (99.552)
Epoch: [145][192/196]	Time 0.153 (0.147)	Data 0.000 (0.002)	Loss 0.3644 (0.3759)	Acc@1 87.500 (86.840)	Acc@5 100.000 (99.555)
after train
n1: 30 for:
wAcc: 75.45076909528152
test acc: 82.73
Epoche: [146/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [146][0/196]	Time 0.186 (0.186)	Data 0.357 (0.357)	Loss 0.3732 (0.3732)	Acc@1 88.281 (88.281)	Acc@5 98.438 (98.438)
Epoch: [146][64/196]	Time 0.148 (0.135)	Data 0.000 (0.006)	Loss 0.3669 (0.3557)	Acc@1 87.891 (87.524)	Acc@5 100.000 (99.627)
Epoch: [146][128/196]	Time 0.127 (0.136)	Data 0.000 (0.003)	Loss 0.4343 (0.3581)	Acc@1 84.375 (87.300)	Acc@5 99.219 (99.658)
Epoch: [146][192/196]	Time 0.127 (0.133)	Data 0.000 (0.002)	Loss 0.3969 (0.3647)	Acc@1 86.328 (87.069)	Acc@5 99.609 (99.617)
after train
n1: 30 for:
wAcc: 76.88751371354113
test acc: 81.75
Epoche: [147/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [147][0/196]	Time 0.202 (0.202)	Data 0.266 (0.266)	Loss 0.3393 (0.3393)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [147][64/196]	Time 0.127 (0.129)	Data 0.000 (0.004)	Loss 0.4072 (0.3600)	Acc@1 86.328 (87.254)	Acc@5 100.000 (99.657)
Epoch: [147][128/196]	Time 0.129 (0.129)	Data 0.000 (0.002)	Loss 0.4314 (0.3560)	Acc@1 85.156 (87.442)	Acc@5 98.828 (99.646)
Epoch: [147][192/196]	Time 0.126 (0.129)	Data 0.000 (0.002)	Loss 0.3295 (0.3632)	Acc@1 88.281 (87.261)	Acc@5 99.219 (99.603)
after train
n1: 30 for:
wAcc: 76.7863308369287
test acc: 81.19
Epoche: [148/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [148][0/196]	Time 0.211 (0.211)	Data 0.257 (0.257)	Loss 0.3565 (0.3565)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [148][64/196]	Time 0.147 (0.132)	Data 0.000 (0.004)	Loss 0.2898 (0.3578)	Acc@1 90.625 (87.428)	Acc@5 99.609 (99.651)
Epoch: [148][128/196]	Time 0.128 (0.129)	Data 0.000 (0.002)	Loss 0.3032 (0.3559)	Acc@1 91.016 (87.533)	Acc@5 100.000 (99.676)
Epoch: [148][192/196]	Time 0.129 (0.129)	Data 0.000 (0.002)	Loss 0.4030 (0.3572)	Acc@1 85.938 (87.528)	Acc@5 100.000 (99.646)
after train
n1: 30 for:
wAcc: 78.27319068112989
test acc: 77.66
Epoche: [149/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [149][0/196]	Time 0.157 (0.157)	Data 0.368 (0.368)	Loss 0.3161 (0.3161)	Acc@1 88.672 (88.672)	Acc@5 99.609 (99.609)
Epoch: [149][64/196]	Time 0.152 (0.152)	Data 0.000 (0.006)	Loss 0.2905 (0.3499)	Acc@1 87.500 (87.560)	Acc@5 100.000 (99.621)
Epoch: [149][128/196]	Time 0.154 (0.152)	Data 0.000 (0.003)	Loss 0.2853 (0.3522)	Acc@1 89.062 (87.600)	Acc@5 100.000 (99.664)
Epoch: [149][192/196]	Time 0.125 (0.149)	Data 0.000 (0.002)	Loss 0.4356 (0.3547)	Acc@1 83.594 (87.486)	Acc@5 99.609 (99.644)
after train
n1: 30 for:
wAcc: 77.840422556324
test acc: 80.9
Epoche: [150/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [150][0/196]	Time 0.161 (0.161)	Data 0.349 (0.349)	Loss 0.3956 (0.3956)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [150][64/196]	Time 0.128 (0.129)	Data 0.000 (0.006)	Loss 0.3151 (0.3238)	Acc@1 89.453 (88.612)	Acc@5 99.219 (99.633)
Epoch: [150][128/196]	Time 0.126 (0.128)	Data 0.000 (0.003)	Loss 0.4508 (0.3389)	Acc@1 83.594 (88.166)	Acc@5 99.609 (99.612)
Epoch: [150][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.2562 (0.3427)	Acc@1 91.016 (88.048)	Acc@5 100.000 (99.638)
after train
n1: 30 for:
wAcc: 78.33272022623996
test acc: 79.8
Epoche: [151/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [151][0/196]	Time 0.201 (0.201)	Data 0.287 (0.287)	Loss 0.3232 (0.3232)	Acc@1 89.453 (89.453)	Acc@5 99.609 (99.609)
Epoch: [151][64/196]	Time 0.169 (0.167)	Data 0.000 (0.005)	Loss 0.3223 (0.3387)	Acc@1 89.453 (88.323)	Acc@5 100.000 (99.651)
Epoch: [151][128/196]	Time 0.174 (0.169)	Data 0.000 (0.003)	Loss 0.2894 (0.3376)	Acc@1 90.234 (88.278)	Acc@5 100.000 (99.643)
Epoch: [151][192/196]	Time 0.171 (0.170)	Data 0.000 (0.002)	Loss 0.3562 (0.3403)	Acc@1 87.891 (88.156)	Acc@5 99.609 (99.630)
after train
n1: 30 for:
wAcc: 78.33341842524234
test acc: 82.41
Epoche: [152/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [152][0/196]	Time 0.172 (0.172)	Data 0.300 (0.300)	Loss 0.3199 (0.3199)	Acc@1 88.672 (88.672)	Acc@5 99.609 (99.609)
Epoch: [152][64/196]	Time 0.125 (0.138)	Data 0.000 (0.005)	Loss 0.3943 (0.3251)	Acc@1 85.547 (88.672)	Acc@5 100.000 (99.639)
Epoch: [152][128/196]	Time 0.126 (0.132)	Data 0.000 (0.003)	Loss 0.4627 (0.3323)	Acc@1 83.594 (88.287)	Acc@5 99.219 (99.664)
Epoch: [152][192/196]	Time 0.125 (0.130)	Data 0.000 (0.002)	Loss 0.4379 (0.3362)	Acc@1 83.203 (88.188)	Acc@5 99.219 (99.660)
after train
n1: 30 for:
wAcc: 78.2061074835741
test acc: 81.18
Epoche: [153/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [153][0/196]	Time 0.164 (0.164)	Data 0.295 (0.295)	Loss 0.3696 (0.3696)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [153][64/196]	Time 0.128 (0.130)	Data 0.000 (0.005)	Loss 0.3660 (0.3217)	Acc@1 89.062 (88.534)	Acc@5 99.609 (99.669)
Epoch: [153][128/196]	Time 0.127 (0.128)	Data 0.000 (0.003)	Loss 0.3178 (0.3268)	Acc@1 89.062 (88.381)	Acc@5 99.219 (99.670)
Epoch: [153][192/196]	Time 0.126 (0.128)	Data 0.000 (0.002)	Loss 0.3488 (0.3291)	Acc@1 87.500 (88.338)	Acc@5 99.609 (99.640)
after train
n1: 30 for:
wAcc: 77.80237790105106
test acc: 83.87
Epoche: [154/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [154][0/196]	Time 0.167 (0.167)	Data 0.299 (0.299)	Loss 0.2849 (0.2849)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [154][64/196]	Time 0.126 (0.128)	Data 0.000 (0.005)	Loss 0.3359 (0.3303)	Acc@1 88.672 (88.239)	Acc@5 100.000 (99.681)
Epoch: [154][128/196]	Time 0.127 (0.127)	Data 0.000 (0.003)	Loss 0.3286 (0.3282)	Acc@1 88.672 (88.281)	Acc@5 99.609 (99.685)
Epoch: [154][192/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.3810 (0.3281)	Acc@1 86.719 (88.257)	Acc@5 99.609 (99.646)
after train
n1: 30 for:
wAcc: 77.7081105589696
test acc: 83.26
Epoche: [155/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [155][0/196]	Time 0.188 (0.188)	Data 0.312 (0.312)	Loss 0.2908 (0.2908)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [155][64/196]	Time 0.127 (0.129)	Data 0.000 (0.005)	Loss 0.3411 (0.3198)	Acc@1 87.500 (88.750)	Acc@5 100.000 (99.681)
Epoch: [155][128/196]	Time 0.127 (0.128)	Data 0.000 (0.003)	Loss 0.3652 (0.3232)	Acc@1 87.109 (88.620)	Acc@5 99.219 (99.655)
Epoch: [155][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.4067 (0.3277)	Acc@1 85.547 (88.494)	Acc@5 100.000 (99.644)
after train
n1: 30 for:
wAcc: 79.0247400990405
test acc: 81.44
Epoche: [156/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [156][0/196]	Time 0.169 (0.169)	Data 0.282 (0.282)	Loss 0.2691 (0.2691)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [156][64/196]	Time 0.127 (0.130)	Data 0.000 (0.005)	Loss 0.2854 (0.3090)	Acc@1 90.625 (89.405)	Acc@5 100.000 (99.730)
Epoch: [156][128/196]	Time 0.128 (0.129)	Data 0.000 (0.002)	Loss 0.3720 (0.3140)	Acc@1 85.547 (89.023)	Acc@5 99.609 (99.706)
Epoch: [156][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.4347 (0.3175)	Acc@1 83.594 (88.822)	Acc@5 99.609 (99.715)
after train
n1: 30 for:
wAcc: 79.55208652057158
test acc: 80.67
Epoche: [157/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [157][0/196]	Time 0.199 (0.199)	Data 0.263 (0.263)	Loss 0.2862 (0.2862)	Acc@1 91.016 (91.016)	Acc@5 99.219 (99.219)
Epoch: [157][64/196]	Time 0.128 (0.131)	Data 0.000 (0.004)	Loss 0.3547 (0.3133)	Acc@1 89.453 (89.099)	Acc@5 99.609 (99.675)
Epoch: [157][128/196]	Time 0.128 (0.129)	Data 0.000 (0.002)	Loss 0.3360 (0.3137)	Acc@1 87.891 (89.023)	Acc@5 99.609 (99.730)
Epoch: [157][192/196]	Time 0.154 (0.137)	Data 0.000 (0.002)	Loss 0.3359 (0.3151)	Acc@1 89.453 (88.969)	Acc@5 99.219 (99.694)
after train
n1: 30 for:
wAcc: 80.10849118762107
test acc: 83.35
Epoche: [158/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [158][0/196]	Time 0.159 (0.159)	Data 0.365 (0.365)	Loss 0.2158 (0.2158)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [158][64/196]	Time 0.127 (0.128)	Data 0.000 (0.006)	Loss 0.3027 (0.3191)	Acc@1 87.500 (88.780)	Acc@5 100.000 (99.778)
Epoch: [158][128/196]	Time 0.153 (0.131)	Data 0.000 (0.003)	Loss 0.2919 (0.3178)	Acc@1 89.062 (88.878)	Acc@5 100.000 (99.737)
Epoch: [158][192/196]	Time 0.154 (0.137)	Data 0.000 (0.002)	Loss 0.2550 (0.3191)	Acc@1 90.234 (88.773)	Acc@5 99.609 (99.719)
after train
n1: 30 for:
wAcc: 80.02705205836142
test acc: 82.23
Epoche: [159/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [159][0/196]	Time 0.160 (0.160)	Data 0.367 (0.367)	Loss 0.3924 (0.3924)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [159][64/196]	Time 0.128 (0.129)	Data 0.000 (0.006)	Loss 0.3167 (0.3139)	Acc@1 86.328 (88.876)	Acc@5 99.609 (99.724)
Epoch: [159][128/196]	Time 0.128 (0.128)	Data 0.000 (0.003)	Loss 0.2817 (0.3091)	Acc@1 91.406 (88.953)	Acc@5 100.000 (99.743)
Epoch: [159][192/196]	Time 0.126 (0.128)	Data 0.000 (0.002)	Loss 0.3092 (0.3094)	Acc@1 88.672 (89.014)	Acc@5 100.000 (99.737)
after train
n1: 30 for:
wAcc: 80.43227976619768
test acc: 83.72
Epoche: [160/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [160][0/196]	Time 0.159 (0.159)	Data 0.257 (0.257)	Loss 0.2272 (0.2272)	Acc@1 94.531 (94.531)	Acc@5 99.219 (99.219)
Epoch: [160][64/196]	Time 0.125 (0.129)	Data 0.000 (0.004)	Loss 0.3158 (0.2890)	Acc@1 88.672 (89.657)	Acc@5 100.000 (99.736)
Epoch: [160][128/196]	Time 0.152 (0.134)	Data 0.000 (0.002)	Loss 0.2660 (0.2981)	Acc@1 91.797 (89.353)	Acc@5 100.000 (99.770)
Epoch: [160][192/196]	Time 0.125 (0.133)	Data 0.000 (0.002)	Loss 0.3475 (0.3005)	Acc@1 88.672 (89.423)	Acc@5 99.609 (99.741)
after train
n1: 30 for:
wAcc: 79.57897207214329
test acc: 81.11
Epoche: [161/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [161][0/196]	Time 0.141 (0.141)	Data 0.348 (0.348)	Loss 0.2838 (0.2838)	Acc@1 90.234 (90.234)	Acc@5 99.609 (99.609)
Epoch: [161][64/196]	Time 0.154 (0.151)	Data 0.000 (0.006)	Loss 0.2718 (0.2986)	Acc@1 89.453 (89.447)	Acc@5 99.609 (99.718)
Epoch: [161][128/196]	Time 0.126 (0.142)	Data 0.000 (0.003)	Loss 0.3225 (0.2963)	Acc@1 87.891 (89.568)	Acc@5 99.609 (99.740)
Epoch: [161][192/196]	Time 0.175 (0.145)	Data 0.000 (0.002)	Loss 0.3143 (0.2991)	Acc@1 89.844 (89.441)	Acc@5 100.000 (99.733)
after train
n1: 30 for:
wAcc: 80.89061953275699
test acc: 83.8
Epoche: [162/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [162][0/196]	Time 0.166 (0.166)	Data 0.301 (0.301)	Loss 0.2387 (0.2387)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [162][64/196]	Time 0.126 (0.127)	Data 0.000 (0.005)	Loss 0.3407 (0.2902)	Acc@1 89.062 (89.898)	Acc@5 100.000 (99.760)
Epoch: [162][128/196]	Time 0.125 (0.127)	Data 0.000 (0.003)	Loss 0.2946 (0.2952)	Acc@1 87.500 (89.553)	Acc@5 99.609 (99.761)
Epoch: [162][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.3023 (0.2977)	Acc@1 90.234 (89.552)	Acc@5 100.000 (99.741)
after train
n1: 30 for:
wAcc: 80.21673461721778
test acc: 83.42
Epoche: [163/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [163][0/196]	Time 0.233 (0.233)	Data 0.295 (0.295)	Loss 0.2327 (0.2327)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [163][64/196]	Time 0.127 (0.152)	Data 0.000 (0.005)	Loss 0.2850 (0.2865)	Acc@1 89.062 (89.808)	Acc@5 99.609 (99.772)
Epoch: [163][128/196]	Time 0.127 (0.142)	Data 0.000 (0.003)	Loss 0.2571 (0.2879)	Acc@1 91.797 (89.756)	Acc@5 100.000 (99.743)
Epoch: [163][192/196]	Time 0.127 (0.140)	Data 0.000 (0.002)	Loss 0.2969 (0.2913)	Acc@1 88.281 (89.660)	Acc@5 99.609 (99.739)
after train
n1: 30 for:
wAcc: 81.42376287609895
test acc: 77.0
Epoche: [164/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [164][0/196]	Time 0.182 (0.182)	Data 0.294 (0.294)	Loss 0.2563 (0.2563)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [164][64/196]	Time 0.125 (0.135)	Data 0.000 (0.005)	Loss 0.2145 (0.2937)	Acc@1 91.797 (89.615)	Acc@5 100.000 (99.796)
Epoch: [164][128/196]	Time 0.127 (0.131)	Data 0.000 (0.003)	Loss 0.3008 (0.2921)	Acc@1 89.844 (89.692)	Acc@5 100.000 (99.788)
Epoch: [164][192/196]	Time 0.127 (0.129)	Data 0.000 (0.002)	Loss 0.3153 (0.2928)	Acc@1 89.844 (89.629)	Acc@5 99.609 (99.779)
after train
n1: 30 for:
wAcc: 80.54710205045288
test acc: 82.52
Epoche: [165/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [165][0/196]	Time 0.156 (0.156)	Data 0.357 (0.357)	Loss 0.2662 (0.2662)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [165][64/196]	Time 0.154 (0.153)	Data 0.000 (0.006)	Loss 0.2337 (0.2901)	Acc@1 90.234 (89.549)	Acc@5 100.000 (99.790)
Epoch: [165][128/196]	Time 0.153 (0.153)	Data 0.000 (0.003)	Loss 0.1908 (0.2920)	Acc@1 93.359 (89.635)	Acc@5 100.000 (99.767)
Epoch: [165][192/196]	Time 0.156 (0.154)	Data 0.000 (0.002)	Loss 0.3095 (0.2919)	Acc@1 88.281 (89.629)	Acc@5 100.000 (99.761)
after train
n1: 30 for:
wAcc: 81.15288454360741
test acc: 83.11
Epoche: [166/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [166][0/196]	Time 0.166 (0.166)	Data 0.298 (0.298)	Loss 0.2240 (0.2240)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [166][64/196]	Time 0.127 (0.128)	Data 0.000 (0.005)	Loss 0.3072 (0.2704)	Acc@1 89.453 (90.397)	Acc@5 100.000 (99.814)
Epoch: [166][128/196]	Time 0.126 (0.127)	Data 0.000 (0.003)	Loss 0.2813 (0.2722)	Acc@1 89.453 (90.319)	Acc@5 99.219 (99.794)
Epoch: [166][192/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.3380 (0.2758)	Acc@1 87.891 (90.297)	Acc@5 99.609 (99.794)
after train
n1: 30 for:
wAcc: 81.42660284531085
test acc: 83.61
Epoche: [167/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [167][0/196]	Time 0.168 (0.168)	Data 0.414 (0.414)	Loss 0.1861 (0.1861)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [167][64/196]	Time 0.153 (0.150)	Data 0.000 (0.007)	Loss 0.2561 (0.2736)	Acc@1 89.453 (90.451)	Acc@5 100.000 (99.766)
Epoch: [167][128/196]	Time 0.126 (0.138)	Data 0.000 (0.004)	Loss 0.2054 (0.2730)	Acc@1 91.016 (90.413)	Acc@5 99.609 (99.767)
Epoch: [167][192/196]	Time 0.167 (0.137)	Data 0.000 (0.002)	Loss 0.3206 (0.2779)	Acc@1 90.234 (90.216)	Acc@5 99.609 (99.751)
after train
n1: 30 for:
wAcc: 81.47494778123595
test acc: 84.38
Epoche: [168/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [168][0/196]	Time 0.166 (0.166)	Data 0.287 (0.287)	Loss 0.2606 (0.2606)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [168][64/196]	Time 0.153 (0.138)	Data 0.000 (0.005)	Loss 0.2700 (0.2761)	Acc@1 90.625 (90.204)	Acc@5 100.000 (99.802)
Epoch: [168][128/196]	Time 0.155 (0.146)	Data 0.000 (0.003)	Loss 0.2241 (0.2771)	Acc@1 92.188 (90.198)	Acc@5 99.609 (99.782)
Epoch: [168][192/196]	Time 0.126 (0.146)	Data 0.000 (0.002)	Loss 0.2446 (0.2756)	Acc@1 91.797 (90.277)	Acc@5 100.000 (99.781)
after train
n1: 30 for:
wAcc: 81.77223728853183
test acc: 83.57
Epoche: [169/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [169][0/196]	Time 0.204 (0.204)	Data 0.287 (0.287)	Loss 0.2148 (0.2148)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [169][64/196]	Time 0.148 (0.130)	Data 0.000 (0.005)	Loss 0.3408 (0.2805)	Acc@1 90.234 (90.168)	Acc@5 100.000 (99.802)
Epoch: [169][128/196]	Time 0.126 (0.138)	Data 0.000 (0.003)	Loss 0.2258 (0.2756)	Acc@1 92.188 (90.362)	Acc@5 99.609 (99.800)
Epoch: [169][192/196]	Time 0.155 (0.140)	Data 0.000 (0.002)	Loss 0.3193 (0.2751)	Acc@1 89.453 (90.315)	Acc@5 100.000 (99.779)
after train
n1: 30 for:
wAcc: 81.94460098691903
test acc: 81.82
Epoche: [170/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [170][0/196]	Time 0.168 (0.168)	Data 0.291 (0.291)	Loss 0.3358 (0.3358)	Acc@1 88.672 (88.672)	Acc@5 99.219 (99.219)
Epoch: [170][64/196]	Time 0.170 (0.138)	Data 0.000 (0.005)	Loss 0.3383 (0.2647)	Acc@1 91.016 (90.829)	Acc@5 100.000 (99.766)
Epoch: [170][128/196]	Time 0.173 (0.155)	Data 0.000 (0.003)	Loss 0.2599 (0.2703)	Acc@1 91.797 (90.589)	Acc@5 99.609 (99.761)
Epoch: [170][192/196]	Time 0.176 (0.161)	Data 0.000 (0.002)	Loss 0.2957 (0.2717)	Acc@1 87.109 (90.457)	Acc@5 100.000 (99.792)
after train
n1: 30 for:
wAcc: 81.78332696289192
test acc: 83.15
Epoche: [171/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [171][0/196]	Time 0.185 (0.185)	Data 0.236 (0.236)	Loss 0.3178 (0.3178)	Acc@1 90.234 (90.234)	Acc@5 100.000 (100.000)
Epoch: [171][64/196]	Time 0.174 (0.162)	Data 0.000 (0.004)	Loss 0.2606 (0.2632)	Acc@1 90.625 (90.571)	Acc@5 99.609 (99.826)
Epoch: [171][128/196]	Time 0.171 (0.167)	Data 0.000 (0.002)	Loss 0.2966 (0.2686)	Acc@1 89.062 (90.404)	Acc@5 99.609 (99.803)
Epoch: [171][192/196]	Time 0.173 (0.169)	Data 0.000 (0.002)	Loss 0.3037 (0.2702)	Acc@1 91.406 (90.408)	Acc@5 99.609 (99.810)
after train
n1: 30 for:
wAcc: 81.04749854061424
test acc: 83.15
Epoche: [172/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [172][0/196]	Time 0.170 (0.170)	Data 0.275 (0.275)	Loss 0.2595 (0.2595)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [172][64/196]	Time 0.126 (0.129)	Data 0.000 (0.005)	Loss 0.1862 (0.2706)	Acc@1 92.188 (90.355)	Acc@5 100.000 (99.832)
Epoch: [172][128/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.2652 (0.2681)	Acc@1 88.672 (90.498)	Acc@5 99.609 (99.840)
Epoch: [172][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.3497 (0.2668)	Acc@1 89.453 (90.585)	Acc@5 100.000 (99.818)
after train
n1: 30 for:
wAcc: 81.9218533535698
test acc: 84.99
Epoche: [173/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [173][0/196]	Time 0.182 (0.182)	Data 0.293 (0.293)	Loss 0.2695 (0.2695)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [173][64/196]	Time 0.157 (0.153)	Data 0.000 (0.005)	Loss 0.2090 (0.2550)	Acc@1 92.578 (90.871)	Acc@5 100.000 (99.838)
Epoch: [173][128/196]	Time 0.155 (0.153)	Data 0.000 (0.003)	Loss 0.2965 (0.2568)	Acc@1 91.797 (90.776)	Acc@5 99.609 (99.818)
Epoch: [173][192/196]	Time 0.126 (0.145)	Data 0.000 (0.002)	Loss 0.2683 (0.2632)	Acc@1 91.016 (90.595)	Acc@5 100.000 (99.804)
after train
n1: 30 for:
wAcc: 82.45951795802164
test acc: 83.84
Epoche: [174/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [174][0/196]	Time 0.165 (0.165)	Data 0.297 (0.297)	Loss 0.2339 (0.2339)	Acc@1 92.188 (92.188)	Acc@5 99.609 (99.609)
Epoch: [174][64/196]	Time 0.126 (0.131)	Data 0.000 (0.005)	Loss 0.2382 (0.2567)	Acc@1 90.625 (90.781)	Acc@5 100.000 (99.856)
Epoch: [174][128/196]	Time 0.127 (0.129)	Data 0.000 (0.003)	Loss 0.3155 (0.2506)	Acc@1 86.328 (90.904)	Acc@5 100.000 (99.849)
Epoch: [174][192/196]	Time 0.126 (0.128)	Data 0.000 (0.002)	Loss 0.3437 (0.2563)	Acc@1 87.891 (90.718)	Acc@5 99.609 (99.854)
after train
n1: 30 for:
wAcc: 82.714827106398
test acc: 84.51
Epoche: [175/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [175][0/196]	Time 0.196 (0.196)	Data 0.273 (0.273)	Loss 0.2136 (0.2136)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [175][64/196]	Time 0.127 (0.130)	Data 0.000 (0.005)	Loss 0.3449 (0.2548)	Acc@1 89.453 (91.070)	Acc@5 99.219 (99.820)
Epoch: [175][128/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.1846 (0.2513)	Acc@1 92.969 (91.079)	Acc@5 100.000 (99.821)
Epoch: [175][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.2727 (0.2538)	Acc@1 90.625 (90.961)	Acc@5 100.000 (99.844)
after train
n1: 30 for:
wAcc: 82.68897438633878
test acc: 84.17
Epoche: [176/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [176][0/196]	Time 0.195 (0.195)	Data 0.290 (0.290)	Loss 0.1913 (0.1913)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [176][64/196]	Time 0.127 (0.130)	Data 0.000 (0.005)	Loss 0.2102 (0.2541)	Acc@1 92.969 (91.004)	Acc@5 99.609 (99.790)
Epoch: [176][128/196]	Time 0.126 (0.131)	Data 0.000 (0.003)	Loss 0.1971 (0.2499)	Acc@1 92.969 (90.997)	Acc@5 99.609 (99.800)
Epoch: [176][192/196]	Time 0.128 (0.129)	Data 0.000 (0.002)	Loss 0.1727 (0.2525)	Acc@1 93.359 (90.957)	Acc@5 100.000 (99.810)
after train
n1: 30 for:
wAcc: 82.70356995387381
test acc: 84.8
Epoche: [177/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [177][0/196]	Time 0.204 (0.204)	Data 0.279 (0.279)	Loss 0.2022 (0.2022)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [177][64/196]	Time 0.156 (0.157)	Data 0.000 (0.005)	Loss 0.1655 (0.2357)	Acc@1 93.359 (91.647)	Acc@5 100.000 (99.868)
Epoch: [177][128/196]	Time 0.128 (0.145)	Data 0.000 (0.002)	Loss 0.2778 (0.2434)	Acc@1 91.016 (91.321)	Acc@5 100.000 (99.846)
Epoch: [177][192/196]	Time 0.176 (0.147)	Data 0.000 (0.002)	Loss 0.2353 (0.2488)	Acc@1 90.625 (91.123)	Acc@5 100.000 (99.840)
after train
n1: 30 for:
wAcc: 82.32852120816926
test acc: 82.13
Epoche: [178/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [178][0/196]	Time 0.160 (0.160)	Data 0.368 (0.368)	Loss 0.2045 (0.2045)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [178][64/196]	Time 0.126 (0.128)	Data 0.000 (0.006)	Loss 0.2653 (0.2431)	Acc@1 89.453 (91.376)	Acc@5 99.609 (99.880)
Epoch: [178][128/196]	Time 0.127 (0.127)	Data 0.000 (0.003)	Loss 0.2139 (0.2471)	Acc@1 92.188 (91.237)	Acc@5 99.609 (99.852)
Epoch: [178][192/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.2976 (0.2487)	Acc@1 89.844 (91.198)	Acc@5 99.609 (99.838)
after train
n1: 30 for:
wAcc: 82.78409283375444
test acc: 84.98
Epoche: [179/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [179][0/196]	Time 0.190 (0.190)	Data 0.294 (0.294)	Loss 0.2322 (0.2322)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [179][64/196]	Time 0.154 (0.155)	Data 0.000 (0.005)	Loss 0.1904 (0.2398)	Acc@1 92.578 (91.538)	Acc@5 100.000 (99.850)
Epoch: [179][128/196]	Time 0.152 (0.154)	Data 0.000 (0.003)	Loss 0.2499 (0.2386)	Acc@1 91.406 (91.570)	Acc@5 100.000 (99.888)
Epoch: [179][192/196]	Time 0.153 (0.154)	Data 0.000 (0.002)	Loss 0.3057 (0.2426)	Acc@1 90.234 (91.412)	Acc@5 100.000 (99.866)
after train
n1: 30 for:
wAcc: 82.76674655086761
test acc: 85.86
Epoche: [180/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [180][0/196]	Time 0.182 (0.182)	Data 0.285 (0.285)	Loss 0.3330 (0.3330)	Acc@1 89.062 (89.062)	Acc@5 99.609 (99.609)
Epoch: [180][64/196]	Time 0.127 (0.129)	Data 0.000 (0.005)	Loss 0.2312 (0.2395)	Acc@1 92.188 (91.478)	Acc@5 99.609 (99.802)
Epoch: [180][128/196]	Time 0.128 (0.128)	Data 0.000 (0.003)	Loss 0.2003 (0.2363)	Acc@1 92.969 (91.552)	Acc@5 100.000 (99.815)
Epoch: [180][192/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.2629 (0.2384)	Acc@1 90.625 (91.509)	Acc@5 99.609 (99.818)
after train
n1: 30 for:
wAcc: 83.34361695392533
test acc: 84.14
Epoche: [181/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [181][0/196]	Time 0.208 (0.208)	Data 0.356 (0.356)	Loss 0.2749 (0.2749)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [181][64/196]	Time 0.171 (0.170)	Data 0.000 (0.006)	Loss 0.1685 (0.2309)	Acc@1 94.922 (91.605)	Acc@5 99.609 (99.880)
Epoch: [181][128/196]	Time 0.169 (0.167)	Data 0.000 (0.003)	Loss 0.2830 (0.2401)	Acc@1 91.406 (91.434)	Acc@5 99.609 (99.870)
Epoch: [181][192/196]	Time 0.126 (0.166)	Data 0.000 (0.002)	Loss 0.2368 (0.2419)	Acc@1 90.234 (91.386)	Acc@5 100.000 (99.854)
after train
n1: 30 for:
wAcc: 83.21718578987621
test acc: 83.89
Epoche: [182/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [182][0/196]	Time 0.162 (0.162)	Data 0.304 (0.304)	Loss 0.1761 (0.1761)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [182][64/196]	Time 0.126 (0.127)	Data 0.000 (0.005)	Loss 0.2157 (0.2320)	Acc@1 92.188 (91.887)	Acc@5 99.219 (99.826)
Epoch: [182][128/196]	Time 0.126 (0.127)	Data 0.000 (0.003)	Loss 0.1855 (0.2326)	Acc@1 93.750 (91.733)	Acc@5 99.219 (99.809)
Epoch: [182][192/196]	Time 0.152 (0.132)	Data 0.000 (0.002)	Loss 0.2471 (0.2374)	Acc@1 91.797 (91.594)	Acc@5 99.609 (99.814)
after train
n1: 30 for:
wAcc: 83.64946374725473
test acc: 83.69
Epoche: [183/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [183][0/196]	Time 0.174 (0.174)	Data 0.288 (0.288)	Loss 0.1323 (0.1323)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [183][64/196]	Time 0.173 (0.152)	Data 0.000 (0.005)	Loss 0.2269 (0.2232)	Acc@1 93.359 (92.049)	Acc@5 99.609 (99.880)
Epoch: [183][128/196]	Time 0.170 (0.162)	Data 0.000 (0.003)	Loss 0.2135 (0.2307)	Acc@1 92.188 (91.927)	Acc@5 99.219 (99.836)
Epoch: [183][192/196]	Time 0.173 (0.166)	Data 0.000 (0.002)	Loss 0.3172 (0.2328)	Acc@1 88.672 (91.799)	Acc@5 100.000 (99.836)
after train
n1: 30 for:
wAcc: 83.56389643944917
test acc: 85.53
Epoche: [184/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [184][0/196]	Time 0.217 (0.217)	Data 0.278 (0.278)	Loss 0.2096 (0.2096)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [184][64/196]	Time 0.126 (0.130)	Data 0.000 (0.005)	Loss 0.1786 (0.2258)	Acc@1 93.750 (92.097)	Acc@5 100.000 (99.874)
Epoch: [184][128/196]	Time 0.126 (0.128)	Data 0.000 (0.002)	Loss 0.2179 (0.2250)	Acc@1 91.797 (92.103)	Acc@5 100.000 (99.876)
Epoch: [184][192/196]	Time 0.122 (0.127)	Data 0.000 (0.002)	Loss 0.2410 (0.2311)	Acc@1 89.844 (91.900)	Acc@5 99.609 (99.858)
after train
n1: 30 for:
wAcc: 83.42763979627033
test acc: 84.91
Epoche: [185/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [185][0/196]	Time 0.204 (0.204)	Data 0.412 (0.412)	Loss 0.2908 (0.2908)	Acc@1 88.672 (88.672)	Acc@5 99.609 (99.609)
Epoch: [185][64/196]	Time 0.125 (0.128)	Data 0.000 (0.007)	Loss 0.1823 (0.2214)	Acc@1 94.531 (92.230)	Acc@5 100.000 (99.826)
Epoch: [185][128/196]	Time 0.173 (0.144)	Data 0.000 (0.004)	Loss 0.2798 (0.2268)	Acc@1 89.844 (92.088)	Acc@5 99.609 (99.846)
Epoch: [185][192/196]	Time 0.175 (0.154)	Data 0.000 (0.003)	Loss 0.2194 (0.2246)	Acc@1 90.625 (92.048)	Acc@5 99.609 (99.856)
after train
n1: 30 for:
wAcc: 83.41196353936938
test acc: 84.48
Epoche: [186/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [186][0/196]	Time 0.157 (0.157)	Data 0.370 (0.370)	Loss 0.1156 (0.1156)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [186][64/196]	Time 0.127 (0.129)	Data 0.000 (0.006)	Loss 0.2136 (0.2229)	Acc@1 91.797 (91.977)	Acc@5 100.000 (99.880)
Epoch: [186][128/196]	Time 0.127 (0.128)	Data 0.000 (0.003)	Loss 0.2429 (0.2204)	Acc@1 92.188 (91.994)	Acc@5 100.000 (99.873)
Epoch: [186][192/196]	Time 0.168 (0.136)	Data 0.000 (0.002)	Loss 0.2358 (0.2219)	Acc@1 92.578 (92.022)	Acc@5 100.000 (99.870)
after train
n1: 30 for:
wAcc: 83.86829409088564
test acc: 85.55
Epoche: [187/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [187][0/196]	Time 0.213 (0.213)	Data 0.276 (0.276)	Loss 0.1693 (0.1693)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [187][64/196]	Time 0.154 (0.137)	Data 0.000 (0.005)	Loss 0.2335 (0.2180)	Acc@1 92.969 (92.308)	Acc@5 100.000 (99.904)
Epoch: [187][128/196]	Time 0.126 (0.136)	Data 0.000 (0.002)	Loss 0.2050 (0.2206)	Acc@1 92.578 (92.188)	Acc@5 100.000 (99.879)
Epoch: [187][192/196]	Time 0.126 (0.133)	Data 0.000 (0.002)	Loss 0.2120 (0.2222)	Acc@1 91.797 (92.107)	Acc@5 100.000 (99.872)
after train
n1: 30 for:
wAcc: 83.81488230220029
test acc: 84.04
Epoche: [188/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [188][0/196]	Time 0.177 (0.177)	Data 0.282 (0.282)	Loss 0.1958 (0.1958)	Acc@1 92.969 (92.969)	Acc@5 99.609 (99.609)
Epoch: [188][64/196]	Time 0.126 (0.129)	Data 0.000 (0.005)	Loss 0.2677 (0.2130)	Acc@1 90.625 (92.542)	Acc@5 99.609 (99.874)
Epoch: [188][128/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.2404 (0.2162)	Acc@1 91.406 (92.354)	Acc@5 99.609 (99.885)
Epoch: [188][192/196]	Time 0.126 (0.128)	Data 0.000 (0.002)	Loss 0.1774 (0.2195)	Acc@1 94.531 (92.252)	Acc@5 100.000 (99.860)
after train
n1: 30 for:
wAcc: 84.04480274493093
test acc: 84.83
Epoche: [189/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [189][0/196]	Time 0.163 (0.163)	Data 0.354 (0.354)	Loss 0.1944 (0.1944)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [189][64/196]	Time 0.127 (0.128)	Data 0.000 (0.006)	Loss 0.1415 (0.2138)	Acc@1 95.703 (92.500)	Acc@5 100.000 (99.838)
Epoch: [189][128/196]	Time 0.162 (0.129)	Data 0.000 (0.003)	Loss 0.2343 (0.2184)	Acc@1 92.188 (92.309)	Acc@5 100.000 (99.861)
Epoch: [189][192/196]	Time 0.175 (0.143)	Data 0.000 (0.002)	Loss 0.2329 (0.2187)	Acc@1 92.578 (92.268)	Acc@5 100.000 (99.872)
after train
n1: 30 for:
wAcc: 83.71815496795071
test acc: 85.64
Epoche: [190/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [190][0/196]	Time 0.197 (0.197)	Data 0.241 (0.241)	Loss 0.1915 (0.1915)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [190][64/196]	Time 0.125 (0.128)	Data 0.000 (0.004)	Loss 0.2051 (0.2135)	Acc@1 93.359 (92.530)	Acc@5 100.000 (99.880)
Epoch: [190][128/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.2086 (0.2126)	Acc@1 92.969 (92.457)	Acc@5 100.000 (99.882)
Epoch: [190][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.3111 (0.2166)	Acc@1 89.844 (92.339)	Acc@5 100.000 (99.891)
after train
n1: 30 for:
wAcc: 84.23101555900189
test acc: 85.35
Epoche: [191/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [191][0/196]	Time 0.190 (0.190)	Data 0.286 (0.286)	Loss 0.3015 (0.3015)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [191][64/196]	Time 0.126 (0.128)	Data 0.000 (0.005)	Loss 0.1952 (0.2050)	Acc@1 92.969 (92.734)	Acc@5 100.000 (99.880)
Epoch: [191][128/196]	Time 0.127 (0.128)	Data 0.000 (0.003)	Loss 0.2655 (0.2091)	Acc@1 88.281 (92.660)	Acc@5 100.000 (99.879)
Epoch: [191][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.1622 (0.2112)	Acc@1 94.531 (92.487)	Acc@5 100.000 (99.872)
after train
n1: 30 for:
wAcc: 84.24827471183012
test acc: 84.64
Epoche: [192/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [192][0/196]	Time 0.167 (0.167)	Data 0.274 (0.274)	Loss 0.2266 (0.2266)	Acc@1 91.797 (91.797)	Acc@5 99.219 (99.219)
Epoch: [192][64/196]	Time 0.174 (0.158)	Data 0.000 (0.005)	Loss 0.1819 (0.1962)	Acc@1 91.406 (92.879)	Acc@5 100.000 (99.880)
Epoch: [192][128/196]	Time 0.176 (0.166)	Data 0.000 (0.003)	Loss 0.2089 (0.2042)	Acc@1 92.969 (92.624)	Acc@5 100.000 (99.870)
Epoch: [192][192/196]	Time 0.174 (0.169)	Data 0.000 (0.002)	Loss 0.2249 (0.2083)	Acc@1 91.797 (92.554)	Acc@5 100.000 (99.887)
after train
n1: 30 for:
wAcc: 83.34546211356732
test acc: 85.06
Epoche: [193/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [193][0/196]	Time 0.204 (0.204)	Data 0.282 (0.282)	Loss 0.2123 (0.2123)	Acc@1 91.016 (91.016)	Acc@5 100.000 (100.000)
Epoch: [193][64/196]	Time 0.126 (0.128)	Data 0.000 (0.005)	Loss 0.2662 (0.2085)	Acc@1 91.406 (92.548)	Acc@5 99.609 (99.874)
Epoch: [193][128/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.1970 (0.2090)	Acc@1 93.359 (92.617)	Acc@5 100.000 (99.894)
Epoch: [193][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.1813 (0.2108)	Acc@1 92.578 (92.495)	Acc@5 100.000 (99.899)
after train
n1: 30 for:
wAcc: 84.25405725706003
test acc: 83.99
Epoche: [194/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [194][0/196]	Time 0.160 (0.160)	Data 0.365 (0.365)	Loss 0.1772 (0.1772)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [194][64/196]	Time 0.173 (0.149)	Data 0.000 (0.006)	Loss 0.1429 (0.1974)	Acc@1 93.750 (92.897)	Acc@5 100.000 (99.904)
Epoch: [194][128/196]	Time 0.171 (0.161)	Data 0.000 (0.003)	Loss 0.1483 (0.2051)	Acc@1 94.531 (92.624)	Acc@5 100.000 (99.894)
Epoch: [194][192/196]	Time 0.173 (0.165)	Data 0.000 (0.002)	Loss 0.1802 (0.2078)	Acc@1 91.406 (92.594)	Acc@5 100.000 (99.887)
after train
n1: 30 for:
wAcc: 84.32231262376497
test acc: 85.75
Epoche: [195/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [195][0/196]	Time 0.158 (0.158)	Data 0.369 (0.369)	Loss 0.2050 (0.2050)	Acc@1 93.359 (93.359)	Acc@5 99.609 (99.609)
Epoch: [195][64/196]	Time 0.127 (0.127)	Data 0.000 (0.006)	Loss 0.1428 (0.1986)	Acc@1 94.531 (92.837)	Acc@5 100.000 (99.904)
Epoch: [195][128/196]	Time 0.126 (0.127)	Data 0.000 (0.003)	Loss 0.1495 (0.1984)	Acc@1 93.750 (92.826)	Acc@5 100.000 (99.918)
Epoch: [195][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.1771 (0.2042)	Acc@1 93.359 (92.689)	Acc@5 100.000 (99.891)
after train
n1: 30 for:
wAcc: 84.48670226536935
test acc: 85.6
Epoche: [196/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [196][0/196]	Time 0.184 (0.184)	Data 0.281 (0.281)	Loss 0.1519 (0.1519)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [196][64/196]	Time 0.163 (0.147)	Data 0.000 (0.005)	Loss 0.1800 (0.1943)	Acc@1 92.578 (92.951)	Acc@5 100.000 (99.886)
Epoch: [196][128/196]	Time 0.169 (0.158)	Data 0.000 (0.003)	Loss 0.2590 (0.2011)	Acc@1 92.188 (92.799)	Acc@5 99.609 (99.885)
Epoch: [196][192/196]	Time 0.166 (0.161)	Data 0.000 (0.002)	Loss 0.1273 (0.2022)	Acc@1 94.531 (92.776)	Acc@5 100.000 (99.895)
after train
n1: 30 for:
wAcc: 84.66984032474515
test acc: 84.15
Epoche: [197/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [197][0/196]	Time 0.157 (0.157)	Data 0.358 (0.358)	Loss 0.2491 (0.2491)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [197][64/196]	Time 0.127 (0.127)	Data 0.000 (0.006)	Loss 0.1903 (0.2024)	Acc@1 93.359 (92.674)	Acc@5 100.000 (99.898)
Epoch: [197][128/196]	Time 0.126 (0.127)	Data 0.000 (0.003)	Loss 0.1951 (0.2041)	Acc@1 94.922 (92.578)	Acc@5 100.000 (99.888)
Epoch: [197][192/196]	Time 0.126 (0.126)	Data 0.000 (0.002)	Loss 0.2849 (0.2043)	Acc@1 91.016 (92.621)	Acc@5 100.000 (99.895)
after train
n1: 30 for:
wAcc: 84.51920737791096
test acc: 83.52
Epoche: [198/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [198][0/196]	Time 0.206 (0.206)	Data 0.359 (0.359)	Loss 0.2719 (0.2719)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [198][64/196]	Time 0.172 (0.173)	Data 0.000 (0.006)	Loss 0.1932 (0.1908)	Acc@1 92.578 (93.029)	Acc@5 100.000 (99.922)
Epoch: [198][128/196]	Time 0.154 (0.157)	Data 0.000 (0.003)	Loss 0.1698 (0.1908)	Acc@1 93.750 (93.105)	Acc@5 100.000 (99.894)
Epoch: [198][192/196]	Time 0.156 (0.156)	Data 0.000 (0.002)	Loss 0.2239 (0.1942)	Acc@1 93.750 (93.003)	Acc@5 100.000 (99.903)
after train
n1: 30 for:
wAcc: 84.20175966061258
test acc: 85.42
Epoche: [199/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [199][0/196]	Time 0.162 (0.162)	Data 0.350 (0.350)	Loss 0.1014 (0.1014)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [199][64/196]	Time 0.128 (0.128)	Data 0.000 (0.006)	Loss 0.1469 (0.1923)	Acc@1 96.484 (93.131)	Acc@5 100.000 (99.898)
Epoch: [199][128/196]	Time 0.126 (0.127)	Data 0.000 (0.003)	Loss 0.2021 (0.1936)	Acc@1 91.797 (93.120)	Acc@5 100.000 (99.894)
Epoch: [199][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.2177 (0.1967)	Acc@1 91.016 (92.949)	Acc@5 99.609 (99.903)
after train
n1: 30 for:
wAcc: 84.47262268267389
test acc: 85.71
Epoche: [200/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [200][0/196]	Time 0.175 (0.175)	Data 0.262 (0.262)	Loss 0.1905 (0.1905)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [200][64/196]	Time 0.126 (0.127)	Data 0.000 (0.004)	Loss 0.2105 (0.1916)	Acc@1 92.578 (93.035)	Acc@5 100.000 (99.904)
Epoch: [200][128/196]	Time 0.170 (0.132)	Data 0.000 (0.002)	Loss 0.2472 (0.1906)	Acc@1 91.797 (93.051)	Acc@5 100.000 (99.909)
Epoch: [200][192/196]	Time 0.174 (0.145)	Data 0.000 (0.002)	Loss 0.1803 (0.1942)	Acc@1 94.141 (92.898)	Acc@5 100.000 (99.909)
after train
n1: 30 for:
wAcc: 84.5524534773401
test acc: 85.67
Epoche: [201/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [201][0/196]	Time 0.177 (0.177)	Data 0.294 (0.294)	Loss 0.1809 (0.1809)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [201][64/196]	Time 0.153 (0.151)	Data 0.000 (0.005)	Loss 0.1791 (0.1867)	Acc@1 94.922 (93.263)	Acc@5 100.000 (99.940)
Epoch: [201][128/196]	Time 0.126 (0.147)	Data 0.000 (0.003)	Loss 0.1806 (0.1867)	Acc@1 93.750 (93.371)	Acc@5 100.000 (99.906)
Epoch: [201][192/196]	Time 0.124 (0.140)	Data 0.000 (0.002)	Loss 0.1477 (0.1875)	Acc@1 96.484 (93.386)	Acc@5 99.609 (99.895)
after train
n1: 30 for:
wAcc: 84.89054651832254
test acc: 85.74
Epoche: [202/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [202][0/196]	Time 0.170 (0.170)	Data 0.310 (0.310)	Loss 0.1324 (0.1324)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [202][64/196]	Time 0.126 (0.130)	Data 0.000 (0.005)	Loss 0.1898 (0.1891)	Acc@1 92.969 (93.167)	Acc@5 100.000 (99.922)
Epoch: [202][128/196]	Time 0.126 (0.128)	Data 0.000 (0.003)	Loss 0.1837 (0.1898)	Acc@1 93.750 (93.269)	Acc@5 100.000 (99.897)
Epoch: [202][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.1903 (0.1902)	Acc@1 94.141 (93.197)	Acc@5 100.000 (99.903)
after train
n1: 30 for:
wAcc: 84.779104177924
test acc: 85.41
Epoche: [203/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [203][0/196]	Time 0.160 (0.160)	Data 0.367 (0.367)	Loss 0.1178 (0.1178)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [203][64/196]	Time 0.127 (0.127)	Data 0.000 (0.006)	Loss 0.2000 (0.1729)	Acc@1 92.578 (93.816)	Acc@5 100.000 (99.934)
Epoch: [203][128/196]	Time 0.126 (0.126)	Data 0.000 (0.003)	Loss 0.2374 (0.1792)	Acc@1 92.969 (93.689)	Acc@5 99.219 (99.897)
Epoch: [203][192/196]	Time 0.127 (0.126)	Data 0.000 (0.002)	Loss 0.2109 (0.1842)	Acc@1 92.188 (93.471)	Acc@5 99.609 (99.895)
after train
n1: 30 for:
wAcc: 84.91666337753975
test acc: 86.61
Epoche: [204/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [204][0/196]	Time 0.195 (0.195)	Data 0.295 (0.295)	Loss 0.1623 (0.1623)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [204][64/196]	Time 0.125 (0.128)	Data 0.000 (0.005)	Loss 0.1956 (0.1850)	Acc@1 92.969 (93.191)	Acc@5 99.609 (99.928)
Epoch: [204][128/196]	Time 0.126 (0.128)	Data 0.000 (0.003)	Loss 0.2151 (0.1859)	Acc@1 92.188 (93.208)	Acc@5 100.000 (99.918)
Epoch: [204][192/196]	Time 0.128 (0.128)	Data 0.000 (0.002)	Loss 0.2506 (0.1893)	Acc@1 90.625 (93.139)	Acc@5 100.000 (99.913)
after train
n1: 30 for:
wAcc: 84.97675997210683
test acc: 85.58
Epoche: [205/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [205][0/196]	Time 0.192 (0.192)	Data 0.268 (0.268)	Loss 0.1530 (0.1530)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [205][64/196]	Time 0.127 (0.129)	Data 0.000 (0.004)	Loss 0.1530 (0.1862)	Acc@1 93.750 (93.281)	Acc@5 100.000 (99.910)
Epoch: [205][128/196]	Time 0.129 (0.129)	Data 0.000 (0.002)	Loss 0.2663 (0.1830)	Acc@1 93.750 (93.441)	Acc@5 99.219 (99.921)
Epoch: [205][192/196]	Time 0.127 (0.129)	Data 0.000 (0.002)	Loss 0.1940 (0.1854)	Acc@1 92.969 (93.268)	Acc@5 100.000 (99.927)
after train
n1: 30 for:
wAcc: 85.10675246464683
test acc: 86.5
Epoche: [206/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [206][0/196]	Time 0.226 (0.226)	Data 0.342 (0.342)	Loss 0.2419 (0.2419)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [206][64/196]	Time 0.125 (0.130)	Data 0.000 (0.006)	Loss 0.1935 (0.1799)	Acc@1 92.188 (93.690)	Acc@5 100.000 (99.928)
Epoch: [206][128/196]	Time 0.126 (0.129)	Data 0.000 (0.003)	Loss 0.1338 (0.1769)	Acc@1 96.875 (93.759)	Acc@5 100.000 (99.933)
Epoch: [206][192/196]	Time 0.128 (0.129)	Data 0.000 (0.002)	Loss 0.1617 (0.1809)	Acc@1 95.312 (93.558)	Acc@5 100.000 (99.925)
after train
n1: 30 for:
wAcc: 84.81066004457297
test acc: 86.98
Epoche: [207/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [207][0/196]	Time 0.159 (0.159)	Data 0.357 (0.357)	Loss 0.1393 (0.1393)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [207][64/196]	Time 0.128 (0.129)	Data 0.000 (0.006)	Loss 0.1127 (0.1755)	Acc@1 96.094 (93.630)	Acc@5 100.000 (99.904)
Epoch: [207][128/196]	Time 0.126 (0.128)	Data 0.000 (0.003)	Loss 0.1799 (0.1826)	Acc@1 94.531 (93.459)	Acc@5 100.000 (99.888)
Epoch: [207][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.1779 (0.1841)	Acc@1 94.531 (93.452)	Acc@5 99.609 (99.891)
after train
n1: 30 for:
wAcc: 85.36261789919448
test acc: 85.7
Epoche: [208/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [208][0/196]	Time 0.242 (0.242)	Data 0.284 (0.284)	Loss 0.1984 (0.1984)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [208][64/196]	Time 0.128 (0.165)	Data 0.000 (0.005)	Loss 0.2336 (0.1849)	Acc@1 92.578 (93.299)	Acc@5 100.000 (99.928)
Epoch: [208][128/196]	Time 0.172 (0.156)	Data 0.000 (0.003)	Loss 0.2161 (0.1805)	Acc@1 93.359 (93.396)	Acc@5 99.219 (99.933)
Epoch: [208][192/196]	Time 0.173 (0.162)	Data 0.000 (0.002)	Loss 0.2040 (0.1803)	Acc@1 93.359 (93.465)	Acc@5 99.609 (99.933)
after train
n1: 30 for:
wAcc: 85.51159865671698
test acc: 85.04
Epoche: [209/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [209][0/196]	Time 0.176 (0.176)	Data 0.277 (0.277)	Loss 0.1604 (0.1604)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [209][64/196]	Time 0.126 (0.129)	Data 0.000 (0.005)	Loss 0.1457 (0.1661)	Acc@1 93.359 (94.093)	Acc@5 100.000 (99.952)
Epoch: [209][128/196]	Time 0.126 (0.128)	Data 0.000 (0.002)	Loss 0.1813 (0.1750)	Acc@1 92.578 (93.702)	Acc@5 100.000 (99.936)
Epoch: [209][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.2151 (0.1791)	Acc@1 92.188 (93.540)	Acc@5 99.609 (99.923)
after train
n1: 30 for:
wAcc: 85.23252705847096
test acc: 85.87
Epoche: [210/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [210][0/196]	Time 0.190 (0.190)	Data 0.270 (0.270)	Loss 0.1654 (0.1654)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [210][64/196]	Time 0.152 (0.147)	Data 0.000 (0.004)	Loss 0.1748 (0.1715)	Acc@1 93.750 (93.684)	Acc@5 99.609 (99.910)
Epoch: [210][128/196]	Time 0.155 (0.150)	Data 0.000 (0.002)	Loss 0.1420 (0.1752)	Acc@1 94.922 (93.683)	Acc@5 100.000 (99.918)
Epoch: [210][192/196]	Time 0.153 (0.151)	Data 0.000 (0.002)	Loss 0.2159 (0.1766)	Acc@1 92.578 (93.641)	Acc@5 100.000 (99.921)
after train
n1: 30 for:
wAcc: 85.23751395571053
test acc: 83.86
Epoche: [211/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [211][0/196]	Time 0.162 (0.162)	Data 0.292 (0.292)	Loss 0.1776 (0.1776)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [211][64/196]	Time 0.125 (0.128)	Data 0.000 (0.005)	Loss 0.1536 (0.1695)	Acc@1 93.359 (93.918)	Acc@5 100.000 (99.892)
Epoch: [211][128/196]	Time 0.126 (0.127)	Data 0.000 (0.003)	Loss 0.2076 (0.1721)	Acc@1 91.406 (93.789)	Acc@5 100.000 (99.921)
Epoch: [211][192/196]	Time 0.125 (0.127)	Data 0.000 (0.002)	Loss 0.1929 (0.1753)	Acc@1 94.141 (93.687)	Acc@5 100.000 (99.929)
after train
n1: 30 for:
wAcc: 85.11972977615159
test acc: 85.73
Epoche: [212/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [212][0/196]	Time 0.211 (0.211)	Data 0.364 (0.364)	Loss 0.1668 (0.1668)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [212][64/196]	Time 0.126 (0.170)	Data 0.000 (0.006)	Loss 0.2127 (0.1765)	Acc@1 93.359 (93.534)	Acc@5 100.000 (99.946)
Epoch: [212][128/196]	Time 0.125 (0.148)	Data 0.000 (0.003)	Loss 0.1352 (0.1730)	Acc@1 95.312 (93.720)	Acc@5 100.000 (99.952)
Epoch: [212][192/196]	Time 0.126 (0.141)	Data 0.000 (0.002)	Loss 0.1863 (0.1721)	Acc@1 92.578 (93.718)	Acc@5 100.000 (99.949)
after train
n1: 30 for:
wAcc: 85.42509531398491
test acc: 85.33
Epoche: [213/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [213][0/196]	Time 0.167 (0.167)	Data 0.325 (0.325)	Loss 0.1157 (0.1157)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [213][64/196]	Time 0.125 (0.127)	Data 0.000 (0.005)	Loss 0.1901 (0.1697)	Acc@1 92.969 (93.780)	Acc@5 100.000 (99.898)
Epoch: [213][128/196]	Time 0.127 (0.127)	Data 0.000 (0.003)	Loss 0.1886 (0.1676)	Acc@1 92.578 (93.929)	Acc@5 100.000 (99.930)
Epoch: [213][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.1591 (0.1689)	Acc@1 94.141 (93.912)	Acc@5 100.000 (99.923)
after train
n1: 30 for:
wAcc: 85.32933196694685
test acc: 86.76
Epoche: [214/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [214][0/196]	Time 0.162 (0.162)	Data 0.360 (0.360)	Loss 0.1228 (0.1228)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [214][64/196]	Time 0.127 (0.129)	Data 0.000 (0.006)	Loss 0.1777 (0.1697)	Acc@1 93.750 (94.050)	Acc@5 99.609 (99.934)
Epoch: [214][128/196]	Time 0.155 (0.135)	Data 0.000 (0.003)	Loss 0.1507 (0.1656)	Acc@1 94.141 (94.122)	Acc@5 100.000 (99.933)
Epoch: [214][192/196]	Time 0.153 (0.141)	Data 0.000 (0.002)	Loss 0.1447 (0.1659)	Acc@1 93.750 (94.056)	Acc@5 100.000 (99.933)
after train
n1: 30 for:
wAcc: 85.35947166075522
test acc: 85.76
Epoche: [215/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [215][0/196]	Time 0.190 (0.190)	Data 0.226 (0.226)	Loss 0.1235 (0.1235)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [215][64/196]	Time 0.126 (0.129)	Data 0.000 (0.004)	Loss 0.2500 (0.1667)	Acc@1 91.797 (94.147)	Acc@5 100.000 (99.928)
Epoch: [215][128/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.1657 (0.1675)	Acc@1 94.922 (94.086)	Acc@5 100.000 (99.930)
Epoch: [215][192/196]	Time 0.127 (0.127)	Data 0.000 (0.001)	Loss 0.1136 (0.1679)	Acc@1 95.703 (94.064)	Acc@5 100.000 (99.935)
after train
n1: 30 for:
wAcc: 85.53999306502094
test acc: 82.76
Epoche: [216/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [216][0/196]	Time 0.158 (0.158)	Data 0.367 (0.367)	Loss 0.1328 (0.1328)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [216][64/196]	Time 0.126 (0.127)	Data 0.000 (0.006)	Loss 0.1301 (0.1682)	Acc@1 94.531 (94.129)	Acc@5 100.000 (99.928)
Epoch: [216][128/196]	Time 0.128 (0.128)	Data 0.000 (0.003)	Loss 0.1663 (0.1651)	Acc@1 92.969 (94.150)	Acc@5 100.000 (99.933)
Epoch: [216][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.1231 (0.1655)	Acc@1 96.484 (94.122)	Acc@5 99.609 (99.937)
after train
n1: 30 for:
wAcc: 85.14235072229245
test acc: 85.54
Epoche: [217/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [217][0/196]	Time 0.193 (0.193)	Data 0.287 (0.287)	Loss 0.1663 (0.1663)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [217][64/196]	Time 0.169 (0.172)	Data 0.000 (0.005)	Loss 0.0979 (0.1595)	Acc@1 95.703 (93.990)	Acc@5 100.000 (99.940)
Epoch: [217][128/196]	Time 0.170 (0.172)	Data 0.000 (0.003)	Loss 0.1459 (0.1603)	Acc@1 95.703 (94.044)	Acc@5 100.000 (99.958)
Epoch: [217][192/196]	Time 0.170 (0.171)	Data 0.000 (0.002)	Loss 0.1613 (0.1630)	Acc@1 93.750 (93.969)	Acc@5 99.609 (99.953)
after train
n1: 30 for:
wAcc: 85.28220914462453
test acc: 85.35
Epoche: [218/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [218][0/196]	Time 0.173 (0.173)	Data 0.285 (0.285)	Loss 0.1774 (0.1774)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [218][64/196]	Time 0.126 (0.139)	Data 0.000 (0.005)	Loss 0.1725 (0.1487)	Acc@1 94.141 (94.537)	Acc@5 100.000 (99.946)
Epoch: [218][128/196]	Time 0.126 (0.133)	Data 0.000 (0.003)	Loss 0.1746 (0.1546)	Acc@1 93.750 (94.401)	Acc@5 100.000 (99.939)
Epoch: [218][192/196]	Time 0.126 (0.131)	Data 0.000 (0.002)	Loss 0.1689 (0.1606)	Acc@1 94.141 (94.167)	Acc@5 99.219 (99.927)
after train
n1: 30 for:
wAcc: 85.4036776095639
test acc: 86.07
Epoche: [219/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [219][0/196]	Time 0.238 (0.238)	Data 0.296 (0.296)	Loss 0.1997 (0.1997)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [219][64/196]	Time 0.127 (0.138)	Data 0.000 (0.005)	Loss 0.1201 (0.1586)	Acc@1 95.703 (94.459)	Acc@5 100.000 (99.946)
Epoch: [219][128/196]	Time 0.159 (0.133)	Data 0.000 (0.003)	Loss 0.2395 (0.1609)	Acc@1 90.625 (94.298)	Acc@5 99.609 (99.927)
Epoch: [219][192/196]	Time 0.170 (0.144)	Data 0.000 (0.002)	Loss 0.1891 (0.1646)	Acc@1 93.750 (94.151)	Acc@5 100.000 (99.915)
after train
n1: 30 for:
wAcc: 85.40474329928192
test acc: 86.37
Epoche: [220/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [220][0/196]	Time 0.184 (0.184)	Data 0.264 (0.264)	Loss 0.1369 (0.1369)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [220][64/196]	Time 0.128 (0.129)	Data 0.000 (0.004)	Loss 0.1815 (0.1610)	Acc@1 92.188 (94.249)	Acc@5 100.000 (99.934)
Epoch: [220][128/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.1727 (0.1584)	Acc@1 94.141 (94.316)	Acc@5 100.000 (99.945)
Epoch: [220][192/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.1876 (0.1594)	Acc@1 95.312 (94.313)	Acc@5 100.000 (99.939)
after train
n1: 30 for:
wAcc: 85.36437921949224
test acc: 86.16
Epoche: [221/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [221][0/196]	Time 0.224 (0.224)	Data 0.303 (0.303)	Loss 0.1666 (0.1666)	Acc@1 92.578 (92.578)	Acc@5 100.000 (100.000)
Epoch: [221][64/196]	Time 0.168 (0.172)	Data 0.000 (0.005)	Loss 0.1129 (0.1498)	Acc@1 95.312 (94.591)	Acc@5 100.000 (99.958)
Epoch: [221][128/196]	Time 0.126 (0.154)	Data 0.000 (0.003)	Loss 0.1390 (0.1500)	Acc@1 95.703 (94.559)	Acc@5 100.000 (99.949)
Epoch: [221][192/196]	Time 0.126 (0.144)	Data 0.000 (0.002)	Loss 0.2330 (0.1528)	Acc@1 91.016 (94.458)	Acc@5 100.000 (99.947)
after train
n1: 30 for:
wAcc: 85.47642544647024
test acc: 85.49
Epoche: [222/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [222][0/196]	Time 0.209 (0.209)	Data 0.286 (0.286)	Loss 0.1673 (0.1673)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [222][64/196]	Time 0.170 (0.169)	Data 0.000 (0.005)	Loss 0.1968 (0.1491)	Acc@1 92.578 (94.657)	Acc@5 100.000 (99.940)
Epoch: [222][128/196]	Time 0.127 (0.155)	Data 0.000 (0.003)	Loss 0.1627 (0.1561)	Acc@1 93.359 (94.389)	Acc@5 100.000 (99.939)
Epoch: [222][192/196]	Time 0.125 (0.145)	Data 0.000 (0.002)	Loss 0.1809 (0.1562)	Acc@1 94.922 (94.343)	Acc@5 100.000 (99.933)
after train
n1: 30 for:
wAcc: 85.32262035786742
test acc: 87.47
Epoche: [223/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [223][0/196]	Time 0.197 (0.197)	Data 0.262 (0.262)	Loss 0.0995 (0.0995)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [223][64/196]	Time 0.150 (0.136)	Data 0.000 (0.004)	Loss 0.2184 (0.1508)	Acc@1 91.797 (94.627)	Acc@5 99.609 (99.922)
Epoch: [223][128/196]	Time 0.126 (0.141)	Data 0.000 (0.002)	Loss 0.1055 (0.1528)	Acc@1 96.875 (94.453)	Acc@5 99.609 (99.933)
Epoch: [223][192/196]	Time 0.125 (0.136)	Data 0.000 (0.002)	Loss 0.2395 (0.1558)	Acc@1 92.188 (94.361)	Acc@5 100.000 (99.939)
after train
n1: 30 for:
wAcc: 85.71558932068803
test acc: 86.62
Epoche: [224/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [224][0/196]	Time 0.169 (0.169)	Data 0.284 (0.284)	Loss 0.1227 (0.1227)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [224][64/196]	Time 0.125 (0.128)	Data 0.000 (0.005)	Loss 0.1421 (0.1573)	Acc@1 96.094 (94.327)	Acc@5 100.000 (99.934)
Epoch: [224][128/196]	Time 0.126 (0.127)	Data 0.000 (0.003)	Loss 0.1382 (0.1518)	Acc@1 94.922 (94.595)	Acc@5 99.609 (99.942)
Epoch: [224][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.1118 (0.1531)	Acc@1 95.312 (94.515)	Acc@5 99.609 (99.951)
after train
n1: 30 for:
wAcc: 85.75225416318624
test acc: 86.37
Epoche: [225/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [225][0/196]	Time 0.210 (0.210)	Data 0.238 (0.238)	Loss 0.1339 (0.1339)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [225][64/196]	Time 0.126 (0.129)	Data 0.000 (0.004)	Loss 0.2400 (0.1471)	Acc@1 91.406 (94.808)	Acc@5 100.000 (99.934)
Epoch: [225][128/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.1251 (0.1462)	Acc@1 95.703 (94.707)	Acc@5 100.000 (99.936)
Epoch: [225][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.1403 (0.1498)	Acc@1 94.922 (94.604)	Acc@5 99.609 (99.929)
after train
n1: 30 for:
wAcc: 85.58249447530103
test acc: 86.56
Epoche: [226/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [226][0/196]	Time 0.194 (0.194)	Data 0.289 (0.289)	Loss 0.1900 (0.1900)	Acc@1 93.750 (93.750)	Acc@5 99.609 (99.609)
Epoch: [226][64/196]	Time 0.125 (0.128)	Data 0.000 (0.005)	Loss 0.0607 (0.1475)	Acc@1 99.219 (94.597)	Acc@5 100.000 (99.970)
Epoch: [226][128/196]	Time 0.127 (0.127)	Data 0.000 (0.003)	Loss 0.0864 (0.1468)	Acc@1 96.875 (94.671)	Acc@5 100.000 (99.964)
Epoch: [226][192/196]	Time 0.125 (0.127)	Data 0.000 (0.002)	Loss 0.2065 (0.1485)	Acc@1 92.578 (94.669)	Acc@5 100.000 (99.962)
after train
n1: 30 for:
wAcc: 85.5544855667992
test acc: 85.18
Epoche: [227/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [227][0/196]	Time 0.181 (0.181)	Data 0.278 (0.278)	Loss 0.1282 (0.1282)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [227][64/196]	Time 0.126 (0.146)	Data 0.000 (0.005)	Loss 0.1423 (0.1391)	Acc@1 94.141 (94.964)	Acc@5 100.000 (99.958)
Epoch: [227][128/196]	Time 0.127 (0.136)	Data 0.000 (0.002)	Loss 0.1308 (0.1403)	Acc@1 94.922 (94.952)	Acc@5 100.000 (99.961)
Epoch: [227][192/196]	Time 0.127 (0.136)	Data 0.000 (0.002)	Loss 0.0861 (0.1447)	Acc@1 96.875 (94.754)	Acc@5 100.000 (99.960)
after train
n1: 30 for:
wAcc: 85.80499216641245
test acc: 85.99
Epoche: [228/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [228][0/196]	Time 0.155 (0.155)	Data 0.356 (0.356)	Loss 0.1425 (0.1425)	Acc@1 94.922 (94.922)	Acc@5 100.000 (100.000)
Epoch: [228][64/196]	Time 0.127 (0.116)	Data 0.000 (0.006)	Loss 0.1297 (0.1427)	Acc@1 95.312 (94.850)	Acc@5 100.000 (99.940)
Epoch: [228][128/196]	Time 0.126 (0.121)	Data 0.000 (0.003)	Loss 0.1974 (0.1445)	Acc@1 92.188 (94.807)	Acc@5 100.000 (99.945)
Epoch: [228][192/196]	Time 0.129 (0.123)	Data 0.000 (0.002)	Loss 0.1005 (0.1469)	Acc@1 95.703 (94.709)	Acc@5 100.000 (99.945)
after train
n1: 30 for:
wAcc: 85.85885100727661
test acc: 83.07
Epoche: [229/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [229][0/196]	Time 0.167 (0.167)	Data 0.252 (0.252)	Loss 0.1911 (0.1911)	Acc@1 91.797 (91.797)	Acc@5 100.000 (100.000)
Epoch: [229][64/196]	Time 0.128 (0.128)	Data 0.000 (0.004)	Loss 0.1384 (0.1368)	Acc@1 94.531 (94.928)	Acc@5 100.000 (99.970)
Epoch: [229][128/196]	Time 0.124 (0.136)	Data 0.000 (0.002)	Loss 0.2115 (0.1446)	Acc@1 92.969 (94.813)	Acc@5 99.219 (99.964)
Epoch: [229][192/196]	Time 0.125 (0.135)	Data 0.000 (0.002)	Loss 0.2349 (0.1444)	Acc@1 93.359 (94.750)	Acc@5 99.609 (99.960)
after train
n1: 30 for:
wAcc: 85.67314267354968
test acc: 86.31
Epoche: [230/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [230][0/196]	Time 0.159 (0.159)	Data 0.295 (0.295)	Loss 0.1503 (0.1503)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [230][64/196]	Time 0.152 (0.145)	Data 0.000 (0.005)	Loss 0.1372 (0.1391)	Acc@1 93.359 (94.928)	Acc@5 100.000 (99.934)
Epoch: [230][128/196]	Time 0.154 (0.150)	Data 0.000 (0.003)	Loss 0.1013 (0.1441)	Acc@1 97.266 (94.767)	Acc@5 100.000 (99.942)
Epoch: [230][192/196]	Time 0.152 (0.151)	Data 0.000 (0.002)	Loss 0.1644 (0.1462)	Acc@1 92.969 (94.653)	Acc@5 100.000 (99.945)
after train
n1: 30 for:
wAcc: 85.72434955200508
test acc: 86.43
Epoche: [231/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [231][0/196]	Time 0.178 (0.178)	Data 0.290 (0.290)	Loss 0.1185 (0.1185)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [231][64/196]	Time 0.127 (0.129)	Data 0.000 (0.005)	Loss 0.1732 (0.1432)	Acc@1 94.141 (94.832)	Acc@5 100.000 (99.964)
Epoch: [231][128/196]	Time 0.126 (0.128)	Data 0.000 (0.003)	Loss 0.1742 (0.1413)	Acc@1 92.188 (94.916)	Acc@5 100.000 (99.964)
Epoch: [231][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.1096 (0.1418)	Acc@1 96.094 (94.894)	Acc@5 100.000 (99.964)
after train
n1: 30 for:
wAcc: 85.72217007346944
test acc: 85.58
Epoche: [232/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [232][0/196]	Time 0.174 (0.174)	Data 0.271 (0.271)	Loss 0.1489 (0.1489)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [232][64/196]	Time 0.126 (0.129)	Data 0.000 (0.004)	Loss 0.1941 (0.1357)	Acc@1 93.750 (95.204)	Acc@5 100.000 (99.958)
Epoch: [232][128/196]	Time 0.126 (0.128)	Data 0.000 (0.002)	Loss 0.1039 (0.1354)	Acc@1 97.266 (95.176)	Acc@5 100.000 (99.952)
Epoch: [232][192/196]	Time 0.152 (0.129)	Data 0.000 (0.002)	Loss 0.1063 (0.1356)	Acc@1 95.312 (95.157)	Acc@5 100.000 (99.960)
after train
n1: 30 for:
wAcc: 85.88647167935648
test acc: 86.13
Epoche: [233/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [233][0/196]	Time 0.208 (0.208)	Data 0.368 (0.368)	Loss 0.0703 (0.0703)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [233][64/196]	Time 0.160 (0.159)	Data 0.000 (0.006)	Loss 0.1396 (0.1388)	Acc@1 95.703 (94.934)	Acc@5 99.609 (99.964)
Epoch: [233][128/196]	Time 0.126 (0.144)	Data 0.000 (0.003)	Loss 0.1166 (0.1343)	Acc@1 96.094 (95.125)	Acc@5 100.000 (99.949)
Epoch: [233][192/196]	Time 0.126 (0.142)	Data 0.000 (0.002)	Loss 0.1860 (0.1353)	Acc@1 94.141 (95.051)	Acc@5 99.609 (99.951)
after train
n1: 30 for:
wAcc: 85.7532847799539
test acc: 86.73
Epoche: [234/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [234][0/196]	Time 0.212 (0.212)	Data 0.342 (0.342)	Loss 0.1943 (0.1943)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [234][64/196]	Time 0.171 (0.172)	Data 0.000 (0.006)	Loss 0.1194 (0.1412)	Acc@1 96.094 (94.952)	Acc@5 100.000 (99.982)
Epoch: [234][128/196]	Time 0.125 (0.167)	Data 0.000 (0.003)	Loss 0.0766 (0.1413)	Acc@1 98.047 (94.867)	Acc@5 100.000 (99.979)
Epoch: [234][192/196]	Time 0.125 (0.153)	Data 0.000 (0.002)	Loss 0.1731 (0.1397)	Acc@1 94.141 (94.924)	Acc@5 100.000 (99.966)
after train
n1: 30 for:
wAcc: 85.94929529778169
test acc: 85.5
Epoche: [235/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [235][0/196]	Time 0.212 (0.212)	Data 0.308 (0.308)	Loss 0.1504 (0.1504)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [235][64/196]	Time 0.127 (0.129)	Data 0.000 (0.005)	Loss 0.1163 (0.1371)	Acc@1 95.703 (95.222)	Acc@5 100.000 (99.970)
Epoch: [235][128/196]	Time 0.127 (0.129)	Data 0.000 (0.003)	Loss 0.2029 (0.1356)	Acc@1 94.922 (95.258)	Acc@5 100.000 (99.976)
Epoch: [235][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.1395 (0.1362)	Acc@1 95.312 (95.173)	Acc@5 100.000 (99.962)
after train
n1: 30 for:
wAcc: 85.989698051853
test acc: 85.87
Epoche: [236/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [236][0/196]	Time 0.159 (0.159)	Data 0.367 (0.367)	Loss 0.1353 (0.1353)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [236][64/196]	Time 0.127 (0.128)	Data 0.000 (0.006)	Loss 0.1681 (0.1273)	Acc@1 94.141 (95.391)	Acc@5 100.000 (99.952)
Epoch: [236][128/196]	Time 0.125 (0.128)	Data 0.000 (0.003)	Loss 0.1544 (0.1327)	Acc@1 92.969 (95.222)	Acc@5 100.000 (99.949)
Epoch: [236][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.2037 (0.1321)	Acc@1 92.969 (95.240)	Acc@5 100.000 (99.951)
after train
n1: 30 for:
wAcc: 85.79693680362382
test acc: 87.03
Epoche: [237/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [237][0/196]	Time 0.165 (0.165)	Data 0.307 (0.307)	Loss 0.1152 (0.1152)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [237][64/196]	Time 0.127 (0.129)	Data 0.000 (0.005)	Loss 0.1700 (0.1253)	Acc@1 94.531 (95.511)	Acc@5 100.000 (99.958)
Epoch: [237][128/196]	Time 0.127 (0.128)	Data 0.000 (0.003)	Loss 0.0784 (0.1309)	Acc@1 96.875 (95.270)	Acc@5 100.000 (99.955)
Epoch: [237][192/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.0814 (0.1304)	Acc@1 98.047 (95.321)	Acc@5 100.000 (99.964)
after train
n1: 30 for:
wAcc: 85.78107864012584
test acc: 84.46
Epoche: [238/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [238][0/196]	Time 0.173 (0.173)	Data 0.269 (0.269)	Loss 0.1032 (0.1032)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [238][64/196]	Time 0.127 (0.129)	Data 0.000 (0.004)	Loss 0.1630 (0.1300)	Acc@1 93.359 (95.192)	Acc@5 100.000 (99.970)
Epoch: [238][128/196]	Time 0.127 (0.128)	Data 0.000 (0.002)	Loss 0.1183 (0.1338)	Acc@1 97.266 (95.070)	Acc@5 99.609 (99.964)
Epoch: [238][192/196]	Time 0.128 (0.128)	Data 0.000 (0.002)	Loss 0.1952 (0.1347)	Acc@1 92.188 (95.080)	Acc@5 99.609 (99.962)
after train
n1: 30 for:
wAcc: 85.81583385262932
test acc: 85.12
Epoche: [239/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [239][0/196]	Time 0.165 (0.165)	Data 0.273 (0.273)	Loss 0.1073 (0.1073)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [239][64/196]	Time 0.130 (0.129)	Data 0.000 (0.005)	Loss 0.1916 (0.1312)	Acc@1 93.359 (95.174)	Acc@5 100.000 (99.976)
Epoch: [239][128/196]	Time 0.127 (0.130)	Data 0.000 (0.002)	Loss 0.1165 (0.1289)	Acc@1 96.094 (95.285)	Acc@5 100.000 (99.970)
Epoch: [239][192/196]	Time 0.152 (0.131)	Data 0.000 (0.002)	Loss 0.1075 (0.1289)	Acc@1 96.094 (95.292)	Acc@5 100.000 (99.957)
after train
n1: 30 for:
wAcc: 85.48037261594985
test acc: 87.32
Epoche: [240/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [240][0/196]	Time 0.172 (0.172)	Data 0.291 (0.291)	Loss 0.1192 (0.1192)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [240][64/196]	Time 0.161 (0.135)	Data 0.000 (0.005)	Loss 0.1429 (0.1226)	Acc@1 95.703 (95.649)	Acc@5 100.000 (99.964)
Epoch: [240][128/196]	Time 0.124 (0.146)	Data 0.000 (0.003)	Loss 0.1360 (0.1262)	Acc@1 94.922 (95.449)	Acc@5 100.000 (99.982)
Epoch: [240][192/196]	Time 0.126 (0.139)	Data 0.000 (0.002)	Loss 0.1015 (0.1288)	Acc@1 95.703 (95.347)	Acc@5 100.000 (99.984)
after train
n1: 30 for:
wAcc: 85.86938836567475
test acc: 86.05
Epoche: [241/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [241][0/196]	Time 0.205 (0.205)	Data 0.291 (0.291)	Loss 0.1513 (0.1513)	Acc@1 95.312 (95.312)	Acc@5 99.609 (99.609)
Epoch: [241][64/196]	Time 0.127 (0.135)	Data 0.000 (0.005)	Loss 0.1087 (0.1272)	Acc@1 96.875 (95.439)	Acc@5 100.000 (99.958)
Epoch: [241][128/196]	Time 0.127 (0.131)	Data 0.000 (0.003)	Loss 0.1439 (0.1287)	Acc@1 94.141 (95.297)	Acc@5 100.000 (99.973)
Epoch: [241][192/196]	Time 0.175 (0.143)	Data 0.000 (0.002)	Loss 0.0827 (0.1301)	Acc@1 97.266 (95.250)	Acc@5 100.000 (99.974)
after train
n1: 30 for:
wAcc: 85.82321610628244
test acc: 85.25
Epoche: [242/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [242][0/196]	Time 0.223 (0.223)	Data 0.233 (0.233)	Loss 0.1287 (0.1287)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [242][64/196]	Time 0.126 (0.147)	Data 0.000 (0.004)	Loss 0.0875 (0.1316)	Acc@1 95.703 (95.282)	Acc@5 100.000 (99.988)
Epoch: [242][128/196]	Time 0.126 (0.137)	Data 0.000 (0.002)	Loss 0.0977 (0.1317)	Acc@1 96.484 (95.252)	Acc@5 100.000 (99.967)
Epoch: [242][192/196]	Time 0.125 (0.133)	Data 0.000 (0.002)	Loss 0.1148 (0.1316)	Acc@1 95.312 (95.264)	Acc@5 100.000 (99.966)
after train
n1: 30 for:
wAcc: 85.99295744886356
test acc: 86.7
Epoche: [243/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [243][0/196]	Time 0.158 (0.158)	Data 0.358 (0.358)	Loss 0.1435 (0.1435)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [243][64/196]	Time 0.124 (0.127)	Data 0.000 (0.006)	Loss 0.0689 (0.1202)	Acc@1 97.266 (95.583)	Acc@5 100.000 (99.958)
Epoch: [243][128/196]	Time 0.124 (0.126)	Data 0.000 (0.003)	Loss 0.1265 (0.1215)	Acc@1 95.312 (95.585)	Acc@5 100.000 (99.967)
Epoch: [243][192/196]	Time 0.152 (0.132)	Data 0.000 (0.002)	Loss 0.1013 (0.1223)	Acc@1 96.484 (95.598)	Acc@5 100.000 (99.960)
after train
n1: 30 for:
wAcc: 85.89401154008104
test acc: 86.73
Epoche: [244/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [244][0/196]	Time 0.159 (0.159)	Data 0.354 (0.354)	Loss 0.0960 (0.0960)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [244][64/196]	Time 0.172 (0.143)	Data 0.000 (0.006)	Loss 0.0753 (0.1177)	Acc@1 98.047 (95.763)	Acc@5 100.000 (99.958)
Epoch: [244][128/196]	Time 0.127 (0.153)	Data 0.000 (0.003)	Loss 0.1871 (0.1219)	Acc@1 93.359 (95.658)	Acc@5 100.000 (99.952)
Epoch: [244][192/196]	Time 0.126 (0.144)	Data 0.000 (0.002)	Loss 0.1136 (0.1212)	Acc@1 96.094 (95.622)	Acc@5 100.000 (99.964)
after train
n1: 30 for:
wAcc: 85.51426160770188
test acc: 85.88
Epoche: [245/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [245][0/196]	Time 0.184 (0.184)	Data 0.281 (0.281)	Loss 0.1236 (0.1236)	Acc@1 95.312 (95.312)	Acc@5 99.609 (99.609)
Epoch: [245][64/196]	Time 0.126 (0.129)	Data 0.000 (0.005)	Loss 0.1132 (0.1206)	Acc@1 96.094 (95.715)	Acc@5 100.000 (99.976)
Epoch: [245][128/196]	Time 0.128 (0.128)	Data 0.000 (0.002)	Loss 0.1264 (0.1226)	Acc@1 96.094 (95.624)	Acc@5 100.000 (99.976)
Epoch: [245][192/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.1335 (0.1253)	Acc@1 95.312 (95.497)	Acc@5 100.000 (99.968)
after train
n1: 30 for:
wAcc: 85.93973876214677
test acc: 86.24
Epoche: [246/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [246][0/196]	Time 0.181 (0.181)	Data 0.273 (0.273)	Loss 0.1483 (0.1483)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [246][64/196]	Time 0.126 (0.127)	Data 0.000 (0.005)	Loss 0.0980 (0.1106)	Acc@1 96.875 (96.022)	Acc@5 100.000 (99.994)
Epoch: [246][128/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.1049 (0.1157)	Acc@1 96.484 (95.788)	Acc@5 100.000 (99.994)
Epoch: [246][192/196]	Time 0.128 (0.127)	Data 0.000 (0.002)	Loss 0.1161 (0.1219)	Acc@1 96.094 (95.612)	Acc@5 100.000 (99.984)
after train
n1: 30 for:
wAcc: 85.93164375903534
test acc: 85.7
Epoche: [247/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [247][0/196]	Time 0.167 (0.167)	Data 0.272 (0.272)	Loss 0.1079 (0.1079)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [247][64/196]	Time 0.128 (0.129)	Data 0.000 (0.004)	Loss 0.1739 (0.1171)	Acc@1 94.922 (95.619)	Acc@5 100.000 (99.976)
Epoch: [247][128/196]	Time 0.129 (0.128)	Data 0.000 (0.002)	Loss 0.1334 (0.1227)	Acc@1 94.922 (95.491)	Acc@5 100.000 (99.967)
Epoch: [247][192/196]	Time 0.130 (0.128)	Data 0.000 (0.002)	Loss 0.1558 (0.1241)	Acc@1 93.750 (95.458)	Acc@5 100.000 (99.964)
after train
n1: 30 for:
wAcc: 86.02078332160283
test acc: 86.73
Epoche: [248/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [248][0/196]	Time 0.191 (0.191)	Data 0.310 (0.310)	Loss 0.1465 (0.1465)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [248][64/196]	Time 0.127 (0.129)	Data 0.000 (0.005)	Loss 0.1231 (0.1191)	Acc@1 94.922 (95.793)	Acc@5 100.000 (99.982)
Epoch: [248][128/196]	Time 0.128 (0.129)	Data 0.000 (0.003)	Loss 0.1540 (0.1159)	Acc@1 94.922 (95.867)	Acc@5 100.000 (99.982)
Epoch: [248][192/196]	Time 0.127 (0.129)	Data 0.000 (0.002)	Loss 0.1045 (0.1163)	Acc@1 96.875 (95.845)	Acc@5 100.000 (99.980)
after train
n1: 30 for:
wAcc: 86.10990770351098
test acc: 84.49
Epoche: [249/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [249][0/196]	Time 0.174 (0.174)	Data 0.232 (0.232)	Loss 0.1222 (0.1222)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [249][64/196]	Time 0.155 (0.146)	Data 0.000 (0.004)	Loss 0.1714 (0.1251)	Acc@1 94.141 (95.276)	Acc@5 100.000 (99.982)
Epoch: [249][128/196]	Time 0.127 (0.142)	Data 0.000 (0.002)	Loss 0.1277 (0.1246)	Acc@1 94.531 (95.446)	Acc@5 100.000 (99.976)
Epoch: [249][192/196]	Time 0.127 (0.137)	Data 0.000 (0.002)	Loss 0.0614 (0.1231)	Acc@1 98.047 (95.517)	Acc@5 100.000 (99.980)
after train
n1: 30 for:
wAcc: 85.97503960206993
test acc: 86.03
Epoche: [250/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [250][0/196]	Time 0.199 (0.199)	Data 0.287 (0.287)	Loss 0.1383 (0.1383)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [250][64/196]	Time 0.153 (0.149)	Data 0.000 (0.005)	Loss 0.1967 (0.1084)	Acc@1 93.750 (96.166)	Acc@5 99.219 (99.982)
Epoch: [250][128/196]	Time 0.153 (0.151)	Data 0.000 (0.003)	Loss 0.1558 (0.1131)	Acc@1 94.531 (95.924)	Acc@5 100.000 (99.973)
Epoch: [250][192/196]	Time 0.126 (0.149)	Data 0.000 (0.002)	Loss 0.1510 (0.1174)	Acc@1 94.531 (95.723)	Acc@5 100.000 (99.978)
after train
n1: 30 for:
wAcc: 85.8817291908417
test acc: 87.29
Epoche: [251/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [251][0/196]	Time 0.186 (0.186)	Data 0.319 (0.319)	Loss 0.1345 (0.1345)	Acc@1 93.359 (93.359)	Acc@5 100.000 (100.000)
Epoch: [251][64/196]	Time 0.153 (0.138)	Data 0.000 (0.005)	Loss 0.0863 (0.1052)	Acc@1 97.266 (96.118)	Acc@5 100.000 (99.964)
Epoch: [251][128/196]	Time 0.127 (0.137)	Data 0.000 (0.003)	Loss 0.1254 (0.1175)	Acc@1 94.531 (95.749)	Acc@5 100.000 (99.961)
Epoch: [251][192/196]	Time 0.172 (0.139)	Data 0.000 (0.002)	Loss 0.1541 (0.1205)	Acc@1 94.141 (95.596)	Acc@5 100.000 (99.966)
after train
n1: 30 for:
wAcc: 86.25881725541868
test acc: 86.89
Epoche: [252/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [252][0/196]	Time 0.169 (0.169)	Data 0.281 (0.281)	Loss 0.1000 (0.1000)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [252][64/196]	Time 0.125 (0.127)	Data 0.000 (0.005)	Loss 0.0961 (0.1100)	Acc@1 95.703 (96.118)	Acc@5 100.000 (99.988)
Epoch: [252][128/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.1195 (0.1118)	Acc@1 97.266 (95.982)	Acc@5 100.000 (99.985)
Epoch: [252][192/196]	Time 0.172 (0.136)	Data 0.000 (0.002)	Loss 0.1613 (0.1134)	Acc@1 93.750 (95.897)	Acc@5 99.609 (99.968)
after train
n1: 30 for:
wAcc: 86.17666139915455
test acc: 87.3
Epoche: [253/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [253][0/196]	Time 0.225 (0.225)	Data 0.315 (0.315)	Loss 0.1378 (0.1378)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [253][64/196]	Time 0.171 (0.173)	Data 0.000 (0.005)	Loss 0.0460 (0.1031)	Acc@1 98.828 (96.316)	Acc@5 100.000 (99.946)
Epoch: [253][128/196]	Time 0.127 (0.159)	Data 0.000 (0.003)	Loss 0.1086 (0.1071)	Acc@1 95.312 (96.212)	Acc@5 100.000 (99.967)
Epoch: [253][192/196]	Time 0.126 (0.148)	Data 0.000 (0.002)	Loss 0.1502 (0.1102)	Acc@1 93.359 (96.001)	Acc@5 100.000 (99.970)
after train
n1: 30 for:
wAcc: 86.2129944679629
test acc: 86.41
Epoche: [254/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [254][0/196]	Time 0.175 (0.175)	Data 0.286 (0.286)	Loss 0.1268 (0.1268)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [254][64/196]	Time 0.152 (0.152)	Data 0.000 (0.005)	Loss 0.0927 (0.1152)	Acc@1 97.656 (95.841)	Acc@5 100.000 (99.964)
Epoch: [254][128/196]	Time 0.153 (0.145)	Data 0.000 (0.003)	Loss 0.0990 (0.1143)	Acc@1 96.094 (95.870)	Acc@5 100.000 (99.970)
Epoch: [254][192/196]	Time 0.154 (0.148)	Data 0.000 (0.002)	Loss 0.1012 (0.1183)	Acc@1 96.875 (95.731)	Acc@5 100.000 (99.968)
after train
n1: 30 for:
wAcc: 86.25317119816404
test acc: 85.96
Epoche: [255/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [255][0/196]	Time 0.174 (0.174)	Data 0.293 (0.293)	Loss 0.1148 (0.1148)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [255][64/196]	Time 0.127 (0.129)	Data 0.000 (0.005)	Loss 0.1044 (0.1102)	Acc@1 95.703 (96.022)	Acc@5 100.000 (99.994)
Epoch: [255][128/196]	Time 0.128 (0.128)	Data 0.000 (0.003)	Loss 0.0748 (0.1127)	Acc@1 97.266 (95.951)	Acc@5 100.000 (99.976)
Epoch: [255][192/196]	Time 0.127 (0.127)	Data 0.000 (0.002)	Loss 0.0802 (0.1136)	Acc@1 97.266 (95.879)	Acc@5 100.000 (99.976)
after train
n1: 30 for:
wAcc: 86.03476197831951
test acc: 87.39
Epoche: [256/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [256][0/196]	Time 0.142 (0.142)	Data 0.362 (0.362)	Loss 0.1250 (0.1250)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [256][64/196]	Time 0.127 (0.136)	Data 0.000 (0.006)	Loss 0.1276 (0.1060)	Acc@1 94.922 (96.256)	Acc@5 100.000 (99.988)
Epoch: [256][128/196]	Time 0.127 (0.133)	Data 0.000 (0.003)	Loss 0.0897 (0.1109)	Acc@1 96.484 (96.073)	Acc@5 100.000 (99.976)
Epoch: [256][192/196]	Time 0.154 (0.136)	Data 0.000 (0.002)	Loss 0.1125 (0.1125)	Acc@1 94.922 (96.017)	Acc@5 100.000 (99.980)
after train
n1: 30 for:
wAcc: 86.23929155076245
test acc: 86.92
Epoche: [257/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [257][0/196]	Time 0.134 (0.134)	Data 0.362 (0.362)	Loss 0.0614 (0.0614)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [257][64/196]	Time 0.154 (0.147)	Data 0.000 (0.006)	Loss 0.1332 (0.1016)	Acc@1 95.312 (96.292)	Acc@5 100.000 (99.988)
Epoch: [257][128/196]	Time 0.153 (0.150)	Data 0.000 (0.003)	Loss 0.1928 (0.1064)	Acc@1 93.359 (96.130)	Acc@5 100.000 (99.988)
Epoch: [257][192/196]	Time 0.126 (0.143)	Data 0.000 (0.002)	Loss 0.1347 (0.1069)	Acc@1 95.312 (96.175)	Acc@5 100.000 (99.982)
after train
n1: 30 for:
wAcc: 85.86108847775746
test acc: 85.78
Epoche: [258/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [258][0/196]	Time 0.179 (0.179)	Data 0.289 (0.289)	Loss 0.0602 (0.0602)	Acc@1 97.656 (97.656)	Acc@5 100.000 (100.000)
Epoch: [258][64/196]	Time 0.126 (0.125)	Data 0.000 (0.005)	Loss 0.0692 (0.1030)	Acc@1 97.266 (96.430)	Acc@5 100.000 (99.994)
Epoch: [258][128/196]	Time 0.127 (0.125)	Data 0.000 (0.003)	Loss 0.1030 (0.1057)	Acc@1 98.047 (96.166)	Acc@5 100.000 (99.973)
Epoch: [258][192/196]	Time 0.126 (0.126)	Data 0.000 (0.002)	Loss 0.1228 (0.1062)	Acc@1 95.312 (96.150)	Acc@5 100.000 (99.980)
after train
n1: 30 for:
wAcc: 86.3242364085305
test acc: 86.69
Epoche: [259/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [259][0/196]	Time 0.180 (0.180)	Data 0.283 (0.283)	Loss 0.0907 (0.0907)	Acc@1 95.703 (95.703)	Acc@5 100.000 (100.000)
Epoch: [259][64/196]	Time 0.126 (0.129)	Data 0.000 (0.005)	Loss 0.1345 (0.1023)	Acc@1 96.484 (96.352)	Acc@5 100.000 (99.994)
Epoch: [259][128/196]	Time 0.127 (0.128)	Data 0.000 (0.003)	Loss 0.1441 (0.1040)	Acc@1 93.750 (96.188)	Acc@5 100.000 (99.994)
Epoch: [259][192/196]	Time 0.173 (0.137)	Data 0.000 (0.002)	Loss 0.0787 (0.1097)	Acc@1 96.484 (96.041)	Acc@5 100.000 (99.982)
after train
n1: 30 for:
wAcc: 86.3651814464622
test acc: 87.55
Epoche: [260/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [260][0/196]	Time 0.157 (0.157)	Data 0.371 (0.371)	Loss 0.1333 (0.1333)	Acc@1 94.141 (94.141)	Acc@5 100.000 (100.000)
Epoch: [260][64/196]	Time 0.173 (0.170)	Data 0.000 (0.006)	Loss 0.1030 (0.1017)	Acc@1 96.875 (96.382)	Acc@5 100.000 (99.976)
Epoch: [260][128/196]	Time 0.173 (0.172)	Data 0.000 (0.003)	Loss 0.1075 (0.1017)	Acc@1 95.703 (96.318)	Acc@5 100.000 (99.988)
Epoch: [260][192/196]	Time 0.126 (0.158)	Data 0.000 (0.002)	Loss 0.0893 (0.1042)	Acc@1 97.266 (96.241)	Acc@5 100.000 (99.992)
after train
n1: 30 for:
wAcc: 86.3187440294856
test acc: 86.33
Epoche: [261/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [261][0/196]	Time 0.169 (0.169)	Data 0.295 (0.295)	Loss 0.1148 (0.1148)	Acc@1 96.484 (96.484)	Acc@5 100.000 (100.000)
Epoch: [261][64/196]	Time 0.126 (0.130)	Data 0.000 (0.005)	Loss 0.0958 (0.1134)	Acc@1 96.094 (96.082)	Acc@5 100.000 (99.976)
Epoch: [261][128/196]	Time 0.127 (0.128)	Data 0.000 (0.003)	Loss 0.1054 (0.1126)	Acc@1 95.703 (96.054)	Acc@5 100.000 (99.979)
Epoch: [261][192/196]	Time 0.166 (0.134)	Data 0.000 (0.002)	Loss 0.1142 (0.1109)	Acc@1 95.703 (96.080)	Acc@5 100.000 (99.986)
after train
n1: 30 for:
wAcc: 86.39897907761528
test acc: 85.43
Epoche: [262/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [262][0/196]	Time 0.207 (0.207)	Data 0.295 (0.295)	Loss 0.0974 (0.0974)	Acc@1 96.094 (96.094)	Acc@5 100.000 (100.000)
Epoch: [262][64/196]	Time 0.155 (0.149)	Data 0.000 (0.005)	Loss 0.0782 (0.1020)	Acc@1 97.266 (96.460)	Acc@5 100.000 (99.964)
Epoch: [262][128/196]	Time 0.154 (0.152)	Data 0.000 (0.003)	Loss 0.0945 (0.1037)	Acc@1 97.266 (96.309)	Acc@5 100.000 (99.970)
Epoch: [262][192/196]	Time 0.126 (0.146)	Data 0.000 (0.002)	Loss 0.0826 (0.1068)	Acc@1 96.484 (96.086)	Acc@5 100.000 (99.976)
after train
n1: 30 for:
wAcc: 86.42320123276009
test acc: 87.39
Epoche: [263/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [263][0/196]	Time 0.189 (0.189)	Data 0.319 (0.319)	Loss 0.2239 (0.2239)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [263][64/196]	Time 0.128 (0.128)	Data 0.000 (0.005)	Loss 0.1361 (0.1032)	Acc@1 94.531 (96.430)	Acc@5 100.000 (99.988)
Epoch: [263][128/196]	Time 0.127 (0.127)	Data 0.000 (0.003)	Loss 0.1108 (0.1012)	Acc@1 96.484 (96.424)	Acc@5 100.000 (99.991)
Epoch: [263][192/196]	Time 0.126 (0.127)	Data 0.000 (0.002)	Loss 0.1661 (0.1035)	Acc@1 94.141 (96.312)	Acc@5 100.000 (99.988)
after train
n1: 30 for:
wAcc: 86.30776463136675
test acc: 86.87
Epoche: [264/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [264][0/196]	Time 0.164 (0.164)	Data 0.289 (0.289)	Loss 0.0571 (0.0571)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [264][64/196]	Time 0.126 (0.130)	Data 0.000 (0.005)	Loss 0.1229 (0.1056)	Acc@1 96.094 (96.184)	Acc@5 100.000 (99.988)
Epoch: [264][128/196]	Time 0.125 (0.129)	Data 0.000 (0.003)	Loss 0.0866 (0.1043)	Acc@1 97.266 (96.221)	Acc@5 100.000 (99.988)
Epoch: [264][192/196]	Time 0.127 (0.129)	Data 0.000 (0.002)	Loss 0.1112 (0.1056)	Acc@1 96.094 (96.179)	Acc@5 100.000 (99.986)
after train
n1: 30 for:
wAcc: 86.39752565713589
test acc: 86.75
Epoche: [265/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [265][0/196]	Time 0.208 (0.208)	Data 0.284 (0.284)	Loss 0.0596 (0.0596)	Acc@1 98.047 (98.047)	Acc@5 100.000 (100.000)
Epoch: [265][64/196]	Time 0.175 (0.174)	Data 0.000 (0.005)	Loss 0.0893 (0.1047)	Acc@1 96.875 (96.172)	Acc@5 100.000 (99.994)
Epoch: [265][128/196]	Time 0.172 (0.173)	Data 0.000 (0.003)	Loss 0.0808 (0.1001)	Acc@1 96.875 (96.384)	Acc@5 100.000 (99.985)
Epoch: [265][192/196]	Time 0.170 (0.173)	Data 0.000 (0.002)	Loss 0.1210 (0.1025)	Acc@1 95.703 (96.272)	Acc@5 100.000 (99.982)
after train
n1: 30 for:
wAcc: 86.58795734372245
test acc: 86.52
Epoche: [266/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [266][0/196]	Time 0.228 (0.228)	Data 0.293 (0.293)	Loss 0.0978 (0.0978)	Acc@1 96.875 (96.875)	Acc@5 100.000 (100.000)
Epoch: [266][64/196]	Time 0.171 (0.173)	Data 0.000 (0.005)	Loss 0.1384 (0.1066)	Acc@1 94.141 (96.106)	Acc@5 100.000 (99.976)
Epoch: [266][128/196]	Time 0.175 (0.172)	Data 0.000 (0.003)	Loss 0.1196 (0.1074)	Acc@1 95.703 (96.100)	Acc@5 100.000 (99.982)
Epoch: [266][192/196]	Time 0.173 (0.173)	Data 0.000 (0.002)	Loss 0.0793 (0.1057)	Acc@1 96.484 (96.187)	Acc@5 100.000 (99.982)
after train
n1: 30 for:
wAcc: 86.21204979685186
test acc: 86.29
Epoche: [267/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [267][0/196]	Time 0.191 (0.191)	Data 0.290 (0.290)	Loss 0.0631 (0.0631)	Acc@1 98.438 (98.438)	Acc@5 100.000 (100.000)
Epoch: [267][64/196]	Time 0.175 (0.150)	Data 0.000 (0.005)	Loss 0.0853 (0.1015)	Acc@1 96.875 (96.412)	Acc@5 100.000 (99.988)
Epoch: [267][128/196]	Time 0.172 (0.162)	Data 0.000 (0.003)	Loss 0.1141 (0.1024)	Acc@1 94.922 (96.345)	Acc@5 100.000 (99.985)
Epoch: [267][192/196]	Time 0.171 (0.165)	Data 0.000 (0.002)	Loss 0.1440 (0.1044)	Acc@1 95.312 (96.252)	Acc@5 100.000 (99.980)
after train
n1: 30 for:
wAcc: 86.31248946999659
test acc: 86.47
Epoche: [268/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [268][0/196]	Time 0.194 (0.194)	Data 0.274 (0.274)	Loss 0.0865 (0.0865)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [268][64/196]	Time 0.155 (0.154)	Data 0.000 (0.005)	Loss 0.0682 (0.0948)	Acc@1 98.438 (96.556)	Acc@5 100.000 (99.982)
Epoch: [268][128/196]	Time 0.154 (0.154)	Data 0.000 (0.002)	Loss 0.0497 (0.0966)	Acc@1 98.047 (96.439)	Acc@5 100.000 (99.982)
Epoch: [268][192/196]	Time 0.126 (0.152)	Data 0.000 (0.002)	Loss 0.1676 (0.0984)	Acc@1 94.531 (96.379)	Acc@5 100.000 (99.982)
after train
n1: 30 for:
wAcc: 86.64068686560867
test acc: 87.0
Epoche: [269/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [269][0/196]	Time 0.163 (0.163)	Data 0.294 (0.294)	Loss 0.1395 (0.1395)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [269][64/196]	Time 0.126 (0.128)	Data 0.000 (0.005)	Loss 0.1124 (0.1024)	Acc@1 96.875 (96.466)	Acc@5 100.000 (99.988)
Epoch: [269][128/196]	Time 0.126 (0.127)	Data 0.000 (0.003)	Loss 0.0990 (0.1002)	Acc@1 97.266 (96.496)	Acc@5 100.000 (99.988)
Epoch: [269][192/196]	Time 0.169 (0.130)	Data 0.000 (0.002)	Loss 0.0983 (0.1016)	Acc@1 95.703 (96.420)	Acc@5 100.000 (99.988)
after train
n1: 30 for:
wAcc: 86.48027518045151
test acc: 87.6
Epoche: [270/270]; Lr: 0.1
batch Size 256
befor train
Epoch: [270][0/196]	Time 0.173 (0.173)	Data 0.292 (0.292)	Loss 0.0769 (0.0769)	Acc@1 97.266 (97.266)	Acc@5 100.000 (100.000)
Epoch: [270][64/196]	Time 0.126 (0.127)	Data 0.000 (0.005)	Loss 0.0972 (0.0960)	Acc@1 95.703 (96.376)	Acc@5 100.000 (99.994)
Epoch: [270][128/196]	Time 0.126 (0.129)	Data 0.000 (0.003)	Loss 0.0886 (0.0966)	Acc@1 97.266 (96.496)	Acc@5 100.000 (99.985)
Epoch: [270][192/196]	Time 0.126 (0.128)	Data 0.000 (0.002)	Loss 0.0776 (0.0955)	Acc@1 97.266 (96.517)	Acc@5 100.000 (99.984)
after train
n1: 30 for:
wAcc: 86.43686624559581
test acc: 87.48
Max memory: 114.2688768
Traceback (most recent call last):
  File "main.py", line 924, in <module>
    main()
  File "main.py", line 590, in main
    print(' {:5.3f}s'.format(ende - start), end='  ')
UnboundLocalError: local variable 'ende' referenced before assignment
