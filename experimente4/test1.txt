no display found. Using non-interactive Agg backend
[5, 5, 5]
[16, 32, 64]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 1;widthofFirstLayer: 16; Epochen: 180; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/test1/model.nn; checkpoint: ./output/experimente4/test1; saveModell: True; LR: 0.1
random number: 3147
Files already downloaded and verified
width: 16

Arch Num:  [[1, 1, 1, 1, 1], [2, 1, 1, 1, 1], [2, 1, 1, 1, 1]]
conv0: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0
bn1: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
Relu; i: 2
i : 2; block: 0
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 2
i : 3; block: 1
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 3
i : 4; block: 2
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 4
i : 5; block: 3
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 5
i : 6; block: 4
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 6
i : 7; block: 0
Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 7; block: 0
Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 1 if 2
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
); i: 7
seq1: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 8
i : 8; block: 1
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 8
i : 9; block: 2
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 9
i : 10; block: 3
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 10
i : 11; block: 4
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 11
i : 12; block: 0
Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 12; block: 0
Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 1 if 2
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
); i: 12
seq1: Sequential(
  (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 13
i : 13; block: 1
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 13
i : 14; block: 2
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 14
i : 15; block: 3
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 15
i : 16; block: 4
Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 3
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
seq: Sequential(
  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 16
avgpoll: AdaptiveAvgPool2d(output_size=(1, 1))
linear: Linear(in_features=64, out_features=10, bias=True)
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
deeper epoch: 93
Epoche: [1/180]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [1][0/196]	Time 0.105 (0.105)	Data 0.324 (0.324)	Loss 2.6398 (2.6398)	Acc@1 10.547 (10.547)	Acc@5 49.219 (49.219)
Epoch: [1][64/196]	Time 0.060 (0.066)	Data 0.000 (0.005)	Loss 1.7778 (2.0227)	Acc@1 29.297 (24.724)	Acc@5 86.719 (78.161)
Epoch: [1][128/196]	Time 0.059 (0.066)	Data 0.000 (0.003)	Loss 1.5329 (1.8452)	Acc@1 41.016 (30.263)	Acc@5 92.188 (83.527)
Epoch: [1][192/196]	Time 0.059 (0.065)	Data 0.000 (0.002)	Loss 1.2619 (1.7129)	Acc@1 54.688 (35.429)	Acc@5 95.312 (86.567)
after train
n1: 1 for:
wAcc: 43.7
test acc: 43.7
Epoche: [2/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [2][0/196]	Time 0.086 (0.086)	Data 0.355 (0.355)	Loss 1.3578 (1.3578)	Acc@1 53.125 (53.125)	Acc@5 93.750 (93.750)
Epoch: [2][64/196]	Time 0.060 (0.065)	Data 0.000 (0.006)	Loss 1.3776 (1.3152)	Acc@1 50.781 (52.085)	Acc@5 92.969 (94.207)
Epoch: [2][128/196]	Time 0.064 (0.065)	Data 0.000 (0.003)	Loss 1.1923 (1.2503)	Acc@1 56.250 (54.642)	Acc@5 96.094 (94.761)
Epoch: [2][192/196]	Time 0.063 (0.065)	Data 0.000 (0.002)	Loss 0.9828 (1.2089)	Acc@1 64.062 (56.017)	Acc@5 96.875 (95.130)
after train
n1: 2 for:
wAcc: 43.7
test acc: 55.81
Epoche: [3/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [3][0/196]	Time 0.092 (0.092)	Data 0.302 (0.302)	Loss 1.0668 (1.0668)	Acc@1 59.766 (59.766)	Acc@5 96.484 (96.484)
Epoch: [3][64/196]	Time 0.066 (0.064)	Data 0.000 (0.005)	Loss 1.0712 (1.0348)	Acc@1 61.328 (62.506)	Acc@5 96.094 (96.346)
Epoch: [3][128/196]	Time 0.059 (0.064)	Data 0.000 (0.003)	Loss 1.1019 (1.0285)	Acc@1 63.672 (62.869)	Acc@5 95.312 (96.439)
Epoch: [3][192/196]	Time 0.062 (0.064)	Data 0.000 (0.002)	Loss 0.8851 (1.0026)	Acc@1 65.625 (63.988)	Acc@5 98.438 (96.642)
after train
n1: 3 for:
wAcc: 49.755
test acc: 55.73
Epoche: [4/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [4][0/196]	Time 0.096 (0.096)	Data 0.289 (0.289)	Loss 1.0562 (1.0562)	Acc@1 64.844 (64.844)	Acc@5 98.047 (98.047)
Epoch: [4][64/196]	Time 0.060 (0.065)	Data 0.000 (0.005)	Loss 0.9482 (0.9046)	Acc@1 66.406 (68.275)	Acc@5 98.047 (97.458)
Epoch: [4][128/196]	Time 0.062 (0.065)	Data 0.000 (0.002)	Loss 0.7613 (0.8828)	Acc@1 75.781 (68.765)	Acc@5 98.047 (97.614)
Epoch: [4][192/196]	Time 0.064 (0.065)	Data 0.000 (0.002)	Loss 0.7685 (0.8723)	Acc@1 71.094 (69.258)	Acc@5 99.609 (97.672)
after train
n1: 4 for:
wAcc: 51.418400000000005
test acc: 68.59
Epoche: [5/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [5][0/196]	Time 0.078 (0.078)	Data 0.327 (0.327)	Loss 0.9061 (0.9061)	Acc@1 68.750 (68.750)	Acc@5 99.609 (99.609)
Epoch: [5][64/196]	Time 0.061 (0.064)	Data 0.000 (0.005)	Loss 0.7822 (0.8041)	Acc@1 77.344 (71.779)	Acc@5 98.438 (97.993)
Epoch: [5][128/196]	Time 0.067 (0.064)	Data 0.000 (0.003)	Loss 0.7447 (0.7854)	Acc@1 72.656 (72.726)	Acc@5 98.438 (98.077)
Epoch: [5][192/196]	Time 0.074 (0.064)	Data 0.000 (0.002)	Loss 0.7968 (0.7760)	Acc@1 70.703 (72.992)	Acc@5 98.438 (98.140)
after train
n1: 5 for:
wAcc: 56.46407407407409
test acc: 70.29
Prune Train:
[INFO] Force the sparse filters to zero...
altList: [(0, None), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 0), (16, 0), (17, 0), (18, 0), (19, 0), (21, None)]


 Reconf: 
[(0, None)]: [16, 3, 3, 3]


 Reconf: 
[(3, 0)]: [16, 16, 3, 3]


 Reconf: 
[(4, 0)]: [16, 16, 3, 3]


 Reconf: 
[(5, 0)]: [16, 16, 3, 3]


 Reconf: 
[(6, 0)]: [16, 16, 3, 3]


 Reconf: 
[(7, 0)]: [16, 16, 3, 3]


 Reconf: 
[(8, 0)]: [32, 16, 3, 3]


 Reconf: 
[(9, 0)]: [32, 16, 1, 1]


 Reconf: 
[(10, 0)]: [32, 32, 3, 3]


 Reconf: 
[(11, 0)]: [32, 32, 3, 3]


 Reconf: 
[(12, 0)]: [32, 32, 3, 3]


 Reconf: 
[(13, 0)]: [32, 32, 3, 3]


 Reconf: 
[(14, 0)]: [64, 32, 3, 3]


 Reconf: 
[(15, 0)]: [64, 32, 1, 1]


 Reconf: 
[(16, 0)]: [64, 64, 3, 3]


 Reconf: 
[(17, 0)]: [64, 64, 3, 3]


 Reconf: 
[(18, 0)]: [64, 64, 3, 3]


 Reconf: 
[(19, 0)]: [64, 64, 3, 3]


 Reconf: 
[(21, None)]: [10, 64]
Start width: 16
Append O : (0, None)
Traceback (most recent call last):
  File "main.py", line 919, in <module>
    main()
  File "main.py", line 401, in main
    dense_chs, chs_map = makeSparse(optimizer, model, args.threshold)
  File "/home/jessica.buehler/MA_Source/src/checkpoint_utils.py", line 185, in makeSparse
    print(f'Append O: {(idx,j)}')
KeyboardInterrupt
