BSize 1
j: 0 bis 5
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 539
Files already downloaded and verified
numoFStages: 3
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
Max memory: 0.202496
batch_size berechnet: 250;389.1 ; lr: 0.1
lr: 0.09765625
1
Epoche:1/5; Lr: 0.09765625
batch Size 250
Epoch: [1][0/200]	Time 0.244 (0.244)	Data 0.305 (0.305)	Loss 3.1302 (3.1302)	Acc@1 8.000 (8.000)	Acc@5 40.800 (40.800)
Epoch: [1][64/200]	Time 0.134 (0.150)	Data 0.000 (0.005)	Loss 2.3278 (2.5888)	Acc@1 33.200 (26.449)	Acc@5 89.200 (79.692)
Epoch: [1][128/200]	Time 0.147 (0.147)	Data 0.000 (0.003)	Loss 2.1472 (2.4042)	Acc@1 42.400 (32.766)	Acc@5 89.200 (84.608)
Epoch: [1][192/200]	Time 0.153 (0.147)	Data 0.000 (0.002)	Loss 1.9921 (2.2709)	Acc@1 46.000 (37.737)	Acc@5 92.800 (87.173)
Max memory in training epoch: 66.4657408
lr: 0.09765625
1
Epoche:2/5; Lr: 0.09765625
batch Size 250
Epoch: [2][0/200]	Time 0.216 (0.216)	Data 0.312 (0.312)	Loss 1.9892 (1.9892)	Acc@1 51.600 (51.600)	Acc@5 92.000 (92.000)
Epoch: [2][64/200]	Time 0.136 (0.150)	Data 0.000 (0.005)	Loss 1.8205 (1.8134)	Acc@1 53.200 (54.178)	Acc@5 92.800 (94.351)
Epoch: [2][128/200]	Time 0.142 (0.148)	Data 0.000 (0.003)	Loss 1.5886 (1.7351)	Acc@1 62.800 (56.856)	Acc@5 96.800 (95.051)
Epoch: [2][192/200]	Time 0.150 (0.148)	Data 0.000 (0.002)	Loss 1.4825 (1.6757)	Acc@1 65.600 (58.815)	Acc@5 96.000 (95.496)
Max memory in training epoch: 66.0135424
lr: 0.09765625
1
Epoche:3/5; Lr: 0.09765625
batch Size 250
Epoch: [3][0/200]	Time 0.195 (0.195)	Data 0.363 (0.363)	Loss 1.4912 (1.4912)	Acc@1 64.400 (64.400)	Acc@5 97.200 (97.200)
Epoch: [3][64/200]	Time 0.202 (0.150)	Data 0.000 (0.006)	Loss 1.3315 (1.4330)	Acc@1 71.200 (66.658)	Acc@5 98.000 (97.052)
Epoch: [3][128/200]	Time 0.147 (0.148)	Data 0.000 (0.003)	Loss 1.3628 (1.4062)	Acc@1 68.000 (67.219)	Acc@5 96.000 (97.262)
Epoch: [3][192/200]	Time 0.151 (0.144)	Data 0.000 (0.002)	Loss 1.2041 (1.3713)	Acc@1 73.200 (68.327)	Acc@5 99.200 (97.451)
Max memory in training epoch: 66.0135424
Drin!!
old memory: 0
new memory: 660135424
lr: 0.09765625
1
Epoche:4/5; Lr: 0.09765625
batch Size 250
Epoch: [4][0/200]	Time 0.190 (0.190)	Data 0.304 (0.304)	Loss 1.2228 (1.2228)	Acc@1 74.400 (74.400)	Acc@5 97.200 (97.200)
Epoch: [4][64/200]	Time 0.134 (0.146)	Data 0.000 (0.005)	Loss 1.2000 (1.2337)	Acc@1 74.400 (72.443)	Acc@5 98.800 (98.172)
Epoch: [4][128/200]	Time 0.158 (0.147)	Data 0.000 (0.003)	Loss 1.1678 (1.2131)	Acc@1 73.200 (73.144)	Acc@5 99.200 (98.174)
Epoch: [4][192/200]	Time 0.150 (0.147)	Data 0.000 (0.002)	Loss 1.0729 (1.1976)	Acc@1 75.600 (73.355)	Acc@5 98.000 (98.193)
Max memory in training epoch: 66.0135424
lr: 0.09765625
1
Epoche:5/5; Lr: 0.09765625
batch Size 250
Epoch: [5][0/200]	Time 0.194 (0.194)	Data 0.320 (0.320)	Loss 1.0741 (1.0741)	Acc@1 76.800 (76.800)	Acc@5 99.200 (99.200)
Epoch: [5][64/200]	Time 0.134 (0.152)	Data 0.000 (0.005)	Loss 1.1103 (1.1015)	Acc@1 73.200 (75.871)	Acc@5 99.200 (98.332)
Epoch: [5][128/200]	Time 0.146 (0.151)	Data 0.000 (0.003)	Loss 0.9609 (1.0907)	Acc@1 79.600 (75.780)	Acc@5 99.600 (98.428)
Epoch: [5][192/200]	Time 0.152 (0.150)	Data 0.000 (0.002)	Loss 1.1137 (1.0785)	Acc@1 76.000 (76.118)	Acc@5 98.000 (98.510)
Max memory in training epoch: 66.0135424
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  74.78
Max memory: 103.3835008
 30.408s  ./run_BSize.sh: 7: ./run_BSize.sh: cannot open j: 6 bis 10: No such file
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6105
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 6
Max memory: 0.202496
lr: 0.095367431640625
1
Epoche:6/10; Lr: 0.095367431640625
batch Size 250
Epoch: [6][0/200]	Time 0.223 (0.223)	Data 0.315 (0.315)	Loss 1.0470 (1.0470)	Acc@1 78.000 (78.000)	Acc@5 98.400 (98.400)
Epoch: [6][64/200]	Time 0.148 (0.153)	Data 0.000 (0.005)	Loss 0.9457 (0.9944)	Acc@1 80.400 (78.271)	Acc@5 97.600 (98.794)
Epoch: [6][128/200]	Time 0.148 (0.148)	Data 0.000 (0.003)	Loss 1.0368 (0.9900)	Acc@1 74.400 (78.338)	Acc@5 98.800 (98.757)
Epoch: [6][192/200]	Time 0.146 (0.149)	Data 0.000 (0.002)	Loss 0.9214 (0.9927)	Acc@1 80.800 (78.060)	Acc@5 98.400 (98.709)
Max memory in training epoch: 66.4656384
lr: 0.095367431640625
1
Epoche:7/10; Lr: 0.095367431640625
batch Size 250
Epoch: [7][0/200]	Time 0.232 (0.232)	Data 0.317 (0.317)	Loss 0.9212 (0.9212)	Acc@1 82.000 (82.000)	Acc@5 98.800 (98.800)
Epoch: [7][64/200]	Time 0.150 (0.151)	Data 0.000 (0.005)	Loss 0.9067 (0.9388)	Acc@1 80.400 (79.717)	Acc@5 99.200 (98.788)
Epoch: [7][128/200]	Time 0.140 (0.154)	Data 0.000 (0.003)	Loss 0.8236 (0.9378)	Acc@1 81.200 (79.451)	Acc@5 98.400 (98.757)
Epoch: [7][192/200]	Time 0.138 (0.153)	Data 0.000 (0.002)	Loss 0.8533 (0.9355)	Acc@1 82.400 (79.447)	Acc@5 98.800 (98.744)
Max memory in training epoch: 66.01344
lr: 0.095367431640625
1
Epoche:8/10; Lr: 0.095367431640625
batch Size 250
Epoch: [8][0/200]	Time 0.206 (0.206)	Data 0.309 (0.309)	Loss 0.8591 (0.8591)	Acc@1 82.800 (82.800)	Acc@5 98.400 (98.400)
Epoch: [8][64/200]	Time 0.177 (0.158)	Data 0.000 (0.005)	Loss 0.8588 (0.8841)	Acc@1 81.200 (80.720)	Acc@5 98.400 (98.818)
Epoch: [8][128/200]	Time 0.159 (0.157)	Data 0.000 (0.003)	Loss 0.8626 (0.8835)	Acc@1 81.200 (80.512)	Acc@5 98.800 (98.890)
Epoch: [8][192/200]	Time 0.151 (0.157)	Data 0.000 (0.002)	Loss 0.9623 (0.8828)	Acc@1 77.600 (80.466)	Acc@5 99.200 (98.885)
Max memory in training epoch: 66.01344
Drin!!
old memory: 660135424
new memory: 660134400
Faktor: 0.9999984488031353
New batch Size größer 253!!
lr: 0.095367431640625
1
Epoche:9/10; Lr: 0.095367431640625
batch Size 253
Epoch: [9][0/200]	Time 0.215 (0.215)	Data 0.480 (0.480)	Loss 0.8551 (0.8551)	Acc@1 79.600 (79.600)	Acc@5 98.800 (98.800)
Epoch: [9][64/200]	Time 0.151 (0.162)	Data 0.000 (0.008)	Loss 0.8010 (0.8598)	Acc@1 83.600 (80.794)	Acc@5 99.600 (99.034)
Epoch: [9][128/200]	Time 0.176 (0.159)	Data 0.000 (0.004)	Loss 0.8944 (0.8580)	Acc@1 79.200 (80.816)	Acc@5 100.000 (99.057)
Epoch: [9][192/200]	Time 0.139 (0.157)	Data 0.000 (0.003)	Loss 0.7666 (0.8533)	Acc@1 86.000 (81.042)	Acc@5 99.200 (99.020)
Max memory in training epoch: 66.01344
lr: 0.095367431640625
1
Epoche:10/10; Lr: 0.095367431640625
batch Size 253
Epoch: [10][0/200]	Time 0.218 (0.218)	Data 0.405 (0.405)	Loss 0.8247 (0.8247)	Acc@1 79.600 (79.600)	Acc@5 98.400 (98.400)
Epoch: [10][64/200]	Time 0.160 (0.158)	Data 0.000 (0.006)	Loss 0.7805 (0.8376)	Acc@1 84.800 (81.292)	Acc@5 99.200 (99.163)
Epoch: [10][128/200]	Time 0.150 (0.158)	Data 0.000 (0.003)	Loss 0.8396 (0.8313)	Acc@1 82.400 (81.457)	Acc@5 99.200 (99.157)
Epoch: [10][192/200]	Time 0.154 (0.158)	Data 0.000 (0.002)	Loss 0.8010 (0.8349)	Acc@1 83.600 (81.355)	Acc@5 98.400 (99.132)
Max memory in training epoch: 66.01344
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  75.14
Max memory: 103.3833984
 32.210s  j: 11 bis 15
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1524
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 11
Max memory: 0.202496
lr: 0.09424984455108643
1
Epoche:11/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [11][0/198]	Time 0.246 (0.246)	Data 0.524 (0.524)	Loss 0.8131 (0.8131)	Acc@1 79.447 (79.447)	Acc@5 98.814 (98.814)
Epoch: [11][64/198]	Time 0.149 (0.162)	Data 0.000 (0.008)	Loss 0.7673 (0.7914)	Acc@1 84.980 (82.670)	Acc@5 99.209 (99.118)
Epoch: [11][128/198]	Time 0.151 (0.160)	Data 0.000 (0.004)	Loss 0.7733 (0.7982)	Acc@1 84.585 (82.474)	Acc@5 99.209 (99.139)
Epoch: [11][192/198]	Time 0.148 (0.160)	Data 0.000 (0.003)	Loss 0.8259 (0.8013)	Acc@1 82.609 (82.289)	Acc@5 98.814 (99.119)
Max memory in training epoch: 66.5037312
lr: 0.09424984455108643
1
Epoche:12/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [12][0/198]	Time 0.215 (0.215)	Data 0.421 (0.421)	Loss 0.7877 (0.7877)	Acc@1 85.375 (85.375)	Acc@5 99.209 (99.209)
Epoch: [12][64/198]	Time 0.153 (0.159)	Data 0.000 (0.007)	Loss 0.7003 (0.7824)	Acc@1 84.585 (82.815)	Acc@5 100.000 (99.143)
Epoch: [12][128/198]	Time 0.158 (0.161)	Data 0.000 (0.003)	Loss 0.7448 (0.7943)	Acc@1 82.213 (82.532)	Acc@5 100.000 (99.087)
Epoch: [12][192/198]	Time 0.127 (0.157)	Data 0.000 (0.002)	Loss 0.7384 (0.7950)	Acc@1 83.399 (82.474)	Acc@5 99.605 (99.128)
Max memory in training epoch: 66.277632
lr: 0.09424984455108643
1
Epoche:13/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [13][0/198]	Time 0.191 (0.191)	Data 0.438 (0.438)	Loss 0.7549 (0.7549)	Acc@1 82.213 (82.213)	Acc@5 99.209 (99.209)
Epoch: [13][64/198]	Time 0.161 (0.162)	Data 0.000 (0.007)	Loss 0.7492 (0.7775)	Acc@1 84.980 (82.986)	Acc@5 99.605 (99.149)
Epoch: [13][128/198]	Time 0.169 (0.161)	Data 0.000 (0.004)	Loss 0.8164 (0.7848)	Acc@1 82.609 (82.707)	Acc@5 99.209 (99.160)
Epoch: [13][192/198]	Time 0.170 (0.160)	Data 0.000 (0.002)	Loss 0.8644 (0.7844)	Acc@1 77.866 (82.701)	Acc@5 98.814 (99.171)
Max memory in training epoch: 66.277632
Drin!!
old memory: 660134400
new memory: 662776320
Faktor: 1.0040020941190158
New batch Size kleiner 254!!
lr: 0.09424984455108643
1
Epoche:14/15; Lr: 0.09424984455108643
batch Size 254
Epoch: [14][0/198]	Time 0.226 (0.226)	Data 0.343 (0.343)	Loss 0.8352 (0.8352)	Acc@1 79.447 (79.447)	Acc@5 98.419 (98.419)
Epoch: [14][64/198]	Time 0.149 (0.160)	Data 0.000 (0.005)	Loss 0.7587 (0.7708)	Acc@1 83.794 (83.150)	Acc@5 99.209 (99.191)
Epoch: [14][128/198]	Time 0.153 (0.160)	Data 0.000 (0.003)	Loss 0.7584 (0.7698)	Acc@1 84.190 (83.267)	Acc@5 99.605 (99.173)
Epoch: [14][192/198]	Time 0.147 (0.158)	Data 0.000 (0.002)	Loss 0.7748 (0.7735)	Acc@1 82.213 (83.055)	Acc@5 98.814 (99.169)
Max memory in training epoch: 66.277632
lr: 0.09424984455108643
1
Epoche:15/15; Lr: 0.09424984455108643
batch Size 254
Epoch: [15][0/198]	Time 0.199 (0.199)	Data 0.372 (0.372)	Loss 0.6914 (0.6914)	Acc@1 86.166 (86.166)	Acc@5 99.605 (99.605)
Epoch: [15][64/198]	Time 0.137 (0.157)	Data 0.000 (0.006)	Loss 0.8192 (0.7617)	Acc@1 80.632 (83.393)	Acc@5 99.605 (99.319)
Epoch: [15][128/198]	Time 0.157 (0.159)	Data 0.000 (0.003)	Loss 0.7717 (0.7608)	Acc@1 82.213 (83.304)	Acc@5 99.209 (99.295)
Epoch: [15][192/198]	Time 0.131 (0.158)	Data 0.000 (0.002)	Loss 0.7006 (0.7655)	Acc@1 84.585 (83.090)	Acc@5 99.209 (99.252)
Max memory in training epoch: 66.277632
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  70.08
Max memory: 103.3833984
 31.845s  j: 16 bis 20
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6571
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 16
Max memory: 0.202496
lr: 0.09351351764053106
1
Epoche:16/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [16][0/197]	Time 0.244 (0.244)	Data 0.323 (0.323)	Loss 0.7068 (0.7068)	Acc@1 83.465 (83.465)	Acc@5 99.606 (99.606)
Epoch: [16][64/197]	Time 0.151 (0.163)	Data 0.000 (0.005)	Loss 0.8052 (0.7439)	Acc@1 81.890 (83.816)	Acc@5 98.425 (99.273)
Epoch: [16][128/197]	Time 0.147 (0.161)	Data 0.000 (0.003)	Loss 0.7702 (0.7481)	Acc@1 81.496 (83.690)	Acc@5 99.213 (99.277)
Epoch: [16][192/197]	Time 0.132 (0.159)	Data 0.000 (0.002)	Loss 0.7362 (0.7507)	Acc@1 83.858 (83.791)	Acc@5 98.031 (99.262)
Max memory in training epoch: 66.5164288
lr: 0.09351351764053106
1
Epoche:17/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [17][0/197]	Time 0.239 (0.239)	Data 0.422 (0.422)	Loss 0.7818 (0.7818)	Acc@1 80.709 (80.709)	Acc@5 98.425 (98.425)
Epoch: [17][64/197]	Time 0.167 (0.162)	Data 0.000 (0.007)	Loss 0.7495 (0.7379)	Acc@1 83.858 (84.379)	Acc@5 99.213 (99.219)
Epoch: [17][128/197]	Time 0.139 (0.161)	Data 0.000 (0.003)	Loss 0.9003 (0.7442)	Acc@1 79.528 (83.986)	Acc@5 98.819 (99.243)
Epoch: [17][192/197]	Time 0.155 (0.160)	Data 0.000 (0.002)	Loss 0.6885 (0.7471)	Acc@1 86.220 (83.846)	Acc@5 100.000 (99.223)
Max memory in training epoch: 66.365696
lr: 0.09351351764053106
1
Epoche:18/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [18][0/197]	Time 0.216 (0.216)	Data 0.361 (0.361)	Loss 0.8753 (0.8753)	Acc@1 80.315 (80.315)	Acc@5 98.819 (98.819)
Epoch: [18][64/197]	Time 0.160 (0.163)	Data 0.000 (0.006)	Loss 0.7725 (0.7335)	Acc@1 82.677 (84.416)	Acc@5 98.425 (99.316)
Epoch: [18][128/197]	Time 0.162 (0.163)	Data 0.000 (0.003)	Loss 0.7743 (0.7408)	Acc@1 83.858 (84.115)	Acc@5 99.606 (99.322)
Epoch: [18][192/197]	Time 0.158 (0.161)	Data 0.000 (0.002)	Loss 0.7414 (0.7399)	Acc@1 84.252 (84.109)	Acc@5 99.606 (99.308)
Max memory in training epoch: 66.365696
Drin!!
old memory: 662776320
new memory: 663656960
Faktor: 1.0013287137355782
New batch Size kleiner 254!!
lr: 0.09351351764053106
1
Epoche:19/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [19][0/197]	Time 0.225 (0.225)	Data 0.421 (0.421)	Loss 0.7309 (0.7309)	Acc@1 82.677 (82.677)	Acc@5 99.606 (99.606)
Epoch: [19][64/197]	Time 0.138 (0.157)	Data 0.000 (0.007)	Loss 0.7052 (0.7285)	Acc@1 86.220 (84.543)	Acc@5 99.213 (99.316)
Epoch: [19][128/197]	Time 0.142 (0.157)	Data 0.000 (0.003)	Loss 0.7413 (0.7330)	Acc@1 85.039 (84.469)	Acc@5 98.031 (99.274)
Epoch: [19][192/197]	Time 0.156 (0.158)	Data 0.000 (0.002)	Loss 0.8444 (0.7398)	Acc@1 80.315 (84.215)	Acc@5 98.031 (99.270)
Max memory in training epoch: 66.365696
lr: 0.09351351764053106
1
Epoche:20/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [20][0/197]	Time 0.224 (0.224)	Data 0.391 (0.391)	Loss 0.7321 (0.7321)	Acc@1 85.039 (85.039)	Acc@5 99.213 (99.213)
Epoch: [20][64/197]	Time 0.161 (0.159)	Data 0.000 (0.006)	Loss 0.7191 (0.7354)	Acc@1 83.465 (84.252)	Acc@5 98.819 (99.310)
Epoch: [20][128/197]	Time 0.166 (0.162)	Data 0.000 (0.003)	Loss 0.7218 (0.7390)	Acc@1 85.827 (84.267)	Acc@5 98.819 (99.243)
Epoch: [20][192/197]	Time 0.137 (0.161)	Data 0.000 (0.002)	Loss 0.7763 (0.7415)	Acc@1 81.496 (84.111)	Acc@5 99.213 (99.264)
Max memory in training epoch: 66.365696
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  80.8
Max memory: 103.3833984
 32.210s  j: 21 bis 25
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1339
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 21
Max memory: 0.202496
lr: 0.09278294328396441
1
Epoche:21/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [21][0/197]	Time 0.269 (0.269)	Data 0.415 (0.415)	Loss 0.6606 (0.6606)	Acc@1 87.402 (87.402)	Acc@5 99.213 (99.213)
Epoch: [21][64/197]	Time 0.155 (0.161)	Data 0.000 (0.007)	Loss 0.6957 (0.7152)	Acc@1 86.220 (84.858)	Acc@5 98.819 (99.346)
Epoch: [21][128/197]	Time 0.175 (0.160)	Data 0.000 (0.003)	Loss 0.6633 (0.7223)	Acc@1 88.583 (84.774)	Acc@5 99.606 (99.307)
Epoch: [21][192/197]	Time 0.147 (0.160)	Data 0.000 (0.002)	Loss 0.8143 (0.7315)	Acc@1 83.071 (84.427)	Acc@5 98.425 (99.276)
Max memory in training epoch: 66.5164288
lr: 0.09278294328396441
1
Epoche:22/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [22][0/197]	Time 0.226 (0.226)	Data 0.374 (0.374)	Loss 0.7171 (0.7171)	Acc@1 85.827 (85.827)	Acc@5 98.819 (98.819)
Epoch: [22][64/197]	Time 0.124 (0.154)	Data 0.000 (0.006)	Loss 0.7091 (0.7294)	Acc@1 86.220 (84.652)	Acc@5 98.819 (99.291)
Epoch: [22][128/197]	Time 0.159 (0.157)	Data 0.000 (0.003)	Loss 0.6291 (0.7279)	Acc@1 87.008 (84.704)	Acc@5 100.000 (99.326)
Epoch: [22][192/197]	Time 0.169 (0.157)	Data 0.000 (0.002)	Loss 0.7175 (0.7259)	Acc@1 83.465 (84.764)	Acc@5 99.606 (99.347)
Max memory in training epoch: 66.365696
lr: 0.09278294328396441
1
Epoche:23/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [23][0/197]	Time 0.212 (0.212)	Data 0.452 (0.452)	Loss 0.7126 (0.7126)	Acc@1 86.220 (86.220)	Acc@5 98.819 (98.819)
Epoch: [23][64/197]	Time 0.167 (0.165)	Data 0.000 (0.007)	Loss 0.8231 (0.7142)	Acc@1 80.709 (84.961)	Acc@5 99.213 (99.406)
Epoch: [23][128/197]	Time 0.146 (0.162)	Data 0.000 (0.004)	Loss 0.7797 (0.7180)	Acc@1 83.071 (84.878)	Acc@5 99.213 (99.374)
Epoch: [23][192/197]	Time 0.158 (0.160)	Data 0.000 (0.003)	Loss 0.6910 (0.7246)	Acc@1 85.433 (84.660)	Acc@5 98.819 (99.315)
Max memory in training epoch: 66.365696
Drin!!
old memory: 663656960
new memory: 663656960
Faktor: 1.0
lr: 0.09278294328396441
1
Epoche:24/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [24][0/197]	Time 0.197 (0.197)	Data 0.425 (0.425)	Loss 0.6664 (0.6664)	Acc@1 87.402 (87.402)	Acc@5 100.000 (100.000)
Epoch: [24][64/197]	Time 0.185 (0.163)	Data 0.000 (0.007)	Loss 0.6261 (0.7088)	Acc@1 90.551 (85.324)	Acc@5 100.000 (99.370)
Epoch: [24][128/197]	Time 0.167 (0.161)	Data 0.000 (0.003)	Loss 0.6884 (0.7149)	Acc@1 84.646 (85.277)	Acc@5 99.213 (99.356)
Epoch: [24][192/197]	Time 0.173 (0.161)	Data 0.000 (0.002)	Loss 0.7483 (0.7181)	Acc@1 84.252 (85.066)	Acc@5 99.606 (99.359)
Max memory in training epoch: 66.365696
lr: 0.09278294328396441
1
Epoche:25/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [25][0/197]	Time 0.208 (0.208)	Data 0.418 (0.418)	Loss 0.7670 (0.7670)	Acc@1 85.433 (85.433)	Acc@5 98.819 (98.819)
Epoch: [25][64/197]	Time 0.148 (0.164)	Data 0.000 (0.007)	Loss 0.7113 (0.7149)	Acc@1 85.039 (85.112)	Acc@5 98.425 (99.364)
Epoch: [25][128/197]	Time 0.167 (0.161)	Data 0.000 (0.003)	Loss 0.6907 (0.7195)	Acc@1 85.433 (85.012)	Acc@5 100.000 (99.362)
Epoch: [25][192/197]	Time 0.153 (0.158)	Data 0.000 (0.002)	Loss 0.6804 (0.7134)	Acc@1 85.827 (85.156)	Acc@5 99.606 (99.370)
Max memory in training epoch: 66.365696
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 483924 ; 487386 ; 0.9928968004825743
[INFO] Storing checkpoint...
  73.53
Max memory: 103.3833984
 31.689s  j: 26 bis 30
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4386
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 26
Max memory: 0.2011648
lr: 0.09205807653955844
1
Epoche:26/30; Lr: 0.09205807653955844
batch Size 254
Epoch: [26][0/197]	Time 0.279 (0.279)	Data 0.410 (0.410)	Loss 0.7216 (0.7216)	Acc@1 83.858 (83.858)	Acc@5 99.606 (99.606)
Epoch: [26][64/197]	Time 0.141 (0.160)	Data 0.000 (0.006)	Loss 0.6363 (0.6769)	Acc@1 89.370 (86.336)	Acc@5 99.606 (99.522)
Epoch: [26][128/197]	Time 0.165 (0.159)	Data 0.000 (0.003)	Loss 0.6820 (0.6854)	Acc@1 84.646 (85.989)	Acc@5 99.606 (99.472)
Epoch: [26][192/197]	Time 0.160 (0.159)	Data 0.000 (0.002)	Loss 0.7996 (0.7045)	Acc@1 82.283 (85.260)	Acc@5 99.213 (99.417)
Max memory in training epoch: 66.511104
lr: 0.09205807653955844
1
Epoche:27/30; Lr: 0.09205807653955844
batch Size 254
Epoch: [27][0/197]	Time 0.208 (0.208)	Data 0.431 (0.431)	Loss 0.6245 (0.6245)	Acc@1 87.008 (87.008)	Acc@5 99.213 (99.213)
Epoch: [27][64/197]	Time 0.135 (0.163)	Data 0.000 (0.007)	Loss 0.7115 (0.6945)	Acc@1 85.827 (85.778)	Acc@5 99.213 (99.382)
Epoch: [27][128/197]	Time 0.157 (0.160)	Data 0.000 (0.004)	Loss 0.6686 (0.7111)	Acc@1 88.189 (85.247)	Acc@5 99.606 (99.341)
Epoch: [27][192/197]	Time 0.144 (0.160)	Data 0.000 (0.002)	Loss 0.6801 (0.7081)	Acc@1 87.402 (85.345)	Acc@5 99.213 (99.343)
Max memory in training epoch: 66.3603712
lr: 0.09205807653955844
1
Epoche:28/30; Lr: 0.09205807653955844
batch Size 254
Epoch: [28][0/197]	Time 0.225 (0.225)	Data 0.339 (0.339)	Loss 0.7914 (0.7914)	Acc@1 84.252 (84.252)	Acc@5 98.425 (98.425)
Epoch: [28][64/197]	Time 0.164 (0.162)	Data 0.000 (0.005)	Loss 0.6525 (0.6899)	Acc@1 86.614 (86.245)	Acc@5 99.606 (99.455)
Epoch: [28][128/197]	Time 0.183 (0.161)	Data 0.000 (0.003)	Loss 0.7870 (0.6943)	Acc@1 83.071 (85.891)	Acc@5 99.213 (99.496)
Epoch: [28][192/197]	Time 0.165 (0.160)	Data 0.000 (0.002)	Loss 0.7385 (0.7010)	Acc@1 85.433 (85.578)	Acc@5 98.819 (99.435)
Max memory in training epoch: 66.3603712
Drin!!
old memory: 663656960
new memory: 663603712
Faktor: 0.9999197657777897
New batch Size größer 256!!
lr: 0.09205807653955844
1
Epoche:29/30; Lr: 0.09205807653955844
batch Size 256
Epoch: [29][0/197]	Time 0.229 (0.229)	Data 0.351 (0.351)	Loss 0.6245 (0.6245)	Acc@1 88.189 (88.189)	Acc@5 99.213 (99.213)
Epoch: [29][64/197]	Time 0.135 (0.162)	Data 0.000 (0.006)	Loss 0.7902 (0.7078)	Acc@1 84.252 (85.282)	Acc@5 99.213 (99.358)
Epoch: [29][128/197]	Time 0.152 (0.160)	Data 0.000 (0.003)	Loss 0.7343 (0.7051)	Acc@1 85.039 (85.195)	Acc@5 98.819 (99.377)
Epoch: [29][192/197]	Time 0.163 (0.159)	Data 0.000 (0.002)	Loss 0.7518 (0.7057)	Acc@1 85.827 (85.207)	Acc@5 99.213 (99.413)
Max memory in training epoch: 66.3603712
lr: 0.09205807653955844
1
Epoche:30/30; Lr: 0.09205807653955844
batch Size 256
Epoch: [30][0/197]	Time 0.186 (0.186)	Data 0.410 (0.410)	Loss 0.6548 (0.6548)	Acc@1 87.008 (87.008)	Acc@5 99.606 (99.606)
Epoch: [30][64/197]	Time 0.170 (0.162)	Data 0.000 (0.007)	Loss 0.7086 (0.6802)	Acc@1 85.039 (86.190)	Acc@5 99.213 (99.515)
Epoch: [30][128/197]	Time 0.159 (0.160)	Data 0.000 (0.003)	Loss 0.6602 (0.6941)	Acc@1 88.189 (85.720)	Acc@5 99.213 (99.435)
Epoch: [30][192/197]	Time 0.204 (0.160)	Data 0.000 (0.002)	Loss 0.6791 (0.6988)	Acc@1 85.827 (85.664)	Acc@5 99.606 (99.400)
Max memory in training epoch: 66.3603712
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 462574 ; 483924 ; 0.9558815020540415
[INFO] Storing checkpoint...
  78.05
Max memory: 103.3794048
 32.114s  j: 31 bis 35
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9924
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 31
Max memory: 0.1926656
lr: 0.09205807653955844
1
Epoche:31/35; Lr: 0.09205807653955844
batch Size 256
Epoch: [31][0/196]	Time 0.230 (0.230)	Data 0.455 (0.455)	Loss 0.7288 (0.7288)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [31][64/196]	Time 0.150 (0.163)	Data 0.000 (0.007)	Loss 0.6322 (0.6583)	Acc@1 88.672 (86.965)	Acc@5 99.609 (99.573)
Epoch: [31][128/196]	Time 0.167 (0.160)	Data 0.000 (0.004)	Loss 0.6211 (0.6856)	Acc@1 89.453 (86.019)	Acc@5 100.000 (99.449)
Epoch: [31][192/196]	Time 0.157 (0.158)	Data 0.000 (0.003)	Loss 0.7488 (0.6916)	Acc@1 83.203 (85.812)	Acc@5 98.828 (99.433)
Max memory in training epoch: 66.3321088
lr: 0.09205807653955844
1
Epoche:32/35; Lr: 0.09205807653955844
batch Size 256
Epoch: [32][0/196]	Time 0.224 (0.224)	Data 0.383 (0.383)	Loss 0.6674 (0.6674)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [32][64/196]	Time 0.173 (0.163)	Data 0.000 (0.006)	Loss 0.6390 (0.6879)	Acc@1 87.891 (86.052)	Acc@5 100.000 (99.393)
Epoch: [32][128/196]	Time 0.140 (0.162)	Data 0.000 (0.003)	Loss 0.7439 (0.6950)	Acc@1 83.594 (85.665)	Acc@5 99.219 (99.379)
Epoch: [32][192/196]	Time 0.150 (0.161)	Data 0.000 (0.002)	Loss 0.7510 (0.6963)	Acc@1 82.422 (85.680)	Acc@5 99.609 (99.415)
Max memory in training epoch: 66.2600192
lr: 0.09205807653955844
1
Epoche:33/35; Lr: 0.09205807653955844
batch Size 256
Epoch: [33][0/196]	Time 0.202 (0.202)	Data 0.407 (0.407)	Loss 0.6846 (0.6846)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [33][64/196]	Time 0.160 (0.164)	Data 0.000 (0.006)	Loss 0.6729 (0.6845)	Acc@1 85.156 (85.944)	Acc@5 100.000 (99.477)
Epoch: [33][128/196]	Time 0.142 (0.161)	Data 0.000 (0.003)	Loss 0.6803 (0.6841)	Acc@1 86.719 (86.050)	Acc@5 99.609 (99.461)
Epoch: [33][192/196]	Time 0.188 (0.161)	Data 0.000 (0.002)	Loss 0.6632 (0.6953)	Acc@1 87.109 (85.648)	Acc@5 100.000 (99.409)
Max memory in training epoch: 66.2338048
Drin!!
old memory: 663603712
new memory: 662338048
Faktor: 0.9980927412292715
New batch Size größer 258!!
lr: 0.09205807653955844
1
Epoche:34/35; Lr: 0.09205807653955844
batch Size 258
Epoch: [34][0/196]	Time 0.198 (0.198)	Data 0.363 (0.363)	Loss 0.6879 (0.6879)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [34][64/196]	Time 0.139 (0.162)	Data 0.000 (0.006)	Loss 0.6874 (0.6799)	Acc@1 85.156 (86.244)	Acc@5 99.609 (99.501)
Epoch: [34][128/196]	Time 0.168 (0.161)	Data 0.000 (0.003)	Loss 0.7627 (0.6910)	Acc@1 81.641 (85.868)	Acc@5 98.828 (99.482)
Epoch: [34][192/196]	Time 0.167 (0.160)	Data 0.000 (0.002)	Loss 0.7201 (0.6960)	Acc@1 85.547 (85.636)	Acc@5 99.219 (99.470)
Max memory in training epoch: 66.2338048
lr: 0.09205807653955844
1
Epoche:35/35; Lr: 0.09205807653955844
batch Size 258
Epoch: [35][0/196]	Time 0.256 (0.256)	Data 0.352 (0.352)	Loss 0.6474 (0.6474)	Acc@1 89.453 (89.453)	Acc@5 98.828 (98.828)
Epoch: [35][64/196]	Time 0.148 (0.160)	Data 0.000 (0.006)	Loss 0.6877 (0.6899)	Acc@1 87.109 (86.070)	Acc@5 99.219 (99.321)
Epoch: [35][128/196]	Time 0.155 (0.160)	Data 0.000 (0.003)	Loss 0.6283 (0.6827)	Acc@1 88.281 (86.177)	Acc@5 99.609 (99.364)
Epoch: [35][192/196]	Time 0.174 (0.161)	Data 0.000 (0.002)	Loss 0.6797 (0.6807)	Acc@1 85.938 (86.223)	Acc@5 99.219 (99.387)
Max memory in training epoch: 66.2338048
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 436028 ; 462574 ; 0.9426124252552024
[INFO] Storing checkpoint...
  76.08
Max memory: 102.886144
 31.934s  j: 36 bis 40
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4555
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 36
Max memory: 0.1821184
lr: 0.09277728026252374
1
Epoche:36/40; Lr: 0.09277728026252374
batch Size 258
Epoch: [36][0/194]	Time 0.288 (0.288)	Data 0.344 (0.344)	Loss 0.7178 (0.7178)	Acc@1 84.884 (84.884)	Acc@5 99.612 (99.612)
Epoch: [36][64/194]	Time 0.150 (0.163)	Data 0.000 (0.005)	Loss 0.7355 (0.6776)	Acc@1 84.884 (86.225)	Acc@5 99.612 (99.457)
Epoch: [36][128/194]	Time 0.162 (0.163)	Data 0.000 (0.003)	Loss 0.7017 (0.6838)	Acc@1 86.822 (86.056)	Acc@5 99.225 (99.477)
Epoch: [36][192/194]	Time 0.139 (0.162)	Data 0.000 (0.002)	Loss 0.8283 (0.6866)	Acc@1 80.620 (85.854)	Acc@5 99.225 (99.478)
Max memory in training epoch: 66.2365696
lr: 0.09277728026252374
1
Epoche:37/40; Lr: 0.09277728026252374
batch Size 258
Epoch: [37][0/194]	Time 0.200 (0.200)	Data 0.344 (0.344)	Loss 0.7107 (0.7107)	Acc@1 86.047 (86.047)	Acc@5 99.225 (99.225)
Epoch: [37][64/194]	Time 0.163 (0.164)	Data 0.000 (0.005)	Loss 0.5388 (0.6741)	Acc@1 89.922 (86.363)	Acc@5 100.000 (99.386)
Epoch: [37][128/194]	Time 0.161 (0.162)	Data 0.000 (0.003)	Loss 0.7432 (0.6794)	Acc@1 83.721 (86.152)	Acc@5 99.612 (99.462)
Epoch: [37][192/194]	Time 0.162 (0.163)	Data 0.000 (0.002)	Loss 0.6477 (0.6884)	Acc@1 86.047 (85.940)	Acc@5 100.000 (99.440)
Max memory in training epoch: 66.224896
lr: 0.09277728026252374
1
Epoche:38/40; Lr: 0.09277728026252374
batch Size 258
Epoch: [38][0/194]	Time 0.213 (0.213)	Data 0.482 (0.482)	Loss 0.5824 (0.5824)	Acc@1 88.760 (88.760)	Acc@5 99.612 (99.612)
Epoch: [38][64/194]	Time 0.146 (0.153)	Data 0.000 (0.008)	Loss 0.6785 (0.6714)	Acc@1 87.209 (86.553)	Acc@5 99.612 (99.493)
Epoch: [38][128/194]	Time 0.191 (0.159)	Data 0.000 (0.004)	Loss 0.7139 (0.6726)	Acc@1 86.434 (86.284)	Acc@5 100.000 (99.474)
Epoch: [38][192/194]	Time 0.147 (0.159)	Data 0.000 (0.003)	Loss 0.6348 (0.6802)	Acc@1 88.760 (86.163)	Acc@5 100.000 (99.486)
Max memory in training epoch: 66.224896
Drin!!
old memory: 662338048
new memory: 662248960
Faktor: 0.9998654946665543
New batch Size größer 261!!
lr: 0.09277728026252374
1
Epoche:39/40; Lr: 0.09277728026252374
batch Size 261
Epoch: [39][0/194]	Time 0.198 (0.198)	Data 0.387 (0.387)	Loss 0.7539 (0.7539)	Acc@1 81.783 (81.783)	Acc@5 99.612 (99.612)
Epoch: [39][64/194]	Time 0.149 (0.161)	Data 0.000 (0.006)	Loss 0.7092 (0.6764)	Acc@1 84.109 (86.208)	Acc@5 99.225 (99.457)
Epoch: [39][128/194]	Time 0.170 (0.162)	Data 0.000 (0.003)	Loss 0.6452 (0.6765)	Acc@1 87.209 (86.257)	Acc@5 100.000 (99.450)
Epoch: [39][192/194]	Time 0.183 (0.162)	Data 0.000 (0.002)	Loss 0.6296 (0.6791)	Acc@1 87.209 (86.261)	Acc@5 100.000 (99.464)
Max memory in training epoch: 66.224896
lr: 0.09277728026252374
1
Epoche:40/40; Lr: 0.09277728026252374
batch Size 261
Epoch: [40][0/194]	Time 0.215 (0.215)	Data 0.485 (0.485)	Loss 0.6660 (0.6660)	Acc@1 87.209 (87.209)	Acc@5 100.000 (100.000)
Epoch: [40][64/194]	Time 0.147 (0.160)	Data 0.000 (0.008)	Loss 0.7114 (0.6640)	Acc@1 84.884 (86.696)	Acc@5 100.000 (99.600)
Epoch: [40][128/194]	Time 0.159 (0.161)	Data 0.000 (0.004)	Loss 0.7356 (0.6777)	Acc@1 84.496 (86.362)	Acc@5 99.612 (99.516)
Epoch: [40][192/194]	Time 0.170 (0.160)	Data 0.000 (0.003)	Loss 0.8143 (0.6811)	Acc@1 84.884 (86.189)	Acc@5 99.225 (99.476)
Max memory in training epoch: 66.224896
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 427078 ; 436028 ; 0.9794737952608548
[INFO] Storing checkpoint...
  64.18
Max memory: 101.1921408
 31.700s  j: 41 bis 45
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5044
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 41
Max memory: 0.1785344
lr: 0.09458933651765115
1
Epoche:41/45; Lr: 0.09458933651765115
batch Size 261
Epoch: [41][0/192]	Time 0.228 (0.228)	Data 0.526 (0.526)	Loss 0.7046 (0.7046)	Acc@1 84.674 (84.674)	Acc@5 99.617 (99.617)
Epoch: [41][64/192]	Time 0.127 (0.152)	Data 0.000 (0.008)	Loss 0.8051 (0.6406)	Acc@1 79.693 (87.492)	Acc@5 98.851 (99.493)
Epoch: [41][128/192]	Time 0.146 (0.157)	Data 0.000 (0.004)	Loss 0.6896 (0.6623)	Acc@1 83.908 (86.771)	Acc@5 100.000 (99.462)
Max memory in training epoch: 66.5604608
lr: 0.09458933651765115
1
Epoche:42/45; Lr: 0.09458933651765115
batch Size 261
Epoch: [42][0/192]	Time 0.238 (0.238)	Data 0.449 (0.449)	Loss 0.5948 (0.5948)	Acc@1 88.506 (88.506)	Acc@5 99.234 (99.234)
Epoch: [42][64/192]	Time 0.166 (0.163)	Data 0.000 (0.007)	Loss 0.7158 (0.6678)	Acc@1 86.973 (86.566)	Acc@5 99.617 (99.493)
Epoch: [42][128/192]	Time 0.177 (0.165)	Data 0.000 (0.004)	Loss 0.6854 (0.6818)	Acc@1 86.207 (85.984)	Acc@5 98.851 (99.436)
Max memory in training epoch: 66.621696
lr: 0.09458933651765115
1
Epoche:43/45; Lr: 0.09458933651765115
batch Size 261
Epoch: [43][0/192]	Time 0.200 (0.200)	Data 0.435 (0.435)	Loss 0.6192 (0.6192)	Acc@1 88.123 (88.123)	Acc@5 98.851 (98.851)
Epoch: [43][64/192]	Time 0.176 (0.169)	Data 0.000 (0.007)	Loss 0.7086 (0.6733)	Acc@1 85.824 (86.407)	Acc@5 100.000 (99.411)
Epoch: [43][128/192]	Time 0.164 (0.164)	Data 0.000 (0.004)	Loss 0.6711 (0.6746)	Acc@1 85.824 (86.382)	Acc@5 99.234 (99.376)
Max memory in training epoch: 66.621696
Drin!!
old memory: 662248960
new memory: 666216960
Faktor: 1.0059917043886335
New batch Size kleiner 262!!
lr: 0.09458933651765115
1
Epoche:44/45; Lr: 0.09458933651765115
batch Size 262
Epoch: [44][0/192]	Time 0.256 (0.256)	Data 0.339 (0.339)	Loss 0.6872 (0.6872)	Acc@1 86.973 (86.973)	Acc@5 98.851 (98.851)
Epoch: [44][64/192]	Time 0.154 (0.162)	Data 0.000 (0.005)	Loss 0.6360 (0.6649)	Acc@1 85.824 (86.513)	Acc@5 99.617 (99.487)
Epoch: [44][128/192]	Time 0.162 (0.161)	Data 0.000 (0.003)	Loss 0.6669 (0.6669)	Acc@1 87.356 (86.605)	Acc@5 98.467 (99.442)
Max memory in training epoch: 66.621696
lr: 0.09458933651765115
1
Epoche:45/45; Lr: 0.09458933651765115
batch Size 262
Epoch: [45][0/192]	Time 0.224 (0.224)	Data 0.465 (0.465)	Loss 0.5663 (0.5663)	Acc@1 89.655 (89.655)	Acc@5 100.000 (100.000)
Epoch: [45][64/192]	Time 0.153 (0.164)	Data 0.000 (0.007)	Loss 0.7105 (0.6706)	Acc@1 86.973 (86.767)	Acc@5 99.617 (99.493)
Epoch: [45][128/192]	Time 0.136 (0.162)	Data 0.000 (0.004)	Loss 0.6365 (0.6643)	Acc@1 88.123 (86.851)	Acc@5 99.617 (99.477)
Max memory in training epoch: 66.621696
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 415528 ; 427078 ; 0.9729557598377814
[INFO] Storing checkpoint...
  78.17
Max memory: 100.4475904
 31.959s  j: 46 bis 50
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9196
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 46
Max memory: 0.1739264
lr: 0.09680627409228361
1
Epoche:46/50; Lr: 0.09680627409228361
batch Size 262
Epoch: [46][0/191]	Time 0.247 (0.247)	Data 0.403 (0.403)	Loss 0.7154 (0.7154)	Acc@1 85.115 (85.115)	Acc@5 99.618 (99.618)
Epoch: [46][64/191]	Time 0.166 (0.160)	Data 0.000 (0.006)	Loss 0.6330 (0.6521)	Acc@1 88.931 (87.058)	Acc@5 99.237 (99.513)
Epoch: [46][128/191]	Time 0.156 (0.159)	Data 0.000 (0.003)	Loss 0.6195 (0.6717)	Acc@1 89.313 (86.372)	Acc@5 100.000 (99.414)
Max memory in training epoch: 66.235648
lr: 0.09680627409228361
1
Epoche:47/50; Lr: 0.09680627409228361
batch Size 262
Epoch: [47][0/191]	Time 0.188 (0.188)	Data 0.397 (0.397)	Loss 0.6658 (0.6658)	Acc@1 85.878 (85.878)	Acc@5 97.710 (97.710)
Epoch: [47][64/191]	Time 0.170 (0.162)	Data 0.000 (0.006)	Loss 0.7718 (0.6585)	Acc@1 81.298 (86.788)	Acc@5 99.618 (99.507)
Epoch: [47][128/191]	Time 0.131 (0.159)	Data 0.000 (0.003)	Loss 0.6557 (0.6708)	Acc@1 88.931 (86.422)	Acc@5 99.237 (99.482)
Max memory in training epoch: 66.2373888
lr: 0.09680627409228361
1
Epoche:48/50; Lr: 0.09680627409228361
batch Size 262
Epoch: [48][0/191]	Time 0.203 (0.203)	Data 0.425 (0.425)	Loss 0.5995 (0.5995)	Acc@1 88.931 (88.931)	Acc@5 99.618 (99.618)
Epoch: [48][64/191]	Time 0.154 (0.161)	Data 0.000 (0.007)	Loss 0.6327 (0.6712)	Acc@1 87.405 (86.583)	Acc@5 100.000 (99.407)
Epoch: [48][128/191]	Time 0.174 (0.161)	Data 0.000 (0.004)	Loss 0.7252 (0.6697)	Acc@1 82.824 (86.499)	Acc@5 100.000 (99.450)
Max memory in training epoch: 66.2373888
Drin!!
old memory: 666216960
new memory: 662373888
Faktor: 0.9942315008011804
New batch Size größer 264!!
lr: 0.09680627409228361
1
Epoche:49/50; Lr: 0.09680627409228361
batch Size 264
Epoch: [49][0/191]	Time 0.246 (0.246)	Data 0.333 (0.333)	Loss 0.6710 (0.6710)	Acc@1 85.878 (85.878)	Acc@5 100.000 (100.000)
Epoch: [49][64/191]	Time 0.162 (0.163)	Data 0.000 (0.005)	Loss 0.6768 (0.6551)	Acc@1 87.405 (86.858)	Acc@5 99.618 (99.477)
Epoch: [49][128/191]	Time 0.162 (0.161)	Data 0.000 (0.003)	Loss 0.6199 (0.6626)	Acc@1 89.313 (86.547)	Acc@5 98.855 (99.456)
Max memory in training epoch: 66.2373888
lr: 0.09680627409228361
1
Epoche:50/50; Lr: 0.09680627409228361
batch Size 264
Epoch: [50][0/191]	Time 0.246 (0.246)	Data 0.490 (0.490)	Loss 0.6687 (0.6687)	Acc@1 88.931 (88.931)	Acc@5 98.855 (98.855)
Epoch: [50][64/191]	Time 0.143 (0.161)	Data 0.000 (0.008)	Loss 0.6590 (0.6568)	Acc@1 88.931 (87.287)	Acc@5 99.618 (99.530)
Epoch: [50][128/191]	Time 0.150 (0.160)	Data 0.000 (0.004)	Loss 0.6413 (0.6621)	Acc@1 86.641 (86.928)	Acc@5 100.000 (99.485)
Max memory in training epoch: 66.2373888
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 408886 ; 415528 ; 0.9840155176065151
[INFO] Storing checkpoint...
  77.15
Max memory: 99.612928
 31.059s  j: 51 bis 55
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8812
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 51
Max memory: 0.171264
lr: 0.09983147015766747
1
Epoche:51/55; Lr: 0.09983147015766747
batch Size 264
Epoch: [51][0/190]	Time 0.307 (0.307)	Data 0.364 (0.364)	Loss 0.6732 (0.6732)	Acc@1 85.985 (85.985)	Acc@5 99.242 (99.242)
Epoch: [51][64/190]	Time 0.143 (0.159)	Data 0.000 (0.006)	Loss 0.5974 (0.6302)	Acc@1 89.773 (87.617)	Acc@5 100.000 (99.522)
Epoch: [51][128/190]	Time 0.187 (0.160)	Data 0.000 (0.003)	Loss 0.6034 (0.6514)	Acc@1 88.258 (87.010)	Acc@5 99.621 (99.474)
Max memory in training epoch: 66.2461952
lr: 0.09983147015766747
1
Epoche:52/55; Lr: 0.09983147015766747
batch Size 264
Epoch: [52][0/190]	Time 0.245 (0.245)	Data 0.444 (0.444)	Loss 0.7223 (0.7223)	Acc@1 87.879 (87.879)	Acc@5 99.242 (99.242)
Epoch: [52][64/190]	Time 0.172 (0.163)	Data 0.000 (0.007)	Loss 0.6409 (0.6743)	Acc@1 89.773 (86.655)	Acc@5 99.621 (99.441)
Epoch: [52][128/190]	Time 0.154 (0.163)	Data 0.000 (0.004)	Loss 0.6545 (0.6781)	Acc@1 85.227 (86.331)	Acc@5 99.242 (99.419)
Max memory in training epoch: 66.2775296
lr: 0.09983147015766747
1
Epoche:53/55; Lr: 0.09983147015766747
batch Size 264
Epoch: [53][0/190]	Time 0.259 (0.259)	Data 0.405 (0.405)	Loss 0.6521 (0.6521)	Acc@1 85.985 (85.985)	Acc@5 100.000 (100.000)
Epoch: [53][64/190]	Time 0.180 (0.163)	Data 0.000 (0.006)	Loss 0.6505 (0.6700)	Acc@1 88.636 (86.620)	Acc@5 99.242 (99.411)
Epoch: [53][128/190]	Time 0.164 (0.161)	Data 0.000 (0.003)	Loss 0.6860 (0.6734)	Acc@1 87.879 (86.478)	Acc@5 99.242 (99.451)
Max memory in training epoch: 66.2775296
Drin!!
old memory: 662373888
new memory: 662775296
Faktor: 1.0006060142274207
New batch Size kleiner 264!!
lr: 0.09983147015766747
1
Epoche:54/55; Lr: 0.09983147015766747
batch Size 264
Epoch: [54][0/190]	Time 0.207 (0.207)	Data 0.387 (0.387)	Loss 0.5576 (0.5576)	Acc@1 90.152 (90.152)	Acc@5 99.621 (99.621)
Epoch: [54][64/190]	Time 0.192 (0.164)	Data 0.000 (0.006)	Loss 0.7378 (0.6589)	Acc@1 84.091 (86.754)	Acc@5 99.242 (99.510)
Epoch: [54][128/190]	Time 0.163 (0.158)	Data 0.000 (0.003)	Loss 0.5761 (0.6613)	Acc@1 90.530 (86.654)	Acc@5 99.621 (99.545)
Max memory in training epoch: 66.2775296
lr: 0.09983147015766747
1
Epoche:55/55; Lr: 0.09983147015766747
batch Size 264
Epoch: [55][0/190]	Time 0.230 (0.230)	Data 0.389 (0.389)	Loss 0.6593 (0.6593)	Acc@1 87.121 (87.121)	Acc@5 99.621 (99.621)
Epoch: [55][64/190]	Time 0.146 (0.164)	Data 0.000 (0.006)	Loss 0.6885 (0.6691)	Acc@1 85.227 (86.585)	Acc@5 99.242 (99.423)
Epoch: [55][128/190]	Time 0.182 (0.162)	Data 0.000 (0.003)	Loss 0.7465 (0.6711)	Acc@1 85.227 (86.560)	Acc@5 98.864 (99.466)
Max memory in training epoch: 66.2775296
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 399648 ; 408886 ; 0.9774069055922677
[INFO] Storing checkpoint...
  77.6
Max memory: 98.9256192
 31.334s  j: 56 bis 60
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7798
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 56
Max memory: 0.16768
lr: 0.10295120360009458
1
Epoche:56/60; Lr: 0.10295120360009458
batch Size 264
Epoch: [56][0/190]	Time 0.292 (0.292)	Data 0.340 (0.340)	Loss 0.6682 (0.6682)	Acc@1 85.227 (85.227)	Acc@5 100.000 (100.000)
Epoch: [56][64/190]	Time 0.185 (0.163)	Data 0.000 (0.005)	Loss 0.6913 (0.6520)	Acc@1 85.227 (86.923)	Acc@5 99.242 (99.499)
Epoch: [56][128/190]	Time 0.158 (0.161)	Data 0.000 (0.003)	Loss 0.6670 (0.6579)	Acc@1 86.364 (86.748)	Acc@5 99.621 (99.516)
Max memory in training epoch: 65.7278464
lr: 0.10295120360009458
1
Epoche:57/60; Lr: 0.10295120360009458
batch Size 264
Epoch: [57][0/190]	Time 0.229 (0.229)	Data 0.482 (0.482)	Loss 0.6837 (0.6837)	Acc@1 86.364 (86.364)	Acc@5 99.621 (99.621)
Epoch: [57][64/190]	Time 0.150 (0.165)	Data 0.000 (0.008)	Loss 0.6897 (0.6749)	Acc@1 84.091 (86.329)	Acc@5 99.621 (99.522)
Epoch: [57][128/190]	Time 0.187 (0.158)	Data 0.000 (0.004)	Loss 0.6000 (0.6723)	Acc@1 89.394 (86.484)	Acc@5 99.621 (99.483)
Max memory in training epoch: 65.7114624
lr: 0.10295120360009458
1
Epoche:58/60; Lr: 0.10295120360009458
batch Size 264
Epoch: [58][0/190]	Time 0.257 (0.257)	Data 0.410 (0.410)	Loss 0.7201 (0.7201)	Acc@1 86.742 (86.742)	Acc@5 98.864 (98.864)
Epoch: [58][64/190]	Time 0.166 (0.162)	Data 0.000 (0.007)	Loss 0.6837 (0.6634)	Acc@1 85.606 (87.075)	Acc@5 99.621 (99.470)
Epoch: [58][128/190]	Time 0.166 (0.161)	Data 0.000 (0.003)	Loss 0.6206 (0.6761)	Acc@1 89.015 (86.484)	Acc@5 100.000 (99.445)
Max memory in training epoch: 65.7114624
Drin!!
old memory: 662775296
new memory: 657114624
Faktor: 0.9914591385132134
New batch Size größer 269!!
lr: 0.10295120360009458
1
Epoche:59/60; Lr: 0.10295120360009458
batch Size 269
Epoch: [59][0/190]	Time 0.210 (0.210)	Data 0.354 (0.354)	Loss 0.6542 (0.6542)	Acc@1 87.879 (87.879)	Acc@5 99.242 (99.242)
Epoch: [59][64/190]	Time 0.155 (0.169)	Data 0.000 (0.006)	Loss 0.6377 (0.6737)	Acc@1 87.121 (86.585)	Acc@5 100.000 (99.476)
Epoch: [59][128/190]	Time 0.167 (0.164)	Data 0.000 (0.003)	Loss 0.6203 (0.6737)	Acc@1 88.636 (86.584)	Acc@5 99.621 (99.445)
Max memory in training epoch: 65.7114624
lr: 0.10295120360009458
1
Epoche:60/60; Lr: 0.10295120360009458
batch Size 269
Epoch: [60][0/190]	Time 0.248 (0.248)	Data 0.395 (0.395)	Loss 0.6629 (0.6629)	Acc@1 87.879 (87.879)	Acc@5 98.485 (98.485)
Epoch: [60][64/190]	Time 0.161 (0.163)	Data 0.000 (0.006)	Loss 0.6324 (0.6635)	Acc@1 87.879 (86.742)	Acc@5 99.242 (99.429)
Epoch: [60][128/190]	Time 0.134 (0.161)	Data 0.000 (0.003)	Loss 0.6913 (0.6625)	Acc@1 84.848 (86.778)	Acc@5 98.485 (99.451)
Max memory in training epoch: 65.7114624
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 391130 ; 399648 ; 0.9786862438946272
[INFO] Storing checkpoint...
  79.35
Max memory: 98.3412224
 30.934s  j: 61 bis 65
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7368
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 61
Max memory: 0.1642496
lr: 0.10817919440791188
1
Epoche:61/65; Lr: 0.10817919440791188
batch Size 269
Epoch: [61][0/186]	Time 0.254 (0.254)	Data 0.512 (0.512)	Loss 0.6525 (0.6525)	Acc@1 87.732 (87.732)	Acc@5 99.628 (99.628)
Epoch: [61][64/186]	Time 0.164 (0.160)	Data 0.000 (0.008)	Loss 0.6315 (0.6566)	Acc@1 85.874 (87.052)	Acc@5 99.628 (99.468)
Epoch: [61][128/186]	Time 0.159 (0.158)	Data 0.000 (0.004)	Loss 0.6599 (0.6636)	Acc@1 86.617 (86.868)	Acc@5 100.000 (99.476)
Max memory in training epoch: 66.4403456
lr: 0.10817919440791188
1
Epoche:62/65; Lr: 0.10817919440791188
batch Size 269
Epoch: [62][0/186]	Time 0.253 (0.253)	Data 0.352 (0.352)	Loss 0.6892 (0.6892)	Acc@1 84.387 (84.387)	Acc@5 100.000 (100.000)
Epoch: [62][64/186]	Time 0.176 (0.162)	Data 0.000 (0.006)	Loss 0.6289 (0.6622)	Acc@1 87.732 (86.840)	Acc@5 99.628 (99.440)
Epoch: [62][128/186]	Time 0.128 (0.162)	Data 0.000 (0.003)	Loss 0.7172 (0.6601)	Acc@1 84.015 (86.842)	Acc@5 99.257 (99.516)
Max memory in training epoch: 66.4340992
lr: 0.10817919440791188
1
Epoche:63/65; Lr: 0.10817919440791188
batch Size 269
Epoch: [63][0/186]	Time 0.266 (0.266)	Data 0.372 (0.372)	Loss 0.6488 (0.6488)	Acc@1 89.219 (89.219)	Acc@5 99.628 (99.628)
Epoch: [63][64/186]	Time 0.161 (0.165)	Data 0.000 (0.006)	Loss 0.5883 (0.6611)	Acc@1 90.706 (86.949)	Acc@5 99.628 (99.422)
Epoch: [63][128/186]	Time 0.172 (0.162)	Data 0.000 (0.003)	Loss 0.7406 (0.6734)	Acc@1 83.271 (86.438)	Acc@5 99.257 (99.473)
Max memory in training epoch: 66.4340992
Drin!!
old memory: 657114624
new memory: 664340992
Faktor: 1.0109971194310234
New batch Size kleiner 271!!
lr: 0.10817919440791188
1
Epoche:64/65; Lr: 0.10817919440791188
batch Size 271
Epoch: [64][0/186]	Time 0.243 (0.243)	Data 0.455 (0.455)	Loss 0.7603 (0.7603)	Acc@1 82.900 (82.900)	Acc@5 99.257 (99.257)
Epoch: [64][64/186]	Time 0.147 (0.159)	Data 0.000 (0.007)	Loss 0.7854 (0.6609)	Acc@1 82.156 (86.812)	Acc@5 98.885 (99.520)
Epoch: [64][128/186]	Time 0.175 (0.159)	Data 0.000 (0.004)	Loss 0.7180 (0.6695)	Acc@1 82.156 (86.562)	Acc@5 99.628 (99.487)
Max memory in training epoch: 66.4340992
lr: 0.10817919440791188
1
Epoche:65/65; Lr: 0.10817919440791188
batch Size 271
Epoch: [65][0/186]	Time 0.220 (0.220)	Data 0.497 (0.497)	Loss 0.6710 (0.6710)	Acc@1 85.130 (85.130)	Acc@5 99.257 (99.257)
Epoch: [65][64/186]	Time 0.165 (0.160)	Data 0.000 (0.008)	Loss 0.7367 (0.6616)	Acc@1 85.874 (86.806)	Acc@5 99.257 (99.497)
Epoch: [65][128/186]	Time 0.164 (0.161)	Data 0.000 (0.004)	Loss 0.7364 (0.6693)	Acc@1 84.015 (86.580)	Acc@5 99.628 (99.516)
Max memory in training epoch: 66.4340992
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv21.weight

 RM:  module.conv22.weight
numoFStages: 3
Count: 381104 ; 391130 ; 0.9743665788868151
[INFO] Storing checkpoint...
  71.17
Max memory: 97.7851392
 30.444s  j: 66 bis 70
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2890
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 66
Max memory: 0.1597952
lr: 0.11451781908025047
1
Epoche:66/70; Lr: 0.11451781908025047
batch Size 271
Epoch: [66][0/185]	Time 0.273 (0.273)	Data 0.447 (0.447)	Loss 0.5765 (0.5765)	Acc@1 88.561 (88.561)	Acc@5 99.631 (99.631)
Epoch: [66][64/185]	Time 0.158 (0.155)	Data 0.000 (0.007)	Loss 0.6367 (0.6470)	Acc@1 86.347 (87.312)	Acc@5 99.631 (99.552)
Epoch: [66][128/185]	Time 0.163 (0.155)	Data 0.000 (0.004)	Loss 0.6467 (0.6644)	Acc@1 85.978 (86.753)	Acc@5 99.262 (99.459)
Max memory in training epoch: 64.5054976
lr: 0.11451781908025047
1
Epoche:67/70; Lr: 0.11451781908025047
batch Size 271
Epoch: [67][0/185]	Time 0.216 (0.216)	Data 0.430 (0.430)	Loss 0.7029 (0.7029)	Acc@1 85.609 (85.609)	Acc@5 99.631 (99.631)
Epoch: [67][64/185]	Time 0.131 (0.149)	Data 0.000 (0.007)	Loss 0.7087 (0.6716)	Acc@1 87.085 (86.801)	Acc@5 99.262 (99.427)
Epoch: [67][128/185]	Time 0.148 (0.148)	Data 0.000 (0.004)	Loss 0.6464 (0.6744)	Acc@1 85.609 (86.610)	Acc@5 100.000 (99.451)
Max memory in training epoch: 64.5538816
lr: 0.11451781908025047
1
Epoche:68/70; Lr: 0.11451781908025047
batch Size 271
Epoch: [68][0/185]	Time 0.228 (0.228)	Data 0.440 (0.440)	Loss 0.6757 (0.6757)	Acc@1 83.395 (83.395)	Acc@5 100.000 (100.000)
Epoch: [68][64/185]	Time 0.155 (0.150)	Data 0.000 (0.007)	Loss 0.6302 (0.6846)	Acc@1 87.454 (86.154)	Acc@5 99.262 (99.381)
Epoch: [68][128/185]	Time 0.141 (0.152)	Data 0.000 (0.004)	Loss 0.6549 (0.6866)	Acc@1 87.085 (86.195)	Acc@5 99.262 (99.408)
Max memory in training epoch: 64.5538816
Drin!!
old memory: 664340992
new memory: 645538816
Faktor: 0.9716980041478458
New batch Size größer 281!!
lr: 0.11451781908025047
1
Epoche:69/70; Lr: 0.11451781908025047
batch Size 281
Epoch: [69][0/185]	Time 0.193 (0.193)	Data 0.426 (0.426)	Loss 0.6859 (0.6859)	Acc@1 87.454 (87.454)	Acc@5 99.631 (99.631)
Epoch: [69][64/185]	Time 0.157 (0.153)	Data 0.000 (0.007)	Loss 0.7178 (0.6812)	Acc@1 83.764 (85.989)	Acc@5 98.524 (99.398)
Epoch: [69][128/185]	Time 0.144 (0.153)	Data 0.000 (0.003)	Loss 0.7025 (0.6841)	Acc@1 86.716 (86.112)	Acc@5 100.000 (99.419)
Max memory in training epoch: 64.5538816
lr: 0.11451781908025047
1
Epoche:70/70; Lr: 0.11451781908025047
batch Size 281
Epoch: [70][0/185]	Time 0.219 (0.219)	Data 0.466 (0.466)	Loss 0.6872 (0.6872)	Acc@1 86.347 (86.347)	Acc@5 98.893 (98.893)
Epoch: [70][64/185]	Time 0.164 (0.155)	Data 0.000 (0.007)	Loss 0.7392 (0.6808)	Acc@1 85.240 (86.370)	Acc@5 98.893 (99.393)
Epoch: [70][128/185]	Time 0.150 (0.154)	Data 0.000 (0.004)	Loss 0.6684 (0.6823)	Acc@1 86.716 (86.247)	Acc@5 99.262 (99.382)
Max memory in training epoch: 64.5538816
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 377204 ; 381104 ; 0.9897665729039842
[INFO] Storing checkpoint...
  75.74
Max memory: 94.3684096
 29.030s  j: 71 bis 75
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1948
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 71
Max memory: 0.158208
lr: 0.12570119984980618
1
Epoche:71/75; Lr: 0.12570119984980618
batch Size 281
Epoch: [71][0/178]	Time 0.243 (0.243)	Data 0.569 (0.569)	Loss 0.5992 (0.5992)	Acc@1 88.256 (88.256)	Acc@5 100.000 (100.000)
Epoch: [71][64/178]	Time 0.149 (0.154)	Data 0.000 (0.009)	Loss 0.7043 (0.6595)	Acc@1 83.986 (87.013)	Acc@5 100.000 (99.507)
Epoch: [71][128/178]	Time 0.153 (0.154)	Data 0.000 (0.005)	Loss 0.8641 (0.6835)	Acc@1 81.851 (86.187)	Acc@5 99.288 (99.446)
Max memory in training epoch: 66.8092928
lr: 0.12570119984980618
1
Epoche:72/75; Lr: 0.12570119984980618
batch Size 281
Epoch: [72][0/178]	Time 0.251 (0.251)	Data 0.413 (0.413)	Loss 0.6944 (0.6944)	Acc@1 85.765 (85.765)	Acc@5 99.288 (99.288)
Epoch: [72][64/178]	Time 0.161 (0.158)	Data 0.000 (0.007)	Loss 0.7584 (0.6818)	Acc@1 83.630 (86.203)	Acc@5 99.288 (99.496)
Epoch: [72][128/178]	Time 0.153 (0.159)	Data 0.000 (0.003)	Loss 0.6592 (0.6881)	Acc@1 85.409 (86.049)	Acc@5 99.288 (99.399)
Max memory in training epoch: 67.8628352
lr: 0.12570119984980618
1
Epoche:73/75; Lr: 0.12570119984980618
batch Size 281
Epoch: [73][0/178]	Time 0.207 (0.207)	Data 0.498 (0.498)	Loss 0.6844 (0.6844)	Acc@1 87.189 (87.189)	Acc@5 99.644 (99.644)
Epoch: [73][64/178]	Time 0.162 (0.158)	Data 0.000 (0.008)	Loss 0.6595 (0.6678)	Acc@1 88.256 (86.926)	Acc@5 98.932 (99.524)
Epoch: [73][128/178]	Time 0.159 (0.158)	Data 0.000 (0.004)	Loss 0.7048 (0.6848)	Acc@1 82.562 (86.358)	Acc@5 100.000 (99.421)
Max memory in training epoch: 67.8628352
Drin!!
old memory: 645538816
new memory: 678628352
Faktor: 1.0512587859627638
New batch Size kleiner 295!!
lr: 0.12570119984980618
1
Epoche:74/75; Lr: 0.12570119984980618
batch Size 295
Epoch: [74][0/178]	Time 0.215 (0.215)	Data 0.384 (0.384)	Loss 0.6210 (0.6210)	Acc@1 87.544 (87.544)	Acc@5 99.644 (99.644)
Epoch: [74][64/178]	Time 0.162 (0.159)	Data 0.000 (0.006)	Loss 0.6935 (0.6947)	Acc@1 86.833 (86.159)	Acc@5 99.644 (99.365)
Epoch: [74][128/178]	Time 0.164 (0.160)	Data 0.000 (0.003)	Loss 0.7229 (0.6894)	Acc@1 86.833 (86.173)	Acc@5 99.644 (99.429)
Max memory in training epoch: 67.8628352
lr: 0.12570119984980618
1
Epoche:75/75; Lr: 0.12570119984980618
batch Size 295
Epoch: [75][0/178]	Time 0.243 (0.243)	Data 0.404 (0.404)	Loss 0.6497 (0.6497)	Acc@1 86.121 (86.121)	Acc@5 99.644 (99.644)
Epoch: [75][64/178]	Time 0.159 (0.158)	Data 0.000 (0.006)	Loss 0.6798 (0.6843)	Acc@1 86.121 (85.990)	Acc@5 98.932 (99.403)
Epoch: [75][128/178]	Time 0.147 (0.156)	Data 0.000 (0.003)	Loss 0.6166 (0.6851)	Acc@1 88.968 (86.035)	Acc@5 99.288 (99.440)
Max memory in training epoch: 67.8628352
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 373878 ; 377204 ; 0.9911824901114517
[INFO] Storing checkpoint...
  73.92
Max memory: 93.8336256
 28.355s  j: 76 bis 80
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9736
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 76
Max memory: 0.1569792
lr: 0.14485099201442508
1
Epoche:76/80; Lr: 0.14485099201442508
batch Size 295
Epoch: [76][0/170]	Time 0.305 (0.305)	Data 0.416 (0.416)	Loss 0.6929 (0.6929)	Acc@1 85.085 (85.085)	Acc@5 99.661 (99.661)
Epoch: [76][64/170]	Time 0.161 (0.155)	Data 0.000 (0.007)	Loss 0.6928 (0.6827)	Acc@1 84.068 (86.169)	Acc@5 99.661 (99.379)
Epoch: [76][128/170]	Time 0.160 (0.155)	Data 0.000 (0.003)	Loss 0.7199 (0.6979)	Acc@1 86.441 (85.721)	Acc@5 99.661 (99.382)
Max memory in training epoch: 68.9179648
lr: 0.14485099201442508
1
Epoche:77/80; Lr: 0.14485099201442508
batch Size 295
Epoch: [77][0/170]	Time 0.198 (0.198)	Data 0.474 (0.474)	Loss 0.6804 (0.6804)	Acc@1 87.119 (87.119)	Acc@5 99.322 (99.322)
Epoch: [77][64/170]	Time 0.173 (0.157)	Data 0.000 (0.007)	Loss 0.7139 (0.7199)	Acc@1 85.085 (85.163)	Acc@5 98.983 (99.369)
Epoch: [77][128/170]	Time 0.162 (0.157)	Data 0.000 (0.004)	Loss 0.7290 (0.7147)	Acc@1 84.746 (85.305)	Acc@5 98.644 (99.388)
Max memory in training epoch: 69.0510848
lr: 0.14485099201442508
1
Epoche:78/80; Lr: 0.14485099201442508
batch Size 295
Epoch: [78][0/170]	Time 0.233 (0.233)	Data 0.408 (0.408)	Loss 0.6869 (0.6869)	Acc@1 86.102 (86.102)	Acc@5 100.000 (100.000)
Epoch: [78][64/170]	Time 0.148 (0.156)	Data 0.000 (0.006)	Loss 0.7502 (0.6916)	Acc@1 84.068 (86.050)	Acc@5 99.322 (99.489)
Epoch: [78][128/170]	Time 0.167 (0.156)	Data 0.000 (0.003)	Loss 0.7296 (0.7044)	Acc@1 86.780 (85.581)	Acc@5 99.322 (99.448)
Max memory in training epoch: 69.0510848
Drin!!
old memory: 678628352
new memory: 690510848
Faktor: 1.0175095779081154
New batch Size kleiner 300!!
lr: 0.14485099201442508
1
Epoche:79/80; Lr: 0.14485099201442508
batch Size 300
Epoch: [79][0/170]	Time 0.222 (0.222)	Data 0.439 (0.439)	Loss 0.6744 (0.6744)	Acc@1 87.797 (87.797)	Acc@5 99.661 (99.661)
Epoch: [79][64/170]	Time 0.153 (0.160)	Data 0.000 (0.007)	Loss 0.7335 (0.7052)	Acc@1 84.068 (85.867)	Acc@5 100.000 (99.390)
Epoch: [79][128/170]	Time 0.162 (0.159)	Data 0.000 (0.004)	Loss 0.7154 (0.7098)	Acc@1 87.797 (85.673)	Acc@5 98.644 (99.432)
Max memory in training epoch: 69.0510848
lr: 0.14485099201442508
1
Epoche:80/80; Lr: 0.14485099201442508
batch Size 300
Epoch: [80][0/170]	Time 0.223 (0.223)	Data 0.428 (0.428)	Loss 0.6590 (0.6590)	Acc@1 89.153 (89.153)	Acc@5 99.322 (99.322)
Epoch: [80][64/170]	Time 0.154 (0.160)	Data 0.000 (0.007)	Loss 0.7352 (0.6964)	Acc@1 84.407 (85.930)	Acc@5 99.322 (99.385)
Epoch: [80][128/170]	Time 0.154 (0.159)	Data 0.000 (0.004)	Loss 0.6970 (0.6983)	Acc@1 86.102 (85.907)	Acc@5 99.322 (99.403)
Max memory in training epoch: 69.0510848
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 373300 ; 373878 ; 0.9984540411578109
[INFO] Storing checkpoint...
  54.4
Max memory: 92.5284352
 27.523s  j: 81 bis 85
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8380
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 81
Max memory: 0.156672
lr: 0.1697472562669044
1
Epoche:81/85; Lr: 0.1697472562669044
batch Size 300
Epoch: [81][0/167]	Time 0.318 (0.318)	Data 0.368 (0.368)	Loss 0.6267 (0.6267)	Acc@1 88.333 (88.333)	Acc@5 99.667 (99.667)
Epoch: [81][64/167]	Time 0.147 (0.160)	Data 0.000 (0.006)	Loss 0.7481 (0.7193)	Acc@1 85.000 (84.985)	Acc@5 100.000 (99.390)
Epoch: [81][128/167]	Time 0.160 (0.158)	Data 0.000 (0.003)	Loss 0.8938 (0.7315)	Acc@1 81.667 (84.744)	Acc@5 99.333 (99.328)
Max memory in training epoch: 69.8788864
lr: 0.1697472562669044
1
Epoche:82/85; Lr: 0.1697472562669044
batch Size 300
Epoch: [82][0/167]	Time 0.176 (0.176)	Data 0.330 (0.330)	Loss 0.7086 (0.7086)	Acc@1 87.000 (87.000)	Acc@5 99.000 (99.000)
Epoch: [82][64/167]	Time 0.171 (0.157)	Data 0.000 (0.005)	Loss 0.7973 (0.7350)	Acc@1 82.000 (84.831)	Acc@5 99.333 (99.308)
Epoch: [82][128/167]	Time 0.161 (0.158)	Data 0.000 (0.003)	Loss 0.6865 (0.7354)	Acc@1 85.667 (84.755)	Acc@5 99.333 (99.341)
Max memory in training epoch: 70.05184
lr: 0.1697472562669044
1
Epoche:83/85; Lr: 0.1697472562669044
batch Size 300
Epoch: [83][0/167]	Time 0.184 (0.184)	Data 0.391 (0.391)	Loss 0.7452 (0.7452)	Acc@1 86.000 (86.000)	Acc@5 99.000 (99.000)
Epoch: [83][64/167]	Time 0.157 (0.160)	Data 0.000 (0.006)	Loss 0.8161 (0.7359)	Acc@1 82.667 (84.908)	Acc@5 99.333 (99.308)
Epoch: [83][128/167]	Time 0.185 (0.160)	Data 0.000 (0.003)	Loss 0.6608 (0.7316)	Acc@1 87.000 (85.101)	Acc@5 99.333 (99.341)
Max memory in training epoch: 70.05184
Drin!!
old memory: 690510848
new memory: 700518400
Faktor: 1.0144929685449344
New batch Size kleiner 304!!
lr: 0.1697472562669044
1
Epoche:84/85; Lr: 0.1697472562669044
batch Size 304
Epoch: [84][0/167]	Time 0.254 (0.254)	Data 0.416 (0.416)	Loss 0.7757 (0.7757)	Acc@1 83.667 (83.667)	Acc@5 99.333 (99.333)
Epoch: [84][64/167]	Time 0.159 (0.160)	Data 0.000 (0.007)	Loss 0.6912 (0.7166)	Acc@1 86.667 (85.636)	Acc@5 99.333 (99.395)
Epoch: [84][128/167]	Time 0.156 (0.159)	Data 0.000 (0.003)	Loss 0.7981 (0.7295)	Acc@1 80.000 (85.039)	Acc@5 99.000 (99.362)
Max memory in training epoch: 70.05184
lr: 0.1697472562669044
1
Epoche:85/85; Lr: 0.1697472562669044
batch Size 304
Epoch: [85][0/167]	Time 0.234 (0.234)	Data 0.464 (0.464)	Loss 0.5874 (0.5874)	Acc@1 91.333 (91.333)	Acc@5 100.000 (100.000)
Epoch: [85][64/167]	Time 0.200 (0.162)	Data 0.000 (0.007)	Loss 0.7781 (0.7297)	Acc@1 84.333 (85.144)	Acc@5 98.000 (99.313)
Epoch: [85][128/167]	Time 0.152 (0.161)	Data 0.000 (0.004)	Loss 0.7922 (0.7256)	Acc@1 82.667 (85.124)	Acc@5 99.333 (99.357)
Max memory in training epoch: 70.05184
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 368962 ; 373300 ; 0.9883793195821056
[INFO] Storing checkpoint...
  68.09
Max memory: 92.5070336
 27.298s  j: 86 bis 90
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6784
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 86
Max memory: 0.1550336
lr: 0.20157486681694897
1
Epoche:86/90; Lr: 0.20157486681694897
batch Size 304
Epoch: [86][0/165]	Time 0.302 (0.302)	Data 0.402 (0.402)	Loss 0.7056 (0.7056)	Acc@1 85.855 (85.855)	Acc@5 100.000 (100.000)
Epoch: [86][64/165]	Time 0.154 (0.155)	Data 0.000 (0.006)	Loss 0.8483 (0.7528)	Acc@1 81.908 (84.180)	Acc@5 98.355 (99.342)
Epoch: [86][128/165]	Time 0.154 (0.157)	Data 0.000 (0.003)	Loss 0.6731 (0.7691)	Acc@1 87.171 (83.815)	Acc@5 100.000 (99.225)
Max memory in training epoch: 72.4779008
lr: 0.20157486681694897
1
Epoche:87/90; Lr: 0.20157486681694897
batch Size 304
Epoch: [87][0/165]	Time 0.212 (0.212)	Data 0.399 (0.399)	Loss 0.7706 (0.7706)	Acc@1 84.539 (84.539)	Acc@5 99.342 (99.342)
Epoch: [87][64/165]	Time 0.136 (0.159)	Data 0.000 (0.006)	Loss 0.8631 (0.7865)	Acc@1 82.237 (83.391)	Acc@5 98.355 (99.251)
Epoch: [87][128/165]	Time 0.158 (0.158)	Data 0.000 (0.003)	Loss 0.7404 (0.7780)	Acc@1 85.855 (83.823)	Acc@5 99.671 (99.245)
Max memory in training epoch: 72.1907712
lr: 0.20157486681694897
1
Epoche:88/90; Lr: 0.20157486681694897
batch Size 304
Epoch: [88][0/165]	Time 0.216 (0.216)	Data 0.396 (0.396)	Loss 0.6457 (0.6457)	Acc@1 88.487 (88.487)	Acc@5 100.000 (100.000)
Epoch: [88][64/165]	Time 0.139 (0.163)	Data 0.000 (0.006)	Loss 0.7836 (0.7607)	Acc@1 84.868 (84.433)	Acc@5 99.342 (99.231)
Epoch: [88][128/165]	Time 0.163 (0.160)	Data 0.000 (0.003)	Loss 0.6804 (0.7629)	Acc@1 86.842 (84.369)	Acc@5 99.342 (99.227)
Max memory in training epoch: 72.0596992
Drin!!
old memory: 700518400
new memory: 720596992
Faktor: 1.028662476246163
New batch Size kleiner 312!!
lr: 0.20157486681694897
1
Epoche:89/90; Lr: 0.20157486681694897
batch Size 312
Epoch: [89][0/165]	Time 0.269 (0.269)	Data 0.420 (0.420)	Loss 0.7216 (0.7216)	Acc@1 86.842 (86.842)	Acc@5 98.684 (98.684)
Epoch: [89][64/165]	Time 0.158 (0.163)	Data 0.000 (0.007)	Loss 0.6761 (0.7574)	Acc@1 87.829 (84.550)	Acc@5 98.684 (99.286)
Epoch: [89][128/165]	Time 0.137 (0.160)	Data 0.000 (0.003)	Loss 0.7638 (0.7644)	Acc@1 84.868 (84.198)	Acc@5 98.355 (99.286)
Max memory in training epoch: 72.0596992
lr: 0.20157486681694897
1
Epoche:90/90; Lr: 0.20157486681694897
batch Size 312
Epoch: [90][0/165]	Time 0.248 (0.248)	Data 0.374 (0.374)	Loss 0.7170 (0.7170)	Acc@1 87.171 (87.171)	Acc@5 99.671 (99.671)
Epoch: [90][64/165]	Time 0.158 (0.161)	Data 0.000 (0.006)	Loss 0.7840 (0.7529)	Acc@1 82.566 (84.327)	Acc@5 99.013 (99.383)
Epoch: [90][128/165]	Time 0.152 (0.160)	Data 0.000 (0.003)	Loss 0.8590 (0.7598)	Acc@1 80.921 (84.231)	Acc@5 98.684 (99.281)
Max memory in training epoch: 72.0596992
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 362900 ; 368962 ; 0.9835701237525816
[INFO] Storing checkpoint...
  72.36
Max memory: 91.2026624
 26.576s  j: 91 bis 95
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7475
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 91
Max memory: 0.1526784
lr: 0.24566936893315655
1
Epoche:91/95; Lr: 0.24566936893315655
batch Size 312
Epoch: [91][0/161]	Time 0.280 (0.280)	Data 0.467 (0.467)	Loss 0.7781 (0.7781)	Acc@1 83.333 (83.333)	Acc@5 99.679 (99.679)
Epoch: [91][64/161]	Time 0.168 (0.162)	Data 0.000 (0.007)	Loss 0.6953 (0.7788)	Acc@1 87.821 (83.851)	Acc@5 99.679 (99.191)
Epoch: [91][128/161]	Time 0.169 (0.159)	Data 0.000 (0.004)	Loss 0.8256 (0.7962)	Acc@1 80.128 (83.261)	Acc@5 99.359 (99.111)
Max memory in training epoch: 72.5361152
lr: 0.24566936893315655
1
Epoche:92/95; Lr: 0.24566936893315655
batch Size 312
Epoch: [92][0/161]	Time 0.211 (0.211)	Data 0.447 (0.447)	Loss 0.8337 (0.8337)	Acc@1 80.449 (80.449)	Acc@5 99.359 (99.359)
Epoch: [92][64/161]	Time 0.162 (0.164)	Data 0.000 (0.007)	Loss 0.7127 (0.8309)	Acc@1 86.538 (81.977)	Acc@5 99.359 (99.068)
Epoch: [92][128/161]	Time 0.179 (0.162)	Data 0.000 (0.004)	Loss 0.7221 (0.8192)	Acc@1 84.615 (82.419)	Acc@5 98.718 (99.108)
Max memory in training epoch: 72.2637312
lr: 0.24566936893315655
1
Epoche:93/95; Lr: 0.024566936893315655
batch Size 312
Epoch: [93][0/161]	Time 0.258 (0.258)	Data 0.438 (0.438)	Loss 0.7993 (0.7993)	Acc@1 83.333 (83.333)	Acc@5 99.679 (99.679)
Epoch: [93][64/161]	Time 0.163 (0.154)	Data 0.000 (0.007)	Loss 0.6565 (0.6922)	Acc@1 88.141 (87.170)	Acc@5 99.679 (99.512)
Epoch: [93][128/161]	Time 0.156 (0.157)	Data 0.000 (0.004)	Loss 0.6145 (0.6482)	Acc@1 88.141 (88.404)	Acc@5 99.359 (99.620)
Max memory in training epoch: 72.2637312
Drin!!
old memory: 720596992
new memory: 722637312
Faktor: 1.0028314300817953
New batch Size kleiner 312!!
lr: 0.024566936893315655
1
Epoche:94/95; Lr: 0.024566936893315655
batch Size 312
Epoch: [94][0/161]	Time 0.252 (0.252)	Data 0.426 (0.426)	Loss 0.5678 (0.5678)	Acc@1 90.385 (90.385)	Acc@5 100.000 (100.000)
Epoch: [94][64/161]	Time 0.170 (0.162)	Data 0.000 (0.007)	Loss 0.5317 (0.5652)	Acc@1 92.628 (90.858)	Acc@5 100.000 (99.739)
Epoch: [94][128/161]	Time 0.153 (0.161)	Data 0.000 (0.004)	Loss 0.5439 (0.5582)	Acc@1 90.385 (91.016)	Acc@5 99.679 (99.752)
Max memory in training epoch: 72.2637312
lr: 0.024566936893315655
1
Epoche:95/95; Lr: 0.024566936893315655
batch Size 312
Epoch: [95][0/161]	Time 0.254 (0.254)	Data 0.461 (0.461)	Loss 0.4640 (0.4640)	Acc@1 94.231 (94.231)	Acc@5 100.000 (100.000)
Epoch: [95][64/161]	Time 0.155 (0.162)	Data 0.000 (0.007)	Loss 0.5232 (0.5234)	Acc@1 91.346 (91.879)	Acc@5 100.000 (99.813)
Epoch: [95][128/161]	Time 0.164 (0.163)	Data 0.000 (0.004)	Loss 0.4776 (0.5223)	Acc@1 93.590 (91.848)	Acc@5 99.359 (99.799)
Max memory in training epoch: 72.2637312
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 359432 ; 362900 ; 0.9904436483879857
[INFO] Storing checkpoint...
  90.08
Max memory: 90.5693184
 26.726s  j: 96 bis 100
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4292
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 96
Max memory: 0.1512448
lr: 0.029940954338728454
1
Epoche:96/100; Lr: 0.029940954338728454
batch Size 312
Epoch: [96][0/161]	Time 0.272 (0.272)	Data 0.441 (0.441)	Loss 0.4677 (0.4677)	Acc@1 93.590 (93.590)	Acc@5 100.000 (100.000)
Epoch: [96][64/161]	Time 0.158 (0.157)	Data 0.000 (0.007)	Loss 0.4629 (0.4970)	Acc@1 93.590 (92.372)	Acc@5 100.000 (99.808)
Epoch: [96][128/161]	Time 0.144 (0.157)	Data 0.000 (0.004)	Loss 0.4738 (0.4979)	Acc@1 92.308 (92.392)	Acc@5 100.000 (99.791)
Max memory in training epoch: 71.8098944
lr: 0.029940954338728454
1
Epoche:97/100; Lr: 0.029940954338728454
batch Size 312
Epoch: [97][0/161]	Time 0.219 (0.219)	Data 0.411 (0.411)	Loss 0.4566 (0.4566)	Acc@1 93.910 (93.910)	Acc@5 100.000 (100.000)
Epoch: [97][64/161]	Time 0.163 (0.158)	Data 0.000 (0.007)	Loss 0.4359 (0.4710)	Acc@1 94.231 (93.097)	Acc@5 99.679 (99.877)
Epoch: [97][128/161]	Time 0.158 (0.157)	Data 0.000 (0.003)	Loss 0.4425 (0.4760)	Acc@1 94.551 (92.911)	Acc@5 99.679 (99.868)
Max memory in training epoch: 71.6554752
lr: 0.029940954338728454
1
Epoche:98/100; Lr: 0.029940954338728454
batch Size 312
Epoch: [98][0/161]	Time 0.195 (0.195)	Data 0.366 (0.366)	Loss 0.5090 (0.5090)	Acc@1 91.667 (91.667)	Acc@5 100.000 (100.000)
Epoch: [98][64/161]	Time 0.163 (0.161)	Data 0.000 (0.006)	Loss 0.4578 (0.4559)	Acc@1 92.308 (93.200)	Acc@5 100.000 (99.847)
Epoch: [98][128/161]	Time 0.144 (0.161)	Data 0.000 (0.003)	Loss 0.4195 (0.4588)	Acc@1 94.231 (93.011)	Acc@5 100.000 (99.868)
Max memory in training epoch: 71.888128
Drin!!
old memory: 722637312
new memory: 718881280
Faktor: 0.9948023276163188
New batch Size größer 290!!
lr: 0.029940954338728454
1
Epoche:99/100; Lr: 0.029940954338728454
batch Size 290
Epoch: [99][0/161]	Time 0.273 (0.273)	Data 0.372 (0.372)	Loss 0.4468 (0.4468)	Acc@1 93.269 (93.269)	Acc@5 100.000 (100.000)
Epoch: [99][64/161]	Time 0.160 (0.165)	Data 0.000 (0.006)	Loss 0.4184 (0.4438)	Acc@1 94.872 (93.245)	Acc@5 100.000 (99.877)
Epoch: [99][128/161]	Time 0.180 (0.163)	Data 0.000 (0.003)	Loss 0.4956 (0.4456)	Acc@1 90.705 (93.157)	Acc@5 99.679 (99.868)
Max memory in training epoch: 71.888128
lr: 0.029940954338728454
1
Epoche:100/100; Lr: 0.029940954338728454
batch Size 290
Epoch: [100][0/161]	Time 0.239 (0.239)	Data 0.430 (0.430)	Loss 0.4163 (0.4163)	Acc@1 94.231 (94.231)	Acc@5 100.000 (100.000)
Epoch: [100][64/161]	Time 0.139 (0.165)	Data 0.000 (0.007)	Loss 0.4197 (0.4313)	Acc@1 94.551 (93.442)	Acc@5 100.000 (99.872)
Epoch: [100][128/161]	Time 0.203 (0.162)	Data 0.000 (0.004)	Loss 0.4042 (0.4328)	Acc@1 94.231 (93.403)	Acc@5 100.000 (99.863)
Max memory in training epoch: 71.888128
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 358278 ; 359432 ; 0.9967893787976585
[INFO] Storing checkpoint...
  89.33
Max memory: 90.0562944
 26.696s  j: 101 bis 105
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1429
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 101
Max memory: 0.1508352
lr: 0.03391748733684083
1
Epoche:101/105; Lr: 0.03391748733684083
batch Size 290
Epoch: [101][0/173]	Time 0.253 (0.253)	Data 0.433 (0.433)	Loss 0.4303 (0.4303)	Acc@1 93.793 (93.793)	Acc@5 100.000 (100.000)
Epoch: [101][64/173]	Time 0.159 (0.158)	Data 0.000 (0.007)	Loss 0.4819 (0.4255)	Acc@1 91.379 (93.284)	Acc@5 99.655 (99.905)
Epoch: [101][128/173]	Time 0.151 (0.157)	Data 0.000 (0.004)	Loss 0.4046 (0.4311)	Acc@1 93.103 (93.077)	Acc@5 100.000 (99.904)
Max memory in training epoch: 65.553664
lr: 0.03391748733684083
1
Epoche:102/105; Lr: 0.03391748733684083
batch Size 290
Epoch: [102][0/173]	Time 0.235 (0.235)	Data 0.384 (0.384)	Loss 0.4677 (0.4677)	Acc@1 91.379 (91.379)	Acc@5 99.310 (99.310)
Epoch: [102][64/173]	Time 0.140 (0.154)	Data 0.000 (0.006)	Loss 0.4666 (0.4322)	Acc@1 92.069 (92.902)	Acc@5 100.000 (99.889)
Epoch: [102][128/173]	Time 0.157 (0.155)	Data 0.000 (0.003)	Loss 0.4523 (0.4376)	Acc@1 93.103 (92.713)	Acc@5 99.310 (99.856)
Max memory in training epoch: 65.7908224
lr: 0.03391748733684083
1
Epoche:103/105; Lr: 0.03391748733684083
batch Size 290
Epoch: [103][0/173]	Time 0.257 (0.257)	Data 0.451 (0.451)	Loss 0.4467 (0.4467)	Acc@1 93.448 (93.448)	Acc@5 100.000 (100.000)
Epoch: [103][64/173]	Time 0.166 (0.158)	Data 0.000 (0.007)	Loss 0.4261 (0.4290)	Acc@1 93.448 (93.114)	Acc@5 100.000 (99.825)
Epoch: [103][128/173]	Time 0.143 (0.159)	Data 0.000 (0.004)	Loss 0.4383 (0.4290)	Acc@1 92.414 (92.970)	Acc@5 100.000 (99.837)
Max memory in training epoch: 65.7908224
Drin!!
old memory: 718881280
new memory: 657908224
Faktor: 0.9151834138732894
New batch Size größer 295!!
lr: 0.03391748733684083
1
Epoche:104/105; Lr: 0.03391748733684083
batch Size 295
Epoch: [104][0/173]	Time 0.199 (0.199)	Data 0.420 (0.420)	Loss 0.4392 (0.4392)	Acc@1 92.759 (92.759)	Acc@5 100.000 (100.000)
Epoch: [104][64/173]	Time 0.145 (0.158)	Data 0.000 (0.007)	Loss 0.4186 (0.4247)	Acc@1 92.069 (92.891)	Acc@5 100.000 (99.894)
Epoch: [104][128/173]	Time 0.148 (0.156)	Data 0.000 (0.003)	Loss 0.4018 (0.4255)	Acc@1 93.448 (92.735)	Acc@5 100.000 (99.877)
Max memory in training epoch: 65.7908224
lr: 0.03391748733684083
1
Epoche:105/105; Lr: 0.03391748733684083
batch Size 295
Epoch: [105][0/173]	Time 0.231 (0.231)	Data 0.504 (0.504)	Loss 0.3993 (0.3993)	Acc@1 93.793 (93.793)	Acc@5 100.000 (100.000)
Epoch: [105][64/173]	Time 0.158 (0.155)	Data 0.000 (0.008)	Loss 0.5090 (0.4196)	Acc@1 90.000 (92.674)	Acc@5 100.000 (99.867)
Epoch: [105][128/173]	Time 0.175 (0.156)	Data 0.000 (0.004)	Loss 0.4624 (0.4259)	Acc@1 91.724 (92.555)	Acc@5 100.000 (99.856)
Max memory in training epoch: 65.7908224
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 356832 ; 358278 ; 0.9959640279336158
[INFO] Storing checkpoint...
  87.95
Max memory: 89.9057664
 27.645s  j: 106 bis 110
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4826
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 106
Max memory: 0.1503232
lr: 0.039084604548312675
1
Epoche:106/110; Lr: 0.039084604548312675
batch Size 295
Epoch: [106][0/170]	Time 0.277 (0.277)	Data 0.385 (0.385)	Loss 0.4141 (0.4141)	Acc@1 93.220 (93.220)	Acc@5 100.000 (100.000)
Epoch: [106][64/170]	Time 0.166 (0.158)	Data 0.000 (0.006)	Loss 0.4644 (0.4272)	Acc@1 90.847 (92.381)	Acc@5 100.000 (99.870)
Epoch: [106][128/170]	Time 0.161 (0.158)	Data 0.000 (0.003)	Loss 0.4402 (0.4384)	Acc@1 92.881 (91.920)	Acc@5 99.661 (99.863)
Max memory in training epoch: 66.5298944
lr: 0.039084604548312675
1
Epoche:107/110; Lr: 0.039084604548312675
batch Size 295
Epoch: [107][0/170]	Time 0.243 (0.243)	Data 0.403 (0.403)	Loss 0.3711 (0.3711)	Acc@1 94.576 (94.576)	Acc@5 100.000 (100.000)
Epoch: [107][64/170]	Time 0.248 (0.158)	Data 0.000 (0.006)	Loss 0.4969 (0.4422)	Acc@1 88.814 (91.943)	Acc@5 100.000 (99.833)
Epoch: [107][128/170]	Time 0.154 (0.158)	Data 0.000 (0.003)	Loss 0.4243 (0.4456)	Acc@1 91.864 (91.817)	Acc@5 100.000 (99.832)
Max memory in training epoch: 66.6200064
lr: 0.039084604548312675
1
Epoche:108/110; Lr: 0.039084604548312675
batch Size 295
Epoch: [108][0/170]	Time 0.173 (0.173)	Data 0.431 (0.431)	Loss 0.4280 (0.4280)	Acc@1 91.864 (91.864)	Acc@5 100.000 (100.000)
Epoch: [108][64/170]	Time 0.145 (0.155)	Data 0.000 (0.007)	Loss 0.4297 (0.4344)	Acc@1 91.864 (92.151)	Acc@5 99.661 (99.817)
Epoch: [108][128/170]	Time 0.156 (0.157)	Data 0.000 (0.004)	Loss 0.4088 (0.4417)	Acc@1 93.559 (91.917)	Acc@5 100.000 (99.821)
Max memory in training epoch: 66.6200064
Drin!!
old memory: 657908224
new memory: 666200064
Faktor: 1.0126033384255126
New batch Size kleiner 298!!
lr: 0.039084604548312675
1
Epoche:109/110; Lr: 0.039084604548312675
batch Size 298
Epoch: [109][0/170]	Time 0.248 (0.248)	Data 0.396 (0.396)	Loss 0.4608 (0.4608)	Acc@1 90.508 (90.508)	Acc@5 100.000 (100.000)
Epoch: [109][64/170]	Time 0.143 (0.161)	Data 0.000 (0.006)	Loss 0.4835 (0.4447)	Acc@1 90.169 (91.880)	Acc@5 100.000 (99.807)
Epoch: [109][128/170]	Time 0.178 (0.161)	Data 0.000 (0.003)	Loss 0.3977 (0.4452)	Acc@1 92.542 (91.746)	Acc@5 100.000 (99.827)
Max memory in training epoch: 66.6200064
lr: 0.039084604548312675
1
Epoche:110/110; Lr: 0.039084604548312675
batch Size 298
Epoch: [110][0/170]	Time 0.243 (0.243)	Data 0.404 (0.404)	Loss 0.5072 (0.5072)	Acc@1 89.492 (89.492)	Acc@5 100.000 (100.000)
Epoch: [110][64/170]	Time 0.171 (0.157)	Data 0.000 (0.006)	Loss 0.3928 (0.4380)	Acc@1 93.898 (92.089)	Acc@5 100.000 (99.854)
Epoch: [110][128/170]	Time 0.192 (0.159)	Data 0.016 (0.003)	Loss 0.4617 (0.4478)	Acc@1 91.864 (91.830)	Acc@5 99.661 (99.858)
Max memory in training epoch: 66.6200064
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 355820 ; 356832 ; 0.9971639314859654
[INFO] Storing checkpoint...
  82.99
Max memory: 89.2025856
 27.348s  j: 111 bis 115
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7972
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 111
Max memory: 0.1498624
lr: 0.045496922482020224
1
Epoche:111/115; Lr: 0.045496922482020224
batch Size 298
Epoch: [111][0/168]	Time 0.238 (0.238)	Data 0.388 (0.388)	Loss 0.4278 (0.4278)	Acc@1 93.624 (93.624)	Acc@5 100.000 (100.000)
Epoch: [111][64/168]	Time 0.141 (0.153)	Data 0.000 (0.006)	Loss 0.4264 (0.4320)	Acc@1 92.617 (92.070)	Acc@5 100.000 (99.861)
Epoch: [111][128/168]	Time 0.163 (0.151)	Data 0.000 (0.003)	Loss 0.4792 (0.4529)	Acc@1 90.604 (91.426)	Acc@5 99.664 (99.797)
Max memory in training epoch: 66.9950464
lr: 0.045496922482020224
1
Epoche:112/115; Lr: 0.045496922482020224
batch Size 298
Epoch: [112][0/168]	Time 0.211 (0.211)	Data 0.418 (0.418)	Loss 0.4674 (0.4674)	Acc@1 92.282 (92.282)	Acc@5 99.329 (99.329)
Epoch: [112][64/168]	Time 0.179 (0.157)	Data 0.000 (0.007)	Loss 0.5056 (0.4661)	Acc@1 88.255 (91.131)	Acc@5 99.664 (99.757)
Epoch: [112][128/168]	Time 0.148 (0.154)	Data 0.000 (0.003)	Loss 0.5125 (0.4704)	Acc@1 89.262 (90.966)	Acc@5 99.664 (99.784)
Max memory in training epoch: 66.9348352
lr: 0.045496922482020224
1
Epoche:113/115; Lr: 0.045496922482020224
batch Size 298
Epoch: [113][0/168]	Time 0.225 (0.225)	Data 0.349 (0.349)	Loss 0.4082 (0.4082)	Acc@1 93.289 (93.289)	Acc@5 100.000 (100.000)
Epoch: [113][64/168]	Time 0.157 (0.159)	Data 0.000 (0.006)	Loss 0.4612 (0.4647)	Acc@1 89.933 (91.193)	Acc@5 99.664 (99.814)
Epoch: [113][128/168]	Time 0.166 (0.158)	Data 0.000 (0.003)	Loss 0.4540 (0.4709)	Acc@1 90.940 (90.960)	Acc@5 99.664 (99.795)
Max memory in training epoch: 66.9348352
Drin!!
old memory: 666200064
new memory: 669348352
Faktor: 1.004725739564024
New batch Size kleiner 299!!
lr: 0.045496922482020224
1
Epoche:114/115; Lr: 0.045496922482020224
batch Size 299
Epoch: [114][0/168]	Time 0.236 (0.236)	Data 0.434 (0.434)	Loss 0.5014 (0.5014)	Acc@1 91.275 (91.275)	Acc@5 100.000 (100.000)
Epoch: [114][64/168]	Time 0.130 (0.161)	Data 0.000 (0.007)	Loss 0.4335 (0.4617)	Acc@1 93.624 (91.693)	Acc@5 100.000 (99.799)
Epoch: [114][128/168]	Time 0.168 (0.159)	Data 0.000 (0.004)	Loss 0.4699 (0.4671)	Acc@1 91.611 (91.208)	Acc@5 100.000 (99.797)
Max memory in training epoch: 66.9348352
lr: 0.045496922482020224
1
Epoche:115/115; Lr: 0.045496922482020224
batch Size 299
Epoch: [115][0/168]	Time 0.239 (0.239)	Data 0.420 (0.420)	Loss 0.3928 (0.3928)	Acc@1 93.289 (93.289)	Acc@5 100.000 (100.000)
Epoch: [115][64/168]	Time 0.150 (0.158)	Data 0.000 (0.007)	Loss 0.5058 (0.4592)	Acc@1 89.933 (91.384)	Acc@5 100.000 (99.850)
Epoch: [115][128/168]	Time 0.138 (0.158)	Data 0.000 (0.003)	Loss 0.4728 (0.4704)	Acc@1 90.268 (90.976)	Acc@5 99.664 (99.813)
Max memory in training epoch: 66.9348352
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 353218 ; 355820 ; 0.9926873138103536
[INFO] Storing checkpoint...
  84.94
Max memory: 89.1602432
 26.822s  j: 116 bis 120
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 3377
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 116
Max memory: 0.1489408
lr: 0.05313898368017206
1
Epoche:116/120; Lr: 0.05313898368017206
batch Size 299
Epoch: [116][0/168]	Time 0.283 (0.283)	Data 0.388 (0.388)	Loss 0.4556 (0.4556)	Acc@1 91.639 (91.639)	Acc@5 99.331 (99.331)
Epoch: [116][64/168]	Time 0.186 (0.157)	Data 0.000 (0.006)	Loss 0.4389 (0.4646)	Acc@1 91.639 (91.407)	Acc@5 99.666 (99.810)
Epoch: [116][128/168]	Time 0.168 (0.156)	Data 0.000 (0.003)	Loss 0.4895 (0.4884)	Acc@1 92.308 (90.576)	Acc@5 99.331 (99.741)
Max memory in training epoch: 66.4547328
lr: 0.05313898368017206
1
Epoche:117/120; Lr: 0.05313898368017206
batch Size 299
Epoch: [117][0/168]	Time 0.288 (0.288)	Data 0.429 (0.429)	Loss 0.5015 (0.5015)	Acc@1 90.970 (90.970)	Acc@5 99.666 (99.666)
Epoch: [117][64/168]	Time 0.155 (0.160)	Data 0.000 (0.007)	Loss 0.5033 (0.4981)	Acc@1 88.629 (90.409)	Acc@5 100.000 (99.743)
Epoch: [117][128/168]	Time 0.149 (0.159)	Data 0.000 (0.004)	Loss 0.5309 (0.4998)	Acc@1 89.967 (90.197)	Acc@5 99.666 (99.754)
Max memory in training epoch: 66.5767936
lr: 0.05313898368017206
1
Epoche:118/120; Lr: 0.05313898368017206
batch Size 299
Epoch: [118][0/168]	Time 0.226 (0.226)	Data 0.457 (0.457)	Loss 0.4773 (0.4773)	Acc@1 89.967 (89.967)	Acc@5 99.666 (99.666)
Epoch: [118][64/168]	Time 0.152 (0.159)	Data 0.000 (0.007)	Loss 0.4424 (0.4864)	Acc@1 90.970 (90.759)	Acc@5 100.000 (99.768)
Epoch: [118][128/168]	Time 0.149 (0.159)	Data 0.000 (0.004)	Loss 0.5076 (0.4852)	Acc@1 88.629 (90.718)	Acc@5 99.666 (99.790)
Max memory in training epoch: 66.5767936
Drin!!
old memory: 669348352
new memory: 665767936
Faktor: 0.9946508929329522
New batch Size größer 300!!
lr: 0.05313898368017206
1
Epoche:119/120; Lr: 0.05313898368017206
batch Size 300
Epoch: [119][0/168]	Time 0.237 (0.237)	Data 0.379 (0.379)	Loss 0.4231 (0.4231)	Acc@1 93.645 (93.645)	Acc@5 100.000 (100.000)
Epoch: [119][64/168]	Time 0.138 (0.159)	Data 0.000 (0.006)	Loss 0.4476 (0.5042)	Acc@1 92.977 (90.178)	Acc@5 99.331 (99.717)
Epoch: [119][128/168]	Time 0.156 (0.157)	Data 0.000 (0.003)	Loss 0.4494 (0.4992)	Acc@1 92.977 (90.431)	Acc@5 100.000 (99.738)
Max memory in training epoch: 66.5767936
lr: 0.05313898368017206
1
Epoche:120/120; Lr: 0.05313898368017206
batch Size 300
Epoch: [120][0/168]	Time 0.239 (0.239)	Data 0.326 (0.326)	Loss 0.5184 (0.5184)	Acc@1 88.629 (88.629)	Acc@5 100.000 (100.000)
Epoch: [120][64/168]	Time 0.157 (0.161)	Data 0.000 (0.005)	Loss 0.4769 (0.4942)	Acc@1 90.301 (90.687)	Acc@5 99.666 (99.763)
Epoch: [120][128/168]	Time 0.161 (0.159)	Data 0.000 (0.003)	Loss 0.4877 (0.4978)	Acc@1 90.635 (90.514)	Acc@5 98.997 (99.749)
Max memory in training epoch: 66.5767936
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 352064 ; 353218 ; 0.9967328958320357
[INFO] Storing checkpoint...
  85.26
Max memory: 88.124672
 26.938s  j: 121 bis 125
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1788
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 121
Max memory: 0.1484288
lr: 0.06227224650020163
1
Epoche:121/125; Lr: 0.06227224650020163
batch Size 300
Epoch: [121][0/167]	Time 0.286 (0.286)	Data 0.401 (0.401)	Loss 0.4704 (0.4704)	Acc@1 92.000 (92.000)	Acc@5 100.000 (100.000)
Epoch: [121][64/167]	Time 0.167 (0.158)	Data 0.000 (0.006)	Loss 0.6439 (0.4905)	Acc@1 83.000 (90.728)	Acc@5 99.667 (99.728)
Epoch: [121][128/167]	Time 0.156 (0.157)	Data 0.000 (0.003)	Loss 0.5132 (0.5174)	Acc@1 88.333 (89.770)	Acc@5 99.667 (99.711)
Max memory in training epoch: 66.6451968
lr: 0.06227224650020163
1
Epoche:122/125; Lr: 0.06227224650020163
batch Size 300
Epoch: [122][0/167]	Time 0.239 (0.239)	Data 0.417 (0.417)	Loss 0.5167 (0.5167)	Acc@1 91.667 (91.667)	Acc@5 99.667 (99.667)
Epoch: [122][64/167]	Time 0.135 (0.154)	Data 0.000 (0.007)	Loss 0.5561 (0.5148)	Acc@1 88.333 (90.103)	Acc@5 99.667 (99.795)
Epoch: [122][128/167]	Time 0.142 (0.155)	Data 0.000 (0.003)	Loss 0.5335 (0.5236)	Acc@1 90.333 (89.860)	Acc@5 100.000 (99.726)
Max memory in training epoch: 66.6261504
lr: 0.06227224650020163
1
Epoche:123/125; Lr: 0.06227224650020163
batch Size 300
Epoch: [123][0/167]	Time 0.223 (0.223)	Data 0.437 (0.437)	Loss 0.5269 (0.5269)	Acc@1 87.667 (87.667)	Acc@5 100.000 (100.000)
Epoch: [123][64/167]	Time 0.162 (0.160)	Data 0.000 (0.007)	Loss 0.5952 (0.5154)	Acc@1 88.667 (90.185)	Acc@5 99.000 (99.769)
Epoch: [123][128/167]	Time 0.169 (0.160)	Data 0.000 (0.004)	Loss 0.5304 (0.5246)	Acc@1 89.667 (89.819)	Acc@5 99.667 (99.752)
Max memory in training epoch: 66.6261504
Drin!!
old memory: 665767936
new memory: 666261504
Faktor: 1.0007413514128742
New batch Size kleiner 300!!
lr: 0.06227224650020163
1
Epoche:124/125; Lr: 0.06227224650020163
batch Size 300
Epoch: [124][0/167]	Time 0.205 (0.205)	Data 0.453 (0.453)	Loss 0.5197 (0.5197)	Acc@1 91.000 (91.000)	Acc@5 100.000 (100.000)
Epoch: [124][64/167]	Time 0.165 (0.159)	Data 0.000 (0.007)	Loss 0.6154 (0.5187)	Acc@1 88.333 (90.159)	Acc@5 100.000 (99.738)
Epoch: [124][128/167]	Time 0.147 (0.157)	Data 0.000 (0.004)	Loss 0.4886 (0.5181)	Acc@1 92.667 (90.165)	Acc@5 100.000 (99.749)
Max memory in training epoch: 66.6261504
lr: 0.06227224650020163
1
Epoche:125/125; Lr: 0.06227224650020163
batch Size 300
Epoch: [125][0/167]	Time 0.231 (0.231)	Data 0.514 (0.514)	Loss 0.4691 (0.4691)	Acc@1 91.667 (91.667)	Acc@5 100.000 (100.000)
Epoch: [125][64/167]	Time 0.156 (0.160)	Data 0.000 (0.008)	Loss 0.5569 (0.5003)	Acc@1 87.000 (90.826)	Acc@5 99.667 (99.749)
Epoch: [125][128/167]	Time 0.150 (0.160)	Data 0.000 (0.004)	Loss 0.5272 (0.5103)	Acc@1 91.000 (90.481)	Acc@5 100.000 (99.767)
Max memory in training epoch: 66.6261504
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  81.77
Max memory: 87.8503424
 27.254s  j: 126 bis 130
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6364
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 126
Max memory: 0.1484288
lr: 0.07297528886742378
1
Epoche:126/130; Lr: 0.07297528886742378
batch Size 300
Epoch: [126][0/167]	Time 0.314 (0.314)	Data 0.403 (0.403)	Loss 0.4748 (0.4748)	Acc@1 90.000 (90.000)	Acc@5 100.000 (100.000)
Epoch: [126][64/167]	Time 0.157 (0.161)	Data 0.000 (0.006)	Loss 0.5622 (0.5311)	Acc@1 88.667 (89.908)	Acc@5 99.333 (99.697)
Epoch: [126][128/167]	Time 0.168 (0.155)	Data 0.000 (0.003)	Loss 0.5637 (0.5490)	Acc@1 86.667 (89.310)	Acc@5 100.000 (99.690)
Max memory in training epoch: 66.6451968
lr: 0.07297528886742378
1
Epoche:127/130; Lr: 0.07297528886742378
batch Size 300
Epoch: [127][0/167]	Time 0.268 (0.268)	Data 0.481 (0.481)	Loss 0.5153 (0.5153)	Acc@1 90.000 (90.000)	Acc@5 99.667 (99.667)
Epoch: [127][64/167]	Time 0.143 (0.157)	Data 0.000 (0.008)	Loss 0.5582 (0.5376)	Acc@1 89.000 (89.605)	Acc@5 99.000 (99.713)
Epoch: [127][128/167]	Time 0.167 (0.155)	Data 0.000 (0.004)	Loss 0.5750 (0.5475)	Acc@1 89.000 (89.292)	Acc@5 99.333 (99.667)
Max memory in training epoch: 66.6261504
lr: 0.07297528886742378
1
Epoche:128/130; Lr: 0.07297528886742378
batch Size 300
Epoch: [128][0/167]	Time 0.214 (0.214)	Data 0.361 (0.361)	Loss 0.5464 (0.5464)	Acc@1 89.667 (89.667)	Acc@5 100.000 (100.000)
Epoch: [128][64/167]	Time 0.166 (0.160)	Data 0.000 (0.006)	Loss 0.6101 (0.5522)	Acc@1 86.333 (89.128)	Acc@5 99.333 (99.687)
Epoch: [128][128/167]	Time 0.155 (0.158)	Data 0.000 (0.003)	Loss 0.5817 (0.5513)	Acc@1 88.333 (89.251)	Acc@5 99.667 (99.693)
Max memory in training epoch: 66.6261504
Drin!!
old memory: 666261504
new memory: 666261504
Faktor: 1.0
lr: 0.07297528886742378
1
Epoche:129/130; Lr: 0.07297528886742378
batch Size 300
Epoch: [129][0/167]	Time 0.233 (0.233)	Data 0.455 (0.455)	Loss 0.5012 (0.5012)	Acc@1 90.000 (90.000)	Acc@5 100.000 (100.000)
Epoch: [129][64/167]	Time 0.169 (0.157)	Data 0.000 (0.007)	Loss 0.6267 (0.5423)	Acc@1 87.333 (89.492)	Acc@5 99.333 (99.703)
Epoch: [129][128/167]	Time 0.158 (0.157)	Data 0.000 (0.004)	Loss 0.5178 (0.5495)	Acc@1 90.667 (89.279)	Acc@5 99.333 (99.693)
Max memory in training epoch: 66.6261504
lr: 0.07297528886742378
1
Epoche:130/130; Lr: 0.07297528886742378
batch Size 300
Epoch: [130][0/167]	Time 0.233 (0.233)	Data 0.452 (0.452)	Loss 0.5879 (0.5879)	Acc@1 87.667 (87.667)	Acc@5 99.667 (99.667)
Epoch: [130][64/167]	Time 0.172 (0.162)	Data 0.000 (0.007)	Loss 0.4169 (0.5283)	Acc@1 94.000 (90.195)	Acc@5 100.000 (99.677)
Epoch: [130][128/167]	Time 0.165 (0.160)	Data 0.000 (0.004)	Loss 0.6115 (0.5313)	Acc@1 87.667 (90.116)	Acc@5 100.000 (99.724)
Max memory in training epoch: 66.6261504
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  78.21
Max memory: 87.8503424
 27.103s  j: 131 bis 135
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1490
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.1484288
lr: 0.08551791664151225
1
Epoche:131/135; Lr: 0.08551791664151225
batch Size 300
Epoch: [131][0/167]	Time 0.238 (0.238)	Data 0.480 (0.480)	Loss 0.4840 (0.4840)	Acc@1 92.667 (92.667)	Acc@5 100.000 (100.000)
Epoch: [131][64/167]	Time 0.128 (0.155)	Data 0.000 (0.008)	Loss 0.5599 (0.5520)	Acc@1 90.333 (89.267)	Acc@5 99.667 (99.800)
Epoch: [131][128/167]	Time 0.164 (0.158)	Data 0.000 (0.004)	Loss 0.5662 (0.5701)	Acc@1 89.667 (88.744)	Acc@5 99.333 (99.677)
Max memory in training epoch: 66.6451968
lr: 0.08551791664151225
1
Epoche:132/135; Lr: 0.08551791664151225
batch Size 300
Epoch: [132][0/167]	Time 0.226 (0.226)	Data 0.419 (0.419)	Loss 0.6450 (0.6450)	Acc@1 86.000 (86.000)	Acc@5 100.000 (100.000)
Epoch: [132][64/167]	Time 0.144 (0.159)	Data 0.000 (0.007)	Loss 0.5359 (0.5688)	Acc@1 90.000 (88.979)	Acc@5 99.667 (99.626)
Epoch: [132][128/167]	Time 0.172 (0.156)	Data 0.000 (0.003)	Loss 0.6093 (0.5764)	Acc@1 87.000 (88.705)	Acc@5 99.667 (99.628)
Max memory in training epoch: 66.6261504
lr: 0.08551791664151225
1
Epoche:133/135; Lr: 0.08551791664151225
batch Size 300
Epoch: [133][0/167]	Time 0.216 (0.216)	Data 0.423 (0.423)	Loss 0.5812 (0.5812)	Acc@1 88.333 (88.333)	Acc@5 99.667 (99.667)
Epoch: [133][64/167]	Time 0.140 (0.159)	Data 0.000 (0.007)	Loss 0.5948 (0.5650)	Acc@1 87.667 (89.421)	Acc@5 99.333 (99.672)
Epoch: [133][128/167]	Time 0.152 (0.158)	Data 0.000 (0.003)	Loss 0.5179 (0.5685)	Acc@1 91.333 (89.150)	Acc@5 100.000 (99.659)
Max memory in training epoch: 66.6261504
Drin!!
old memory: 666261504
new memory: 666261504
Faktor: 1.0
lr: 0.08551791664151225
1
Epoche:134/135; Lr: 0.08551791664151225
batch Size 300
Epoch: [134][0/167]	Time 0.239 (0.239)	Data 0.490 (0.490)	Loss 0.5944 (0.5944)	Acc@1 88.000 (88.000)	Acc@5 99.667 (99.667)
Epoch: [134][64/167]	Time 0.129 (0.150)	Data 0.000 (0.008)	Loss 0.5678 (0.5686)	Acc@1 90.333 (89.082)	Acc@5 99.667 (99.718)
Epoch: [134][128/167]	Time 0.161 (0.154)	Data 0.000 (0.004)	Loss 0.5803 (0.5726)	Acc@1 89.000 (88.897)	Acc@5 99.667 (99.682)
Max memory in training epoch: 66.6261504
lr: 0.08551791664151225
1
Epoche:135/135; Lr: 0.08551791664151225
batch Size 300
Epoch: [135][0/167]	Time 0.248 (0.248)	Data 0.440 (0.440)	Loss 0.5740 (0.5740)	Acc@1 89.333 (89.333)	Acc@5 99.333 (99.333)
Epoch: [135][64/167]	Time 0.142 (0.160)	Data 0.000 (0.007)	Loss 0.5288 (0.5571)	Acc@1 89.000 (89.385)	Acc@5 100.000 (99.651)
Epoch: [135][128/167]	Time 0.167 (0.159)	Data 0.000 (0.004)	Loss 0.5764 (0.5706)	Acc@1 88.000 (88.842)	Acc@5 100.000 (99.646)
Max memory in training epoch: 66.6261504
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  85.01
Max memory: 87.8503424
 26.814s  j: 136 bis 140
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5058
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.1484288
lr: 0.10021630856427216
1
Epoche:136/140; Lr: 0.10021630856427216
batch Size 300
Epoch: [136][0/167]	Time 0.229 (0.229)	Data 0.442 (0.442)	Loss 0.5095 (0.5095)	Acc@1 91.333 (91.333)	Acc@5 99.667 (99.667)
Epoch: [136][64/167]	Time 0.154 (0.155)	Data 0.000 (0.007)	Loss 0.5878 (0.5729)	Acc@1 89.667 (88.749)	Acc@5 99.667 (99.728)
Epoch: [136][128/167]	Time 0.147 (0.156)	Data 0.000 (0.004)	Loss 0.6541 (0.5976)	Acc@1 86.667 (88.171)	Acc@5 99.000 (99.649)
Max memory in training epoch: 66.6451968
lr: 0.10021630856427216
1
Epoche:137/140; Lr: 0.10021630856427216
batch Size 300
Epoch: [137][0/167]	Time 0.229 (0.229)	Data 0.544 (0.544)	Loss 0.5734 (0.5734)	Acc@1 88.000 (88.000)	Acc@5 100.000 (100.000)
Epoch: [137][64/167]	Time 0.157 (0.158)	Data 0.000 (0.009)	Loss 0.5844 (0.5921)	Acc@1 86.667 (88.262)	Acc@5 100.000 (99.585)
Epoch: [137][128/167]	Time 0.150 (0.157)	Data 0.000 (0.004)	Loss 0.6666 (0.6054)	Acc@1 84.000 (87.860)	Acc@5 100.000 (99.597)
Max memory in training epoch: 66.6261504
lr: 0.10021630856427216
1
Epoche:138/140; Lr: 0.10021630856427216
batch Size 300
Epoch: [138][0/167]	Time 0.224 (0.224)	Data 0.550 (0.550)	Loss 0.5197 (0.5197)	Acc@1 91.000 (91.000)	Acc@5 100.000 (100.000)
Epoch: [138][64/167]	Time 0.163 (0.158)	Data 0.000 (0.009)	Loss 0.5806 (0.5997)	Acc@1 89.333 (88.277)	Acc@5 100.000 (99.600)
Epoch: [138][128/167]	Time 0.212 (0.158)	Data 0.000 (0.004)	Loss 0.6284 (0.6082)	Acc@1 83.000 (87.941)	Acc@5 99.000 (99.615)
Max memory in training epoch: 66.6261504
Drin!!
old memory: 666261504
new memory: 666261504
Faktor: 1.0
lr: 0.10021630856427216
1
Epoche:139/140; Lr: 0.10021630856427216
batch Size 300
Epoch: [139][0/167]	Time 0.208 (0.208)	Data 0.487 (0.487)	Loss 0.5186 (0.5186)	Acc@1 90.667 (90.667)	Acc@5 100.000 (100.000)
Epoch: [139][64/167]	Time 0.157 (0.158)	Data 0.000 (0.008)	Loss 0.5417 (0.5892)	Acc@1 90.000 (88.692)	Acc@5 100.000 (99.600)
Epoch: [139][128/167]	Time 0.153 (0.159)	Data 0.000 (0.004)	Loss 0.6163 (0.5973)	Acc@1 87.333 (88.284)	Acc@5 99.000 (99.587)
Max memory in training epoch: 66.6261504
lr: 0.10021630856427216
1
Epoche:140/140; Lr: 0.10021630856427216
batch Size 300
Epoch: [140][0/167]	Time 0.222 (0.222)	Data 0.453 (0.453)	Loss 0.5819 (0.5819)	Acc@1 89.667 (89.667)	Acc@5 99.667 (99.667)
Epoch: [140][64/167]	Time 0.140 (0.159)	Data 0.000 (0.007)	Loss 0.4966 (0.5915)	Acc@1 93.000 (88.692)	Acc@5 100.000 (99.621)
Epoch: [140][128/167]	Time 0.154 (0.159)	Data 0.000 (0.004)	Loss 0.6478 (0.5993)	Acc@1 87.000 (88.385)	Acc@5 99.333 (99.636)
Max memory in training epoch: 66.6261504
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 351486 ; 352064 ; 0.9983582530449009
[INFO] Storing checkpoint...
  79.01
Max memory: 87.8503424
 27.152s  j: 141 bis 145
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1628
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 141
Max memory: 0.148224
lr: 0.11744098659875644
1
Epoche:141/145; Lr: 0.11744098659875644
batch Size 300
Epoch: [141][0/167]	Time 0.318 (0.318)	Data 0.405 (0.405)	Loss 0.6501 (0.6501)	Acc@1 87.000 (87.000)	Acc@5 99.333 (99.333)
Epoch: [141][64/167]	Time 0.149 (0.153)	Data 0.000 (0.006)	Loss 0.6509 (0.5979)	Acc@1 84.333 (88.528)	Acc@5 99.667 (99.605)
Epoch: [141][128/167]	Time 0.147 (0.153)	Data 0.000 (0.003)	Loss 0.5930 (0.6266)	Acc@1 90.333 (87.579)	Acc@5 99.667 (99.514)
Max memory in training epoch: 66.6136576
lr: 0.11744098659875644
1
Epoche:142/145; Lr: 0.11744098659875644
batch Size 300
Epoch: [142][0/167]	Time 0.247 (0.247)	Data 0.491 (0.491)	Loss 0.6662 (0.6662)	Acc@1 85.667 (85.667)	Acc@5 99.667 (99.667)
Epoch: [142][64/167]	Time 0.179 (0.159)	Data 0.000 (0.008)	Loss 0.6971 (0.6407)	Acc@1 86.333 (87.113)	Acc@5 100.000 (99.528)
Epoch: [142][128/167]	Time 0.134 (0.158)	Data 0.000 (0.004)	Loss 0.6288 (0.6441)	Acc@1 87.000 (87.010)	Acc@5 100.000 (99.522)
Max memory in training epoch: 66.5638912
lr: 0.11744098659875644
1
Epoche:143/145; Lr: 0.11744098659875644
batch Size 300
Epoch: [143][0/167]	Time 0.210 (0.210)	Data 0.512 (0.512)	Loss 0.6244 (0.6244)	Acc@1 87.333 (87.333)	Acc@5 100.000 (100.000)
Epoch: [143][64/167]	Time 0.152 (0.156)	Data 0.000 (0.008)	Loss 0.6891 (0.6444)	Acc@1 84.000 (86.892)	Acc@5 99.333 (99.451)
Epoch: [143][128/167]	Time 0.155 (0.157)	Data 0.000 (0.004)	Loss 0.6527 (0.6431)	Acc@1 86.667 (87.132)	Acc@5 99.667 (99.509)
Max memory in training epoch: 66.7262976
Drin!!
old memory: 666261504
new memory: 667262976
Faktor: 1.0015031215130807
New batch Size kleiner 300!!
lr: 0.11744098659875644
1
Epoche:144/145; Lr: 0.11744098659875644
batch Size 300
Epoch: [144][0/167]	Time 0.211 (0.211)	Data 0.331 (0.331)	Loss 0.6408 (0.6408)	Acc@1 88.667 (88.667)	Acc@5 99.667 (99.667)
Epoch: [144][64/167]	Time 0.164 (0.157)	Data 0.000 (0.005)	Loss 0.6186 (0.6224)	Acc@1 88.333 (87.903)	Acc@5 100.000 (99.497)
Epoch: [144][128/167]	Time 0.154 (0.158)	Data 0.000 (0.003)	Loss 0.7144 (0.6305)	Acc@1 86.000 (87.543)	Acc@5 99.000 (99.537)
Max memory in training epoch: 66.7262976
lr: 0.11744098659875644
1
Epoche:145/145; Lr: 0.11744098659875644
batch Size 300
Epoch: [145][0/167]	Time 0.216 (0.216)	Data 0.455 (0.455)	Loss 0.5186 (0.5186)	Acc@1 91.333 (91.333)	Acc@5 99.667 (99.667)
Epoch: [145][64/167]	Time 0.143 (0.163)	Data 0.000 (0.007)	Loss 0.5949 (0.6282)	Acc@1 89.333 (87.636)	Acc@5 99.667 (99.564)
Epoch: [145][128/167]	Time 0.153 (0.157)	Data 0.000 (0.004)	Loss 0.5977 (0.6282)	Acc@1 88.667 (87.576)	Acc@5 100.000 (99.568)
Max memory in training epoch: 66.7262976
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 350618 ; 351486 ; 0.9975304848557268
[INFO] Storing checkpoint...
  73.9
Max memory: 88.0328192
 26.708s  j: 146 bis 150
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 582
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 146
Max memory: 0.1479168
lr: 0.1376261561704177
1
Epoche:146/150; Lr: 0.1376261561704177
batch Size 300
Epoch: [146][0/167]	Time 0.282 (0.282)	Data 0.384 (0.384)	Loss 0.6020 (0.6020)	Acc@1 87.333 (87.333)	Acc@5 99.333 (99.333)
Epoch: [146][64/167]	Time 0.163 (0.158)	Data 0.000 (0.006)	Loss 0.6023 (0.6485)	Acc@1 89.000 (86.851)	Acc@5 99.333 (99.472)
Epoch: [146][128/167]	Time 0.159 (0.158)	Data 0.000 (0.003)	Loss 0.7425 (0.6561)	Acc@1 83.000 (86.618)	Acc@5 99.667 (99.483)
Max memory in training epoch: 66.0856832
lr: 0.1376261561704177
1
Epoche:147/150; Lr: 0.1376261561704177
batch Size 300
Epoch: [147][0/167]	Time 0.313 (0.313)	Data 0.412 (0.412)	Loss 0.6037 (0.6037)	Acc@1 90.333 (90.333)	Acc@5 99.667 (99.667)
Epoch: [147][64/167]	Time 0.134 (0.158)	Data 0.000 (0.007)	Loss 0.7124 (0.6861)	Acc@1 83.667 (85.615)	Acc@5 99.333 (99.456)
Epoch: [147][128/167]	Time 0.128 (0.155)	Data 0.000 (0.003)	Loss 0.7159 (0.6784)	Acc@1 85.667 (86.243)	Acc@5 99.333 (99.463)
Max memory in training epoch: 66.2437888
lr: 0.1376261561704177
1
Epoche:148/150; Lr: 0.1376261561704177
batch Size 300
Epoch: [148][0/167]	Time 0.210 (0.210)	Data 0.436 (0.436)	Loss 0.6557 (0.6557)	Acc@1 88.000 (88.000)	Acc@5 99.667 (99.667)
Epoch: [148][64/167]	Time 0.159 (0.157)	Data 0.000 (0.007)	Loss 0.5757 (0.6622)	Acc@1 87.333 (87.051)	Acc@5 100.000 (99.497)
Epoch: [148][128/167]	Time 0.163 (0.158)	Data 0.000 (0.004)	Loss 0.7421 (0.6733)	Acc@1 83.333 (86.457)	Acc@5 99.333 (99.465)
Max memory in training epoch: 66.2437888
Drin!!
old memory: 667262976
new memory: 662437888
Faktor: 0.9927688360158619
New batch Size größer 303!!
lr: 0.1376261561704177
1
Epoche:149/150; Lr: 0.1376261561704177
batch Size 303
Epoch: [149][0/167]	Time 0.225 (0.225)	Data 0.435 (0.435)	Loss 0.5979 (0.5979)	Acc@1 88.667 (88.667)	Acc@5 99.667 (99.667)
Epoch: [149][64/167]	Time 0.156 (0.154)	Data 0.000 (0.007)	Loss 0.6398 (0.6632)	Acc@1 87.667 (86.759)	Acc@5 99.667 (99.544)
Epoch: [149][128/167]	Time 0.154 (0.157)	Data 0.000 (0.004)	Loss 0.6189 (0.6676)	Acc@1 87.667 (86.687)	Acc@5 99.333 (99.478)
Max memory in training epoch: 66.2437888
lr: 0.1376261561704177
1
Epoche:150/150; Lr: 0.01376261561704177
batch Size 303
Epoch: [150][0/167]	Time 0.220 (0.220)	Data 0.530 (0.530)	Loss 0.6438 (0.6438)	Acc@1 88.333 (88.333)	Acc@5 99.667 (99.667)
Epoch: [150][64/167]	Time 0.189 (0.161)	Data 0.000 (0.008)	Loss 0.5205 (0.5666)	Acc@1 91.333 (89.908)	Acc@5 99.667 (99.723)
Epoch: [150][128/167]	Time 0.192 (0.160)	Data 0.000 (0.004)	Loss 0.5554 (0.5320)	Acc@1 90.333 (91.183)	Acc@5 99.333 (99.755)
Max memory in training epoch: 66.2437888
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 350328 ; 350618 ; 0.999172889013114
[INFO] Storing checkpoint...
  90.73
Max memory: 88.2539008
 27.272s  j: 151 bis 155
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 3970
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 151
Max memory: 0.1478144
lr: 0.016289345827983033
1
Epoche:151/155; Lr: 0.016289345827983033
batch Size 303
Epoch: [151][0/166]	Time 0.269 (0.269)	Data 0.392 (0.392)	Loss 0.4772 (0.4772)	Acc@1 93.399 (93.399)	Acc@5 100.000 (100.000)
Epoch: [151][64/166]	Time 0.167 (0.156)	Data 0.000 (0.006)	Loss 0.4819 (0.4734)	Acc@1 93.069 (93.288)	Acc@5 100.000 (99.838)
Epoch: [151][128/166]	Time 0.143 (0.154)	Data 0.000 (0.003)	Loss 0.5078 (0.4689)	Acc@1 90.759 (93.236)	Acc@5 100.000 (99.836)
Max memory in training epoch: 66.727168
lr: 0.016289345827983033
1
Epoche:152/155; Lr: 0.016289345827983033
batch Size 303
Epoch: [152][0/166]	Time 0.263 (0.263)	Data 0.432 (0.432)	Loss 0.4157 (0.4157)	Acc@1 95.380 (95.380)	Acc@5 100.000 (100.000)
Epoch: [152][64/166]	Time 0.167 (0.157)	Data 0.000 (0.007)	Loss 0.4346 (0.4601)	Acc@1 94.389 (93.440)	Acc@5 100.000 (99.827)
Epoch: [152][128/166]	Time 0.141 (0.155)	Data 0.000 (0.004)	Loss 0.4276 (0.4552)	Acc@1 94.059 (93.484)	Acc@5 100.000 (99.849)
Max memory in training epoch: 66.6054656
lr: 0.016289345827983033
1
Epoche:153/155; Lr: 0.016289345827983033
batch Size 303
Epoch: [153][0/166]	Time 0.244 (0.244)	Data 0.455 (0.455)	Loss 0.4516 (0.4516)	Acc@1 94.059 (94.059)	Acc@5 99.670 (99.670)
Epoch: [153][64/166]	Time 0.133 (0.156)	Data 0.000 (0.007)	Loss 0.4124 (0.4443)	Acc@1 93.729 (93.653)	Acc@5 99.670 (99.878)
Epoch: [153][128/166]	Time 0.165 (0.157)	Data 0.000 (0.004)	Loss 0.4104 (0.4359)	Acc@1 95.050 (93.990)	Acc@5 100.000 (99.877)
Max memory in training epoch: 66.6054656
Drin!!
old memory: 662437888
new memory: 666054656
Faktor: 1.005459784329244
New batch Size kleiner 304!!
lr: 0.016289345827983033
1
Epoche:154/155; Lr: 0.016289345827983033
batch Size 304
Epoch: [154][0/166]	Time 0.293 (0.293)	Data 0.391 (0.391)	Loss 0.4022 (0.4022)	Acc@1 95.050 (95.050)	Acc@5 99.670 (99.670)
Epoch: [154][64/166]	Time 0.148 (0.161)	Data 0.000 (0.006)	Loss 0.4398 (0.4290)	Acc@1 94.059 (93.892)	Acc@5 99.670 (99.893)
Epoch: [154][128/166]	Time 0.160 (0.158)	Data 0.000 (0.003)	Loss 0.4332 (0.4203)	Acc@1 93.729 (94.203)	Acc@5 100.000 (99.903)
Max memory in training epoch: 66.6054656
lr: 0.016289345827983033
1
Epoche:155/155; Lr: 0.016289345827983033
batch Size 304
Epoch: [155][0/166]	Time 0.201 (0.201)	Data 0.461 (0.461)	Loss 0.3951 (0.3951)	Acc@1 93.729 (93.729)	Acc@5 100.000 (100.000)
Epoch: [155][64/166]	Time 0.146 (0.155)	Data 0.000 (0.007)	Loss 0.3769 (0.4351)	Acc@1 96.700 (93.547)	Acc@5 100.000 (99.909)
Epoch: [155][128/166]	Time 0.166 (0.155)	Data 0.000 (0.004)	Loss 0.3916 (0.4193)	Acc@1 95.380 (94.075)	Acc@5 100.000 (99.916)
Max memory in training epoch: 66.6054656
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 348882 ; 350328 ; 0.9958724395423717
[INFO] Storing checkpoint...
  91.53
Max memory: 87.0030848
 26.477s  j: 156 bis 160
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5628
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 156
Max memory: 0.1473024
lr: 0.01934359817072985
1
Epoche:156/160; Lr: 0.01934359817072985
batch Size 304
Epoch: [156][0/165]	Time 0.227 (0.227)	Data 0.380 (0.380)	Loss 0.4088 (0.4088)	Acc@1 95.066 (95.066)	Acc@5 100.000 (100.000)
Epoch: [156][64/165]	Time 0.126 (0.153)	Data 0.000 (0.006)	Loss 0.3730 (0.3843)	Acc@1 95.066 (95.273)	Acc@5 100.000 (99.899)
Epoch: [156][128/165]	Time 0.157 (0.154)	Data 0.000 (0.003)	Loss 0.3985 (0.3856)	Acc@1 95.395 (95.117)	Acc@5 100.000 (99.908)
Max memory in training epoch: 68.492288
lr: 0.01934359817072985
1
Epoche:157/160; Lr: 0.01934359817072985
batch Size 304
Epoch: [157][0/165]	Time 0.245 (0.245)	Data 0.426 (0.426)	Loss 0.3554 (0.3554)	Acc@1 96.053 (96.053)	Acc@5 100.000 (100.000)
Epoch: [157][64/165]	Time 0.163 (0.157)	Data 0.000 (0.007)	Loss 0.3568 (0.3762)	Acc@1 94.737 (95.364)	Acc@5 100.000 (99.894)
Epoch: [157][128/165]	Time 0.136 (0.157)	Data 0.000 (0.003)	Loss 0.3745 (0.3778)	Acc@1 94.737 (95.219)	Acc@5 100.000 (99.890)
Max memory in training epoch: 68.6594048
lr: 0.01934359817072985
1
Epoche:158/160; Lr: 0.01934359817072985
batch Size 304
Epoch: [158][0/165]	Time 0.260 (0.260)	Data 0.437 (0.437)	Loss 0.3570 (0.3570)	Acc@1 94.737 (94.737)	Acc@5 100.000 (100.000)
Epoch: [158][64/165]	Time 0.156 (0.159)	Data 0.000 (0.007)	Loss 0.3552 (0.3625)	Acc@1 95.395 (95.395)	Acc@5 100.000 (99.960)
Epoch: [158][128/165]	Time 0.146 (0.159)	Data 0.000 (0.004)	Loss 0.4295 (0.3622)	Acc@1 93.092 (95.433)	Acc@5 100.000 (99.939)
Max memory in training epoch: 68.6594048
Drin!!
old memory: 666054656
new memory: 686594048
Faktor: 1.03083739722405
New batch Size kleiner 313!!
lr: 0.01934359817072985
1
Epoche:159/160; Lr: 0.01934359817072985
batch Size 313
Epoch: [159][0/165]	Time 0.261 (0.261)	Data 0.405 (0.405)	Loss 0.3560 (0.3560)	Acc@1 95.724 (95.724)	Acc@5 99.671 (99.671)
Epoch: [159][64/165]	Time 0.168 (0.159)	Data 0.000 (0.006)	Loss 0.3152 (0.3605)	Acc@1 96.053 (95.091)	Acc@5 100.000 (99.924)
Epoch: [159][128/165]	Time 0.148 (0.158)	Data 0.000 (0.003)	Loss 0.3737 (0.3592)	Acc@1 94.408 (95.173)	Acc@5 99.671 (99.924)
Max memory in training epoch: 68.6594048
lr: 0.01934359817072985
1
Epoche:160/160; Lr: 0.01934359817072985
batch Size 313
Epoch: [160][0/165]	Time 0.251 (0.251)	Data 0.433 (0.433)	Loss 0.3739 (0.3739)	Acc@1 94.737 (94.737)	Acc@5 100.000 (100.000)
Epoch: [160][64/165]	Time 0.147 (0.159)	Data 0.000 (0.007)	Loss 0.3439 (0.3450)	Acc@1 96.711 (95.592)	Acc@5 100.000 (99.954)
Epoch: [160][128/165]	Time 0.167 (0.156)	Data 0.000 (0.004)	Loss 0.3878 (0.3519)	Acc@1 92.434 (95.356)	Acc@5 100.000 (99.939)
Max memory in training epoch: 68.6594048
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  90.45
Max memory: 87.3308672
 26.618s  j: 161 bis 165
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2368
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 161
Max memory: 0.1473024
lr: 0.02365057120093142
1
Epoche:161/165; Lr: 0.02365057120093142
batch Size 313
Epoch: [161][0/160]	Time 0.255 (0.255)	Data 0.466 (0.466)	Loss 0.3657 (0.3657)	Acc@1 94.569 (94.569)	Acc@5 100.000 (100.000)
Epoch: [161][64/160]	Time 0.160 (0.158)	Data 0.000 (0.007)	Loss 0.3519 (0.3441)	Acc@1 95.208 (95.576)	Acc@5 100.000 (99.946)
Epoch: [161][128/160]	Time 0.145 (0.157)	Data 0.000 (0.004)	Loss 0.3392 (0.3518)	Acc@1 96.486 (95.250)	Acc@5 100.000 (99.938)
Max memory in training epoch: 69.541632
lr: 0.02365057120093142
1
Epoche:162/165; Lr: 0.02365057120093142
batch Size 313
Epoch: [162][0/160]	Time 0.202 (0.202)	Data 0.510 (0.510)	Loss 0.3208 (0.3208)	Acc@1 96.805 (96.805)	Acc@5 100.000 (100.000)
Epoch: [162][64/160]	Time 0.181 (0.157)	Data 0.000 (0.008)	Loss 0.3695 (0.3395)	Acc@1 93.930 (95.616)	Acc@5 100.000 (99.946)
Epoch: [162][128/160]	Time 0.147 (0.158)	Data 0.000 (0.004)	Loss 0.4313 (0.3513)	Acc@1 92.013 (95.094)	Acc@5 99.681 (99.938)
Max memory in training epoch: 69.6690176
lr: 0.02365057120093142
1
Epoche:163/165; Lr: 0.02365057120093142
batch Size 313
Epoch: [163][0/160]	Time 0.243 (0.243)	Data 0.471 (0.471)	Loss 0.4100 (0.4100)	Acc@1 92.971 (92.971)	Acc@5 100.000 (100.000)
Epoch: [163][64/160]	Time 0.174 (0.158)	Data 0.000 (0.008)	Loss 0.4230 (0.3500)	Acc@1 93.930 (95.188)	Acc@5 99.361 (99.897)
Epoch: [163][128/160]	Time 0.128 (0.158)	Data 0.000 (0.004)	Loss 0.3403 (0.3540)	Acc@1 95.527 (94.975)	Acc@5 100.000 (99.908)
Max memory in training epoch: 69.6690176
Drin!!
old memory: 686594048
new memory: 696690176
Faktor: 1.01470465412482
New batch Size kleiner 317!!
lr: 0.02365057120093142
1
Epoche:164/165; Lr: 0.02365057120093142
batch Size 317
Epoch: [164][0/160]	Time 0.252 (0.252)	Data 0.339 (0.339)	Loss 0.3454 (0.3454)	Acc@1 93.610 (93.610)	Acc@5 100.000 (100.000)
Epoch: [164][64/160]	Time 0.154 (0.155)	Data 0.000 (0.005)	Loss 0.3654 (0.3479)	Acc@1 95.208 (94.873)	Acc@5 99.681 (99.936)
Epoch: [164][128/160]	Time 0.159 (0.157)	Data 0.000 (0.003)	Loss 0.3454 (0.3547)	Acc@1 95.208 (94.737)	Acc@5 100.000 (99.948)
Max memory in training epoch: 69.6690176
lr: 0.02365057120093142
1
Epoche:165/165; Lr: 0.02365057120093142
batch Size 317
Epoch: [165][0/160]	Time 0.243 (0.243)	Data 0.398 (0.398)	Loss 0.3397 (0.3397)	Acc@1 93.610 (93.610)	Acc@5 100.000 (100.000)
Epoch: [165][64/160]	Time 0.119 (0.160)	Data 0.000 (0.006)	Loss 0.3834 (0.3452)	Acc@1 92.332 (95.036)	Acc@5 100.000 (99.971)
Epoch: [165][128/160]	Time 0.168 (0.159)	Data 0.000 (0.003)	Loss 0.3539 (0.3506)	Acc@1 95.847 (94.759)	Acc@5 100.000 (99.946)
Max memory in training epoch: 69.6690176
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 348448 ; 348882 ; 0.9987560263928779
[INFO] Storing checkpoint...
  89.06
Max memory: 87.3308672
 26.100s  j: 166 bis 170
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9135
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 166
Max memory: 0.1470464
lr: 0.029286058869903358
1
Epoche:166/170; Lr: 0.029286058869903358
batch Size 317
Epoch: [166][0/158]	Time 0.279 (0.279)	Data 0.385 (0.385)	Loss 0.3382 (0.3382)	Acc@1 95.584 (95.584)	Acc@5 100.000 (100.000)
Epoch: [166][64/158]	Time 0.143 (0.159)	Data 0.000 (0.006)	Loss 0.3986 (0.3495)	Acc@1 93.691 (94.797)	Acc@5 99.685 (99.932)
Epoch: [166][128/158]	Time 0.166 (0.160)	Data 0.000 (0.003)	Loss 0.3487 (0.3680)	Acc@1 95.268 (94.016)	Acc@5 100.000 (99.922)
Max memory in training epoch: 69.8874368
lr: 0.029286058869903358
1
Epoche:167/170; Lr: 0.029286058869903358
batch Size 317
Epoch: [167][0/158]	Time 0.200 (0.200)	Data 0.359 (0.359)	Loss 0.3921 (0.3921)	Acc@1 92.429 (92.429)	Acc@5 99.369 (99.369)
Epoch: [167][64/158]	Time 0.177 (0.160)	Data 0.000 (0.006)	Loss 0.4195 (0.3675)	Acc@1 92.114 (94.016)	Acc@5 100.000 (99.937)
Epoch: [167][128/158]	Time 0.153 (0.158)	Data 0.000 (0.003)	Loss 0.3831 (0.3831)	Acc@1 92.429 (93.481)	Acc@5 100.000 (99.900)
Max memory in training epoch: 70.0497408
lr: 0.029286058869903358
1
Epoche:168/170; Lr: 0.029286058869903358
batch Size 317
Epoch: [168][0/158]	Time 0.259 (0.259)	Data 0.363 (0.363)	Loss 0.3009 (0.3009)	Acc@1 96.845 (96.845)	Acc@5 100.000 (100.000)
Epoch: [168][64/158]	Time 0.171 (0.164)	Data 0.000 (0.006)	Loss 0.3192 (0.3598)	Acc@1 95.899 (94.268)	Acc@5 100.000 (99.937)
Epoch: [168][128/158]	Time 0.155 (0.161)	Data 0.000 (0.003)	Loss 0.4436 (0.3713)	Acc@1 91.167 (93.850)	Acc@5 99.685 (99.924)
Max memory in training epoch: 70.0497408
Drin!!
old memory: 696690176
new memory: 700497408
Faktor: 1.00546474190559
New batch Size kleiner 318!!
lr: 0.029286058869903358
1
Epoche:169/170; Lr: 0.029286058869903358
batch Size 318
Epoch: [169][0/158]	Time 0.228 (0.228)	Data 0.511 (0.511)	Loss 0.3503 (0.3503)	Acc@1 94.322 (94.322)	Acc@5 100.000 (100.000)
Epoch: [169][64/158]	Time 0.159 (0.164)	Data 0.000 (0.008)	Loss 0.4084 (0.3645)	Acc@1 92.744 (94.089)	Acc@5 100.000 (99.937)
Epoch: [169][128/158]	Time 0.176 (0.161)	Data 0.000 (0.004)	Loss 0.3508 (0.3726)	Acc@1 93.375 (93.820)	Acc@5 100.000 (99.907)
Max memory in training epoch: 70.0497408
lr: 0.029286058869903358
1
Epoche:170/170; Lr: 0.029286058869903358
batch Size 318
Epoch: [170][0/158]	Time 0.235 (0.235)	Data 0.451 (0.451)	Loss 0.3725 (0.3725)	Acc@1 94.637 (94.637)	Acc@5 99.369 (99.369)
Epoch: [170][64/158]	Time 0.151 (0.160)	Data 0.000 (0.007)	Loss 0.3633 (0.3673)	Acc@1 93.060 (94.162)	Acc@5 100.000 (99.879)
Epoch: [170][128/158]	Time 0.154 (0.160)	Data 0.000 (0.004)	Loss 0.3681 (0.3772)	Acc@1 93.060 (93.664)	Acc@5 100.000 (99.880)
Max memory in training epoch: 70.0497408
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  87.8
Max memory: 86.7922944
 25.838s  j: 171 bis 175
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2814
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 171
Max memory: 0.1470464
lr: 0.03637877625245808
1
Epoche:171/175; Lr: 0.03637877625245808
batch Size 318
Epoch: [171][0/158]	Time 0.289 (0.289)	Data 0.380 (0.380)	Loss 0.3665 (0.3665)	Acc@1 94.969 (94.969)	Acc@5 100.000 (100.000)
Epoch: [171][64/158]	Time 0.160 (0.161)	Data 0.000 (0.006)	Loss 0.4393 (0.3833)	Acc@1 90.252 (93.435)	Acc@5 100.000 (99.874)
Epoch: [171][128/158]	Time 0.170 (0.156)	Data 0.000 (0.003)	Loss 0.3986 (0.4054)	Acc@1 91.824 (92.792)	Acc@5 100.000 (99.854)
Max memory in training epoch: 69.9340288
lr: 0.03637877625245808
1
Epoche:172/175; Lr: 0.03637877625245808
batch Size 318
Epoch: [172][0/158]	Time 0.206 (0.206)	Data 0.421 (0.421)	Loss 0.4750 (0.4750)	Acc@1 90.881 (90.881)	Acc@5 99.686 (99.686)
Epoch: [172][64/158]	Time 0.169 (0.161)	Data 0.000 (0.007)	Loss 0.3971 (0.4216)	Acc@1 92.767 (92.284)	Acc@5 99.686 (99.826)
Epoch: [172][128/158]	Time 0.145 (0.162)	Data 0.000 (0.004)	Loss 0.3674 (0.4183)	Acc@1 94.969 (92.499)	Acc@5 100.000 (99.832)
Max memory in training epoch: 70.0968448
lr: 0.03637877625245808
1
Epoche:173/175; Lr: 0.03637877625245808
batch Size 318
Epoch: [173][0/158]	Time 0.210 (0.210)	Data 0.447 (0.447)	Loss 0.4152 (0.4152)	Acc@1 92.138 (92.138)	Acc@5 100.000 (100.000)
Epoch: [173][64/158]	Time 0.164 (0.163)	Data 0.000 (0.007)	Loss 0.3504 (0.4252)	Acc@1 94.969 (92.104)	Acc@5 100.000 (99.855)
Epoch: [173][128/158]	Time 0.147 (0.163)	Data 0.000 (0.004)	Loss 0.4096 (0.4197)	Acc@1 91.824 (92.324)	Acc@5 99.686 (99.871)
Max memory in training epoch: 70.0968448
Drin!!
old memory: 700497408
new memory: 700968448
Faktor: 1.0006724364638906
New batch Size kleiner 318!!
lr: 0.03637877625245808
1
Epoche:174/175; Lr: 0.03637877625245808
batch Size 318
Epoch: [174][0/158]	Time 0.255 (0.255)	Data 0.457 (0.457)	Loss 0.3534 (0.3534)	Acc@1 93.711 (93.711)	Acc@5 100.000 (100.000)
Epoch: [174][64/158]	Time 0.149 (0.161)	Data 0.000 (0.007)	Loss 0.3756 (0.4122)	Acc@1 94.654 (92.700)	Acc@5 100.000 (99.850)
Epoch: [174][128/158]	Time 0.170 (0.161)	Data 0.000 (0.004)	Loss 0.3939 (0.4153)	Acc@1 94.025 (92.485)	Acc@5 99.686 (99.863)
Max memory in training epoch: 70.0968448
lr: 0.03637877625245808
1
Epoche:175/175; Lr: 0.03637877625245808
batch Size 318
Epoch: [175][0/158]	Time 0.209 (0.209)	Data 0.546 (0.546)	Loss 0.4116 (0.4116)	Acc@1 92.453 (92.453)	Acc@5 100.000 (100.000)
Epoch: [175][64/158]	Time 0.160 (0.161)	Data 0.000 (0.009)	Loss 0.3826 (0.4243)	Acc@1 94.340 (92.404)	Acc@5 100.000 (99.836)
Epoch: [175][128/158]	Time 0.156 (0.162)	Data 0.000 (0.004)	Loss 0.4277 (0.4197)	Acc@1 92.767 (92.465)	Acc@5 100.000 (99.861)
Max memory in training epoch: 70.0968448
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  86.98
Max memory: 86.7922944
 26.037s  j: 176 bis 180
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6739
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 176
Max memory: 0.1470464
lr: 0.04518926112610027
1
Epoche:176/180; Lr: 0.04518926112610027
batch Size 318
Epoch: [176][0/158]	Time 0.267 (0.267)	Data 0.483 (0.483)	Loss 0.4587 (0.4587)	Acc@1 88.994 (88.994)	Acc@5 99.686 (99.686)
Epoch: [176][64/158]	Time 0.139 (0.157)	Data 0.000 (0.008)	Loss 0.4279 (0.4095)	Acc@1 91.824 (92.961)	Acc@5 99.686 (99.811)
Epoch: [176][128/158]	Time 0.137 (0.159)	Data 0.000 (0.004)	Loss 0.4789 (0.4370)	Acc@1 89.308 (92.026)	Acc@5 100.000 (99.820)
Max memory in training epoch: 69.9340288
lr: 0.04518926112610027
1
Epoche:177/180; Lr: 0.04518926112610027
batch Size 318
Epoch: [177][0/158]	Time 0.231 (0.231)	Data 0.468 (0.468)	Loss 0.4221 (0.4221)	Acc@1 91.509 (91.509)	Acc@5 100.000 (100.000)
Epoch: [177][64/158]	Time 0.169 (0.159)	Data 0.000 (0.007)	Loss 0.4508 (0.4436)	Acc@1 91.195 (91.896)	Acc@5 100.000 (99.806)
Epoch: [177][128/158]	Time 0.133 (0.159)	Data 0.000 (0.004)	Loss 0.3937 (0.4495)	Acc@1 94.340 (91.746)	Acc@5 99.686 (99.785)
Max memory in training epoch: 70.0968448
lr: 0.04518926112610027
1
Epoche:178/180; Lr: 0.04518926112610027
batch Size 318
Epoch: [178][0/158]	Time 0.261 (0.261)	Data 0.428 (0.428)	Loss 0.4805 (0.4805)	Acc@1 90.566 (90.566)	Acc@5 99.686 (99.686)
Epoch: [178][64/158]	Time 0.166 (0.161)	Data 0.000 (0.007)	Loss 0.4753 (0.4497)	Acc@1 90.881 (91.679)	Acc@5 99.686 (99.802)
Epoch: [178][128/158]	Time 0.160 (0.161)	Data 0.000 (0.004)	Loss 0.4829 (0.4516)	Acc@1 88.994 (91.551)	Acc@5 100.000 (99.822)
Max memory in training epoch: 70.0968448
Drin!!
old memory: 700968448
new memory: 700968448
Faktor: 1.0
lr: 0.04518926112610027
1
Epoche:179/180; Lr: 0.04518926112610027
batch Size 318
Epoch: [179][0/158]	Time 0.195 (0.195)	Data 0.512 (0.512)	Loss 0.4524 (0.4524)	Acc@1 90.881 (90.881)	Acc@5 99.371 (99.371)
Epoch: [179][64/158]	Time 0.166 (0.163)	Data 0.000 (0.008)	Loss 0.4565 (0.4551)	Acc@1 92.138 (91.558)	Acc@5 99.686 (99.777)
Epoch: [179][128/158]	Time 0.151 (0.157)	Data 0.000 (0.004)	Loss 0.4885 (0.4548)	Acc@1 88.365 (91.546)	Acc@5 99.686 (99.798)
Max memory in training epoch: 70.0968448
lr: 0.04518926112610027
1
Epoche:180/180; Lr: 0.04518926112610027
batch Size 318
Epoch: [180][0/158]	Time 0.292 (0.292)	Data 0.455 (0.455)	Loss 0.4579 (0.4579)	Acc@1 89.937 (89.937)	Acc@5 100.000 (100.000)
Epoch: [180][64/158]	Time 0.147 (0.163)	Data 0.000 (0.007)	Loss 0.4566 (0.4506)	Acc@1 92.138 (91.790)	Acc@5 99.686 (99.826)
Epoch: [180][128/158]	Time 0.176 (0.162)	Data 0.000 (0.004)	Loss 0.4617 (0.4503)	Acc@1 91.195 (91.726)	Acc@5 99.057 (99.815)
Max memory in training epoch: 70.0968448
Model: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (41): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): AdaptiveAvgPool2d(output_size=(1, 1))
    (63): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
[INFO] Storing checkpoint...
  86.62
Max memory: 86.7922944
 26.213s  BSize 2
j: 0 bis 5
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 876
Files already downloaded and verified
numoFStages: 3
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
Max memory: 0.202496
batch_size berechnet: 250;389.1 ; lr: 0.1
lr: 0.09765625
1
Epoche:1/5; Lr: 0.09765625
batch Size 250
Epoch: [1][0/200]	Time 0.249 (0.249)	Data 0.419 (0.419)	Loss 3.5133 (3.5133)	Acc@1 11.600 (11.600)	Acc@5 52.000 (52.000)
Epoch: [1][64/200]	Time 0.197 (0.160)	Data 0.000 (0.007)	Loss 2.5504 (2.8753)	Acc@1 36.800 (25.040)	Acc@5 84.800 (78.302)
Epoch: [1][128/200]	Time 0.148 (0.160)	Data 0.000 (0.003)	Loss 2.4469 (2.7090)	Acc@1 38.800 (29.575)	Acc@5 92.000 (83.250)
Epoch: [1][192/200]	Time 0.141 (0.158)	Data 0.000 (0.002)	Loss 2.2542 (2.5952)	Acc@1 46.400 (33.476)	Acc@5 91.600 (85.523)
Max memory in training epoch: 66.4657408
lr: 0.09765625
1
Epoche:2/5; Lr: 0.09765625
batch Size 250
Epoch: [2][0/200]	Time 0.202 (0.202)	Data 0.433 (0.433)	Loss 2.3287 (2.3287)	Acc@1 42.400 (42.400)	Acc@5 91.200 (91.200)
Epoch: [2][64/200]	Time 0.171 (0.155)	Data 0.000 (0.007)	Loss 2.2531 (2.1583)	Acc@1 46.400 (48.455)	Acc@5 91.200 (93.022)
Epoch: [2][128/200]	Time 0.169 (0.153)	Data 0.000 (0.004)	Loss 1.8689 (2.0556)	Acc@1 61.200 (51.916)	Acc@5 94.000 (93.950)
Epoch: [2][192/200]	Time 0.179 (0.155)	Data 0.000 (0.002)	Loss 1.7854 (1.9800)	Acc@1 63.200 (54.263)	Acc@5 94.800 (94.547)
Max memory in training epoch: 66.0135424
lr: 0.09765625
1
Epoche:3/5; Lr: 0.09765625
batch Size 250
Epoch: [3][0/200]	Time 0.242 (0.242)	Data 0.494 (0.494)	Loss 1.7051 (1.7051)	Acc@1 62.400 (62.400)	Acc@5 96.800 (96.800)
Epoch: [3][64/200]	Time 0.153 (0.160)	Data 0.000 (0.008)	Loss 1.7139 (1.6784)	Acc@1 61.200 (62.757)	Acc@5 97.200 (96.634)
Epoch: [3][128/200]	Time 0.174 (0.158)	Data 0.000 (0.004)	Loss 1.5129 (1.6369)	Acc@1 69.200 (64.133)	Acc@5 99.600 (96.887)
Epoch: [3][192/200]	Time 0.159 (0.160)	Data 0.000 (0.003)	Loss 1.4205 (1.5914)	Acc@1 67.600 (65.299)	Acc@5 97.200 (97.038)
Max memory in training epoch: 66.0135424
Drin!!
old memory: 0
new memory: 660135424
lr: 0.09765625
1
Epoche:4/5; Lr: 0.09765625
batch Size 250
Epoch: [4][0/200]	Time 0.207 (0.207)	Data 0.394 (0.394)	Loss 1.4306 (1.4306)	Acc@1 70.000 (70.000)	Acc@5 96.800 (96.800)
Epoch: [4][64/200]	Time 0.146 (0.157)	Data 0.000 (0.006)	Loss 1.3143 (1.4083)	Acc@1 72.000 (70.652)	Acc@5 98.400 (97.594)
Epoch: [4][128/200]	Time 0.156 (0.158)	Data 0.000 (0.003)	Loss 1.4216 (1.3751)	Acc@1 70.000 (71.594)	Acc@5 96.400 (97.870)
Epoch: [4][192/200]	Time 0.156 (0.158)	Data 0.000 (0.002)	Loss 1.2281 (1.3436)	Acc@1 75.200 (72.321)	Acc@5 98.800 (97.990)
Max memory in training epoch: 66.0135424
lr: 0.09765625
1
Epoche:5/5; Lr: 0.09765625
batch Size 250
Epoch: [5][0/200]	Time 0.197 (0.197)	Data 0.385 (0.385)	Loss 1.2655 (1.2655)	Acc@1 75.600 (75.600)	Acc@5 99.200 (99.200)
Epoch: [5][64/200]	Time 0.169 (0.160)	Data 0.000 (0.006)	Loss 1.1285 (1.2117)	Acc@1 78.400 (75.723)	Acc@5 98.000 (98.308)
Epoch: [5][128/200]	Time 0.172 (0.159)	Data 0.000 (0.003)	Loss 1.2082 (1.2103)	Acc@1 73.200 (75.253)	Acc@5 97.200 (98.298)
Epoch: [5][192/200]	Time 0.127 (0.159)	Data 0.000 (0.002)	Loss 1.1515 (1.2002)	Acc@1 78.800 (75.324)	Acc@5 98.000 (98.342)
Max memory in training epoch: 66.0135424
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  68.62
Max memory: 103.3835008
 32.145s  ./run_BSize.sh: 7: ./run_BSize.sh: cannot open j: 6 bis 10: No such file
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4053
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 6
Max memory: 0.202496
lr: 0.095367431640625
1
Epoche:6/10; Lr: 0.095367431640625
batch Size 250
Epoch: [6][0/200]	Time 0.266 (0.266)	Data 0.344 (0.344)	Loss 1.2017 (1.2017)	Acc@1 74.000 (74.000)	Acc@5 98.400 (98.400)
Epoch: [6][64/200]	Time 0.138 (0.165)	Data 0.000 (0.005)	Loss 1.0937 (1.1053)	Acc@1 80.000 (77.982)	Acc@5 98.800 (98.775)
Epoch: [6][128/200]	Time 0.162 (0.163)	Data 0.000 (0.003)	Loss 1.0473 (1.0986)	Acc@1 79.600 (77.823)	Acc@5 99.200 (98.698)
Epoch: [6][192/200]	Time 0.175 (0.163)	Data 0.000 (0.002)	Loss 1.1357 (1.0883)	Acc@1 72.000 (77.901)	Acc@5 98.000 (98.703)
Max memory in training epoch: 66.4656384
lr: 0.095367431640625
1
Epoche:7/10; Lr: 0.095367431640625
batch Size 250
Epoch: [7][0/200]	Time 0.222 (0.222)	Data 0.378 (0.378)	Loss 1.1418 (1.1418)	Acc@1 76.400 (76.400)	Acc@5 97.200 (97.200)
Epoch: [7][64/200]	Time 0.155 (0.162)	Data 0.000 (0.006)	Loss 1.0392 (1.0295)	Acc@1 78.400 (79.126)	Acc@5 97.600 (98.757)
Epoch: [7][128/200]	Time 0.157 (0.165)	Data 0.000 (0.003)	Loss 1.0246 (1.0277)	Acc@1 79.200 (79.017)	Acc@5 97.600 (98.757)
Epoch: [7][192/200]	Time 0.169 (0.164)	Data 0.000 (0.002)	Loss 0.9809 (1.0231)	Acc@1 78.400 (78.960)	Acc@5 98.800 (98.746)
Max memory in training epoch: 66.01344
lr: 0.095367431640625
1
Epoche:8/10; Lr: 0.095367431640625
batch Size 250
Epoch: [8][0/200]	Time 0.229 (0.229)	Data 0.459 (0.459)	Loss 0.9132 (0.9132)	Acc@1 82.400 (82.400)	Acc@5 99.200 (99.200)
Epoch: [8][64/200]	Time 0.147 (0.156)	Data 0.000 (0.007)	Loss 0.9517 (0.9861)	Acc@1 80.000 (79.545)	Acc@5 98.800 (98.874)
Epoch: [8][128/200]	Time 0.139 (0.157)	Data 0.000 (0.004)	Loss 0.9987 (0.9765)	Acc@1 81.200 (79.755)	Acc@5 98.400 (98.946)
Epoch: [8][192/200]	Time 0.142 (0.154)	Data 0.000 (0.003)	Loss 0.9491 (0.9693)	Acc@1 82.000 (79.874)	Acc@5 99.200 (98.964)
Max memory in training epoch: 66.01344
Drin!!
old memory: 660135424
new memory: 660134400
Faktor: 0.9999984488031353
New batch Size größer 253!!
lr: 0.095367431640625
1
Epoche:9/10; Lr: 0.095367431640625
batch Size 253
Epoch: [9][0/200]	Time 0.240 (0.240)	Data 0.370 (0.370)	Loss 0.9202 (0.9202)	Acc@1 82.400 (82.400)	Acc@5 99.200 (99.200)
Epoch: [9][64/200]	Time 0.173 (0.166)	Data 0.000 (0.006)	Loss 0.8551 (0.9431)	Acc@1 82.800 (80.400)	Acc@5 99.600 (99.009)
Epoch: [9][128/200]	Time 0.169 (0.162)	Data 0.000 (0.003)	Loss 0.8867 (0.9328)	Acc@1 82.800 (80.673)	Acc@5 98.800 (99.079)
Epoch: [9][192/200]	Time 0.142 (0.163)	Data 0.000 (0.002)	Loss 0.9140 (0.9346)	Acc@1 80.800 (80.630)	Acc@5 96.800 (99.007)
Max memory in training epoch: 66.01344
lr: 0.095367431640625
1
Epoche:10/10; Lr: 0.095367431640625
batch Size 253
Epoch: [10][0/200]	Time 0.200 (0.200)	Data 0.433 (0.433)	Loss 0.9089 (0.9089)	Acc@1 80.800 (80.800)	Acc@5 99.600 (99.600)
Epoch: [10][64/200]	Time 0.162 (0.161)	Data 0.000 (0.007)	Loss 0.9839 (0.9067)	Acc@1 80.800 (81.588)	Acc@5 99.200 (99.052)
Epoch: [10][128/200]	Time 0.167 (0.163)	Data 0.000 (0.004)	Loss 0.9350 (0.9135)	Acc@1 81.600 (81.309)	Acc@5 99.200 (99.064)
Epoch: [10][192/200]	Time 0.162 (0.162)	Data 0.000 (0.002)	Loss 0.8993 (0.9164)	Acc@1 80.800 (81.026)	Acc@5 99.200 (99.038)
Max memory in training epoch: 66.01344
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  73.25
Max memory: 103.3833984
 33.004s  j: 11 bis 15
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8990
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 11
Max memory: 0.202496
lr: 0.09424984455108643
1
Epoche:11/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [11][0/198]	Time 0.277 (0.277)	Data 0.480 (0.480)	Loss 0.9080 (0.9080)	Acc@1 84.585 (84.585)	Acc@5 98.419 (98.419)
Epoch: [11][64/198]	Time 0.155 (0.166)	Data 0.000 (0.008)	Loss 0.9945 (0.8801)	Acc@1 75.099 (82.493)	Acc@5 99.209 (99.112)
Epoch: [11][128/198]	Time 0.161 (0.161)	Data 0.000 (0.004)	Loss 0.9720 (0.8906)	Acc@1 75.494 (81.889)	Acc@5 98.814 (99.065)
Epoch: [11][192/198]	Time 0.157 (0.157)	Data 0.000 (0.003)	Loss 0.8651 (0.8875)	Acc@1 84.980 (81.949)	Acc@5 99.605 (99.074)
Max memory in training epoch: 66.5037312
lr: 0.09424984455108643
1
Epoche:12/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [12][0/198]	Time 0.203 (0.203)	Data 0.353 (0.353)	Loss 0.8802 (0.8802)	Acc@1 83.794 (83.794)	Acc@5 99.209 (99.209)
Epoch: [12][64/198]	Time 0.153 (0.160)	Data 0.000 (0.006)	Loss 0.8843 (0.8879)	Acc@1 81.028 (81.806)	Acc@5 98.814 (99.076)
Epoch: [12][128/198]	Time 0.158 (0.159)	Data 0.000 (0.003)	Loss 0.8846 (0.8895)	Acc@1 79.842 (81.910)	Acc@5 98.024 (99.016)
Epoch: [12][192/198]	Time 0.165 (0.159)	Data 0.000 (0.002)	Loss 0.8942 (0.8871)	Acc@1 83.399 (81.962)	Acc@5 99.605 (99.040)
Max memory in training epoch: 66.277632
lr: 0.09424984455108643
1
Epoche:13/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [13][0/198]	Time 0.216 (0.216)	Data 0.483 (0.483)	Loss 0.7696 (0.7696)	Acc@1 87.352 (87.352)	Acc@5 100.000 (100.000)
Epoch: [13][64/198]	Time 0.159 (0.159)	Data 0.000 (0.008)	Loss 0.8674 (0.8660)	Acc@1 80.237 (82.724)	Acc@5 99.605 (99.082)
Epoch: [13][128/198]	Time 0.158 (0.158)	Data 0.000 (0.004)	Loss 0.8326 (0.8754)	Acc@1 84.190 (82.367)	Acc@5 99.605 (99.053)
Epoch: [13][192/198]	Time 0.153 (0.158)	Data 0.000 (0.003)	Loss 0.9363 (0.8785)	Acc@1 80.237 (82.132)	Acc@5 99.209 (99.064)
Max memory in training epoch: 66.277632
Drin!!
old memory: 660134400
new memory: 662776320
Faktor: 1.0040020941190158
New batch Size kleiner 254!!
lr: 0.09424984455108643
1
Epoche:14/15; Lr: 0.09424984455108643
batch Size 254
Epoch: [14][0/198]	Time 0.264 (0.264)	Data 0.442 (0.442)	Loss 0.9118 (0.9118)	Acc@1 82.213 (82.213)	Acc@5 98.814 (98.814)
Epoch: [14][64/198]	Time 0.151 (0.160)	Data 0.000 (0.007)	Loss 0.8966 (0.8583)	Acc@1 82.213 (82.797)	Acc@5 99.209 (99.057)
Epoch: [14][128/198]	Time 0.184 (0.160)	Data 0.000 (0.004)	Loss 0.8340 (0.8644)	Acc@1 83.399 (82.682)	Acc@5 100.000 (99.118)
Epoch: [14][192/198]	Time 0.155 (0.159)	Data 0.000 (0.002)	Loss 0.9766 (0.8656)	Acc@1 81.028 (82.656)	Acc@5 98.814 (99.136)
Max memory in training epoch: 66.277632
lr: 0.09424984455108643
1
Epoche:15/15; Lr: 0.09424984455108643
batch Size 254
Epoch: [15][0/198]	Time 0.231 (0.231)	Data 0.419 (0.419)	Loss 0.7710 (0.7710)	Acc@1 85.375 (85.375)	Acc@5 99.605 (99.605)
Epoch: [15][64/198]	Time 0.154 (0.155)	Data 0.000 (0.007)	Loss 0.8616 (0.8747)	Acc@1 81.818 (82.165)	Acc@5 99.605 (99.082)
Epoch: [15][128/198]	Time 0.157 (0.155)	Data 0.000 (0.003)	Loss 0.8374 (0.8676)	Acc@1 83.004 (82.523)	Acc@5 98.814 (99.102)
Epoch: [15][192/198]	Time 0.164 (0.156)	Data 0.000 (0.002)	Loss 0.8006 (0.8631)	Acc@1 84.585 (82.584)	Acc@5 99.209 (99.085)
Max memory in training epoch: 66.277632
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  75.29
Max memory: 103.3833984
 31.486s  j: 16 bis 20
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8883
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 16
Max memory: 0.202496
lr: 0.09351351764053106
1
Epoche:16/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [16][0/197]	Time 0.247 (0.247)	Data 0.439 (0.439)	Loss 0.8714 (0.8714)	Acc@1 83.071 (83.071)	Acc@5 99.606 (99.606)
Epoch: [16][64/197]	Time 0.129 (0.159)	Data 0.000 (0.007)	Loss 0.8233 (0.8321)	Acc@1 87.008 (84.070)	Acc@5 100.000 (99.207)
Epoch: [16][128/197]	Time 0.188 (0.160)	Data 0.000 (0.004)	Loss 0.8365 (0.8440)	Acc@1 83.071 (83.532)	Acc@5 99.606 (99.206)
Epoch: [16][192/197]	Time 0.157 (0.161)	Data 0.000 (0.002)	Loss 0.9254 (0.8495)	Acc@1 79.134 (83.214)	Acc@5 99.606 (99.180)
Max memory in training epoch: 66.5164288
lr: 0.09351351764053106
1
Epoche:17/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [17][0/197]	Time 0.213 (0.213)	Data 0.378 (0.378)	Loss 0.8742 (0.8742)	Acc@1 81.496 (81.496)	Acc@5 98.819 (98.819)
Epoch: [17][64/197]	Time 0.156 (0.159)	Data 0.000 (0.006)	Loss 0.9487 (0.8408)	Acc@1 80.709 (83.452)	Acc@5 99.213 (99.231)
Epoch: [17][128/197]	Time 0.164 (0.159)	Data 0.000 (0.003)	Loss 0.8357 (0.8471)	Acc@1 84.646 (83.107)	Acc@5 98.819 (99.164)
Epoch: [17][192/197]	Time 0.161 (0.158)	Data 0.000 (0.002)	Loss 0.9412 (0.8487)	Acc@1 79.528 (83.063)	Acc@5 99.213 (99.215)
Max memory in training epoch: 66.365696
lr: 0.09351351764053106
1
Epoche:18/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [18][0/197]	Time 0.193 (0.193)	Data 0.450 (0.450)	Loss 0.8405 (0.8405)	Acc@1 83.858 (83.858)	Acc@5 99.213 (99.213)
Epoch: [18][64/197]	Time 0.152 (0.153)	Data 0.000 (0.007)	Loss 0.8416 (0.8357)	Acc@1 82.283 (83.477)	Acc@5 99.606 (99.285)
Epoch: [18][128/197]	Time 0.166 (0.155)	Data 0.000 (0.004)	Loss 0.7284 (0.8449)	Acc@1 86.220 (83.346)	Acc@5 99.606 (99.155)
Epoch: [18][192/197]	Time 0.130 (0.157)	Data 0.000 (0.003)	Loss 0.8847 (0.8450)	Acc@1 85.039 (83.363)	Acc@5 98.819 (99.182)
Max memory in training epoch: 66.365696
Drin!!
old memory: 662776320
new memory: 663656960
Faktor: 1.0013287137355782
New batch Size kleiner 254!!
lr: 0.09351351764053106
1
Epoche:19/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [19][0/197]	Time 0.251 (0.251)	Data 0.440 (0.440)	Loss 0.7672 (0.7672)	Acc@1 87.402 (87.402)	Acc@5 99.606 (99.606)
Epoch: [19][64/197]	Time 0.147 (0.162)	Data 0.000 (0.007)	Loss 0.8651 (0.8200)	Acc@1 82.677 (84.240)	Acc@5 99.213 (99.328)
Epoch: [19][128/197]	Time 0.179 (0.161)	Data 0.000 (0.004)	Loss 0.7716 (0.8307)	Acc@1 86.614 (83.724)	Acc@5 99.213 (99.313)
Epoch: [19][192/197]	Time 0.151 (0.161)	Data 0.000 (0.002)	Loss 0.7148 (0.8368)	Acc@1 87.402 (83.514)	Acc@5 100.000 (99.245)
Max memory in training epoch: 66.365696
lr: 0.09351351764053106
1
Epoche:20/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [20][0/197]	Time 0.205 (0.205)	Data 0.456 (0.456)	Loss 0.8526 (0.8526)	Acc@1 82.677 (82.677)	Acc@5 99.213 (99.213)
Epoch: [20][64/197]	Time 0.137 (0.161)	Data 0.000 (0.007)	Loss 0.9061 (0.8239)	Acc@1 78.740 (83.889)	Acc@5 98.819 (99.322)
Epoch: [20][128/197]	Time 0.165 (0.160)	Data 0.000 (0.004)	Loss 0.8082 (0.8160)	Acc@1 85.433 (84.191)	Acc@5 98.425 (99.316)
Epoch: [20][192/197]	Time 0.136 (0.160)	Data 0.000 (0.003)	Loss 0.9749 (0.8302)	Acc@1 79.921 (83.732)	Acc@5 98.031 (99.257)
Max memory in training epoch: 66.365696
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 482768 ; 487386 ; 0.9905249637864034
[INFO] Storing checkpoint...
  73.72
Max memory: 103.3833984
 32.075s  j: 21 bis 25
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7869
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 21
Max memory: 0.2007552
lr: 0.09278294328396441
1
Epoche:21/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [21][0/197]	Time 0.255 (0.255)	Data 0.339 (0.339)	Loss 0.9629 (0.9629)	Acc@1 79.134 (79.134)	Acc@5 99.213 (99.213)
Epoch: [21][64/197]	Time 0.170 (0.158)	Data 0.000 (0.005)	Loss 0.8310 (0.8156)	Acc@1 83.071 (84.349)	Acc@5 99.606 (99.425)
Epoch: [21][128/197]	Time 0.155 (0.159)	Data 0.000 (0.003)	Loss 0.7960 (0.8198)	Acc@1 83.465 (84.170)	Acc@5 100.000 (99.322)
Epoch: [21][192/197]	Time 0.164 (0.159)	Data 0.000 (0.002)	Loss 0.7012 (0.8266)	Acc@1 88.583 (83.862)	Acc@5 99.606 (99.266)
Max memory in training epoch: 66.5094656
lr: 0.09278294328396441
1
Epoche:22/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [22][0/197]	Time 0.217 (0.217)	Data 0.343 (0.343)	Loss 0.8357 (0.8357)	Acc@1 84.252 (84.252)	Acc@5 99.213 (99.213)
Epoch: [22][64/197]	Time 0.167 (0.163)	Data 0.000 (0.005)	Loss 0.7817 (0.8119)	Acc@1 83.858 (84.428)	Acc@5 99.213 (99.310)
Epoch: [22][128/197]	Time 0.136 (0.162)	Data 0.000 (0.003)	Loss 0.7963 (0.8146)	Acc@1 83.465 (84.340)	Acc@5 100.000 (99.295)
Epoch: [22][192/197]	Time 0.158 (0.160)	Data 0.000 (0.002)	Loss 0.7995 (0.8198)	Acc@1 86.614 (84.203)	Acc@5 99.606 (99.249)
Max memory in training epoch: 66.280704
lr: 0.09278294328396441
1
Epoche:23/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [23][0/197]	Time 0.208 (0.208)	Data 0.408 (0.408)	Loss 0.7558 (0.7558)	Acc@1 85.827 (85.827)	Acc@5 100.000 (100.000)
Epoch: [23][64/197]	Time 0.160 (0.167)	Data 0.000 (0.006)	Loss 0.7326 (0.8209)	Acc@1 87.795 (84.034)	Acc@5 99.213 (99.322)
Epoch: [23][128/197]	Time 0.160 (0.164)	Data 0.000 (0.003)	Loss 0.7720 (0.8224)	Acc@1 85.827 (84.054)	Acc@5 99.213 (99.261)
Epoch: [23][192/197]	Time 0.160 (0.162)	Data 0.000 (0.002)	Loss 0.7508 (0.8249)	Acc@1 87.402 (83.962)	Acc@5 99.606 (99.227)
Max memory in training epoch: 66.280704
Drin!!
old memory: 663656960
new memory: 662807040
Faktor: 0.9987193383762599
New batch Size größer 256!!
lr: 0.09278294328396441
1
Epoche:24/25; Lr: 0.09278294328396441
batch Size 256
Epoch: [24][0/197]	Time 0.264 (0.264)	Data 0.491 (0.491)	Loss 0.8771 (0.8771)	Acc@1 85.039 (85.039)	Acc@5 99.213 (99.213)
Epoch: [24][64/197]	Time 0.175 (0.165)	Data 0.000 (0.008)	Loss 0.7472 (0.8220)	Acc@1 89.764 (84.300)	Acc@5 99.606 (99.303)
Epoch: [24][128/197]	Time 0.165 (0.161)	Data 0.000 (0.004)	Loss 0.7574 (0.8160)	Acc@1 86.614 (84.463)	Acc@5 99.606 (99.292)
Epoch: [24][192/197]	Time 0.177 (0.161)	Data 0.000 (0.003)	Loss 0.8385 (0.8146)	Acc@1 83.071 (84.470)	Acc@5 99.606 (99.302)
Max memory in training epoch: 66.280704
lr: 0.09278294328396441
1
Epoche:25/25; Lr: 0.09278294328396441
batch Size 256
Epoch: [25][0/197]	Time 0.195 (0.195)	Data 0.507 (0.507)	Loss 0.7566 (0.7566)	Acc@1 88.583 (88.583)	Acc@5 99.606 (99.606)
Epoch: [25][64/197]	Time 0.159 (0.164)	Data 0.000 (0.008)	Loss 0.7409 (0.8063)	Acc@1 85.827 (84.476)	Acc@5 98.425 (99.340)
Epoch: [25][128/197]	Time 0.165 (0.162)	Data 0.000 (0.004)	Loss 0.8628 (0.8100)	Acc@1 83.858 (84.521)	Acc@5 99.606 (99.304)
Epoch: [25][192/197]	Time 0.162 (0.162)	Data 0.000 (0.003)	Loss 0.8837 (0.8146)	Acc@1 77.953 (84.423)	Acc@5 99.213 (99.296)
Max memory in training epoch: 66.280704
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 464000 ; 482768 ; 0.961124183872999
[INFO] Storing checkpoint...
  73.82
Max memory: 103.337216
 32.512s  j: 26 bis 30
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 156
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 26
Max memory: 0.1931776
lr: 0.09278294328396441
1
Epoche:26/30; Lr: 0.09278294328396441
batch Size 256
Epoch: [26][0/196]	Time 0.302 (0.302)	Data 0.434 (0.434)	Loss 0.7553 (0.7553)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [26][64/196]	Time 0.152 (0.159)	Data 0.000 (0.007)	Loss 0.7768 (0.7881)	Acc@1 87.500 (85.222)	Acc@5 99.609 (99.279)
Epoch: [26][128/196]	Time 0.154 (0.158)	Data 0.000 (0.004)	Loss 0.8227 (0.7970)	Acc@1 83.594 (84.790)	Acc@5 99.609 (99.294)
Epoch: [26][192/196]	Time 0.141 (0.159)	Data 0.000 (0.003)	Loss 0.7774 (0.8091)	Acc@1 86.719 (84.304)	Acc@5 99.219 (99.300)
Max memory in training epoch: 65.7705472
lr: 0.09278294328396441
1
Epoche:27/30; Lr: 0.09278294328396441
batch Size 256
Epoch: [27][0/196]	Time 0.227 (0.227)	Data 0.449 (0.449)	Loss 0.7528 (0.7528)	Acc@1 86.328 (86.328)	Acc@5 99.219 (99.219)
Epoch: [27][64/196]	Time 0.152 (0.157)	Data 0.000 (0.007)	Loss 0.8493 (0.7975)	Acc@1 81.641 (84.657)	Acc@5 100.000 (99.459)
Epoch: [27][128/196]	Time 0.206 (0.153)	Data 0.000 (0.004)	Loss 0.8407 (0.8023)	Acc@1 81.250 (84.539)	Acc@5 98.438 (99.328)
Epoch: [27][192/196]	Time 0.154 (0.154)	Data 0.000 (0.003)	Loss 0.9569 (0.8089)	Acc@1 77.344 (84.270)	Acc@5 98.828 (99.350)
Max memory in training epoch: 65.954048
lr: 0.09278294328396441
1
Epoche:28/30; Lr: 0.09278294328396441
batch Size 256
Epoch: [28][0/196]	Time 0.249 (0.249)	Data 0.440 (0.440)	Loss 0.7795 (0.7795)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [28][64/196]	Time 0.154 (0.164)	Data 0.000 (0.007)	Loss 0.7137 (0.8107)	Acc@1 87.109 (84.459)	Acc@5 100.000 (99.351)
Epoch: [28][128/196]	Time 0.176 (0.162)	Data 0.000 (0.004)	Loss 0.8405 (0.8078)	Acc@1 82.422 (84.554)	Acc@5 99.219 (99.334)
Epoch: [28][192/196]	Time 0.161 (0.161)	Data 0.000 (0.002)	Loss 0.7674 (0.8077)	Acc@1 85.156 (84.474)	Acc@5 99.609 (99.306)
Max memory in training epoch: 65.954048
Drin!!
old memory: 662807040
new memory: 659540480
Faktor: 0.9950716274830153
New batch Size größer 260!!
lr: 0.09278294328396441
1
Epoche:29/30; Lr: 0.09278294328396441
batch Size 260
Epoch: [29][0/196]	Time 0.193 (0.193)	Data 0.410 (0.410)	Loss 0.7882 (0.7882)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [29][64/196]	Time 0.161 (0.163)	Data 0.000 (0.007)	Loss 0.8771 (0.7834)	Acc@1 81.641 (85.090)	Acc@5 100.000 (99.453)
Epoch: [29][128/196]	Time 0.142 (0.162)	Data 0.000 (0.003)	Loss 0.8308 (0.7941)	Acc@1 81.641 (84.741)	Acc@5 99.219 (99.425)
Epoch: [29][192/196]	Time 0.165 (0.160)	Data 0.000 (0.002)	Loss 0.7856 (0.7958)	Acc@1 85.938 (84.796)	Acc@5 98.438 (99.411)
Max memory in training epoch: 65.954048
lr: 0.09278294328396441
1
Epoche:30/30; Lr: 0.09278294328396441
batch Size 260
Epoch: [30][0/196]	Time 0.201 (0.201)	Data 0.581 (0.581)	Loss 0.7874 (0.7874)	Acc@1 86.328 (86.328)	Acc@5 99.219 (99.219)
Epoch: [30][64/196]	Time 0.184 (0.163)	Data 0.000 (0.009)	Loss 0.7796 (0.7815)	Acc@1 83.984 (85.264)	Acc@5 99.609 (99.375)
Epoch: [30][128/196]	Time 0.162 (0.163)	Data 0.000 (0.005)	Loss 0.7627 (0.7896)	Acc@1 85.938 (84.990)	Acc@5 99.609 (99.334)
Epoch: [30][192/196]	Time 0.123 (0.159)	Data 0.000 (0.003)	Loss 0.8348 (0.7894)	Acc@1 83.203 (84.903)	Acc@5 100.000 (99.354)
Max memory in training epoch: 65.954048
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 434550 ; 464000 ; 0.9365301724137931
[INFO] Storing checkpoint...
  77.4
Max memory: 101.8776064
 31.893s  j: 31 bis 35
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7393
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 31
Max memory: 0.1816064
lr: 0.09423267677277636
1
Epoche:31/35; Lr: 0.09423267677277636
batch Size 260
Epoch: [31][0/193]	Time 0.243 (0.243)	Data 0.428 (0.428)	Loss 0.7291 (0.7291)	Acc@1 85.769 (85.769)	Acc@5 99.615 (99.615)
Epoch: [31][64/193]	Time 0.165 (0.154)	Data 0.000 (0.007)	Loss 0.8506 (0.7516)	Acc@1 83.462 (86.172)	Acc@5 100.000 (99.467)
Epoch: [31][128/193]	Time 0.158 (0.155)	Data 0.000 (0.004)	Loss 0.7443 (0.7738)	Acc@1 86.923 (85.379)	Acc@5 99.231 (99.371)
Epoch: [31][192/193]	Time 0.114 (0.154)	Data 0.000 (0.002)	Loss 0.7316 (0.7765)	Acc@1 86.250 (85.360)	Acc@5 98.750 (99.382)
Max memory in training epoch: 64.9104896
lr: 0.09423267677277636
1
Epoche:32/35; Lr: 0.09423267677277636
batch Size 260
Epoch: [32][0/193]	Time 0.256 (0.256)	Data 0.406 (0.406)	Loss 0.7689 (0.7689)	Acc@1 83.462 (83.462)	Acc@5 98.846 (98.846)
Epoch: [32][64/193]	Time 0.160 (0.159)	Data 0.000 (0.006)	Loss 0.8007 (0.7892)	Acc@1 86.538 (84.858)	Acc@5 99.231 (99.479)
Epoch: [32][128/193]	Time 0.177 (0.158)	Data 0.000 (0.003)	Loss 0.7110 (0.7913)	Acc@1 86.538 (84.687)	Acc@5 99.615 (99.404)
Epoch: [32][192/193]	Time 0.102 (0.159)	Data 0.000 (0.002)	Loss 0.9407 (0.7938)	Acc@1 77.500 (84.654)	Acc@5 98.750 (99.344)
Max memory in training epoch: 64.9399808
lr: 0.09423267677277636
1
Epoche:33/35; Lr: 0.09423267677277636
batch Size 260
Epoch: [33][0/193]	Time 0.221 (0.221)	Data 0.428 (0.428)	Loss 0.9252 (0.9252)	Acc@1 81.154 (81.154)	Acc@5 98.846 (98.846)
Epoch: [33][64/193]	Time 0.157 (0.158)	Data 0.000 (0.007)	Loss 0.6965 (0.7972)	Acc@1 86.923 (84.325)	Acc@5 99.231 (99.343)
Epoch: [33][128/193]	Time 0.159 (0.157)	Data 0.000 (0.003)	Loss 0.7359 (0.7909)	Acc@1 86.154 (84.642)	Acc@5 100.000 (99.368)
Epoch: [33][192/193]	Time 0.137 (0.156)	Data 0.000 (0.002)	Loss 0.7592 (0.7861)	Acc@1 87.500 (84.920)	Acc@5 98.750 (99.370)
Max memory in training epoch: 64.862976
Drin!!
old memory: 659540480
new memory: 648629760
Faktor: 0.983457088183579
New batch Size größer 268!!
lr: 0.09423267677277636
1
Epoche:34/35; Lr: 0.09423267677277636
batch Size 268
Epoch: [34][0/193]	Time 0.272 (0.272)	Data 0.335 (0.335)	Loss 0.8128 (0.8128)	Acc@1 84.231 (84.231)	Acc@5 99.231 (99.231)
Epoch: [34][64/193]	Time 0.134 (0.163)	Data 0.000 (0.005)	Loss 0.8762 (0.7796)	Acc@1 83.077 (85.183)	Acc@5 99.231 (99.438)
Epoch: [34][128/193]	Time 0.153 (0.159)	Data 0.000 (0.003)	Loss 0.8963 (0.7838)	Acc@1 81.154 (84.899)	Acc@5 98.462 (99.344)
Epoch: [34][192/193]	Time 0.119 (0.157)	Data 0.000 (0.002)	Loss 0.9753 (0.7834)	Acc@1 78.750 (84.866)	Acc@5 98.750 (99.306)
Max memory in training epoch: 64.862976
lr: 0.09423267677277636
1
Epoche:35/35; Lr: 0.09423267677277636
batch Size 268
Epoch: [35][0/193]	Time 0.213 (0.213)	Data 0.448 (0.448)	Loss 0.7306 (0.7306)	Acc@1 86.923 (86.923)	Acc@5 99.615 (99.615)
Epoch: [35][64/193]	Time 0.175 (0.160)	Data 0.000 (0.007)	Loss 0.8527 (0.7756)	Acc@1 83.462 (85.337)	Acc@5 100.000 (99.462)
Epoch: [35][128/193]	Time 0.165 (0.158)	Data 0.000 (0.004)	Loss 0.9137 (0.7793)	Acc@1 82.308 (85.203)	Acc@5 98.462 (99.398)
Epoch: [35][192/193]	Time 0.106 (0.156)	Data 0.000 (0.002)	Loss 0.8815 (0.7804)	Acc@1 82.500 (85.194)	Acc@5 98.750 (99.402)
Max memory in training epoch: 64.862976
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 402798 ; 434550 ; 0.9269313082499137
[INFO] Storing checkpoint...
  78.1
Max memory: 98.5497088
 30.677s  j: 36 bis 40
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8785
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 36
Max memory: 0.1690112
lr: 0.09864983349650025
1
Epoche:36/40; Lr: 0.09864983349650025
batch Size 268
Epoch: [36][0/187]	Time 0.268 (0.268)	Data 0.453 (0.453)	Loss 0.7389 (0.7389)	Acc@1 85.075 (85.075)	Acc@5 100.000 (100.000)
Epoch: [36][64/187]	Time 0.169 (0.158)	Data 0.000 (0.007)	Loss 0.7659 (0.7490)	Acc@1 86.194 (85.855)	Acc@5 98.881 (99.489)
Epoch: [36][128/187]	Time 0.130 (0.156)	Data 0.000 (0.004)	Loss 0.7816 (0.7773)	Acc@1 85.448 (85.118)	Acc@5 99.627 (99.390)
Max memory in training epoch: 64.701184
lr: 0.09864983349650025
1
Epoche:37/40; Lr: 0.09864983349650025
batch Size 268
Epoch: [37][0/187]	Time 0.178 (0.178)	Data 0.346 (0.346)	Loss 0.7775 (0.7775)	Acc@1 83.582 (83.582)	Acc@5 99.627 (99.627)
Epoch: [37][64/187]	Time 0.150 (0.154)	Data 0.000 (0.006)	Loss 0.8173 (0.7593)	Acc@1 82.836 (85.700)	Acc@5 99.254 (99.392)
Epoch: [37][128/187]	Time 0.150 (0.155)	Data 0.000 (0.003)	Loss 0.7531 (0.7735)	Acc@1 86.567 (85.271)	Acc@5 99.627 (99.381)
Max memory in training epoch: 64.6893056
lr: 0.09864983349650025
1
Epoche:38/40; Lr: 0.09864983349650025
batch Size 268
Epoch: [38][0/187]	Time 0.252 (0.252)	Data 0.370 (0.370)	Loss 0.7091 (0.7091)	Acc@1 87.687 (87.687)	Acc@5 100.000 (100.000)
Epoch: [38][64/187]	Time 0.175 (0.160)	Data 0.000 (0.006)	Loss 0.7763 (0.7729)	Acc@1 85.821 (85.367)	Acc@5 99.627 (99.328)
Epoch: [38][128/187]	Time 0.145 (0.161)	Data 0.000 (0.003)	Loss 0.7586 (0.7739)	Acc@1 86.940 (85.271)	Acc@5 98.881 (99.332)
Max memory in training epoch: 64.6893056
Drin!!
old memory: 648629760
new memory: 646893056
Faktor: 0.9973225033646314
New batch Size größer 277!!
lr: 0.09864983349650025
1
Epoche:39/40; Lr: 0.09864983349650025
batch Size 277
Epoch: [39][0/187]	Time 0.232 (0.232)	Data 0.411 (0.411)	Loss 0.7863 (0.7863)	Acc@1 83.955 (83.955)	Acc@5 99.254 (99.254)
Epoch: [39][64/187]	Time 0.152 (0.160)	Data 0.000 (0.007)	Loss 0.8738 (0.7739)	Acc@1 82.836 (85.149)	Acc@5 99.254 (99.265)
Epoch: [39][128/187]	Time 0.174 (0.159)	Data 0.000 (0.003)	Loss 0.8214 (0.7634)	Acc@1 86.567 (85.763)	Acc@5 98.881 (99.294)
Max memory in training epoch: 64.6893056
lr: 0.09864983349650025
1
Epoche:40/40; Lr: 0.09864983349650025
batch Size 277
Epoch: [40][0/187]	Time 0.221 (0.221)	Data 0.481 (0.481)	Loss 0.7802 (0.7802)	Acc@1 85.075 (85.075)	Acc@5 98.881 (98.881)
Epoch: [40][64/187]	Time 0.160 (0.159)	Data 0.000 (0.008)	Loss 0.7777 (0.7610)	Acc@1 86.567 (85.769)	Acc@5 99.627 (99.403)
Epoch: [40][128/187]	Time 0.159 (0.158)	Data 0.000 (0.004)	Loss 0.6960 (0.7718)	Acc@1 89.179 (85.370)	Acc@5 99.254 (99.335)
Max memory in training epoch: 64.6893056
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 390382 ; 402798 ; 0.9691756165621478
[INFO] Storing checkpoint...
  80.3
Max memory: 95.9609344
 29.818s  j: 41 bis 45
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9918
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 41
Max memory: 0.1643008
lr: 0.10674220265051004
1
Epoche:41/45; Lr: 0.10674220265051004
batch Size 277
Epoch: [41][0/181]	Time 0.285 (0.285)	Data 0.421 (0.421)	Loss 0.7173 (0.7173)	Acc@1 85.921 (85.921)	Acc@5 99.639 (99.639)
Epoch: [41][64/181]	Time 0.157 (0.161)	Data 0.000 (0.007)	Loss 0.7948 (0.7538)	Acc@1 85.921 (85.815)	Acc@5 99.639 (99.489)
Epoch: [41][128/181]	Time 0.139 (0.160)	Data 0.004 (0.003)	Loss 0.8590 (0.7659)	Acc@1 82.671 (85.408)	Acc@5 98.917 (99.429)
Max memory in training epoch: 67.1015936
lr: 0.10674220265051004
1
Epoche:42/45; Lr: 0.10674220265051004
batch Size 277
Epoch: [42][0/181]	Time 0.213 (0.213)	Data 0.491 (0.491)	Loss 0.8248 (0.8248)	Acc@1 83.032 (83.032)	Acc@5 99.639 (99.639)
Epoch: [42][64/181]	Time 0.176 (0.163)	Data 0.000 (0.008)	Loss 0.8836 (0.7966)	Acc@1 85.199 (84.604)	Acc@5 99.639 (99.361)
Epoch: [42][128/181]	Time 0.193 (0.161)	Data 0.000 (0.004)	Loss 0.7616 (0.7910)	Acc@1 86.643 (84.818)	Acc@5 99.639 (99.312)
Max memory in training epoch: 67.3722368
lr: 0.10674220265051004
1
Epoche:43/45; Lr: 0.10674220265051004
batch Size 277
Epoch: [43][0/181]	Time 0.240 (0.240)	Data 0.479 (0.479)	Loss 0.7126 (0.7126)	Acc@1 87.004 (87.004)	Acc@5 99.639 (99.639)
Epoch: [43][64/181]	Time 0.158 (0.163)	Data 0.000 (0.008)	Loss 0.7200 (0.7624)	Acc@1 87.004 (85.498)	Acc@5 99.639 (99.345)
Epoch: [43][128/181]	Time 0.168 (0.162)	Data 0.000 (0.004)	Loss 0.7936 (0.7661)	Acc@1 84.116 (85.406)	Acc@5 98.195 (99.390)
Max memory in training epoch: 67.3722368
Drin!!
old memory: 646893056
new memory: 673722368
Faktor: 1.0414741072750053
New batch Size kleiner 288!!
lr: 0.10674220265051004
1
Epoche:44/45; Lr: 0.10674220265051004
batch Size 288
Epoch: [44][0/181]	Time 0.217 (0.217)	Data 0.453 (0.453)	Loss 0.7935 (0.7935)	Acc@1 84.838 (84.838)	Acc@5 98.917 (98.917)
Epoch: [44][64/181]	Time 0.174 (0.165)	Data 0.000 (0.007)	Loss 0.8276 (0.7719)	Acc@1 85.560 (85.137)	Acc@5 98.917 (99.395)
Epoch: [44][128/181]	Time 0.164 (0.164)	Data 0.000 (0.004)	Loss 0.7869 (0.7753)	Acc@1 85.199 (85.151)	Acc@5 98.917 (99.407)
Max memory in training epoch: 67.3722368
lr: 0.10674220265051004
1
Epoche:45/45; Lr: 0.10674220265051004
batch Size 288
Epoch: [45][0/181]	Time 0.245 (0.245)	Data 0.417 (0.417)	Loss 0.7488 (0.7488)	Acc@1 85.921 (85.921)	Acc@5 99.639 (99.639)
Epoch: [45][64/181]	Time 0.148 (0.163)	Data 0.000 (0.007)	Loss 0.7938 (0.7768)	Acc@1 84.838 (85.054)	Acc@5 99.278 (99.328)
Epoch: [45][128/181]	Time 0.177 (0.162)	Data 0.000 (0.003)	Loss 0.8015 (0.7746)	Acc@1 83.394 (85.179)	Acc@5 99.278 (99.359)
Max memory in training epoch: 67.3722368
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 373350 ; 390382 ; 0.9563709392338786
[INFO] Storing checkpoint...
  76.24
Max memory: 94.8367872
 29.830s  j: 46 bis 50
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4487
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 46
Max memory: 0.1573376
lr: 0.1200849779818238
1
Epoche:46/50; Lr: 0.1200849779818238
batch Size 288
Epoch: [46][0/174]	Time 0.320 (0.320)	Data 0.413 (0.413)	Loss 0.7285 (0.7285)	Acc@1 87.153 (87.153)	Acc@5 99.306 (99.306)
Epoch: [46][64/174]	Time 0.167 (0.161)	Data 0.000 (0.007)	Loss 0.8080 (0.7511)	Acc@1 86.111 (85.801)	Acc@5 99.653 (99.567)
Epoch: [46][128/174]	Time 0.169 (0.159)	Data 0.000 (0.003)	Loss 0.9470 (0.7846)	Acc@1 78.819 (84.833)	Acc@5 98.611 (99.381)
Max memory in training epoch: 67.5312128
lr: 0.1200849779818238
1
Epoche:47/50; Lr: 0.1200849779818238
batch Size 288
Epoch: [47][0/174]	Time 0.223 (0.223)	Data 0.375 (0.375)	Loss 0.8174 (0.8174)	Acc@1 84.722 (84.722)	Acc@5 99.306 (99.306)
Epoch: [47][64/174]	Time 0.164 (0.157)	Data 0.000 (0.006)	Loss 0.8013 (0.7779)	Acc@1 85.069 (85.000)	Acc@5 99.653 (99.396)
Epoch: [47][128/174]	Time 0.178 (0.158)	Data 0.000 (0.003)	Loss 0.7932 (0.7922)	Acc@1 83.333 (84.598)	Acc@5 99.306 (99.373)
Max memory in training epoch: 67.9277056
lr: 0.1200849779818238
1
Epoche:48/50; Lr: 0.1200849779818238
batch Size 288
Epoch: [48][0/174]	Time 0.241 (0.241)	Data 0.422 (0.422)	Loss 0.7806 (0.7806)	Acc@1 84.722 (84.722)	Acc@5 99.653 (99.653)
Epoch: [48][64/174]	Time 0.172 (0.165)	Data 0.000 (0.007)	Loss 0.7350 (0.7580)	Acc@1 88.542 (85.721)	Acc@5 100.000 (99.519)
Epoch: [48][128/174]	Time 0.171 (0.163)	Data 0.000 (0.003)	Loss 0.7346 (0.7674)	Acc@1 87.847 (85.409)	Acc@5 99.653 (99.478)
Max memory in training epoch: 67.9277056
Drin!!
old memory: 673722368
new memory: 679277056
Faktor: 1.0082447730160564
New batch Size kleiner 290!!
lr: 0.1200849779818238
1
Epoche:49/50; Lr: 0.1200849779818238
batch Size 290
Epoch: [49][0/174]	Time 0.253 (0.253)	Data 0.410 (0.410)	Loss 0.8432 (0.8432)	Acc@1 85.417 (85.417)	Acc@5 99.653 (99.653)
Epoch: [49][64/174]	Time 0.163 (0.162)	Data 0.000 (0.007)	Loss 0.7645 (0.7816)	Acc@1 86.806 (85.272)	Acc@5 99.306 (99.354)
Epoch: [49][128/174]	Time 0.146 (0.162)	Data 0.000 (0.004)	Loss 0.7387 (0.7844)	Acc@1 85.764 (84.989)	Acc@5 99.653 (99.324)
Max memory in training epoch: 67.9277056
lr: 0.1200849779818238
1
Epoche:50/50; Lr: 0.1200849779818238
batch Size 290
Epoch: [50][0/174]	Time 0.224 (0.224)	Data 0.411 (0.411)	Loss 0.7246 (0.7246)	Acc@1 87.847 (87.847)	Acc@5 99.653 (99.653)
Epoch: [50][64/174]	Time 0.167 (0.167)	Data 0.000 (0.007)	Loss 0.8171 (0.7823)	Acc@1 85.069 (85.171)	Acc@5 98.611 (99.284)
Epoch: [50][128/174]	Time 0.173 (0.164)	Data 0.000 (0.003)	Loss 0.7518 (0.7811)	Acc@1 86.806 (85.207)	Acc@5 99.653 (99.322)
Max memory in training epoch: 67.9277056
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv21.weight

 RM:  module.conv22.weight
numoFStages: 3
Count: 365194 ; 373350 ; 0.9781545466720236
[INFO] Storing checkpoint...
  78.16
Max memory: 93.22624
 28.912s  j: 51 bis 55
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9345
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 51
Max memory: 0.1537024
lr: 0.13603376412003476
1
Epoche:51/55; Lr: 0.13603376412003476
batch Size 290
Epoch: [51][0/173]	Time 0.266 (0.266)	Data 0.515 (0.515)	Loss 0.7594 (0.7594)	Acc@1 86.552 (86.552)	Acc@5 99.310 (99.310)
Epoch: [51][64/173]	Time 0.161 (0.155)	Data 0.000 (0.008)	Loss 0.8463 (0.7917)	Acc@1 83.448 (84.748)	Acc@5 98.966 (99.294)
Epoch: [51][128/173]	Time 0.167 (0.155)	Data 0.000 (0.004)	Loss 0.7945 (0.8060)	Acc@1 86.897 (84.277)	Acc@5 99.310 (99.260)
Max memory in training epoch: 64.7316992
lr: 0.13603376412003476
1
Epoche:52/55; Lr: 0.13603376412003476
batch Size 290
Epoch: [52][0/173]	Time 0.247 (0.247)	Data 0.405 (0.405)	Loss 0.7694 (0.7694)	Acc@1 85.172 (85.172)	Acc@5 98.966 (98.966)
Epoch: [52][64/173]	Time 0.156 (0.156)	Data 0.000 (0.006)	Loss 0.8693 (0.8089)	Acc@1 84.483 (84.345)	Acc@5 98.966 (99.273)
Epoch: [52][128/173]	Time 0.168 (0.155)	Data 0.000 (0.003)	Loss 0.7245 (0.8064)	Acc@1 87.931 (84.440)	Acc@5 99.310 (99.334)
Max memory in training epoch: 65.059584
lr: 0.13603376412003476
1
Epoche:53/55; Lr: 0.13603376412003476
batch Size 290
Epoch: [53][0/173]	Time 0.228 (0.228)	Data 0.442 (0.442)	Loss 0.7557 (0.7557)	Acc@1 84.828 (84.828)	Acc@5 98.966 (98.966)
Epoch: [53][64/173]	Time 0.154 (0.155)	Data 0.000 (0.007)	Loss 0.7782 (0.7803)	Acc@1 84.138 (85.438)	Acc@5 98.621 (99.406)
Epoch: [53][128/173]	Time 0.151 (0.155)	Data 0.000 (0.004)	Loss 0.7956 (0.7908)	Acc@1 85.517 (84.972)	Acc@5 100.000 (99.342)
Max memory in training epoch: 65.048576
Drin!!
old memory: 679277056
new memory: 650485760
Faktor: 0.9576147968701596
New batch Size größer 298!!
lr: 0.13603376412003476
1
Epoche:54/55; Lr: 0.13603376412003476
batch Size 298
Epoch: [54][0/173]	Time 0.222 (0.222)	Data 0.397 (0.397)	Loss 0.7750 (0.7750)	Acc@1 86.207 (86.207)	Acc@5 98.621 (98.621)
Epoch: [54][64/173]	Time 0.134 (0.161)	Data 0.000 (0.006)	Loss 0.8555 (0.8040)	Acc@1 82.414 (84.446)	Acc@5 98.966 (99.279)
Epoch: [54][128/173]	Time 0.160 (0.158)	Data 0.000 (0.003)	Loss 0.7420 (0.7994)	Acc@1 84.138 (84.592)	Acc@5 99.655 (99.329)
Max memory in training epoch: 65.048576
lr: 0.13603376412003476
1
Epoche:55/55; Lr: 0.13603376412003476
batch Size 298
Epoch: [55][0/173]	Time 0.194 (0.194)	Data 0.564 (0.564)	Loss 0.7925 (0.7925)	Acc@1 86.897 (86.897)	Acc@5 99.310 (99.310)
Epoch: [55][64/173]	Time 0.165 (0.157)	Data 0.000 (0.009)	Loss 0.8527 (0.8002)	Acc@1 83.793 (84.727)	Acc@5 98.276 (99.294)
Epoch: [55][128/173]	Time 0.199 (0.158)	Data 0.000 (0.005)	Loss 0.7865 (0.7993)	Acc@1 82.759 (84.587)	Acc@5 99.310 (99.260)
Max memory in training epoch: 65.048576
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 352492 ; 365194 ; 0.9652184866125949
[INFO] Storing checkpoint...
  77.71
Max memory: 89.07776
 27.788s  j: 56 bis 60
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 186
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 56
Max memory: 0.1485824
lr: 0.15835180354597797
1
Epoche:56/60; Lr: 0.15835180354597797
batch Size 298
Epoch: [56][0/168]	Time 0.278 (0.278)	Data 0.425 (0.425)	Loss 0.8294 (0.8294)	Acc@1 86.577 (86.577)	Acc@5 98.993 (98.993)
Epoch: [56][64/168]	Time 0.157 (0.158)	Data 0.000 (0.007)	Loss 0.8786 (0.7906)	Acc@1 83.893 (84.765)	Acc@5 98.658 (99.288)
Epoch: [56][128/168]	Time 0.135 (0.155)	Data 0.000 (0.003)	Loss 0.8245 (0.8161)	Acc@1 82.550 (83.929)	Acc@5 100.000 (99.235)
Max memory in training epoch: 65.9990016
lr: 0.15835180354597797
1
Epoche:57/60; Lr: 0.15835180354597797
batch Size 298
Epoch: [57][0/168]	Time 0.186 (0.186)	Data 0.468 (0.468)	Loss 0.8212 (0.8212)	Acc@1 84.564 (84.564)	Acc@5 98.322 (98.322)
Epoch: [57][64/168]	Time 0.151 (0.155)	Data 0.000 (0.007)	Loss 0.8610 (0.8219)	Acc@1 83.557 (83.707)	Acc@5 99.664 (99.344)
Epoch: [57][128/168]	Time 0.142 (0.154)	Data 0.000 (0.004)	Loss 0.8435 (0.8248)	Acc@1 84.228 (83.716)	Acc@5 99.329 (99.311)
Max memory in training epoch: 66.0420096
lr: 0.15835180354597797
1
Epoche:58/60; Lr: 0.15835180354597797
batch Size 298
Epoch: [58][0/168]	Time 0.210 (0.210)	Data 0.431 (0.431)	Loss 0.8745 (0.8745)	Acc@1 82.550 (82.550)	Acc@5 98.658 (98.658)
Epoch: [58][64/168]	Time 0.167 (0.155)	Data 0.000 (0.007)	Loss 0.9372 (0.8314)	Acc@1 77.852 (83.351)	Acc@5 99.329 (99.350)
Epoch: [58][128/168]	Time 0.150 (0.155)	Data 0.000 (0.004)	Loss 0.9012 (0.8297)	Acc@1 81.879 (83.713)	Acc@5 99.329 (99.264)
Max memory in training epoch: 66.0420096
Drin!!
old memory: 650485760
new memory: 660420096
Faktor: 1.0152721805931617
New batch Size kleiner 302!!
lr: 0.15835180354597797
1
Epoche:59/60; Lr: 0.15835180354597797
batch Size 302
Epoch: [59][0/168]	Time 0.233 (0.233)	Data 0.435 (0.435)	Loss 0.8463 (0.8463)	Acc@1 82.215 (82.215)	Acc@5 100.000 (100.000)
Epoch: [59][64/168]	Time 0.149 (0.155)	Data 0.000 (0.007)	Loss 0.8658 (0.8164)	Acc@1 82.550 (84.311)	Acc@5 98.658 (99.267)
Epoch: [59][128/168]	Time 0.160 (0.158)	Data 0.000 (0.004)	Loss 0.8172 (0.8223)	Acc@1 83.893 (84.109)	Acc@5 99.329 (99.209)
Max memory in training epoch: 66.0420096
lr: 0.15835180354597797
1
Epoche:60/60; Lr: 0.15835180354597797
batch Size 302
Epoch: [60][0/168]	Time 0.211 (0.211)	Data 0.406 (0.406)	Loss 0.7996 (0.7996)	Acc@1 84.899 (84.899)	Acc@5 97.987 (97.987)
Epoch: [60][64/168]	Time 0.133 (0.155)	Data 0.000 (0.006)	Loss 0.7646 (0.8056)	Acc@1 84.899 (84.440)	Acc@5 99.329 (99.355)
Epoch: [60][128/168]	Time 0.145 (0.154)	Data 0.000 (0.003)	Loss 0.9156 (0.8153)	Acc@1 79.530 (83.984)	Acc@5 96.980 (99.269)
Max memory in training epoch: 66.0420096
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 339496 ; 352492 ; 0.9631310781521283
[INFO] Storing checkpoint...
  75.28
Max memory: 87.81824
 26.370s  j: 61 bis 65
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1850
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 61
Max memory: 0.1436672
lr: 0.18680564324564589
1
Epoche:61/65; Lr: 0.18680564324564589
batch Size 302
Epoch: [61][0/166]	Time 0.275 (0.275)	Data 0.415 (0.415)	Loss 0.7479 (0.7479)	Acc@1 87.417 (87.417)	Acc@5 100.000 (100.000)
Epoch: [61][64/166]	Time 0.146 (0.151)	Data 0.000 (0.007)	Loss 0.8886 (0.8090)	Acc@1 80.464 (84.330)	Acc@5 99.338 (99.287)
Epoch: [61][128/166]	Time 0.160 (0.148)	Data 0.000 (0.003)	Loss 0.9129 (0.8449)	Acc@1 79.801 (83.236)	Acc@5 99.669 (99.199)
Max memory in training epoch: 65.2355584
lr: 0.18680564324564589
1
Epoche:62/65; Lr: 0.18680564324564589
batch Size 302
Epoch: [62][0/166]	Time 0.174 (0.174)	Data 0.463 (0.463)	Loss 0.7840 (0.7840)	Acc@1 85.099 (85.099)	Acc@5 100.000 (100.000)
Epoch: [62][64/166]	Time 0.160 (0.152)	Data 0.000 (0.007)	Loss 0.7249 (0.8526)	Acc@1 86.755 (82.919)	Acc@5 99.669 (99.093)
Epoch: [62][128/166]	Time 0.156 (0.153)	Data 0.000 (0.004)	Loss 0.9138 (0.8504)	Acc@1 81.788 (83.208)	Acc@5 98.013 (99.156)
Max memory in training epoch: 65.2626432
lr: 0.18680564324564589
1
Epoche:63/65; Lr: 0.18680564324564589
batch Size 302
Epoch: [63][0/166]	Time 0.211 (0.211)	Data 0.490 (0.490)	Loss 0.9112 (0.9112)	Acc@1 80.464 (80.464)	Acc@5 99.338 (99.338)
Epoch: [63][64/166]	Time 0.136 (0.158)	Data 0.000 (0.008)	Loss 0.9239 (0.8403)	Acc@1 80.795 (83.377)	Acc@5 99.007 (99.205)
Epoch: [63][128/166]	Time 0.169 (0.155)	Data 0.000 (0.004)	Loss 0.7961 (0.8483)	Acc@1 84.106 (83.177)	Acc@5 99.007 (99.166)
Max memory in training epoch: 65.2626432
Drin!!
old memory: 660420096
new memory: 652626432
Faktor: 0.9881989296703655
New batch Size größer 310!!
lr: 0.18680564324564589
1
Epoche:64/65; Lr: 0.18680564324564589
batch Size 310
Epoch: [64][0/166]	Time 0.238 (0.238)	Data 0.436 (0.436)	Loss 0.8501 (0.8501)	Acc@1 82.119 (82.119)	Acc@5 100.000 (100.000)
Epoch: [64][64/166]	Time 0.158 (0.158)	Data 0.000 (0.007)	Loss 0.8528 (0.8360)	Acc@1 82.450 (83.551)	Acc@5 99.007 (99.256)
Epoch: [64][128/166]	Time 0.156 (0.156)	Data 0.000 (0.004)	Loss 0.8736 (0.8365)	Acc@1 82.450 (83.477)	Acc@5 99.669 (99.233)
Max memory in training epoch: 65.2626432
lr: 0.18680564324564589
1
Epoche:65/65; Lr: 0.18680564324564589
batch Size 310
Epoch: [65][0/166]	Time 0.246 (0.246)	Data 0.364 (0.364)	Loss 0.8559 (0.8559)	Acc@1 80.132 (80.132)	Acc@5 100.000 (100.000)
Epoch: [65][64/166]	Time 0.137 (0.156)	Data 0.000 (0.006)	Loss 0.8726 (0.8385)	Acc@1 80.464 (83.535)	Acc@5 99.007 (99.272)
Epoch: [65][128/166]	Time 0.166 (0.152)	Data 0.000 (0.003)	Loss 0.7557 (0.8387)	Acc@1 87.086 (83.380)	Acc@5 99.338 (99.261)
Max memory in training epoch: 65.2626432
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 325924 ; 339496 ; 0.9600230930555883
[INFO] Storing checkpoint...
  73.52
Max memory: 85.7176064
 25.879s  j: 66 bis 70
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7164
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 66
Max memory: 0.1380864
lr: 0.22620995861777432
1
Epoche:66/70; Lr: 0.22620995861777432
batch Size 310
Epoch: [66][0/162]	Time 0.248 (0.248)	Data 0.363 (0.363)	Loss 0.8187 (0.8187)	Acc@1 83.548 (83.548)	Acc@5 99.355 (99.355)
Epoch: [66][64/162]	Time 0.160 (0.157)	Data 0.000 (0.006)	Loss 0.8900 (0.8675)	Acc@1 82.258 (82.427)	Acc@5 99.032 (99.062)
Epoch: [66][128/162]	Time 0.126 (0.154)	Data 0.000 (0.003)	Loss 0.9043 (0.8836)	Acc@1 80.968 (82.018)	Acc@5 99.032 (99.035)
Max memory in training epoch: 67.1017472
lr: 0.22620995861777432
1
Epoche:67/70; Lr: 0.22620995861777432
batch Size 310
Epoch: [67][0/162]	Time 0.245 (0.245)	Data 0.388 (0.388)	Loss 0.8485 (0.8485)	Acc@1 84.194 (84.194)	Acc@5 98.710 (98.710)
Epoch: [67][64/162]	Time 0.169 (0.155)	Data 0.000 (0.006)	Loss 0.8741 (0.8853)	Acc@1 83.226 (82.387)	Acc@5 99.677 (98.983)
Epoch: [67][128/162]	Time 0.141 (0.154)	Data 0.000 (0.003)	Loss 0.9888 (0.8858)	Acc@1 77.097 (82.161)	Acc@5 99.355 (99.042)
Max memory in training epoch: 67.0716416
lr: 0.22620995861777432
1
Epoche:68/70; Lr: 0.22620995861777432
batch Size 310
Epoch: [68][0/162]	Time 0.202 (0.202)	Data 0.523 (0.523)	Loss 0.8332 (0.8332)	Acc@1 83.548 (83.548)	Acc@5 99.677 (99.677)
Epoch: [68][64/162]	Time 0.132 (0.155)	Data 0.000 (0.008)	Loss 0.8151 (0.8718)	Acc@1 84.194 (82.556)	Acc@5 99.677 (99.146)
Epoch: [68][128/162]	Time 0.139 (0.152)	Data 0.000 (0.004)	Loss 0.8774 (0.8800)	Acc@1 83.548 (82.306)	Acc@5 98.710 (99.127)
Max memory in training epoch: 67.0716416
Drin!!
old memory: 652626432
new memory: 670716416
Faktor: 1.027718742473489
New batch Size kleiner 318!!
lr: 0.22620995861777432
1
Epoche:69/70; Lr: 0.22620995861777432
batch Size 318
Epoch: [69][0/162]	Time 0.202 (0.202)	Data 0.509 (0.509)	Loss 0.9866 (0.9866)	Acc@1 79.677 (79.677)	Acc@5 98.387 (98.387)
Epoch: [69][64/162]	Time 0.167 (0.154)	Data 0.000 (0.008)	Loss 0.9066 (0.8614)	Acc@1 79.677 (82.968)	Acc@5 98.387 (99.067)
Epoch: [69][128/162]	Time 0.162 (0.153)	Data 0.000 (0.004)	Loss 0.8636 (0.8678)	Acc@1 80.645 (82.633)	Acc@5 99.032 (99.097)
Max memory in training epoch: 67.0716416
lr: 0.22620995861777432
1
Epoche:70/70; Lr: 0.22620995861777432
batch Size 318
Epoch: [70][0/162]	Time 0.205 (0.205)	Data 0.449 (0.449)	Loss 0.9467 (0.9467)	Acc@1 80.000 (80.000)	Acc@5 99.677 (99.677)
Epoch: [70][64/162]	Time 0.169 (0.156)	Data 0.000 (0.007)	Loss 0.8551 (0.8823)	Acc@1 82.903 (82.099)	Acc@5 99.355 (99.017)
Epoch: [70][128/162]	Time 0.149 (0.155)	Data 0.000 (0.004)	Loss 0.9532 (0.8718)	Acc@1 80.968 (82.568)	Acc@5 98.710 (99.102)
Max memory in training epoch: 67.0716416
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 302106 ; 325924 ; 0.9269216136277169
[INFO] Storing checkpoint...
  67.24
Max memory: 84.2801664
 25.622s  j: 71 bis 75
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8288
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 71
Max memory: 0.128512
lr: 0.28099518297051657
1
Epoche:71/75; Lr: 0.28099518297051657
batch Size 318
Epoch: [71][0/158]	Time 0.304 (0.304)	Data 0.411 (0.411)	Loss 0.8358 (0.8358)	Acc@1 82.075 (82.075)	Acc@5 100.000 (100.000)
Epoch: [71][64/158]	Time 0.140 (0.158)	Data 0.000 (0.007)	Loss 0.9778 (0.8829)	Acc@1 78.616 (81.780)	Acc@5 98.742 (99.071)
Epoch: [71][128/158]	Time 0.144 (0.156)	Data 0.000 (0.003)	Loss 0.9802 (0.9184)	Acc@1 78.931 (80.905)	Acc@5 99.371 (98.959)
Max memory in training epoch: 65.3926912
lr: 0.28099518297051657
1
Epoche:72/75; Lr: 0.28099518297051657
batch Size 318
Epoch: [72][0/158]	Time 0.231 (0.231)	Data 0.367 (0.367)	Loss 0.9262 (0.9262)	Acc@1 80.189 (80.189)	Acc@5 99.686 (99.686)
Epoch: [72][64/158]	Time 0.145 (0.158)	Data 0.000 (0.006)	Loss 1.0221 (0.9346)	Acc@1 75.157 (80.658)	Acc@5 97.799 (98.849)
Epoch: [72][128/158]	Time 0.164 (0.156)	Data 0.000 (0.003)	Loss 0.9550 (0.9213)	Acc@1 79.560 (81.081)	Acc@5 99.057 (98.910)
Max memory in training epoch: 65.3822464
lr: 0.28099518297051657
1
Epoche:73/75; Lr: 0.28099518297051657
batch Size 318
Epoch: [73][0/158]	Time 0.244 (0.244)	Data 0.438 (0.438)	Loss 0.8743 (0.8743)	Acc@1 82.390 (82.390)	Acc@5 99.371 (99.371)
Epoch: [73][64/158]	Time 0.164 (0.149)	Data 0.000 (0.007)	Loss 0.9254 (0.9300)	Acc@1 79.245 (80.682)	Acc@5 99.371 (99.115)
Epoch: [73][128/158]	Time 0.162 (0.152)	Data 0.000 (0.004)	Loss 0.8886 (0.9259)	Acc@1 81.447 (80.766)	Acc@5 98.742 (99.018)
Max memory in training epoch: 65.3822464
Drin!!
old memory: 670716416
new memory: 653822464
Faktor: 0.9748120791485145
New batch Size größer 325!!
lr: 0.28099518297051657
1
Epoche:74/75; Lr: 0.28099518297051657
batch Size 325
Epoch: [74][0/158]	Time 0.191 (0.191)	Data 0.432 (0.432)	Loss 0.9216 (0.9216)	Acc@1 80.189 (80.189)	Acc@5 99.686 (99.686)
Epoch: [74][64/158]	Time 0.149 (0.156)	Data 0.000 (0.007)	Loss 0.9404 (0.9385)	Acc@1 79.245 (80.324)	Acc@5 98.742 (98.911)
Epoch: [74][128/158]	Time 0.145 (0.155)	Data 0.000 (0.004)	Loss 0.9531 (0.9309)	Acc@1 79.874 (80.593)	Acc@5 98.113 (98.876)
Max memory in training epoch: 65.3822464
lr: 0.28099518297051657
1
Epoche:75/75; Lr: 0.28099518297051657
batch Size 325
Epoch: [75][0/158]	Time 0.228 (0.228)	Data 0.390 (0.390)	Loss 0.9059 (0.9059)	Acc@1 78.931 (78.931)	Acc@5 98.428 (98.428)
Epoch: [75][64/158]	Time 0.172 (0.161)	Data 0.000 (0.006)	Loss 0.7847 (0.9178)	Acc@1 83.333 (80.518)	Acc@5 99.371 (98.940)
Epoch: [75][128/158]	Time 0.146 (0.160)	Data 0.000 (0.003)	Loss 0.9040 (0.9153)	Acc@1 78.302 (80.810)	Acc@5 99.686 (98.879)
Max memory in training epoch: 65.3822464
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv19.weight

 RM:  module.conv20.weight
numoFStages: 3
Count: 283560 ; 302106 ; 0.9386109511231158
[INFO] Storing checkpoint...
  70.8
Max memory: 81.5536128
 25.582s  j: 76 bis 80
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 596
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 76
Max memory: 0.1204736
lr: 0.35673216588053863
1
Epoche:76/80; Lr: 0.35673216588053863
batch Size 325
Epoch: [76][0/154]	Time 0.252 (0.252)	Data 0.446 (0.446)	Loss 0.9330 (0.9330)	Acc@1 79.692 (79.692)	Acc@5 99.692 (99.692)
Epoch: [76][64/154]	Time 0.146 (0.150)	Data 0.000 (0.007)	Loss 0.9275 (0.9436)	Acc@1 80.615 (79.849)	Acc@5 98.154 (98.802)
Epoch: [76][128/154]	Time 0.164 (0.148)	Data 0.000 (0.004)	Loss 0.9634 (0.9607)	Acc@1 79.692 (79.585)	Acc@5 98.462 (98.738)
Max memory in training epoch: 63.0740992
lr: 0.35673216588053863
1
Epoche:77/80; Lr: 0.35673216588053863
batch Size 325
Epoch: [77][0/154]	Time 0.197 (0.197)	Data 0.482 (0.482)	Loss 0.8990 (0.8990)	Acc@1 82.462 (82.462)	Acc@5 99.077 (99.077)
Epoch: [77][64/154]	Time 0.120 (0.142)	Data 0.000 (0.008)	Loss 0.9201 (0.9689)	Acc@1 79.692 (79.394)	Acc@5 98.769 (98.731)
Epoch: [77][128/154]	Time 0.142 (0.145)	Data 0.000 (0.004)	Loss 0.9144 (0.9669)	Acc@1 81.538 (79.335)	Acc@5 99.077 (98.750)
Max memory in training epoch: 63.0740992
lr: 0.35673216588053863
1
Epoche:78/80; Lr: 0.35673216588053863
batch Size 325
Epoch: [78][0/154]	Time 0.212 (0.212)	Data 0.481 (0.481)	Loss 0.8828 (0.8828)	Acc@1 80.308 (80.308)	Acc@5 99.077 (99.077)
Epoch: [78][64/154]	Time 0.139 (0.149)	Data 0.000 (0.008)	Loss 0.9433 (0.9544)	Acc@1 79.385 (79.782)	Acc@5 97.846 (98.717)
Epoch: [78][128/154]	Time 0.157 (0.151)	Data 0.000 (0.004)	Loss 0.9454 (0.9601)	Acc@1 80.000 (79.664)	Acc@5 99.385 (98.753)
Max memory in training epoch: 63.0740992
Drin!!
old memory: 653822464
new memory: 630740992
Faktor: 0.9646976461181976
New batch Size größer 345!!
lr: 0.35673216588053863
1
Epoche:79/80; Lr: 0.35673216588053863
batch Size 345
Epoch: [79][0/154]	Time 0.186 (0.186)	Data 0.390 (0.390)	Loss 0.9470 (0.9470)	Acc@1 80.308 (80.308)	Acc@5 97.846 (97.846)
Epoch: [79][64/154]	Time 0.160 (0.149)	Data 0.000 (0.006)	Loss 1.0225 (0.9498)	Acc@1 77.538 (79.479)	Acc@5 97.231 (98.883)
Epoch: [79][128/154]	Time 0.151 (0.148)	Data 0.000 (0.003)	Loss 0.9554 (0.9604)	Acc@1 80.615 (79.287)	Acc@5 98.769 (98.776)
Max memory in training epoch: 63.0740992
lr: 0.35673216588053863
1
Epoche:80/80; Lr: 0.35673216588053863
batch Size 345
Epoch: [80][0/154]	Time 0.200 (0.200)	Data 0.422 (0.422)	Loss 0.9580 (0.9580)	Acc@1 81.538 (81.538)	Acc@5 97.846 (97.846)
Epoch: [80][64/154]	Time 0.157 (0.149)	Data 0.000 (0.007)	Loss 0.9351 (0.9371)	Acc@1 79.077 (79.991)	Acc@5 98.462 (98.826)
Epoch: [80][128/154]	Time 0.126 (0.148)	Data 0.000 (0.003)	Loss 0.9425 (0.9446)	Acc@1 81.231 (79.864)	Acc@5 97.846 (98.736)
Max memory in training epoch: 63.0740992
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 262048 ; 283560 ; 0.9241359853293836
[INFO] Storing checkpoint...
  53.61
Max memory: 76.9151488
 23.406s  j: 81 bis 85
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5915
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 81
Max memory: 0.1119744
lr: 0.4807523329249446
1
Epoche:81/85; Lr: 0.4807523329249446
batch Size 345
Epoch: [81][0/145]	Time 0.305 (0.305)	Data 0.506 (0.506)	Loss 0.9523 (0.9523)	Acc@1 79.420 (79.420)	Acc@5 99.130 (99.130)
Epoch: [81][64/145]	Time 0.155 (0.150)	Data 0.000 (0.008)	Loss 1.0721 (0.9890)	Acc@1 74.783 (78.667)	Acc@5 98.261 (98.604)
Epoch: [81][128/145]	Time 0.147 (0.148)	Data 0.000 (0.004)	Loss 0.9963 (1.0215)	Acc@1 79.420 (77.713)	Acc@5 97.681 (98.535)
Max memory in training epoch: 64.8822784
lr: 0.4807523329249446
1
Epoche:82/85; Lr: 0.4807523329249446
batch Size 345
Epoch: [82][0/145]	Time 0.247 (0.247)	Data 0.411 (0.411)	Loss 1.0454 (1.0454)	Acc@1 75.652 (75.652)	Acc@5 97.391 (97.391)
Epoch: [82][64/145]	Time 0.144 (0.149)	Data 0.000 (0.007)	Loss 0.9871 (1.0164)	Acc@1 77.101 (77.975)	Acc@5 98.261 (98.573)
Epoch: [82][128/145]	Time 0.140 (0.150)	Data 0.000 (0.003)	Loss 0.9363 (1.0172)	Acc@1 79.710 (77.865)	Acc@5 99.710 (98.573)
Max memory in training epoch: 64.732672
lr: 0.4807523329249446
1
Epoche:83/85; Lr: 0.4807523329249446
batch Size 345
Epoch: [83][0/145]	Time 0.301 (0.301)	Data 0.442 (0.442)	Loss 0.9367 (0.9367)	Acc@1 78.261 (78.261)	Acc@5 98.841 (98.841)
Epoch: [83][64/145]	Time 0.143 (0.155)	Data 0.000 (0.007)	Loss 0.9232 (1.0098)	Acc@1 80.580 (77.958)	Acc@5 98.841 (98.671)
Epoch: [83][128/145]	Time 0.160 (0.154)	Data 0.000 (0.004)	Loss 1.0130 (1.0213)	Acc@1 79.710 (77.492)	Acc@5 98.261 (98.580)
Max memory in training epoch: 64.732672
Drin!!
old memory: 630740992
new memory: 647326720
Faktor: 1.0262956240522894
New batch Size kleiner 354!!
lr: 0.4807523329249446
1
Epoche:84/85; Lr: 0.4807523329249446
batch Size 354
Epoch: [84][0/145]	Time 0.254 (0.254)	Data 0.418 (0.418)	Loss 1.0521 (1.0521)	Acc@1 76.522 (76.522)	Acc@5 98.261 (98.261)
Epoch: [84][64/145]	Time 0.132 (0.155)	Data 0.000 (0.007)	Loss 1.0407 (1.0174)	Acc@1 75.362 (77.503)	Acc@5 99.130 (98.631)
Epoch: [84][128/145]	Time 0.154 (0.154)	Data 0.000 (0.003)	Loss 0.9628 (1.0134)	Acc@1 79.130 (77.656)	Acc@5 97.971 (98.580)
Max memory in training epoch: 64.732672
lr: 0.4807523329249446
1
Epoche:85/85; Lr: 0.4807523329249446
batch Size 354
Epoch: [85][0/145]	Time 0.205 (0.205)	Data 0.482 (0.482)	Loss 0.9328 (0.9328)	Acc@1 80.000 (80.000)	Acc@5 98.841 (98.841)
Epoch: [85][64/145]	Time 0.127 (0.154)	Data 0.000 (0.008)	Loss 0.9394 (0.9946)	Acc@1 79.130 (78.038)	Acc@5 98.261 (98.631)
Epoch: [85][128/145]	Time 0.180 (0.152)	Data 0.000 (0.004)	Loss 1.0309 (1.0042)	Acc@1 78.551 (77.926)	Acc@5 98.551 (98.591)
Max memory in training epoch: 64.732672
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 245590 ; 262048 ; 0.9371947124190988
[INFO] Storing checkpoint...
  50.98
Max memory: 73.9528192
 22.616s  j: 86 bis 90
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4124
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 86
Max memory: 0.1055744
lr: 0.664790335372775
1
Epoche:86/90; Lr: 0.664790335372775
batch Size 354
Epoch: [86][0/142]	Time 0.245 (0.245)	Data 0.472 (0.472)	Loss 1.0938 (1.0938)	Acc@1 75.989 (75.989)	Acc@5 98.305 (98.305)
Epoch: [86][64/142]	Time 0.148 (0.155)	Data 0.000 (0.007)	Loss 1.1748 (1.0811)	Acc@1 72.316 (75.576)	Acc@5 98.305 (98.110)
Epoch: [86][128/142]	Time 0.141 (0.151)	Data 0.000 (0.004)	Loss 1.0931 (1.0932)	Acc@1 75.424 (75.400)	Acc@5 97.458 (98.176)
Max memory in training epoch: 64.3549184
lr: 0.664790335372775
1
Epoche:87/90; Lr: 0.664790335372775
batch Size 354
Epoch: [87][0/142]	Time 0.201 (0.201)	Data 0.444 (0.444)	Loss 1.2342 (1.2342)	Acc@1 75.141 (75.141)	Acc@5 96.893 (96.893)
Epoch: [87][64/142]	Time 0.138 (0.152)	Data 0.000 (0.007)	Loss 1.1072 (1.1477)	Acc@1 75.706 (73.885)	Acc@5 99.153 (98.096)
Epoch: [87][128/142]	Time 0.151 (0.152)	Data 0.000 (0.004)	Loss 1.0712 (1.1246)	Acc@1 75.989 (74.493)	Acc@5 98.305 (98.189)
Max memory in training epoch: 64.3872256
lr: 0.664790335372775
1
Epoche:88/90; Lr: 0.664790335372775
batch Size 354
Epoch: [88][0/142]	Time 0.248 (0.248)	Data 0.429 (0.429)	Loss 0.9877 (0.9877)	Acc@1 77.119 (77.119)	Acc@5 99.718 (99.718)
Epoch: [88][64/142]	Time 0.156 (0.157)	Data 0.000 (0.007)	Loss 1.1436 (1.1169)	Acc@1 73.164 (74.568)	Acc@5 99.153 (98.057)
Epoch: [88][128/142]	Time 0.160 (0.154)	Data 0.000 (0.004)	Loss 1.1423 (1.1039)	Acc@1 72.881 (74.968)	Acc@5 96.610 (98.099)
Max memory in training epoch: 64.3872256
Drin!!
old memory: 647326720
new memory: 643872256
Faktor: 0.9946634923396952
New batch Size größer 368!!
lr: 0.664790335372775
1
Epoche:89/90; Lr: 0.664790335372775
batch Size 368
Epoch: [89][0/142]	Time 0.212 (0.212)	Data 0.443 (0.443)	Loss 0.9442 (0.9442)	Acc@1 81.921 (81.921)	Acc@5 98.870 (98.870)
Epoch: [89][64/142]	Time 0.167 (0.153)	Data 0.000 (0.007)	Loss 0.9911 (1.1131)	Acc@1 79.379 (74.628)	Acc@5 98.588 (98.209)
Epoch: [89][128/142]	Time 0.165 (0.153)	Data 0.000 (0.004)	Loss 1.0996 (1.0957)	Acc@1 73.446 (74.986)	Acc@5 97.740 (98.200)
Max memory in training epoch: 64.3872256
lr: 0.664790335372775
1
Epoche:90/90; Lr: 0.664790335372775
batch Size 368
Epoch: [90][0/142]	Time 0.266 (0.266)	Data 0.475 (0.475)	Loss 1.0534 (1.0534)	Acc@1 75.424 (75.424)	Acc@5 98.023 (98.023)
Epoch: [90][64/142]	Time 0.144 (0.153)	Data 0.000 (0.008)	Loss 1.1664 (1.0954)	Acc@1 71.469 (74.898)	Acc@5 97.458 (98.201)
Epoch: [90][128/142]	Time 0.158 (0.151)	Data 0.000 (0.004)	Loss 1.0861 (1.0884)	Acc@1 74.011 (74.959)	Acc@5 98.305 (98.261)
Max memory in training epoch: 64.3872256
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 221916 ; 245590 ; 0.9036035669204772
[INFO] Storing checkpoint...
  21.36
Max memory: 71.4827776
 21.925s  j: 91 bis 95
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 699
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 91
Max memory: 0.0962048
lr: 0.955636107098364
1
Epoche:91/95; Lr: 0.955636107098364
batch Size 368
Epoch: [91][0/136]	Time 0.269 (0.269)	Data 0.534 (0.534)	Loss 1.2266 (1.2266)	Acc@1 69.565 (69.565)	Acc@5 98.370 (98.370)
Epoch: [91][64/136]	Time 0.141 (0.152)	Data 0.000 (0.009)	Loss 1.2685 (1.1827)	Acc@1 67.935 (71.564)	Acc@5 97.826 (97.880)
Epoch: [91][128/136]	Time 0.150 (0.152)	Data 0.000 (0.004)	Loss 1.2215 (1.1909)	Acc@1 69.022 (71.775)	Acc@5 98.641 (97.733)
Max memory in training epoch: 65.5177728
lr: 0.955636107098364
1
Epoche:92/95; Lr: 0.955636107098364
batch Size 368
Epoch: [92][0/136]	Time 0.262 (0.262)	Data 0.450 (0.450)	Loss 1.2111 (1.2111)	Acc@1 69.565 (69.565)	Acc@5 98.098 (98.098)
Epoch: [92][64/136]	Time 0.159 (0.151)	Data 0.000 (0.007)	Loss 1.1489 (1.2013)	Acc@1 70.652 (71.568)	Acc@5 98.098 (97.797)
Epoch: [92][128/136]	Time 0.159 (0.150)	Data 0.000 (0.004)	Loss 1.3175 (1.1860)	Acc@1 65.489 (72.024)	Acc@5 97.283 (97.875)
Max memory in training epoch: 65.5501312
lr: 0.955636107098364
1
Epoche:93/95; Lr: 0.0955636107098364
batch Size 368
Epoch: [93][0/136]	Time 0.234 (0.234)	Data 0.450 (0.450)	Loss 1.2477 (1.2477)	Acc@1 71.467 (71.467)	Acc@5 97.826 (97.826)
Epoch: [93][64/136]	Time 0.153 (0.155)	Data 0.000 (0.007)	Loss 0.9379 (0.9778)	Acc@1 80.435 (78.800)	Acc@5 97.283 (98.650)
Epoch: [93][128/136]	Time 0.127 (0.153)	Data 0.000 (0.004)	Loss 0.8069 (0.9183)	Acc@1 80.707 (80.555)	Acc@5 99.185 (98.848)
Max memory in training epoch: 65.5501312
Drin!!
old memory: 643872256
new memory: 655501312
Faktor: 1.0180611229815126
New batch Size kleiner 374!!
lr: 0.0955636107098364
1
Epoche:94/95; Lr: 0.0955636107098364
batch Size 374
Epoch: [94][0/136]	Time 0.219 (0.219)	Data 0.470 (0.470)	Loss 0.8526 (0.8526)	Acc@1 80.707 (80.707)	Acc@5 99.457 (99.457)
Epoch: [94][64/136]	Time 0.163 (0.154)	Data 0.000 (0.007)	Loss 0.7510 (0.7999)	Acc@1 83.696 (83.587)	Acc@5 99.457 (99.151)
Epoch: [94][128/136]	Time 0.135 (0.153)	Data 0.000 (0.004)	Loss 0.7369 (0.7875)	Acc@1 84.239 (83.809)	Acc@5 99.185 (99.183)
Max memory in training epoch: 65.5501312
lr: 0.0955636107098364
1
Epoche:95/95; Lr: 0.0955636107098364
batch Size 374
Epoch: [95][0/136]	Time 0.222 (0.222)	Data 0.408 (0.408)	Loss 0.6903 (0.6903)	Acc@1 86.685 (86.685)	Acc@5 98.641 (98.641)
Epoch: [95][64/136]	Time 0.158 (0.153)	Data 0.000 (0.007)	Loss 0.7805 (0.7288)	Acc@1 80.435 (84.996)	Acc@5 99.728 (99.373)
Epoch: [95][128/136]	Time 0.161 (0.153)	Data 0.000 (0.003)	Loss 0.6780 (0.7198)	Acc@1 87.500 (85.035)	Acc@5 99.185 (99.353)
Max memory in training epoch: 65.5501312
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv17.weight

 RM:  module.conv18.weight

 RM:  module.conv24.weight

 RM:  module.conv25.weight

 RM:  module.conv28.weight

 RM:  module.conv29.weight
numoFStages: 3
Count: 194351 ; 221916 ; 0.8757863335676562
[INFO] Storing checkpoint...
  82.85
Max memory: 69.480192
 21.274s  j: 96 bis 100
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2793
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 96
Max memory: 0.0835584
lr: 0.13961246252140164
1
Epoche:96/100; Lr: 0.13961246252140164
batch Size 374
Epoch: [96][0/134]	Time 0.268 (0.268)	Data 0.455 (0.455)	Loss 0.7410 (0.7410)	Acc@1 82.888 (82.888)	Acc@5 99.733 (99.733)
Epoch: [96][64/134]	Time 0.117 (0.134)	Data 0.000 (0.007)	Loss 0.6701 (0.7187)	Acc@1 86.898 (84.578)	Acc@5 98.930 (99.235)
Epoch: [96][128/134]	Time 0.133 (0.130)	Data 0.000 (0.004)	Loss 0.6942 (0.7148)	Acc@1 84.492 (84.337)	Acc@5 99.733 (99.312)
Max memory in training epoch: 59.182336
lr: 0.13961246252140164
1
Epoche:97/100; Lr: 0.13961246252140164
batch Size 374
Epoch: [97][0/134]	Time 0.173 (0.173)	Data 0.391 (0.391)	Loss 0.7467 (0.7467)	Acc@1 82.353 (82.353)	Acc@5 99.465 (99.465)
Epoch: [97][64/134]	Time 0.137 (0.130)	Data 0.000 (0.006)	Loss 0.7452 (0.7090)	Acc@1 83.690 (84.262)	Acc@5 98.663 (99.194)
Epoch: [97][128/134]	Time 0.123 (0.129)	Data 0.000 (0.003)	Loss 0.6472 (0.7037)	Acc@1 85.561 (84.266)	Acc@5 99.465 (99.221)
Max memory in training epoch: 59.2333312
lr: 0.13961246252140164
1
Epoche:98/100; Lr: 0.13961246252140164
batch Size 374
Epoch: [98][0/134]	Time 0.166 (0.166)	Data 0.444 (0.444)	Loss 0.7555 (0.7555)	Acc@1 80.749 (80.749)	Acc@5 98.663 (98.663)
Epoch: [98][64/134]	Time 0.120 (0.129)	Data 0.000 (0.007)	Loss 0.6470 (0.6758)	Acc@1 85.561 (84.714)	Acc@5 99.465 (99.325)
Epoch: [98][128/134]	Time 0.113 (0.130)	Data 0.000 (0.004)	Loss 0.6157 (0.6897)	Acc@1 88.503 (84.295)	Acc@5 99.465 (99.283)
Max memory in training epoch: 59.2333312
Drin!!
old memory: 655501312
new memory: 592333312
Faktor: 0.903634060155779
New batch Size größer 423!!
lr: 0.13961246252140164
1
Epoche:99/100; Lr: 0.13961246252140164
batch Size 423
Epoch: [99][0/134]	Time 0.160 (0.160)	Data 0.554 (0.554)	Loss 0.7022 (0.7022)	Acc@1 83.422 (83.422)	Acc@5 99.465 (99.465)
Epoch: [99][64/134]	Time 0.131 (0.133)	Data 0.000 (0.009)	Loss 0.6750 (0.6852)	Acc@1 82.888 (84.459)	Acc@5 99.198 (99.329)
Epoch: [99][128/134]	Time 0.132 (0.132)	Data 0.000 (0.005)	Loss 0.6802 (0.6871)	Acc@1 85.294 (84.299)	Acc@5 98.663 (99.347)
Max memory in training epoch: 59.2333312
lr: 0.13961246252140164
1
Epoche:100/100; Lr: 0.13961246252140164
batch Size 423
Epoch: [100][0/134]	Time 0.181 (0.181)	Data 0.541 (0.541)	Loss 0.6508 (0.6508)	Acc@1 85.027 (85.027)	Acc@5 99.733 (99.733)
Epoch: [100][64/134]	Time 0.136 (0.125)	Data 0.000 (0.009)	Loss 0.6453 (0.6809)	Acc@1 85.294 (84.467)	Acc@5 98.663 (99.362)
Epoch: [100][128/134]	Time 0.123 (0.128)	Data 0.000 (0.004)	Loss 0.7778 (0.6881)	Acc@1 81.016 (84.274)	Acc@5 99.465 (99.316)
Max memory in training epoch: 59.2333312
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 187978 ; 194351 ; 0.9672088129209523
[INFO] Storing checkpoint...
  80.11
Max memory: 62.852096
 17.762s  j: 101 bis 105
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2827
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 101
Max memory: 0.0812032
lr: 0.23068777986934724
1
Epoche:101/105; Lr: 0.23068777986934724
batch Size 423
Epoch: [101][0/119]	Time 0.290 (0.290)	Data 0.469 (0.469)	Loss 0.6781 (0.6781)	Acc@1 82.979 (82.979)	Acc@5 99.291 (99.291)
Epoch: [101][64/119]	Time 0.148 (0.137)	Data 0.000 (0.007)	Loss 0.7489 (0.7500)	Acc@1 82.270 (82.157)	Acc@5 99.054 (99.167)
Max memory in training epoch: 65.369088
lr: 0.23068777986934724
1
Epoche:102/105; Lr: 0.23068777986934724
batch Size 423
Epoch: [102][0/119]	Time 0.214 (0.214)	Data 0.444 (0.444)	Loss 0.7802 (0.7802)	Acc@1 82.033 (82.033)	Acc@5 99.527 (99.527)
Epoch: [102][64/119]	Time 0.149 (0.138)	Data 0.000 (0.007)	Loss 0.7401 (0.7784)	Acc@1 83.215 (82.022)	Acc@5 99.054 (99.076)
Max memory in training epoch: 65.369088
lr: 0.23068777986934724
1
Epoche:103/105; Lr: 0.23068777986934724
batch Size 423
Epoch: [103][0/119]	Time 0.223 (0.223)	Data 0.497 (0.497)	Loss 0.7612 (0.7612)	Acc@1 82.979 (82.979)	Acc@5 98.345 (98.345)
Epoch: [103][64/119]	Time 0.118 (0.138)	Data 0.000 (0.008)	Loss 0.8527 (0.7753)	Acc@1 80.851 (82.368)	Acc@5 98.582 (99.105)
Max memory in training epoch: 65.369088
Drin!!
old memory: 592333312
new memory: 653690880
Faktor: 1.1035862186997851
New batch Size kleiner 466!!
lr: 0.23068777986934724
1
Epoche:104/105; Lr: 0.23068777986934724
batch Size 466
Epoch: [104][0/119]	Time 0.249 (0.249)	Data 0.516 (0.516)	Loss 0.7582 (0.7582)	Acc@1 82.742 (82.742)	Acc@5 98.818 (98.818)
Epoch: [104][64/119]	Time 0.135 (0.142)	Data 0.000 (0.008)	Loss 0.7152 (0.7599)	Acc@1 84.634 (82.761)	Acc@5 99.527 (99.193)
Max memory in training epoch: 65.369088
lr: 0.23068777986934724
1
Epoche:105/105; Lr: 0.23068777986934724
batch Size 466
Epoch: [105][0/119]	Time 0.190 (0.190)	Data 0.533 (0.533)	Loss 0.7428 (0.7428)	Acc@1 82.506 (82.506)	Acc@5 99.291 (99.291)
Epoch: [105][64/119]	Time 0.137 (0.141)	Data 0.000 (0.008)	Loss 0.7721 (0.7680)	Acc@1 81.797 (82.459)	Acc@5 99.291 (99.087)
Max memory in training epoch: 65.369088
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 184604 ; 187978 ; 0.982051091085127
[INFO] Storing checkpoint...
  71.54
Max memory: 65.369088
 17.045s  j: 106 bis 110
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4459
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 106
Max memory: 0.079872
lr: 0.41992384929342114
1
Epoche:106/110; Lr: 0.41992384929342114
batch Size 466
Epoch: [106][0/108]	Time 0.331 (0.331)	Data 0.559 (0.559)	Loss 0.7339 (0.7339)	Acc@1 81.974 (81.974)	Acc@5 99.356 (99.356)
Epoch: [106][64/108]	Time 0.168 (0.143)	Data 0.000 (0.009)	Loss 0.9684 (0.8605)	Acc@1 77.039 (79.848)	Acc@5 98.283 (98.868)
Max memory in training epoch: 72.1971712
lr: 0.41992384929342114
1
Epoche:107/110; Lr: 0.41992384929342114
batch Size 466
Epoch: [107][0/108]	Time 0.194 (0.194)	Data 0.521 (0.521)	Loss 0.8900 (0.8900)	Acc@1 78.326 (78.326)	Acc@5 98.498 (98.498)
Epoch: [107][64/108]	Time 0.131 (0.145)	Data 0.000 (0.008)	Loss 0.8010 (0.8829)	Acc@1 81.116 (79.769)	Acc@5 99.356 (98.811)
Max memory in training epoch: 72.1971712
lr: 0.41992384929342114
1
Epoche:108/110; Lr: 0.41992384929342114
batch Size 466
Epoch: [108][0/108]	Time 0.201 (0.201)	Data 0.507 (0.507)	Loss 0.8659 (0.8659)	Acc@1 81.330 (81.330)	Acc@5 99.142 (99.142)
Epoch: [108][64/108]	Time 0.157 (0.147)	Data 0.000 (0.008)	Loss 0.8109 (0.8750)	Acc@1 84.120 (80.221)	Acc@5 98.498 (98.821)
Max memory in training epoch: 72.1971712
Drin!!
old memory: 653690880
new memory: 721971712
Faktor: 1.1044543133292604
New batch Size kleiner 514!!
lr: 0.41992384929342114
1
Epoche:109/110; Lr: 0.41992384929342114
batch Size 514
Epoch: [109][0/108]	Time 0.239 (0.239)	Data 0.460 (0.460)	Loss 0.9230 (0.9230)	Acc@1 77.682 (77.682)	Acc@5 98.927 (98.927)
Epoch: [109][64/108]	Time 0.127 (0.147)	Data 0.000 (0.007)	Loss 0.8724 (0.8576)	Acc@1 78.970 (80.584)	Acc@5 99.571 (98.924)
Max memory in training epoch: 72.1971712
lr: 0.41992384929342114
1
Epoche:110/110; Lr: 0.41992384929342114
batch Size 514
Epoch: [110][0/108]	Time 0.247 (0.247)	Data 0.477 (0.477)	Loss 0.8933 (0.8933)	Acc@1 79.185 (79.185)	Acc@5 99.142 (99.142)
Epoch: [110][64/108]	Time 0.150 (0.144)	Data 0.000 (0.008)	Loss 0.9267 (0.8835)	Acc@1 75.966 (79.432)	Acc@5 99.571 (98.937)
Max memory in training epoch: 72.1971712
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 180796 ; 184604 ; 0.9793720612771121
[INFO] Storing checkpoint...
  62.79
Max memory: 72.1971712
 16.008s  j: 111 bis 115
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5015
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 111
Max memory: 0.0782848
lr: 0.8431283536594472
1
Epoche:111/115; Lr: 0.8431283536594472
batch Size 514
Epoch: [111][0/98]	Time 0.250 (0.250)	Data 0.591 (0.591)	Loss 0.8088 (0.8088)	Acc@1 81.323 (81.323)	Acc@5 99.222 (99.222)
Epoch: [111][64/98]	Time 0.149 (0.144)	Data 0.000 (0.009)	Loss 1.0880 (1.0409)	Acc@1 75.875 (75.121)	Acc@5 97.471 (98.195)
Max memory in training epoch: 77.7918976
lr: 0.8431283536594472
1
Epoche:112/115; Lr: 0.8431283536594472
batch Size 514
Epoch: [112][0/98]	Time 0.200 (0.200)	Data 0.463 (0.463)	Loss 0.9027 (0.9027)	Acc@1 82.296 (82.296)	Acc@5 98.833 (98.833)
Epoch: [112][64/98]	Time 0.156 (0.151)	Data 0.000 (0.007)	Loss 1.0767 (1.0323)	Acc@1 73.930 (76.283)	Acc@5 97.860 (98.321)
Max memory in training epoch: 77.7918976
lr: 0.8431283536594472
1
Epoche:113/115; Lr: 0.8431283536594472
batch Size 514
Epoch: [113][0/98]	Time 0.214 (0.214)	Data 0.423 (0.423)	Loss 1.0503 (1.0503)	Acc@1 75.486 (75.486)	Acc@5 99.027 (99.027)
Epoch: [113][64/98]	Time 0.149 (0.152)	Data 0.000 (0.007)	Loss 0.9813 (1.0505)	Acc@1 76.848 (75.732)	Acc@5 98.833 (98.258)
Max memory in training epoch: 77.7918976
Drin!!
old memory: 721971712
new memory: 777918976
Faktor: 1.077492321472008
New batch Size kleiner 553!!
lr: 0.8431283536594472
1
Epoche:114/115; Lr: 0.8431283536594472
batch Size 553
Epoch: [114][0/98]	Time 0.189 (0.189)	Data 0.561 (0.561)	Loss 1.0262 (1.0262)	Acc@1 76.654 (76.654)	Acc@5 99.027 (99.027)
Epoch: [114][64/98]	Time 0.156 (0.150)	Data 0.000 (0.009)	Loss 0.9547 (1.0399)	Acc@1 79.572 (75.561)	Acc@5 98.054 (98.375)
Max memory in training epoch: 77.7918976
lr: 0.8431283536594472
1
Epoche:115/115; Lr: 0.8431283536594472
batch Size 553
Epoch: [115][0/98]	Time 0.178 (0.178)	Data 0.514 (0.514)	Loss 1.0412 (1.0412)	Acc@1 75.486 (75.486)	Acc@5 97.860 (97.860)
Epoch: [115][64/98]	Time 0.152 (0.150)	Data 0.000 (0.008)	Loss 1.0510 (1.0376)	Acc@1 76.459 (75.929)	Acc@5 97.860 (98.315)
Max memory in training epoch: 77.7918976
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 173490 ; 180796 ; 0.9595898139339366
[INFO] Storing checkpoint...
  41.27
Max memory: 77.7918976
 15.300s  j: 116 bis 120
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8782
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 116
Max memory: 0.0753664
lr: 1.8212889827096652
1
Epoche:116/120; Lr: 1.8212889827096652
batch Size 553
Epoch: [116][0/91]	Time 0.293 (0.293)	Data 0.529 (0.529)	Loss 1.0630 (1.0630)	Acc@1 73.418 (73.418)	Acc@5 98.192 (98.192)
Epoch: [116][64/91]	Time 0.135 (0.154)	Data 0.000 (0.008)	Loss 1.3523 (1.3282)	Acc@1 68.716 (67.795)	Acc@5 97.649 (96.834)
Max memory in training epoch: 83.1543296
lr: 1.8212889827096652
1
Epoche:117/120; Lr: 1.8212889827096652
batch Size 553
Epoch: [117][0/91]	Time 0.197 (0.197)	Data 0.550 (0.550)	Loss 1.2249 (1.2249)	Acc@1 70.163 (70.163)	Acc@5 98.373 (98.373)
Epoch: [117][64/91]	Time 0.158 (0.155)	Data 0.000 (0.009)	Loss 1.2902 (1.2819)	Acc@1 69.078 (68.811)	Acc@5 96.926 (97.137)
Max memory in training epoch: 83.1543296
lr: 1.8212889827096652
1
Epoche:118/120; Lr: 1.8212889827096652
batch Size 553
Epoch: [118][0/91]	Time 0.227 (0.227)	Data 0.633 (0.633)	Loss 1.3083 (1.3083)	Acc@1 67.812 (67.812)	Acc@5 97.649 (97.649)
Epoch: [118][64/91]	Time 0.130 (0.156)	Data 0.000 (0.010)	Loss 1.3651 (1.2664)	Acc@1 63.834 (68.752)	Acc@5 96.564 (97.243)
Max memory in training epoch: 83.1543296
Drin!!
old memory: 777918976
new memory: 831543296
Faktor: 1.0689330401422166
New batch Size kleiner 591!!
lr: 1.8212889827096652
1
Epoche:119/120; Lr: 1.8212889827096652
batch Size 591
Epoch: [119][0/91]	Time 0.257 (0.257)	Data 0.510 (0.510)	Loss 1.1705 (1.1705)	Acc@1 69.801 (69.801)	Acc@5 97.830 (97.830)
Epoch: [119][64/91]	Time 0.167 (0.157)	Data 0.000 (0.008)	Loss 1.2511 (1.2490)	Acc@1 67.631 (68.758)	Acc@5 97.830 (97.357)
Max memory in training epoch: 83.1543296
lr: 1.8212889827096652
1
Epoche:120/120; Lr: 1.8212889827096652
batch Size 591
Epoch: [120][0/91]	Time 0.256 (0.256)	Data 0.584 (0.584)	Loss 1.4691 (1.4691)	Acc@1 59.855 (59.855)	Acc@5 95.118 (95.118)
Epoch: [120][64/91]	Time 0.148 (0.163)	Data 0.000 (0.009)	Loss 1.3240 (1.2519)	Acc@1 65.823 (68.616)	Acc@5 96.383 (97.132)
Max memory in training epoch: 83.1543296
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 163698 ; 173490 ; 0.9435587065536919
[INFO] Storing checkpoint...
  34.87
Max memory: 83.1543296
 15.301s  j: 121 bis 125
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2478
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 121
Max memory: 0.071424
lr: 4.204616362427391
1
Epoche:121/125; Lr: 4.204616362427391
batch Size 591
Epoch: [121][0/85]	Time 0.299 (0.299)	Data 0.613 (0.613)	Loss 1.2411 (1.2411)	Acc@1 71.066 (71.066)	Acc@5 97.462 (97.462)
Epoch: [121][64/85]	Time 0.159 (0.158)	Data 0.000 (0.010)	Loss 2.3835 (2.4353)	Acc@1 21.658 (28.065)	Acc@5 76.650 (79.409)
Max memory in training epoch: 88.4325376
lr: 4.204616362427391
1
Epoche:122/125; Lr: 4.204616362427391
batch Size 591
Epoch: [122][0/85]	Time 0.279 (0.279)	Data 0.581 (0.581)	Loss 2.3746 (2.3746)	Acc@1 19.797 (19.797)	Acc@5 72.758 (72.758)
Epoch: [122][64/85]	Time 0.156 (0.160)	Data 0.000 (0.009)	Loss 2.3927 (2.6025)	Acc@1 15.398 (16.999)	Acc@5 73.096 (67.471)
Max memory in training epoch: 88.4325376
lr: 4.204616362427391
1
Epoche:123/125; Lr: 4.204616362427391
batch Size 591
Epoch: [123][0/85]	Time 0.210 (0.210)	Data 0.583 (0.583)	Loss 2.2624 (2.2624)	Acc@1 17.936 (17.936)	Acc@5 75.635 (75.635)
Epoch: [123][64/85]	Time 0.151 (0.160)	Data 0.000 (0.009)	Loss 2.0887 (2.2410)	Acc@1 23.012 (19.620)	Acc@5 78.003 (75.218)
Max memory in training epoch: 88.4325376
Drin!!
old memory: 831543296
new memory: 884325376
Faktor: 1.0634748428060203
New batch Size kleiner 628!!
lr: 4.204616362427391
1
Epoche:124/125; Lr: 4.204616362427391
batch Size 628
Epoch: [124][0/85]	Time 0.289 (0.289)	Data 0.536 (0.536)	Loss 2.1348 (2.1348)	Acc@1 19.797 (19.797)	Acc@5 79.019 (79.019)
Epoch: [124][64/85]	Time 0.160 (0.164)	Data 0.000 (0.009)	Loss 2.1608 (2.1936)	Acc@1 17.766 (19.610)	Acc@5 80.372 (74.570)
Max memory in training epoch: 88.4325376
lr: 4.204616362427391
1
Epoche:125/125; Lr: 4.204616362427391
batch Size 628
Epoch: [125][0/85]	Time 0.246 (0.246)	Data 0.481 (0.481)	Loss 2.1128 (2.1128)	Acc@1 24.196 (24.196)	Acc@5 80.034 (80.034)
Epoch: [125][64/85]	Time 0.155 (0.165)	Data 0.000 (0.008)	Loss 2.1673 (2.1808)	Acc@1 18.613 (18.969)	Acc@5 74.788 (73.482)
Max memory in training epoch: 88.4325376
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 158648 ; 163698 ; 0.9691505088638835
[INFO] Storing checkpoint...
  10.71
Max memory: 88.4325376
 14.390s  j: 126 bis 130
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1957
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 126
Max memory: 0.0694272
lr: 10.314449514079694
1
Epoche:126/130; Lr: 10.314449514079694
batch Size 628
Epoch: [126][0/80]	Time 0.335 (0.335)	Data 0.540 (0.540)	Loss 2.1178 (2.1178)	Acc@1 20.382 (20.382)	Acc@5 76.433 (76.433)
Epoch: [126][64/80]	Time 0.158 (0.161)	Data 0.000 (0.009)	Loss 6.4235 (8.0865)	Acc@1 9.713 (10.331)	Acc@5 48.726 (51.124)
Max memory in training epoch: 95.1438848
lr: 10.314449514079694
1
Epoche:127/130; Lr: 10.314449514079694
batch Size 628
Epoch: [127][0/80]	Time 0.253 (0.253)	Data 0.526 (0.526)	Loss 17509.4512 (17509.4512)	Acc@1 8.280 (8.280)	Acc@5 47.452 (47.452)
Epoch: [127][64/80]	Time 0.201 (0.165)	Data 0.000 (0.008)	Loss 770643008.0000 (32823069.7815)	Acc@1 13.376 (10.203)	Acc@5 54.140 (50.431)
Max memory in training epoch: 95.1438848
lr: 10.314449514079694
1
Epoche:128/130; Lr: 10.314449514079694
batch Size 628
Epoch: [128][0/80]	Time 0.252 (0.252)	Data 0.621 (0.621)	Loss 102927.3438 (102927.3438)	Acc@1 9.873 (9.873)	Acc@5 50.637 (50.637)
Epoch: [128][64/80]	Time 0.171 (0.167)	Data 0.000 (0.010)	Loss 54859.6172 (2791856.7707)	Acc@1 10.510 (10.086)	Acc@5 51.592 (50.294)
Max memory in training epoch: 95.1438848
Drin!!
old memory: 884325376
new memory: 951438848
Faktor: 1.0758922833398372
New batch Size kleiner 675!!
lr: 10.314449514079694
1
Epoche:129/130; Lr: 10.314449514079694
batch Size 675
Epoch: [129][0/80]	Time 0.273 (0.273)	Data 0.594 (0.594)	Loss 24053.8965 (24053.8965)	Acc@1 11.943 (11.943)	Acc@5 49.204 (49.204)
Epoch: [129][64/80]	Time 0.163 (0.167)	Data 0.000 (0.009)	Loss 7614.1060 (5835839.4908)	Acc@1 11.146 (9.584)	Acc@5 53.503 (49.809)
Max memory in training epoch: 95.1438848
lr: 10.314449514079694
1
Epoche:130/130; Lr: 10.314449514079694
batch Size 675
Epoch: [130][0/80]	Time 0.257 (0.257)	Data 0.551 (0.551)	Loss 3695600.5000 (3695600.5000)	Acc@1 7.803 (7.803)	Acc@5 50.000 (50.000)
Epoch: [130][64/80]	Time 0.177 (0.167)	Data 0.000 (0.009)	Loss 14633958.0000 (8049538.7870)	Acc@1 8.599 (10.512)	Acc@5 45.382 (50.781)
Max memory in training epoch: 95.1438848
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  9.93
Max memory: 95.1438848
 13.886s  j: 131 bis 135
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1386
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0694272
lr: 27.196302429702317
1
Epoche:131/135; Lr: 27.196302429702317
batch Size 675
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [131][0/75]	Time 0.257 (0.257)	Data 0.381 (0.381)	Loss 1736814.5000 (1736814.5000)	Acc@1 7.111 (7.111)	Acc@5 45.778 (45.778)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 136 bis 140
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7689
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0694272
lr: 27.196302429702317
1
Epoche:131/135; Lr: 27.196302429702317
batch Size 675
Epoch: [131][0/75]	Time 0.359 (0.359)	Data 0.557 (0.557)	Loss 1676511.7500 (1676511.7500)	Acc@1 9.630 (9.630)	Acc@5 51.111 (51.111)
Epoch: [131][64/75]	Time 0.153 (0.169)	Data 0.000 (0.009)	Loss 5306877440.0000 (13952820979558590.0000)	Acc@1 12.296 (10.081)	Acc@5 52.444 (49.689)
Max memory in training epoch: 101.4895616
lr: 27.196302429702317
1
Epoche:132/135; Lr: 27.196302429702317
batch Size 675
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [132][0/75]	Time 0.274 (0.274)	Data 0.560 (0.560)	Loss 880878138639253504.0000 (880878138639253504.0000)	Acc@1 10.815 (10.815)	Acc@5 53.778 (53.778)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 141 bis 145
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2029
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0694272
lr: 27.196302429702317
1
Epoche:131/135; Lr: 27.196302429702317
batch Size 675
Epoch: [131][0/75]	Time 0.365 (0.365)	Data 0.531 (0.531)	Loss 1705773.3750 (1705773.3750)	Acc@1 9.037 (9.037)	Acc@5 50.370 (50.370)
Epoch: [131][64/75]	Time 0.167 (0.169)	Data 0.000 (0.008)	Loss 269344302415914991616.0000 (306262498571635392512.0000)	Acc@1 12.000 (10.188)	Acc@5 50.815 (50.117)
Max memory in training epoch: 101.4895616
lr: 27.196302429702317
1
Epoche:132/135; Lr: 27.196302429702317
batch Size 675
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [132][0/75]	Time 0.207 (0.207)	Data 0.508 (0.508)	Loss 23950470472821374976.0000 (23950470472821374976.0000)	Acc@1 10.074 (10.074)	Acc@5 48.741 (48.741)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 146 bis 150
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 520
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0694272
lr: 27.196302429702317
1
Epoche:131/135; Lr: 27.196302429702317
batch Size 675
Epoch: [131][0/75]	Time 0.340 (0.340)	Data 0.597 (0.597)	Loss 1723529.3750 (1723529.3750)	Acc@1 10.815 (10.815)	Acc@5 50.519 (50.519)
Epoch: [131][64/75]	Time 0.164 (0.168)	Data 0.000 (0.009)	Loss 1528640128.0000 (29331772518173508.0000)	Acc@1 12.148 (10.202)	Acc@5 50.222 (50.578)
Max memory in training epoch: 101.4895616
lr: 27.196302429702317
1
Epoche:132/135; Lr: 27.196302429702317
batch Size 675
Epoch: [132][0/75]	Time 0.282 (0.282)	Data 0.543 (0.543)	Loss 56133528059904.0000 (56133528059904.0000)	Acc@1 11.852 (11.852)	Acc@5 52.444 (52.444)
Epoch: [132][64/75]	Time 0.163 (0.170)	Data 0.000 (0.009)	Loss 433836291733024269139968.0000 (6985441855633657692160.0000)	Acc@1 9.037 (10.238)	Acc@5 48.148 (50.044)
Max memory in training epoch: 101.4895616
lr: 27.196302429702317
1
Epoche:133/135; Lr: 27.196302429702317
batch Size 675
Epoch: [133][0/75]	Time 0.236 (0.236)	Data 0.577 (0.577)	Loss 192891174912.0000 (192891174912.0000)	Acc@1 9.926 (9.926)	Acc@5 53.481 (53.481)
Epoch: [133][64/75]	Time 0.153 (0.168)	Data 0.000 (0.009)	Loss 306821256445952.0000 (121248388046115801478954221568.0000)	Acc@1 8.741 (9.944)	Acc@5 50.222 (49.862)
Max memory in training epoch: 101.4895616
Drin!!
old memory: 951438848
new memory: 1014895616
Faktor: 1.066695582310299
New batch Size kleiner 720!!
lr: 27.196302429702317
1
Epoche:134/135; Lr: 27.196302429702317
batch Size 720
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [134][0/75]	Time 0.233 (0.233)	Data 0.618 (0.618)	Loss 147860909719552.0000 (147860909719552.0000)	Acc@1 8.889 (8.889)	Acc@5 49.481 (49.481)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 151 bis 155
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 207
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0694272
lr: 27.196302429702317
1
Epoche:131/135; Lr: 27.196302429702317
batch Size 675
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [131][0/75]	Time 0.306 (0.306)	Data 0.652 (0.652)	Loss 1738751.1250 (1738751.1250)	Acc@1 8.148 (8.148)	Acc@5 49.481 (49.481)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 156 bis 160
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8283
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0694272
lr: 27.196302429702317
1
Epoche:131/135; Lr: 27.196302429702317
batch Size 675
Epoch: [131][0/75]	Time 0.366 (0.366)	Data 0.550 (0.550)	Loss 1787159.3750 (1787159.3750)	Acc@1 9.630 (9.630)	Acc@5 50.074 (50.074)
Epoch: [131][64/75]	Time 0.162 (0.167)	Data 0.000 (0.009)	Loss 2697431351296.0000 (18908457688289714896896.0000)	Acc@1 9.481 (9.819)	Acc@5 49.037 (49.851)
Max memory in training epoch: 101.4895616
lr: 27.196302429702317
1
Epoche:132/135; Lr: 27.196302429702317
batch Size 675
Epoch: [132][0/75]	Time 0.234 (0.234)	Data 0.528 (0.528)	Loss 96483950592.0000 (96483950592.0000)	Acc@1 11.556 (11.556)	Acc@5 54.074 (54.074)
Epoch: [132][64/75]	Time 0.143 (0.167)	Data 0.000 (0.008)	Loss 150739701652741160960.0000 (80527886866450274322481152.0000)	Acc@1 10.519 (10.111)	Acc@5 54.074 (49.887)
Max memory in training epoch: 101.4895616
lr: 27.196302429702317
1
Epoche:133/135; Lr: 27.196302429702317
batch Size 675
Epoch: [133][0/75]	Time 0.245 (0.245)	Data 0.531 (0.531)	Loss 20374651757593754599424.0000 (20374651757593754599424.0000)	Acc@1 10.222 (10.222)	Acc@5 51.852 (51.852)
Epoch: [133][64/75]	Time 0.158 (0.169)	Data 0.000 (0.008)	Loss 6741146448955041854259200.0000 (14734362325882938564425526804480.0000)	Acc@1 9.778 (9.810)	Acc@5 51.407 (50.058)
Max memory in training epoch: 101.4895616
Drin!!
old memory: 951438848
new memory: 1014895616
Faktor: 1.066695582310299
New batch Size kleiner 720!!
lr: 27.196302429702317
1
Epoche:134/135; Lr: 27.196302429702317
batch Size 720
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [134][0/75]	Time 0.251 (0.251)	Data 0.576 (0.576)	Loss 1150242439299072.0000 (1150242439299072.0000)	Acc@1 9.185 (9.185)	Acc@5 50.370 (50.370)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 161 bis 165
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 3221
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0694272
lr: 27.196302429702317
1
Epoche:131/135; Lr: 27.196302429702317
batch Size 675
Epoch: [131][0/75]	Time 0.372 (0.372)	Data 0.622 (0.622)	Loss 1720984.7500 (1720984.7500)	Acc@1 9.333 (9.333)	Acc@5 47.852 (47.852)
Epoch: [131][64/75]	Time 0.164 (0.169)	Data 0.000 (0.010)	Loss 13722717937205248.0000 (4892339247790891008.0000)	Acc@1 7.852 (9.666)	Acc@5 48.444 (49.869)
Max memory in training epoch: 101.4895616
lr: 27.196302429702317
1
Epoche:132/135; Lr: 27.196302429702317
batch Size 675
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [132][0/75]	Time 0.245 (0.245)	Data 0.582 (0.582)	Loss 12932120414854840320.0000 (12932120414854840320.0000)	Acc@1 9.185 (9.185)	Acc@5 47.111 (47.111)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 166 bis 170
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6139
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0694272
lr: 27.196302429702317
1
Epoche:131/135; Lr: 27.196302429702317
batch Size 675
Epoch: [131][0/75]	Time 0.372 (0.372)	Data 0.562 (0.562)	Loss 1720013.8750 (1720013.8750)	Acc@1 8.296 (8.296)	Acc@5 48.000 (48.000)
Epoch: [131][64/75]	Time 0.148 (0.167)	Data 0.000 (0.009)	Loss 12960483648733184.0000 (172862138749152448.0000)	Acc@1 9.630 (10.031)	Acc@5 51.704 (50.067)
Max memory in training epoch: 101.4895616
lr: 27.196302429702317
1
Epoche:132/135; Lr: 27.196302429702317
batch Size 675
Epoch: [132][0/75]	Time 0.225 (0.225)	Data 0.504 (0.504)	Loss 2483234450511260286976.0000 (2483234450511260286976.0000)	Acc@1 11.556 (11.556)	Acc@5 49.926 (49.926)
Epoch: [132][64/75]	Time 0.164 (0.171)	Data 0.000 (0.008)	Loss 17652398045534036688896.0000 (12926229996823482074136576.0000)	Acc@1 7.852 (10.149)	Acc@5 46.370 (49.798)
Max memory in training epoch: 101.4895616
lr: 27.196302429702317
1
Epoche:133/135; Lr: 27.196302429702317
batch Size 675
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [133][0/75]	Time 0.248 (0.248)	Data 0.621 (0.621)	Loss 2421267890176.0000 (2421267890176.0000)	Acc@1 10.815 (10.815)	Acc@5 49.778 (49.778)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 171 bis 175
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8004
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0694272
lr: 27.196302429702317
1
Epoche:131/135; Lr: 27.196302429702317
batch Size 675
Epoch: [131][0/75]	Time 0.359 (0.359)	Data 0.649 (0.649)	Loss 1711518.7500 (1711518.7500)	Acc@1 9.630 (9.630)	Acc@5 52.593 (52.593)
Epoch: [131][64/75]	Time 0.162 (0.168)	Data 0.000 (0.010)	Loss 1644043016601600.0000 (504559099303314259968.0000)	Acc@1 10.519 (10.199)	Acc@5 49.778 (50.345)
Max memory in training epoch: 101.4895616
lr: 27.196302429702317
1
Epoche:132/135; Lr: 27.196302429702317
batch Size 675
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [132][0/75]	Time 0.265 (0.265)	Data 0.580 (0.580)	Loss 16398897152.0000 (16398897152.0000)	Acc@1 10.963 (10.963)	Acc@5 50.963 (50.963)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 176 bis 180
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9848
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0694272
lr: 27.196302429702317
1
Epoche:131/135; Lr: 27.196302429702317
batch Size 675
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [131][0/75]	Time 0.368 (0.368)	Data 0.551 (0.551)	Loss 1742137.8750 (1742137.8750)	Acc@1 10.815 (10.815)	Acc@5 49.778 (49.778)
Epoch: [131][64/75]	Time 0.165 (0.168)	Data 0.000 (0.009)	Loss 15043204743168.0000 (264214303149869.6562)	Acc@1 8.593 (10.060)	Acc@5 50.222 (50.076)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
BSize 3
j: 0 bis 5
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6920
Files already downloaded and verified
numoFStages: 3
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
Max memory: 0.202496
batch_size berechnet: 250;389.1 ; lr: 0.1
lr: 0.09765625
1
Epoche:1/5; Lr: 0.09765625
batch Size 250
Epoch: [1][0/200]	Time 0.262 (0.262)	Data 0.434 (0.434)	Loss 3.6000 (3.6000)	Acc@1 12.800 (12.800)	Acc@5 52.000 (52.000)
Epoch: [1][64/200]	Time 0.150 (0.165)	Data 0.000 (0.007)	Loss 2.8357 (3.0998)	Acc@1 21.200 (18.769)	Acc@5 80.800 (72.117)
Epoch: [1][128/200]	Time 0.157 (0.160)	Data 0.000 (0.004)	Loss 2.6840 (2.8927)	Acc@1 28.800 (23.901)	Acc@5 84.800 (78.322)
Epoch: [1][192/200]	Time 0.149 (0.160)	Data 0.000 (0.002)	Loss 2.3825 (2.7569)	Acc@1 35.600 (28.081)	Acc@5 90.000 (81.820)
Max memory in training epoch: 66.4657408
lr: 0.09765625
1
Epoche:2/5; Lr: 0.09765625
batch Size 250
Epoch: [2][0/200]	Time 0.212 (0.212)	Data 0.352 (0.352)	Loss 2.3083 (2.3083)	Acc@1 44.400 (44.400)	Acc@5 92.000 (92.000)
Epoch: [2][64/200]	Time 0.137 (0.158)	Data 0.000 (0.006)	Loss 2.1763 (2.2660)	Acc@1 47.200 (44.363)	Acc@5 90.000 (91.600)
Epoch: [2][128/200]	Time 0.171 (0.158)	Data 0.000 (0.003)	Loss 2.0391 (2.1814)	Acc@1 50.800 (47.324)	Acc@5 92.800 (92.468)
Epoch: [2][192/200]	Time 0.156 (0.157)	Data 0.000 (0.002)	Loss 1.9028 (2.0994)	Acc@1 58.400 (49.952)	Acc@5 95.200 (93.159)
Max memory in training epoch: 66.0135424
lr: 0.09765625
1
Epoche:3/5; Lr: 0.09765625
batch Size 250
Epoch: [3][0/200]	Time 0.233 (0.233)	Data 0.307 (0.307)	Loss 1.8005 (1.8005)	Acc@1 58.800 (58.800)	Acc@5 98.000 (98.000)
Epoch: [3][64/200]	Time 0.165 (0.165)	Data 0.000 (0.005)	Loss 1.6951 (1.7943)	Acc@1 61.600 (58.732)	Acc@5 95.600 (95.932)
Epoch: [3][128/200]	Time 0.160 (0.161)	Data 0.000 (0.003)	Loss 1.5428 (1.7513)	Acc@1 66.400 (60.009)	Acc@5 96.800 (96.009)
Epoch: [3][192/200]	Time 0.145 (0.159)	Data 0.000 (0.002)	Loss 1.5491 (1.7074)	Acc@1 64.400 (61.328)	Acc@5 97.200 (96.143)
Max memory in training epoch: 66.0135424
Drin!!
old memory: 0
new memory: 660135424
lr: 0.09765625
1
Epoche:4/5; Lr: 0.09765625
batch Size 250
Epoch: [4][0/200]	Time 0.257 (0.257)	Data 0.426 (0.426)	Loss 1.5471 (1.5471)	Acc@1 65.200 (65.200)	Acc@5 96.400 (96.400)
Epoch: [4][64/200]	Time 0.148 (0.154)	Data 0.000 (0.007)	Loss 1.5364 (1.5101)	Acc@1 64.800 (67.249)	Acc@5 98.000 (97.108)
Epoch: [4][128/200]	Time 0.153 (0.155)	Data 0.000 (0.003)	Loss 1.3965 (1.4794)	Acc@1 69.200 (67.718)	Acc@5 99.200 (97.212)
Epoch: [4][192/200]	Time 0.152 (0.155)	Data 0.000 (0.002)	Loss 1.3770 (1.4382)	Acc@1 68.400 (68.875)	Acc@5 97.200 (97.405)
Max memory in training epoch: 66.0135424
lr: 0.09765625
1
Epoche:5/5; Lr: 0.09765625
batch Size 250
Epoch: [5][0/200]	Time 0.215 (0.215)	Data 0.355 (0.355)	Loss 1.3889 (1.3889)	Acc@1 71.600 (71.600)	Acc@5 98.400 (98.400)
Epoch: [5][64/200]	Time 0.156 (0.162)	Data 0.000 (0.006)	Loss 1.1612 (1.2924)	Acc@1 76.400 (72.898)	Acc@5 98.400 (98.049)
Epoch: [5][128/200]	Time 0.130 (0.159)	Data 0.000 (0.003)	Loss 1.2494 (1.2652)	Acc@1 73.200 (73.476)	Acc@5 98.400 (98.167)
Epoch: [5][192/200]	Time 0.162 (0.158)	Data 0.000 (0.002)	Loss 1.2221 (1.2497)	Acc@1 73.600 (73.598)	Acc@5 99.600 (98.180)
Max memory in training epoch: 66.0135424
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  68.5
Max memory: 103.3835008
 32.013s  ./run_BSize.sh: 7: ./run_BSize.sh: cannot open j: 6 bis 10: No such file
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1735
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 6
Max memory: 0.202496
lr: 0.095367431640625
1
Epoche:6/10; Lr: 0.095367431640625
batch Size 250
Epoch: [6][0/200]	Time 0.276 (0.276)	Data 0.414 (0.414)	Loss 1.0958 (1.0958)	Acc@1 80.000 (80.000)	Acc@5 98.000 (98.000)
Epoch: [6][64/200]	Time 0.150 (0.159)	Data 0.000 (0.007)	Loss 1.1367 (1.1371)	Acc@1 74.000 (76.646)	Acc@5 98.400 (98.462)
Epoch: [6][128/200]	Time 0.149 (0.159)	Data 0.000 (0.003)	Loss 1.0645 (1.1193)	Acc@1 80.800 (76.738)	Acc@5 98.000 (98.614)
Epoch: [6][192/200]	Time 0.152 (0.158)	Data 0.000 (0.002)	Loss 1.0637 (1.1147)	Acc@1 76.800 (76.732)	Acc@5 98.000 (98.607)
Max memory in training epoch: 66.4656384
lr: 0.095367431640625
1
Epoche:7/10; Lr: 0.095367431640625
batch Size 250
Epoch: [7][0/200]	Time 0.222 (0.222)	Data 0.450 (0.450)	Loss 1.0414 (1.0414)	Acc@1 79.200 (79.200)	Acc@5 99.600 (99.600)
Epoch: [7][64/200]	Time 0.166 (0.152)	Data 0.000 (0.007)	Loss 0.9691 (1.0540)	Acc@1 80.400 (78.111)	Acc@5 99.600 (98.868)
Epoch: [7][128/200]	Time 0.152 (0.153)	Data 0.000 (0.004)	Loss 1.0596 (1.0410)	Acc@1 78.400 (78.363)	Acc@5 98.000 (98.772)
Epoch: [7][192/200]	Time 0.145 (0.154)	Data 0.000 (0.003)	Loss 1.1667 (1.0371)	Acc@1 72.800 (78.118)	Acc@5 98.000 (98.750)
Max memory in training epoch: 66.01344
lr: 0.095367431640625
1
Epoche:8/10; Lr: 0.095367431640625
batch Size 250
Epoch: [8][0/200]	Time 0.208 (0.208)	Data 0.418 (0.418)	Loss 0.8796 (0.8796)	Acc@1 81.600 (81.600)	Acc@5 99.600 (99.600)
Epoch: [8][64/200]	Time 0.124 (0.160)	Data 0.000 (0.007)	Loss 1.0197 (0.9928)	Acc@1 78.800 (79.538)	Acc@5 99.200 (98.757)
Epoch: [8][128/200]	Time 0.145 (0.159)	Data 0.000 (0.003)	Loss 1.0148 (0.9889)	Acc@1 78.000 (79.299)	Acc@5 98.800 (98.862)
Epoch: [8][192/200]	Time 0.163 (0.159)	Data 0.000 (0.002)	Loss 0.9008 (0.9890)	Acc@1 82.400 (79.086)	Acc@5 100.000 (98.846)
Max memory in training epoch: 66.01344
Drin!!
old memory: 660135424
new memory: 660134400
Faktor: 0.9999984488031353
New batch Size größer 253!!
lr: 0.095367431640625
1
Epoche:9/10; Lr: 0.095367431640625
batch Size 253
Epoch: [9][0/200]	Time 0.236 (0.236)	Data 0.372 (0.372)	Loss 0.9160 (0.9160)	Acc@1 81.200 (81.200)	Acc@5 98.800 (98.800)
Epoch: [9][64/200]	Time 0.155 (0.162)	Data 0.000 (0.006)	Loss 0.9740 (0.9614)	Acc@1 80.800 (79.914)	Acc@5 98.400 (98.831)
Epoch: [9][128/200]	Time 0.152 (0.160)	Data 0.000 (0.003)	Loss 0.8789 (0.9584)	Acc@1 80.400 (79.901)	Acc@5 99.200 (98.825)
Epoch: [9][192/200]	Time 0.146 (0.161)	Data 0.000 (0.002)	Loss 1.0504 (0.9538)	Acc@1 74.800 (79.921)	Acc@5 98.800 (98.875)
Max memory in training epoch: 66.01344
lr: 0.095367431640625
1
Epoche:10/10; Lr: 0.095367431640625
batch Size 253
Epoch: [10][0/200]	Time 0.209 (0.209)	Data 0.405 (0.405)	Loss 0.9870 (0.9870)	Acc@1 78.400 (78.400)	Acc@5 98.000 (98.000)
Epoch: [10][64/200]	Time 0.143 (0.159)	Data 0.000 (0.006)	Loss 0.8123 (0.9427)	Acc@1 85.200 (80.191)	Acc@5 98.800 (98.868)
Epoch: [10][128/200]	Time 0.141 (0.156)	Data 0.000 (0.003)	Loss 0.9139 (0.9261)	Acc@1 81.600 (80.657)	Acc@5 99.200 (98.933)
Epoch: [10][192/200]	Time 0.171 (0.156)	Data 0.000 (0.002)	Loss 0.8729 (0.9257)	Acc@1 82.400 (80.684)	Acc@5 98.400 (98.922)
Max memory in training epoch: 66.01344
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  68.84
Max memory: 103.3833984
 31.578s  j: 11 bis 15
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 3233
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 11
Max memory: 0.202496
lr: 0.09424984455108643
1
Epoche:11/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [11][0/198]	Time 0.249 (0.249)	Data 0.415 (0.415)	Loss 0.9173 (0.9173)	Acc@1 77.866 (77.866)	Acc@5 99.209 (99.209)
Epoch: [11][64/198]	Time 0.155 (0.160)	Data 0.000 (0.007)	Loss 0.9782 (0.8743)	Acc@1 79.842 (82.481)	Acc@5 98.024 (98.954)
Epoch: [11][128/198]	Time 0.142 (0.158)	Data 0.000 (0.003)	Loss 0.8657 (0.8925)	Acc@1 80.237 (81.760)	Acc@5 98.024 (98.921)
Epoch: [11][192/198]	Time 0.148 (0.157)	Data 0.000 (0.002)	Loss 0.9012 (0.8977)	Acc@1 82.609 (81.525)	Acc@5 99.209 (98.945)
Max memory in training epoch: 66.5037312
lr: 0.09424984455108643
1
Epoche:12/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [12][0/198]	Time 0.216 (0.216)	Data 0.375 (0.375)	Loss 0.9429 (0.9429)	Acc@1 81.028 (81.028)	Acc@5 97.628 (97.628)
Epoch: [12][64/198]	Time 0.162 (0.157)	Data 0.000 (0.006)	Loss 0.8750 (0.8969)	Acc@1 84.980 (81.660)	Acc@5 98.419 (98.960)
Epoch: [12][128/198]	Time 0.229 (0.157)	Data 0.000 (0.003)	Loss 0.8764 (0.8951)	Acc@1 84.190 (81.506)	Acc@5 99.605 (99.044)
Epoch: [12][192/198]	Time 0.171 (0.157)	Data 0.000 (0.002)	Loss 0.9774 (0.8921)	Acc@1 75.889 (81.718)	Acc@5 98.814 (99.060)
Max memory in training epoch: 66.277632
lr: 0.09424984455108643
1
Epoche:13/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [13][0/198]	Time 0.224 (0.224)	Data 0.397 (0.397)	Loss 0.9751 (0.9751)	Acc@1 77.470 (77.470)	Acc@5 97.628 (97.628)
Epoch: [13][64/198]	Time 0.133 (0.160)	Data 0.000 (0.006)	Loss 0.9347 (0.8880)	Acc@1 81.423 (81.684)	Acc@5 98.419 (99.045)
Epoch: [13][128/198]	Time 0.195 (0.155)	Data 0.000 (0.003)	Loss 0.8118 (0.8917)	Acc@1 84.190 (81.591)	Acc@5 99.605 (98.989)
Epoch: [13][192/198]	Time 0.154 (0.156)	Data 0.000 (0.002)	Loss 0.9401 (0.8929)	Acc@1 81.028 (81.536)	Acc@5 99.605 (99.017)
Max memory in training epoch: 66.277632
Drin!!
old memory: 660134400
new memory: 662776320
Faktor: 1.0040020941190158
New batch Size kleiner 254!!
lr: 0.09424984455108643
1
Epoche:14/15; Lr: 0.09424984455108643
batch Size 254
Epoch: [14][0/198]	Time 0.230 (0.230)	Data 0.450 (0.450)	Loss 0.8727 (0.8727)	Acc@1 81.818 (81.818)	Acc@5 98.814 (98.814)
Epoch: [14][64/198]	Time 0.152 (0.164)	Data 0.000 (0.007)	Loss 0.9018 (0.8800)	Acc@1 82.609 (81.928)	Acc@5 98.419 (98.984)
Epoch: [14][128/198]	Time 0.146 (0.161)	Data 0.000 (0.004)	Loss 0.9475 (0.8831)	Acc@1 80.237 (81.852)	Acc@5 99.209 (99.087)
Epoch: [14][192/198]	Time 0.138 (0.159)	Data 0.000 (0.003)	Loss 0.9115 (0.8813)	Acc@1 80.237 (81.988)	Acc@5 99.605 (99.142)
Max memory in training epoch: 66.277632
lr: 0.09424984455108643
1
Epoche:15/15; Lr: 0.09424984455108643
batch Size 254
Epoch: [15][0/198]	Time 0.245 (0.245)	Data 0.373 (0.373)	Loss 0.8304 (0.8304)	Acc@1 84.190 (84.190)	Acc@5 98.419 (98.419)
Epoch: [15][64/198]	Time 0.160 (0.159)	Data 0.000 (0.006)	Loss 0.8157 (0.8627)	Acc@1 83.794 (82.645)	Acc@5 99.209 (99.112)
Epoch: [15][128/198]	Time 0.175 (0.158)	Data 0.000 (0.003)	Loss 0.8507 (0.8728)	Acc@1 82.609 (82.339)	Acc@5 99.209 (99.075)
Epoch: [15][192/198]	Time 0.150 (0.159)	Data 0.000 (0.002)	Loss 0.8286 (0.8736)	Acc@1 82.609 (82.250)	Acc@5 99.605 (99.093)
Max memory in training epoch: 66.277632
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  74.96
Max memory: 103.3833984
 31.879s  j: 16 bis 20
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5447
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 16
Max memory: 0.202496
lr: 0.09351351764053106
1
Epoche:16/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [16][0/197]	Time 0.247 (0.247)	Data 0.370 (0.370)	Loss 0.7948 (0.7948)	Acc@1 83.858 (83.858)	Acc@5 99.213 (99.213)
Epoch: [16][64/197]	Time 0.171 (0.164)	Data 0.000 (0.006)	Loss 0.8434 (0.8347)	Acc@1 85.827 (83.610)	Acc@5 98.819 (99.194)
Epoch: [16][128/197]	Time 0.183 (0.156)	Data 0.000 (0.003)	Loss 0.8728 (0.8492)	Acc@1 79.528 (83.159)	Acc@5 99.213 (99.152)
Epoch: [16][192/197]	Time 0.144 (0.157)	Data 0.000 (0.002)	Loss 0.8160 (0.8560)	Acc@1 82.677 (82.883)	Acc@5 100.000 (99.166)
Max memory in training epoch: 66.5164288
lr: 0.09351351764053106
1
Epoche:17/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [17][0/197]	Time 0.240 (0.240)	Data 0.395 (0.395)	Loss 0.8018 (0.8018)	Acc@1 85.827 (85.827)	Acc@5 99.606 (99.606)
Epoch: [17][64/197]	Time 0.173 (0.162)	Data 0.000 (0.006)	Loss 0.8291 (0.8469)	Acc@1 80.709 (83.325)	Acc@5 99.606 (99.182)
Epoch: [17][128/197]	Time 0.166 (0.162)	Data 0.000 (0.003)	Loss 0.8024 (0.8500)	Acc@1 85.827 (83.129)	Acc@5 100.000 (99.173)
Epoch: [17][192/197]	Time 0.152 (0.160)	Data 0.000 (0.002)	Loss 0.8731 (0.8522)	Acc@1 83.465 (83.067)	Acc@5 98.031 (99.135)
Max memory in training epoch: 66.365696
lr: 0.09351351764053106
1
Epoche:18/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [18][0/197]	Time 0.236 (0.236)	Data 0.348 (0.348)	Loss 0.8971 (0.8971)	Acc@1 81.496 (81.496)	Acc@5 99.213 (99.213)
Epoch: [18][64/197]	Time 0.168 (0.166)	Data 0.000 (0.006)	Loss 0.8032 (0.8376)	Acc@1 83.465 (83.434)	Acc@5 99.213 (99.116)
Epoch: [18][128/197]	Time 0.160 (0.163)	Data 0.000 (0.003)	Loss 0.7929 (0.8453)	Acc@1 85.039 (83.187)	Acc@5 98.819 (99.121)
Epoch: [18][192/197]	Time 0.159 (0.162)	Data 0.000 (0.002)	Loss 0.8238 (0.8505)	Acc@1 86.614 (83.159)	Acc@5 98.425 (99.119)
Max memory in training epoch: 66.365696
Drin!!
old memory: 662776320
new memory: 663656960
Faktor: 1.0013287137355782
New batch Size kleiner 254!!
lr: 0.09351351764053106
1
Epoche:19/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [19][0/197]	Time 0.230 (0.230)	Data 0.443 (0.443)	Loss 0.8085 (0.8085)	Acc@1 83.858 (83.858)	Acc@5 100.000 (100.000)
Epoch: [19][64/197]	Time 0.132 (0.163)	Data 0.000 (0.007)	Loss 0.7706 (0.8406)	Acc@1 87.795 (83.465)	Acc@5 98.425 (99.158)
Epoch: [19][128/197]	Time 0.141 (0.161)	Data 0.000 (0.004)	Loss 0.8991 (0.8546)	Acc@1 79.528 (83.059)	Acc@5 99.606 (99.139)
Epoch: [19][192/197]	Time 0.133 (0.159)	Data 0.000 (0.002)	Loss 0.8274 (0.8574)	Acc@1 85.039 (83.034)	Acc@5 99.213 (99.182)
Max memory in training epoch: 66.365696
lr: 0.09351351764053106
1
Epoche:20/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [20][0/197]	Time 0.207 (0.207)	Data 0.403 (0.403)	Loss 0.8649 (0.8649)	Acc@1 80.709 (80.709)	Acc@5 99.213 (99.213)
Epoch: [20][64/197]	Time 0.152 (0.163)	Data 0.000 (0.006)	Loss 0.8192 (0.8339)	Acc@1 84.252 (83.531)	Acc@5 99.606 (99.267)
Epoch: [20][128/197]	Time 0.172 (0.160)	Data 0.000 (0.003)	Loss 0.7593 (0.8334)	Acc@1 87.795 (83.660)	Acc@5 99.606 (99.274)
Epoch: [20][192/197]	Time 0.139 (0.160)	Data 0.000 (0.002)	Loss 0.9017 (0.8413)	Acc@1 80.315 (83.438)	Acc@5 98.819 (99.245)
Max memory in training epoch: 66.365696
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 482478 ; 487386 ; 0.9899299528505128
[INFO] Storing checkpoint...
  63.26
Max memory: 103.3833984
 32.132s  j: 21 bis 25
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 3453
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 21
Max memory: 0.2006528
lr: 0.09278294328396441
1
Epoche:21/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [21][0/197]	Time 0.235 (0.235)	Data 0.400 (0.400)	Loss 0.8201 (0.8201)	Acc@1 85.827 (85.827)	Acc@5 98.031 (98.031)
Epoch: [21][64/197]	Time 0.162 (0.157)	Data 0.000 (0.006)	Loss 0.8473 (0.8029)	Acc@1 85.827 (84.809)	Acc@5 99.213 (99.225)
Epoch: [21][128/197]	Time 0.167 (0.158)	Data 0.000 (0.003)	Loss 0.7990 (0.8172)	Acc@1 86.220 (84.130)	Acc@5 99.606 (99.258)
Epoch: [21][192/197]	Time 0.130 (0.159)	Data 0.000 (0.002)	Loss 0.8563 (0.8290)	Acc@1 83.858 (83.762)	Acc@5 99.606 (99.235)
Max memory in training epoch: 66.2747648
lr: 0.09278294328396441
1
Epoche:22/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [22][0/197]	Time 0.215 (0.215)	Data 0.415 (0.415)	Loss 0.8325 (0.8325)	Acc@1 85.039 (85.039)	Acc@5 99.606 (99.606)
Epoch: [22][64/197]	Time 0.185 (0.160)	Data 0.000 (0.007)	Loss 0.8876 (0.8070)	Acc@1 82.283 (84.767)	Acc@5 98.819 (99.316)
Epoch: [22][128/197]	Time 0.165 (0.160)	Data 0.000 (0.003)	Loss 0.8241 (0.8254)	Acc@1 83.858 (83.959)	Acc@5 98.425 (99.191)
Epoch: [22][192/197]	Time 0.149 (0.157)	Data 0.000 (0.002)	Loss 0.7746 (0.8313)	Acc@1 85.827 (83.713)	Acc@5 100.000 (99.188)
Max memory in training epoch: 66.0199936
lr: 0.09278294328396441
1
Epoche:23/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [23][0/197]	Time 0.205 (0.205)	Data 0.469 (0.469)	Loss 0.7811 (0.7811)	Acc@1 84.646 (84.646)	Acc@5 100.000 (100.000)
Epoch: [23][64/197]	Time 0.146 (0.158)	Data 0.000 (0.007)	Loss 0.7835 (0.8205)	Acc@1 85.039 (83.961)	Acc@5 100.000 (99.322)
Epoch: [23][128/197]	Time 0.157 (0.160)	Data 0.000 (0.004)	Loss 0.8502 (0.8306)	Acc@1 81.496 (83.684)	Acc@5 98.425 (99.249)
Epoch: [23][192/197]	Time 0.155 (0.160)	Data 0.000 (0.003)	Loss 0.8280 (0.8319)	Acc@1 83.858 (83.689)	Acc@5 99.606 (99.247)
Max memory in training epoch: 66.0199936
Drin!!
old memory: 663656960
new memory: 660199936
Faktor: 0.9947909474195825
New batch Size größer 257!!
lr: 0.09278294328396441
1
Epoche:24/25; Lr: 0.09278294328396441
batch Size 257
Epoch: [24][0/197]	Time 0.224 (0.224)	Data 0.396 (0.396)	Loss 0.8807 (0.8807)	Acc@1 82.677 (82.677)	Acc@5 98.031 (98.031)
Epoch: [24][64/197]	Time 0.172 (0.159)	Data 0.000 (0.006)	Loss 0.7577 (0.8144)	Acc@1 87.008 (84.216)	Acc@5 99.606 (99.406)
Epoch: [24][128/197]	Time 0.147 (0.158)	Data 0.000 (0.003)	Loss 0.8348 (0.8275)	Acc@1 84.646 (83.745)	Acc@5 98.819 (99.310)
Epoch: [24][192/197]	Time 0.129 (0.159)	Data 0.000 (0.002)	Loss 0.9709 (0.8286)	Acc@1 81.102 (83.791)	Acc@5 99.213 (99.300)
Max memory in training epoch: 66.0199936
lr: 0.09278294328396441
1
Epoche:25/25; Lr: 0.09278294328396441
batch Size 257
Epoch: [25][0/197]	Time 0.271 (0.271)	Data 0.328 (0.328)	Loss 0.8047 (0.8047)	Acc@1 85.039 (85.039)	Acc@5 99.606 (99.606)
Epoch: [25][64/197]	Time 0.159 (0.162)	Data 0.000 (0.005)	Loss 0.7807 (0.8137)	Acc@1 85.433 (84.131)	Acc@5 99.606 (99.376)
Epoch: [25][128/197]	Time 0.132 (0.162)	Data 0.000 (0.003)	Loss 0.7928 (0.8223)	Acc@1 84.646 (84.035)	Acc@5 99.606 (99.286)
Epoch: [25][192/197]	Time 0.165 (0.160)	Data 0.000 (0.002)	Loss 0.8200 (0.8296)	Acc@1 84.646 (83.744)	Acc@5 98.819 (99.257)
Max memory in training epoch: 66.0199936
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 465152 ; 482478 ; 0.9640895543423742
[INFO] Storing checkpoint...
  70.33
Max memory: 103.08992
 32.100s  j: 26 bis 30
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 209
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 26
Max memory: 0.193792
lr: 0.0931453766561674
1
Epoche:26/30; Lr: 0.0931453766561674
batch Size 257
Epoch: [26][0/195]	Time 0.234 (0.234)	Data 0.400 (0.400)	Loss 0.7883 (0.7883)	Acc@1 84.825 (84.825)	Acc@5 99.611 (99.611)
Epoch: [26][64/195]	Time 0.158 (0.159)	Data 0.000 (0.006)	Loss 0.8624 (0.8087)	Acc@1 83.268 (84.448)	Acc@5 99.611 (99.288)
Epoch: [26][128/195]	Time 0.156 (0.156)	Data 0.000 (0.003)	Loss 0.7465 (0.8220)	Acc@1 87.549 (83.893)	Acc@5 99.222 (99.246)
Epoch: [26][192/195]	Time 0.143 (0.157)	Data 0.000 (0.002)	Loss 0.7839 (0.8168)	Acc@1 86.381 (84.182)	Acc@5 99.611 (99.282)
Max memory in training epoch: 65.6374272
lr: 0.0931453766561674
1
Epoche:27/30; Lr: 0.0931453766561674
batch Size 257
Epoch: [27][0/195]	Time 0.209 (0.209)	Data 0.341 (0.341)	Loss 0.7546 (0.7546)	Acc@1 85.603 (85.603)	Acc@5 100.000 (100.000)
Epoch: [27][64/195]	Time 0.135 (0.152)	Data 0.000 (0.005)	Loss 0.8637 (0.8106)	Acc@1 80.934 (84.005)	Acc@5 98.833 (99.312)
Epoch: [27][128/195]	Time 0.161 (0.155)	Data 0.000 (0.003)	Loss 0.8609 (0.8184)	Acc@1 82.490 (84.017)	Acc@5 99.611 (99.273)
Epoch: [27][192/195]	Time 0.142 (0.155)	Data 0.000 (0.002)	Loss 0.7579 (0.8237)	Acc@1 84.436 (83.912)	Acc@5 99.611 (99.274)
Max memory in training epoch: 65.772288
lr: 0.0931453766561674
1
Epoche:28/30; Lr: 0.0931453766561674
batch Size 257
Epoch: [28][0/195]	Time 0.209 (0.209)	Data 0.366 (0.366)	Loss 0.8714 (0.8714)	Acc@1 83.658 (83.658)	Acc@5 98.444 (98.444)
Epoch: [28][64/195]	Time 0.170 (0.167)	Data 0.000 (0.006)	Loss 0.8232 (0.7991)	Acc@1 84.047 (84.777)	Acc@5 98.054 (99.246)
Epoch: [28][128/195]	Time 0.145 (0.165)	Data 0.000 (0.003)	Loss 0.7383 (0.8082)	Acc@1 87.549 (84.409)	Acc@5 99.222 (99.252)
Epoch: [28][192/195]	Time 0.162 (0.163)	Data 0.000 (0.002)	Loss 0.7912 (0.8116)	Acc@1 86.381 (84.295)	Acc@5 98.833 (99.294)
Max memory in training epoch: 65.772288
Drin!!
old memory: 660199936
new memory: 657722880
Faktor: 0.9962480214478543
New batch Size größer 261!!
lr: 0.0931453766561674
1
Epoche:29/30; Lr: 0.0931453766561674
batch Size 261
Epoch: [29][0/195]	Time 0.217 (0.217)	Data 0.365 (0.365)	Loss 0.7958 (0.7958)	Acc@1 85.214 (85.214)	Acc@5 100.000 (100.000)
Epoch: [29][64/195]	Time 0.163 (0.154)	Data 0.000 (0.006)	Loss 0.7675 (0.7939)	Acc@1 86.381 (84.627)	Acc@5 100.000 (99.371)
Epoch: [29][128/195]	Time 0.184 (0.159)	Data 0.000 (0.003)	Loss 0.7447 (0.8002)	Acc@1 87.938 (84.354)	Acc@5 99.611 (99.312)
Epoch: [29][192/195]	Time 0.164 (0.160)	Data 0.000 (0.002)	Loss 0.7412 (0.8071)	Acc@1 87.160 (84.305)	Acc@5 99.222 (99.341)
Max memory in training epoch: 65.772288
lr: 0.0931453766561674
1
Epoche:30/30; Lr: 0.0931453766561674
batch Size 261
Epoch: [30][0/195]	Time 0.220 (0.220)	Data 0.374 (0.374)	Loss 0.8117 (0.8117)	Acc@1 83.658 (83.658)	Acc@5 99.222 (99.222)
Epoch: [30][64/195]	Time 0.139 (0.159)	Data 0.000 (0.006)	Loss 0.7924 (0.7987)	Acc@1 85.992 (84.837)	Acc@5 98.833 (99.294)
Epoch: [30][128/195]	Time 0.167 (0.159)	Data 0.000 (0.003)	Loss 0.8733 (0.8101)	Acc@1 81.323 (84.463)	Acc@5 99.611 (99.261)
Epoch: [30][192/195]	Time 0.177 (0.159)	Data 0.000 (0.002)	Loss 0.7332 (0.8095)	Acc@1 87.938 (84.369)	Acc@5 99.222 (99.282)
Max memory in training epoch: 65.772288
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 432242 ; 465152 ; 0.9292489336818932
[INFO] Storing checkpoint...
  80.84
Max memory: 100.8050688
 31.522s  j: 31 bis 35
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7877
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 31
Max memory: 0.1806848
lr: 0.09496462229398317
1
Epoche:31/35; Lr: 0.09496462229398317
batch Size 261
Epoch: [31][0/192]	Time 0.338 (0.338)	Data 0.364 (0.364)	Loss 0.7633 (0.7633)	Acc@1 85.441 (85.441)	Acc@5 99.617 (99.617)
Epoch: [31][64/192]	Time 0.147 (0.156)	Data 0.000 (0.006)	Loss 0.7546 (0.7717)	Acc@1 88.123 (85.464)	Acc@5 99.234 (99.405)
Epoch: [31][128/192]	Time 0.143 (0.156)	Data 0.000 (0.003)	Loss 0.8345 (0.7884)	Acc@1 84.291 (84.983)	Acc@5 99.234 (99.305)
Max memory in training epoch: 65.33248
lr: 0.09496462229398317
1
Epoche:32/35; Lr: 0.09496462229398317
batch Size 261
Epoch: [32][0/192]	Time 0.243 (0.243)	Data 0.413 (0.413)	Loss 0.8482 (0.8482)	Acc@1 81.226 (81.226)	Acc@5 98.851 (98.851)
Epoch: [32][64/192]	Time 0.170 (0.154)	Data 0.000 (0.007)	Loss 0.8054 (0.8039)	Acc@1 83.908 (84.150)	Acc@5 99.617 (99.316)
Epoch: [32][128/192]	Time 0.155 (0.158)	Data 0.000 (0.003)	Loss 0.8069 (0.8090)	Acc@1 85.441 (84.092)	Acc@5 100.000 (99.344)
Max memory in training epoch: 65.2734464
lr: 0.09496462229398317
1
Epoche:33/35; Lr: 0.09496462229398317
batch Size 261
Epoch: [33][0/192]	Time 0.248 (0.248)	Data 0.446 (0.446)	Loss 0.8282 (0.8282)	Acc@1 82.759 (82.759)	Acc@5 99.617 (99.617)
Epoch: [33][64/192]	Time 0.142 (0.164)	Data 0.000 (0.007)	Loss 0.7124 (0.7810)	Acc@1 88.123 (85.116)	Acc@5 99.234 (99.363)
Epoch: [33][128/192]	Time 0.167 (0.161)	Data 0.000 (0.004)	Loss 0.8061 (0.7897)	Acc@1 85.824 (84.956)	Acc@5 98.851 (99.311)
Max memory in training epoch: 65.2734464
Drin!!
old memory: 657722880
new memory: 652734464
Faktor: 0.9924156264717444
New batch Size größer 267!!
lr: 0.09496462229398317
1
Epoche:34/35; Lr: 0.09496462229398317
batch Size 267
Epoch: [34][0/192]	Time 0.201 (0.201)	Data 0.388 (0.388)	Loss 0.7545 (0.7545)	Acc@1 88.889 (88.889)	Acc@5 98.851 (98.851)
Epoch: [34][64/192]	Time 0.162 (0.155)	Data 0.000 (0.006)	Loss 0.7393 (0.7861)	Acc@1 85.441 (85.046)	Acc@5 100.000 (99.346)
Epoch: [34][128/192]	Time 0.163 (0.157)	Data 0.004 (0.003)	Loss 0.8385 (0.7976)	Acc@1 85.441 (84.811)	Acc@5 99.234 (99.293)
Max memory in training epoch: 65.2734464
lr: 0.09496462229398317
1
Epoche:35/35; Lr: 0.09496462229398317
batch Size 267
Epoch: [35][0/192]	Time 0.246 (0.246)	Data 0.336 (0.336)	Loss 0.6891 (0.6891)	Acc@1 90.038 (90.038)	Acc@5 99.617 (99.617)
Epoch: [35][64/192]	Time 0.163 (0.163)	Data 0.000 (0.005)	Loss 0.7861 (0.7742)	Acc@1 85.441 (85.718)	Acc@5 99.617 (99.381)
Epoch: [35][128/192]	Time 0.159 (0.160)	Data 0.000 (0.003)	Loss 0.7818 (0.7910)	Acc@1 87.739 (84.945)	Acc@5 99.617 (99.287)
Max memory in training epoch: 65.2734464
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 407122 ; 432242 ; 0.941884407345885
[INFO] Storing checkpoint...
  77.63
Max memory: 98.6432
 30.947s  j: 36 bis 40
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 816
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 36
Max memory: 0.1708544
lr: 0.09904513340817776
1
Epoche:36/40; Lr: 0.09904513340817776
batch Size 267
Epoch: [36][0/188]	Time 0.267 (0.267)	Data 0.380 (0.380)	Loss 0.7381 (0.7381)	Acc@1 86.517 (86.517)	Acc@5 99.625 (99.625)
Epoch: [36][64/188]	Time 0.164 (0.162)	Data 0.000 (0.006)	Loss 0.7876 (0.7671)	Acc@1 82.397 (85.727)	Acc@5 100.000 (99.441)
Epoch: [36][128/188]	Time 0.152 (0.160)	Data 0.000 (0.003)	Loss 0.7194 (0.7831)	Acc@1 87.640 (85.297)	Acc@5 100.000 (99.399)
Max memory in training epoch: 65.039104
lr: 0.09904513340817776
1
Epoche:37/40; Lr: 0.09904513340817776
batch Size 267
Epoch: [37][0/188]	Time 0.267 (0.267)	Data 0.432 (0.432)	Loss 0.7855 (0.7855)	Acc@1 86.142 (86.142)	Acc@5 98.876 (98.876)
Epoch: [37][64/188]	Time 0.145 (0.160)	Data 0.000 (0.007)	Loss 0.7207 (0.7862)	Acc@1 87.640 (85.088)	Acc@5 100.000 (99.366)
Epoch: [37][128/188]	Time 0.195 (0.159)	Data 0.000 (0.004)	Loss 0.7403 (0.7967)	Acc@1 86.142 (84.781)	Acc@5 99.251 (99.329)
Max memory in training epoch: 65.0164736
lr: 0.09904513340817776
1
Epoche:38/40; Lr: 0.09904513340817776
batch Size 267
Epoch: [38][0/188]	Time 0.235 (0.235)	Data 0.363 (0.363)	Loss 0.7871 (0.7871)	Acc@1 85.768 (85.768)	Acc@5 99.625 (99.625)
Epoch: [38][64/188]	Time 0.178 (0.165)	Data 0.000 (0.006)	Loss 0.6912 (0.7849)	Acc@1 87.266 (84.777)	Acc@5 100.000 (99.268)
Epoch: [38][128/188]	Time 0.149 (0.163)	Data 0.000 (0.003)	Loss 0.8072 (0.7977)	Acc@1 86.517 (84.490)	Acc@5 98.502 (99.277)
Max memory in training epoch: 65.0779648
Drin!!
old memory: 652734464
new memory: 650779648
Faktor: 0.997005189540597
New batch Size größer 274!!
lr: 0.09904513340817776
1
Epoche:39/40; Lr: 0.09904513340817776
batch Size 274
Epoch: [39][0/188]	Time 0.205 (0.205)	Data 0.402 (0.402)	Loss 0.7797 (0.7797)	Acc@1 87.640 (87.640)	Acc@5 99.251 (99.251)
Epoch: [39][64/188]	Time 0.178 (0.159)	Data 0.000 (0.006)	Loss 0.7761 (0.7822)	Acc@1 84.644 (85.387)	Acc@5 98.502 (99.286)
Epoch: [39][128/188]	Time 0.155 (0.159)	Data 0.000 (0.003)	Loss 0.8484 (0.7903)	Acc@1 80.524 (85.042)	Acc@5 98.876 (99.265)
Max memory in training epoch: 65.0779648
lr: 0.09904513340817776
1
Epoche:40/40; Lr: 0.09904513340817776
batch Size 274
Epoch: [40][0/188]	Time 0.259 (0.259)	Data 0.320 (0.320)	Loss 0.7500 (0.7500)	Acc@1 85.393 (85.393)	Acc@5 99.625 (99.625)
Epoch: [40][64/188]	Time 0.181 (0.162)	Data 0.000 (0.005)	Loss 0.7940 (0.7952)	Acc@1 83.895 (84.713)	Acc@5 99.251 (99.286)
Epoch: [40][128/188]	Time 0.148 (0.160)	Data 0.000 (0.003)	Loss 0.8371 (0.7926)	Acc@1 84.644 (84.868)	Acc@5 98.876 (99.347)
Max memory in training epoch: 65.0779648
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 386476 ; 407122 ; 0.9492879284342286
[INFO] Storing checkpoint...
  80.62
Max memory: 96.3604992
 30.464s  j: 41 bis 45
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 490
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 41
Max memory: 0.1627136
lr: 0.10600924435094027
1
Epoche:41/45; Lr: 0.10600924435094027
batch Size 274
Epoch: [41][0/183]	Time 0.245 (0.245)	Data 0.375 (0.375)	Loss 0.7834 (0.7834)	Acc@1 86.861 (86.861)	Acc@5 98.175 (98.175)
Epoch: [41][64/183]	Time 0.155 (0.159)	Data 0.000 (0.006)	Loss 0.8517 (0.7677)	Acc@1 81.387 (86.075)	Acc@5 98.905 (99.388)
Epoch: [41][128/183]	Time 0.167 (0.158)	Data 0.000 (0.003)	Loss 0.8037 (0.7886)	Acc@1 84.307 (85.158)	Acc@5 100.000 (99.301)
Max memory in training epoch: 65.9290112
lr: 0.10600924435094027
1
Epoche:42/45; Lr: 0.10600924435094027
batch Size 274
Epoch: [42][0/183]	Time 0.221 (0.221)	Data 0.325 (0.325)	Loss 0.7770 (0.7770)	Acc@1 85.766 (85.766)	Acc@5 99.270 (99.270)
Epoch: [42][64/183]	Time 0.144 (0.161)	Data 0.000 (0.005)	Loss 0.7995 (0.7942)	Acc@1 86.131 (84.722)	Acc@5 99.635 (99.326)
Epoch: [42][128/183]	Time 0.149 (0.161)	Data 0.000 (0.003)	Loss 0.8197 (0.7949)	Acc@1 84.672 (84.657)	Acc@5 98.905 (99.327)
Max memory in training epoch: 65.8783232
lr: 0.10600924435094027
1
Epoche:43/45; Lr: 0.10600924435094027
batch Size 274
Epoch: [43][0/183]	Time 0.204 (0.204)	Data 0.427 (0.427)	Loss 0.8008 (0.8008)	Acc@1 83.577 (83.577)	Acc@5 98.905 (98.905)
Epoch: [43][64/183]	Time 0.165 (0.167)	Data 0.000 (0.007)	Loss 0.7477 (0.8046)	Acc@1 88.321 (84.823)	Acc@5 99.635 (99.248)
Epoch: [43][128/183]	Time 0.142 (0.164)	Data 0.000 (0.004)	Loss 0.8659 (0.8076)	Acc@1 81.752 (84.403)	Acc@5 99.635 (99.270)
Max memory in training epoch: 65.8783232
Drin!!
old memory: 650779648
new memory: 658783232
Faktor: 1.0122984546683305
New batch Size kleiner 277!!
lr: 0.10600924435094027
1
Epoche:44/45; Lr: 0.10600924435094027
batch Size 277
Epoch: [44][0/183]	Time 0.226 (0.226)	Data 0.470 (0.470)	Loss 0.7715 (0.7715)	Acc@1 86.496 (86.496)	Acc@5 98.540 (98.540)
Epoch: [44][64/183]	Time 0.142 (0.163)	Data 0.000 (0.007)	Loss 0.8101 (0.7851)	Acc@1 82.847 (85.042)	Acc@5 100.000 (99.287)
Epoch: [44][128/183]	Time 0.168 (0.163)	Data 0.000 (0.004)	Loss 0.8342 (0.7893)	Acc@1 81.752 (84.889)	Acc@5 99.635 (99.327)
Max memory in training epoch: 65.8783232
lr: 0.10600924435094027
1
Epoche:45/45; Lr: 0.10600924435094027
batch Size 277
Epoch: [45][0/183]	Time 0.212 (0.212)	Data 0.387 (0.387)	Loss 0.8403 (0.8403)	Acc@1 83.212 (83.212)	Acc@5 99.635 (99.635)
Epoch: [45][64/183]	Time 0.163 (0.164)	Data 0.000 (0.006)	Loss 0.8649 (0.7797)	Acc@1 81.387 (85.154)	Acc@5 99.270 (99.450)
Epoch: [45][128/183]	Time 0.128 (0.160)	Data 0.000 (0.003)	Loss 0.7769 (0.7886)	Acc@1 85.036 (84.867)	Acc@5 98.905 (99.361)
Max memory in training epoch: 65.8783232
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 375940 ; 386476 ; 0.9727382812904294
[INFO] Storing checkpoint...
  76.72
Max memory: 94.3929344
 29.654s  j: 46 bis 50
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2008
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 46
Max memory: 0.158464
lr: 0.11470531517660333
1
Epoche:46/50; Lr: 0.11470531517660333
batch Size 277
Epoch: [46][0/181]	Time 0.297 (0.297)	Data 0.420 (0.420)	Loss 0.6905 (0.6905)	Acc@1 89.892 (89.892)	Acc@5 99.639 (99.639)
Epoch: [46][64/181]	Time 0.162 (0.163)	Data 0.000 (0.007)	Loss 0.8528 (0.7679)	Acc@1 82.671 (85.698)	Acc@5 99.278 (99.433)
Epoch: [46][128/181]	Time 0.156 (0.162)	Data 0.001 (0.003)	Loss 0.7910 (0.7908)	Acc@1 84.477 (84.832)	Acc@5 99.278 (99.404)
Max memory in training epoch: 66.262784
lr: 0.11470531517660333
1
Epoche:47/50; Lr: 0.11470531517660333
batch Size 277
Epoch: [47][0/181]	Time 0.268 (0.268)	Data 0.449 (0.449)	Loss 0.7581 (0.7581)	Acc@1 85.199 (85.199)	Acc@5 100.000 (100.000)
Epoch: [47][64/181]	Time 0.150 (0.169)	Data 0.000 (0.007)	Loss 0.7987 (0.7871)	Acc@1 85.921 (85.104)	Acc@5 98.917 (99.306)
Epoch: [47][128/181]	Time 0.161 (0.164)	Data 0.000 (0.004)	Loss 0.8302 (0.7962)	Acc@1 83.755 (84.829)	Acc@5 98.917 (99.337)
Max memory in training epoch: 66.0338688
lr: 0.11470531517660333
1
Epoche:48/50; Lr: 0.11470531517660333
batch Size 277
Epoch: [48][0/181]	Time 0.247 (0.247)	Data 0.485 (0.485)	Loss 0.8675 (0.8675)	Acc@1 84.116 (84.116)	Acc@5 98.917 (98.917)
Epoch: [48][64/181]	Time 0.168 (0.162)	Data 0.000 (0.008)	Loss 0.7631 (0.8093)	Acc@1 87.365 (84.377)	Acc@5 99.639 (99.356)
Epoch: [48][128/181]	Time 0.176 (0.162)	Data 0.000 (0.004)	Loss 0.8134 (0.8045)	Acc@1 82.671 (84.611)	Acc@5 98.917 (99.320)
Max memory in training epoch: 65.9474944
Drin!!
old memory: 658783232
new memory: 659474944
Faktor: 1.001049984223035
New batch Size kleiner 277!!
lr: 0.11470531517660333
1
Epoche:49/50; Lr: 0.11470531517660333
batch Size 277
Epoch: [49][0/181]	Time 0.231 (0.231)	Data 0.489 (0.489)	Loss 0.8135 (0.8135)	Acc@1 84.477 (84.477)	Acc@5 98.556 (98.556)
Epoch: [49][64/181]	Time 0.169 (0.166)	Data 0.000 (0.008)	Loss 0.8496 (0.8024)	Acc@1 83.755 (84.982)	Acc@5 99.278 (99.256)
Epoch: [49][128/181]	Time 0.149 (0.163)	Data 0.000 (0.004)	Loss 0.8279 (0.8014)	Acc@1 84.116 (84.751)	Acc@5 99.278 (99.286)
Max memory in training epoch: 65.9474944
lr: 0.11470531517660333
1
Epoche:50/50; Lr: 0.11470531517660333
batch Size 277
Epoch: [50][0/181]	Time 0.199 (0.199)	Data 0.416 (0.416)	Loss 0.7340 (0.7340)	Acc@1 85.921 (85.921)	Acc@5 98.917 (98.917)
Epoch: [50][64/181]	Time 0.152 (0.162)	Data 0.000 (0.007)	Loss 0.7072 (0.7910)	Acc@1 87.365 (85.160)	Acc@5 99.278 (99.261)
Epoch: [50][128/181]	Time 0.165 (0.162)	Data 0.000 (0.003)	Loss 0.8532 (0.7948)	Acc@1 82.671 (84.857)	Acc@5 98.917 (99.250)
Max memory in training epoch: 65.9474944
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv21.weight

 RM:  module.conv22.weight
numoFStages: 3
Count: 365484 ; 375940 ; 0.9721870511251796
[INFO] Storing checkpoint...
  77.48
Max memory: 93.2294144
 29.517s  j: 51 bis 55
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5601
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 51
Max memory: 0.153856
lr: 0.12411473556218407
1
Epoche:51/55; Lr: 0.12411473556218407
batch Size 277
Epoch: [51][0/181]	Time 0.255 (0.255)	Data 0.387 (0.387)	Loss 0.7253 (0.7253)	Acc@1 89.170 (89.170)	Acc@5 99.278 (99.278)
Epoch: [51][64/181]	Time 0.149 (0.152)	Data 0.000 (0.006)	Loss 0.8148 (0.7727)	Acc@1 84.116 (85.337)	Acc@5 98.917 (99.361)
Epoch: [51][128/181]	Time 0.129 (0.149)	Data 0.000 (0.003)	Loss 0.8324 (0.8021)	Acc@1 83.032 (84.443)	Acc@5 99.278 (99.281)
Max memory in training epoch: 63.4556416
lr: 0.12411473556218407
1
Epoche:52/55; Lr: 0.12411473556218407
batch Size 277
Epoch: [52][0/181]	Time 0.200 (0.200)	Data 0.283 (0.283)	Loss 0.8090 (0.8090)	Acc@1 85.560 (85.560)	Acc@5 99.278 (99.278)
Epoch: [52][64/181]	Time 0.164 (0.151)	Data 0.000 (0.005)	Loss 0.8743 (0.8156)	Acc@1 83.394 (84.238)	Acc@5 98.556 (99.250)
Epoch: [52][128/181]	Time 0.160 (0.150)	Data 0.000 (0.002)	Loss 0.8696 (0.8193)	Acc@1 80.144 (84.107)	Acc@5 99.278 (99.230)
Max memory in training epoch: 63.4960896
lr: 0.12411473556218407
1
Epoche:53/55; Lr: 0.12411473556218407
batch Size 277
Epoch: [53][0/181]	Time 0.211 (0.211)	Data 0.374 (0.374)	Loss 0.7349 (0.7349)	Acc@1 86.282 (86.282)	Acc@5 99.278 (99.278)
Epoch: [53][64/181]	Time 0.145 (0.151)	Data 0.000 (0.006)	Loss 0.7098 (0.7943)	Acc@1 86.282 (84.854)	Acc@5 99.639 (99.278)
Epoch: [53][128/181]	Time 0.139 (0.152)	Data 0.000 (0.003)	Loss 0.7876 (0.8028)	Acc@1 84.838 (84.731)	Acc@5 98.556 (99.289)
Max memory in training epoch: 63.4960896
Drin!!
old memory: 659474944
new memory: 634960896
Faktor: 0.9628279311852066
New batch Size größer 292!!
lr: 0.12411473556218407
1
Epoche:54/55; Lr: 0.12411473556218407
batch Size 292
Epoch: [54][0/181]	Time 0.189 (0.189)	Data 0.457 (0.457)	Loss 0.7914 (0.7914)	Acc@1 85.560 (85.560)	Acc@5 99.278 (99.278)
Epoch: [54][64/181]	Time 0.148 (0.150)	Data 0.000 (0.007)	Loss 0.7537 (0.8050)	Acc@1 87.004 (84.510)	Acc@5 99.278 (99.272)
Epoch: [54][128/181]	Time 0.146 (0.152)	Data 0.000 (0.004)	Loss 0.6958 (0.8050)	Acc@1 88.448 (84.588)	Acc@5 100.000 (99.275)
Max memory in training epoch: 63.4960896
lr: 0.12411473556218407
1
Epoche:55/55; Lr: 0.12411473556218407
batch Size 292
Epoch: [55][0/181]	Time 0.206 (0.206)	Data 0.395 (0.395)	Loss 0.6536 (0.6536)	Acc@1 87.726 (87.726)	Acc@5 100.000 (100.000)
Epoch: [55][64/181]	Time 0.163 (0.155)	Data 0.000 (0.006)	Loss 0.7263 (0.7872)	Acc@1 87.004 (85.099)	Acc@5 99.278 (99.395)
Epoch: [55][128/181]	Time 0.154 (0.154)	Data 0.000 (0.003)	Loss 0.8278 (0.7989)	Acc@1 83.032 (84.717)	Acc@5 99.278 (99.384)
Max memory in training epoch: 63.4960896
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 351630 ; 365484 ; 0.9620940998785172
[INFO] Storing checkpoint...
  73.61
Max memory: 89.7704448
 28.032s  j: 56 bis 60
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8209
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 56
Max memory: 0.148224
lr: 0.1415683702506162
1
Epoche:56/60; Lr: 0.1415683702506162
batch Size 292
Epoch: [56][0/172]	Time 0.273 (0.273)	Data 0.410 (0.410)	Loss 0.7653 (0.7653)	Acc@1 84.247 (84.247)	Acc@5 98.630 (98.630)
Epoch: [56][64/172]	Time 0.139 (0.154)	Data 0.000 (0.006)	Loss 0.8599 (0.7894)	Acc@1 81.507 (85.079)	Acc@5 97.603 (99.257)
Epoch: [56][128/172]	Time 0.141 (0.151)	Data 0.000 (0.003)	Loss 0.7582 (0.8095)	Acc@1 85.959 (84.358)	Acc@5 99.315 (99.241)
Max memory in training epoch: 64.8454656
lr: 0.1415683702506162
1
Epoche:57/60; Lr: 0.1415683702506162
batch Size 292
Epoch: [57][0/172]	Time 0.213 (0.213)	Data 0.426 (0.426)	Loss 0.8860 (0.8860)	Acc@1 81.164 (81.164)	Acc@5 98.630 (98.630)
Epoch: [57][64/172]	Time 0.158 (0.150)	Data 0.000 (0.007)	Loss 0.8774 (0.8424)	Acc@1 82.192 (83.288)	Acc@5 99.658 (99.199)
Epoch: [57][128/172]	Time 0.157 (0.152)	Data 0.000 (0.003)	Loss 0.7735 (0.8344)	Acc@1 84.589 (83.716)	Acc@5 99.315 (99.249)
Max memory in training epoch: 65.314048
lr: 0.1415683702506162
1
Epoche:58/60; Lr: 0.1415683702506162
batch Size 292
Epoch: [58][0/172]	Time 0.229 (0.229)	Data 0.411 (0.411)	Loss 0.7596 (0.7596)	Acc@1 86.301 (86.301)	Acc@5 99.658 (99.658)
Epoch: [58][64/172]	Time 0.167 (0.156)	Data 0.000 (0.007)	Loss 0.7947 (0.8140)	Acc@1 84.589 (84.146)	Acc@5 99.315 (99.368)
Epoch: [58][128/172]	Time 0.129 (0.157)	Data 0.000 (0.003)	Loss 0.8692 (0.8216)	Acc@1 83.904 (84.037)	Acc@5 99.658 (99.291)
Max memory in training epoch: 65.2197376
Drin!!
old memory: 634960896
new memory: 652197376
Faktor: 1.0271457346563906
New batch Size kleiner 299!!
lr: 0.1415683702506162
1
Epoche:59/60; Lr: 0.1415683702506162
batch Size 299
Epoch: [59][0/172]	Time 0.210 (0.210)	Data 0.392 (0.392)	Loss 0.7494 (0.7494)	Acc@1 85.959 (85.959)	Acc@5 99.658 (99.658)
Epoch: [59][64/172]	Time 0.151 (0.153)	Data 0.000 (0.006)	Loss 0.8471 (0.8337)	Acc@1 81.164 (83.683)	Acc@5 99.315 (99.210)
Epoch: [59][128/172]	Time 0.179 (0.151)	Data 0.000 (0.003)	Loss 0.7856 (0.8263)	Acc@1 84.932 (83.809)	Acc@5 98.973 (99.249)
Max memory in training epoch: 65.2197376
lr: 0.1415683702506162
1
Epoche:60/60; Lr: 0.1415683702506162
batch Size 299
Epoch: [60][0/172]	Time 0.238 (0.238)	Data 0.405 (0.405)	Loss 0.7768 (0.7768)	Acc@1 85.274 (85.274)	Acc@5 98.973 (98.973)
Epoch: [60][64/172]	Time 0.160 (0.157)	Data 0.000 (0.007)	Loss 0.8017 (0.8293)	Acc@1 83.219 (83.593)	Acc@5 99.658 (99.231)
Epoch: [60][128/172]	Time 0.140 (0.156)	Data 0.000 (0.003)	Loss 0.9000 (0.8231)	Acc@1 81.849 (84.008)	Acc@5 99.315 (99.212)
Max memory in training epoch: 65.2197376
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 333878 ; 351630 ; 0.9495151153200808
[INFO] Storing checkpoint...
  72.42
Max memory: 88.4196864
 27.143s  j: 61 bis 65
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5416
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 61
Max memory: 0.1412096
lr: 0.16534743244114938
1
Epoche:61/65; Lr: 0.16534743244114938
batch Size 299
Epoch: [61][0/168]	Time 0.304 (0.304)	Data 0.454 (0.454)	Loss 0.7048 (0.7048)	Acc@1 86.957 (86.957)	Acc@5 99.666 (99.666)
Epoch: [61][64/168]	Time 0.158 (0.156)	Data 0.000 (0.007)	Loss 0.8437 (0.8165)	Acc@1 82.943 (84.106)	Acc@5 98.662 (99.213)
Epoch: [61][128/168]	Time 0.137 (0.155)	Data 0.000 (0.004)	Loss 0.7856 (0.8360)	Acc@1 85.619 (83.521)	Acc@5 98.997 (99.165)
Max memory in training epoch: 65.5540224
lr: 0.16534743244114938
1
Epoche:62/65; Lr: 0.16534743244114938
batch Size 299
Epoch: [62][0/168]	Time 0.215 (0.215)	Data 0.380 (0.380)	Loss 0.8456 (0.8456)	Acc@1 81.605 (81.605)	Acc@5 99.666 (99.666)
Epoch: [62][64/168]	Time 0.156 (0.156)	Data 0.000 (0.006)	Loss 0.8618 (0.8442)	Acc@1 83.612 (83.458)	Acc@5 97.993 (99.280)
Epoch: [62][128/168]	Time 0.150 (0.155)	Data 0.000 (0.003)	Loss 0.7848 (0.8432)	Acc@1 86.622 (83.591)	Acc@5 99.666 (99.287)
Max memory in training epoch: 65.7496064
lr: 0.16534743244114938
1
Epoche:63/65; Lr: 0.16534743244114938
batch Size 299
Epoch: [63][0/168]	Time 0.182 (0.182)	Data 0.316 (0.316)	Loss 0.7312 (0.7312)	Acc@1 87.625 (87.625)	Acc@5 100.000 (100.000)
Epoch: [63][64/168]	Time 0.157 (0.155)	Data 0.000 (0.005)	Loss 0.9492 (0.8552)	Acc@1 78.261 (83.262)	Acc@5 98.997 (99.228)
Epoch: [63][128/168]	Time 0.152 (0.154)	Data 0.000 (0.003)	Loss 0.9171 (0.8553)	Acc@1 83.278 (83.174)	Acc@5 98.328 (99.204)
Max memory in training epoch: 65.7496064
Drin!!
old memory: 652197376
new memory: 657496064
Faktor: 1.0081243626469298
New batch Size kleiner 301!!
lr: 0.16534743244114938
1
Epoche:64/65; Lr: 0.16534743244114938
batch Size 301
Epoch: [64][0/168]	Time 0.230 (0.230)	Data 0.389 (0.389)	Loss 0.8720 (0.8720)	Acc@1 82.609 (82.609)	Acc@5 99.666 (99.666)
Epoch: [64][64/168]	Time 0.145 (0.158)	Data 0.000 (0.006)	Loss 0.8777 (0.8416)	Acc@1 81.940 (83.442)	Acc@5 98.662 (99.187)
Epoch: [64][128/168]	Time 0.161 (0.159)	Data 0.000 (0.003)	Loss 0.9342 (0.8371)	Acc@1 79.264 (83.475)	Acc@5 98.662 (99.217)
Max memory in training epoch: 65.7496064
lr: 0.16534743244114938
1
Epoche:65/65; Lr: 0.16534743244114938
batch Size 301
Epoch: [65][0/168]	Time 0.191 (0.191)	Data 0.395 (0.395)	Loss 0.8833 (0.8833)	Acc@1 83.278 (83.278)	Acc@5 98.328 (98.328)
Epoch: [65][64/168]	Time 0.138 (0.154)	Data 0.000 (0.006)	Loss 0.7785 (0.8349)	Acc@1 84.950 (83.483)	Acc@5 99.666 (99.233)
Epoch: [65][128/168]	Time 0.169 (0.156)	Data 0.000 (0.003)	Loss 0.8890 (0.8358)	Acc@1 81.940 (83.456)	Acc@5 99.331 (99.279)
Max memory in training epoch: 65.7496064
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv19.weight

 RM:  module.conv20.weight
numoFStages: 3
Count: 323710 ; 333878 ; 0.9695457622245252
[INFO] Storing checkpoint...
  75.21
Max memory: 87.5300864
 26.701s  j: 66 bis 70
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1102
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 66
Max memory: 0.1366016
lr: 0.19441241079994517
1
Epoche:66/70; Lr: 0.19441241079994517
batch Size 301
Epoch: [66][0/167]	Time 0.300 (0.300)	Data 0.355 (0.355)	Loss 0.7574 (0.7574)	Acc@1 85.050 (85.050)	Acc@5 99.003 (99.003)
Epoch: [66][64/167]	Time 0.155 (0.148)	Data 0.000 (0.006)	Loss 0.8414 (0.8333)	Acc@1 81.728 (83.486)	Acc@5 98.671 (99.198)
Epoch: [66][128/167]	Time 0.121 (0.142)	Data 0.000 (0.003)	Loss 1.0008 (0.8637)	Acc@1 78.073 (82.724)	Acc@5 99.003 (99.130)
Max memory in training epoch: 63.0109696
lr: 0.19441241079994517
1
Epoche:67/70; Lr: 0.19441241079994517
batch Size 301
Epoch: [67][0/167]	Time 0.220 (0.220)	Data 0.434 (0.434)	Loss 0.8639 (0.8639)	Acc@1 83.389 (83.389)	Acc@5 98.339 (98.339)
Epoch: [67][64/167]	Time 0.149 (0.146)	Data 0.000 (0.007)	Loss 0.8351 (0.8958)	Acc@1 84.718 (82.142)	Acc@5 98.007 (99.049)
Epoch: [67][128/167]	Time 0.144 (0.146)	Data 0.000 (0.004)	Loss 0.8762 (0.8890)	Acc@1 82.060 (82.258)	Acc@5 99.336 (99.065)
Max memory in training epoch: 63.0265856
lr: 0.19441241079994517
1
Epoche:68/70; Lr: 0.19441241079994517
batch Size 301
Epoch: [68][0/167]	Time 0.216 (0.216)	Data 0.423 (0.423)	Loss 0.8792 (0.8792)	Acc@1 81.728 (81.728)	Acc@5 98.007 (98.007)
Epoch: [68][64/167]	Time 0.158 (0.150)	Data 0.000 (0.007)	Loss 0.8274 (0.8728)	Acc@1 83.389 (82.949)	Acc@5 99.668 (99.116)
Epoch: [68][128/167]	Time 0.171 (0.148)	Data 0.000 (0.003)	Loss 0.7862 (0.8698)	Acc@1 85.382 (82.845)	Acc@5 99.336 (99.142)
Max memory in training epoch: 63.19232
Drin!!
old memory: 657496064
new memory: 631923200
Faktor: 0.9611056774326181
New batch Size größer 319!!
lr: 0.19441241079994517
1
Epoche:69/70; Lr: 0.19441241079994517
batch Size 319
Epoch: [69][0/167]	Time 0.192 (0.192)	Data 0.417 (0.417)	Loss 0.9455 (0.9455)	Acc@1 81.063 (81.063)	Acc@5 99.003 (99.003)
Epoch: [69][64/167]	Time 0.158 (0.150)	Data 0.002 (0.007)	Loss 0.8388 (0.8915)	Acc@1 82.392 (81.993)	Acc@5 99.336 (99.054)
Epoch: [69][128/167]	Time 0.126 (0.148)	Data 0.000 (0.004)	Loss 0.8708 (0.8811)	Acc@1 84.053 (82.415)	Acc@5 99.003 (99.050)
Max memory in training epoch: 63.19232
lr: 0.19441241079994517
1
Epoche:70/70; Lr: 0.19441241079994517
batch Size 319
Epoch: [70][0/167]	Time 0.203 (0.203)	Data 0.413 (0.413)	Loss 0.8541 (0.8541)	Acc@1 83.389 (83.389)	Acc@5 99.668 (99.668)
Epoch: [70][64/167]	Time 0.153 (0.150)	Data 0.000 (0.007)	Loss 0.8852 (0.8918)	Acc@1 85.050 (82.285)	Acc@5 99.336 (99.034)
Epoch: [70][128/167]	Time 0.143 (0.152)	Data 0.000 (0.003)	Loss 0.9126 (0.8819)	Acc@1 79.734 (82.459)	Acc@5 99.336 (99.083)
Max memory in training epoch: 63.19232
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 309996 ; 323710 ; 0.9576349201445739
[INFO] Storing checkpoint...
  69.97
Max memory: 83.7639168
 25.687s  j: 71 bis 75
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4177
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 71
Max memory: 0.131072
lr: 0.24225609002024417
1
Epoche:71/75; Lr: 0.24225609002024417
batch Size 319
Epoch: [71][0/157]	Time 0.261 (0.261)	Data 0.454 (0.454)	Loss 0.8578 (0.8578)	Acc@1 82.759 (82.759)	Acc@5 99.060 (99.060)
Epoch: [71][64/157]	Time 0.132 (0.150)	Data 0.000 (0.007)	Loss 0.8665 (0.8675)	Acc@1 79.937 (82.373)	Acc@5 98.746 (99.069)
Epoch: [71][128/157]	Time 0.150 (0.149)	Data 0.000 (0.004)	Loss 0.9127 (0.8936)	Acc@1 78.056 (81.653)	Acc@5 98.433 (98.975)
Max memory in training epoch: 65.561344
lr: 0.24225609002024417
1
Epoche:72/75; Lr: 0.24225609002024417
batch Size 319
Epoch: [72][0/157]	Time 0.243 (0.243)	Data 0.418 (0.418)	Loss 0.8600 (0.8600)	Acc@1 83.386 (83.386)	Acc@5 99.373 (99.373)
Epoch: [72][64/157]	Time 0.151 (0.150)	Data 0.000 (0.007)	Loss 0.8797 (0.8940)	Acc@1 83.386 (82.006)	Acc@5 100.000 (99.007)
Epoch: [72][128/157]	Time 0.119 (0.148)	Data 0.000 (0.003)	Loss 0.8911 (0.8946)	Acc@1 84.326 (81.918)	Acc@5 99.687 (99.052)
Max memory in training epoch: 65.6123392
lr: 0.24225609002024417
1
Epoche:73/75; Lr: 0.24225609002024417
batch Size 319
Epoch: [73][0/157]	Time 0.210 (0.210)	Data 0.374 (0.374)	Loss 0.8740 (0.8740)	Acc@1 81.818 (81.818)	Acc@5 99.373 (99.373)
Epoch: [73][64/157]	Time 0.137 (0.148)	Data 0.000 (0.006)	Loss 0.8914 (0.8850)	Acc@1 81.818 (82.223)	Acc@5 99.060 (99.117)
Epoch: [73][128/157]	Time 0.180 (0.151)	Data 0.000 (0.003)	Loss 0.8010 (0.8975)	Acc@1 84.013 (81.782)	Acc@5 99.687 (99.069)
Max memory in training epoch: 65.6496128
Drin!!
old memory: 631923200
new memory: 656496128
Faktor: 1.0388859405699933
New batch Size kleiner 331!!
lr: 0.24225609002024417
1
Epoche:74/75; Lr: 0.24225609002024417
batch Size 331
Epoch: [74][0/157]	Time 0.215 (0.215)	Data 0.393 (0.393)	Loss 0.8786 (0.8786)	Acc@1 81.818 (81.818)	Acc@5 99.060 (99.060)
Epoch: [74][64/157]	Time 0.145 (0.148)	Data 0.000 (0.006)	Loss 0.9667 (0.9019)	Acc@1 80.251 (81.572)	Acc@5 99.373 (99.021)
Epoch: [74][128/157]	Time 0.148 (0.150)	Data 0.000 (0.003)	Loss 0.9809 (0.9043)	Acc@1 79.310 (81.638)	Acc@5 98.433 (98.982)
Max memory in training epoch: 65.6496128
lr: 0.24225609002024417
1
Epoche:75/75; Lr: 0.24225609002024417
batch Size 331
Epoch: [75][0/157]	Time 0.179 (0.179)	Data 0.317 (0.317)	Loss 0.8561 (0.8561)	Acc@1 84.013 (84.013)	Acc@5 99.373 (99.373)
Epoch: [75][64/157]	Time 0.142 (0.147)	Data 0.000 (0.005)	Loss 0.8659 (0.8887)	Acc@1 82.445 (81.958)	Acc@5 99.687 (98.982)
Epoch: [75][128/157]	Time 0.159 (0.148)	Data 0.000 (0.003)	Loss 0.8405 (0.8887)	Acc@1 82.132 (81.986)	Acc@5 99.373 (99.006)
Max memory in training epoch: 65.6496128
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 285174 ; 309996 ; 0.9199279990709558
[INFO] Storing checkpoint...
  67.14
Max memory: 81.3632512
 23.628s  j: 76 bis 80
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 307
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 76
Max memory: 0.1212416
lr: 0.3132295538933626
1
Epoche:76/80; Lr: 0.3132295538933626
batch Size 331
Epoch: [76][0/152]	Time 0.265 (0.265)	Data 0.429 (0.429)	Loss 0.8518 (0.8518)	Acc@1 84.290 (84.290)	Acc@5 98.489 (98.489)
Epoch: [76][64/152]	Time 0.140 (0.150)	Data 0.000 (0.007)	Loss 1.0023 (0.9278)	Acc@1 78.248 (80.776)	Acc@5 98.792 (98.922)
Epoch: [76][128/152]	Time 0.168 (0.151)	Data 0.000 (0.004)	Loss 0.9824 (0.9431)	Acc@1 79.154 (80.358)	Acc@5 97.885 (98.916)
Max memory in training epoch: 66.160128
lr: 0.3132295538933626
1
Epoche:77/80; Lr: 0.3132295538933626
batch Size 331
Epoch: [77][0/152]	Time 0.278 (0.278)	Data 0.403 (0.403)	Loss 0.9838 (0.9838)	Acc@1 78.248 (78.248)	Acc@5 99.396 (99.396)
Epoch: [77][64/152]	Time 0.149 (0.153)	Data 0.000 (0.007)	Loss 0.9791 (1.0033)	Acc@1 78.852 (78.787)	Acc@5 99.396 (98.638)
Epoch: [77][128/152]	Time 0.153 (0.152)	Data 0.000 (0.003)	Loss 0.8929 (0.9675)	Acc@1 81.873 (79.864)	Acc@5 99.396 (98.787)
Max memory in training epoch: 66.0817408
lr: 0.3132295538933626
1
Epoche:78/80; Lr: 0.3132295538933626
batch Size 331
Epoch: [78][0/152]	Time 0.232 (0.232)	Data 0.475 (0.475)	Loss 1.0241 (1.0241)	Acc@1 77.341 (77.341)	Acc@5 98.489 (98.489)
Epoch: [78][64/152]	Time 0.159 (0.155)	Data 0.000 (0.008)	Loss 0.9183 (1.0279)	Acc@1 82.477 (77.718)	Acc@5 99.698 (98.647)
Epoch: [78][128/152]	Time 0.157 (0.154)	Data 0.000 (0.004)	Loss 0.9343 (0.9883)	Acc@1 77.644 (79.042)	Acc@5 99.094 (98.763)
Max memory in training epoch: 66.0585472
Drin!!
old memory: 656496128
new memory: 660585472
Faktor: 1.0062290451163178
New batch Size kleiner 333!!
lr: 0.3132295538933626
1
Epoche:79/80; Lr: 0.3132295538933626
batch Size 333
Epoch: [79][0/152]	Time 0.208 (0.208)	Data 0.386 (0.386)	Loss 1.1651 (1.1651)	Acc@1 75.227 (75.227)	Acc@5 97.583 (97.583)
Epoch: [79][64/152]	Time 0.141 (0.150)	Data 0.000 (0.006)	Loss 0.9349 (1.0565)	Acc@1 80.665 (77.225)	Acc@5 98.489 (98.369)
Epoch: [79][128/152]	Time 0.171 (0.152)	Data 0.000 (0.003)	Loss 0.9538 (1.0064)	Acc@1 79.154 (78.749)	Acc@5 99.094 (98.663)
Max memory in training epoch: 66.0585472
lr: 0.3132295538933626
1
Epoche:80/80; Lr: 0.3132295538933626
batch Size 333
Epoch: [80][0/152]	Time 0.214 (0.214)	Data 0.366 (0.366)	Loss 0.9736 (0.9736)	Acc@1 78.852 (78.852)	Acc@5 99.094 (99.094)
Epoch: [80][64/152]	Time 0.166 (0.152)	Data 0.000 (0.006)	Loss 0.8871 (0.9929)	Acc@1 81.269 (78.866)	Acc@5 98.489 (98.638)
Epoch: [80][128/152]	Time 0.136 (0.152)	Data 0.000 (0.003)	Loss 0.9515 (0.9603)	Acc@1 76.737 (79.672)	Acc@5 99.396 (98.794)
Max memory in training epoch: 66.0585472
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 258466 ; 285174 ; 0.906344898202501
[INFO] Storing checkpoint...
  53.17
Max memory: 79.8092288
 23.566s  j: 81 bis 85
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5162
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 81
Max memory: 0.110848
lr: 0.4074431306503506
1
Epoche:81/85; Lr: 0.4074431306503506
batch Size 333
Epoch: [81][0/151]	Time 0.269 (0.269)	Data 0.438 (0.438)	Loss 0.9277 (0.9277)	Acc@1 80.180 (80.180)	Acc@5 99.099 (99.099)
Epoch: [81][64/151]	Time 0.170 (0.150)	Data 0.000 (0.007)	Loss 0.9477 (0.9912)	Acc@1 78.979 (78.217)	Acc@5 99.099 (98.545)
Epoch: [81][128/151]	Time 0.132 (0.149)	Data 0.009 (0.004)	Loss 1.0545 (1.0119)	Acc@1 78.679 (77.780)	Acc@5 97.898 (98.547)
Max memory in training epoch: 63.0186496
lr: 0.4074431306503506
1
Epoche:82/85; Lr: 0.4074431306503506
batch Size 333
Epoch: [82][0/151]	Time 0.227 (0.227)	Data 0.560 (0.560)	Loss 1.1017 (1.1017)	Acc@1 76.276 (76.276)	Acc@5 98.799 (98.799)
Epoch: [82][64/151]	Time 0.154 (0.152)	Data 0.000 (0.009)	Loss 1.0577 (1.0258)	Acc@1 76.577 (77.939)	Acc@5 99.099 (98.531)
Epoch: [82][128/151]	Time 0.146 (0.150)	Data 0.000 (0.005)	Loss 1.0560 (1.0122)	Acc@1 76.877 (78.190)	Acc@5 98.198 (98.582)
Max memory in training epoch: 62.9889536
lr: 0.4074431306503506
1
Epoche:83/85; Lr: 0.4074431306503506
batch Size 333
Epoch: [83][0/151]	Time 0.197 (0.197)	Data 0.382 (0.382)	Loss 1.1674 (1.1674)	Acc@1 73.273 (73.273)	Acc@5 98.198 (98.198)
Epoch: [83][64/151]	Time 0.158 (0.146)	Data 0.000 (0.006)	Loss 1.1072 (1.0155)	Acc@1 75.375 (78.046)	Acc@5 97.598 (98.563)
Epoch: [83][128/151]	Time 0.156 (0.148)	Data 0.000 (0.003)	Loss 1.0053 (1.0125)	Acc@1 77.177 (78.015)	Acc@5 98.799 (98.547)
Max memory in training epoch: 62.9889536
Drin!!
old memory: 660585472
new memory: 629889536
Faktor: 0.9535322266366766
New batch Size größer 354!!
lr: 0.4074431306503506
1
Epoche:84/85; Lr: 0.4074431306503506
batch Size 354
Epoch: [84][0/151]	Time 0.292 (0.292)	Data 0.424 (0.424)	Loss 1.0048 (1.0048)	Acc@1 78.679 (78.679)	Acc@5 98.498 (98.498)
Epoch: [84][64/151]	Time 0.147 (0.154)	Data 0.000 (0.007)	Loss 1.0390 (1.0221)	Acc@1 77.477 (78.032)	Acc@5 97.898 (98.304)
Epoch: [84][128/151]	Time 0.161 (0.152)	Data 0.000 (0.004)	Loss 1.0669 (1.0251)	Acc@1 77.477 (77.799)	Acc@5 98.198 (98.408)
Max memory in training epoch: 62.9889536
lr: 0.4074431306503506
1
Epoche:85/85; Lr: 0.4074431306503506
batch Size 354
Epoch: [85][0/151]	Time 0.213 (0.213)	Data 0.456 (0.456)	Loss 0.9832 (0.9832)	Acc@1 77.177 (77.177)	Acc@5 99.399 (99.399)
Epoch: [85][64/151]	Time 0.154 (0.154)	Data 0.000 (0.007)	Loss 0.9099 (1.0329)	Acc@1 82.282 (77.436)	Acc@5 98.799 (98.508)
Epoch: [85][128/151]	Time 0.142 (0.153)	Data 0.000 (0.004)	Loss 1.0663 (1.0122)	Acc@1 77.477 (78.071)	Acc@5 99.099 (98.575)
Max memory in training epoch: 62.9889536
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 242446 ; 258466 ; 0.9380189270542354
[INFO] Storing checkpoint...
  60.26
Max memory: 75.1241728
 23.526s  j: 86 bis 90
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7967
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 86
Max memory: 0.1043456
lr: 0.5634174541024379
1
Epoche:86/90; Lr: 0.5634174541024379
batch Size 354
Epoch: [86][0/142]	Time 0.224 (0.224)	Data 0.497 (0.497)	Loss 1.0248 (1.0248)	Acc@1 77.401 (77.401)	Acc@5 98.305 (98.305)
Epoch: [86][64/142]	Time 0.149 (0.152)	Data 0.000 (0.008)	Loss 0.9785 (1.0662)	Acc@1 79.661 (76.380)	Acc@5 99.435 (98.214)
Epoch: [86][128/142]	Time 0.158 (0.149)	Data 0.000 (0.004)	Loss 0.9853 (1.0770)	Acc@1 78.249 (76.153)	Acc@5 99.435 (98.294)
Max memory in training epoch: 66.358016
lr: 0.5634174541024379
1
Epoche:87/90; Lr: 0.5634174541024379
batch Size 354
Epoch: [87][0/142]	Time 0.259 (0.259)	Data 0.386 (0.386)	Loss 0.9398 (0.9398)	Acc@1 78.531 (78.531)	Acc@5 99.153 (99.153)
Epoch: [87][64/142]	Time 0.146 (0.152)	Data 0.000 (0.006)	Loss 1.0384 (1.0780)	Acc@1 76.554 (76.219)	Acc@5 98.305 (98.392)
Epoch: [87][128/142]	Time 0.156 (0.152)	Data 0.000 (0.003)	Loss 1.1757 (1.0778)	Acc@1 71.751 (76.039)	Acc@5 98.305 (98.406)
Max memory in training epoch: 66.4950272
lr: 0.5634174541024379
1
Epoche:88/90; Lr: 0.5634174541024379
batch Size 354
Epoch: [88][0/142]	Time 0.200 (0.200)	Data 0.466 (0.466)	Loss 1.2293 (1.2293)	Acc@1 72.034 (72.034)	Acc@5 98.870 (98.870)
Epoch: [88][64/142]	Time 0.146 (0.155)	Data 0.000 (0.007)	Loss 1.1865 (1.0799)	Acc@1 70.339 (76.093)	Acc@5 97.740 (98.275)
Epoch: [88][128/142]	Time 0.146 (0.154)	Data 0.000 (0.004)	Loss 1.0599 (1.0888)	Acc@1 75.424 (75.630)	Acc@5 98.870 (98.167)
Max memory in training epoch: 66.4950272
Drin!!
old memory: 629889536
new memory: 664950272
Faktor: 1.0556617216133592
New batch Size kleiner 373!!
lr: 0.5634174541024379
1
Epoche:89/90; Lr: 0.5634174541024379
batch Size 373
Epoch: [89][0/142]	Time 0.198 (0.198)	Data 0.416 (0.416)	Loss 1.1432 (1.1432)	Acc@1 75.424 (75.424)	Acc@5 96.893 (96.893)
Epoch: [89][64/142]	Time 0.161 (0.154)	Data 0.000 (0.007)	Loss 1.0012 (1.0678)	Acc@1 78.531 (75.924)	Acc@5 99.153 (98.401)
Epoch: [89][128/142]	Time 0.122 (0.155)	Data 0.000 (0.003)	Loss 1.0225 (1.0636)	Acc@1 77.119 (76.341)	Acc@5 98.870 (98.355)
Max memory in training epoch: 66.4950272
lr: 0.5634174541024379
1
Epoche:90/90; Lr: 0.5634174541024379
batch Size 373
Epoch: [90][0/142]	Time 0.191 (0.191)	Data 0.447 (0.447)	Loss 1.0105 (1.0105)	Acc@1 79.096 (79.096)	Acc@5 98.023 (98.023)
Epoch: [90][64/142]	Time 0.142 (0.154)	Data 0.000 (0.007)	Loss 1.1369 (1.0704)	Acc@1 73.729 (76.132)	Acc@5 98.023 (98.157)
Epoch: [90][128/142]	Time 0.156 (0.152)	Data 0.000 (0.004)	Loss 1.0768 (1.0680)	Acc@1 75.706 (76.043)	Acc@5 98.305 (98.213)
Max memory in training epoch: 66.4950272
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv4.weight

 RM:  module.conv5.weight

 RM:  module.conv17.weight

 RM:  module.conv18.weight
numoFStages: 3
Count: 227036 ; 242446 ; 0.9364394545589534
[INFO] Storing checkpoint...
  52.17
Max memory: 73.8013184
 21.997s  j: 91 bis 95
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6635
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 91
Max memory: 0.0970752
lr: 0.8209168374226927
1
Epoche:91/95; Lr: 0.8209168374226927
batch Size 373
Epoch: [91][0/135]	Time 0.251 (0.251)	Data 0.449 (0.449)	Loss 1.0492 (1.0492)	Acc@1 75.871 (75.871)	Acc@5 98.123 (98.123)
Epoch: [91][64/135]	Time 0.130 (0.135)	Data 0.000 (0.007)	Loss 1.2776 (1.1621)	Acc@1 69.437 (73.017)	Acc@5 97.319 (97.896)
Epoch: [91][128/135]	Time 0.124 (0.132)	Data 0.000 (0.004)	Loss 1.0982 (1.1695)	Acc@1 78.284 (73.041)	Acc@5 97.855 (97.924)
Max memory in training epoch: 60.0437248
lr: 0.8209168374226927
1
Epoche:92/95; Lr: 0.8209168374226927
batch Size 373
Epoch: [92][0/135]	Time 0.182 (0.182)	Data 0.327 (0.327)	Loss 1.9600 (1.9600)	Acc@1 51.743 (51.743)	Acc@5 93.029 (93.029)
Epoch: [92][64/135]	Time 0.137 (0.132)	Data 0.000 (0.005)	Loss 1.2595 (1.3997)	Acc@1 73.458 (67.148)	Acc@5 95.979 (96.572)
Epoch: [92][128/135]	Time 0.113 (0.132)	Data 0.000 (0.003)	Loss 1.2163 (1.2971)	Acc@1 69.705 (69.736)	Acc@5 97.855 (97.163)
Max memory in training epoch: 59.9536128
lr: 0.8209168374226927
1
Epoche:93/95; Lr: 0.08209168374226927
batch Size 373
Epoch: [93][0/135]	Time 0.236 (0.236)	Data 0.416 (0.416)	Loss 1.8034 (1.8034)	Acc@1 53.887 (53.887)	Acc@5 93.566 (93.566)
Epoch: [93][64/135]	Time 0.128 (0.134)	Data 0.000 (0.007)	Loss 0.9206 (1.0811)	Acc@1 81.233 (75.694)	Acc@5 99.196 (98.239)
Epoch: [93][128/135]	Time 0.118 (0.135)	Data 0.000 (0.004)	Loss 0.8798 (0.9857)	Acc@1 80.429 (78.633)	Acc@5 99.464 (98.657)
Max memory in training epoch: 59.9536128
Drin!!
old memory: 664950272
new memory: 599536128
Faktor: 0.9016255098245903
New batch Size größer 416!!
lr: 0.08209168374226927
1
Epoche:94/95; Lr: 0.08209168374226927
batch Size 416
Epoch: [94][0/135]	Time 0.179 (0.179)	Data 0.456 (0.456)	Loss 0.7202 (0.7202)	Acc@1 87.399 (87.399)	Acc@5 99.732 (99.732)
Epoch: [94][64/135]	Time 0.142 (0.133)	Data 0.000 (0.007)	Loss 0.7697 (0.8371)	Acc@1 85.791 (83.052)	Acc@5 98.660 (99.072)
Epoch: [94][128/135]	Time 0.126 (0.133)	Data 0.000 (0.004)	Loss 0.8231 (0.8123)	Acc@1 84.450 (83.596)	Acc@5 98.391 (99.210)
Max memory in training epoch: 59.9536128
lr: 0.08209168374226927
1
Epoche:95/95; Lr: 0.08209168374226927
batch Size 416
Epoch: [95][0/135]	Time 0.191 (0.191)	Data 0.451 (0.451)	Loss 0.7043 (0.7043)	Acc@1 85.791 (85.791)	Acc@5 98.928 (98.928)
Epoch: [95][64/135]	Time 0.133 (0.138)	Data 0.000 (0.007)	Loss 0.7395 (0.7641)	Acc@1 84.718 (84.273)	Acc@5 99.732 (99.311)
Epoch: [95][128/135]	Time 0.121 (0.136)	Data 0.000 (0.004)	Loss 0.7549 (0.7499)	Acc@1 83.646 (84.585)	Acc@5 98.928 (99.329)
Max memory in training epoch: 59.9536128
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv24.weight

 RM:  module.conv25.weight
numoFStages: 3
Count: 207710 ; 227036 ; 0.9148769358163463
[INFO] Storing checkpoint...
  81.67
Max memory: 63.458816
 18.908s  j: 96 bis 100
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9640
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 96
Max memory: 0.0889344
lr: 0.13339898608118755
1
Epoche:96/100; Lr: 0.13339898608118755
batch Size 416
Epoch: [96][0/121]	Time 0.310 (0.310)	Data 0.438 (0.438)	Loss 0.7011 (0.7011)	Acc@1 85.817 (85.817)	Acc@5 99.279 (99.279)
Epoch: [96][64/121]	Time 0.123 (0.131)	Data 0.000 (0.007)	Loss 0.7734 (0.7327)	Acc@1 84.615 (84.512)	Acc@5 98.798 (99.238)
Max memory in training epoch: 62.427904
lr: 0.13339898608118755
1
Epoche:97/100; Lr: 0.13339898608118755
batch Size 416
Epoch: [97][0/121]	Time 0.178 (0.178)	Data 0.409 (0.409)	Loss 0.7166 (0.7166)	Acc@1 84.615 (84.615)	Acc@5 99.038 (99.038)
Epoch: [97][64/121]	Time 0.125 (0.128)	Data 0.000 (0.007)	Loss 0.6565 (0.7120)	Acc@1 86.779 (84.327)	Acc@5 99.279 (99.360)
Max memory in training epoch: 62.427904
lr: 0.13339898608118755
1
Epoche:98/100; Lr: 0.13339898608118755
batch Size 416
Epoch: [98][0/121]	Time 0.258 (0.258)	Data 0.447 (0.447)	Loss 0.6585 (0.6585)	Acc@1 85.337 (85.337)	Acc@5 99.760 (99.760)
Epoch: [98][64/121]	Time 0.117 (0.136)	Data 0.000 (0.007)	Loss 0.7184 (0.6900)	Acc@1 84.375 (84.671)	Acc@5 99.519 (99.316)
Max memory in training epoch: 62.427904
Drin!!
old memory: 599536128
new memory: 624279040
Faktor: 1.041270093401277
New batch Size kleiner 433!!
lr: 0.13339898608118755
1
Epoche:99/100; Lr: 0.13339898608118755
batch Size 433
Epoch: [99][0/121]	Time 0.209 (0.209)	Data 0.443 (0.443)	Loss 0.7144 (0.7144)	Acc@1 84.375 (84.375)	Acc@5 99.038 (99.038)
Epoch: [99][64/121]	Time 0.135 (0.133)	Data 0.000 (0.007)	Loss 0.6794 (0.6928)	Acc@1 83.413 (84.320)	Acc@5 99.279 (99.301)
Max memory in training epoch: 62.427904
lr: 0.13339898608118755
1
Epoche:100/100; Lr: 0.13339898608118755
batch Size 433
Epoch: [100][0/121]	Time 0.196 (0.196)	Data 0.412 (0.412)	Loss 0.6228 (0.6228)	Acc@1 84.375 (84.375)	Acc@5 99.760 (99.760)
Epoch: [100][64/121]	Time 0.140 (0.134)	Data 0.000 (0.007)	Loss 0.6538 (0.6905)	Acc@1 85.817 (84.308)	Acc@5 99.760 (99.364)
Max memory in training epoch: 62.427904
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 197606 ; 207710 ; 0.9513552549227288
[INFO] Storing checkpoint...
  77.92
Max memory: 62.427904
 16.376s  j: 101 bis 105
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1890
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 101
Max memory: 0.0848896
lr: 0.22563187880138363
1
Epoche:101/105; Lr: 0.22563187880138363
batch Size 433
Epoch: [101][0/116]	Time 0.265 (0.265)	Data 0.483 (0.483)	Loss 0.5850 (0.5850)	Acc@1 88.453 (88.453)	Acc@5 99.769 (99.769)
Epoch: [101][64/116]	Time 0.137 (0.135)	Data 0.000 (0.008)	Loss 0.8060 (0.7608)	Acc@1 81.293 (81.769)	Acc@5 98.614 (99.126)
Max memory in training epoch: 65.3906944
lr: 0.22563187880138363
1
Epoche:102/105; Lr: 0.22563187880138363
batch Size 433
Epoch: [102][0/116]	Time 0.192 (0.192)	Data 0.456 (0.456)	Loss 0.7766 (0.7766)	Acc@1 82.217 (82.217)	Acc@5 99.076 (99.076)
Epoch: [102][64/116]	Time 0.162 (0.134)	Data 0.000 (0.007)	Loss 0.7476 (0.7747)	Acc@1 82.910 (81.887)	Acc@5 98.845 (99.083)
Max memory in training epoch: 65.3906944
lr: 0.22563187880138363
1
Epoche:103/105; Lr: 0.22563187880138363
batch Size 433
Epoch: [103][0/116]	Time 0.187 (0.187)	Data 0.487 (0.487)	Loss 0.8865 (0.8865)	Acc@1 77.136 (77.136)	Acc@5 99.538 (99.538)
Epoch: [103][64/116]	Time 0.131 (0.130)	Data 0.000 (0.008)	Loss 0.7392 (0.7593)	Acc@1 83.603 (82.793)	Acc@5 98.614 (99.186)
Max memory in training epoch: 65.3906944
Drin!!
old memory: 624279040
new memory: 653906944
Faktor: 1.0474593925178075
New batch Size kleiner 453!!
lr: 0.22563187880138363
1
Epoche:104/105; Lr: 0.22563187880138363
batch Size 453
Epoch: [104][0/116]	Time 0.191 (0.191)	Data 0.428 (0.428)	Loss 0.8050 (0.8050)	Acc@1 81.755 (81.755)	Acc@5 99.307 (99.307)
Epoch: [104][64/116]	Time 0.127 (0.138)	Data 0.000 (0.007)	Loss 0.8342 (0.7683)	Acc@1 79.446 (82.476)	Acc@5 99.307 (99.183)
Max memory in training epoch: 65.3906944
lr: 0.22563187880138363
1
Epoche:105/105; Lr: 0.22563187880138363
batch Size 453
Epoch: [105][0/116]	Time 0.188 (0.188)	Data 0.463 (0.463)	Loss 0.7037 (0.7037)	Acc@1 86.374 (86.374)	Acc@5 99.769 (99.769)
Epoch: [105][64/116]	Time 0.112 (0.136)	Data 0.000 (0.007)	Loss 0.7190 (0.7690)	Acc@1 83.834 (82.579)	Acc@5 99.538 (99.130)
Max memory in training epoch: 65.3906944
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv20.weight

 RM:  module.conv21.weight
numoFStages: 3
Count: 190838 ; 197606 ; 0.965750027833163
[INFO] Storing checkpoint...
  71.65
Max memory: 65.3906944
 16.117s  j: 106 bis 110
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4648
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 106
Max memory: 0.0817664
lr: 0.3992626605352609
1
Epoche:106/110; Lr: 0.3992626605352609
batch Size 453
Epoch: [106][0/111]	Time 0.259 (0.259)	Data 0.501 (0.501)	Loss 0.7911 (0.7911)	Acc@1 82.119 (82.119)	Acc@5 98.675 (98.675)
Epoch: [106][64/111]	Time 0.116 (0.128)	Data 0.000 (0.008)	Loss 0.8905 (0.8685)	Acc@1 79.470 (79.273)	Acc@5 98.013 (98.828)
Max memory in training epoch: 64.6572544
lr: 0.3992626605352609
1
Epoche:107/110; Lr: 0.3992626605352609
batch Size 453
Epoch: [107][0/111]	Time 0.168 (0.168)	Data 0.478 (0.478)	Loss 0.9272 (0.9272)	Acc@1 76.159 (76.159)	Acc@5 99.117 (99.117)
Epoch: [107][64/111]	Time 0.116 (0.129)	Data 0.000 (0.008)	Loss 0.8703 (0.9050)	Acc@1 80.574 (78.910)	Acc@5 99.559 (98.825)
Max memory in training epoch: 64.6572544
lr: 0.3992626605352609
1
Epoche:108/110; Lr: 0.3992626605352609
batch Size 453
Epoch: [108][0/111]	Time 0.178 (0.178)	Data 0.514 (0.514)	Loss 0.8524 (0.8524)	Acc@1 80.795 (80.795)	Acc@5 98.234 (98.234)
Epoch: [108][64/111]	Time 0.135 (0.128)	Data 0.000 (0.008)	Loss 0.9321 (0.9091)	Acc@1 78.146 (79.365)	Acc@5 98.234 (98.764)
Max memory in training epoch: 64.6572544
Drin!!
old memory: 653906944
new memory: 646572544
Faktor: 0.9887837251656407
New batch Size größer 469!!
lr: 0.3992626605352609
1
Epoche:109/110; Lr: 0.3992626605352609
batch Size 469
Epoch: [109][0/111]	Time 0.203 (0.203)	Data 0.439 (0.439)	Loss 0.8506 (0.8506)	Acc@1 82.119 (82.119)	Acc@5 98.455 (98.455)
Epoch: [109][64/111]	Time 0.110 (0.129)	Data 0.000 (0.007)	Loss 0.9798 (0.8756)	Acc@1 75.055 (80.336)	Acc@5 98.675 (98.937)
Max memory in training epoch: 64.6572544
lr: 0.3992626605352609
1
Epoche:110/110; Lr: 0.3992626605352609
batch Size 469
Epoch: [110][0/111]	Time 0.236 (0.236)	Data 0.419 (0.419)	Loss 0.8844 (0.8844)	Acc@1 78.366 (78.366)	Acc@5 98.896 (98.896)
Epoch: [110][64/111]	Time 0.136 (0.129)	Data 0.000 (0.007)	Loss 0.8648 (0.8795)	Acc@1 81.457 (80.149)	Acc@5 99.338 (98.845)
Max memory in training epoch: 64.6572544
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 188238 ; 190838 ; 0.9863758790178057
[INFO] Storing checkpoint...
  60.62
Max memory: 64.6572544
 14.687s  j: 111 bis 115
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6974
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 111
Max memory: 0.0805888
lr: 0.7314616710587397
1
Epoche:111/115; Lr: 0.7314616710587397
batch Size 469
Epoch: [111][0/107]	Time 0.266 (0.266)	Data 0.496 (0.496)	Loss 0.8445 (0.8445)	Acc@1 81.876 (81.876)	Acc@5 98.081 (98.081)
Epoch: [111][64/107]	Time 0.125 (0.132)	Data 0.000 (0.008)	Loss 1.0383 (1.0342)	Acc@1 78.038 (75.545)	Acc@5 98.294 (98.222)
Max memory in training epoch: 67.0650368
lr: 0.7314616710587397
1
Epoche:112/115; Lr: 0.7314616710587397
batch Size 469
Epoch: [112][0/107]	Time 0.200 (0.200)	Data 0.479 (0.479)	Loss 1.0108 (1.0108)	Acc@1 76.546 (76.546)	Acc@5 98.081 (98.081)
Epoch: [112][64/107]	Time 0.126 (0.129)	Data 0.000 (0.008)	Loss 1.0928 (1.0289)	Acc@1 75.906 (76.267)	Acc@5 97.868 (98.396)
Max memory in training epoch: 67.0650368
lr: 0.7314616710587397
1
Epoche:113/115; Lr: 0.7314616710587397
batch Size 469
Epoch: [113][0/107]	Time 0.199 (0.199)	Data 0.531 (0.531)	Loss 1.0296 (1.0296)	Acc@1 75.906 (75.906)	Acc@5 98.934 (98.934)
Epoch: [113][64/107]	Time 0.104 (0.130)	Data 0.000 (0.008)	Loss 1.0336 (1.0562)	Acc@1 74.840 (75.362)	Acc@5 99.787 (98.189)
Max memory in training epoch: 67.0650368
Drin!!
old memory: 646572544
new memory: 670650368
Faktor: 1.0372391686337983
New batch Size kleiner 486!!
lr: 0.7314616710587397
1
Epoche:114/115; Lr: 0.7314616710587397
batch Size 486
Epoch: [114][0/107]	Time 0.230 (0.230)	Data 0.526 (0.526)	Loss 1.0085 (1.0085)	Acc@1 76.333 (76.333)	Acc@5 97.655 (97.655)
Epoch: [114][64/107]	Time 0.126 (0.135)	Data 0.000 (0.008)	Loss 1.0490 (1.0277)	Acc@1 74.627 (76.133)	Acc@5 98.721 (98.537)
Max memory in training epoch: 67.0650368
lr: 0.7314616710587397
1
Epoche:115/115; Lr: 0.7314616710587397
batch Size 486
Epoch: [115][0/107]	Time 0.214 (0.214)	Data 0.443 (0.443)	Loss 1.0396 (1.0396)	Acc@1 75.906 (75.906)	Acc@5 98.721 (98.721)
Epoch: [115][64/107]	Time 0.160 (0.132)	Data 0.000 (0.007)	Loss 0.9269 (1.0194)	Acc@1 78.891 (76.375)	Acc@5 98.934 (98.455)
Max memory in training epoch: 67.0650368
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 177700 ; 188238 ; 0.9440176797458536
[INFO] Storing checkpoint...
  23.12
Max memory: 67.0650368
 14.565s  j: 116 bis 120
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8117
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 116
Max memory: 0.0763392
lr: 1.3886342661505762
1
Epoche:116/120; Lr: 1.3886342661505762
batch Size 486
Epoch: [116][0/103]	Time 0.251 (0.251)	Data 0.540 (0.540)	Loss 1.0853 (1.0853)	Acc@1 73.663 (73.663)	Acc@5 98.148 (98.148)
Epoch: [116][64/103]	Time 0.129 (0.131)	Data 0.000 (0.009)	Loss 1.3963 (1.2643)	Acc@1 67.078 (69.459)	Acc@5 96.914 (97.221)
Max memory in training epoch: 67.6942336
lr: 1.3886342661505762
1
Epoche:117/120; Lr: 1.3886342661505762
batch Size 486
Epoch: [117][0/103]	Time 0.175 (0.175)	Data 0.494 (0.494)	Loss 1.1698 (1.1698)	Acc@1 74.074 (74.074)	Acc@5 96.502 (96.502)
Epoch: [117][64/103]	Time 0.132 (0.132)	Data 0.000 (0.008)	Loss 1.1872 (1.2434)	Acc@1 71.605 (70.453)	Acc@5 97.531 (97.452)
Max memory in training epoch: 67.5385856
lr: 1.3886342661505762
1
Epoche:118/120; Lr: 1.3886342661505762
batch Size 486
Epoch: [118][0/103]	Time 0.185 (0.185)	Data 0.441 (0.441)	Loss 1.2804 (1.2804)	Acc@1 68.519 (68.519)	Acc@5 96.091 (96.091)
Epoch: [118][64/103]	Time 0.126 (0.133)	Data 0.000 (0.007)	Loss 1.2354 (1.2547)	Acc@1 71.193 (69.503)	Acc@5 97.119 (97.306)
Max memory in training epoch: 67.5385856
Drin!!
old memory: 670650368
new memory: 675385856
Faktor: 1.0070610383978795
New batch Size kleiner 489!!
lr: 1.3886342661505762
1
Epoche:119/120; Lr: 1.3886342661505762
batch Size 489
Epoch: [119][0/103]	Time 0.174 (0.174)	Data 0.405 (0.405)	Loss 1.1929 (1.1929)	Acc@1 71.399 (71.399)	Acc@5 97.119 (97.119)
Epoch: [119][64/103]	Time 0.119 (0.135)	Data 0.000 (0.007)	Loss 1.1531 (1.2293)	Acc@1 73.045 (69.959)	Acc@5 97.531 (97.468)
Max memory in training epoch: 67.5385856
lr: 1.3886342661505762
1
Epoche:120/120; Lr: 1.3886342661505762
batch Size 489
Epoch: [120][0/103]	Time 0.216 (0.216)	Data 0.490 (0.490)	Loss 1.1318 (1.1318)	Acc@1 71.605 (71.605)	Acc@5 98.148 (98.148)
Epoch: [120][64/103]	Time 0.106 (0.131)	Data 0.000 (0.008)	Loss 1.1479 (1.1969)	Acc@1 71.605 (70.244)	Acc@5 97.737 (97.604)
Max memory in training epoch: 67.5385856
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 161822 ; 177700 ; 0.9106471581316826
[INFO] Storing checkpoint...
  25.1
Max memory: 67.5385856
 14.109s  j: 121 bis 125
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4031
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 121
Max memory: 0.0700928
lr: 2.6525084224516866
1
Epoche:121/125; Lr: 2.6525084224516866
batch Size 489
Epoch: [121][0/103]	Time 0.283 (0.283)	Data 0.622 (0.622)	Loss 1.1627 (1.1627)	Acc@1 72.597 (72.597)	Acc@5 96.319 (96.319)
Epoch: [121][64/103]	Time 0.116 (0.131)	Data 0.000 (0.010)	Loss 1.5737 (1.5566)	Acc@1 61.350 (60.176)	Acc@5 95.501 (95.039)
Max memory in training epoch: 66.8804096
lr: 2.6525084224516866
1
Epoche:122/125; Lr: 2.6525084224516866
batch Size 489
Epoch: [122][0/103]	Time 0.239 (0.239)	Data 0.504 (0.504)	Loss 1.8462 (1.8462)	Acc@1 49.080 (49.080)	Acc@5 90.389 (90.389)
Epoch: [122][64/103]	Time 0.195 (0.136)	Data 0.000 (0.008)	Loss 1.5010 (1.5204)	Acc@1 59.509 (60.047)	Acc@5 96.524 (95.287)
Max memory in training epoch: 66.8804096
lr: 2.6525084224516866
1
Epoche:123/125; Lr: 2.6525084224516866
batch Size 489
Epoch: [123][0/103]	Time 0.330 (0.330)	Data 0.444 (0.444)	Loss 1.5672 (1.5672)	Acc@1 59.714 (59.714)	Acc@5 94.683 (94.683)
Epoch: [123][64/103]	Time 0.134 (0.136)	Data 0.000 (0.007)	Loss 1.3807 (1.4896)	Acc@1 64.213 (59.758)	Acc@5 96.728 (95.221)
Max memory in training epoch: 66.8804096
Drin!!
old memory: 675385856
new memory: 668804096
Faktor: 0.9902548151674648
New batch Size größer 489!!
lr: 2.6525084224516866
1
Epoche:124/125; Lr: 2.6525084224516866
batch Size 489
Epoch: [124][0/103]	Time 0.196 (0.196)	Data 0.479 (0.479)	Loss 1.5126 (1.5126)	Acc@1 59.918 (59.918)	Acc@5 95.297 (95.297)
Epoch: [124][64/103]	Time 0.138 (0.134)	Data 0.000 (0.008)	Loss 1.4109 (1.4519)	Acc@1 60.123 (60.337)	Acc@5 95.092 (95.347)
Max memory in training epoch: 66.8804096
lr: 2.6525084224516866
1
Epoche:125/125; Lr: 2.6525084224516866
batch Size 489
Epoch: [125][0/103]	Time 0.233 (0.233)	Data 0.511 (0.511)	Loss 1.4608 (1.4608)	Acc@1 60.327 (60.327)	Acc@5 94.683 (94.683)
Epoch: [125][64/103]	Time 0.132 (0.134)	Data 0.000 (0.008)	Loss 1.4689 (1.4766)	Acc@1 59.305 (58.399)	Acc@5 94.479 (95.155)
Max memory in training epoch: 66.8804096
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 157492 ; 161822 ; 0.9732422043974244
[INFO] Storing checkpoint...
  17.48
Max memory: 66.8804096
 14.286s  j: 126 bis 130
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9476
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 126
Max memory: 0.068352
lr: 5.06670554132373
1
Epoche:126/130; Lr: 5.06670554132373
batch Size 489
Epoch: [126][0/103]	Time 0.320 (0.320)	Data 0.537 (0.537)	Loss 1.6774 (1.6774)	Acc@1 56.442 (56.442)	Acc@5 92.434 (92.434)
Epoch: [126][64/103]	Time 0.159 (0.133)	Data 0.000 (0.009)	Loss 2.7670 (2.3347)	Acc@1 21.063 (28.844)	Acc@5 75.051 (80.777)
Max memory in training epoch: 66.3560704
lr: 5.06670554132373
1
Epoche:127/130; Lr: 5.06670554132373
batch Size 489
Epoch: [127][0/103]	Time 0.208 (0.208)	Data 0.552 (0.552)	Loss 2.7510 (2.7510)	Acc@1 10.838 (10.838)	Acc@5 54.806 (54.806)
Epoch: [127][64/103]	Time 0.162 (0.131)	Data 0.000 (0.009)	Loss 2.3847 (2.3464)	Acc@1 16.360 (18.021)	Acc@5 67.894 (71.024)
Max memory in training epoch: 65.9800064
lr: 5.06670554132373
1
Epoche:128/130; Lr: 5.06670554132373
batch Size 489
Epoch: [128][0/103]	Time 0.198 (0.198)	Data 0.448 (0.448)	Loss 2.6397 (2.6397)	Acc@1 10.020 (10.020)	Acc@5 49.693 (49.693)
Epoch: [128][64/103]	Time 0.200 (0.135)	Data 0.000 (0.007)	Loss 2.4684 (2.5841)	Acc@1 9.202 (13.896)	Acc@5 49.693 (59.978)
Max memory in training epoch: 65.9800064
Drin!!
old memory: 668804096
new memory: 659800064
Faktor: 0.9865371159449359
New batch Size größer 496!!
lr: 5.06670554132373
1
Epoche:129/130; Lr: 5.06670554132373
batch Size 496
Epoch: [129][0/103]	Time 0.212 (0.212)	Data 0.513 (0.513)	Loss 2.2122 (2.2122)	Acc@1 19.632 (19.632)	Acc@5 68.916 (68.916)
Epoch: [129][64/103]	Time 0.132 (0.131)	Data 0.000 (0.008)	Loss 2.6428 (2.3579)	Acc@1 10.429 (15.586)	Acc@5 51.329 (64.225)
Max memory in training epoch: 65.9800064
lr: 5.06670554132373
1
Epoche:130/130; Lr: 5.06670554132373
batch Size 496
Epoch: [130][0/103]	Time 0.165 (0.165)	Data 0.408 (0.408)	Loss 2.4884 (2.4884)	Acc@1 16.973 (16.973)	Acc@5 63.190 (63.190)
Epoch: [130][64/103]	Time 0.131 (0.130)	Data 0.000 (0.007)	Loss 7.0077 (10.3079)	Acc@1 16.564 (13.893)	Acc@5 70.348 (61.564)
Max memory in training epoch: 65.9800064
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 154028 ; 157492 ; 0.9780052320117847
[INFO] Storing checkpoint...
  13.23
Max memory: 65.9800064
 13.712s  j: 131 bis 135
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 3485
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0669696
lr: 9.816741986314726
1
Epoche:131/135; Lr: 9.816741986314726
batch Size 496
Epoch: [131][0/101]	Time 0.271 (0.271)	Data 0.531 (0.531)	Loss 3.5393 (3.5393)	Acc@1 14.113 (14.113)	Acc@5 63.105 (63.105)
Epoch: [131][64/101]	Time 0.125 (0.134)	Data 0.000 (0.008)	Loss 74915420045312.0000 (3607307764678.2163)	Acc@1 8.871 (11.117)	Acc@5 48.790 (52.838)
Max memory in training epoch: 67.5930112
lr: 9.816741986314726
1
Epoche:132/135; Lr: 9.816741986314726
batch Size 496
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [132][0/101]	Time 0.192 (0.192)	Data 0.467 (0.467)	Loss 16014425194496.0000 (16014425194496.0000)	Acc@1 7.661 (7.661)	Acc@5 45.565 (45.565)
Epoch: [132][64/101]	Time 0.133 (0.131)	Data 0.000 (0.007)	Loss 550389284864.0000 (91057365316338672.0000)	Acc@1 8.871 (10.124)	Acc@5 51.815 (49.910)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 136 bis 140
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9323
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0669696
lr: 9.816741986314726
1
Epoche:131/135; Lr: 9.816741986314726
batch Size 496
Epoch: [131][0/101]	Time 0.279 (0.279)	Data 0.500 (0.500)	Loss 3.5162 (3.5162)	Acc@1 16.331 (16.331)	Acc@5 64.315 (64.315)
Epoch: [131][64/101]	Time 0.122 (0.130)	Data 0.000 (0.008)	Loss 2.8035 (2.8835)	Acc@1 13.105 (10.233)	Acc@5 54.839 (50.639)
Max memory in training epoch: 67.5930112
lr: 9.816741986314726
1
Epoche:132/135; Lr: 9.816741986314726
batch Size 496
Epoch: [132][0/101]	Time 0.209 (0.209)	Data 0.554 (0.554)	Loss 2.5294 (2.5294)	Acc@1 9.073 (9.073)	Acc@5 50.605 (50.605)
Epoch: [132][64/101]	Time 0.113 (0.133)	Data 0.000 (0.009)	Loss 2.6461 (2.8028)	Acc@1 10.282 (10.022)	Acc@5 50.806 (50.211)
Max memory in training epoch: 67.5497984
lr: 9.816741986314726
1
Epoche:133/135; Lr: 9.816741986314726
batch Size 496
Epoch: [133][0/101]	Time 0.158 (0.158)	Data 0.620 (0.620)	Loss 2.7082 (2.7082)	Acc@1 10.484 (10.484)	Acc@5 51.210 (51.210)
Epoch: [133][64/101]	Time 0.139 (0.131)	Data 0.000 (0.010)	Loss 2.6510 (2.7404)	Acc@1 9.476 (11.300)	Acc@5 49.597 (53.496)
Max memory in training epoch: 67.5497984
Drin!!
old memory: 659800064
new memory: 675497984
Faktor: 1.023791934642795
New batch Size kleiner 507!!
lr: 9.816741986314726
1
Epoche:134/135; Lr: 9.816741986314726
batch Size 507
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [134][0/101]	Time 0.171 (0.171)	Data 0.497 (0.497)	Loss 25604.0117 (25604.0117)	Acc@1 11.290 (11.290)	Acc@5 47.581 (47.581)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 141 bis 145
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6194
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0669696
lr: 9.816741986314726
1
Epoche:131/135; Lr: 9.816741986314726
batch Size 496
Epoch: [131][0/101]	Time 0.251 (0.251)	Data 0.526 (0.526)	Loss 3.5363 (3.5363)	Acc@1 15.323 (15.323)	Acc@5 60.282 (60.282)
Epoch: [131][64/101]	Time 0.116 (0.134)	Data 0.000 (0.008)	Loss 2.7171 (2.9920)	Acc@1 10.685 (11.172)	Acc@5 51.008 (53.046)
Max memory in training epoch: 67.5930112
lr: 9.816741986314726
1
Epoche:132/135; Lr: 9.816741986314726
batch Size 496
Epoch: [132][0/101]	Time 0.197 (0.197)	Data 0.533 (0.533)	Loss 6.6207 (6.6207)	Acc@1 9.476 (9.476)	Acc@5 47.177 (47.177)
Epoch: [132][64/101]	Time 0.119 (0.134)	Data 0.000 (0.009)	Loss 4.2952 (4.9300)	Acc@1 8.065 (10.214)	Acc@5 49.597 (50.357)
Max memory in training epoch: 67.5497984
lr: 9.816741986314726
1
Epoche:133/135; Lr: 9.816741986314726
batch Size 496
Epoch: [133][0/101]	Time 0.192 (0.192)	Data 0.456 (0.456)	Loss 8.1752 (8.1752)	Acc@1 11.895 (11.895)	Acc@5 50.403 (50.403)
Epoch: [133][64/101]	Time 0.133 (0.134)	Data 0.000 (0.007)	Loss 199.7208 (201.4586)	Acc@1 9.073 (9.566)	Acc@5 48.387 (49.522)
Max memory in training epoch: 67.5497984
Drin!!
old memory: 659800064
new memory: 675497984
Faktor: 1.023791934642795
New batch Size kleiner 507!!
lr: 9.816741986314726
1
Epoche:134/135; Lr: 9.816741986314726
batch Size 507
Epoch: [134][0/101]	Time 0.216 (0.216)	Data 0.502 (0.502)	Loss 27.1899 (27.1899)	Acc@1 9.274 (9.274)	Acc@5 54.032 (54.032)
Epoch: [134][64/101]	Time 0.190 (0.133)	Data 0.000 (0.008)	Loss 11.9722 (338.0631)	Acc@1 9.677 (9.953)	Acc@5 46.976 (49.311)
Max memory in training epoch: 67.5497984
lr: 9.816741986314726
1
Epoche:135/135; Lr: 9.816741986314726
batch Size 507
Epoch: [135][0/101]	Time 0.194 (0.194)	Data 0.516 (0.516)	Loss 7393.2559 (7393.2559)	Acc@1 9.879 (9.879)	Acc@5 46.169 (46.169)
Epoch: [135][64/101]	Time 0.131 (0.132)	Data 0.000 (0.008)	Loss 8497.7529 (549.6495)	Acc@1 9.677 (10.226)	Acc@5 47.984 (49.724)
Max memory in training epoch: 67.5497984
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 152874 ; 154028 ; 0.9925078557145454
[INFO] Storing checkpoint...
  10.0
Max memory: 67.5497984
 13.833s  j: 146 bis 150
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 3898
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.0664576
lr: 19.44175073070924
1
Epoche:136/140; Lr: 19.44175073070924
batch Size 507
Epoch: [136][0/99]	Time 0.267 (0.267)	Data 0.596 (0.596)	Loss 111552.4375 (111552.4375)	Acc@1 9.467 (9.467)	Acc@5 60.355 (60.355)
Epoch: [136][64/99]	Time 0.136 (0.134)	Data 0.000 (0.009)	Loss 222555340275712.0000 (139252599082496064.0000)	Acc@1 12.032 (10.065)	Acc@5 51.874 (50.038)
Max memory in training epoch: 68.3042304
lr: 19.44175073070924
1
Epoche:137/140; Lr: 19.44175073070924
batch Size 507
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [137][0/99]	Time 0.315 (0.315)	Data 0.515 (0.515)	Loss 60963717906432.0000 (60963717906432.0000)	Acc@1 9.270 (9.270)	Acc@5 53.452 (53.452)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 151 bis 155
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 3993
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.0664576
lr: 19.44175073070924
1
Epoche:136/140; Lr: 19.44175073070924
batch Size 507
Epoch: [136][0/99]	Time 0.257 (0.257)	Data 0.491 (0.491)	Loss 103058.1328 (103058.1328)	Acc@1 9.862 (9.862)	Acc@5 55.621 (55.621)
Epoch: [136][64/99]	Time 0.128 (0.132)	Data 0.000 (0.008)	Loss 3935834112.0000 (12902056745.9136)	Acc@1 12.426 (10.208)	Acc@5 54.241 (49.901)
Max memory in training epoch: 68.3042304
lr: 19.44175073070924
1
Epoche:137/140; Lr: 19.44175073070924
batch Size 507
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [137][0/99]	Time 0.168 (0.168)	Data 0.444 (0.444)	Loss 14234987528192.0000 (14234987528192.0000)	Acc@1 7.101 (7.101)	Acc@5 49.507 (49.507)
Epoch: [137][64/99]	Time 0.128 (0.131)	Data 0.000 (0.007)	Loss 472052662272.0000 (681425433106443392.0000)	Acc@1 9.270 (9.671)	Acc@5 51.282 (49.856)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 156 bis 160
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5626
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.0664576
lr: 19.44175073070924
1
Epoche:136/140; Lr: 19.44175073070924
batch Size 507
Epoch: [136][0/99]	Time 0.261 (0.261)	Data 0.535 (0.535)	Loss 115697.5312 (115697.5312)	Acc@1 9.665 (9.665)	Acc@5 56.213 (56.213)
Epoch: [136][64/99]	Time 0.134 (0.135)	Data 0.000 (0.008)	Loss 1746741760.0000 (38712394.4377)	Acc@1 11.045 (9.971)	Acc@5 47.140 (49.798)
Max memory in training epoch: 68.3042304
lr: 19.44175073070924
1
Epoche:137/140; Lr: 19.44175073070924
batch Size 507
Epoch: [137][0/99]	Time 0.227 (0.227)	Data 0.485 (0.485)	Loss 227306480074752.0000 (227306480074752.0000)	Acc@1 11.045 (11.045)	Acc@5 49.310 (49.310)
Epoch: [137][64/99]	Time 0.130 (0.132)	Data 0.000 (0.008)	Loss 184975599964520448.0000 (22003983495303249920.0000)	Acc@1 10.059 (10.074)	Acc@5 53.254 (50.184)
Max memory in training epoch: 68.3042304
lr: 19.44175073070924
1
Epoche:138/140; Lr: 19.44175073070924
batch Size 507
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [138][0/99]	Time 0.259 (0.259)	Data 0.555 (0.555)	Loss 94733622782017404928.0000 (94733622782017404928.0000)	Acc@1 10.059 (10.059)	Acc@5 47.535 (47.535)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 161 bis 165
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1790
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.0664576
lr: 19.44175073070924
1
Epoche:136/140; Lr: 19.44175073070924
batch Size 507
Epoch: [136][0/99]	Time 0.348 (0.348)	Data 0.528 (0.528)	Loss 114857.1719 (114857.1719)	Acc@1 8.876 (8.876)	Acc@5 57.594 (57.594)
Epoch: [136][64/99]	Time 0.132 (0.131)	Data 0.000 (0.008)	Loss 4841123840.0000 (24996040489655600.0000)	Acc@1 11.243 (9.944)	Acc@5 50.690 (49.865)
Max memory in training epoch: 68.3042304
lr: 19.44175073070924
1
Epoche:137/140; Lr: 19.44175073070924
batch Size 507
Epoch: [137][0/99]	Time 0.228 (0.228)	Data 0.459 (0.459)	Loss inf (inf)	Acc@1 9.467 (9.467)	Acc@5 52.465 (52.465)
Epoch: [137][64/99]	Time 0.119 (0.130)	Data 0.000 (0.007)	Loss 776928453923438592.0000 (inf)	Acc@1 9.467 (10.059)	Acc@5 52.663 (49.995)
Max memory in training epoch: 68.3042304
lr: 19.44175073070924
1
Epoche:138/140; Lr: 19.44175073070924
batch Size 507
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [138][0/99]	Time 0.210 (0.210)	Data 0.459 (0.459)	Loss 10646355705856.0000 (10646355705856.0000)	Acc@1 11.045 (11.045)	Acc@5 53.254 (53.254)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 166 bis 170
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 281
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.0664576
lr: 19.44175073070924
1
Epoche:136/140; Lr: 19.44175073070924
batch Size 507
Epoch: [136][0/99]	Time 0.284 (0.284)	Data 0.502 (0.502)	Loss 111582.7734 (111582.7734)	Acc@1 10.454 (10.454)	Acc@5 56.805 (56.805)
Epoch: [136][64/99]	Time 0.139 (0.133)	Data 0.000 (0.008)	Loss 16085890048.0000 (22156061511.6152)	Acc@1 11.045 (10.026)	Acc@5 50.690 (49.871)
Max memory in training epoch: 68.3042304
lr: 19.44175073070924
1
Epoche:137/140; Lr: 19.44175073070924
batch Size 507
Epoch: [137][0/99]	Time 0.214 (0.214)	Data 0.525 (0.525)	Loss 1373410816.0000 (1373410816.0000)	Acc@1 9.467 (9.467)	Acc@5 50.888 (50.888)
Epoch: [137][64/99]	Time 0.117 (0.132)	Data 0.000 (0.008)	Loss 12154937802752.0000 (2046165240487160.7500)	Acc@1 8.876 (9.947)	Acc@5 49.704 (50.329)
Max memory in training epoch: 68.3042304
lr: 19.44175073070924
1
Epoche:138/140; Lr: 19.44175073070924
batch Size 507
Epoch: [138][0/99]	Time 0.216 (0.216)	Data 0.610 (0.610)	Loss 283431746630844416.0000 (283431746630844416.0000)	Acc@1 11.637 (11.637)	Acc@5 52.860 (52.860)
Epoch: [138][64/99]	Time 0.131 (0.132)	Data 0.000 (0.010)	Loss 513370981260066816.0000 (2761379412190779559378944.0000)	Acc@1 13.412 (10.284)	Acc@5 48.915 (50.047)
Max memory in training epoch: 68.3042304
Drin!!
old memory: 675497984
new memory: 683042304
Faktor: 1.0111685307413145
New batch Size kleiner 512!!
lr: 19.44175073070924
1
Epoche:139/140; Lr: 19.44175073070924
batch Size 512
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [139][0/99]	Time 0.302 (0.302)	Data 0.444 (0.444)	Loss 3360581222400.0000 (3360581222400.0000)	Acc@1 10.059 (10.059)	Acc@5 49.901 (49.901)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 171 bis 175
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8450
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.0664576
lr: 19.44175073070924
1
Epoche:136/140; Lr: 19.44175073070924
batch Size 507
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [136][0/99]	Time 0.255 (0.255)	Data 0.501 (0.501)	Loss 114170.6641 (114170.6641)	Acc@1 9.665 (9.665)	Acc@5 57.002 (57.002)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 176 bis 180
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5988
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.0664576
lr: 19.44175073070924
1
Epoche:136/140; Lr: 19.44175073070924
batch Size 507
Epoch: [136][0/99]	Time 0.316 (0.316)	Data 0.454 (0.454)	Loss 113637.9922 (113637.9922)	Acc@1 10.059 (10.059)	Acc@5 60.552 (60.552)
Epoch: [136][64/99]	Time 0.143 (0.133)	Data 0.001 (0.007)	Loss 31373824.0000 (8183049.8571)	Acc@1 9.665 (10.132)	Acc@5 53.057 (50.481)
Max memory in training epoch: 68.3042304
lr: 19.44175073070924
1
Epoche:137/140; Lr: 19.44175073070924
batch Size 507
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [137][0/99]	Time 0.198 (0.198)	Data 0.466 (0.466)	Loss 2983710069679456256.0000 (2983710069679456256.0000)	Acc@1 8.679 (8.679)	Acc@5 48.126 (48.126)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
BSize 4
j: 0 bis 5
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8822
Files already downloaded and verified
numoFStages: 3
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
Max memory: 0.202496
batch_size berechnet: 250;389.1 ; lr: 0.1
lr: 0.09765625
1
Epoche:1/5; Lr: 0.09765625
batch Size 250
Epoch: [1][0/200]	Time 0.222 (0.222)	Data 0.434 (0.434)	Loss 3.3586 (3.3586)	Acc@1 10.400 (10.400)	Acc@5 50.000 (50.000)
Epoch: [1][64/200]	Time 0.164 (0.156)	Data 0.000 (0.007)	Loss 2.6331 (2.8193)	Acc@1 28.800 (24.997)	Acc@5 84.800 (78.474)
Epoch: [1][128/200]	Time 0.151 (0.157)	Data 0.000 (0.004)	Loss 2.3243 (2.6544)	Acc@1 39.200 (30.378)	Acc@5 92.800 (83.157)
Epoch: [1][192/200]	Time 0.155 (0.156)	Data 0.000 (0.002)	Loss 2.1130 (2.5274)	Acc@1 48.000 (34.742)	Acc@5 93.600 (85.961)
Max memory in training epoch: 66.4657408
lr: 0.09765625
1
Epoche:2/5; Lr: 0.09765625
batch Size 250
Epoch: [2][0/200]	Time 0.185 (0.185)	Data 0.378 (0.378)	Loss 2.0302 (2.0302)	Acc@1 52.000 (52.000)	Acc@5 92.000 (92.000)
Epoch: [2][64/200]	Time 0.152 (0.155)	Data 0.000 (0.006)	Loss 2.0805 (2.0575)	Acc@1 45.200 (50.566)	Acc@5 96.400 (93.938)
Epoch: [2][128/200]	Time 0.166 (0.153)	Data 0.000 (0.003)	Loss 1.9051 (1.9938)	Acc@1 56.000 (52.602)	Acc@5 96.000 (94.338)
Epoch: [2][192/200]	Time 0.134 (0.153)	Data 0.000 (0.002)	Loss 1.6755 (1.9241)	Acc@1 63.200 (55.005)	Acc@5 97.200 (94.852)
Max memory in training epoch: 66.0135424
lr: 0.09765625
1
Epoche:3/5; Lr: 0.09765625
batch Size 250
Epoch: [3][0/200]	Time 0.262 (0.262)	Data 0.444 (0.444)	Loss 1.6060 (1.6060)	Acc@1 66.800 (66.800)	Acc@5 98.000 (98.000)
Epoch: [3][64/200]	Time 0.137 (0.156)	Data 0.000 (0.007)	Loss 1.5677 (1.6469)	Acc@1 69.600 (63.532)	Acc@5 97.600 (96.560)
Epoch: [3][128/200]	Time 0.146 (0.155)	Data 0.000 (0.004)	Loss 1.4749 (1.6169)	Acc@1 67.600 (64.260)	Acc@5 97.200 (96.769)
Epoch: [3][192/200]	Time 0.138 (0.152)	Data 0.000 (0.002)	Loss 1.6514 (1.5764)	Acc@1 60.800 (65.503)	Acc@5 98.000 (96.947)
Max memory in training epoch: 66.0135424
Drin!!
old memory: 0
new memory: 660135424
lr: 0.09765625
1
Epoche:4/5; Lr: 0.09765625
batch Size 250
Epoch: [4][0/200]	Time 0.227 (0.227)	Data 0.357 (0.357)	Loss 1.4487 (1.4487)	Acc@1 67.600 (67.600)	Acc@5 98.800 (98.800)
Epoch: [4][64/200]	Time 0.158 (0.158)	Data 0.000 (0.006)	Loss 1.2485 (1.3863)	Acc@1 72.000 (70.745)	Acc@5 97.600 (97.920)
Epoch: [4][128/200]	Time 0.164 (0.156)	Data 0.000 (0.003)	Loss 1.3352 (1.3645)	Acc@1 69.600 (71.135)	Acc@5 99.200 (97.941)
Epoch: [4][192/200]	Time 0.149 (0.157)	Data 0.000 (0.002)	Loss 1.2680 (1.3403)	Acc@1 72.800 (71.867)	Acc@5 98.800 (97.954)
Max memory in training epoch: 66.0135424
lr: 0.09765625
1
Epoche:5/5; Lr: 0.09765625
batch Size 250
Epoch: [5][0/200]	Time 0.186 (0.186)	Data 0.433 (0.433)	Loss 1.0939 (1.0939)	Acc@1 80.800 (80.800)	Acc@5 98.400 (98.400)
Epoch: [5][64/200]	Time 0.153 (0.156)	Data 0.000 (0.007)	Loss 1.2437 (1.2225)	Acc@1 75.200 (74.714)	Acc@5 97.600 (98.302)
Epoch: [5][128/200]	Time 0.138 (0.156)	Data 0.000 (0.004)	Loss 1.2825 (1.2050)	Acc@1 71.600 (74.952)	Acc@5 96.800 (98.366)
Epoch: [5][192/200]	Time 0.139 (0.156)	Data 0.000 (0.002)	Loss 1.0633 (1.1936)	Acc@1 76.800 (75.148)	Acc@5 99.200 (98.354)
Max memory in training epoch: 66.0135424
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  67.29
Max memory: 103.3835008
 31.764s  ./run_BSize.sh: 7: ./run_BSize.sh: cannot open j: 6 bis 10: No such file
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9150
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 6
Max memory: 0.202496
lr: 0.095367431640625
1
Epoche:6/10; Lr: 0.095367431640625
batch Size 250
Epoch: [6][0/200]	Time 0.295 (0.295)	Data 0.378 (0.378)	Loss 1.0940 (1.0940)	Acc@1 74.000 (74.000)	Acc@5 99.600 (99.600)
Epoch: [6][64/200]	Time 0.152 (0.159)	Data 0.000 (0.006)	Loss 1.0604 (1.1183)	Acc@1 77.600 (76.849)	Acc@5 99.200 (98.480)
Epoch: [6][128/200]	Time 0.154 (0.156)	Data 0.000 (0.003)	Loss 1.0803 (1.1050)	Acc@1 76.000 (77.023)	Acc@5 97.600 (98.580)
Epoch: [6][192/200]	Time 0.145 (0.155)	Data 0.000 (0.002)	Loss 1.1289 (1.0899)	Acc@1 75.600 (77.355)	Acc@5 98.800 (98.651)
Max memory in training epoch: 66.4656384
lr: 0.095367431640625
1
Epoche:7/10; Lr: 0.095367431640625
batch Size 250
Epoch: [7][0/200]	Time 0.223 (0.223)	Data 0.446 (0.446)	Loss 1.0907 (1.0907)	Acc@1 74.400 (74.400)	Acc@5 98.800 (98.800)
Epoch: [7][64/200]	Time 0.169 (0.155)	Data 0.000 (0.007)	Loss 0.9361 (1.0330)	Acc@1 82.400 (78.209)	Acc@5 99.200 (98.837)
Epoch: [7][128/200]	Time 0.167 (0.158)	Data 0.000 (0.004)	Loss 1.0947 (1.0241)	Acc@1 76.400 (78.496)	Acc@5 98.000 (98.784)
Epoch: [7][192/200]	Time 0.180 (0.158)	Data 0.000 (0.002)	Loss 0.9318 (1.0214)	Acc@1 82.000 (78.435)	Acc@5 99.600 (98.827)
Max memory in training epoch: 66.01344
lr: 0.095367431640625
1
Epoche:8/10; Lr: 0.095367431640625
batch Size 250
Epoch: [8][0/200]	Time 0.211 (0.211)	Data 0.371 (0.371)	Loss 0.9223 (0.9223)	Acc@1 83.600 (83.600)	Acc@5 98.800 (98.800)
Epoch: [8][64/200]	Time 0.152 (0.161)	Data 0.000 (0.006)	Loss 1.1022 (0.9727)	Acc@1 74.800 (79.760)	Acc@5 98.800 (98.843)
Epoch: [8][128/200]	Time 0.146 (0.160)	Data 0.000 (0.003)	Loss 0.9881 (0.9662)	Acc@1 79.600 (79.752)	Acc@5 99.600 (98.878)
Epoch: [8][192/200]	Time 0.167 (0.159)	Data 0.000 (0.002)	Loss 1.0767 (0.9682)	Acc@1 75.200 (79.641)	Acc@5 98.800 (98.904)
Max memory in training epoch: 66.01344
Drin!!
old memory: 660135424
new memory: 660134400
Faktor: 0.9999984488031353
New batch Size größer 253!!
lr: 0.095367431640625
1
Epoche:9/10; Lr: 0.095367431640625
batch Size 253
Epoch: [9][0/200]	Time 0.215 (0.215)	Data 0.519 (0.519)	Loss 0.8984 (0.8984)	Acc@1 80.400 (80.400)	Acc@5 99.200 (99.200)
Epoch: [9][64/200]	Time 0.161 (0.160)	Data 0.000 (0.008)	Loss 1.0001 (0.9267)	Acc@1 79.200 (80.689)	Acc@5 98.400 (99.089)
Epoch: [9][128/200]	Time 0.155 (0.160)	Data 0.000 (0.004)	Loss 0.9057 (0.9326)	Acc@1 79.600 (80.490)	Acc@5 99.200 (99.036)
Epoch: [9][192/200]	Time 0.169 (0.159)	Data 0.000 (0.003)	Loss 0.8273 (0.9276)	Acc@1 85.600 (80.599)	Acc@5 99.600 (99.005)
Max memory in training epoch: 66.01344
lr: 0.095367431640625
1
Epoche:10/10; Lr: 0.095367431640625
batch Size 253
Epoch: [10][0/200]	Time 0.200 (0.200)	Data 0.518 (0.518)	Loss 0.9288 (0.9288)	Acc@1 81.200 (81.200)	Acc@5 99.600 (99.600)
Epoch: [10][64/200]	Time 0.144 (0.155)	Data 0.000 (0.008)	Loss 0.8204 (0.8947)	Acc@1 85.200 (81.471)	Acc@5 99.200 (99.052)
Epoch: [10][128/200]	Time 0.158 (0.155)	Data 0.000 (0.004)	Loss 0.9867 (0.8930)	Acc@1 78.000 (81.491)	Acc@5 98.800 (99.073)
Epoch: [10][192/200]	Time 0.172 (0.157)	Data 0.000 (0.003)	Loss 0.8852 (0.8976)	Acc@1 80.400 (81.345)	Acc@5 99.600 (99.049)
Max memory in training epoch: 66.01344
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  73.29
Max memory: 103.3833984
 32.023s  j: 11 bis 15
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5384
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 11
Max memory: 0.202496
lr: 0.09424984455108643
1
Epoche:11/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [11][0/198]	Time 0.304 (0.304)	Data 0.329 (0.329)	Loss 0.8245 (0.8245)	Acc@1 83.399 (83.399)	Acc@5 99.605 (99.605)
Epoch: [11][64/198]	Time 0.162 (0.157)	Data 0.000 (0.005)	Loss 0.8642 (0.8676)	Acc@1 82.609 (82.451)	Acc@5 99.209 (99.143)
Epoch: [11][128/198]	Time 0.155 (0.155)	Data 0.000 (0.003)	Loss 0.8108 (0.8743)	Acc@1 84.190 (82.002)	Acc@5 99.605 (99.133)
Epoch: [11][192/198]	Time 0.128 (0.155)	Data 0.000 (0.002)	Loss 0.8043 (0.8807)	Acc@1 85.375 (81.716)	Acc@5 98.814 (99.078)
Max memory in training epoch: 66.5037312
lr: 0.09424984455108643
1
Epoche:12/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [12][0/198]	Time 0.216 (0.216)	Data 0.445 (0.445)	Loss 0.9037 (0.9037)	Acc@1 82.609 (82.609)	Acc@5 99.209 (99.209)
Epoch: [12][64/198]	Time 0.164 (0.164)	Data 0.000 (0.007)	Loss 0.7952 (0.8618)	Acc@1 87.352 (82.378)	Acc@5 99.605 (99.216)
Epoch: [12][128/198]	Time 0.165 (0.161)	Data 0.000 (0.004)	Loss 0.9047 (0.8683)	Acc@1 81.028 (82.164)	Acc@5 99.209 (99.121)
Epoch: [12][192/198]	Time 0.149 (0.160)	Data 0.000 (0.002)	Loss 0.8968 (0.8742)	Acc@1 81.818 (81.943)	Acc@5 98.814 (99.117)
Max memory in training epoch: 66.277632
lr: 0.09424984455108643
1
Epoche:13/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [13][0/198]	Time 0.175 (0.175)	Data 0.332 (0.332)	Loss 0.7832 (0.7832)	Acc@1 84.980 (84.980)	Acc@5 99.605 (99.605)
Epoch: [13][64/198]	Time 0.163 (0.155)	Data 0.000 (0.005)	Loss 0.9603 (0.8411)	Acc@1 78.656 (82.755)	Acc@5 98.024 (99.155)
Epoch: [13][128/198]	Time 0.150 (0.157)	Data 0.000 (0.003)	Loss 0.8331 (0.8439)	Acc@1 82.213 (82.624)	Acc@5 99.605 (99.157)
Epoch: [13][192/198]	Time 0.149 (0.158)	Data 0.000 (0.002)	Loss 0.8589 (0.8535)	Acc@1 82.609 (82.349)	Acc@5 99.209 (99.164)
Max memory in training epoch: 66.277632
Drin!!
old memory: 660134400
new memory: 662776320
Faktor: 1.0040020941190158
New batch Size kleiner 254!!
lr: 0.09424984455108643
1
Epoche:14/15; Lr: 0.09424984455108643
batch Size 254
Epoch: [14][0/198]	Time 0.213 (0.213)	Data 0.443 (0.443)	Loss 0.8341 (0.8341)	Acc@1 85.771 (85.771)	Acc@5 99.605 (99.605)
Epoch: [14][64/198]	Time 0.144 (0.159)	Data 0.000 (0.007)	Loss 0.8391 (0.8597)	Acc@1 84.585 (82.566)	Acc@5 98.814 (99.203)
Epoch: [14][128/198]	Time 0.158 (0.159)	Data 0.000 (0.004)	Loss 0.8714 (0.8519)	Acc@1 84.190 (82.609)	Acc@5 98.814 (99.194)
Epoch: [14][192/198]	Time 0.163 (0.158)	Data 0.000 (0.002)	Loss 0.7750 (0.8546)	Acc@1 86.166 (82.535)	Acc@5 100.000 (99.197)
Max memory in training epoch: 66.277632
lr: 0.09424984455108643
1
Epoche:15/15; Lr: 0.09424984455108643
batch Size 254
Epoch: [15][0/198]	Time 0.215 (0.215)	Data 0.369 (0.369)	Loss 0.7543 (0.7543)	Acc@1 84.980 (84.980)	Acc@5 100.000 (100.000)
Epoch: [15][64/198]	Time 0.139 (0.161)	Data 0.000 (0.006)	Loss 0.8251 (0.8171)	Acc@1 85.375 (83.855)	Acc@5 98.024 (99.282)
Epoch: [15][128/198]	Time 0.171 (0.159)	Data 0.000 (0.003)	Loss 0.7954 (0.8364)	Acc@1 83.794 (83.133)	Acc@5 98.814 (99.216)
Epoch: [15][192/198]	Time 0.143 (0.159)	Data 0.000 (0.002)	Loss 0.8654 (0.8396)	Acc@1 83.399 (83.049)	Acc@5 99.605 (99.203)
Max memory in training epoch: 66.277632
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  76.95
Max memory: 103.3833984
 31.850s  j: 16 bis 20
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 13
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 16
Max memory: 0.202496
lr: 0.09351351764053106
1
Epoche:16/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [16][0/197]	Time 0.232 (0.232)	Data 0.409 (0.409)	Loss 0.8527 (0.8527)	Acc@1 84.646 (84.646)	Acc@5 98.819 (98.819)
Epoch: [16][64/197]	Time 0.165 (0.163)	Data 0.000 (0.007)	Loss 0.7864 (0.8019)	Acc@1 85.433 (84.422)	Acc@5 98.819 (99.128)
Epoch: [16][128/197]	Time 0.152 (0.159)	Data 0.000 (0.003)	Loss 0.9187 (0.8244)	Acc@1 81.496 (83.400)	Acc@5 98.031 (99.158)
Epoch: [16][192/197]	Time 0.144 (0.158)	Data 0.000 (0.002)	Loss 0.8199 (0.8266)	Acc@1 84.646 (83.297)	Acc@5 99.606 (99.194)
Max memory in training epoch: 66.5164288
lr: 0.09351351764053106
1
Epoche:17/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [17][0/197]	Time 0.208 (0.208)	Data 0.384 (0.384)	Loss 0.7728 (0.7728)	Acc@1 85.433 (85.433)	Acc@5 99.213 (99.213)
Epoch: [17][64/197]	Time 0.163 (0.160)	Data 0.000 (0.006)	Loss 0.7871 (0.8106)	Acc@1 82.677 (84.034)	Acc@5 99.606 (99.279)
Epoch: [17][128/197]	Time 0.162 (0.159)	Data 0.000 (0.003)	Loss 0.7787 (0.8275)	Acc@1 84.252 (83.410)	Acc@5 100.000 (99.252)
Epoch: [17][192/197]	Time 0.157 (0.159)	Data 0.000 (0.002)	Loss 0.8081 (0.8294)	Acc@1 86.220 (83.391)	Acc@5 98.819 (99.223)
Max memory in training epoch: 66.365696
lr: 0.09351351764053106
1
Epoche:18/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [18][0/197]	Time 0.220 (0.220)	Data 0.347 (0.347)	Loss 0.8764 (0.8764)	Acc@1 80.709 (80.709)	Acc@5 99.606 (99.606)
Epoch: [18][64/197]	Time 0.166 (0.161)	Data 0.000 (0.006)	Loss 0.8979 (0.8329)	Acc@1 81.496 (83.319)	Acc@5 99.213 (99.194)
Epoch: [18][128/197]	Time 0.151 (0.160)	Data 0.000 (0.003)	Loss 0.7763 (0.8242)	Acc@1 83.465 (83.678)	Acc@5 99.606 (99.194)
Epoch: [18][192/197]	Time 0.169 (0.159)	Data 0.000 (0.002)	Loss 0.7975 (0.8252)	Acc@1 83.071 (83.605)	Acc@5 99.213 (99.227)
Max memory in training epoch: 66.365696
Drin!!
old memory: 662776320
new memory: 663656960
Faktor: 1.0013287137355782
New batch Size kleiner 254!!
lr: 0.09351351764053106
1
Epoche:19/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [19][0/197]	Time 0.201 (0.201)	Data 0.392 (0.392)	Loss 0.8220 (0.8220)	Acc@1 83.858 (83.858)	Acc@5 100.000 (100.000)
Epoch: [19][64/197]	Time 0.151 (0.158)	Data 0.000 (0.006)	Loss 0.7205 (0.8089)	Acc@1 86.614 (83.979)	Acc@5 100.000 (99.328)
Epoch: [19][128/197]	Time 0.195 (0.158)	Data 0.000 (0.003)	Loss 0.8062 (0.8202)	Acc@1 83.858 (83.562)	Acc@5 99.606 (99.237)
Epoch: [19][192/197]	Time 0.171 (0.159)	Data 0.000 (0.002)	Loss 0.8649 (0.8237)	Acc@1 83.071 (83.479)	Acc@5 99.606 (99.237)
Max memory in training epoch: 66.365696
lr: 0.09351351764053106
1
Epoche:20/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [20][0/197]	Time 0.217 (0.217)	Data 0.391 (0.391)	Loss 0.7827 (0.7827)	Acc@1 83.858 (83.858)	Acc@5 99.606 (99.606)
Epoch: [20][64/197]	Time 0.145 (0.162)	Data 0.000 (0.006)	Loss 0.8014 (0.8198)	Acc@1 82.677 (83.561)	Acc@5 100.000 (99.237)
Epoch: [20][128/197]	Time 0.180 (0.160)	Data 0.000 (0.003)	Loss 0.8545 (0.8128)	Acc@1 81.102 (83.870)	Acc@5 99.213 (99.264)
Epoch: [20][192/197]	Time 0.164 (0.159)	Data 0.000 (0.002)	Loss 0.9400 (0.8147)	Acc@1 78.740 (83.866)	Acc@5 98.425 (99.272)
Max memory in training epoch: 66.365696
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 486232 ; 487386 ; 0.9976322668275248
[INFO] Storing checkpoint...
  76.51
Max memory: 103.3833984
 31.923s  j: 21 bis 25
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6219
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 21
Max memory: 0.2020864
lr: 0.09278294328396441
1
Epoche:21/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [21][0/197]	Time 0.265 (0.265)	Data 0.413 (0.413)	Loss 0.7878 (0.7878)	Acc@1 85.039 (85.039)	Acc@5 99.213 (99.213)
Epoch: [21][64/197]	Time 0.160 (0.160)	Data 0.000 (0.007)	Loss 0.8634 (0.7778)	Acc@1 80.315 (85.033)	Acc@5 99.606 (99.364)
Epoch: [21][128/197]	Time 0.162 (0.159)	Data 0.000 (0.003)	Loss 0.6875 (0.7879)	Acc@1 87.795 (84.756)	Acc@5 98.819 (99.316)
Epoch: [21][192/197]	Time 0.162 (0.160)	Data 0.000 (0.002)	Loss 0.8204 (0.7986)	Acc@1 84.252 (84.444)	Acc@5 100.000 (99.284)
Max memory in training epoch: 66.5147904
lr: 0.09278294328396441
1
Epoche:22/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [22][0/197]	Time 0.249 (0.249)	Data 0.384 (0.384)	Loss 0.7739 (0.7739)	Acc@1 85.039 (85.039)	Acc@5 98.819 (98.819)
Epoch: [22][64/197]	Time 0.120 (0.158)	Data 0.000 (0.006)	Loss 0.7665 (0.8045)	Acc@1 83.071 (84.173)	Acc@5 100.000 (99.267)
Epoch: [22][128/197]	Time 0.154 (0.158)	Data 0.000 (0.003)	Loss 0.8963 (0.8184)	Acc@1 82.677 (83.703)	Acc@5 99.606 (99.252)
Epoch: [22][192/197]	Time 0.140 (0.158)	Data 0.000 (0.002)	Loss 0.7619 (0.8149)	Acc@1 84.646 (83.752)	Acc@5 99.606 (99.270)
Max memory in training epoch: 66.3640576
lr: 0.09278294328396441
1
Epoche:23/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [23][0/197]	Time 0.220 (0.220)	Data 0.355 (0.355)	Loss 0.7531 (0.7531)	Acc@1 86.614 (86.614)	Acc@5 100.000 (100.000)
Epoch: [23][64/197]	Time 0.156 (0.164)	Data 0.000 (0.006)	Loss 0.9121 (0.7893)	Acc@1 79.921 (84.942)	Acc@5 98.819 (99.419)
Epoch: [23][128/197]	Time 0.171 (0.162)	Data 0.000 (0.003)	Loss 0.7568 (0.7972)	Acc@1 87.402 (84.682)	Acc@5 99.606 (99.292)
Epoch: [23][192/197]	Time 0.168 (0.162)	Data 0.000 (0.002)	Loss 0.8099 (0.8019)	Acc@1 84.252 (84.540)	Acc@5 99.606 (99.253)
Max memory in training epoch: 66.3640576
Drin!!
old memory: 663656960
new memory: 663640576
Faktor: 0.9999753125470122
New batch Size größer 256!!
lr: 0.09278294328396441
1
Epoche:24/25; Lr: 0.09278294328396441
batch Size 256
Epoch: [24][0/197]	Time 0.218 (0.218)	Data 0.390 (0.390)	Loss 0.8117 (0.8117)	Acc@1 85.433 (85.433)	Acc@5 99.213 (99.213)
Epoch: [24][64/197]	Time 0.164 (0.160)	Data 0.000 (0.006)	Loss 0.7939 (0.7834)	Acc@1 82.677 (84.839)	Acc@5 99.213 (99.412)
Epoch: [24][128/197]	Time 0.155 (0.159)	Data 0.000 (0.003)	Loss 0.7685 (0.7874)	Acc@1 85.827 (84.521)	Acc@5 98.425 (99.384)
Epoch: [24][192/197]	Time 0.166 (0.161)	Data 0.000 (0.002)	Loss 0.8575 (0.7946)	Acc@1 83.465 (84.311)	Acc@5 99.213 (99.392)
Max memory in training epoch: 66.3640576
lr: 0.09278294328396441
1
Epoche:25/25; Lr: 0.09278294328396441
batch Size 256
Epoch: [25][0/197]	Time 0.222 (0.222)	Data 0.311 (0.311)	Loss 0.6920 (0.6920)	Acc@1 87.795 (87.795)	Acc@5 100.000 (100.000)
Epoch: [25][64/197]	Time 0.164 (0.161)	Data 0.000 (0.005)	Loss 0.7989 (0.7811)	Acc@1 84.646 (85.076)	Acc@5 99.606 (99.364)
Epoch: [25][128/197]	Time 0.154 (0.159)	Data 0.000 (0.003)	Loss 0.7800 (0.7839)	Acc@1 87.402 (84.917)	Acc@5 99.606 (99.353)
Epoch: [25][192/197]	Time 0.155 (0.157)	Data 0.000 (0.002)	Loss 0.7750 (0.7914)	Acc@1 87.008 (84.689)	Acc@5 99.213 (99.323)
Max memory in training epoch: 66.3640576
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 468336 ; 486232 ; 0.9631945244245546
[INFO] Storing checkpoint...
  73.66
Max memory: 103.3821696
 31.383s  j: 26 bis 30
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7992
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 26
Max memory: 0.1951232
lr: 0.09278294328396441
1
Epoche:26/30; Lr: 0.09278294328396441
batch Size 256
Epoch: [26][0/196]	Time 0.257 (0.257)	Data 0.392 (0.392)	Loss 0.7503 (0.7503)	Acc@1 82.812 (82.812)	Acc@5 99.609 (99.609)
Epoch: [26][64/196]	Time 0.179 (0.160)	Data 0.000 (0.006)	Loss 0.6919 (0.7676)	Acc@1 86.719 (85.355)	Acc@5 99.609 (99.357)
Epoch: [26][128/196]	Time 0.167 (0.161)	Data 0.000 (0.003)	Loss 0.8002 (0.7834)	Acc@1 85.938 (84.917)	Acc@5 99.609 (99.310)
Epoch: [26][192/196]	Time 0.163 (0.160)	Data 0.000 (0.002)	Loss 0.7864 (0.7902)	Acc@1 85.547 (84.774)	Acc@5 99.219 (99.346)
Max memory in training epoch: 66.164992
lr: 0.09278294328396441
1
Epoche:27/30; Lr: 0.09278294328396441
batch Size 256
Epoch: [27][0/196]	Time 0.226 (0.226)	Data 0.486 (0.486)	Loss 0.7970 (0.7970)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [27][64/196]	Time 0.158 (0.163)	Data 0.000 (0.008)	Loss 0.8165 (0.8069)	Acc@1 84.375 (84.327)	Acc@5 99.609 (99.189)
Epoch: [27][128/196]	Time 0.161 (0.162)	Data 0.000 (0.004)	Loss 0.7444 (0.7945)	Acc@1 85.547 (84.575)	Acc@5 100.000 (99.264)
Epoch: [27][192/196]	Time 0.161 (0.161)	Data 0.000 (0.003)	Loss 0.8262 (0.7905)	Acc@1 85.547 (84.804)	Acc@5 99.609 (99.326)
Max memory in training epoch: 66.2239744
lr: 0.09278294328396441
1
Epoche:28/30; Lr: 0.09278294328396441
batch Size 256
Epoch: [28][0/196]	Time 0.216 (0.216)	Data 0.426 (0.426)	Loss 0.7707 (0.7707)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [28][64/196]	Time 0.160 (0.160)	Data 0.000 (0.007)	Loss 0.7713 (0.7794)	Acc@1 85.547 (85.276)	Acc@5 98.438 (99.399)
Epoch: [28][128/196]	Time 0.133 (0.157)	Data 0.000 (0.003)	Loss 0.6968 (0.7841)	Acc@1 87.891 (85.074)	Acc@5 99.219 (99.440)
Epoch: [28][192/196]	Time 0.155 (0.157)	Data 0.000 (0.002)	Loss 0.8261 (0.7846)	Acc@1 82.422 (84.978)	Acc@5 99.609 (99.407)
Max memory in training epoch: 66.2239744
Drin!!
old memory: 663640576
new memory: 662239744
Faktor: 0.9978891706585463
New batch Size größer 258!!
lr: 0.09278294328396441
1
Epoche:29/30; Lr: 0.09278294328396441
batch Size 258
Epoch: [29][0/196]	Time 0.225 (0.225)	Data 0.412 (0.412)	Loss 0.7798 (0.7798)	Acc@1 82.812 (82.812)	Acc@5 99.219 (99.219)
Epoch: [29][64/196]	Time 0.170 (0.163)	Data 0.000 (0.007)	Loss 0.8317 (0.7829)	Acc@1 83.984 (84.892)	Acc@5 99.219 (99.483)
Epoch: [29][128/196]	Time 0.144 (0.162)	Data 0.000 (0.003)	Loss 0.8632 (0.7852)	Acc@1 82.812 (84.769)	Acc@5 99.219 (99.400)
Epoch: [29][192/196]	Time 0.162 (0.161)	Data 0.000 (0.002)	Loss 0.7198 (0.7916)	Acc@1 85.938 (84.583)	Acc@5 99.219 (99.385)
Max memory in training epoch: 66.2239744
lr: 0.09278294328396441
1
Epoche:30/30; Lr: 0.09278294328396441
batch Size 258
Epoch: [30][0/196]	Time 0.205 (0.205)	Data 0.456 (0.456)	Loss 0.7896 (0.7896)	Acc@1 83.984 (83.984)	Acc@5 98.828 (98.828)
Epoch: [30][64/196]	Time 0.153 (0.157)	Data 0.000 (0.007)	Loss 0.7332 (0.7833)	Acc@1 85.547 (85.084)	Acc@5 99.609 (99.399)
Epoch: [30][128/196]	Time 0.149 (0.158)	Data 0.000 (0.004)	Loss 0.6785 (0.7792)	Acc@1 87.500 (85.029)	Acc@5 100.000 (99.343)
Epoch: [30][192/196]	Time 0.153 (0.158)	Data 0.000 (0.003)	Loss 0.9077 (0.7810)	Acc@1 83.203 (84.986)	Acc@5 98.828 (99.338)
Max memory in training epoch: 66.2239744
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 442356 ; 468336 ; 0.9445270062519217
[INFO] Storing checkpoint...
  70.95
Max memory: 102.6758144
 31.539s  j: 31 bis 35
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1451
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 31
Max memory: 0.1848832
lr: 0.09350781002837039
1
Epoche:31/35; Lr: 0.09350781002837039
batch Size 258
Epoch: [31][0/194]	Time 0.265 (0.265)	Data 0.418 (0.418)	Loss 0.6980 (0.6980)	Acc@1 86.434 (86.434)	Acc@5 99.612 (99.612)
Epoch: [31][64/194]	Time 0.148 (0.161)	Data 0.000 (0.007)	Loss 0.6534 (0.7515)	Acc@1 91.085 (85.796)	Acc@5 100.000 (99.529)
Epoch: [31][128/194]	Time 0.140 (0.155)	Data 0.000 (0.003)	Loss 0.7399 (0.7700)	Acc@1 85.271 (85.163)	Acc@5 99.612 (99.429)
Epoch: [31][192/194]	Time 0.151 (0.155)	Data 0.000 (0.002)	Loss 0.8448 (0.7755)	Acc@1 82.946 (85.058)	Acc@5 99.612 (99.355)
Max memory in training epoch: 65.5498752
lr: 0.09350781002837039
1
Epoche:32/35; Lr: 0.09350781002837039
batch Size 258
Epoch: [32][0/194]	Time 0.227 (0.227)	Data 0.378 (0.378)	Loss 0.6928 (0.6928)	Acc@1 88.372 (88.372)	Acc@5 99.612 (99.612)
Epoch: [32][64/194]	Time 0.158 (0.159)	Data 0.000 (0.006)	Loss 0.8000 (0.7564)	Acc@1 82.558 (85.671)	Acc@5 100.000 (99.386)
Epoch: [32][128/194]	Time 0.151 (0.158)	Data 0.000 (0.003)	Loss 0.8100 (0.7647)	Acc@1 83.333 (85.422)	Acc@5 99.225 (99.420)
Epoch: [32][192/194]	Time 0.145 (0.159)	Data 0.000 (0.002)	Loss 0.8239 (0.7730)	Acc@1 81.783 (85.203)	Acc@5 98.837 (99.361)
Max memory in training epoch: 65.5597056
lr: 0.09350781002837039
1
Epoche:33/35; Lr: 0.09350781002837039
batch Size 258
Epoch: [33][0/194]	Time 0.205 (0.205)	Data 0.397 (0.397)	Loss 0.7235 (0.7235)	Acc@1 85.271 (85.271)	Acc@5 100.000 (100.000)
Epoch: [33][64/194]	Time 0.153 (0.158)	Data 0.000 (0.006)	Loss 0.7186 (0.7809)	Acc@1 86.047 (84.961)	Acc@5 99.225 (99.362)
Epoch: [33][128/194]	Time 0.128 (0.160)	Data 0.000 (0.003)	Loss 0.7809 (0.7704)	Acc@1 85.271 (85.331)	Acc@5 99.612 (99.387)
Epoch: [33][192/194]	Time 0.165 (0.158)	Data 0.000 (0.002)	Loss 0.7173 (0.7689)	Acc@1 89.922 (85.438)	Acc@5 99.612 (99.345)
Max memory in training epoch: 65.5597056
Drin!!
old memory: 662239744
new memory: 655597056
Faktor: 0.9899693607033044
New batch Size größer 263!!
lr: 0.09350781002837039
1
Epoche:34/35; Lr: 0.09350781002837039
batch Size 263
Epoch: [34][0/194]	Time 0.222 (0.222)	Data 0.414 (0.414)	Loss 0.7560 (0.7560)	Acc@1 85.271 (85.271)	Acc@5 99.225 (99.225)
Epoch: [34][64/194]	Time 0.160 (0.160)	Data 0.000 (0.007)	Loss 0.7515 (0.7492)	Acc@1 88.760 (85.999)	Acc@5 98.837 (99.392)
Epoch: [34][128/194]	Time 0.160 (0.160)	Data 0.000 (0.003)	Loss 0.8106 (0.7524)	Acc@1 85.659 (85.857)	Acc@5 98.837 (99.375)
Epoch: [34][192/194]	Time 0.139 (0.159)	Data 0.000 (0.002)	Loss 0.8496 (0.7630)	Acc@1 82.171 (85.567)	Acc@5 99.225 (99.341)
Max memory in training epoch: 65.5597056
lr: 0.09350781002837039
1
Epoche:35/35; Lr: 0.09350781002837039
batch Size 263
Epoch: [35][0/194]	Time 0.215 (0.215)	Data 0.385 (0.385)	Loss 0.8346 (0.8346)	Acc@1 83.333 (83.333)	Acc@5 98.450 (98.450)
Epoch: [35][64/194]	Time 0.181 (0.153)	Data 0.000 (0.006)	Loss 0.7970 (0.7523)	Acc@1 84.884 (85.653)	Acc@5 99.225 (99.398)
Epoch: [35][128/194]	Time 0.154 (0.160)	Data 0.000 (0.003)	Loss 0.7683 (0.7655)	Acc@1 85.271 (85.094)	Acc@5 98.837 (99.399)
Epoch: [35][192/194]	Time 0.157 (0.159)	Data 0.000 (0.002)	Loss 0.8012 (0.7704)	Acc@1 83.333 (85.018)	Acc@5 99.225 (99.377)
Max memory in training epoch: 65.5597056
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 412478 ; 442356 ; 0.9324571159880277
[INFO] Storing checkpoint...
  74.39
Max memory: 99.79264
 31.432s  j: 36 bis 40
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1683
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 36
Max memory: 0.1728512
lr: 0.09606466420883364
1
Epoche:36/40; Lr: 0.09606466420883364
batch Size 263
Epoch: [36][0/191]	Time 0.297 (0.297)	Data 0.349 (0.349)	Loss 0.7389 (0.7389)	Acc@1 85.932 (85.932)	Acc@5 99.620 (99.620)
Epoch: [36][64/191]	Time 0.158 (0.161)	Data 0.000 (0.006)	Loss 0.7226 (0.7320)	Acc@1 85.551 (86.247)	Acc@5 100.000 (99.532)
Epoch: [36][128/191]	Time 0.137 (0.160)	Data 0.000 (0.003)	Loss 0.8513 (0.7587)	Acc@1 80.608 (85.401)	Acc@5 98.479 (99.387)
Max memory in training epoch: 65.1881984
lr: 0.09606466420883364
1
Epoche:37/40; Lr: 0.09606466420883364
batch Size 263
Epoch: [37][0/191]	Time 0.245 (0.245)	Data 0.351 (0.351)	Loss 0.7856 (0.7856)	Acc@1 84.411 (84.411)	Acc@5 98.859 (98.859)
Epoch: [37][64/191]	Time 0.162 (0.161)	Data 0.000 (0.006)	Loss 0.7299 (0.7866)	Acc@1 88.973 (84.803)	Acc@5 99.620 (99.333)
Epoch: [37][128/191]	Time 0.159 (0.160)	Data 0.000 (0.003)	Loss 0.8495 (0.7777)	Acc@1 80.989 (85.056)	Acc@5 99.240 (99.352)
Max memory in training epoch: 65.3827072
lr: 0.09606466420883364
1
Epoche:38/40; Lr: 0.09606466420883364
batch Size 263
Epoch: [38][0/191]	Time 0.236 (0.236)	Data 0.339 (0.339)	Loss 0.9191 (0.9191)	Acc@1 80.989 (80.989)	Acc@5 99.240 (99.240)
Epoch: [38][64/191]	Time 0.161 (0.153)	Data 0.000 (0.005)	Loss 0.7646 (0.7911)	Acc@1 87.452 (84.627)	Acc@5 99.620 (99.292)
Epoch: [38][128/191]	Time 0.168 (0.154)	Data 0.000 (0.003)	Loss 0.6989 (0.7786)	Acc@1 87.452 (84.944)	Acc@5 98.859 (99.307)
Max memory in training epoch: 65.3827072
Drin!!
old memory: 655597056
new memory: 653827072
Faktor: 0.997300195319974
New batch Size größer 269!!
lr: 0.09606466420883364
1
Epoche:39/40; Lr: 0.09606466420883364
batch Size 269
Epoch: [39][0/191]	Time 0.253 (0.253)	Data 0.343 (0.343)	Loss 0.7131 (0.7131)	Acc@1 87.452 (87.452)	Acc@5 99.620 (99.620)
Epoch: [39][64/191]	Time 0.167 (0.159)	Data 0.000 (0.005)	Loss 0.8087 (0.7784)	Acc@1 84.791 (84.943)	Acc@5 97.719 (99.310)
Epoch: [39][128/191]	Time 0.149 (0.158)	Data 0.000 (0.003)	Loss 0.8433 (0.7714)	Acc@1 81.369 (85.112)	Acc@5 99.620 (99.387)
Max memory in training epoch: 65.3827072
lr: 0.09606466420883364
1
Epoche:40/40; Lr: 0.09606466420883364
batch Size 269
Epoch: [40][0/191]	Time 0.233 (0.233)	Data 0.343 (0.343)	Loss 0.6641 (0.6641)	Acc@1 90.114 (90.114)	Acc@5 98.859 (98.859)
Epoch: [40][64/191]	Time 0.155 (0.162)	Data 0.000 (0.005)	Loss 0.7155 (0.7979)	Acc@1 85.171 (84.317)	Acc@5 98.859 (99.228)
Epoch: [40][128/191]	Time 0.140 (0.160)	Data 0.000 (0.003)	Loss 0.7139 (0.7786)	Acc@1 88.213 (85.036)	Acc@5 99.620 (99.346)
Max memory in training epoch: 65.3827072
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 393712 ; 412478 ; 0.9545042402261454
[INFO] Storing checkpoint...
  72.83
Max memory: 97.8983936
 30.953s  j: 41 bis 45
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 41
Max memory: 0.1654784
lr: 0.10094294793818848
1
Epoche:41/45; Lr: 0.10094294793818848
batch Size 269
Epoch: [41][0/186]	Time 0.303 (0.303)	Data 0.408 (0.408)	Loss 0.7707 (0.7707)	Acc@1 85.874 (85.874)	Acc@5 100.000 (100.000)
Epoch: [41][64/186]	Time 0.156 (0.156)	Data 0.000 (0.006)	Loss 0.6861 (0.7365)	Acc@1 89.591 (86.285)	Acc@5 99.257 (99.502)
Epoch: [41][128/186]	Time 0.146 (0.156)	Data 0.000 (0.003)	Loss 0.7508 (0.7493)	Acc@1 84.758 (85.776)	Acc@5 99.257 (99.441)
Max memory in training epoch: 65.282048
lr: 0.10094294793818848
1
Epoche:42/45; Lr: 0.10094294793818848
batch Size 269
Epoch: [42][0/186]	Time 0.196 (0.196)	Data 0.470 (0.470)	Loss 0.7171 (0.7171)	Acc@1 88.104 (88.104)	Acc@5 99.628 (99.628)
Epoch: [42][64/186]	Time 0.166 (0.155)	Data 0.000 (0.007)	Loss 0.8407 (0.7572)	Acc@1 84.015 (85.330)	Acc@5 98.513 (99.462)
Epoch: [42][128/186]	Time 0.155 (0.159)	Data 0.000 (0.004)	Loss 0.6235 (0.7589)	Acc@1 89.219 (85.410)	Acc@5 100.000 (99.438)
Max memory in training epoch: 65.4819328
lr: 0.10094294793818848
1
Epoche:43/45; Lr: 0.10094294793818848
batch Size 269
Epoch: [43][0/186]	Time 0.242 (0.242)	Data 0.373 (0.373)	Loss 0.6031 (0.6031)	Acc@1 90.706 (90.706)	Acc@5 100.000 (100.000)
Epoch: [43][64/186]	Time 0.144 (0.162)	Data 0.000 (0.006)	Loss 0.7414 (0.7466)	Acc@1 85.502 (85.954)	Acc@5 98.513 (99.377)
Epoch: [43][128/186]	Time 0.160 (0.160)	Data 0.000 (0.003)	Loss 0.8016 (0.7459)	Acc@1 84.387 (85.776)	Acc@5 99.257 (99.415)
Max memory in training epoch: 65.4819328
Drin!!
old memory: 653827072
new memory: 654819328
Faktor: 1.0015176122899971
New batch Size kleiner 269!!
lr: 0.10094294793818848
1
Epoche:44/45; Lr: 0.10094294793818848
batch Size 269
Epoch: [44][0/186]	Time 0.261 (0.261)	Data 0.403 (0.403)	Loss 0.7320 (0.7320)	Acc@1 85.502 (85.502)	Acc@5 100.000 (100.000)
Epoch: [44][64/186]	Time 0.165 (0.162)	Data 0.000 (0.006)	Loss 0.8090 (0.7565)	Acc@1 83.643 (85.565)	Acc@5 98.885 (99.365)
Epoch: [44][128/186]	Time 0.174 (0.160)	Data 0.000 (0.003)	Loss 0.7414 (0.7584)	Acc@1 85.130 (85.487)	Acc@5 99.628 (99.363)
Max memory in training epoch: 65.4819328
lr: 0.10094294793818848
1
Epoche:45/45; Lr: 0.10094294793818848
batch Size 269
Epoch: [45][0/186]	Time 0.260 (0.260)	Data 0.457 (0.457)	Loss 0.7218 (0.7218)	Acc@1 86.989 (86.989)	Acc@5 100.000 (100.000)
Epoch: [45][64/186]	Time 0.153 (0.164)	Data 0.000 (0.007)	Loss 0.7518 (0.7486)	Acc@1 82.900 (85.799)	Acc@5 100.000 (99.417)
Epoch: [45][128/186]	Time 0.162 (0.162)	Data 0.000 (0.004)	Loss 0.8087 (0.7478)	Acc@1 83.643 (85.620)	Acc@5 98.885 (99.401)
Max memory in training epoch: 65.4819328
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 379568 ; 393712 ; 0.9640752631365058
[INFO] Storing checkpoint...
  75.17
Max memory: 96.7154688
 30.522s  j: 46 bis 50
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7520
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 46
Max memory: 0.1598464
lr: 0.1060689570131746
1
Epoche:46/50; Lr: 0.1060689570131746
batch Size 269
Epoch: [46][0/186]	Time 0.271 (0.271)	Data 0.396 (0.396)	Loss 0.7305 (0.7305)	Acc@1 87.361 (87.361)	Acc@5 100.000 (100.000)
Epoch: [46][64/186]	Time 0.163 (0.162)	Data 0.000 (0.007)	Loss 0.7358 (0.7382)	Acc@1 86.989 (86.120)	Acc@5 98.885 (99.531)
Epoch: [46][128/186]	Time 0.172 (0.157)	Data 0.000 (0.003)	Loss 0.7447 (0.7442)	Acc@1 86.617 (85.934)	Acc@5 99.628 (99.441)
Max memory in training epoch: 64.6336
lr: 0.1060689570131746
1
Epoche:47/50; Lr: 0.1060689570131746
batch Size 269
Epoch: [47][0/186]	Time 0.198 (0.198)	Data 0.459 (0.459)	Loss 0.7007 (0.7007)	Acc@1 86.989 (86.989)	Acc@5 99.628 (99.628)
Epoch: [47][64/186]	Time 0.142 (0.160)	Data 0.000 (0.007)	Loss 0.8055 (0.7586)	Acc@1 83.271 (85.170)	Acc@5 98.885 (99.268)
Epoch: [47][128/186]	Time 0.153 (0.160)	Data 0.000 (0.004)	Loss 0.7213 (0.7623)	Acc@1 88.476 (85.110)	Acc@5 99.257 (99.331)
Max memory in training epoch: 64.5598208
lr: 0.1060689570131746
1
Epoche:48/50; Lr: 0.1060689570131746
batch Size 269
Epoch: [48][0/186]	Time 0.155 (0.155)	Data 0.321 (0.321)	Loss 0.7545 (0.7545)	Acc@1 87.732 (87.732)	Acc@5 99.257 (99.257)
Epoch: [48][64/186]	Time 0.175 (0.158)	Data 0.000 (0.005)	Loss 0.8124 (0.7428)	Acc@1 84.015 (86.051)	Acc@5 99.257 (99.445)
Epoch: [48][128/186]	Time 0.145 (0.159)	Data 0.000 (0.003)	Loss 0.7765 (0.7611)	Acc@1 87.361 (85.320)	Acc@5 98.885 (99.401)
Max memory in training epoch: 64.5598208
Drin!!
old memory: 654819328
new memory: 645598208
Faktor: 0.9859180699076738
New batch Size größer 279!!
lr: 0.1060689570131746
1
Epoche:49/50; Lr: 0.1060689570131746
batch Size 279
Epoch: [49][0/186]	Time 0.247 (0.247)	Data 0.369 (0.369)	Loss 0.7243 (0.7243)	Acc@1 85.874 (85.874)	Acc@5 98.885 (98.885)
Epoch: [49][64/186]	Time 0.145 (0.163)	Data 0.000 (0.006)	Loss 0.7927 (0.7545)	Acc@1 85.874 (85.439)	Acc@5 99.257 (99.302)
Epoch: [49][128/186]	Time 0.147 (0.161)	Data 0.000 (0.003)	Loss 0.7875 (0.7609)	Acc@1 84.015 (85.306)	Acc@5 99.257 (99.329)
Max memory in training epoch: 64.5598208
lr: 0.1060689570131746
1
Epoche:50/50; Lr: 0.1060689570131746
batch Size 279
Epoch: [50][0/186]	Time 0.239 (0.239)	Data 0.536 (0.536)	Loss 0.6964 (0.6964)	Acc@1 87.732 (87.732)	Acc@5 99.628 (99.628)
Epoch: [50][64/186]	Time 0.175 (0.163)	Data 0.000 (0.008)	Loss 0.7776 (0.7486)	Acc@1 85.874 (85.839)	Acc@5 99.257 (99.405)
Epoch: [50][128/186]	Time 0.135 (0.162)	Data 0.000 (0.004)	Loss 0.7044 (0.7489)	Acc@1 87.732 (85.810)	Acc@5 99.628 (99.429)
Max memory in training epoch: 64.5598208
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 368886 ; 379568 ; 0.9718574800826203
[INFO] Storing checkpoint...
  81.6
Max memory: 95.209472
 30.784s  j: 51 bis 55
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7849
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 51
Max memory: 0.155648
lr: 0.11559858986982702
1
Epoche:51/55; Lr: 0.11559858986982702
batch Size 279
Epoch: [51][0/180]	Time 0.275 (0.275)	Data 0.466 (0.466)	Loss 0.7443 (0.7443)	Acc@1 84.946 (84.946)	Acc@5 99.283 (99.283)
Epoch: [51][64/180]	Time 0.169 (0.157)	Data 0.000 (0.007)	Loss 0.7194 (0.7353)	Acc@1 87.814 (86.154)	Acc@5 100.000 (99.322)
Epoch: [51][128/180]	Time 0.155 (0.159)	Data 0.000 (0.004)	Loss 0.7803 (0.7469)	Acc@1 83.871 (85.694)	Acc@5 99.283 (99.372)
Max memory in training epoch: 66.8417024
lr: 0.11559858986982702
1
Epoche:52/55; Lr: 0.11559858986982702
batch Size 279
Epoch: [52][0/180]	Time 0.242 (0.242)	Data 0.356 (0.356)	Loss 0.8196 (0.8196)	Acc@1 83.871 (83.871)	Acc@5 98.566 (98.566)
Epoch: [52][64/180]	Time 0.208 (0.158)	Data 0.000 (0.006)	Loss 0.7242 (0.7759)	Acc@1 87.814 (84.665)	Acc@5 99.642 (99.371)
Epoch: [52][128/180]	Time 0.157 (0.159)	Data 0.000 (0.003)	Loss 0.8150 (0.7719)	Acc@1 83.871 (85.082)	Acc@5 99.642 (99.350)
Max memory in training epoch: 67.5674624
lr: 0.11559858986982702
1
Epoche:53/55; Lr: 0.11559858986982702
batch Size 279
Epoch: [53][0/180]	Time 0.204 (0.204)	Data 0.410 (0.410)	Loss 0.7342 (0.7342)	Acc@1 87.097 (87.097)	Acc@5 99.283 (99.283)
Epoch: [53][64/180]	Time 0.170 (0.154)	Data 0.000 (0.006)	Loss 0.9679 (0.7706)	Acc@1 78.136 (85.128)	Acc@5 98.925 (99.278)
Epoch: [53][128/180]	Time 0.160 (0.156)	Data 0.000 (0.003)	Loss 0.7520 (0.7629)	Acc@1 84.946 (85.332)	Acc@5 99.283 (99.344)
Max memory in training epoch: 67.5674624
Drin!!
old memory: 645598208
new memory: 675674624
Faktor: 1.0465868951110844
New batch Size kleiner 291!!
lr: 0.11559858986982702
1
Epoche:54/55; Lr: 0.11559858986982702
batch Size 291
Epoch: [54][0/180]	Time 0.214 (0.214)	Data 0.455 (0.455)	Loss 0.6681 (0.6681)	Acc@1 89.606 (89.606)	Acc@5 99.283 (99.283)
Epoch: [54][64/180]	Time 0.132 (0.153)	Data 0.000 (0.007)	Loss 0.8383 (0.7741)	Acc@1 82.796 (84.957)	Acc@5 100.000 (99.327)
Epoch: [54][128/180]	Time 0.145 (0.152)	Data 0.000 (0.004)	Loss 0.7364 (0.7660)	Acc@1 87.097 (84.993)	Acc@5 99.642 (99.386)
Max memory in training epoch: 67.5674624
lr: 0.11559858986982702
1
Epoche:55/55; Lr: 0.11559858986982702
batch Size 291
Epoch: [55][0/180]	Time 0.194 (0.194)	Data 0.310 (0.310)	Loss 0.6873 (0.6873)	Acc@1 88.889 (88.889)	Acc@5 99.283 (99.283)
Epoch: [55][64/180]	Time 0.140 (0.153)	Data 0.000 (0.005)	Loss 0.8912 (0.7533)	Acc@1 82.796 (85.536)	Acc@5 98.208 (99.371)
Epoch: [55][128/180]	Time 0.150 (0.154)	Data 0.000 (0.003)	Loss 0.7954 (0.7612)	Acc@1 82.796 (85.277)	Acc@5 99.642 (99.328)
Max memory in training epoch: 67.5674624
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 355172 ; 368886 ; 0.9628232028323114
[INFO] Storing checkpoint...
  71.48
Max memory: 93.7204736
 28.159s  j: 56 bis 60
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8728
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 56
Max memory: 0.150272
lr: 0.13140308457859243
1
Epoche:56/60; Lr: 0.13140308457859243
batch Size 291
Epoch: [56][0/172]	Time 0.279 (0.279)	Data 0.314 (0.314)	Loss 0.7798 (0.7798)	Acc@1 85.567 (85.567)	Acc@5 98.969 (98.969)
Epoch: [56][64/172]	Time 0.146 (0.156)	Data 0.000 (0.005)	Loss 0.8385 (0.7527)	Acc@1 82.131 (85.514)	Acc@5 99.656 (99.376)
Epoch: [56][128/172]	Time 0.152 (0.153)	Data 0.000 (0.003)	Loss 0.8116 (0.7685)	Acc@1 84.536 (84.914)	Acc@5 99.656 (99.345)
Max memory in training epoch: 67.6543488
lr: 0.13140308457859243
1
Epoche:57/60; Lr: 0.13140308457859243
batch Size 291
Epoch: [57][0/172]	Time 0.229 (0.229)	Data 0.384 (0.384)	Loss 0.7850 (0.7850)	Acc@1 83.505 (83.505)	Acc@5 99.656 (99.656)
Epoch: [57][64/172]	Time 0.144 (0.157)	Data 0.000 (0.006)	Loss 0.7534 (0.7718)	Acc@1 87.285 (84.917)	Acc@5 99.313 (99.376)
Epoch: [57][128/172]	Time 0.138 (0.153)	Data 0.000 (0.003)	Loss 0.8053 (0.7771)	Acc@1 85.911 (84.765)	Acc@5 99.656 (99.393)
Max memory in training epoch: 68.152576
lr: 0.13140308457859243
1
Epoche:58/60; Lr: 0.13140308457859243
batch Size 291
Epoch: [58][0/172]	Time 0.217 (0.217)	Data 0.422 (0.422)	Loss 0.7638 (0.7638)	Acc@1 87.285 (87.285)	Acc@5 98.969 (98.969)
Epoch: [58][64/172]	Time 0.157 (0.161)	Data 0.000 (0.007)	Loss 0.7033 (0.7717)	Acc@1 86.942 (84.991)	Acc@5 99.656 (99.307)
Epoch: [58][128/172]	Time 0.141 (0.159)	Data 0.000 (0.003)	Loss 0.7906 (0.7776)	Acc@1 85.223 (84.869)	Acc@5 99.313 (99.310)
Max memory in training epoch: 68.152576
Drin!!
old memory: 675674624
new memory: 681525760
Faktor: 1.0086596947586417
New batch Size kleiner 293!!
lr: 0.13140308457859243
1
Epoche:59/60; Lr: 0.13140308457859243
batch Size 293
Epoch: [59][0/172]	Time 0.190 (0.190)	Data 0.339 (0.339)	Loss 0.8036 (0.8036)	Acc@1 82.818 (82.818)	Acc@5 99.656 (99.656)
Epoch: [59][64/172]	Time 0.138 (0.156)	Data 0.000 (0.005)	Loss 0.8685 (0.7626)	Acc@1 80.412 (84.938)	Acc@5 100.000 (99.440)
Epoch: [59][128/172]	Time 0.170 (0.156)	Data 0.000 (0.003)	Loss 0.7468 (0.7666)	Acc@1 86.254 (85.069)	Acc@5 98.625 (99.390)
Max memory in training epoch: 68.152576
lr: 0.13140308457859243
1
Epoche:60/60; Lr: 0.13140308457859243
batch Size 293
Epoch: [60][0/172]	Time 0.217 (0.217)	Data 0.370 (0.370)	Loss 0.8057 (0.8057)	Acc@1 83.849 (83.849)	Acc@5 100.000 (100.000)
Epoch: [60][64/172]	Time 0.157 (0.158)	Data 0.000 (0.006)	Loss 0.8714 (0.7596)	Acc@1 82.474 (85.276)	Acc@5 99.313 (99.392)
Epoch: [60][128/172]	Time 0.143 (0.157)	Data 0.000 (0.003)	Loss 0.7800 (0.7654)	Acc@1 85.223 (85.173)	Acc@5 98.969 (99.334)
Max memory in training epoch: 68.152576
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 340740 ; 355172 ; 0.9593661662518442
[INFO] Storing checkpoint...
  68.01
Max memory: 92.796672
 27.297s  j: 61 bis 65
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2757
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 61
Max memory: 0.1445376
lr: 0.15039493664659212
1
Epoche:61/65; Lr: 0.15039493664659212
batch Size 293
Epoch: [61][0/171]	Time 0.221 (0.221)	Data 0.346 (0.346)	Loss 0.8306 (0.8306)	Acc@1 84.300 (84.300)	Acc@5 98.294 (98.294)
Epoch: [61][64/171]	Time 0.156 (0.156)	Data 0.000 (0.006)	Loss 0.7989 (0.7764)	Acc@1 84.642 (84.657)	Acc@5 99.317 (99.391)
Epoch: [61][128/171]	Time 0.153 (0.154)	Data 0.000 (0.003)	Loss 0.7657 (0.7926)	Acc@1 87.713 (84.144)	Acc@5 99.659 (99.307)
Max memory in training epoch: 67.3486336
lr: 0.15039493664659212
1
Epoche:62/65; Lr: 0.15039493664659212
batch Size 293
Epoch: [62][0/171]	Time 0.257 (0.257)	Data 0.405 (0.405)	Loss 0.8281 (0.8281)	Acc@1 82.935 (82.935)	Acc@5 99.317 (99.317)
Epoch: [62][64/171]	Time 0.161 (0.159)	Data 0.000 (0.006)	Loss 0.7690 (0.8070)	Acc@1 83.618 (84.090)	Acc@5 99.317 (99.359)
Epoch: [62][128/171]	Time 0.141 (0.157)	Data 0.000 (0.003)	Loss 0.7846 (0.8101)	Acc@1 84.983 (84.036)	Acc@5 99.317 (99.302)
Max memory in training epoch: 67.6366848
lr: 0.15039493664659212
1
Epoche:63/65; Lr: 0.15039493664659212
batch Size 293
Epoch: [63][0/171]	Time 0.214 (0.214)	Data 0.310 (0.310)	Loss 0.6875 (0.6875)	Acc@1 88.737 (88.737)	Acc@5 100.000 (100.000)
Epoch: [63][64/171]	Time 0.151 (0.162)	Data 0.000 (0.005)	Loss 0.8247 (0.7915)	Acc@1 85.666 (84.526)	Acc@5 99.317 (99.260)
Epoch: [63][128/171]	Time 0.152 (0.159)	Data 0.000 (0.003)	Loss 0.7795 (0.7887)	Acc@1 84.983 (84.634)	Acc@5 99.317 (99.243)
Max memory in training epoch: 67.7589504
Drin!!
old memory: 681525760
new memory: 677589504
Faktor: 0.9942243474406602
New batch Size größer 289!!
lr: 0.15039493664659212
1
Epoche:64/65; Lr: 0.15039493664659212
batch Size 289
Epoch: [64][0/171]	Time 0.177 (0.177)	Data 0.284 (0.284)	Loss 0.7359 (0.7359)	Acc@1 85.324 (85.324)	Acc@5 100.000 (100.000)
Epoch: [64][64/171]	Time 0.140 (0.152)	Data 0.000 (0.005)	Loss 0.7253 (0.7861)	Acc@1 85.324 (84.332)	Acc@5 99.659 (99.354)
Epoch: [64][128/171]	Time 0.160 (0.153)	Data 0.000 (0.002)	Loss 0.7370 (0.7907)	Acc@1 83.276 (84.300)	Acc@5 99.659 (99.307)
Max memory in training epoch: 67.7589504
lr: 0.15039493664659212
1
Epoche:65/65; Lr: 0.15039493664659212
batch Size 289
Epoch: [65][0/171]	Time 0.217 (0.217)	Data 0.410 (0.410)	Loss 0.7475 (0.7475)	Acc@1 83.618 (83.618)	Acc@5 99.659 (99.659)
Epoch: [65][64/171]	Time 0.147 (0.157)	Data 0.000 (0.007)	Loss 0.7952 (0.7864)	Acc@1 84.300 (84.584)	Acc@5 99.659 (99.302)
Epoch: [65][128/171]	Time 0.182 (0.156)	Data 0.000 (0.003)	Loss 0.7742 (0.7844)	Acc@1 84.642 (84.687)	Acc@5 100.000 (99.288)
Max memory in training epoch: 67.7589504
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 335394 ; 340740 ; 0.9843106180665611
[INFO] Storing checkpoint...
  73.37
Max memory: 91.5736064
 27.241s  j: 66 bis 70
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6608
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 66
Max memory: 0.1424384
lr: 0.1697817839486919
1
Epoche:66/70; Lr: 0.1697817839486919
batch Size 289
Epoch: [66][0/174]	Time 0.203 (0.203)	Data 0.474 (0.474)	Loss 0.7956 (0.7956)	Acc@1 84.775 (84.775)	Acc@5 99.654 (99.654)
Epoch: [66][64/174]	Time 0.137 (0.152)	Data 0.000 (0.007)	Loss 0.8750 (0.8007)	Acc@1 82.353 (83.625)	Acc@5 98.616 (99.303)
Epoch: [66][128/174]	Time 0.142 (0.151)	Data 0.000 (0.004)	Loss 0.7851 (0.8186)	Acc@1 82.699 (83.260)	Acc@5 99.308 (99.185)
Max memory in training epoch: 65.4201344
lr: 0.1697817839486919
1
Epoche:67/70; Lr: 0.1697817839486919
batch Size 289
Epoch: [67][0/174]	Time 0.187 (0.187)	Data 0.261 (0.261)	Loss 1.3801 (1.3801)	Acc@1 69.204 (69.204)	Acc@5 94.810 (94.810)
Epoch: [67][64/174]	Time 0.155 (0.154)	Data 0.000 (0.004)	Loss 1.1876 (1.3778)	Acc@1 73.702 (67.442)	Acc@5 98.616 (95.928)
Epoch: [67][128/174]	Time 0.169 (0.155)	Data 0.000 (0.002)	Loss 0.9387 (1.2115)	Acc@1 81.661 (72.874)	Acc@5 99.654 (97.277)
Max memory in training epoch: 65.6980992
lr: 0.1697817839486919
1
Epoche:68/70; Lr: 0.1697817839486919
batch Size 289
Epoch: [68][0/174]	Time 0.204 (0.204)	Data 0.356 (0.356)	Loss 1.1621 (1.1621)	Acc@1 73.356 (73.356)	Acc@5 98.616 (98.616)
Epoch: [68][64/174]	Time 0.172 (0.158)	Data 0.000 (0.006)	Loss 0.9667 (1.0992)	Acc@1 82.353 (75.986)	Acc@5 99.308 (98.259)
Epoch: [68][128/174]	Time 0.153 (0.156)	Data 0.000 (0.003)	Loss 1.0150 (1.0137)	Acc@1 78.201 (78.643)	Acc@5 98.616 (98.616)
Max memory in training epoch: 65.6980992
Drin!!
old memory: 677589504
new memory: 656980992
Faktor: 0.9695855501327246
New batch Size größer 294!!
lr: 0.1697817839486919
1
Epoche:69/70; Lr: 0.1697817839486919
batch Size 294
Epoch: [69][0/174]	Time 0.221 (0.221)	Data 0.401 (0.401)	Loss 1.0700 (1.0700)	Acc@1 77.163 (77.163)	Acc@5 97.924 (97.924)
Epoch: [69][64/174]	Time 0.139 (0.154)	Data 0.000 (0.006)	Loss 0.8771 (1.0207)	Acc@1 81.315 (78.052)	Acc@5 99.654 (98.547)
Epoch: [69][128/174]	Time 0.137 (0.151)	Data 0.000 (0.003)	Loss 0.7822 (0.9462)	Acc@1 86.159 (80.491)	Acc@5 99.308 (98.750)
Max memory in training epoch: 65.6980992
lr: 0.1697817839486919
1
Epoche:70/70; Lr: 0.1697817839486919
batch Size 294
Epoch: [70][0/174]	Time 0.202 (0.202)	Data 0.365 (0.365)	Loss 0.8706 (0.8706)	Acc@1 81.315 (81.315)	Acc@5 99.308 (99.308)
Epoch: [70][64/174]	Time 0.141 (0.156)	Data 0.000 (0.006)	Loss 0.9503 (0.9119)	Acc@1 78.201 (81.107)	Acc@5 100.000 (98.877)
Epoch: [70][128/174]	Time 0.143 (0.152)	Data 0.000 (0.003)	Loss 0.8723 (0.8851)	Acc@1 85.121 (81.779)	Acc@5 98.616 (99.026)
Max memory in training epoch: 65.6980992
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 316628 ; 335394 ; 0.9440478959074998
[INFO] Storing checkpoint...
  54.73
Max memory: 89.856
 26.955s  j: 71 bis 75
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9646
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 71
Max memory: 0.1349632
lr: 0.19498376750357585
1
Epoche:71/75; Lr: 0.19498376750357585
batch Size 294
Epoch: [71][0/171]	Time 0.254 (0.254)	Data 0.411 (0.411)	Loss 0.9902 (0.9902)	Acc@1 78.571 (78.571)	Acc@5 97.959 (97.959)
Epoch: [71][64/171]	Time 0.151 (0.157)	Data 0.000 (0.007)	Loss 0.8430 (0.8387)	Acc@1 79.932 (82.868)	Acc@5 99.320 (99.095)
Epoch: [71][128/171]	Time 0.163 (0.155)	Data 0.000 (0.003)	Loss 0.8548 (0.8461)	Acc@1 83.673 (82.437)	Acc@5 99.320 (99.064)
Max memory in training epoch: 65.3798912
lr: 0.19498376750357585
1
Epoche:72/75; Lr: 0.19498376750357585
batch Size 294
Epoch: [72][0/171]	Time 0.209 (0.209)	Data 0.327 (0.327)	Loss 0.7768 (0.7768)	Acc@1 82.313 (82.313)	Acc@5 99.660 (99.660)
Epoch: [72][64/171]	Time 0.179 (0.152)	Data 0.000 (0.005)	Loss 0.7971 (0.8514)	Acc@1 85.714 (82.245)	Acc@5 98.639 (99.084)
Epoch: [72][128/171]	Time 0.156 (0.152)	Data 0.000 (0.003)	Loss 0.8278 (0.8485)	Acc@1 84.694 (82.186)	Acc@5 99.660 (99.111)
Max memory in training epoch: 65.3491712
lr: 0.19498376750357585
1
Epoche:73/75; Lr: 0.19498376750357585
batch Size 294
Epoch: [73][0/171]	Time 0.229 (0.229)	Data 0.344 (0.344)	Loss 0.9009 (0.9009)	Acc@1 81.973 (81.973)	Acc@5 97.959 (97.959)
Epoch: [73][64/171]	Time 0.155 (0.155)	Data 0.000 (0.005)	Loss 0.7845 (0.8885)	Acc@1 84.014 (81.094)	Acc@5 99.320 (98.812)
Epoch: [73][128/171]	Time 0.159 (0.152)	Data 0.000 (0.003)	Loss 0.8960 (0.8649)	Acc@1 78.231 (81.859)	Acc@5 99.320 (98.932)
Max memory in training epoch: 65.47712
Drin!!
old memory: 656980992
new memory: 654771200
Faktor: 0.9966364445441978
New batch Size größer 300!!
lr: 0.19498376750357585
1
Epoche:74/75; Lr: 0.19498376750357585
batch Size 300
Epoch: [74][0/171]	Time 0.237 (0.237)	Data 0.372 (0.372)	Loss 0.7412 (0.7412)	Acc@1 85.034 (85.034)	Acc@5 100.000 (100.000)
Epoch: [74][64/171]	Time 0.132 (0.154)	Data 0.000 (0.006)	Loss 0.8034 (0.8635)	Acc@1 82.653 (81.586)	Acc@5 98.299 (98.990)
Epoch: [74][128/171]	Time 0.149 (0.154)	Data 0.000 (0.003)	Loss 0.8382 (0.8540)	Acc@1 83.673 (81.881)	Acc@5 98.639 (99.024)
Max memory in training epoch: 65.47712
lr: 0.19498376750357585
1
Epoche:75/75; Lr: 0.19498376750357585
batch Size 300
Epoch: [75][0/171]	Time 0.226 (0.226)	Data 0.445 (0.445)	Loss 0.8294 (0.8294)	Acc@1 82.653 (82.653)	Acc@5 99.320 (99.320)
Epoch: [75][64/171]	Time 0.136 (0.157)	Data 0.000 (0.007)	Loss 0.8357 (0.8909)	Acc@1 81.973 (80.916)	Acc@5 98.639 (98.885)
Epoch: [75][128/171]	Time 0.139 (0.157)	Data 0.000 (0.004)	Loss 0.8315 (0.8595)	Acc@1 84.014 (81.783)	Acc@5 99.660 (99.009)
Max memory in training epoch: 65.47712
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv21.weight

 RM:  module.conv22.weight
numoFStages: 3
Count: 273550 ; 316628 ; 0.8639475978119433
[INFO] Storing checkpoint...
  72.22
Max memory: 88.4079616
 27.047s  j: 76 bis 80
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4746
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 76
Max memory: 0.1171456
lr: 0.22849660254325294
1
Epoche:76/80; Lr: 0.22849660254325294
batch Size 300
Epoch: [76][0/167]	Time 0.206 (0.206)	Data 0.388 (0.388)	Loss 0.8706 (0.8706)	Acc@1 81.667 (81.667)	Acc@5 99.333 (99.333)
Epoch: [76][64/167]	Time 0.155 (0.145)	Data 0.000 (0.006)	Loss 0.8764 (0.8281)	Acc@1 82.000 (82.410)	Acc@5 99.333 (99.138)
Epoch: [76][128/167]	Time 0.136 (0.142)	Data 0.000 (0.003)	Loss 0.9218 (0.8611)	Acc@1 81.000 (81.605)	Acc@5 99.000 (98.997)
Max memory in training epoch: 62.0141568
lr: 0.22849660254325294
1
Epoche:77/80; Lr: 0.22849660254325294
batch Size 300
Epoch: [77][0/167]	Time 0.216 (0.216)	Data 0.380 (0.380)	Loss 0.8789 (0.8789)	Acc@1 82.333 (82.333)	Acc@5 97.667 (97.667)
Epoch: [77][64/167]	Time 0.153 (0.148)	Data 0.000 (0.006)	Loss 0.7851 (0.8686)	Acc@1 85.333 (81.549)	Acc@5 99.667 (99.015)
Epoch: [77][128/167]	Time 0.144 (0.147)	Data 0.000 (0.003)	Loss 0.8482 (0.8682)	Acc@1 82.667 (81.638)	Acc@5 100.000 (98.959)
Max memory in training epoch: 62.20288
lr: 0.22849660254325294
1
Epoche:78/80; Lr: 0.22849660254325294
batch Size 300
Epoch: [78][0/167]	Time 0.186 (0.186)	Data 0.348 (0.348)	Loss 0.8170 (0.8170)	Acc@1 83.000 (83.000)	Acc@5 98.667 (98.667)
Epoch: [78][64/167]	Time 0.141 (0.146)	Data 0.000 (0.006)	Loss 0.7759 (0.8579)	Acc@1 83.667 (81.856)	Acc@5 99.333 (98.974)
Epoch: [78][128/167]	Time 0.136 (0.145)	Data 0.000 (0.003)	Loss 0.9079 (0.8600)	Acc@1 80.667 (81.736)	Acc@5 99.667 (99.000)
Max memory in training epoch: 62.20288
Drin!!
old memory: 654771200
new memory: 622028800
Faktor: 0.9499941353559839
New batch Size größer 323!!
lr: 0.22849660254325294
1
Epoche:79/80; Lr: 0.22849660254325294
batch Size 323
Epoch: [79][0/167]	Time 0.201 (0.201)	Data 0.363 (0.363)	Loss 0.9063 (0.9063)	Acc@1 80.667 (80.667)	Acc@5 99.000 (99.000)
Epoch: [79][64/167]	Time 0.151 (0.146)	Data 0.000 (0.006)	Loss 0.8727 (0.8542)	Acc@1 80.333 (81.923)	Acc@5 98.667 (99.056)
Epoch: [79][128/167]	Time 0.168 (0.146)	Data 0.000 (0.003)	Loss 0.9125 (0.8561)	Acc@1 81.333 (81.744)	Acc@5 98.667 (99.052)
Max memory in training epoch: 62.20288
lr: 0.22849660254325294
1
Epoche:80/80; Lr: 0.22849660254325294
batch Size 323
Epoch: [80][0/167]	Time 0.215 (0.215)	Data 0.451 (0.451)	Loss 0.7834 (0.7834)	Acc@1 83.000 (83.000)	Acc@5 99.333 (99.333)
Epoch: [80][64/167]	Time 0.122 (0.145)	Data 0.000 (0.007)	Loss 0.8250 (0.8537)	Acc@1 80.333 (81.441)	Acc@5 100.000 (99.046)
Epoch: [80][128/167]	Time 0.129 (0.147)	Data 0.000 (0.004)	Loss 0.7307 (0.8482)	Acc@1 87.000 (81.824)	Acc@5 99.333 (99.085)
Max memory in training epoch: 62.20288
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv17.weight

 RM:  module.conv18.weight
numoFStages: 3
Count: 259873 ; 273550 ; 0.9500018278194114
[INFO] Storing checkpoint...
  72.84
Max memory: 82.9461504
 24.928s  j: 81 bis 85
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9711
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 81
Max memory: 0.1115648
lr: 0.2882984477401199
1
Epoche:81/85; Lr: 0.2882984477401199
batch Size 323
Epoch: [81][0/155]	Time 0.250 (0.250)	Data 0.347 (0.347)	Loss 0.8085 (0.8085)	Acc@1 83.282 (83.282)	Acc@5 98.142 (98.142)
Epoch: [81][64/155]	Time 0.146 (0.142)	Data 0.000 (0.006)	Loss 0.9365 (0.8667)	Acc@1 78.638 (81.377)	Acc@5 99.071 (98.952)
Epoch: [81][128/155]	Time 0.143 (0.141)	Data 0.000 (0.003)	Loss 0.8499 (0.8871)	Acc@1 82.353 (80.848)	Acc@5 99.071 (98.862)
Max memory in training epoch: 63.3330688
lr: 0.2882984477401199
1
Epoche:82/85; Lr: 0.2882984477401199
batch Size 323
Epoch: [82][0/155]	Time 0.204 (0.204)	Data 0.377 (0.377)	Loss 0.8326 (0.8326)	Acc@1 84.211 (84.211)	Acc@5 99.381 (99.381)
Epoch: [82][64/155]	Time 0.137 (0.141)	Data 0.000 (0.006)	Loss 0.8745 (0.8993)	Acc@1 82.663 (80.533)	Acc@5 98.142 (98.938)
Epoch: [82][128/155]	Time 0.152 (0.142)	Data 0.000 (0.003)	Loss 0.9297 (0.9055)	Acc@1 78.638 (80.275)	Acc@5 99.071 (98.879)
Max memory in training epoch: 63.4363904
lr: 0.2882984477401199
1
Epoche:83/85; Lr: 0.2882984477401199
batch Size 323
Epoch: [83][0/155]	Time 0.218 (0.218)	Data 0.401 (0.401)	Loss 0.8080 (0.8080)	Acc@1 81.424 (81.424)	Acc@5 100.000 (100.000)
Epoch: [83][64/155]	Time 0.133 (0.142)	Data 0.000 (0.006)	Loss 0.8363 (0.8810)	Acc@1 83.591 (81.472)	Acc@5 99.071 (99.033)
Epoch: [83][128/155]	Time 0.141 (0.143)	Data 0.000 (0.003)	Loss 0.9021 (0.8948)	Acc@1 78.947 (80.932)	Acc@5 99.690 (98.884)
Max memory in training epoch: 63.4363904
Drin!!
old memory: 622028800
new memory: 634363904
Faktor: 1.019830438719236
New batch Size kleiner 329!!
lr: 0.2882984477401199
1
Epoche:84/85; Lr: 0.2882984477401199
batch Size 329
Epoch: [84][0/155]	Time 0.184 (0.184)	Data 0.403 (0.403)	Loss 0.9158 (0.9158)	Acc@1 81.115 (81.115)	Acc@5 98.452 (98.452)
Epoch: [84][64/155]	Time 0.151 (0.140)	Data 0.000 (0.006)	Loss 0.9576 (0.8835)	Acc@1 78.638 (81.319)	Acc@5 98.762 (98.952)
Epoch: [84][128/155]	Time 0.133 (0.141)	Data 0.000 (0.003)	Loss 0.8798 (0.8881)	Acc@1 82.972 (81.047)	Acc@5 99.381 (98.954)
Max memory in training epoch: 63.4363904
lr: 0.2882984477401199
1
Epoche:85/85; Lr: 0.2882984477401199
batch Size 329
Epoch: [85][0/155]	Time 0.185 (0.185)	Data 0.465 (0.465)	Loss 0.8292 (0.8292)	Acc@1 81.115 (81.115)	Acc@5 99.071 (99.071)
Epoch: [85][64/155]	Time 0.150 (0.142)	Data 0.000 (0.007)	Loss 0.8879 (0.8905)	Acc@1 81.734 (80.948)	Acc@5 99.381 (98.866)
Epoch: [85][128/155]	Time 0.144 (0.142)	Data 0.000 (0.004)	Loss 0.8611 (0.8851)	Acc@1 82.353 (81.002)	Acc@5 98.762 (98.918)
Max memory in training epoch: 63.4363904
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv17.weight

 RM:  module.conv18.weight
numoFStages: 3
Count: 247971 ; 259873 ; 0.954200705729337
[INFO] Storing checkpoint...
  63.97
Max memory: 78.2490112
 22.576s  j: 86 bis 90
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1809
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 86
Max memory: 0.1062912
lr: 0.3705085519785135
1
Epoche:86/90; Lr: 0.3705085519785135
batch Size 329
Epoch: [86][0/152]	Time 0.216 (0.216)	Data 0.484 (0.484)	Loss 0.8689 (0.8689)	Acc@1 82.067 (82.067)	Acc@5 98.784 (98.784)
Epoch: [86][64/152]	Time 0.121 (0.133)	Data 0.000 (0.008)	Loss 0.9542 (0.9320)	Acc@1 80.243 (79.285)	Acc@5 98.784 (98.719)
Epoch: [86][128/152]	Time 0.127 (0.131)	Data 0.000 (0.004)	Loss 1.0781 (0.9563)	Acc@1 75.076 (78.768)	Acc@5 98.784 (98.695)
Max memory in training epoch: 61.241088
lr: 0.3705085519785135
1
Epoche:87/90; Lr: 0.3705085519785135
batch Size 329
Epoch: [87][0/152]	Time 0.165 (0.165)	Data 0.350 (0.350)	Loss 0.9285 (0.9285)	Acc@1 79.331 (79.331)	Acc@5 98.784 (98.784)
Epoch: [87][64/152]	Time 0.119 (0.135)	Data 0.000 (0.006)	Loss 1.0260 (0.9565)	Acc@1 77.812 (79.168)	Acc@5 97.568 (98.695)
Epoch: [87][128/152]	Time 0.151 (0.131)	Data 0.000 (0.003)	Loss 0.9455 (0.9575)	Acc@1 80.851 (79.124)	Acc@5 99.088 (98.666)
Max memory in training epoch: 61.3084672
lr: 0.3705085519785135
1
Epoche:88/90; Lr: 0.3705085519785135
batch Size 329
Epoch: [88][0/152]	Time 0.192 (0.192)	Data 0.434 (0.434)	Loss 0.9536 (0.9536)	Acc@1 78.723 (78.723)	Acc@5 98.480 (98.480)
Epoch: [88][64/152]	Time 0.127 (0.134)	Data 0.000 (0.007)	Loss 0.8494 (0.9398)	Acc@1 81.155 (79.514)	Acc@5 99.392 (98.850)
Epoch: [88][128/152]	Time 0.140 (0.136)	Data 0.000 (0.004)	Loss 0.9924 (0.9495)	Acc@1 78.723 (79.301)	Acc@5 98.176 (98.754)
Max memory in training epoch: 61.3252608
Drin!!
old memory: 634363904
new memory: 613252608
Faktor: 0.9667205276547387
New batch Size größer 359!!
lr: 0.3705085519785135
1
Epoche:89/90; Lr: 0.3705085519785135
batch Size 359
Epoch: [89][0/152]	Time 0.193 (0.193)	Data 0.345 (0.345)	Loss 0.9213 (0.9213)	Acc@1 79.635 (79.635)	Acc@5 98.176 (98.176)
Epoch: [89][64/152]	Time 0.134 (0.137)	Data 0.000 (0.006)	Loss 0.9033 (0.9536)	Acc@1 82.371 (79.261)	Acc@5 98.784 (98.667)
Epoch: [89][128/152]	Time 0.132 (0.135)	Data 0.000 (0.003)	Loss 0.9963 (0.9613)	Acc@1 77.204 (79.004)	Acc@5 98.176 (98.659)
Max memory in training epoch: 61.3252608
lr: 0.3705085519785135
1
Epoche:90/90; Lr: 0.3705085519785135
batch Size 359
Epoch: [90][0/152]	Time 0.158 (0.158)	Data 0.400 (0.400)	Loss 0.9749 (0.9749)	Acc@1 77.204 (77.204)	Acc@5 97.872 (97.872)
Epoch: [90][64/152]	Time 0.128 (0.135)	Data 0.000 (0.006)	Loss 0.8734 (0.9479)	Acc@1 82.979 (79.444)	Acc@5 99.088 (98.719)
Epoch: [90][128/152]	Time 0.142 (0.134)	Data 0.000 (0.003)	Loss 1.0256 (0.9570)	Acc@1 75.988 (79.077)	Acc@5 98.480 (98.655)
Max memory in training epoch: 61.3252608
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 237424 ; 247971 ; 0.9574668005532905
[INFO] Storing checkpoint...
  57.13
Max memory: 74.2463488
 20.917s  j: 91 bis 95
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1749
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 91
Max memory: 0.1020416
lr: 0.5195803521886185
1
Epoche:91/95; Lr: 0.5195803521886185
batch Size 359
Epoch: [91][0/140]	Time 0.220 (0.220)	Data 0.381 (0.381)	Loss 0.9797 (0.9797)	Acc@1 77.159 (77.159)	Acc@5 98.050 (98.050)
Epoch: [91][64/140]	Time 0.145 (0.137)	Data 0.000 (0.006)	Loss 1.1684 (1.0116)	Acc@1 74.652 (77.489)	Acc@5 98.329 (98.414)
Epoch: [91][128/140]	Time 0.133 (0.133)	Data 0.000 (0.003)	Loss 1.1272 (1.0241)	Acc@1 74.095 (77.176)	Acc@5 97.493 (98.370)
Max memory in training epoch: 64.9381376
lr: 0.5195803521886185
1
Epoche:92/95; Lr: 0.5195803521886185
batch Size 359
Epoch: [92][0/140]	Time 0.199 (0.199)	Data 0.430 (0.430)	Loss 1.0035 (1.0035)	Acc@1 76.323 (76.323)	Acc@5 99.164 (99.164)
Epoch: [92][64/140]	Time 0.149 (0.138)	Data 0.000 (0.007)	Loss 1.0387 (1.0257)	Acc@1 79.666 (77.519)	Acc@5 98.607 (98.556)
Epoch: [92][128/140]	Time 0.142 (0.137)	Data 0.000 (0.004)	Loss 1.0207 (1.0140)	Acc@1 76.602 (77.720)	Acc@5 98.050 (98.549)
Max memory in training epoch: 64.809472
lr: 0.5195803521886185
1
Epoche:93/95; Lr: 0.05195803521886186
batch Size 359
Epoch: [93][0/140]	Time 0.220 (0.220)	Data 0.460 (0.460)	Loss 1.0847 (1.0847)	Acc@1 74.930 (74.930)	Acc@5 98.329 (98.329)
Epoch: [93][64/140]	Time 0.141 (0.141)	Data 0.000 (0.007)	Loss 0.8959 (0.8502)	Acc@1 80.501 (83.081)	Acc@5 99.443 (99.134)
Epoch: [93][128/140]	Time 0.126 (0.141)	Data 0.000 (0.004)	Loss 0.7378 (0.7972)	Acc@1 86.908 (84.678)	Acc@5 99.443 (99.274)
Max memory in training epoch: 64.9381376
Drin!!
old memory: 613252608
new memory: 649381376
Faktor: 1.0589133540219693
New batch Size kleiner 380!!
lr: 0.05195803521886186
1
Epoche:94/95; Lr: 0.05195803521886186
batch Size 380
Epoch: [94][0/140]	Time 0.218 (0.218)	Data 0.401 (0.401)	Loss 0.6611 (0.6611)	Acc@1 88.301 (88.301)	Acc@5 100.000 (100.000)
Epoch: [94][64/140]	Time 0.113 (0.142)	Data 0.000 (0.006)	Loss 0.6372 (0.6986)	Acc@1 90.251 (87.302)	Acc@5 99.721 (99.520)
Epoch: [94][128/140]	Time 0.145 (0.141)	Data 0.000 (0.003)	Loss 0.7090 (0.6866)	Acc@1 86.908 (87.519)	Acc@5 99.443 (99.531)
Max memory in training epoch: 64.9381376
lr: 0.05195803521886186
1
Epoche:95/95; Lr: 0.05195803521886186
batch Size 380
Epoch: [95][0/140]	Time 0.177 (0.177)	Data 0.394 (0.394)	Loss 0.6626 (0.6626)	Acc@1 86.072 (86.072)	Acc@5 99.721 (99.721)
Epoch: [95][64/140]	Time 0.153 (0.141)	Data 0.000 (0.006)	Loss 0.6570 (0.6433)	Acc@1 87.465 (88.301)	Acc@5 99.443 (99.623)
Epoch: [95][128/140]	Time 0.133 (0.138)	Data 0.000 (0.003)	Loss 0.6299 (0.6398)	Acc@1 88.579 (88.456)	Acc@5 99.721 (99.588)
Max memory in training epoch: 64.9381376
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 229498 ; 237424 ; 0.966616685760496
[INFO] Storing checkpoint...
  86.03
Max memory: 72.116992
 19.758s  j: 96 bis 100
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 508
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 96
Max memory: 0.0988672
lr: 0.07712520852799808
1
Epoche:96/100; Lr: 0.07712520852799808
batch Size 380
Epoch: [96][0/132]	Time 0.251 (0.251)	Data 0.434 (0.434)	Loss 0.6253 (0.6253)	Acc@1 87.895 (87.895)	Acc@5 100.000 (100.000)
Epoch: [96][64/132]	Time 0.135 (0.142)	Data 0.000 (0.007)	Loss 0.6674 (0.6243)	Acc@1 86.316 (88.409)	Acc@5 99.474 (99.538)
Epoch: [96][128/132]	Time 0.119 (0.140)	Data 0.000 (0.004)	Loss 0.6108 (0.6223)	Acc@1 88.684 (88.441)	Acc@5 99.737 (99.568)
Max memory in training epoch: 67.919616
lr: 0.07712520852799808
1
Epoche:97/100; Lr: 0.07712520852799808
batch Size 380
Epoch: [97][0/132]	Time 0.248 (0.248)	Data 0.414 (0.414)	Loss 0.5836 (0.5836)	Acc@1 90.789 (90.789)	Acc@5 100.000 (100.000)
Epoch: [97][64/132]	Time 0.151 (0.144)	Data 0.000 (0.007)	Loss 0.5694 (0.5977)	Acc@1 90.000 (88.567)	Acc@5 100.000 (99.668)
Epoch: [97][128/132]	Time 0.140 (0.142)	Data 0.000 (0.003)	Loss 0.6219 (0.5959)	Acc@1 89.211 (88.537)	Acc@5 99.474 (99.643)
Max memory in training epoch: 67.6855296
lr: 0.07712520852799808
1
Epoche:98/100; Lr: 0.07712520852799808
batch Size 380
Epoch: [98][0/132]	Time 0.214 (0.214)	Data 0.432 (0.432)	Loss 0.5746 (0.5746)	Acc@1 89.474 (89.474)	Acc@5 100.000 (100.000)
Epoch: [98][64/132]	Time 0.136 (0.142)	Data 0.000 (0.007)	Loss 0.5937 (0.5937)	Acc@1 88.684 (88.474)	Acc@5 99.737 (99.615)
Epoch: [98][128/132]	Time 0.124 (0.140)	Data 0.000 (0.004)	Loss 0.6316 (0.5880)	Acc@1 87.632 (88.509)	Acc@5 99.737 (99.631)
Max memory in training epoch: 67.6855296
Drin!!
old memory: 649381376
new memory: 676855296
Faktor: 1.0423078348338712
New batch Size kleiner 396!!
lr: 0.07712520852799808
1
Epoche:99/100; Lr: 0.07712520852799808
batch Size 396
Epoch: [99][0/132]	Time 0.184 (0.184)	Data 0.401 (0.401)	Loss 0.5614 (0.5614)	Acc@1 88.684 (88.684)	Acc@5 99.474 (99.474)
Epoch: [99][64/132]	Time 0.129 (0.147)	Data 0.000 (0.006)	Loss 0.5270 (0.5694)	Acc@1 90.263 (88.883)	Acc@5 99.737 (99.587)
Epoch: [99][128/132]	Time 0.144 (0.144)	Data 0.000 (0.003)	Loss 0.6384 (0.5758)	Acc@1 84.737 (88.576)	Acc@5 100.000 (99.565)
Max memory in training epoch: 67.6855296
lr: 0.07712520852799808
1
Epoche:100/100; Lr: 0.07712520852799808
batch Size 396
Epoch: [100][0/132]	Time 0.196 (0.196)	Data 0.395 (0.395)	Loss 0.5558 (0.5558)	Acc@1 87.105 (87.105)	Acc@5 100.000 (100.000)
Epoch: [100][64/132]	Time 0.142 (0.142)	Data 0.000 (0.006)	Loss 0.5436 (0.5572)	Acc@1 89.211 (89.012)	Acc@5 100.000 (99.632)
Epoch: [100][128/132]	Time 0.126 (0.138)	Data 0.000 (0.003)	Loss 0.5728 (0.5643)	Acc@1 88.158 (88.611)	Acc@5 99.737 (99.612)
Max memory in training epoch: 67.6855296
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 228090 ; 229498 ; 0.9938648702820939
[INFO] Storing checkpoint...
  83.84
Max memory: 71.0875648
 18.694s  j: 101 bis 105
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8093
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 101
Max memory: 0.0982528
lr: 0.11930305694174702
1
Epoche:101/105; Lr: 0.11930305694174702
batch Size 396
Epoch: [101][0/127]	Time 0.254 (0.254)	Data 0.361 (0.361)	Loss 0.5531 (0.5531)	Acc@1 90.152 (90.152)	Acc@5 99.242 (99.242)
Epoch: [101][64/127]	Time 0.151 (0.139)	Data 0.000 (0.006)	Loss 0.6190 (0.6058)	Acc@1 87.121 (87.222)	Acc@5 99.747 (99.542)
Max memory in training epoch: 69.9116032
lr: 0.11930305694174702
1
Epoche:102/105; Lr: 0.11930305694174702
batch Size 396
Epoch: [102][0/127]	Time 0.186 (0.186)	Data 0.393 (0.393)	Loss 0.5605 (0.5605)	Acc@1 88.131 (88.131)	Acc@5 98.990 (98.990)
Epoch: [102][64/127]	Time 0.132 (0.142)	Data 0.000 (0.006)	Loss 0.6368 (0.6516)	Acc@1 86.364 (86.142)	Acc@5 99.242 (99.398)
Max memory in training epoch: 69.9116032
lr: 0.11930305694174702
1
Epoche:103/105; Lr: 0.11930305694174702
batch Size 396
Epoch: [103][0/127]	Time 0.218 (0.218)	Data 0.412 (0.412)	Loss 0.6473 (0.6473)	Acc@1 85.859 (85.859)	Acc@5 98.485 (98.485)
Epoch: [103][64/127]	Time 0.134 (0.144)	Data 0.000 (0.007)	Loss 0.7102 (0.6551)	Acc@1 83.586 (85.715)	Acc@5 98.990 (99.375)
Max memory in training epoch: 69.9116032
Drin!!
old memory: 676855296
new memory: 699116032
Faktor: 1.0328884713343516
New batch Size kleiner 409!!
lr: 0.11930305694174702
1
Epoche:104/105; Lr: 0.11930305694174702
batch Size 409
Epoch: [104][0/127]	Time 0.175 (0.175)	Data 0.345 (0.345)	Loss 0.7153 (0.7153)	Acc@1 86.869 (86.869)	Acc@5 98.990 (98.990)
Epoch: [104][64/127]	Time 0.136 (0.146)	Data 0.000 (0.006)	Loss 0.6435 (0.6422)	Acc@1 87.626 (86.690)	Acc@5 99.495 (99.409)
Max memory in training epoch: 69.9116032
lr: 0.11930305694174702
1
Epoche:105/105; Lr: 0.11930305694174702
batch Size 409
Epoch: [105][0/127]	Time 0.250 (0.250)	Data 0.472 (0.472)	Loss 0.5897 (0.5897)	Acc@1 89.141 (89.141)	Acc@5 99.495 (99.495)
Epoch: [105][64/127]	Time 0.164 (0.140)	Data 0.000 (0.007)	Loss 0.7072 (0.6334)	Acc@1 85.859 (86.795)	Acc@5 98.990 (99.487)
Max memory in training epoch: 69.9116032
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 225276 ; 228090 ; 0.9876627646981455
[INFO] Storing checkpoint...
  78.11
Max memory: 71.113984
 18.499s  j: 106 bis 110
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2999
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 106
Max memory: 0.0969216
lr: 0.190605274567088
1
Epoche:106/110; Lr: 0.190605274567088
batch Size 409
Epoch: [106][0/123]	Time 0.276 (0.276)	Data 0.382 (0.382)	Loss 0.5915 (0.5915)	Acc@1 88.753 (88.753)	Acc@5 99.756 (99.756)
Epoch: [106][64/123]	Time 0.149 (0.146)	Data 0.000 (0.006)	Loss 0.7654 (0.7120)	Acc@1 83.619 (84.138)	Acc@5 99.022 (99.225)
Max memory in training epoch: 72.3298304
lr: 0.190605274567088
1
Epoche:107/110; Lr: 0.190605274567088
batch Size 409
Epoch: [107][0/123]	Time 0.194 (0.194)	Data 0.410 (0.410)	Loss 0.7527 (0.7527)	Acc@1 85.575 (85.575)	Acc@5 99.022 (99.022)
Epoch: [107][64/123]	Time 0.134 (0.143)	Data 0.000 (0.007)	Loss 0.6999 (0.7304)	Acc@1 84.108 (83.957)	Acc@5 99.756 (99.383)
Max memory in training epoch: 72.3298304
lr: 0.190605274567088
1
Epoche:108/110; Lr: 0.190605274567088
batch Size 409
Epoch: [108][0/123]	Time 0.232 (0.232)	Data 0.392 (0.392)	Loss 0.7902 (0.7902)	Acc@1 81.174 (81.174)	Acc@5 99.511 (99.511)
Epoch: [108][64/123]	Time 0.152 (0.147)	Data 0.000 (0.006)	Loss 0.8062 (0.7229)	Acc@1 83.619 (84.661)	Acc@5 99.267 (99.285)
Max memory in training epoch: 72.3298304
Drin!!
old memory: 699116032
new memory: 723298304
Faktor: 1.0345897832307185
New batch Size kleiner 423!!
lr: 0.190605274567088
1
Epoche:109/110; Lr: 0.190605274567088
batch Size 423
Epoch: [109][0/123]	Time 0.240 (0.240)	Data 0.403 (0.403)	Loss 0.7352 (0.7352)	Acc@1 83.619 (83.619)	Acc@5 99.267 (99.267)
Epoch: [109][64/123]	Time 0.154 (0.149)	Data 0.000 (0.006)	Loss 0.6853 (0.7286)	Acc@1 86.797 (84.672)	Acc@5 99.756 (99.214)
Max memory in training epoch: 72.3298304
lr: 0.190605274567088
1
Epoche:110/110; Lr: 0.190605274567088
batch Size 423
Epoch: [110][0/123]	Time 0.211 (0.211)	Data 0.365 (0.365)	Loss 0.7412 (0.7412)	Acc@1 83.619 (83.619)	Acc@5 99.511 (99.511)
Epoch: [110][64/123]	Time 0.148 (0.146)	Data 0.000 (0.006)	Loss 0.7753 (0.7312)	Acc@1 82.885 (84.153)	Acc@5 99.022 (99.278)
Max memory in training epoch: 72.3298304
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 224158 ; 225276 ; 0.9950371988138994
[INFO] Storing checkpoint...
  76.84
Max memory: 72.3298304
 18.385s  j: 111 bis 115
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4340
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 111
Max memory: 0.096512
lr: 0.31494543414796183
1
Epoche:111/115; Lr: 0.31494543414796183
batch Size 423
Epoch: [111][0/119]	Time 0.271 (0.271)	Data 0.457 (0.457)	Loss 0.6665 (0.6665)	Acc@1 87.470 (87.470)	Acc@5 99.764 (99.764)
Epoch: [111][64/119]	Time 0.136 (0.146)	Data 0.000 (0.007)	Loss 0.8467 (0.8174)	Acc@1 83.215 (81.604)	Acc@5 99.054 (98.982)
Max memory in training epoch: 74.4992256
lr: 0.31494543414796183
1
Epoche:112/115; Lr: 0.31494543414796183
batch Size 423
Epoch: [112][0/119]	Time 0.195 (0.195)	Data 0.508 (0.508)	Loss 0.8305 (0.8305)	Acc@1 82.979 (82.979)	Acc@5 99.291 (99.291)
Epoch: [112][64/119]	Time 0.141 (0.147)	Data 0.000 (0.008)	Loss 0.8623 (0.8662)	Acc@1 82.033 (80.942)	Acc@5 99.054 (98.985)
Max memory in training epoch: 74.4992256
lr: 0.31494543414796183
1
Epoche:113/115; Lr: 0.31494543414796183
batch Size 423
Epoch: [113][0/119]	Time 0.227 (0.227)	Data 0.400 (0.400)	Loss 0.8398 (0.8398)	Acc@1 82.033 (82.033)	Acc@5 99.527 (99.527)
Epoch: [113][64/119]	Time 0.153 (0.149)	Data 0.000 (0.006)	Loss 0.8304 (0.8531)	Acc@1 81.560 (81.546)	Acc@5 99.764 (99.011)
Max memory in training epoch: 74.4992256
Drin!!
old memory: 723298304
new memory: 744992256
Faktor: 1.0299930912045938
New batch Size kleiner 435!!
lr: 0.31494543414796183
1
Epoche:114/115; Lr: 0.31494543414796183
batch Size 435
Epoch: [114][0/119]	Time 0.211 (0.211)	Data 0.398 (0.398)	Loss 0.8527 (0.8527)	Acc@1 82.270 (82.270)	Acc@5 98.818 (98.818)
Epoch: [114][64/119]	Time 0.144 (0.143)	Data 0.000 (0.006)	Loss 0.8768 (0.8600)	Acc@1 80.851 (81.244)	Acc@5 99.291 (98.989)
Max memory in training epoch: 74.4992256
lr: 0.31494543414796183
1
Epoche:115/115; Lr: 0.31494543414796183
batch Size 435
Epoch: [115][0/119]	Time 0.200 (0.200)	Data 0.484 (0.484)	Loss 0.8205 (0.8205)	Acc@1 81.797 (81.797)	Acc@5 99.054 (99.054)
Epoch: [115][64/119]	Time 0.140 (0.150)	Data 0.000 (0.008)	Loss 0.8668 (0.8420)	Acc@1 81.560 (81.720)	Acc@5 98.345 (99.116)
Max memory in training epoch: 74.4992256
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 223146 ; 224158 ; 0.9954853273137697
[INFO] Storing checkpoint...
  64.14
Max memory: 74.4992256
 18.170s  j: 116 bis 120
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 139
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 116
Max memory: 0.0961536
lr: 0.535161186931107
1
Epoche:116/120; Lr: 0.535161186931107
batch Size 435
Epoch: [116][0/115]	Time 0.246 (0.246)	Data 0.431 (0.431)	Loss 0.8358 (0.8358)	Acc@1 81.379 (81.379)	Acc@5 99.540 (99.540)
Epoch: [116][64/115]	Time 0.175 (0.148)	Data 0.000 (0.007)	Loss 1.0667 (0.9541)	Acc@1 72.874 (78.500)	Acc@5 98.851 (98.649)
Max memory in training epoch: 77.2859904
lr: 0.535161186931107
1
Epoche:117/120; Lr: 0.535161186931107
batch Size 435
Epoch: [117][0/115]	Time 0.215 (0.215)	Data 0.395 (0.395)	Loss 1.0492 (1.0492)	Acc@1 76.092 (76.092)	Acc@5 99.080 (99.080)
Epoch: [117][64/115]	Time 0.121 (0.152)	Data 0.000 (0.006)	Loss 1.0090 (0.9606)	Acc@1 78.391 (79.070)	Acc@5 98.161 (98.766)
Max memory in training epoch: 77.2859904
lr: 0.535161186931107
1
Epoche:118/120; Lr: 0.535161186931107
batch Size 435
Epoch: [118][0/115]	Time 0.190 (0.190)	Data 0.393 (0.393)	Loss 1.0058 (1.0058)	Acc@1 77.241 (77.241)	Acc@5 98.391 (98.391)
Epoch: [118][64/115]	Time 0.160 (0.153)	Data 0.000 (0.006)	Loss 0.9941 (0.9754)	Acc@1 77.471 (78.539)	Acc@5 98.621 (98.656)
Max memory in training epoch: 77.2859904
Drin!!
old memory: 744992256
new memory: 772859904
Faktor: 1.0374066277542622
New batch Size kleiner 451!!
lr: 0.535161186931107
1
Epoche:119/120; Lr: 0.535161186931107
batch Size 451
Epoch: [119][0/115]	Time 0.305 (0.305)	Data 0.455 (0.455)	Loss 0.9055 (0.9055)	Acc@1 83.448 (83.448)	Acc@5 98.161 (98.161)
Epoch: [119][64/115]	Time 0.158 (0.154)	Data 0.000 (0.007)	Loss 0.8939 (0.9676)	Acc@1 80.230 (78.207)	Acc@5 99.540 (98.716)
Max memory in training epoch: 77.2859904
lr: 0.535161186931107
1
Epoche:120/120; Lr: 0.535161186931107
batch Size 451
Epoch: [120][0/115]	Time 0.192 (0.192)	Data 0.492 (0.492)	Loss 1.0046 (1.0046)	Acc@1 78.851 (78.851)	Acc@5 99.080 (99.080)
Epoch: [120][64/115]	Time 0.137 (0.150)	Data 0.000 (0.008)	Loss 0.9705 (0.9598)	Acc@1 78.391 (78.568)	Acc@5 98.851 (98.670)
Max memory in training epoch: 77.2859904
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 216708 ; 223146 ; 0.9711489338818531
[INFO] Storing checkpoint...
  54.74
Max memory: 77.2859904
 17.699s  j: 121 bis 125
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2556
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 121
Max memory: 0.0935936
lr: 0.9428034972887862
1
Epoche:121/125; Lr: 0.9428034972887862
batch Size 451
Epoch: [121][0/111]	Time 0.276 (0.276)	Data 0.433 (0.433)	Loss 0.8988 (0.8988)	Acc@1 82.040 (82.040)	Acc@5 98.891 (98.891)
Epoch: [121][64/111]	Time 0.144 (0.151)	Data 0.000 (0.007)	Loss 1.2297 (1.1224)	Acc@1 70.953 (73.713)	Acc@5 97.561 (97.885)
Max memory in training epoch: 79.0296576
lr: 0.9428034972887862
1
Epoche:122/125; Lr: 0.9428034972887862
batch Size 451
Epoch: [122][0/111]	Time 0.218 (0.218)	Data 0.378 (0.378)	Loss 1.1250 (1.1250)	Acc@1 74.058 (74.058)	Acc@5 98.004 (98.004)
Epoch: [122][64/111]	Time 0.133 (0.149)	Data 0.000 (0.006)	Loss 1.0593 (1.1229)	Acc@1 75.831 (74.593)	Acc@5 98.226 (97.981)
Max memory in training epoch: 79.0296576
lr: 0.9428034972887862
1
Epoche:123/125; Lr: 0.9428034972887862
batch Size 451
Epoch: [123][0/111]	Time 0.247 (0.247)	Data 0.344 (0.344)	Loss 1.1830 (1.1830)	Acc@1 70.732 (70.732)	Acc@5 97.561 (97.561)
Epoch: [123][64/111]	Time 0.134 (0.154)	Data 0.000 (0.006)	Loss 1.1676 (1.1202)	Acc@1 74.058 (73.911)	Acc@5 97.561 (97.981)
Max memory in training epoch: 79.0296576
Drin!!
old memory: 772859904
new memory: 790296576
Faktor: 1.0225612325206095
New batch Size kleiner 461!!
lr: 0.9428034972887862
1
Epoche:124/125; Lr: 0.9428034972887862
batch Size 461
Epoch: [124][0/111]	Time 0.256 (0.256)	Data 0.462 (0.462)	Loss 1.1320 (1.1320)	Acc@1 72.949 (72.949)	Acc@5 98.226 (98.226)
Epoch: [124][64/111]	Time 0.146 (0.153)	Data 0.000 (0.007)	Loss 1.1036 (1.0957)	Acc@1 74.945 (74.511)	Acc@5 96.896 (98.161)
Max memory in training epoch: 79.0296576
lr: 0.9428034972887862
1
Epoche:125/125; Lr: 0.9428034972887862
batch Size 461
Epoch: [125][0/111]	Time 0.206 (0.206)	Data 0.440 (0.440)	Loss 0.9785 (0.9785)	Acc@1 80.710 (80.710)	Acc@5 98.670 (98.670)
Epoch: [125][64/111]	Time 0.155 (0.149)	Data 0.000 (0.007)	Loss 1.1349 (1.1114)	Acc@1 72.949 (73.812)	Acc@5 97.339 (97.991)
Max memory in training epoch: 79.0296576
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 195226 ; 216708 ; 0.900871218413718
[INFO] Storing checkpoint...
  48.65
Max memory: 79.0296576
 17.044s  j: 126 bis 130
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1575
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 126
Max memory: 0.0849408
lr: 1.697782860352072
1
Epoche:126/130; Lr: 1.697782860352072
batch Size 461
Epoch: [126][0/109]	Time 0.260 (0.260)	Data 0.444 (0.444)	Loss 1.1000 (1.1000)	Acc@1 74.837 (74.837)	Acc@5 98.048 (98.048)
Epoch: [126][64/109]	Time 0.131 (0.149)	Data 0.000 (0.007)	Loss 1.2734 (1.3024)	Acc@1 71.800 (68.350)	Acc@5 97.180 (97.023)
Max memory in training epoch: 76.1839616
lr: 1.697782860352072
1
Epoche:127/130; Lr: 1.697782860352072
batch Size 461
Epoch: [127][0/109]	Time 0.199 (0.199)	Data 0.382 (0.382)	Loss 1.3645 (1.3645)	Acc@1 65.510 (65.510)	Acc@5 96.746 (96.746)
Epoch: [127][64/109]	Time 0.158 (0.152)	Data 0.000 (0.006)	Loss 1.3321 (1.3218)	Acc@1 67.245 (67.509)	Acc@5 96.963 (97.010)
Max memory in training epoch: 76.1839616
lr: 1.697782860352072
1
Epoche:128/130; Lr: 1.697782860352072
batch Size 461
Epoch: [128][0/109]	Time 0.217 (0.217)	Data 0.463 (0.463)	Loss 1.3867 (1.3867)	Acc@1 65.510 (65.510)	Acc@5 96.095 (96.095)
Epoch: [128][64/109]	Time 0.153 (0.147)	Data 0.000 (0.007)	Loss 1.2267 (1.2917)	Acc@1 70.282 (68.123)	Acc@5 96.312 (96.890)
Max memory in training epoch: 76.1839616
Drin!!
old memory: 790296576
new memory: 761839616
Faktor: 0.9639920494859894
New batch Size größer 405!!
lr: 1.697782860352072
1
Epoche:129/130; Lr: 1.697782860352072
batch Size 405
Epoch: [129][0/109]	Time 0.198 (0.198)	Data 0.402 (0.402)	Loss 1.4162 (1.4162)	Acc@1 64.642 (64.642)	Acc@5 96.095 (96.095)
Epoch: [129][64/109]	Time 0.139 (0.151)	Data 0.000 (0.006)	Loss 1.3993 (1.2935)	Acc@1 62.907 (67.903)	Acc@5 97.831 (96.913)
Max memory in training epoch: 76.1839616
lr: 1.697782860352072
1
Epoche:130/130; Lr: 1.697782860352072
batch Size 405
Epoch: [130][0/109]	Time 0.250 (0.250)	Data 0.401 (0.401)	Loss 1.3380 (1.3380)	Acc@1 65.944 (65.944)	Acc@5 96.312 (96.312)
Epoch: [130][64/109]	Time 0.143 (0.153)	Data 0.000 (0.006)	Loss 1.3291 (1.3158)	Acc@1 60.304 (66.508)	Acc@5 97.180 (96.746)
Max memory in training epoch: 76.1839616
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 176028 ; 195226 ; 0.9016626883714259
[INFO] Storing checkpoint...
  26.37
Max memory: 76.1839616
 17.051s  j: 131 bis 135
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9110
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0773632
lr: 2.685945540791364
1
Epoche:131/135; Lr: 2.685945540791364
batch Size 405
Epoch: [131][0/124]	Time 0.307 (0.307)	Data 0.381 (0.381)	Loss 1.3863 (1.3863)	Acc@1 63.210 (63.210)	Acc@5 95.802 (95.802)
Epoch: [131][64/124]	Time 0.125 (0.143)	Data 0.000 (0.006)	Loss 1.6782 (1.5277)	Acc@1 53.333 (59.696)	Acc@5 91.358 (95.069)
Max memory in training epoch: 66.6604544
lr: 2.685945540791364
1
Epoche:132/135; Lr: 2.685945540791364
batch Size 405
Epoch: [132][0/124]	Time 0.184 (0.184)	Data 0.373 (0.373)	Loss 1.5684 (1.5684)	Acc@1 55.802 (55.802)	Acc@5 93.580 (93.580)
Epoch: [132][64/124]	Time 0.144 (0.145)	Data 0.000 (0.006)	Loss 1.5301 (1.5018)	Acc@1 57.284 (59.069)	Acc@5 95.556 (95.172)
Max memory in training epoch: 66.6604544
lr: 2.685945540791364
1
Epoche:133/135; Lr: 2.685945540791364
batch Size 405
Epoch: [133][0/124]	Time 0.213 (0.213)	Data 0.396 (0.396)	Loss 1.6273 (1.6273)	Acc@1 55.802 (55.802)	Acc@5 95.309 (95.309)
Epoch: [133][64/124]	Time 0.133 (0.137)	Data 0.000 (0.006)	Loss 1.5221 (1.5140)	Acc@1 57.037 (58.017)	Acc@5 93.333 (94.845)
Max memory in training epoch: 66.6604544
Drin!!
old memory: 761839616
new memory: 666604544
Faktor: 0.874993279425364
New batch Size größer 407!!
lr: 2.685945540791364
1
Epoche:134/135; Lr: 2.685945540791364
batch Size 407
Epoch: [134][0/124]	Time 0.179 (0.179)	Data 0.371 (0.371)	Loss 1.5520 (1.5520)	Acc@1 57.284 (57.284)	Acc@5 94.321 (94.321)
Epoch: [134][64/124]	Time 0.123 (0.139)	Data 0.000 (0.006)	Loss 1.5396 (1.5324)	Acc@1 56.049 (56.395)	Acc@5 93.827 (94.488)
Max memory in training epoch: 66.6604544
lr: 2.685945540791364
1
Epoche:135/135; Lr: 2.685945540791364
batch Size 407
Epoch: [135][0/124]	Time 0.198 (0.198)	Data 0.429 (0.429)	Loss 1.5274 (1.5274)	Acc@1 57.531 (57.531)	Acc@5 93.333 (93.333)
Epoch: [135][64/124]	Time 0.135 (0.145)	Data 0.000 (0.007)	Loss 1.5357 (1.4622)	Acc@1 54.815 (57.508)	Acc@5 94.568 (94.811)
Max memory in training epoch: 66.6604544
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 171786 ; 176028 ; 0.9759015611152771
[INFO] Storing checkpoint...
  20.88
Max memory: 66.6604544
 18.356s  j: 136 bis 140
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6778
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.0756224
lr: 4.2702337308675204
1
Epoche:136/140; Lr: 4.2702337308675204
batch Size 407
Epoch: [136][0/123]	Time 0.235 (0.235)	Data 0.404 (0.404)	Loss 1.7060 (1.7060)	Acc@1 48.894 (48.894)	Acc@5 93.857 (93.857)
Epoch: [136][64/123]	Time 0.115 (0.141)	Data 0.000 (0.006)	Loss 2.2826 (2.2297)	Acc@1 25.799 (31.136)	Acc@5 77.150 (80.159)
Max memory in training epoch: 66.0603904
lr: 4.2702337308675204
1
Epoche:137/140; Lr: 4.2702337308675204
batch Size 407
Epoch: [137][0/123]	Time 0.202 (0.202)	Data 0.420 (0.420)	Loss 2.3238 (2.3238)	Acc@1 22.113 (22.113)	Acc@5 75.430 (75.430)
Epoch: [137][64/123]	Time 0.148 (0.142)	Data 0.000 (0.007)	Loss 1.9807 (2.1291)	Acc@1 29.484 (23.459)	Acc@5 79.607 (80.635)
Max memory in training epoch: 66.0516352
lr: 4.2702337308675204
1
Epoche:138/140; Lr: 4.2702337308675204
batch Size 407
Epoch: [138][0/123]	Time 0.219 (0.219)	Data 0.412 (0.412)	Loss 2.1817 (2.1817)	Acc@1 16.953 (16.953)	Acc@5 77.641 (77.641)
Epoch: [138][64/123]	Time 0.127 (0.141)	Data 0.000 (0.007)	Loss 2.1303 (2.1664)	Acc@1 23.587 (19.543)	Acc@5 71.253 (76.575)
Max memory in training epoch: 66.0516352
Drin!!
old memory: 666604544
new memory: 660516352
Faktor: 0.9908668609375696
New batch Size größer 412!!
lr: 4.2702337308675204
1
Epoche:139/140; Lr: 4.2702337308675204
batch Size 412
Epoch: [139][0/123]	Time 0.185 (0.185)	Data 0.368 (0.368)	Loss 2.2622 (2.2622)	Acc@1 17.199 (17.199)	Acc@5 64.373 (64.373)
Epoch: [139][64/123]	Time 0.138 (0.144)	Data 0.000 (0.006)	Loss 2.3830 (2.2434)	Acc@1 9.091 (16.341)	Acc@5 52.334 (67.775)
Max memory in training epoch: 66.0516352
lr: 4.2702337308675204
1
Epoche:140/140; Lr: 4.2702337308675204
batch Size 412
Epoch: [140][0/123]	Time 0.206 (0.206)	Data 0.407 (0.407)	Loss 2.3280 (2.3280)	Acc@1 12.039 (12.039)	Acc@5 55.528 (55.528)
Epoch: [140][64/123]	Time 0.135 (0.142)	Data 0.000 (0.007)	Loss 2.3075 (2.3352)	Acc@1 11.302 (12.017)	Acc@5 66.830 (57.199)
Max memory in training epoch: 66.0516352
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 170938 ; 171786 ; 0.9950636256738035
[INFO] Storing checkpoint...
  10.0
Max memory: 66.0516352
 17.656s  j: 141 bis 145
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5810
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 141
Max memory: 0.0753152
lr: 6.872407410614915
1
Epoche:141/145; Lr: 6.872407410614915
batch Size 412
Epoch: [141][0/122]	Time 0.299 (0.299)	Data 0.371 (0.371)	Loss 2.3913 (2.3913)	Acc@1 10.922 (10.922)	Acc@5 52.913 (52.913)
Epoch: [141][64/122]	Time 0.175 (0.142)	Data 0.000 (0.006)	Loss 2.3508 (2.3554)	Acc@1 10.680 (10.261)	Acc@5 50.971 (50.254)
Max memory in training epoch: 66.880256
lr: 6.872407410614915
1
Epoche:142/145; Lr: 6.872407410614915
batch Size 412
Epoch: [142][0/122]	Time 0.226 (0.226)	Data 0.440 (0.440)	Loss 2.3215 (2.3215)	Acc@1 12.379 (12.379)	Acc@5 53.883 (53.883)
Epoch: [142][64/122]	Time 0.152 (0.139)	Data 0.000 (0.007)	Loss 2.3592 (2.3525)	Acc@1 8.252 (10.112)	Acc@5 50.485 (49.817)
Max memory in training epoch: 66.880256
lr: 6.872407410614915
1
Epoche:143/145; Lr: 6.872407410614915
batch Size 412
Epoch: [143][0/122]	Time 0.230 (0.230)	Data 0.375 (0.375)	Loss 2.3398 (2.3398)	Acc@1 10.437 (10.437)	Acc@5 50.000 (50.000)
Epoch: [143][64/122]	Time 0.141 (0.141)	Data 0.000 (0.006)	Loss 2.3786 (2.3513)	Acc@1 11.165 (10.000)	Acc@5 48.544 (50.168)
Max memory in training epoch: 66.880256
Drin!!
old memory: 660516352
new memory: 668802560
Faktor: 1.012545045970338
New batch Size kleiner 417!!
lr: 6.872407410614915
1
Epoche:144/145; Lr: 6.872407410614915
batch Size 417
Epoch: [144][0/122]	Time 0.248 (0.248)	Data 0.370 (0.370)	Loss 2.3445 (2.3445)	Acc@1 11.165 (11.165)	Acc@5 50.971 (50.971)
Epoch: [144][64/122]	Time 0.124 (0.142)	Data 0.000 (0.006)	Loss 2.3842 (2.3464)	Acc@1 7.039 (10.134)	Acc@5 46.602 (49.746)
Max memory in training epoch: 66.880256
lr: 6.872407410614915
1
Epoche:145/145; Lr: 6.872407410614915
batch Size 417
Epoch: [145][0/122]	Time 0.191 (0.191)	Data 0.504 (0.504)	Loss 2.4799 (2.4799)	Acc@1 9.709 (9.709)	Acc@5 47.330 (47.330)
Epoch: [145][64/122]	Time 0.146 (0.147)	Data 0.000 (0.008)	Loss 2.3191 (2.3512)	Acc@1 11.165 (9.981)	Acc@5 50.000 (49.996)
Max memory in training epoch: 66.880256
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 168682 ; 170938 ; 0.9868022323883513
[INFO] Storing checkpoint...
  10.0
Max memory: 66.880256
 18.133s  j: 146 bis 150
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5299
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 146
Max memory: 0.074496
lr: 11.194507383696951
1
Epoche:146/150; Lr: 11.194507383696951
batch Size 417
Epoch: [146][0/120]	Time 0.267 (0.267)	Data 0.406 (0.406)	Loss 2.3451 (2.3451)	Acc@1 9.592 (9.592)	Acc@5 51.319 (51.319)
Epoch: [146][64/120]	Time 0.139 (0.142)	Data 0.000 (0.006)	Loss 2.3817 (2.3820)	Acc@1 10.552 (10.172)	Acc@5 53.237 (49.840)
Max memory in training epoch: 66.710784
lr: 11.194507383696951
1
Epoche:147/150; Lr: 11.194507383696951
batch Size 417
Epoch: [147][0/120]	Time 0.228 (0.228)	Data 0.409 (0.409)	Loss 2.6606 (2.6606)	Acc@1 7.914 (7.914)	Acc@5 50.839 (50.839)
Epoch: [147][64/120]	Time 0.147 (0.145)	Data 0.000 (0.007)	Loss 2.4434 (2.5158)	Acc@1 8.153 (9.880)	Acc@5 53.477 (49.729)
Max memory in training epoch: 66.6724352
lr: 11.194507383696951
1
Epoche:148/150; Lr: 11.194507383696951
batch Size 417
Epoch: [148][0/120]	Time 0.225 (0.225)	Data 0.450 (0.450)	Loss 5.9960 (5.9960)	Acc@1 10.072 (10.072)	Acc@5 50.120 (50.120)
Epoch: [148][64/120]	Time 0.160 (0.148)	Data 0.000 (0.007)	Loss 6.6292 (6.1934)	Acc@1 10.312 (9.662)	Acc@5 47.002 (49.954)
Max memory in training epoch: 66.6724352
Drin!!
old memory: 668802560
new memory: 666724352
Faktor: 0.996892643473135
New batch Size größer 419!!
lr: 11.194507383696951
1
Epoche:149/150; Lr: 11.194507383696951
batch Size 419
Epoch: [149][0/120]	Time 0.200 (0.200)	Data 0.424 (0.424)	Loss 19.6139 (19.6139)	Acc@1 9.113 (9.113)	Acc@5 50.360 (50.360)
Epoch: [149][64/120]	Time 0.156 (0.146)	Data 0.000 (0.007)	Loss 11.6773 (44.1621)	Acc@1 10.552 (10.205)	Acc@5 51.319 (49.784)
Max memory in training epoch: 66.6724352
lr: 11.194507383696951
1
Epoche:150/150; Lr: 1.1194507383696952
batch Size 419
Epoch: [150][0/120]	Time 0.200 (0.200)	Data 0.426 (0.426)	Loss 12.4557 (12.4557)	Acc@1 12.950 (12.950)	Acc@5 55.396 (55.396)
Epoch: [150][64/120]	Time 0.152 (0.145)	Data 0.000 (0.007)	Loss 5.4239 (6.8922)	Acc@1 7.914 (10.050)	Acc@5 44.604 (49.954)
Max memory in training epoch: 66.6724352
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 159410 ; 168682 ; 0.9450326650146429
[INFO] Storing checkpoint...
  10.0
Max memory: 66.6724352
 17.808s  j: 151 bis 155
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4581
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 151
Max memory: 0.0707072
lr: 1.8322260131910246
1
Epoche:151/155; Lr: 1.8322260131910246
batch Size 419
Epoch: [151][0/120]	Time 0.258 (0.258)	Data 0.445 (0.445)	Loss 4.5264 (4.5264)	Acc@1 10.024 (10.024)	Acc@5 51.313 (51.313)
Epoch: [151][64/120]	Time 0.128 (0.143)	Data 0.000 (0.007)	Loss 3.5979 (4.0879)	Acc@1 10.263 (9.822)	Acc@5 50.119 (49.932)
Max memory in training epoch: 66.7213824
lr: 1.8322260131910246
1
Epoche:152/155; Lr: 1.8322260131910246
batch Size 419
Epoch: [152][0/120]	Time 0.214 (0.214)	Data 0.423 (0.423)	Loss 3.0207 (3.0207)	Acc@1 8.592 (8.592)	Acc@5 57.041 (57.041)
Epoch: [152][64/120]	Time 0.121 (0.140)	Data 0.000 (0.007)	Loss 2.6855 (2.8541)	Acc@1 13.126 (9.980)	Acc@5 50.358 (50.494)
Max memory in training epoch: 66.2585856
lr: 1.8322260131910246
1
Epoche:153/155; Lr: 1.8322260131910246
batch Size 419
Epoch: [153][0/120]	Time 0.257 (0.257)	Data 0.417 (0.417)	Loss 2.5243 (2.5243)	Acc@1 8.115 (8.115)	Acc@5 50.835 (50.835)
Epoch: [153][64/120]	Time 0.123 (0.146)	Data 0.000 (0.007)	Loss 2.4173 (2.4653)	Acc@1 9.785 (9.983)	Acc@5 49.881 (50.479)
Max memory in training epoch: 66.2585856
Drin!!
old memory: 666724352
new memory: 662585856
Faktor: 0.9937927930971989
New batch Size größer 423!!
lr: 1.8322260131910246
1
Epoche:154/155; Lr: 1.8322260131910246
batch Size 423
Epoch: [154][0/120]	Time 0.249 (0.249)	Data 0.371 (0.371)	Loss 2.3761 (2.3761)	Acc@1 10.501 (10.501)	Acc@5 47.017 (47.017)
Epoch: [154][64/120]	Time 0.150 (0.144)	Data 0.000 (0.006)	Loss 2.3421 (2.3584)	Acc@1 12.411 (9.844)	Acc@5 49.165 (49.433)
Max memory in training epoch: 66.2585856
lr: 1.8322260131910246
1
Epoche:155/155; Lr: 1.8322260131910246
batch Size 423
Epoch: [155][0/120]	Time 0.241 (0.241)	Data 0.408 (0.408)	Loss 2.3184 (2.3184)	Acc@1 10.024 (10.024)	Acc@5 52.983 (52.983)
Epoch: [155][64/120]	Time 0.139 (0.145)	Data 0.000 (0.006)	Loss 2.3162 (2.3216)	Acc@1 8.353 (10.149)	Acc@5 49.403 (49.980)
Max memory in training epoch: 66.2585856
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 155208 ; 159410 ; 0.9736402986010915
[INFO] Storing checkpoint...
  10.0
Max memory: 66.2585856
 17.813s  j: 156 bis 160
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 3190
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 156
Max memory: 0.0691712
lr: 3.0274672014836073
1
Epoche:156/160; Lr: 3.0274672014836073
batch Size 423
Epoch: [156][0/119]	Time 0.271 (0.271)	Data 0.394 (0.394)	Loss 2.3056 (2.3056)	Acc@1 9.456 (9.456)	Acc@5 50.827 (50.827)
Epoch: [156][64/119]	Time 0.119 (0.138)	Data 0.000 (0.006)	Loss 2.3263 (2.3199)	Acc@1 8.511 (9.955)	Acc@5 49.645 (49.965)
Max memory in training epoch: 67.124992
lr: 3.0274672014836073
1
Epoche:157/160; Lr: 3.0274672014836073
batch Size 423
Epoch: [157][0/119]	Time 0.201 (0.201)	Data 0.500 (0.500)	Loss 2.3426 (2.3426)	Acc@1 10.875 (10.875)	Acc@5 47.518 (47.518)
Epoch: [157][64/119]	Time 0.146 (0.145)	Data 0.000 (0.008)	Loss 2.3247 (2.3250)	Acc@1 9.220 (10.235)	Acc@5 50.118 (49.478)
Max memory in training epoch: 66.770688
lr: 3.0274672014836073
1
Epoche:158/160; Lr: 3.0274672014836073
batch Size 423
Epoch: [158][0/119]	Time 0.261 (0.261)	Data 0.496 (0.496)	Loss 2.3473 (2.3473)	Acc@1 8.274 (8.274)	Acc@5 49.409 (49.409)
Epoch: [158][64/119]	Time 0.143 (0.142)	Data 0.000 (0.008)	Loss 2.3151 (2.3217)	Acc@1 12.293 (10.056)	Acc@5 52.482 (50.256)
Max memory in training epoch: 66.770688
Drin!!
old memory: 662585856
new memory: 667706880
Faktor: 1.0077288459354012
New batch Size kleiner 426!!
lr: 3.0274672014836073
1
Epoche:159/160; Lr: 3.0274672014836073
batch Size 426
Epoch: [159][0/119]	Time 0.185 (0.185)	Data 0.433 (0.433)	Loss 2.3440 (2.3440)	Acc@1 9.456 (9.456)	Acc@5 52.955 (52.955)
Epoch: [159][64/119]	Time 0.154 (0.144)	Data 0.000 (0.007)	Loss 2.3068 (2.3240)	Acc@1 10.402 (10.140)	Acc@5 53.901 (49.987)
Max memory in training epoch: 66.770688
lr: 3.0274672014836073
1
Epoche:160/160; Lr: 3.0274672014836073
batch Size 426
Epoch: [160][0/119]	Time 0.220 (0.220)	Data 0.416 (0.416)	Loss 2.3115 (2.3115)	Acc@1 9.220 (9.220)	Acc@5 53.191 (53.191)
Epoch: [160][64/119]	Time 0.157 (0.149)	Data 0.000 (0.007)	Loss 2.3201 (2.3188)	Acc@1 11.820 (10.024)	Acc@5 48.463 (50.136)
Max memory in training epoch: 66.770688
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  10.0
Max memory: 66.770688
 17.979s  j: 161 bis 165
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 3980
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 161
Max memory: 0.0691712
lr: 5.037894639968815
1
Epoche:161/165; Lr: 5.037894639968815
batch Size 426
Epoch: [161][0/118]	Time 0.235 (0.235)	Data 0.441 (0.441)	Loss 2.2905 (2.2905)	Acc@1 13.615 (13.615)	Acc@5 57.512 (57.512)
Epoch: [161][64/118]	Time 0.133 (0.137)	Data 0.000 (0.007)	Loss 2.3279 (2.3323)	Acc@1 10.094 (10.036)	Acc@5 50.000 (49.884)
Max memory in training epoch: 67.445248
lr: 5.037894639968815
1
Epoche:162/165; Lr: 5.037894639968815
batch Size 426
Epoch: [162][0/118]	Time 0.234 (0.234)	Data 0.351 (0.351)	Loss 2.3617 (2.3617)	Acc@1 10.563 (10.563)	Acc@5 49.296 (49.296)
Epoch: [162][64/118]	Time 0.157 (0.149)	Data 0.000 (0.006)	Loss 2.3352 (2.3370)	Acc@1 9.390 (9.993)	Acc@5 46.714 (50.072)
Max memory in training epoch: 67.1852032
lr: 5.037894639968815
1
Epoche:163/165; Lr: 5.037894639968815
batch Size 426
Epoch: [163][0/118]	Time 0.182 (0.182)	Data 0.482 (0.482)	Loss 2.3403 (2.3403)	Acc@1 9.859 (9.859)	Acc@5 49.296 (49.296)
Epoch: [163][64/118]	Time 0.143 (0.145)	Data 0.000 (0.008)	Loss 2.3317 (2.3255)	Acc@1 9.624 (9.884)	Acc@5 51.878 (50.592)
Max memory in training epoch: 67.1852032
Drin!!
old memory: 667706880
new memory: 671852032
Faktor: 1.0062080414687355
New batch Size kleiner 428!!
lr: 5.037894639968815
1
Epoche:164/165; Lr: 5.037894639968815
batch Size 428
Epoch: [164][0/118]	Time 0.206 (0.206)	Data 0.437 (0.437)	Loss 2.3926 (2.3926)	Acc@1 9.624 (9.624)	Acc@5 49.531 (49.531)
Epoch: [164][64/118]	Time 0.140 (0.147)	Data 0.000 (0.007)	Loss 2.3509 (2.3403)	Acc@1 9.390 (9.986)	Acc@5 49.061 (50.282)
Max memory in training epoch: 67.1852032
lr: 5.037894639968815
1
Epoche:165/165; Lr: 5.037894639968815
batch Size 428
Epoch: [165][0/118]	Time 0.196 (0.196)	Data 0.502 (0.502)	Loss 2.3507 (2.3507)	Acc@1 9.859 (9.859)	Acc@5 47.418 (47.418)
Epoch: [165][64/118]	Time 0.147 (0.149)	Data 0.000 (0.008)	Loss 2.3220 (2.3280)	Acc@1 10.329 (9.996)	Acc@5 50.704 (49.939)
Max memory in training epoch: 67.1852032
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 154630 ; 155208 ; 0.9962759651564352
[INFO] Storing checkpoint...
  10.0
Max memory: 67.1852032
 17.912s  j: 166 bis 170
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6562
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 166
Max memory: 0.068864
lr: 8.422730101197864
1
Epoche:166/170; Lr: 8.422730101197864
batch Size 428
Epoch: [166][0/117]	Time 0.230 (0.230)	Data 0.414 (0.414)	Loss 2.3561 (2.3561)	Acc@1 8.178 (8.178)	Acc@5 45.794 (45.794)
Epoch: [166][64/117]	Time 0.138 (0.142)	Data 0.000 (0.007)	Loss 2.3583 (2.3526)	Acc@1 10.047 (9.856)	Acc@5 48.364 (50.158)
Max memory in training epoch: 67.6361216
lr: 8.422730101197864
1
Epoche:167/170; Lr: 8.422730101197864
batch Size 428
Epoch: [167][0/117]	Time 0.189 (0.189)	Data 0.402 (0.402)	Loss 2.3557 (2.3557)	Acc@1 9.579 (9.579)	Acc@5 48.832 (48.832)
Epoch: [167][64/117]	Time 0.151 (0.145)	Data 0.000 (0.006)	Loss 2.4246 (2.3563)	Acc@1 7.710 (10.165)	Acc@5 48.364 (50.086)
Max memory in training epoch: 67.3455104
lr: 8.422730101197864
1
Epoche:168/170; Lr: 8.422730101197864
batch Size 428
Epoch: [168][0/117]	Time 0.217 (0.217)	Data 0.428 (0.428)	Loss 2.3277 (2.3277)	Acc@1 10.748 (10.748)	Acc@5 50.234 (50.234)
Epoch: [168][64/117]	Time 0.152 (0.145)	Data 0.000 (0.007)	Loss 2.3761 (2.3603)	Acc@1 10.748 (9.950)	Acc@5 48.598 (49.676)
Max memory in training epoch: 67.3455104
Drin!!
old memory: 671852032
new memory: 673455104
Faktor: 1.0023860491948322
New batch Size kleiner 429!!
lr: 8.422730101197864
1
Epoche:169/170; Lr: 8.422730101197864
batch Size 429
Epoch: [169][0/117]	Time 0.216 (0.216)	Data 0.426 (0.426)	Loss 2.3607 (2.3607)	Acc@1 10.748 (10.748)	Acc@5 53.271 (53.271)
Epoch: [169][64/117]	Time 0.138 (0.146)	Data 0.000 (0.007)	Loss 2.3611 (2.3561)	Acc@1 7.710 (10.014)	Acc@5 51.869 (50.428)
Max memory in training epoch: 67.3455104
lr: 8.422730101197864
1
Epoche:170/170; Lr: 8.422730101197864
batch Size 429
Epoch: [170][0/117]	Time 0.181 (0.181)	Data 0.501 (0.501)	Loss 2.3559 (2.3559)	Acc@1 10.514 (10.514)	Acc@5 51.168 (51.168)
Epoch: [170][64/117]	Time 0.154 (0.146)	Data 0.000 (0.008)	Loss 2.3295 (2.3536)	Acc@1 10.748 (10.137)	Acc@5 51.168 (49.579)
Max memory in training epoch: 67.3455104
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  10.0
Max memory: 67.3455104
 17.382s  j: 171 bis 175
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 94
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 171
Max memory: 0.068864
lr: 14.114653177397983
1
Epoche:171/175; Lr: 14.114653177397983
batch Size 429
Epoch: [171][0/117]	Time 0.294 (0.294)	Data 0.365 (0.365)	Loss 2.3251 (2.3251)	Acc@1 9.790 (9.790)	Acc@5 53.380 (53.380)
Epoch: [171][64/117]	Time 0.123 (0.147)	Data 0.000 (0.006)	Loss 2.4175 (2.4122)	Acc@1 7.692 (10.109)	Acc@5 47.086 (50.289)
Max memory in training epoch: 67.780352
lr: 14.114653177397983
1
Epoche:172/175; Lr: 14.114653177397983
batch Size 429
Epoch: [172][0/117]	Time 0.222 (0.222)	Data 0.408 (0.408)	Loss 2.8584 (2.8584)	Acc@1 8.159 (8.159)	Acc@5 50.117 (50.117)
Epoch: [172][64/117]	Time 0.144 (0.144)	Data 0.000 (0.007)	Loss 32.6661 (27.6523)	Acc@1 10.256 (10.152)	Acc@5 47.786 (50.131)
Max memory in training epoch: 67.470592
lr: 14.114653177397983
1
Epoche:173/175; Lr: 14.114653177397983
batch Size 429
Epoch: [173][0/117]	Time 0.192 (0.192)	Data 0.418 (0.418)	Loss 61.6682 (61.6682)	Acc@1 10.490 (10.490)	Acc@5 51.282 (51.282)
Epoch: [173][64/117]	Time 0.133 (0.145)	Data 0.000 (0.007)	Loss 420.3604 (7342779.1709)	Acc@1 6.993 (10.213)	Acc@5 48.951 (50.074)
Max memory in training epoch: 67.470592
Drin!!
old memory: 673455104
new memory: 674705920
Faktor: 1.0018573116345406
New batch Size kleiner 429!!
lr: 14.114653177397983
1
Epoche:174/175; Lr: 14.114653177397983
batch Size 429
Epoch: [174][0/117]	Time 0.212 (0.212)	Data 0.439 (0.439)	Loss 2522.2539 (2522.2539)	Acc@1 10.256 (10.256)	Acc@5 50.583 (50.583)
Epoch: [174][64/117]	Time 0.137 (0.151)	Data 0.000 (0.007)	Loss 4402805248.0000 (635646355.8078)	Acc@1 12.121 (10.091)	Acc@5 48.252 (49.543)
Max memory in training epoch: 67.470592
lr: 14.114653177397983
1
Epoche:175/175; Lr: 14.114653177397983
batch Size 429
Epoch: [175][0/117]	Time 0.198 (0.198)	Data 0.428 (0.428)	Loss 22939.1973 (22939.1973)	Acc@1 9.790 (9.790)	Acc@5 48.951 (48.951)
Epoch: [175][64/117]	Time 0.138 (0.146)	Data 0.000 (0.007)	Loss 270261.9688 (129052127499.2938)	Acc@1 10.723 (9.701)	Acc@5 51.748 (49.930)
Max memory in training epoch: 67.470592
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  10.0
Max memory: 67.470592
 17.621s  j: 176 bis 180
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7646
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 176
Max memory: 0.068864
lr: 23.653071144936465
1
Epoche:176/180; Lr: 23.653071144936465
batch Size 429
Epoch: [176][0/117]	Time 0.251 (0.251)	Data 0.441 (0.441)	Loss 82713.6406 (82713.6406)	Acc@1 10.956 (10.956)	Acc@5 47.786 (47.786)
Epoch: [176][64/117]	Time 0.147 (0.145)	Data 0.000 (0.007)	Loss 24758894.0000 (4485714802589811.5000)	Acc@1 7.459 (10.038)	Acc@5 48.019 (50.357)
Max memory in training epoch: 67.780352
lr: 23.653071144936465
1
Epoche:177/180; Lr: 23.653071144936465
batch Size 429
Epoch: [177][0/117]	Time 0.212 (0.212)	Data 0.456 (0.456)	Loss 4142086.5000 (4142086.5000)	Acc@1 10.956 (10.956)	Acc@5 52.214 (52.214)
Epoch: [177][64/117]	Time 0.154 (0.149)	Data 0.000 (0.007)	Loss 55525044.0000 (1114277029304440832.0000)	Acc@1 9.790 (9.916)	Acc@5 50.583 (49.937)
Max memory in training epoch: 67.470592
lr: 23.653071144936465
1
Epoche:178/180; Lr: 23.653071144936465
batch Size 429
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [178][0/117]	Time 0.301 (0.301)	Data 0.528 (0.528)	Loss 9978383360.0000 (9978383360.0000)	Acc@1 10.256 (10.256)	Acc@5 51.515 (51.515)
Epoch: [178][64/117]	Time 0.152 (0.150)	Data 0.000 (0.008)	Loss 2466960113664.0000 (118179442210992012667125760.0000)	Acc@1 9.091 (10.246)	Acc@5 49.417 (50.256)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
BSize 5
j: 0 bis 5
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4731
Files already downloaded and verified
numoFStages: 3
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (14): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (15): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (16): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (17): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (18): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (19): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (21): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (22): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (23): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (26): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (27): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (28): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (29): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (32): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (33): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (34): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (35): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (38): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (39): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (40): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (41): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (44): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (45): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (46): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (47): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (48): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (50): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (51): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (52): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (53): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (56): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (57): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (58): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (59): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (62): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (63): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (64): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (65): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (66): AdaptiveAvgPool2d(output_size=(1, 1))
    (67): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
Max memory: 0.202496
batch_size berechnet: 250;389.1 ; lr: 0.1
lr: 0.09765625
1
Epoche:1/5; Lr: 0.09765625
batch Size 250
Epoch: [1][0/200]	Time 0.268 (0.268)	Data 0.378 (0.378)	Loss 3.7645 (3.7645)	Acc@1 8.000 (8.000)	Acc@5 48.800 (48.800)
Epoch: [1][64/200]	Time 0.130 (0.149)	Data 0.000 (0.006)	Loss 2.5245 (2.9584)	Acc@1 38.000 (25.871)	Acc@5 89.200 (77.618)
Epoch: [1][128/200]	Time 0.129 (0.146)	Data 0.000 (0.003)	Loss 2.5844 (2.7538)	Acc@1 39.200 (31.715)	Acc@5 90.800 (82.983)
Epoch: [1][192/200]	Time 0.153 (0.146)	Data 0.000 (0.002)	Loss 2.3245 (2.6063)	Acc@1 43.200 (36.267)	Acc@5 90.000 (85.917)
Max memory in training epoch: 66.4657408
lr: 0.09765625
1
Epoche:2/5; Lr: 0.09765625
batch Size 250
Epoch: [2][0/200]	Time 0.201 (0.201)	Data 0.368 (0.368)	Loss 2.0921 (2.0921)	Acc@1 51.200 (51.200)	Acc@5 96.000 (96.000)
Epoch: [2][64/200]	Time 0.146 (0.152)	Data 0.000 (0.006)	Loss 1.9963 (2.0997)	Acc@1 54.800 (52.326)	Acc@5 96.400 (93.982)
Epoch: [2][128/200]	Time 0.153 (0.150)	Data 0.000 (0.003)	Loss 1.9307 (2.0257)	Acc@1 57.600 (54.623)	Acc@5 94.400 (94.611)
Epoch: [2][192/200]	Time 0.159 (0.149)	Data 0.000 (0.002)	Loss 1.7254 (1.9554)	Acc@1 63.200 (56.672)	Acc@5 96.000 (95.150)
Max memory in training epoch: 66.0135424
lr: 0.09765625
1
Epoche:3/5; Lr: 0.09765625
batch Size 250
Epoch: [3][0/200]	Time 0.190 (0.190)	Data 0.331 (0.331)	Loss 1.7618 (1.7618)	Acc@1 61.200 (61.200)	Acc@5 97.200 (97.200)
Epoch: [3][64/200]	Time 0.138 (0.148)	Data 0.000 (0.005)	Loss 1.5802 (1.6764)	Acc@1 68.000 (64.800)	Acc@5 96.800 (96.911)
Epoch: [3][128/200]	Time 0.135 (0.150)	Data 0.000 (0.003)	Loss 1.6090 (1.6354)	Acc@1 65.600 (66.071)	Acc@5 98.800 (97.036)
Epoch: [3][192/200]	Time 0.134 (0.149)	Data 0.000 (0.002)	Loss 1.5073 (1.5928)	Acc@1 68.400 (67.086)	Acc@5 98.400 (97.177)
Max memory in training epoch: 66.0135424
Drin!!
old memory: 0
new memory: 660135424
lr: 0.09765625
1
Epoche:4/5; Lr: 0.09765625
batch Size 250
Epoch: [4][0/200]	Time 0.216 (0.216)	Data 0.448 (0.448)	Loss 1.4183 (1.4183)	Acc@1 70.400 (70.400)	Acc@5 98.800 (98.800)
Epoch: [4][64/200]	Time 0.141 (0.154)	Data 0.000 (0.007)	Loss 1.4298 (1.4347)	Acc@1 68.800 (70.905)	Acc@5 98.000 (97.754)
Epoch: [4][128/200]	Time 0.137 (0.149)	Data 0.000 (0.004)	Loss 1.4051 (1.4071)	Acc@1 68.400 (71.522)	Acc@5 97.600 (97.857)
Epoch: [4][192/200]	Time 0.126 (0.149)	Data 0.000 (0.003)	Loss 1.2427 (1.3746)	Acc@1 77.200 (72.332)	Acc@5 99.600 (97.938)
Max memory in training epoch: 66.0135424
lr: 0.09765625
1
Epoche:5/5; Lr: 0.09765625
batch Size 250
Epoch: [5][0/200]	Time 0.208 (0.208)	Data 0.406 (0.406)	Loss 1.1340 (1.1340)	Acc@1 79.200 (79.200)	Acc@5 99.200 (99.200)
Epoch: [5][64/200]	Time 0.146 (0.152)	Data 0.000 (0.006)	Loss 1.3505 (1.2463)	Acc@1 72.800 (75.342)	Acc@5 97.200 (98.418)
Epoch: [5][128/200]	Time 0.135 (0.151)	Data 0.000 (0.003)	Loss 1.2217 (1.2332)	Acc@1 74.000 (75.616)	Acc@5 97.600 (98.450)
Epoch: [5][192/200]	Time 0.161 (0.152)	Data 0.000 (0.002)	Loss 1.0886 (1.2268)	Acc@1 77.600 (75.503)	Acc@5 98.400 (98.419)
Max memory in training epoch: 66.0135424
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  66.93
Max memory: 103.3835008
 30.872s  ./run_BSize.sh: 7: ./run_BSize.sh: cannot open j: 6 bis 10: No such file
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1655
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 6
Max memory: 0.202496
lr: 0.095367431640625
1
Epoche:6/10; Lr: 0.095367431640625
batch Size 250
Epoch: [6][0/200]	Time 0.233 (0.233)	Data 0.363 (0.363)	Loss 1.1248 (1.1248)	Acc@1 78.000 (78.000)	Acc@5 98.000 (98.000)
Epoch: [6][64/200]	Time 0.142 (0.153)	Data 0.000 (0.006)	Loss 1.2823 (1.1227)	Acc@1 70.800 (78.062)	Acc@5 98.000 (98.535)
Epoch: [6][128/200]	Time 0.156 (0.150)	Data 0.000 (0.003)	Loss 1.0605 (1.1112)	Acc@1 78.400 (78.183)	Acc@5 98.400 (98.605)
Epoch: [6][192/200]	Time 0.140 (0.150)	Data 0.000 (0.002)	Loss 1.0486 (1.1108)	Acc@1 80.000 (77.691)	Acc@5 99.600 (98.642)
Max memory in training epoch: 66.4656384
lr: 0.095367431640625
1
Epoche:7/10; Lr: 0.095367431640625
batch Size 250
Epoch: [7][0/200]	Time 0.181 (0.181)	Data 0.352 (0.352)	Loss 1.0273 (1.0273)	Acc@1 79.600 (79.600)	Acc@5 99.200 (99.200)
Epoch: [7][64/200]	Time 0.140 (0.144)	Data 0.000 (0.006)	Loss 1.0015 (1.0665)	Acc@1 81.600 (78.443)	Acc@5 99.600 (98.726)
Epoch: [7][128/200]	Time 0.153 (0.149)	Data 0.000 (0.003)	Loss 1.0053 (1.0489)	Acc@1 80.000 (78.878)	Acc@5 99.600 (98.794)
Epoch: [7][192/200]	Time 0.157 (0.148)	Data 0.000 (0.002)	Loss 1.1157 (1.0456)	Acc@1 75.200 (78.862)	Acc@5 98.000 (98.781)
Max memory in training epoch: 66.01344
lr: 0.095367431640625
1
Epoche:8/10; Lr: 0.095367431640625
batch Size 250
Epoch: [8][0/200]	Time 0.223 (0.223)	Data 0.332 (0.332)	Loss 0.9884 (0.9884)	Acc@1 82.800 (82.800)	Acc@5 98.000 (98.000)
Epoch: [8][64/200]	Time 0.149 (0.153)	Data 0.000 (0.005)	Loss 1.0839 (0.9974)	Acc@1 78.000 (79.686)	Acc@5 98.800 (98.923)
Epoch: [8][128/200]	Time 0.146 (0.152)	Data 0.000 (0.003)	Loss 0.9772 (0.9983)	Acc@1 78.000 (79.687)	Acc@5 98.800 (98.843)
Epoch: [8][192/200]	Time 0.143 (0.154)	Data 0.000 (0.002)	Loss 0.9569 (0.9954)	Acc@1 80.800 (79.602)	Acc@5 99.200 (98.837)
Max memory in training epoch: 66.01344
Drin!!
old memory: 660135424
new memory: 660134400
Faktor: 0.9999984488031353
New batch Size größer 253!!
lr: 0.095367431640625
1
Epoche:9/10; Lr: 0.095367431640625
batch Size 253
Epoch: [9][0/200]	Time 0.204 (0.204)	Data 0.449 (0.449)	Loss 0.8879 (0.8879)	Acc@1 81.600 (81.600)	Acc@5 99.200 (99.200)
Epoch: [9][64/200]	Time 0.150 (0.153)	Data 0.000 (0.007)	Loss 0.8792 (0.9514)	Acc@1 84.800 (80.498)	Acc@5 99.200 (98.960)
Epoch: [9][128/200]	Time 0.163 (0.154)	Data 0.000 (0.004)	Loss 0.8782 (0.9594)	Acc@1 81.600 (80.316)	Acc@5 99.200 (98.899)
Epoch: [9][192/200]	Time 0.149 (0.154)	Data 0.000 (0.002)	Loss 0.9968 (0.9592)	Acc@1 81.600 (80.269)	Acc@5 98.000 (98.881)
Max memory in training epoch: 66.01344
lr: 0.095367431640625
1
Epoche:10/10; Lr: 0.095367431640625
batch Size 253
Epoch: [10][0/200]	Time 0.211 (0.211)	Data 0.314 (0.314)	Loss 1.0403 (1.0403)	Acc@1 79.200 (79.200)	Acc@5 98.800 (98.800)
Epoch: [10][64/200]	Time 0.149 (0.150)	Data 0.000 (0.005)	Loss 0.8605 (0.9314)	Acc@1 82.000 (80.806)	Acc@5 100.000 (99.102)
Epoch: [10][128/200]	Time 0.162 (0.150)	Data 0.000 (0.003)	Loss 0.8553 (0.9352)	Acc@1 84.800 (80.791)	Acc@5 99.200 (99.011)
Epoch: [10][192/200]	Time 0.147 (0.150)	Data 0.000 (0.002)	Loss 0.9233 (0.9388)	Acc@1 82.800 (80.781)	Acc@5 98.800 (98.964)
Max memory in training epoch: 66.01344
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  70.06
Max memory: 103.3833984
 30.473s  j: 11 bis 15
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 3861
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 11
Max memory: 0.202496
lr: 0.09424984455108643
1
Epoche:11/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [11][0/198]	Time 0.219 (0.219)	Data 0.351 (0.351)	Loss 0.8780 (0.8780)	Acc@1 82.609 (82.609)	Acc@5 98.814 (98.814)
Epoch: [11][64/198]	Time 0.152 (0.155)	Data 0.000 (0.006)	Loss 0.8979 (0.9044)	Acc@1 79.051 (81.982)	Acc@5 99.605 (98.893)
Epoch: [11][128/198]	Time 0.172 (0.152)	Data 0.000 (0.003)	Loss 0.9292 (0.8977)	Acc@1 80.632 (82.177)	Acc@5 99.209 (98.974)
Epoch: [11][192/198]	Time 0.134 (0.152)	Data 0.000 (0.002)	Loss 0.8165 (0.9109)	Acc@1 86.561 (81.636)	Acc@5 99.209 (98.953)
Max memory in training epoch: 66.5037312
lr: 0.09424984455108643
1
Epoche:12/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [12][0/198]	Time 0.179 (0.179)	Data 0.416 (0.416)	Loss 0.9022 (0.9022)	Acc@1 81.423 (81.423)	Acc@5 98.814 (98.814)
Epoch: [12][64/198]	Time 0.151 (0.153)	Data 0.000 (0.007)	Loss 0.9335 (0.8996)	Acc@1 81.423 (81.727)	Acc@5 99.209 (99.045)
Epoch: [12][128/198]	Time 0.168 (0.155)	Data 0.000 (0.003)	Loss 0.9259 (0.9087)	Acc@1 80.237 (81.454)	Acc@5 98.814 (99.050)
Epoch: [12][192/198]	Time 0.131 (0.153)	Data 0.000 (0.002)	Loss 0.9152 (0.9098)	Acc@1 82.213 (81.419)	Acc@5 99.209 (99.031)
Max memory in training epoch: 66.277632
lr: 0.09424984455108643
1
Epoche:13/15; Lr: 0.09424984455108643
batch Size 253
Epoch: [13][0/198]	Time 0.204 (0.204)	Data 0.368 (0.368)	Loss 0.9238 (0.9238)	Acc@1 79.447 (79.447)	Acc@5 98.419 (98.419)
Epoch: [13][64/198]	Time 0.190 (0.155)	Data 0.000 (0.006)	Loss 0.8498 (0.9099)	Acc@1 83.004 (81.502)	Acc@5 99.209 (98.948)
Epoch: [13][128/198]	Time 0.168 (0.155)	Data 0.000 (0.003)	Loss 0.9602 (0.8972)	Acc@1 82.609 (81.812)	Acc@5 100.000 (99.013)
Epoch: [13][192/198]	Time 0.130 (0.154)	Data 0.000 (0.002)	Loss 0.8954 (0.8984)	Acc@1 81.028 (81.810)	Acc@5 100.000 (99.056)
Max memory in training epoch: 66.277632
Drin!!
old memory: 660134400
new memory: 662776320
Faktor: 1.0040020941190158
New batch Size kleiner 254!!
lr: 0.09424984455108643
1
Epoche:14/15; Lr: 0.09424984455108643
batch Size 254
Epoch: [14][0/198]	Time 0.205 (0.205)	Data 0.496 (0.496)	Loss 0.8828 (0.8828)	Acc@1 81.818 (81.818)	Acc@5 98.814 (98.814)
Epoch: [14][64/198]	Time 0.162 (0.158)	Data 0.000 (0.008)	Loss 0.8245 (0.8981)	Acc@1 83.794 (81.885)	Acc@5 98.814 (99.100)
Epoch: [14][128/198]	Time 0.157 (0.157)	Data 0.000 (0.004)	Loss 0.9279 (0.8894)	Acc@1 80.237 (82.351)	Acc@5 99.605 (99.133)
Epoch: [14][192/198]	Time 0.141 (0.155)	Data 0.000 (0.003)	Loss 0.8785 (0.8951)	Acc@1 83.004 (82.152)	Acc@5 98.814 (99.103)
Max memory in training epoch: 66.277632
lr: 0.09424984455108643
1
Epoche:15/15; Lr: 0.09424984455108643
batch Size 254
Epoch: [15][0/198]	Time 0.228 (0.228)	Data 0.369 (0.369)	Loss 0.8419 (0.8419)	Acc@1 84.585 (84.585)	Acc@5 98.419 (98.419)
Epoch: [15][64/198]	Time 0.159 (0.153)	Data 0.000 (0.006)	Loss 0.9634 (0.8777)	Acc@1 80.237 (82.560)	Acc@5 99.209 (99.118)
Epoch: [15][128/198]	Time 0.147 (0.154)	Data 0.000 (0.003)	Loss 0.8963 (0.8801)	Acc@1 80.632 (82.495)	Acc@5 99.209 (99.108)
Epoch: [15][192/198]	Time 0.130 (0.153)	Data 0.000 (0.002)	Loss 0.9720 (0.8789)	Acc@1 79.051 (82.553)	Acc@5 99.209 (99.083)
Max memory in training epoch: 66.277632
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
[INFO] Storing checkpoint...
  73.09
Max memory: 103.3833984
 30.666s  j: 16 bis 20
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7781
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 16
Max memory: 0.202496
lr: 0.09351351764053106
1
Epoche:16/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [16][0/197]	Time 0.236 (0.236)	Data 0.344 (0.344)	Loss 0.9106 (0.9106)	Acc@1 80.315 (80.315)	Acc@5 99.606 (99.606)
Epoch: [16][64/197]	Time 0.164 (0.156)	Data 0.000 (0.005)	Loss 0.8250 (0.8477)	Acc@1 85.039 (83.519)	Acc@5 99.213 (99.273)
Epoch: [16][128/197]	Time 0.151 (0.153)	Data 0.000 (0.003)	Loss 0.9326 (0.8624)	Acc@1 79.528 (82.995)	Acc@5 99.606 (99.203)
Epoch: [16][192/197]	Time 0.136 (0.152)	Data 0.000 (0.002)	Loss 0.9838 (0.8686)	Acc@1 78.740 (82.832)	Acc@5 99.213 (99.170)
Max memory in training epoch: 66.5164288
lr: 0.09351351764053106
1
Epoche:17/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [17][0/197]	Time 0.190 (0.190)	Data 0.372 (0.372)	Loss 0.9336 (0.9336)	Acc@1 79.134 (79.134)	Acc@5 98.031 (98.031)
Epoch: [17][64/197]	Time 0.165 (0.156)	Data 0.000 (0.006)	Loss 0.9432 (0.8593)	Acc@1 82.283 (83.186)	Acc@5 98.425 (99.164)
Epoch: [17][128/197]	Time 0.157 (0.154)	Data 0.000 (0.003)	Loss 0.8890 (0.8660)	Acc@1 84.646 (83.156)	Acc@5 98.425 (99.158)
Epoch: [17][192/197]	Time 0.161 (0.153)	Data 0.000 (0.002)	Loss 0.7547 (0.8698)	Acc@1 87.795 (82.957)	Acc@5 99.213 (99.096)
Max memory in training epoch: 66.365696
lr: 0.09351351764053106
1
Epoche:18/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [18][0/197]	Time 0.209 (0.209)	Data 0.447 (0.447)	Loss 0.8706 (0.8706)	Acc@1 84.252 (84.252)	Acc@5 99.213 (99.213)
Epoch: [18][64/197]	Time 0.167 (0.154)	Data 0.000 (0.007)	Loss 0.9488 (0.8699)	Acc@1 77.953 (83.289)	Acc@5 98.031 (99.013)
Epoch: [18][128/197]	Time 0.130 (0.153)	Data 0.000 (0.004)	Loss 1.0250 (0.8765)	Acc@1 79.134 (82.937)	Acc@5 98.425 (99.091)
Epoch: [18][192/197]	Time 0.145 (0.151)	Data 0.000 (0.002)	Loss 0.9762 (0.8789)	Acc@1 80.315 (82.832)	Acc@5 98.425 (99.102)
Max memory in training epoch: 66.365696
Drin!!
old memory: 662776320
new memory: 663656960
Faktor: 1.0013287137355782
New batch Size kleiner 254!!
lr: 0.09351351764053106
1
Epoche:19/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [19][0/197]	Time 0.241 (0.241)	Data 0.363 (0.363)	Loss 0.8096 (0.8096)	Acc@1 84.252 (84.252)	Acc@5 99.213 (99.213)
Epoch: [19][64/197]	Time 0.149 (0.154)	Data 0.000 (0.006)	Loss 0.8361 (0.8682)	Acc@1 84.646 (82.816)	Acc@5 99.213 (99.273)
Epoch: [19][128/197]	Time 0.157 (0.151)	Data 0.000 (0.003)	Loss 0.8403 (0.8655)	Acc@1 83.858 (82.961)	Acc@5 99.606 (99.258)
Epoch: [19][192/197]	Time 0.163 (0.153)	Data 0.000 (0.002)	Loss 0.8662 (0.8658)	Acc@1 84.252 (83.026)	Acc@5 98.031 (99.178)
Max memory in training epoch: 66.365696
lr: 0.09351351764053106
1
Epoche:20/20; Lr: 0.09351351764053106
batch Size 254
Epoch: [20][0/197]	Time 0.210 (0.210)	Data 0.331 (0.331)	Loss 0.7072 (0.7072)	Acc@1 87.402 (87.402)	Acc@5 100.000 (100.000)
Epoch: [20][64/197]	Time 0.193 (0.155)	Data 0.000 (0.005)	Loss 0.7575 (0.8543)	Acc@1 87.795 (83.253)	Acc@5 100.000 (99.213)
Epoch: [20][128/197]	Time 0.163 (0.155)	Data 0.000 (0.003)	Loss 0.8243 (0.8541)	Acc@1 85.827 (83.309)	Acc@5 98.425 (99.237)
Epoch: [20][192/197]	Time 0.164 (0.154)	Data 0.000 (0.002)	Loss 0.8809 (0.8597)	Acc@1 82.677 (83.059)	Acc@5 99.213 (99.239)
Max memory in training epoch: 66.365696
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 482768 ; 487386 ; 0.9905249637864034
[INFO] Storing checkpoint...
  75.45
Max memory: 103.3833984
 30.821s  j: 21 bis 25
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4746
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 21
Max memory: 0.2007552
lr: 0.09278294328396441
1
Epoche:21/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [21][0/197]	Time 0.256 (0.256)	Data 0.353 (0.353)	Loss 0.7686 (0.7686)	Acc@1 83.858 (83.858)	Acc@5 100.000 (100.000)
Epoch: [21][64/197]	Time 0.142 (0.145)	Data 0.000 (0.006)	Loss 0.8322 (0.8119)	Acc@1 82.677 (84.985)	Acc@5 99.606 (99.316)
Epoch: [21][128/197]	Time 0.152 (0.147)	Data 0.000 (0.003)	Loss 0.9260 (0.8397)	Acc@1 82.283 (83.916)	Acc@5 98.425 (99.237)
Epoch: [21][192/197]	Time 0.143 (0.148)	Data 0.000 (0.002)	Loss 1.0147 (0.8475)	Acc@1 79.134 (83.558)	Acc@5 97.244 (99.229)
Max memory in training epoch: 66.5094656
lr: 0.09278294328396441
1
Epoche:22/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [22][0/197]	Time 0.221 (0.221)	Data 0.390 (0.390)	Loss 0.8595 (0.8595)	Acc@1 83.858 (83.858)	Acc@5 98.819 (98.819)
Epoch: [22][64/197]	Time 0.157 (0.152)	Data 0.000 (0.006)	Loss 0.9149 (0.8526)	Acc@1 81.496 (83.295)	Acc@5 99.606 (99.219)
Epoch: [22][128/197]	Time 0.160 (0.152)	Data 0.000 (0.003)	Loss 0.8597 (0.8559)	Acc@1 81.890 (83.318)	Acc@5 98.819 (99.179)
Epoch: [22][192/197]	Time 0.149 (0.152)	Data 0.000 (0.002)	Loss 0.8046 (0.8563)	Acc@1 84.646 (83.324)	Acc@5 98.819 (99.209)
Max memory in training epoch: 66.280704
lr: 0.09278294328396441
1
Epoche:23/25; Lr: 0.09278294328396441
batch Size 254
Epoch: [23][0/197]	Time 0.185 (0.185)	Data 0.393 (0.393)	Loss 0.7603 (0.7603)	Acc@1 86.614 (86.614)	Acc@5 99.213 (99.213)
Epoch: [23][64/197]	Time 0.145 (0.154)	Data 0.000 (0.006)	Loss 0.8093 (0.8384)	Acc@1 82.677 (84.343)	Acc@5 99.606 (99.297)
Epoch: [23][128/197]	Time 0.155 (0.152)	Data 0.000 (0.003)	Loss 0.8132 (0.8397)	Acc@1 85.039 (84.087)	Acc@5 99.213 (99.240)
Epoch: [23][192/197]	Time 0.159 (0.152)	Data 0.000 (0.002)	Loss 0.7182 (0.8424)	Acc@1 88.976 (83.889)	Acc@5 99.213 (99.223)
Max memory in training epoch: 66.280704
Drin!!
old memory: 663656960
new memory: 662807040
Faktor: 0.9987193383762599
New batch Size größer 256!!
lr: 0.09278294328396441
1
Epoche:24/25; Lr: 0.09278294328396441
batch Size 256
Epoch: [24][0/197]	Time 0.198 (0.198)	Data 0.396 (0.396)	Loss 0.8473 (0.8473)	Acc@1 83.465 (83.465)	Acc@5 99.606 (99.606)
Epoch: [24][64/197]	Time 0.149 (0.157)	Data 0.000 (0.006)	Loss 0.8727 (0.8350)	Acc@1 81.102 (84.409)	Acc@5 99.606 (99.267)
Epoch: [24][128/197]	Time 0.151 (0.152)	Data 0.000 (0.003)	Loss 0.7936 (0.8361)	Acc@1 86.614 (84.234)	Acc@5 98.819 (99.237)
Epoch: [24][192/197]	Time 0.146 (0.152)	Data 0.000 (0.002)	Loss 0.8806 (0.8396)	Acc@1 82.677 (84.089)	Acc@5 97.244 (99.194)
Max memory in training epoch: 66.280704
lr: 0.09278294328396441
1
Epoche:25/25; Lr: 0.09278294328396441
batch Size 256
Epoch: [25][0/197]	Time 0.214 (0.214)	Data 0.378 (0.378)	Loss 0.7712 (0.7712)	Acc@1 86.220 (86.220)	Acc@5 99.213 (99.213)
Epoch: [25][64/197]	Time 0.164 (0.155)	Data 0.000 (0.006)	Loss 0.8536 (0.8422)	Acc@1 84.646 (84.197)	Acc@5 99.606 (99.273)
Epoch: [25][128/197]	Time 0.129 (0.154)	Data 0.000 (0.003)	Loss 0.7666 (0.8374)	Acc@1 87.795 (84.228)	Acc@5 99.606 (99.310)
Epoch: [25][192/197]	Time 0.151 (0.152)	Data 0.000 (0.002)	Loss 0.8461 (0.8368)	Acc@1 83.465 (84.117)	Acc@5 98.819 (99.288)
Max memory in training epoch: 66.280704
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 450592 ; 482768 ; 0.933351009180393
[INFO] Storing checkpoint...
  70.81
Max memory: 103.378176
 30.523s  j: 26 bis 30
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4663
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 26
Max memory: 0.1880064
lr: 0.09278294328396441
1
Epoche:26/30; Lr: 0.09278294328396441
batch Size 256
Epoch: [26][0/196]	Time 0.236 (0.236)	Data 0.394 (0.394)	Loss 0.8894 (0.8894)	Acc@1 82.422 (82.422)	Acc@5 99.219 (99.219)
Epoch: [26][64/196]	Time 0.162 (0.150)	Data 0.000 (0.006)	Loss 0.7523 (0.8168)	Acc@1 89.062 (84.633)	Acc@5 98.828 (99.195)
Epoch: [26][128/196]	Time 0.168 (0.147)	Data 0.000 (0.003)	Loss 0.7947 (0.8312)	Acc@1 84.766 (84.109)	Acc@5 99.609 (99.225)
Epoch: [26][192/196]	Time 0.157 (0.148)	Data 0.000 (0.002)	Loss 0.8344 (0.8352)	Acc@1 84.766 (83.997)	Acc@5 100.000 (99.247)
Max memory in training epoch: 65.9661312
lr: 0.09278294328396441
1
Epoche:27/30; Lr: 0.09278294328396441
batch Size 256
Epoch: [27][0/196]	Time 0.212 (0.212)	Data 0.384 (0.384)	Loss 0.8249 (0.8249)	Acc@1 84.766 (84.766)	Acc@5 98.828 (98.828)
Epoch: [27][64/196]	Time 0.139 (0.143)	Data 0.000 (0.006)	Loss 0.8927 (0.8132)	Acc@1 82.031 (84.886)	Acc@5 99.219 (99.237)
Epoch: [27][128/196]	Time 0.140 (0.148)	Data 0.000 (0.003)	Loss 0.8094 (0.8280)	Acc@1 85.156 (84.399)	Acc@5 98.438 (99.207)
Epoch: [27][192/196]	Time 0.131 (0.148)	Data 0.000 (0.002)	Loss 0.7653 (0.8340)	Acc@1 88.672 (84.150)	Acc@5 99.609 (99.199)
Max memory in training epoch: 65.8022912
lr: 0.09278294328396441
1
Epoche:28/30; Lr: 0.09278294328396441
batch Size 256
Epoch: [28][0/196]	Time 0.207 (0.207)	Data 0.388 (0.388)	Loss 0.8478 (0.8478)	Acc@1 82.812 (82.812)	Acc@5 98.438 (98.438)
Epoch: [28][64/196]	Time 0.197 (0.155)	Data 0.000 (0.006)	Loss 0.8636 (0.8211)	Acc@1 83.203 (84.549)	Acc@5 98.828 (99.201)
Epoch: [28][128/196]	Time 0.124 (0.153)	Data 0.000 (0.003)	Loss 0.9340 (0.8203)	Acc@1 80.859 (84.460)	Acc@5 99.609 (99.264)
Epoch: [28][192/196]	Time 0.147 (0.152)	Data 0.000 (0.002)	Loss 0.8683 (0.8260)	Acc@1 81.250 (84.280)	Acc@5 99.609 (99.241)
Max memory in training epoch: 65.8022912
Drin!!
old memory: 662807040
new memory: 658022912
Faktor: 0.992782019937507
New batch Size größer 260!!
lr: 0.09278294328396441
1
Epoche:29/30; Lr: 0.09278294328396441
batch Size 260
Epoch: [29][0/196]	Time 0.247 (0.247)	Data 0.412 (0.412)	Loss 0.7264 (0.7264)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [29][64/196]	Time 0.153 (0.152)	Data 0.000 (0.007)	Loss 0.7869 (0.8056)	Acc@1 85.547 (84.730)	Acc@5 100.000 (99.363)
Epoch: [29][128/196]	Time 0.158 (0.150)	Data 0.000 (0.003)	Loss 0.9876 (0.8143)	Acc@1 80.859 (84.505)	Acc@5 98.438 (99.352)
Epoch: [29][192/196]	Time 0.140 (0.152)	Data 0.000 (0.002)	Loss 0.8748 (0.8260)	Acc@1 83.203 (84.189)	Acc@5 99.219 (99.277)
Max memory in training epoch: 65.8022912
lr: 0.09278294328396441
1
Epoche:30/30; Lr: 0.09278294328396441
batch Size 260
Epoch: [30][0/196]	Time 0.192 (0.192)	Data 0.412 (0.412)	Loss 0.8399 (0.8399)	Acc@1 84.375 (84.375)	Acc@5 98.047 (98.047)
Epoch: [30][64/196]	Time 0.148 (0.144)	Data 0.000 (0.007)	Loss 0.9025 (0.8258)	Acc@1 82.812 (84.321)	Acc@5 99.219 (99.129)
Epoch: [30][128/196]	Time 0.147 (0.147)	Data 0.000 (0.003)	Loss 0.7968 (0.8234)	Acc@1 84.766 (84.248)	Acc@5 99.609 (99.267)
Epoch: [30][192/196]	Time 0.163 (0.148)	Data 0.000 (0.002)	Loss 0.8032 (0.8256)	Acc@1 82.812 (84.086)	Acc@5 99.609 (99.265)
Max memory in training epoch: 65.8022912
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 415084 ; 450592 ; 0.9211970030537604
[INFO] Storing checkpoint...
  77.98
Max memory: 102.5268736
 29.441s  j: 31 bis 35
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9447
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 31
Max memory: 0.1738752
lr: 0.09423267677277636
1
Epoche:31/35; Lr: 0.09423267677277636
batch Size 260
Epoch: [31][0/193]	Time 0.226 (0.226)	Data 0.443 (0.443)	Loss 0.8098 (0.8098)	Acc@1 85.385 (85.385)	Acc@5 98.846 (98.846)
Epoch: [31][64/193]	Time 0.166 (0.148)	Data 0.000 (0.007)	Loss 0.8097 (0.7786)	Acc@1 86.154 (85.923)	Acc@5 99.615 (99.331)
Epoch: [31][128/193]	Time 0.144 (0.149)	Data 0.000 (0.004)	Loss 0.8047 (0.8015)	Acc@1 85.000 (85.134)	Acc@5 99.231 (99.273)
Epoch: [31][192/193]	Time 0.100 (0.149)	Data 0.000 (0.002)	Loss 0.9704 (0.8099)	Acc@1 82.500 (84.708)	Acc@5 98.750 (99.292)
Max memory in training epoch: 64.9696768
lr: 0.09423267677277636
1
Epoche:32/35; Lr: 0.09423267677277636
batch Size 260
Epoch: [32][0/193]	Time 0.245 (0.245)	Data 0.419 (0.419)	Loss 0.7702 (0.7702)	Acc@1 87.308 (87.308)	Acc@5 99.231 (99.231)
Epoch: [32][64/193]	Time 0.161 (0.156)	Data 0.000 (0.007)	Loss 0.7222 (0.8122)	Acc@1 89.615 (84.746)	Acc@5 99.231 (99.320)
Epoch: [32][128/193]	Time 0.157 (0.155)	Data 0.000 (0.003)	Loss 0.7273 (0.8176)	Acc@1 88.462 (84.562)	Acc@5 99.615 (99.281)
Epoch: [32][192/193]	Time 0.102 (0.153)	Data 0.000 (0.002)	Loss 0.9563 (0.8219)	Acc@1 80.000 (84.286)	Acc@5 100.000 (99.266)
Max memory in training epoch: 65.1064832
lr: 0.09423267677277636
1
Epoche:33/35; Lr: 0.09423267677277636
batch Size 260
Epoch: [33][0/193]	Time 0.183 (0.183)	Data 0.290 (0.290)	Loss 0.7293 (0.7293)	Acc@1 89.231 (89.231)	Acc@5 100.000 (100.000)
Epoch: [33][64/193]	Time 0.151 (0.152)	Data 0.000 (0.005)	Loss 0.7639 (0.8054)	Acc@1 86.923 (84.598)	Acc@5 99.615 (99.391)
Epoch: [33][128/193]	Time 0.148 (0.151)	Data 0.000 (0.002)	Loss 0.7876 (0.8084)	Acc@1 85.769 (84.687)	Acc@5 99.615 (99.308)
Epoch: [33][192/193]	Time 0.103 (0.152)	Data 0.000 (0.002)	Loss 1.0938 (0.8179)	Acc@1 73.750 (84.338)	Acc@5 98.750 (99.272)
Max memory in training epoch: 65.1064832
Drin!!
old memory: 658022912
new memory: 651064832
Faktor: 0.9894257785358088
New batch Size größer 267!!
lr: 0.09423267677277636
1
Epoche:34/35; Lr: 0.09423267677277636
batch Size 267
Epoch: [34][0/193]	Time 0.214 (0.214)	Data 0.368 (0.368)	Loss 0.7802 (0.7802)	Acc@1 87.692 (87.692)	Acc@5 100.000 (100.000)
Epoch: [34][64/193]	Time 0.133 (0.153)	Data 0.000 (0.006)	Loss 0.8524 (0.8047)	Acc@1 86.154 (85.030)	Acc@5 99.615 (99.385)
Epoch: [34][128/193]	Time 0.176 (0.153)	Data 0.000 (0.003)	Loss 0.7773 (0.8086)	Acc@1 84.231 (84.717)	Acc@5 99.615 (99.335)
Epoch: [34][192/193]	Time 0.106 (0.152)	Data 0.000 (0.002)	Loss 0.9499 (0.8183)	Acc@1 81.250 (84.334)	Acc@5 95.000 (99.282)
Max memory in training epoch: 65.1064832
lr: 0.09423267677277636
1
Epoche:35/35; Lr: 0.09423267677277636
batch Size 267
Epoch: [35][0/193]	Time 0.204 (0.204)	Data 0.431 (0.431)	Loss 0.8853 (0.8853)	Acc@1 82.308 (82.308)	Acc@5 99.615 (99.615)
Epoch: [35][64/193]	Time 0.153 (0.155)	Data 0.000 (0.007)	Loss 0.8492 (0.8001)	Acc@1 82.692 (85.148)	Acc@5 98.462 (99.379)
Epoch: [35][128/193]	Time 0.134 (0.152)	Data 0.000 (0.004)	Loss 0.7991 (0.8057)	Acc@1 85.769 (84.744)	Acc@5 100.000 (99.317)
Epoch: [35][192/193]	Time 0.100 (0.152)	Data 0.000 (0.002)	Loss 0.7696 (0.8096)	Acc@1 82.500 (84.628)	Acc@5 98.750 (99.252)
Max memory in training epoch: 65.1064832
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 384922 ; 415084 ; 0.9273351899856415
[INFO] Storing checkpoint...
  68.44
Max memory: 98.5814016
 29.956s  j: 36 bis 40
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6093
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 36
Max memory: 0.1619456
lr: 0.09828173710285659
1
Epoche:36/40; Lr: 0.09828173710285659
batch Size 267
Epoch: [36][0/188]	Time 0.229 (0.229)	Data 0.346 (0.346)	Loss 0.8017 (0.8017)	Acc@1 84.644 (84.644)	Acc@5 100.000 (100.000)
Epoch: [36][64/188]	Time 0.142 (0.152)	Data 0.000 (0.006)	Loss 0.8296 (0.7792)	Acc@1 85.393 (85.537)	Acc@5 99.251 (99.447)
Epoch: [36][128/188]	Time 0.165 (0.150)	Data 0.000 (0.003)	Loss 0.7559 (0.7894)	Acc@1 86.517 (85.228)	Acc@5 99.251 (99.390)
Max memory in training epoch: 65.3026816
lr: 0.09828173710285659
1
Epoche:37/40; Lr: 0.09828173710285659
batch Size 267
Epoch: [37][0/188]	Time 0.205 (0.205)	Data 0.309 (0.309)	Loss 0.7924 (0.7924)	Acc@1 84.644 (84.644)	Acc@5 99.625 (99.625)
Epoch: [37][64/188]	Time 0.163 (0.151)	Data 0.000 (0.005)	Loss 0.7828 (0.8174)	Acc@1 87.640 (84.356)	Acc@5 98.127 (99.337)
Epoch: [37][128/188]	Time 0.147 (0.150)	Data 0.000 (0.003)	Loss 0.7639 (0.8094)	Acc@1 85.768 (84.702)	Acc@5 99.625 (99.315)
Max memory in training epoch: 65.202688
lr: 0.09828173710285659
1
Epoche:38/40; Lr: 0.09828173710285659
batch Size 267
Epoch: [38][0/188]	Time 0.226 (0.226)	Data 0.388 (0.388)	Loss 0.8132 (0.8132)	Acc@1 83.146 (83.146)	Acc@5 99.251 (99.251)
Epoch: [38][64/188]	Time 0.154 (0.153)	Data 0.000 (0.006)	Loss 0.8743 (0.8111)	Acc@1 80.150 (84.552)	Acc@5 99.251 (99.326)
Epoch: [38][128/188]	Time 0.151 (0.152)	Data 0.000 (0.003)	Loss 0.8284 (0.8107)	Acc@1 86.517 (84.485)	Acc@5 97.753 (99.289)
Max memory in training epoch: 65.202688
Drin!!
old memory: 651064832
new memory: 652026880
Faktor: 1.0014776531502165
New batch Size kleiner 267!!
lr: 0.09828173710285659
1
Epoche:39/40; Lr: 0.09828173710285659
batch Size 267
Epoch: [39][0/188]	Time 0.165 (0.165)	Data 0.258 (0.258)	Loss 0.7780 (0.7780)	Acc@1 83.146 (83.146)	Acc@5 100.000 (100.000)
Epoch: [39][64/188]	Time 0.146 (0.147)	Data 0.000 (0.004)	Loss 0.8415 (0.8042)	Acc@1 81.648 (84.708)	Acc@5 100.000 (99.326)
Epoch: [39][128/188]	Time 0.133 (0.148)	Data 0.000 (0.002)	Loss 0.7582 (0.8065)	Acc@1 87.640 (84.604)	Acc@5 98.876 (99.300)
Max memory in training epoch: 65.202688
lr: 0.09828173710285659
1
Epoche:40/40; Lr: 0.09828173710285659
batch Size 267
Epoch: [40][0/188]	Time 0.217 (0.217)	Data 0.367 (0.367)	Loss 0.7802 (0.7802)	Acc@1 87.266 (87.266)	Acc@5 99.625 (99.625)
Epoch: [40][64/188]	Time 0.147 (0.153)	Data 0.000 (0.006)	Loss 0.8031 (0.8016)	Acc@1 85.393 (84.742)	Acc@5 99.251 (99.274)
Epoch: [40][128/188]	Time 0.169 (0.154)	Data 0.000 (0.003)	Loss 0.8258 (0.8003)	Acc@1 82.772 (84.575)	Acc@5 98.876 (99.292)
Max memory in training epoch: 65.202688
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 358064 ; 384922 ; 0.9302248247696936
[INFO] Storing checkpoint...
  79.69
Max memory: 96.7179776
 29.275s  j: 41 bis 45
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4506
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 41
Max memory: 0.151296
lr: 0.10250478049399496
1
Epoche:41/45; Lr: 0.10250478049399496
batch Size 267
Epoch: [41][0/188]	Time 0.285 (0.285)	Data 0.421 (0.421)	Loss 0.8436 (0.8436)	Acc@1 81.273 (81.273)	Acc@5 100.000 (100.000)
Epoch: [41][64/188]	Time 0.149 (0.151)	Data 0.000 (0.007)	Loss 0.8476 (0.7807)	Acc@1 85.019 (85.290)	Acc@5 98.127 (99.389)
Epoch: [41][128/188]	Time 0.145 (0.149)	Data 0.000 (0.003)	Loss 0.7929 (0.7923)	Acc@1 86.142 (85.013)	Acc@5 99.625 (99.335)
Max memory in training epoch: 63.0482432
lr: 0.10250478049399496
1
Epoche:42/45; Lr: 0.10250478049399496
batch Size 267
Epoch: [42][0/188]	Time 0.205 (0.205)	Data 0.420 (0.420)	Loss 0.6742 (0.6742)	Acc@1 88.764 (88.764)	Acc@5 100.000 (100.000)
Epoch: [42][64/188]	Time 0.162 (0.152)	Data 0.000 (0.007)	Loss 0.9768 (0.8110)	Acc@1 77.903 (84.414)	Acc@5 98.876 (99.314)
Epoch: [42][128/188]	Time 0.166 (0.149)	Data 0.000 (0.003)	Loss 0.8088 (0.8084)	Acc@1 83.146 (84.575)	Acc@5 99.625 (99.274)
Max memory in training epoch: 63.0982656
lr: 0.10250478049399496
1
Epoche:43/45; Lr: 0.10250478049399496
batch Size 267
Epoch: [43][0/188]	Time 0.192 (0.192)	Data 0.464 (0.464)	Loss 0.7703 (0.7703)	Acc@1 86.142 (86.142)	Acc@5 98.876 (98.876)
Epoch: [43][64/188]	Time 0.148 (0.151)	Data 0.000 (0.007)	Loss 0.8252 (0.7859)	Acc@1 86.142 (85.399)	Acc@5 99.251 (99.286)
Epoch: [43][128/188]	Time 0.146 (0.151)	Data 0.000 (0.004)	Loss 0.7560 (0.7931)	Acc@1 86.891 (84.978)	Acc@5 99.625 (99.358)
Max memory in training epoch: 63.0982656
Drin!!
old memory: 652026880
new memory: 630982656
Faktor: 0.967724913426882
New batch Size größer 283!!
lr: 0.10250478049399496
1
Epoche:44/45; Lr: 0.10250478049399496
batch Size 283
Epoch: [44][0/188]	Time 0.183 (0.183)	Data 0.423 (0.423)	Loss 0.8311 (0.8311)	Acc@1 82.772 (82.772)	Acc@5 98.876 (98.876)
Epoch: [44][64/188]	Time 0.154 (0.153)	Data 0.000 (0.007)	Loss 0.7647 (0.8172)	Acc@1 86.142 (83.964)	Acc@5 98.876 (99.262)
Epoch: [44][128/188]	Time 0.148 (0.154)	Data 0.000 (0.003)	Loss 0.9205 (0.8078)	Acc@1 79.775 (84.403)	Acc@5 99.251 (99.312)
Max memory in training epoch: 63.0982656
lr: 0.10250478049399496
1
Epoche:45/45; Lr: 0.10250478049399496
batch Size 283
Epoch: [45][0/188]	Time 0.203 (0.203)	Data 0.406 (0.406)	Loss 0.8153 (0.8153)	Acc@1 80.524 (80.524)	Acc@5 99.251 (99.251)
Epoch: [45][64/188]	Time 0.158 (0.145)	Data 0.000 (0.006)	Loss 0.8026 (0.8171)	Acc@1 85.019 (84.247)	Acc@5 98.502 (99.314)
Epoch: [45][128/188]	Time 0.141 (0.150)	Data 0.000 (0.003)	Loss 0.7957 (0.8061)	Acc@1 83.521 (84.479)	Acc@5 99.625 (99.324)
Max memory in training epoch: 63.0982656
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 343912 ; 358064 ; 0.9604763394253542
[INFO] Storing checkpoint...
  54.19
Max memory: 93.8778112
 28.634s  j: 46 bis 50
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4652
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 46
Max memory: 0.145664
lr: 0.113315831561721
1
Epoche:46/50; Lr: 0.113315831561721
batch Size 283
Epoch: [46][0/177]	Time 0.225 (0.225)	Data 0.393 (0.393)	Loss 0.7674 (0.7674)	Acc@1 86.219 (86.219)	Acc@5 99.647 (99.647)
Epoch: [46][64/177]	Time 0.150 (0.155)	Data 0.000 (0.006)	Loss 0.7618 (0.7660)	Acc@1 88.339 (85.719)	Acc@5 99.293 (99.424)
Epoch: [46][128/177]	Time 0.167 (0.154)	Data 0.000 (0.003)	Loss 0.7657 (0.7869)	Acc@1 84.806 (85.060)	Acc@5 99.293 (99.329)
Max memory in training epoch: 65.6157184
lr: 0.113315831561721
1
Epoche:47/50; Lr: 0.113315831561721
batch Size 283
Epoch: [47][0/177]	Time 0.216 (0.216)	Data 0.369 (0.369)	Loss 0.8328 (0.8328)	Acc@1 85.159 (85.159)	Acc@5 99.647 (99.647)
Epoch: [47][64/177]	Time 0.154 (0.153)	Data 0.000 (0.006)	Loss 0.7494 (0.8039)	Acc@1 83.746 (84.724)	Acc@5 100.000 (99.304)
Epoch: [47][128/177]	Time 0.140 (0.152)	Data 0.000 (0.003)	Loss 0.8974 (0.8048)	Acc@1 80.919 (84.562)	Acc@5 99.647 (99.302)
Max memory in training epoch: 65.9272192
lr: 0.113315831561721
1
Epoche:48/50; Lr: 0.113315831561721
batch Size 283
Epoch: [48][0/177]	Time 0.199 (0.199)	Data 0.357 (0.357)	Loss 0.8686 (0.8686)	Acc@1 81.625 (81.625)	Acc@5 99.647 (99.647)
Epoch: [48][64/177]	Time 0.166 (0.151)	Data 0.000 (0.006)	Loss 0.7061 (0.7868)	Acc@1 88.339 (85.170)	Acc@5 100.000 (99.255)
Epoch: [48][128/177]	Time 0.147 (0.152)	Data 0.000 (0.003)	Loss 0.8463 (0.7934)	Acc@1 84.806 (84.844)	Acc@5 98.233 (99.277)
Max memory in training epoch: 65.9272192
Drin!!
old memory: 630982656
new memory: 659272192
Faktor: 1.0448340944572652
New batch Size kleiner 295!!
lr: 0.113315831561721
1
Epoche:49/50; Lr: 0.113315831561721
batch Size 295
Epoch: [49][0/177]	Time 0.200 (0.200)	Data 0.333 (0.333)	Loss 0.7586 (0.7586)	Acc@1 87.279 (87.279)	Acc@5 99.293 (99.293)
Epoch: [49][64/177]	Time 0.143 (0.154)	Data 0.000 (0.005)	Loss 0.7469 (0.7906)	Acc@1 86.219 (84.740)	Acc@5 99.293 (99.320)
Epoch: [49][128/177]	Time 0.165 (0.154)	Data 0.000 (0.003)	Loss 0.9248 (0.7904)	Acc@1 81.272 (84.808)	Acc@5 98.587 (99.293)
Max memory in training epoch: 65.9272192
lr: 0.113315831561721
1
Epoche:50/50; Lr: 0.113315831561721
batch Size 295
Epoch: [50][0/177]	Time 0.231 (0.231)	Data 0.331 (0.331)	Loss 0.7934 (0.7934)	Acc@1 86.219 (86.219)	Acc@5 99.293 (99.293)
Epoch: [50][64/177]	Time 0.163 (0.157)	Data 0.000 (0.005)	Loss 0.8428 (0.7943)	Acc@1 82.332 (85.034)	Acc@5 99.647 (99.315)
Epoch: [50][128/177]	Time 0.152 (0.157)	Data 0.000 (0.003)	Loss 0.8049 (0.7960)	Acc@1 83.039 (84.869)	Acc@5 100.000 (99.310)
Max memory in training epoch: 65.9272192
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 331066 ; 343912 ; 0.9626474214333899
[INFO] Storing checkpoint...
  78.98
Max memory: 91.8047232
 28.142s  j: 51 bis 55
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8117
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 51
Max memory: 0.1405952
lr: 0.13057879027620192
1
Epoche:51/55; Lr: 0.13057879027620192
batch Size 295
Epoch: [51][0/170]	Time 0.197 (0.197)	Data 0.372 (0.372)	Loss 0.7823 (0.7823)	Acc@1 84.746 (84.746)	Acc@5 99.661 (99.661)
Epoch: [51][64/170]	Time 0.152 (0.150)	Data 0.000 (0.006)	Loss 0.7885 (0.7937)	Acc@1 84.068 (84.699)	Acc@5 99.661 (99.249)
Epoch: [51][128/170]	Time 0.158 (0.151)	Data 0.000 (0.003)	Loss 0.9394 (0.8100)	Acc@1 78.983 (84.223)	Acc@5 98.644 (99.199)
Max memory in training epoch: 66.824704
lr: 0.13057879027620192
1
Epoche:52/55; Lr: 0.13057879027620192
batch Size 295
Epoch: [52][0/170]	Time 0.195 (0.195)	Data 0.452 (0.452)	Loss 0.8814 (0.8814)	Acc@1 82.712 (82.712)	Acc@5 98.644 (98.644)
Epoch: [52][64/170]	Time 0.143 (0.155)	Data 0.000 (0.007)	Loss 0.8751 (0.8018)	Acc@1 83.390 (84.803)	Acc@5 99.661 (99.364)
Epoch: [52][128/170]	Time 0.146 (0.155)	Data 0.000 (0.004)	Loss 0.7918 (0.8118)	Acc@1 85.424 (84.430)	Acc@5 100.000 (99.317)
Max memory in training epoch: 66.7840512
lr: 0.13057879027620192
1
Epoche:53/55; Lr: 0.13057879027620192
batch Size 295
Epoch: [53][0/170]	Time 0.205 (0.205)	Data 0.450 (0.450)	Loss 0.7792 (0.7792)	Acc@1 85.763 (85.763)	Acc@5 98.644 (98.644)
Epoch: [53][64/170]	Time 0.159 (0.161)	Data 0.000 (0.007)	Loss 0.8913 (0.8053)	Acc@1 82.034 (84.360)	Acc@5 98.983 (99.213)
Epoch: [53][128/170]	Time 0.150 (0.159)	Data 0.000 (0.004)	Loss 0.6971 (0.8096)	Acc@1 89.831 (84.249)	Acc@5 100.000 (99.212)
Max memory in training epoch: 66.7840512
Drin!!
old memory: 659272192
new memory: 667840512
Faktor: 1.0129966349316308
New batch Size kleiner 298!!
lr: 0.13057879027620192
1
Epoche:54/55; Lr: 0.13057879027620192
batch Size 298
Epoch: [54][0/170]	Time 0.236 (0.236)	Data 0.540 (0.540)	Loss 0.8087 (0.8087)	Acc@1 84.407 (84.407)	Acc@5 99.661 (99.661)
Epoch: [54][64/170]	Time 0.175 (0.158)	Data 0.000 (0.009)	Loss 0.7683 (0.7977)	Acc@1 84.746 (84.782)	Acc@5 99.322 (99.291)
Epoch: [54][128/170]	Time 0.144 (0.153)	Data 0.000 (0.004)	Loss 0.8070 (0.8095)	Acc@1 86.441 (84.470)	Acc@5 98.983 (99.256)
Max memory in training epoch: 66.7840512
lr: 0.13057879027620192
1
Epoche:55/55; Lr: 0.13057879027620192
batch Size 298
Epoch: [55][0/170]	Time 0.215 (0.215)	Data 0.364 (0.364)	Loss 0.8506 (0.8506)	Acc@1 85.763 (85.763)	Acc@5 98.983 (98.983)
Epoch: [55][64/170]	Time 0.169 (0.156)	Data 0.000 (0.006)	Loss 0.8347 (0.7932)	Acc@1 84.407 (84.725)	Acc@5 99.322 (99.369)
Epoch: [55][128/170]	Time 0.206 (0.156)	Data 0.000 (0.003)	Loss 0.9313 (0.8105)	Acc@1 78.983 (84.304)	Acc@5 98.983 (99.346)
Max memory in training epoch: 66.7840512
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv21.weight

 RM:  module.conv22.weight
numoFStages: 3
Count: 318880 ; 331066 ; 0.9631916294636115
[INFO] Storing checkpoint...
  75.94
Max memory: 90.4710144
 26.773s  j: 56 bis 60
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9545
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 56
Max memory: 0.1350656
lr: 0.1520018730558913
1
Epoche:56/60; Lr: 0.1520018730558913
batch Size 298
Epoch: [56][0/168]	Time 0.222 (0.222)	Data 0.419 (0.419)	Loss 0.7792 (0.7792)	Acc@1 84.228 (84.228)	Acc@5 99.329 (99.329)
Epoch: [56][64/168]	Time 0.141 (0.148)	Data 0.000 (0.007)	Loss 0.7692 (0.8151)	Acc@1 84.564 (84.275)	Acc@5 100.000 (99.288)
Epoch: [56][128/168]	Time 0.154 (0.149)	Data 0.000 (0.003)	Loss 0.7997 (0.8371)	Acc@1 85.906 (83.521)	Acc@5 99.329 (99.279)
Max memory in training epoch: 65.0254336
lr: 0.1520018730558913
1
Epoche:57/60; Lr: 0.1520018730558913
batch Size 298
Epoch: [57][0/168]	Time 0.192 (0.192)	Data 0.487 (0.487)	Loss 0.8356 (0.8356)	Acc@1 84.899 (84.899)	Acc@5 98.993 (98.993)
Epoch: [57][64/168]	Time 0.150 (0.151)	Data 0.000 (0.008)	Loss 0.8547 (0.8404)	Acc@1 82.886 (83.738)	Acc@5 99.664 (99.220)
Epoch: [57][128/168]	Time 0.121 (0.149)	Data 0.000 (0.004)	Loss 0.9603 (0.8457)	Acc@1 80.872 (83.565)	Acc@5 98.993 (99.220)
Max memory in training epoch: 65.2778496
lr: 0.1520018730558913
1
Epoche:58/60; Lr: 0.1520018730558913
batch Size 298
Epoch: [58][0/168]	Time 0.197 (0.197)	Data 0.403 (0.403)	Loss 0.8323 (0.8323)	Acc@1 84.564 (84.564)	Acc@5 99.329 (99.329)
Epoch: [58][64/168]	Time 0.137 (0.149)	Data 0.000 (0.006)	Loss 0.8356 (0.8395)	Acc@1 83.893 (83.671)	Acc@5 98.993 (99.251)
Epoch: [58][128/168]	Time 0.155 (0.150)	Data 0.000 (0.003)	Loss 0.8378 (0.8427)	Acc@1 84.228 (83.523)	Acc@5 99.664 (99.238)
Max memory in training epoch: 65.2778496
Drin!!
old memory: 667840512
new memory: 652778496
Faktor: 0.9774466871515576
New batch Size größer 305!!
lr: 0.1520018730558913
1
Epoche:59/60; Lr: 0.1520018730558913
batch Size 305
Epoch: [59][0/168]	Time 0.222 (0.222)	Data 0.359 (0.359)	Loss 0.8486 (0.8486)	Acc@1 83.221 (83.221)	Acc@5 99.664 (99.664)
Epoch: [59][64/168]	Time 0.134 (0.153)	Data 0.000 (0.006)	Loss 0.8638 (0.8418)	Acc@1 81.879 (83.665)	Acc@5 99.329 (99.308)
Epoch: [59][128/168]	Time 0.155 (0.152)	Data 0.000 (0.003)	Loss 0.8068 (0.8418)	Acc@1 85.906 (83.718)	Acc@5 98.993 (99.240)
Max memory in training epoch: 65.2778496
lr: 0.1520018730558913
1
Epoche:60/60; Lr: 0.1520018730558913
batch Size 305
Epoch: [60][0/168]	Time 0.196 (0.196)	Data 0.382 (0.382)	Loss 0.7984 (0.7984)	Acc@1 84.899 (84.899)	Acc@5 98.658 (98.658)
Epoch: [60][64/168]	Time 0.157 (0.150)	Data 0.000 (0.006)	Loss 0.8504 (0.8245)	Acc@1 83.893 (84.337)	Acc@5 99.329 (99.179)
Epoch: [60][128/168]	Time 0.143 (0.151)	Data 0.000 (0.003)	Loss 0.8851 (0.8388)	Acc@1 81.879 (83.825)	Acc@5 98.993 (99.186)
Max memory in training epoch: 65.2778496
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 303440 ; 318880 ; 0.9515805318615153
[INFO] Storing checkpoint...
  73.87
Max memory: 87.0074368
 25.863s  j: 61 bis 65
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8518
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 61
Max memory: 0.1289216
lr: 0.1810959815704955
1
Epoche:61/65; Lr: 0.1810959815704955
batch Size 305
Epoch: [61][0/164]	Time 0.221 (0.221)	Data 0.387 (0.387)	Loss 0.8509 (0.8509)	Acc@1 82.951 (82.951)	Acc@5 99.672 (99.672)
Epoch: [61][64/164]	Time 0.145 (0.150)	Data 0.000 (0.006)	Loss 0.7981 (0.8283)	Acc@1 85.246 (84.171)	Acc@5 99.016 (99.299)
Epoch: [61][128/164]	Time 0.158 (0.147)	Data 0.000 (0.003)	Loss 0.8561 (0.8573)	Acc@1 84.262 (83.266)	Acc@5 100.000 (99.210)
Max memory in training epoch: 68.2806272
lr: 0.1810959815704955
1
Epoche:62/65; Lr: 0.1810959815704955
batch Size 305
Epoch: [62][0/164]	Time 0.244 (0.244)	Data 0.424 (0.424)	Loss 0.8252 (0.8252)	Acc@1 83.934 (83.934)	Acc@5 98.033 (98.033)
Epoch: [62][64/164]	Time 0.132 (0.151)	Data 0.000 (0.007)	Loss 0.9192 (0.8726)	Acc@1 81.311 (82.764)	Acc@5 99.344 (99.193)
Epoch: [62][128/164]	Time 0.147 (0.150)	Data 0.000 (0.003)	Loss 0.9372 (0.8710)	Acc@1 81.967 (82.844)	Acc@5 99.016 (99.121)
Max memory in training epoch: 68.2000384
lr: 0.1810959815704955
1
Epoche:63/65; Lr: 0.1810959815704955
batch Size 305
Epoch: [63][0/164]	Time 0.192 (0.192)	Data 0.472 (0.472)	Loss 0.8275 (0.8275)	Acc@1 83.934 (83.934)	Acc@5 98.033 (98.033)
Epoch: [63][64/164]	Time 0.148 (0.151)	Data 0.000 (0.007)	Loss 0.9235 (0.8638)	Acc@1 80.984 (82.769)	Acc@5 99.672 (99.193)
Epoch: [63][128/164]	Time 0.157 (0.151)	Data 0.000 (0.004)	Loss 0.8864 (0.8672)	Acc@1 81.311 (82.918)	Acc@5 98.689 (99.171)
Max memory in training epoch: 68.2000384
Drin!!
old memory: 652778496
new memory: 682000384
Faktor: 1.0447653961934433
New batch Size kleiner 318!!
lr: 0.1810959815704955
1
Epoche:64/65; Lr: 0.1810959815704955
batch Size 318
Epoch: [64][0/164]	Time 0.196 (0.196)	Data 0.493 (0.493)	Loss 0.8114 (0.8114)	Acc@1 86.230 (86.230)	Acc@5 99.344 (99.344)
Epoch: [64][64/164]	Time 0.152 (0.149)	Data 0.000 (0.008)	Loss 0.8611 (0.8618)	Acc@1 82.295 (83.107)	Acc@5 99.672 (99.208)
Epoch: [64][128/164]	Time 0.144 (0.148)	Data 0.000 (0.004)	Loss 0.9125 (0.8664)	Acc@1 82.951 (82.958)	Acc@5 98.689 (99.141)
Max memory in training epoch: 68.2000384
lr: 0.1810959815704955
1
Epoche:65/65; Lr: 0.1810959815704955
batch Size 318
Epoch: [65][0/164]	Time 0.185 (0.185)	Data 0.453 (0.453)	Loss 0.8997 (0.8997)	Acc@1 78.689 (78.689)	Acc@5 99.344 (99.344)
Epoch: [65][64/164]	Time 0.150 (0.150)	Data 0.000 (0.007)	Loss 0.8276 (0.8366)	Acc@1 82.623 (83.798)	Acc@5 98.689 (99.238)
Epoch: [65][128/164]	Time 0.164 (0.149)	Data 0.000 (0.004)	Loss 0.9011 (0.8533)	Acc@1 81.967 (83.312)	Acc@5 99.344 (99.143)
Max memory in training epoch: 68.2000384
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 290014 ; 303440 ; 0.9557540205641972
[INFO] Storing checkpoint...
  69.32
Max memory: 86.6981888
 24.922s  j: 66 bis 70
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5800
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 66
Max memory: 0.123648
lr: 0.22495516460709988
1
Epoche:66/70; Lr: 0.22495516460709988
batch Size 318
Epoch: [66][0/158]	Time 0.263 (0.263)	Data 0.399 (0.399)	Loss 0.7777 (0.7777)	Acc@1 83.333 (83.333)	Acc@5 100.000 (100.000)
Epoch: [66][64/158]	Time 0.124 (0.149)	Data 0.000 (0.006)	Loss 0.8487 (0.8775)	Acc@1 83.962 (82.153)	Acc@5 99.686 (99.211)
Epoch: [66][128/158]	Time 0.160 (0.150)	Data 0.000 (0.003)	Loss 1.0466 (0.8997)	Acc@1 78.616 (81.734)	Acc@5 99.057 (99.071)
Max memory in training epoch: 68.1577984
lr: 0.22495516460709988
1
Epoche:67/70; Lr: 0.22495516460709988
batch Size 318
Epoch: [67][0/158]	Time 0.195 (0.195)	Data 0.347 (0.347)	Loss 0.8281 (0.8281)	Acc@1 84.906 (84.906)	Acc@5 99.371 (99.371)
Epoch: [67][64/158]	Time 0.158 (0.150)	Data 0.000 (0.006)	Loss 0.8657 (0.9068)	Acc@1 81.447 (81.872)	Acc@5 98.742 (99.061)
Epoch: [67][128/158]	Time 0.127 (0.152)	Data 0.000 (0.003)	Loss 0.9725 (0.9080)	Acc@1 83.962 (81.817)	Acc@5 97.170 (99.022)
Max memory in training epoch: 68.3268096
lr: 0.22495516460709988
1
Epoche:68/70; Lr: 0.22495516460709988
batch Size 318
Epoch: [68][0/158]	Time 0.198 (0.198)	Data 0.349 (0.349)	Loss 0.9233 (0.9233)	Acc@1 79.245 (79.245)	Acc@5 98.428 (98.428)
Epoch: [68][64/158]	Time 0.164 (0.147)	Data 0.000 (0.006)	Loss 0.9504 (0.9149)	Acc@1 81.761 (81.838)	Acc@5 98.428 (98.994)
Epoch: [68][128/158]	Time 0.162 (0.151)	Data 0.000 (0.003)	Loss 0.9444 (0.9116)	Acc@1 81.447 (81.917)	Acc@5 98.742 (99.027)
Max memory in training epoch: 68.3268096
Drin!!
old memory: 682000384
new memory: 683268096
Faktor: 1.0018588142026619
New batch Size kleiner 318!!
lr: 0.22495516460709988
1
Epoche:69/70; Lr: 0.22495516460709988
batch Size 318
Epoch: [69][0/158]	Time 0.193 (0.193)	Data 0.432 (0.432)	Loss 0.9044 (0.9044)	Acc@1 83.019 (83.019)	Acc@5 99.057 (99.057)
Epoch: [69][64/158]	Time 0.148 (0.151)	Data 0.000 (0.007)	Loss 0.9612 (0.9219)	Acc@1 81.761 (81.408)	Acc@5 98.742 (98.984)
Epoch: [69][128/158]	Time 0.138 (0.152)	Data 0.000 (0.004)	Loss 0.9886 (0.9088)	Acc@1 78.616 (81.854)	Acc@5 98.742 (99.025)
Max memory in training epoch: 68.3268096
lr: 0.22495516460709988
1
Epoche:70/70; Lr: 0.22495516460709988
batch Size 318
Epoch: [70][0/158]	Time 0.215 (0.215)	Data 0.334 (0.334)	Loss 0.9149 (0.9149)	Acc@1 81.761 (81.761)	Acc@5 99.371 (99.371)
Epoch: [70][64/158]	Time 0.152 (0.153)	Data 0.000 (0.005)	Loss 0.8668 (0.9091)	Acc@1 82.390 (81.650)	Acc@5 99.686 (98.999)
Epoch: [70][128/158]	Time 0.158 (0.153)	Data 0.000 (0.003)	Loss 0.9953 (0.9036)	Acc@1 79.245 (81.763)	Acc@5 99.371 (99.030)
Max memory in training epoch: 68.3268096
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 278752 ; 290014 ; 0.9611673919190108
[INFO] Storing checkpoint...
  75.68
Max memory: 85.3773824
 24.589s  j: 71 bis 75
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1199
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 71
Max memory: 0.1191424
lr: 0.2794364935353819
1
Epoche:71/75; Lr: 0.2794364935353819
batch Size 318
Epoch: [71][0/158]	Time 0.276 (0.276)	Data 0.348 (0.348)	Loss 0.9033 (0.9033)	Acc@1 82.704 (82.704)	Acc@5 98.428 (98.428)
Epoch: [71][64/158]	Time 0.144 (0.153)	Data 0.000 (0.006)	Loss 1.0181 (0.9202)	Acc@1 78.302 (81.326)	Acc@5 98.428 (98.994)
Epoch: [71][128/158]	Time 0.122 (0.146)	Data 0.000 (0.003)	Loss 0.9138 (0.9412)	Acc@1 82.390 (80.701)	Acc@5 98.742 (98.920)
Max memory in training epoch: 66.7848192
lr: 0.2794364935353819
1
Epoche:72/75; Lr: 0.2794364935353819
batch Size 318
Epoch: [72][0/158]	Time 0.202 (0.202)	Data 0.356 (0.356)	Loss 0.9529 (0.9529)	Acc@1 79.245 (79.245)	Acc@5 98.742 (98.742)
Epoch: [72][64/158]	Time 0.137 (0.152)	Data 0.000 (0.006)	Loss 0.9966 (0.9806)	Acc@1 79.874 (79.555)	Acc@5 99.686 (98.810)
Epoch: [72][128/158]	Time 0.131 (0.151)	Data 0.000 (0.003)	Loss 0.8649 (0.9661)	Acc@1 82.390 (80.137)	Acc@5 98.742 (98.847)
Max memory in training epoch: 66.8837376
lr: 0.2794364935353819
1
Epoche:73/75; Lr: 0.2794364935353819
batch Size 318
Epoch: [73][0/158]	Time 0.207 (0.207)	Data 0.354 (0.354)	Loss 0.9254 (0.9254)	Acc@1 83.019 (83.019)	Acc@5 98.428 (98.428)
Epoch: [73][64/158]	Time 0.191 (0.153)	Data 0.000 (0.006)	Loss 1.0263 (0.9542)	Acc@1 79.245 (80.706)	Acc@5 99.057 (98.887)
Epoch: [73][128/158]	Time 0.131 (0.152)	Data 0.000 (0.003)	Loss 0.9387 (0.9444)	Acc@1 79.560 (80.905)	Acc@5 99.371 (98.944)
Max memory in training epoch: 66.8837376
Drin!!
old memory: 683268096
new memory: 668837376
Faktor: 0.9788798568461186
New batch Size größer 318!!
lr: 0.2794364935353819
1
Epoche:74/75; Lr: 0.2794364935353819
batch Size 318
Epoch: [74][0/158]	Time 0.209 (0.209)	Data 0.401 (0.401)	Loss 0.9758 (0.9758)	Acc@1 79.245 (79.245)	Acc@5 99.371 (99.371)
Epoch: [74][64/158]	Time 0.163 (0.149)	Data 0.000 (0.006)	Loss 1.0003 (0.9679)	Acc@1 81.132 (79.715)	Acc@5 98.428 (98.974)
Epoch: [74][128/158]	Time 0.145 (0.149)	Data 0.000 (0.003)	Loss 0.9937 (0.9624)	Acc@1 73.899 (79.911)	Acc@5 99.371 (98.879)
Max memory in training epoch: 66.8837376
lr: 0.2794364935353819
1
Epoche:75/75; Lr: 0.2794364935353819
batch Size 318
Epoch: [75][0/158]	Time 0.198 (0.198)	Data 0.411 (0.411)	Loss 0.9529 (0.9529)	Acc@1 76.730 (76.730)	Acc@5 99.057 (99.057)
Epoch: [75][64/158]	Time 0.148 (0.152)	Data 0.000 (0.007)	Loss 0.9048 (0.9310)	Acc@1 79.560 (80.643)	Acc@5 99.057 (98.960)
Epoch: [75][128/158]	Time 0.125 (0.148)	Data 0.000 (0.003)	Loss 1.0417 (0.9375)	Acc@1 76.101 (80.432)	Acc@5 98.428 (98.881)
Max memory in training epoch: 66.8837376
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 260408 ; 278752 ; 0.9341924004132706
[INFO] Storing checkpoint...
  61.16
Max memory: 83.264256
 23.904s  j: 76 bis 80
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9011
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 76
Max memory: 0.1118208
lr: 0.3471125193134822
1
Epoche:76/80; Lr: 0.3471125193134822
batch Size 318
Epoch: [76][0/158]	Time 0.309 (0.309)	Data 0.439 (0.439)	Loss 0.9509 (0.9509)	Acc@1 81.132 (81.132)	Acc@5 99.371 (99.371)
Epoch: [76][64/158]	Time 0.141 (0.147)	Data 0.000 (0.007)	Loss 0.9016 (0.9595)	Acc@1 82.075 (79.860)	Acc@5 98.742 (98.887)
Epoch: [76][128/158]	Time 0.151 (0.147)	Data 0.000 (0.004)	Loss 1.0082 (0.9873)	Acc@1 79.560 (79.101)	Acc@5 97.799 (98.732)
Max memory in training epoch: 63.8025216
lr: 0.3471125193134822
1
Epoche:77/80; Lr: 0.3471125193134822
batch Size 318
Epoch: [77][0/158]	Time 0.206 (0.206)	Data 0.391 (0.391)	Loss 1.0210 (1.0210)	Acc@1 80.189 (80.189)	Acc@5 99.057 (99.057)
Epoch: [77][64/158]	Time 0.160 (0.150)	Data 0.000 (0.006)	Loss 0.9244 (1.0091)	Acc@1 83.019 (78.960)	Acc@5 99.371 (98.578)
Epoch: [77][128/158]	Time 0.150 (0.151)	Data 0.000 (0.003)	Loss 1.0109 (0.9995)	Acc@1 78.616 (79.019)	Acc@5 98.742 (98.706)
Max memory in training epoch: 63.8873088
lr: 0.3471125193134822
1
Epoche:78/80; Lr: 0.3471125193134822
batch Size 318
Epoch: [78][0/158]	Time 0.240 (0.240)	Data 0.326 (0.326)	Loss 1.0316 (1.0316)	Acc@1 79.560 (79.560)	Acc@5 97.799 (97.799)
Epoch: [78][64/158]	Time 0.169 (0.150)	Data 0.000 (0.005)	Loss 0.9601 (1.0018)	Acc@1 78.302 (79.057)	Acc@5 98.742 (98.597)
Epoch: [78][128/158]	Time 0.143 (0.151)	Data 0.000 (0.003)	Loss 0.9857 (1.0047)	Acc@1 80.189 (78.826)	Acc@5 98.113 (98.623)
Max memory in training epoch: 63.9225344
Drin!!
old memory: 668837376
new memory: 639225344
Faktor: 0.9557261106173588
New batch Size größer 333!!
lr: 0.3471125193134822
1
Epoche:79/80; Lr: 0.3471125193134822
batch Size 333
Epoch: [79][0/158]	Time 0.226 (0.226)	Data 0.373 (0.373)	Loss 1.0098 (1.0098)	Acc@1 78.616 (78.616)	Acc@5 99.371 (99.371)
Epoch: [79][64/158]	Time 0.166 (0.149)	Data 0.000 (0.006)	Loss 0.9644 (1.0012)	Acc@1 78.931 (78.665)	Acc@5 99.686 (98.713)
Epoch: [79][128/158]	Time 0.154 (0.148)	Data 0.000 (0.003)	Loss 0.9915 (0.9940)	Acc@1 77.044 (79.077)	Acc@5 98.742 (98.723)
Max memory in training epoch: 63.9225344
lr: 0.3471125193134822
1
Epoche:80/80; Lr: 0.3471125193134822
batch Size 333
Epoch: [80][0/158]	Time 0.209 (0.209)	Data 0.412 (0.412)	Loss 1.0912 (1.0912)	Acc@1 73.899 (73.899)	Acc@5 99.057 (99.057)
Epoch: [80][64/158]	Time 0.153 (0.146)	Data 0.000 (0.007)	Loss 0.9675 (0.9835)	Acc@1 79.874 (79.269)	Acc@5 99.057 (98.824)
Epoch: [80][128/158]	Time 0.146 (0.147)	Data 0.000 (0.003)	Loss 0.8795 (0.9919)	Acc@1 84.277 (79.075)	Acc@5 99.686 (98.791)
Max memory in training epoch: 63.9225344
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv17.weight

 RM:  module.conv18.weight

 RM:  module.conv19.weight

 RM:  module.conv20.weight
numoFStages: 3
Count: 246274 ; 260408 ; 0.945723633682529
[INFO] Storing checkpoint...
  51.78
Max memory: 79.5624448
 23.727s  j: 81 bis 85
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 1266
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 81
Max memory: 0.1053184
lr: 0.4515174567632405
1
Epoche:81/85; Lr: 0.4515174567632405
batch Size 333
Epoch: [81][0/151]	Time 0.246 (0.246)	Data 0.351 (0.351)	Loss 1.0054 (1.0054)	Acc@1 79.880 (79.880)	Acc@5 98.198 (98.198)
Epoch: [81][64/151]	Time 0.143 (0.134)	Data 0.000 (0.006)	Loss 1.2048 (1.0484)	Acc@1 72.072 (77.149)	Acc@5 98.498 (98.475)
Epoch: [81][128/151]	Time 0.122 (0.133)	Data 0.000 (0.003)	Loss 1.0694 (1.0538)	Acc@1 75.676 (77.189)	Acc@5 98.498 (98.464)
Max memory in training epoch: 60.292608
lr: 0.4515174567632405
1
Epoche:82/85; Lr: 0.4515174567632405
batch Size 333
Epoch: [82][0/151]	Time 0.201 (0.201)	Data 0.411 (0.411)	Loss 1.1826 (1.1826)	Acc@1 73.874 (73.874)	Acc@5 97.297 (97.297)
Epoch: [82][64/151]	Time 0.117 (0.136)	Data 0.000 (0.007)	Loss 1.0417 (1.0850)	Acc@1 75.676 (76.549)	Acc@5 98.498 (98.406)
Epoch: [82][128/151]	Time 0.150 (0.133)	Data 0.000 (0.003)	Loss 1.0453 (1.0716)	Acc@1 73.874 (76.847)	Acc@5 98.198 (98.429)
Max memory in training epoch: 60.2892288
lr: 0.4515174567632405
1
Epoche:83/85; Lr: 0.4515174567632405
batch Size 333
Epoch: [83][0/151]	Time 0.245 (0.245)	Data 0.371 (0.371)	Loss 0.9596 (0.9596)	Acc@1 83.183 (83.183)	Acc@5 98.198 (98.198)
Epoch: [83][64/151]	Time 0.123 (0.139)	Data 0.000 (0.006)	Loss 1.0767 (1.0712)	Acc@1 75.976 (76.590)	Acc@5 98.198 (98.341)
Epoch: [83][128/151]	Time 0.142 (0.137)	Data 0.000 (0.003)	Loss 1.0690 (1.0565)	Acc@1 76.577 (77.058)	Acc@5 98.498 (98.422)
Max memory in training epoch: 60.2892288
Drin!!
old memory: 639225344
new memory: 602892288
Faktor: 0.9431608018345405
New batch Size größer 370!!
lr: 0.4515174567632405
1
Epoche:84/85; Lr: 0.4515174567632405
batch Size 370
Epoch: [84][0/151]	Time 0.205 (0.205)	Data 0.353 (0.353)	Loss 1.0004 (1.0004)	Acc@1 79.279 (79.279)	Acc@5 99.099 (99.099)
Epoch: [84][64/151]	Time 0.143 (0.138)	Data 0.000 (0.006)	Loss 1.1159 (1.0467)	Acc@1 75.375 (77.233)	Acc@5 98.198 (98.489)
Epoch: [84][128/151]	Time 0.108 (0.138)	Data 0.000 (0.003)	Loss 1.0950 (1.0467)	Acc@1 77.778 (77.303)	Acc@5 99.099 (98.452)
Max memory in training epoch: 60.2892288
lr: 0.4515174567632405
1
Epoche:85/85; Lr: 0.4515174567632405
batch Size 370
Epoch: [85][0/151]	Time 0.207 (0.207)	Data 0.403 (0.403)	Loss 1.0764 (1.0764)	Acc@1 76.877 (76.877)	Acc@5 97.297 (97.297)
Epoch: [85][64/151]	Time 0.129 (0.137)	Data 0.000 (0.006)	Loss 1.1482 (1.0640)	Acc@1 72.372 (76.618)	Acc@5 96.697 (98.272)
Epoch: [85][128/151]	Time 0.126 (0.138)	Data 0.000 (0.003)	Loss 1.1727 (1.0564)	Acc@1 73.574 (76.642)	Acc@5 98.498 (98.375)
Max memory in training epoch: 60.2892288
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...

 RM:  module.conv26.weight

 RM:  module.conv27.weight
numoFStages: 3
Count: 230696 ; 246274 ; 0.9367452512242461
[INFO] Storing checkpoint...
  47.08
Max memory: 72.0083456
 21.107s  j: 86 bis 90
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8557
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 86
Max memory: 0.0984576
lr: 0.652583824228121
1
Epoche:86/90; Lr: 0.652583824228121
batch Size 370
Epoch: [86][0/136]	Time 0.235 (0.235)	Data 0.384 (0.384)	Loss 1.0731 (1.0731)	Acc@1 75.135 (75.135)	Acc@5 98.649 (98.649)
Epoch: [86][64/136]	Time 0.120 (0.130)	Data 0.000 (0.006)	Loss 1.0707 (1.1088)	Acc@1 74.865 (74.765)	Acc@5 98.378 (98.295)
Epoch: [86][128/136]	Time 0.127 (0.127)	Data 0.000 (0.003)	Loss 1.1614 (1.1217)	Acc@1 75.405 (74.896)	Acc@5 97.297 (98.234)
Max memory in training epoch: 64.4671488
lr: 0.652583824228121
1
Epoche:87/90; Lr: 0.652583824228121
batch Size 370
Epoch: [87][0/136]	Time 0.234 (0.234)	Data 0.329 (0.329)	Loss 1.2160 (1.2160)	Acc@1 73.514 (73.514)	Acc@5 97.838 (97.838)
Epoch: [87][64/136]	Time 0.132 (0.131)	Data 0.000 (0.005)	Loss 1.1354 (1.1627)	Acc@1 75.405 (74.225)	Acc@5 98.649 (97.917)
Epoch: [87][128/136]	Time 0.125 (0.130)	Data 0.000 (0.003)	Loss 0.9871 (1.1354)	Acc@1 80.811 (74.930)	Acc@5 99.730 (98.062)
Max memory in training epoch: 64.5429248
lr: 0.652583824228121
1
Epoche:88/90; Lr: 0.652583824228121
batch Size 370
Epoch: [88][0/136]	Time 0.173 (0.173)	Data 0.288 (0.288)	Loss 1.2307 (1.2307)	Acc@1 74.595 (74.595)	Acc@5 96.216 (96.216)
Epoch: [88][64/136]	Time 0.125 (0.137)	Data 0.000 (0.005)	Loss 1.2981 (1.2012)	Acc@1 71.892 (72.686)	Acc@5 96.216 (97.842)
Epoch: [88][128/136]	Time 0.121 (0.134)	Data 0.000 (0.002)	Loss 1.0200 (1.1598)	Acc@1 75.676 (73.662)	Acc@5 99.730 (98.102)
Max memory in training epoch: 64.5429248
Drin!!
old memory: 602892288
new memory: 645429248
Faktor: 1.0705548252095074
New batch Size kleiner 396!!
lr: 0.652583824228121
1
Epoche:89/90; Lr: 0.652583824228121
batch Size 396
Epoch: [89][0/136]	Time 0.183 (0.183)	Data 0.409 (0.409)	Loss 1.1673 (1.1673)	Acc@1 73.243 (73.243)	Acc@5 97.838 (97.838)
Epoch: [89][64/136]	Time 0.139 (0.132)	Data 0.002 (0.007)	Loss 1.0257 (1.1239)	Acc@1 75.676 (74.915)	Acc@5 98.649 (98.162)
Epoch: [89][128/136]	Time 0.140 (0.134)	Data 0.000 (0.003)	Loss 1.0829 (1.1219)	Acc@1 74.595 (74.634)	Acc@5 98.919 (98.091)
Max memory in training epoch: 64.5429248
lr: 0.652583824228121
1
Epoche:90/90; Lr: 0.652583824228121
batch Size 396
Epoch: [90][0/136]	Time 0.230 (0.230)	Data 0.364 (0.364)	Loss 1.2437 (1.2437)	Acc@1 70.270 (70.270)	Acc@5 98.108 (98.108)
Epoch: [90][64/136]	Time 0.115 (0.133)	Data 0.000 (0.006)	Loss 1.0618 (1.1241)	Acc@1 77.297 (74.599)	Acc@5 98.378 (98.091)
Epoch: [90][128/136]	Time 0.128 (0.131)	Data 0.000 (0.003)	Loss 1.0242 (1.1143)	Acc@1 75.405 (74.779)	Acc@5 99.189 (98.160)
Max memory in training epoch: 64.5429248
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 213940 ; 230696 ; 0.9273676179907757
[INFO] Storing checkpoint...
  51.85
Max memory: 67.9169536
 18.247s  j: 91 bis 95
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 7343
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 91
Max memory: 0.0918016
lr: 1.0094656031028746
1
Epoche:91/95; Lr: 1.0094656031028746
batch Size 396
Epoch: [91][0/127]	Time 0.262 (0.262)	Data 0.380 (0.380)	Loss 1.2283 (1.2283)	Acc@1 68.687 (68.687)	Acc@5 97.980 (97.980)
Epoch: [91][64/127]	Time 0.133 (0.128)	Data 0.000 (0.006)	Loss 1.2468 (1.2433)	Acc@1 73.485 (70.672)	Acc@5 98.232 (97.413)
Max memory in training epoch: 63.9948288
lr: 1.0094656031028746
1
Epoche:92/95; Lr: 1.0094656031028746
batch Size 396
Epoch: [92][0/127]	Time 0.233 (0.233)	Data 0.453 (0.453)	Loss 1.2226 (1.2226)	Acc@1 72.222 (72.222)	Acc@5 96.717 (96.717)
Epoch: [92][64/127]	Time 0.151 (0.134)	Data 0.000 (0.007)	Loss 1.2479 (1.2330)	Acc@1 68.434 (71.465)	Acc@5 97.980 (97.537)
Max memory in training epoch: 63.9948288
lr: 1.0094656031028746
1
Epoche:93/95; Lr: 0.10094656031028747
batch Size 396
Epoch: [93][0/127]	Time 0.179 (0.179)	Data 0.367 (0.367)	Loss 1.3851 (1.3851)	Acc@1 66.414 (66.414)	Acc@5 96.465 (96.465)
Epoch: [93][64/127]	Time 0.148 (0.135)	Data 0.000 (0.006)	Loss 0.9684 (1.0248)	Acc@1 80.303 (78.026)	Acc@5 98.485 (98.574)
Max memory in training epoch: 63.9948288
Drin!!
old memory: 645429248
new memory: 639948288
Faktor: 0.9915080390035252
New batch Size größer 414!!
lr: 0.10094656031028747
1
Epoche:94/95; Lr: 0.10094656031028747
batch Size 414
Epoch: [94][0/127]	Time 0.241 (0.241)	Data 0.325 (0.325)	Loss 0.8172 (0.8172)	Acc@1 84.596 (84.596)	Acc@5 99.242 (99.242)
Epoch: [94][64/127]	Time 0.147 (0.136)	Data 0.000 (0.005)	Loss 0.8698 (0.8388)	Acc@1 79.040 (83.108)	Acc@5 99.495 (99.122)
Max memory in training epoch: 63.9948288
lr: 0.10094656031028747
1
Epoche:95/95; Lr: 0.10094656031028747
batch Size 414
Epoch: [95][0/127]	Time 0.191 (0.191)	Data 0.349 (0.349)	Loss 0.7656 (0.7656)	Acc@1 83.586 (83.586)	Acc@5 99.242 (99.242)
Epoch: [95][64/127]	Time 0.120 (0.139)	Data 0.000 (0.006)	Loss 0.7961 (0.7682)	Acc@1 82.828 (83.951)	Acc@5 99.495 (99.308)
Max memory in training epoch: 63.9948288
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 190994 ; 213940 ; 0.8927456296157801
[INFO] Storing checkpoint...
  82.15
Max memory: 65.2677632
 17.786s  j: 96 bis 100
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4455
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 96
Max memory: 0.0827904
lr: 0.163249515501793
1
Epoche:96/100; Lr: 0.163249515501793
batch Size 414
Epoch: [96][0/121]	Time 0.184 (0.184)	Data 0.309 (0.309)	Loss 0.7422 (0.7422)	Acc@1 85.507 (85.507)	Acc@5 99.517 (99.517)
Epoch: [96][64/121]	Time 0.115 (0.134)	Data 0.000 (0.005)	Loss 0.7391 (0.7540)	Acc@1 84.541 (83.876)	Acc@5 99.275 (99.279)
Max memory in training epoch: 65.7952256
lr: 0.163249515501793
1
Epoche:97/100; Lr: 0.163249515501793
batch Size 414
Epoch: [97][0/121]	Time 0.177 (0.177)	Data 0.439 (0.439)	Loss 0.7602 (0.7602)	Acc@1 82.367 (82.367)	Acc@5 99.034 (99.034)
Epoch: [97][64/121]	Time 0.149 (0.137)	Data 0.000 (0.007)	Loss 0.7743 (0.7338)	Acc@1 82.126 (83.757)	Acc@5 99.517 (99.238)
Max memory in training epoch: 65.7952256
lr: 0.163249515501793
1
Epoche:98/100; Lr: 0.163249515501793
batch Size 414
Epoch: [98][0/121]	Time 0.185 (0.185)	Data 0.442 (0.442)	Loss 0.7445 (0.7445)	Acc@1 84.058 (84.058)	Acc@5 98.068 (98.068)
Epoch: [98][64/121]	Time 0.144 (0.141)	Data 0.000 (0.007)	Loss 0.7739 (0.7314)	Acc@1 83.092 (83.393)	Acc@5 99.275 (99.261)
Max memory in training epoch: 65.7952256
Drin!!
old memory: 639948288
new memory: 657952256
Faktor: 1.0281334731846332
New batch Size kleiner 425!!
lr: 0.163249515501793
1
Epoche:99/100; Lr: 0.163249515501793
batch Size 425
Epoch: [99][0/121]	Time 0.200 (0.200)	Data 0.403 (0.403)	Loss 0.7216 (0.7216)	Acc@1 79.710 (79.710)	Acc@5 99.517 (99.517)
Epoch: [99][64/121]	Time 0.135 (0.138)	Data 0.000 (0.006)	Loss 0.6994 (0.7251)	Acc@1 84.783 (83.515)	Acc@5 99.034 (99.205)
Max memory in training epoch: 65.7952256
lr: 0.163249515501793
1
Epoche:100/100; Lr: 0.163249515501793
batch Size 425
Epoch: [100][0/121]	Time 0.181 (0.181)	Data 0.441 (0.441)	Loss 0.6545 (0.6545)	Acc@1 86.473 (86.473)	Acc@5 98.792 (98.792)
Epoch: [100][64/121]	Time 0.128 (0.139)	Data 0.000 (0.007)	Loss 0.7756 (0.7188)	Acc@1 81.884 (83.820)	Acc@5 98.792 (99.264)
Max memory in training epoch: 65.7952256
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 180953 ; 190994 ; 0.947427667884855
[INFO] Storing checkpoint...
  74.98
Max memory: 65.7952256
 17.268s  j: 101 bis 105
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2648
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 101
Max memory: 0.0787456
lr: 0.27101970346977355
1
Epoche:101/105; Lr: 0.27101970346977355
batch Size 425
Epoch: [101][0/118]	Time 0.237 (0.237)	Data 0.334 (0.334)	Loss 0.6665 (0.6665)	Acc@1 88.235 (88.235)	Acc@5 99.294 (99.294)
Epoch: [101][64/118]	Time 0.141 (0.139)	Data 0.000 (0.005)	Loss 0.9018 (0.8026)	Acc@1 76.706 (80.771)	Acc@5 97.882 (98.910)
Max memory in training epoch: 66.3753216
lr: 0.27101970346977355
1
Epoche:102/105; Lr: 0.27101970346977355
batch Size 425
Epoch: [102][0/118]	Time 0.201 (0.201)	Data 0.431 (0.431)	Loss 0.8561 (0.8561)	Acc@1 80.000 (80.000)	Acc@5 98.353 (98.353)
Epoch: [102][64/118]	Time 0.130 (0.136)	Data 0.000 (0.007)	Loss 0.9186 (0.8356)	Acc@1 78.353 (80.438)	Acc@5 99.059 (98.939)
Max memory in training epoch: 66.3753216
lr: 0.27101970346977355
1
Epoche:103/105; Lr: 0.27101970346977355
batch Size 425
Epoch: [103][0/118]	Time 0.185 (0.185)	Data 0.426 (0.426)	Loss 0.8141 (0.8141)	Acc@1 82.353 (82.353)	Acc@5 98.588 (98.588)
Epoch: [103][64/118]	Time 0.122 (0.138)	Data 0.000 (0.007)	Loss 0.9011 (0.8253)	Acc@1 77.882 (81.046)	Acc@5 99.059 (98.954)
Max memory in training epoch: 66.3753216
Drin!!
old memory: 657952256
new memory: 663753216
Faktor: 1.008816688364695
New batch Size kleiner 428!!
lr: 0.27101970346977355
1
Epoche:104/105; Lr: 0.27101970346977355
batch Size 428
Epoch: [104][0/118]	Time 0.170 (0.170)	Data 0.385 (0.385)	Loss 0.8313 (0.8313)	Acc@1 81.882 (81.882)	Acc@5 99.765 (99.765)
Epoch: [104][64/118]	Time 0.154 (0.141)	Data 0.000 (0.006)	Loss 0.7646 (0.8251)	Acc@1 82.588 (81.300)	Acc@5 99.765 (98.863)
Max memory in training epoch: 66.3753216
lr: 0.27101970346977355
1
Epoche:105/105; Lr: 0.27101970346977355
batch Size 428
Epoch: [105][0/118]	Time 0.188 (0.188)	Data 0.413 (0.413)	Loss 0.8027 (0.8027)	Acc@1 82.824 (82.824)	Acc@5 99.765 (99.765)
Epoch: [105][64/118]	Time 0.129 (0.143)	Data 0.000 (0.007)	Loss 0.8762 (0.8210)	Acc@1 78.824 (81.426)	Acc@5 99.059 (98.972)
Max memory in training epoch: 66.3753216
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 173341 ; 180953 ; 0.9579338281211143
[INFO] Storing checkpoint...
  68.06
Max memory: 66.3753216
 17.121s  j: 106 bis 110
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8861
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 106
Max memory: 0.0757248
lr: 0.45311106673852763
1
Epoche:106/110; Lr: 0.45311106673852763
batch Size 428
Epoch: [106][0/117]	Time 0.230 (0.230)	Data 0.362 (0.362)	Loss 0.8555 (0.8555)	Acc@1 80.607 (80.607)	Acc@5 97.664 (97.664)
Epoch: [106][64/117]	Time 0.140 (0.136)	Data 0.000 (0.006)	Loss 0.9620 (0.9322)	Acc@1 75.935 (77.739)	Acc@5 99.065 (98.512)
Max memory in training epoch: 66.548224
lr: 0.45311106673852763
1
Epoche:107/110; Lr: 0.45311106673852763
batch Size 428
Epoch: [107][0/117]	Time 0.187 (0.187)	Data 0.444 (0.444)	Loss 0.9855 (0.9855)	Acc@1 79.439 (79.439)	Acc@5 98.832 (98.832)
Epoch: [107][64/117]	Time 0.118 (0.138)	Data 0.000 (0.007)	Loss 0.8935 (0.9456)	Acc@1 81.075 (78.476)	Acc@5 98.832 (98.605)
Max memory in training epoch: 66.548224
lr: 0.45311106673852763
1
Epoche:108/110; Lr: 0.45311106673852763
batch Size 428
Epoch: [108][0/117]	Time 0.188 (0.188)	Data 0.506 (0.506)	Loss 1.0070 (1.0070)	Acc@1 75.234 (75.234)	Acc@5 98.832 (98.832)
Epoch: [108][64/117]	Time 0.139 (0.139)	Data 0.000 (0.008)	Loss 0.9082 (0.9474)	Acc@1 81.075 (78.113)	Acc@5 99.065 (98.609)
Max memory in training epoch: 66.548224
Drin!!
old memory: 663753216
new memory: 665482240
Faktor: 1.0026049199586853
New batch Size kleiner 429!!
lr: 0.45311106673852763
1
Epoche:109/110; Lr: 0.45311106673852763
batch Size 429
Epoch: [109][0/117]	Time 0.174 (0.174)	Data 0.479 (0.479)	Loss 0.9488 (0.9488)	Acc@1 77.336 (77.336)	Acc@5 98.364 (98.364)
Epoch: [109][64/117]	Time 0.162 (0.139)	Data 0.000 (0.008)	Loss 0.9888 (0.9378)	Acc@1 77.804 (78.648)	Acc@5 99.065 (98.749)
Max memory in training epoch: 66.548224
lr: 0.45311106673852763
1
Epoche:110/110; Lr: 0.45311106673852763
batch Size 429
Epoch: [110][0/117]	Time 0.223 (0.223)	Data 0.375 (0.375)	Loss 0.9359 (0.9359)	Acc@1 78.505 (78.505)	Acc@5 98.832 (98.832)
Epoch: [110][64/117]	Time 0.132 (0.140)	Data 0.000 (0.006)	Loss 0.9298 (0.9366)	Acc@1 80.607 (78.832)	Acc@5 98.832 (98.663)
Max memory in training epoch: 66.548224
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 168597 ; 173341 ; 0.9726319797393577
[INFO] Storing checkpoint...
  54.1
Max memory: 66.548224
 16.869s  j: 111 bis 115
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4469
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 111
Max memory: 0.0738304
lr: 0.7593150298079233
1
Epoche:111/115; Lr: 0.7593150298079233
batch Size 429
Epoch: [111][0/117]	Time 0.245 (0.245)	Data 0.417 (0.417)	Loss 0.9868 (0.9868)	Acc@1 79.021 (79.021)	Acc@5 98.368 (98.368)
Epoch: [111][64/117]	Time 0.134 (0.136)	Data 0.000 (0.007)	Loss 1.0786 (1.0785)	Acc@1 76.457 (74.266)	Acc@5 98.135 (98.196)
Max memory in training epoch: 66.5076736
lr: 0.7593150298079233
1
Epoche:112/115; Lr: 0.7593150298079233
batch Size 429
Epoch: [112][0/117]	Time 0.211 (0.211)	Data 0.458 (0.458)	Loss 1.1736 (1.1736)	Acc@1 69.464 (69.464)	Acc@5 98.368 (98.368)
Epoch: [112][64/117]	Time 0.140 (0.140)	Data 0.000 (0.007)	Loss 1.0976 (1.0853)	Acc@1 73.660 (74.879)	Acc@5 97.669 (98.196)
Max memory in training epoch: 66.5076736
lr: 0.7593150298079233
1
Epoche:113/115; Lr: 0.7593150298079233
batch Size 429
Epoch: [113][0/117]	Time 0.193 (0.193)	Data 0.387 (0.387)	Loss 1.1028 (1.1028)	Acc@1 74.359 (74.359)	Acc@5 99.068 (99.068)
Epoch: [113][64/117]	Time 0.150 (0.140)	Data 0.000 (0.006)	Loss 1.0976 (1.0839)	Acc@1 75.058 (74.990)	Acc@5 97.902 (98.182)
Max memory in training epoch: 66.5076736
Drin!!
old memory: 665482240
new memory: 665076736
Faktor: 0.9993906614247136
New batch Size größer 432!!
lr: 0.7593150298079233
1
Epoche:114/115; Lr: 0.7593150298079233
batch Size 432
Epoch: [114][0/117]	Time 0.199 (0.199)	Data 0.382 (0.382)	Loss 1.0674 (1.0674)	Acc@1 75.058 (75.058)	Acc@5 98.601 (98.601)
Epoch: [114][64/117]	Time 0.160 (0.140)	Data 0.000 (0.006)	Loss 1.0884 (1.0706)	Acc@1 76.690 (75.187)	Acc@5 96.970 (98.114)
Max memory in training epoch: 66.5076736
lr: 0.7593150298079233
1
Epoche:115/115; Lr: 0.7593150298079233
batch Size 432
Epoch: [115][0/117]	Time 0.237 (0.237)	Data 0.449 (0.449)	Loss 1.2078 (1.2078)	Acc@1 70.396 (70.396)	Acc@5 99.068 (99.068)
Epoch: [115][64/117]	Time 0.150 (0.141)	Data 0.000 (0.007)	Loss 1.1669 (1.0908)	Acc@1 71.329 (74.377)	Acc@5 96.270 (98.035)
Max memory in training epoch: 66.5076736
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 164582 ; 168597 ; 0.9761858158804724
[INFO] Storing checkpoint...
  50.68
Max memory: 66.5076736
 16.935s  j: 116 bis 120
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4344
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 116
Max memory: 0.0722432
lr: 1.2813441128008707
1
Epoche:116/120; Lr: 1.2813441128008707
batch Size 432
Epoch: [116][0/116]	Time 0.205 (0.205)	Data 0.326 (0.326)	Loss 1.0432 (1.0432)	Acc@1 77.778 (77.778)	Acc@5 99.074 (99.074)
Epoch: [116][64/116]	Time 0.126 (0.142)	Data 0.000 (0.005)	Loss 1.3551 (1.2554)	Acc@1 65.741 (69.469)	Acc@5 95.833 (97.105)
Max memory in training epoch: 67.9627776
lr: 1.2813441128008707
1
Epoche:117/120; Lr: 1.2813441128008707
batch Size 432
Epoch: [117][0/116]	Time 0.219 (0.219)	Data 0.450 (0.450)	Loss 1.3430 (1.3430)	Acc@1 69.213 (69.213)	Acc@5 95.833 (95.833)
Epoch: [117][64/116]	Time 0.120 (0.140)	Data 0.000 (0.007)	Loss 1.3390 (1.2822)	Acc@1 71.065 (69.797)	Acc@5 96.296 (97.176)
Max memory in training epoch: 67.9627776
lr: 1.2813441128008707
1
Epoche:118/120; Lr: 1.2813441128008707
batch Size 432
Epoch: [118][0/116]	Time 0.200 (0.200)	Data 0.462 (0.462)	Loss 1.2445 (1.2445)	Acc@1 68.519 (68.519)	Acc@5 97.222 (97.222)
Epoch: [118][64/116]	Time 0.129 (0.141)	Data 0.000 (0.007)	Loss 1.1959 (1.2382)	Acc@1 70.602 (70.157)	Acc@5 96.528 (97.347)
Max memory in training epoch: 67.9627776
Drin!!
old memory: 665076736
new memory: 679627776
Faktor: 1.0218787385159718
New batch Size kleiner 441!!
lr: 1.2813441128008707
1
Epoche:119/120; Lr: 1.2813441128008707
batch Size 441
Epoch: [119][0/116]	Time 0.188 (0.188)	Data 0.463 (0.463)	Loss 1.3608 (1.3608)	Acc@1 64.815 (64.815)	Acc@5 97.454 (97.454)
Epoch: [119][64/116]	Time 0.138 (0.145)	Data 0.000 (0.007)	Loss 1.2083 (1.2188)	Acc@1 70.139 (70.192)	Acc@5 96.528 (97.518)
Max memory in training epoch: 67.9627776
lr: 1.2813441128008707
1
Epoche:120/120; Lr: 1.2813441128008707
batch Size 441
Epoch: [120][0/116]	Time 0.199 (0.199)	Data 0.433 (0.433)	Loss 1.2230 (1.2230)	Acc@1 70.833 (70.833)	Acc@5 96.065 (96.065)
Epoch: [120][64/116]	Time 0.148 (0.145)	Data 0.000 (0.007)	Loss 1.3458 (1.2633)	Acc@1 65.509 (68.444)	Acc@5 97.685 (97.105)
Max memory in training epoch: 67.9627776
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 150425 ; 164582 ; 0.9139820879561555
[INFO] Storing checkpoint...
  47.62
Max memory: 67.9627776
 17.169s  j: 121 bis 125
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 6064
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 121
Max memory: 0.0666624
lr: 2.207315444317125
1
Epoche:121/125; Lr: 2.207315444317125
batch Size 441
Epoch: [121][0/114]	Time 0.246 (0.246)	Data 0.414 (0.414)	Loss 1.2375 (1.2375)	Acc@1 67.574 (67.574)	Acc@5 97.506 (97.506)
Epoch: [121][64/114]	Time 0.115 (0.138)	Data 0.000 (0.007)	Loss 1.5782 (1.5179)	Acc@1 61.905 (61.420)	Acc@5 92.971 (95.318)
Max memory in training epoch: 67.0480384
lr: 2.207315444317125
1
Epoche:122/125; Lr: 2.207315444317125
batch Size 441
Epoch: [122][0/114]	Time 0.188 (0.188)	Data 0.466 (0.466)	Loss 1.4078 (1.4078)	Acc@1 64.853 (64.853)	Acc@5 95.238 (95.238)
Epoch: [122][64/114]	Time 0.137 (0.141)	Data 0.000 (0.007)	Loss 1.3768 (1.4830)	Acc@1 67.574 (62.815)	Acc@5 95.465 (95.786)
Max memory in training epoch: 67.0480384
lr: 2.207315444317125
1
Epoche:123/125; Lr: 2.207315444317125
batch Size 441
Epoch: [123][0/114]	Time 0.196 (0.196)	Data 0.417 (0.417)	Loss 1.4442 (1.4442)	Acc@1 63.719 (63.719)	Acc@5 94.331 (94.331)
Epoch: [123][64/114]	Time 0.122 (0.138)	Data 0.000 (0.007)	Loss 1.5064 (1.4329)	Acc@1 59.637 (62.093)	Acc@5 95.692 (95.904)
Max memory in training epoch: 67.0480384
Drin!!
old memory: 679627776
new memory: 670480384
Faktor: 0.9865405854159792
New batch Size größer 440!!
lr: 2.207315444317125
1
Epoche:124/125; Lr: 2.207315444317125
batch Size 440
Epoch: [124][0/114]	Time 0.223 (0.223)	Data 0.411 (0.411)	Loss 1.3783 (1.3783)	Acc@1 63.039 (63.039)	Acc@5 97.279 (97.279)
Epoch: [124][64/114]	Time 0.140 (0.141)	Data 0.000 (0.007)	Loss 1.3976 (1.4256)	Acc@1 59.184 (62.215)	Acc@5 97.279 (95.999)
Max memory in training epoch: 67.0480384
lr: 2.207315444317125
1
Epoche:125/125; Lr: 2.207315444317125
batch Size 440
Epoch: [125][0/114]	Time 0.199 (0.199)	Data 0.428 (0.428)	Loss 1.4765 (1.4765)	Acc@1 60.544 (60.544)	Acc@5 94.558 (94.558)
Epoch: [125][64/114]	Time 0.132 (0.141)	Data 0.000 (0.007)	Loss 1.3408 (1.4402)	Acc@1 64.853 (61.176)	Acc@5 96.825 (95.524)
Max memory in training epoch: 67.0480384
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 143542 ; 150425 ; 0.954242978228353
[INFO] Storing checkpoint...
  37.22
Max memory: 67.0480384
 16.507s  j: 126 bis 130
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5088
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 126
Max memory: 0.0639488
lr: 3.7938234199200584
1
Epoche:126/130; Lr: 3.7938234199200584
batch Size 440
Epoch: [126][0/114]	Time 0.209 (0.209)	Data 0.333 (0.333)	Loss 1.3951 (1.3951)	Acc@1 62.500 (62.500)	Acc@5 95.682 (95.682)
Epoch: [126][64/114]	Time 0.139 (0.135)	Data 0.000 (0.005)	Loss 2.2866 (2.2001)	Acc@1 25.000 (34.990)	Acc@5 81.591 (83.038)
Max memory in training epoch: 66.446592
lr: 3.7938234199200584
1
Epoche:127/130; Lr: 3.7938234199200584
batch Size 440
Epoch: [127][0/114]	Time 0.204 (0.204)	Data 0.364 (0.364)	Loss 2.3077 (2.3077)	Acc@1 20.909 (20.909)	Acc@5 73.409 (73.409)
Epoch: [127][64/114]	Time 0.136 (0.139)	Data 0.000 (0.006)	Loss 2.2113 (2.2305)	Acc@1 15.909 (19.685)	Acc@5 74.773 (75.895)
Max memory in training epoch: 66.431744
lr: 3.7938234199200584
1
Epoche:128/130; Lr: 3.7938234199200584
batch Size 440
Epoch: [128][0/114]	Time 0.208 (0.208)	Data 0.462 (0.462)	Loss 2.1865 (2.1865)	Acc@1 16.364 (16.364)	Acc@5 75.455 (75.455)
Epoch: [128][64/114]	Time 0.134 (0.137)	Data 0.000 (0.007)	Loss 2.1478 (2.1336)	Acc@1 18.864 (18.818)	Acc@5 78.409 (76.983)
Max memory in training epoch: 66.431744
Drin!!
old memory: 670480384
new memory: 664317440
Faktor: 0.990808166581649
New batch Size größer 443!!
lr: 3.7938234199200584
1
Epoche:129/130; Lr: 3.7938234199200584
batch Size 443
Epoch: [129][0/114]	Time 0.194 (0.194)	Data 0.461 (0.461)	Loss 2.2747 (2.2747)	Acc@1 14.773 (14.773)	Acc@5 66.136 (66.136)
Epoch: [129][64/114]	Time 0.151 (0.141)	Data 0.000 (0.007)	Loss 1.9774 (2.1176)	Acc@1 21.591 (18.752)	Acc@5 81.818 (76.542)
Max memory in training epoch: 66.431744
lr: 3.7938234199200584
1
Epoche:130/130; Lr: 3.7938234199200584
batch Size 443
Epoch: [130][0/114]	Time 0.226 (0.226)	Data 0.410 (0.410)	Loss 2.0625 (2.0625)	Acc@1 17.955 (17.955)	Acc@5 80.000 (80.000)
Epoch: [130][64/114]	Time 0.126 (0.143)	Data 0.000 (0.007)	Loss 2.0942 (2.0842)	Acc@1 16.136 (19.297)	Acc@5 79.773 (77.990)
Max memory in training epoch: 66.431744
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 138103 ; 143542 ; 0.9621086511264996
[INFO] Storing checkpoint...
  10.0
Max memory: 66.431744
 16.579s  j: 131 bis 135
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8793
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 131
Max memory: 0.0618496
lr: 6.565092871189789
1
Epoche:131/135; Lr: 6.565092871189789
batch Size 443
Epoch: [131][0/113]	Time 0.221 (0.221)	Data 0.498 (0.498)	Loss 2.2044 (2.2044)	Acc@1 18.510 (18.510)	Acc@5 68.849 (68.849)
Epoch: [131][64/113]	Time 0.145 (0.131)	Data 0.000 (0.008)	Loss 2.6705 (2.8929)	Acc@1 11.061 (11.436)	Acc@5 45.824 (53.291)
Max memory in training epoch: 66.3824896
lr: 6.565092871189789
1
Epoche:132/135; Lr: 6.565092871189789
batch Size 443
Epoch: [132][0/113]	Time 0.236 (0.236)	Data 0.432 (0.432)	Loss 2.5639 (2.5639)	Acc@1 10.384 (10.384)	Acc@5 51.467 (51.467)
Epoch: [132][64/113]	Time 0.151 (0.142)	Data 0.000 (0.007)	Loss 7745290.5000 (4291440.3028)	Acc@1 10.158 (9.953)	Acc@5 49.661 (50.443)
Max memory in training epoch: 66.3824896
lr: 6.565092871189789
1
Epoche:133/135; Lr: 6.565092871189789
batch Size 443
Epoch: [133][0/113]	Time 0.187 (0.187)	Data 0.463 (0.463)	Loss 30486.7754 (30486.7754)	Acc@1 11.512 (11.512)	Acc@5 52.370 (52.370)
Epoch: [133][64/113]	Time 0.135 (0.141)	Data 0.000 (0.007)	Loss 943.2486 (6744.3065)	Acc@1 9.029 (9.880)	Acc@5 48.984 (50.182)
Max memory in training epoch: 66.3824896
Drin!!
old memory: 664317440
new memory: 663824896
Faktor: 0.9992585713239742
New batch Size größer 447!!
lr: 6.565092871189789
1
Epoche:134/135; Lr: 6.565092871189789
batch Size 447
Epoch: [134][0/113]	Time 0.185 (0.185)	Data 0.414 (0.414)	Loss 86.3752 (86.3752)	Acc@1 11.738 (11.738)	Acc@5 51.693 (51.693)
Epoch: [134][64/113]	Time 0.129 (0.143)	Data 0.000 (0.007)	Loss 12.2688 (24.8779)	Acc@1 12.641 (11.551)	Acc@5 55.530 (53.127)
Max memory in training epoch: 66.3824896
lr: 6.565092871189789
1
Epoche:135/135; Lr: 6.565092871189789
batch Size 447
Epoch: [135][0/113]	Time 0.213 (0.213)	Data 0.442 (0.442)	Loss 4.6418 (4.6418)	Acc@1 14.673 (14.673)	Acc@5 59.368 (59.368)
Epoch: [135][64/113]	Time 0.156 (0.141)	Data 0.000 (0.007)	Loss 3.5515 (3.2194)	Acc@1 9.932 (13.582)	Acc@5 52.596 (58.670)
Max memory in training epoch: 66.3824896
[INFO] Force the sparse filters to zero...
[INFO] Squeezing the sparse model to dense one...
numoFStages: 3
Count: 136092 ; 138103 ; 0.9854384046689789
[INFO] Storing checkpoint...
  10.0
Max memory: 66.3824896
 16.392s  j: 136 bis 140
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4748
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.060928
lr: 11.463267630554045
1
Epoche:136/140; Lr: 11.463267630554045
batch Size 447
Epoch: [136][0/112]	Time 0.216 (0.216)	Data 0.422 (0.422)	Loss 2.9657 (2.9657)	Acc@1 10.738 (10.738)	Acc@5 48.322 (48.322)
Epoch: [136][64/112]	Time 0.134 (0.135)	Data 0.000 (0.007)	Loss 4121.7500 (747306.7959)	Acc@1 6.711 (9.909)	Acc@5 47.204 (50.318)
Max memory in training epoch: 66.649856
lr: 11.463267630554045
1
Epoche:137/140; Lr: 11.463267630554045
batch Size 447
Epoch: [137][0/112]	Time 0.233 (0.233)	Data 0.372 (0.372)	Loss 118527.8672 (118527.8672)	Acc@1 11.186 (11.186)	Acc@5 55.705 (55.705)
Epoch: [137][64/112]	Time 0.155 (0.146)	Data 0.000 (0.006)	Loss 20197264.0000 (7204503.8012)	Acc@1 14.094 (10.346)	Acc@5 52.796 (51.172)
Max memory in training epoch: 66.4283648
lr: 11.463267630554045
1
Epoche:138/140; Lr: 11.463267630554045
batch Size 447
Epoch: [138][0/112]	Time 0.191 (0.191)	Data 0.472 (0.472)	Loss 1203622.1250 (1203622.1250)	Acc@1 5.369 (5.369)	Acc@5 53.691 (53.691)
Epoch: [138][64/112]	Time 0.152 (0.139)	Data 0.000 (0.007)	Loss 7139740.0000 (1753876.4712)	Acc@1 8.949 (10.112)	Acc@5 48.322 (51.220)
Max memory in training epoch: 66.4283648
Drin!!
old memory: 663824896
new memory: 664283648
Faktor: 1.0006910738099224
New batch Size kleiner 447!!
lr: 11.463267630554045
1
Epoche:139/140; Lr: 11.463267630554045
batch Size 447
Epoch: [139][0/112]	Time 0.188 (0.188)	Data 0.425 (0.425)	Loss 89825632.0000 (89825632.0000)	Acc@1 9.620 (9.620)	Acc@5 47.875 (47.875)
Epoch: [139][64/112]	Time 0.132 (0.144)	Data 0.000 (0.007)	Loss 62753906688.0000 (26435508508.7154)	Acc@1 12.752 (10.339)	Acc@5 49.217 (50.194)
Max memory in training epoch: 66.4283648
lr: 11.463267630554045
1
Epoche:140/140; Lr: 11.463267630554045
batch Size 447
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [140][0/112]	Time 0.192 (0.192)	Data 0.385 (0.385)	Loss 944840704.0000 (944840704.0000)	Acc@1 9.620 (9.620)	Acc@5 50.112 (50.112)
Epoch: [140][64/112]	Time 0.152 (0.144)	Data 0.000 (0.006)	Loss inf (inf)	Acc@1 10.515 (10.532)	Acc@5 55.034 (50.005)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 141 bis 145
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5384
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.060928
lr: 11.463267630554045
1
Epoche:136/140; Lr: 11.463267630554045
batch Size 447
Epoch: [136][0/112]	Time 0.281 (0.281)	Data 0.391 (0.391)	Loss 3.0407 (3.0407)	Acc@1 11.186 (11.186)	Acc@5 53.691 (53.691)
Epoch: [136][64/112]	Time 0.136 (0.141)	Data 0.000 (0.006)	Loss 14570337.0000 (872175.1992)	Acc@1 10.067 (10.487)	Acc@5 48.322 (51.895)
Max memory in training epoch: 66.649856
lr: 11.463267630554045
1
Epoche:137/140; Lr: 11.463267630554045
batch Size 447
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [137][0/112]	Time 0.202 (0.202)	Data 0.420 (0.420)	Loss 67448.7031 (67448.7031)	Acc@1 9.843 (9.843)	Acc@5 53.691 (53.691)
Epoch: [137][64/112]	Time 0.151 (0.140)	Data 0.000 (0.007)	Loss 4927314944.0000 (16539725004640052.0000)	Acc@1 8.501 (9.888)	Acc@5 49.217 (50.515)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 146 bis 150
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9245
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.060928
lr: 11.463267630554045
1
Epoche:136/140; Lr: 11.463267630554045
batch Size 447
Epoch: [136][0/112]	Time 0.247 (0.247)	Data 0.421 (0.421)	Loss 2.9941 (2.9941)	Acc@1 11.186 (11.186)	Acc@5 51.230 (51.230)
Epoch: [136][64/112]	Time 0.119 (0.135)	Data 0.000 (0.007)	Loss 300.7858 (9913.7701)	Acc@1 10.962 (10.807)	Acc@5 50.112 (50.835)
Max memory in training epoch: 66.649856
lr: 11.463267630554045
1
Epoche:137/140; Lr: 11.463267630554045
batch Size 447
Epoch: [137][0/112]	Time 0.174 (0.174)	Data 0.431 (0.431)	Loss 5470.9551 (5470.9551)	Acc@1 9.172 (9.172)	Acc@5 50.559 (50.559)
Epoch: [137][64/112]	Time 0.136 (0.140)	Data 0.000 (0.007)	Loss 1304720048128.0000 (1119111766657.8767)	Acc@1 12.528 (10.243)	Acc@5 56.376 (51.206)
Max memory in training epoch: 66.4283648
lr: 11.463267630554045
1
Epoche:138/140; Lr: 11.463267630554045
batch Size 447
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [138][0/112]	Time 0.193 (0.193)	Data 0.425 (0.425)	Loss 66109361356800.0000 (66109361356800.0000)	Acc@1 8.277 (8.277)	Acc@5 52.349 (52.349)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 151 bis 155
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 4039
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.060928
lr: 11.463267630554045
1
Epoche:136/140; Lr: 11.463267630554045
batch Size 447
Epoch: [136][0/112]	Time 0.247 (0.247)	Data 0.380 (0.380)	Loss 2.9817 (2.9817)	Acc@1 9.396 (9.396)	Acc@5 51.454 (51.454)
Epoch: [136][64/112]	Time 0.117 (0.139)	Data 0.000 (0.006)	Loss 5982679.5000 (4790883.6653)	Acc@1 8.277 (10.146)	Acc@5 45.861 (50.835)
Max memory in training epoch: 66.649856
lr: 11.463267630554045
1
Epoche:137/140; Lr: 11.463267630554045
batch Size 447
Epoch: [137][0/112]	Time 0.187 (0.187)	Data 0.333 (0.333)	Loss 300294.1875 (300294.1875)	Acc@1 14.318 (14.318)	Acc@5 53.468 (53.468)
Epoch: [137][64/112]	Time 0.136 (0.133)	Data 0.000 (0.005)	Loss 62615112.0000 (113831382807.8611)	Acc@1 12.304 (10.546)	Acc@5 51.454 (50.962)
Max memory in training epoch: 66.4283648
lr: 11.463267630554045
1
Epoche:138/140; Lr: 11.463267630554045
batch Size 447
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [138][0/112]	Time 0.215 (0.215)	Data 0.428 (0.428)	Loss 303940435968.0000 (303940435968.0000)	Acc@1 8.501 (8.501)	Acc@5 48.770 (48.770)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 156 bis 160
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5271
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.060928
lr: 11.463267630554045
1
Epoche:136/140; Lr: 11.463267630554045
batch Size 447
Epoch: [136][0/112]	Time 0.243 (0.243)	Data 0.479 (0.479)	Loss 2.9723 (2.9723)	Acc@1 8.501 (8.501)	Acc@5 51.678 (51.678)
Epoch: [136][64/112]	Time 0.137 (0.140)	Data 0.000 (0.008)	Loss 987351.3750 (4091476.2732)	Acc@1 10.291 (10.597)	Acc@5 53.020 (50.762)
Max memory in training epoch: 66.649856
lr: 11.463267630554045
1
Epoche:137/140; Lr: 11.463267630554045
batch Size 447
Epoch: [137][0/112]	Time 0.192 (0.192)	Data 0.424 (0.424)	Loss 3374511.5000 (3374511.5000)	Acc@1 9.396 (9.396)	Acc@5 46.309 (46.309)
Epoch: [137][64/112]	Time 0.137 (0.137)	Data 0.000 (0.007)	Loss 825420939264.0000 (10186120165195920.0000)	Acc@1 8.501 (10.191)	Acc@5 48.098 (50.133)
Max memory in training epoch: 66.4283648
lr: 11.463267630554045
1
Epoche:138/140; Lr: 11.463267630554045
batch Size 447
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [138][0/112]	Time 0.251 (0.251)	Data 0.402 (0.402)	Loss 123813824036864.0000 (123813824036864.0000)	Acc@1 14.989 (14.989)	Acc@5 55.705 (55.705)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 161 bis 165
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 2225
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.060928
lr: 11.463267630554045
1
Epoche:136/140; Lr: 11.463267630554045
batch Size 447
Epoch: [136][0/112]	Time 0.236 (0.236)	Data 0.474 (0.474)	Loss 3.0937 (3.0937)	Acc@1 8.277 (8.277)	Acc@5 45.190 (45.190)
Epoch: [136][64/112]	Time 0.190 (0.142)	Data 0.000 (0.008)	Loss 12.5911 (37.4303)	Acc@1 10.067 (10.738)	Acc@5 49.441 (51.547)
Max memory in training epoch: 66.649856
lr: 11.463267630554045
1
Epoche:137/140; Lr: 11.463267630554045
batch Size 447
Epoch: [137][0/112]	Time 0.190 (0.190)	Data 0.379 (0.379)	Loss 50.4660 (50.4660)	Acc@1 8.277 (8.277)	Acc@5 50.112 (50.112)
Epoch: [137][64/112]	Time 0.116 (0.135)	Data 0.000 (0.006)	Loss 312.1864 (436.7118)	Acc@1 11.409 (10.590)	Acc@5 50.559 (50.415)
Max memory in training epoch: 66.4283648
lr: 11.463267630554045
1
Epoche:138/140; Lr: 11.463267630554045
batch Size 447
Epoch: [138][0/112]	Time 0.233 (0.233)	Data 0.387 (0.387)	Loss 636892.0000 (636892.0000)	Acc@1 10.962 (10.962)	Acc@5 54.586 (54.586)
Epoch: [138][64/112]	Time 0.131 (0.143)	Data 0.000 (0.006)	Loss 1889155547136.0000 (30148974914.8996)	Acc@1 13.199 (10.597)	Acc@5 56.600 (50.167)
Max memory in training epoch: 66.4283648
Drin!!
old memory: 663824896
new memory: 664283648
Faktor: 1.0006910738099224
New batch Size kleiner 447!!
lr: 11.463267630554045
1
Epoche:139/140; Lr: 11.463267630554045
batch Size 447
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [139][0/112]	Time 0.203 (0.203)	Data 0.440 (0.440)	Loss 228263033044992.0000 (228263033044992.0000)	Acc@1 9.620 (9.620)	Acc@5 47.875 (47.875)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 166 bis 170
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 9522
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.060928
lr: 11.463267630554045
1
Epoche:136/140; Lr: 11.463267630554045
batch Size 447
Epoch: [136][0/112]	Time 0.267 (0.267)	Data 0.430 (0.430)	Loss 2.9590 (2.9590)	Acc@1 11.186 (11.186)	Acc@5 53.468 (53.468)
Epoch: [136][64/112]	Time 0.140 (0.138)	Data 0.000 (0.007)	Loss 696.1818 (1861.2842)	Acc@1 9.396 (10.969)	Acc@5 48.098 (51.433)
Max memory in training epoch: 66.649856
lr: 11.463267630554045
1
Epoche:137/140; Lr: 11.463267630554045
batch Size 447
Epoch: [137][0/112]	Time 0.207 (0.207)	Data 0.436 (0.436)	Loss 37.1521 (37.1521)	Acc@1 10.738 (10.738)	Acc@5 52.573 (52.573)
Epoch: [137][64/112]	Time 0.131 (0.141)	Data 0.000 (0.007)	Loss 4061.7439 (3183.8609)	Acc@1 7.830 (9.806)	Acc@5 49.441 (50.139)
Max memory in training epoch: 66.4283648
lr: 11.463267630554045
1
Epoche:138/140; Lr: 11.463267630554045
batch Size 447
Epoch: [138][0/112]	Time 0.199 (0.199)	Data 0.427 (0.427)	Loss 2094.8359 (2094.8359)	Acc@1 10.515 (10.515)	Acc@5 52.796 (52.796)
Epoch: [138][64/112]	Time 0.142 (0.140)	Data 0.000 (0.007)	Loss 11362.5684 (67482.6959)	Acc@1 13.647 (10.418)	Acc@5 55.034 (51.557)
Max memory in training epoch: 66.4283648
Drin!!
old memory: 663824896
new memory: 664283648
Faktor: 1.0006910738099224
New batch Size kleiner 447!!
lr: 11.463267630554045
1
Epoche:139/140; Lr: 11.463267630554045
batch Size 447
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [139][0/112]	Time 0.215 (0.215)	Data 0.416 (0.416)	Loss 1105520492544.0000 (1105520492544.0000)	Acc@1 11.633 (11.633)	Acc@5 49.217 (49.217)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 171 bis 175
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 8000
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.060928
lr: 11.463267630554045
1
Epoche:136/140; Lr: 11.463267630554045
batch Size 447
Epoch: [136][0/112]	Time 0.260 (0.260)	Data 0.479 (0.479)	Loss 2.9885 (2.9885)	Acc@1 11.633 (11.633)	Acc@5 51.007 (51.007)
Epoch: [136][64/112]	Time 0.135 (0.144)	Data 0.000 (0.008)	Loss 23.6613 (39.6052)	Acc@1 12.752 (10.425)	Acc@5 55.705 (52.156)
Max memory in training epoch: 66.649856
lr: 11.463267630554045
1
Epoche:137/140; Lr: 11.463267630554045
batch Size 447
Epoch: [137][0/112]	Time 0.262 (0.262)	Data 0.378 (0.378)	Loss 3624649.0000 (3624649.0000)	Acc@1 14.541 (14.541)	Acc@5 54.139 (54.139)
Epoch: [137][64/112]	Time 0.149 (0.143)	Data 0.000 (0.006)	Loss 192652224.0000 (101326231.7202)	Acc@1 9.396 (10.442)	Acc@5 49.888 (50.208)
Max memory in training epoch: 66.4283648
lr: 11.463267630554045
1
Epoche:138/140; Lr: 11.463267630554045
batch Size 447
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [138][0/112]	Time 0.165 (0.165)	Data 0.427 (0.427)	Loss 453223.6250 (453223.6250)	Acc@1 11.409 (11.409)	Acc@5 50.559 (50.559)
Epoch: [138][64/112]	Time 0.122 (0.143)	Data 0.000 (0.007)	Loss 12783633.0000 (1414062073.4072)	Acc@1 9.620 (10.941)	Acc@5 49.888 (50.635)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
j: 176 bis 180
no display found. Using non-interactive Agg backend
[5, 5, 5]
random number: 5877
Files already downloaded and verified
==> Resuming from checkpoint..
Startepoche: 136
Max memory: 0.060928
lr: 11.463267630554045
1
Epoche:136/140; Lr: 11.463267630554045
batch Size 447
Epoch: [136][0/112]	Time 0.241 (0.241)	Data 0.414 (0.414)	Loss 3.0285 (3.0285)	Acc@1 8.054 (8.054)	Acc@5 48.546 (48.546)
Epoch: [136][64/112]	Time 0.147 (0.141)	Data 0.000 (0.007)	Loss 15041.9619 (70360.7702)	Acc@1 9.843 (9.816)	Acc@5 47.427 (50.749)
Max memory in training epoch: 66.649856
lr: 11.463267630554045
1
Epoche:137/140; Lr: 11.463267630554045
batch Size 447
Epoch: [137][0/112]	Time 0.170 (0.170)	Data 0.400 (0.400)	Loss 146317.2500 (146317.2500)	Acc@1 8.501 (8.501)	Acc@5 51.454 (51.454)
Epoch: [137][64/112]	Time 0.140 (0.141)	Data 0.000 (0.006)	Loss 283608128.0000 (77510883.4639)	Acc@1 8.054 (10.277)	Acc@5 43.400 (50.325)
Max memory in training epoch: 66.4283648
lr: 11.463267630554045
1
Epoche:138/140; Lr: 11.463267630554045
batch Size 447
Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 680, in train
    loss = criterion(outputs, targets)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 932, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 2317, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/nn/functional.py", line 1535, in log_softmax
    ret = input.log_softmax(dim)
 (print_stack at /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:60)
Epoch: [138][0/112]	Time 0.220 (0.220)	Data 0.448 (0.448)	Loss 38458040.0000 (38458040.0000)	Acc@1 9.620 (9.620)	Acc@5 51.678 (51.678)
Epoch: [138][64/112]	Time 0.153 (0.136)	Data 0.000 (0.007)	Loss 45637701730304.0000 (14313704185585.3086)	Acc@1 8.949 (10.408)	Acc@5 48.770 (50.239)
Traceback (most recent call last):
  File "main.py", line 964, in <module>
    main()
  File "main.py", line 436, in main
    optimizer, epoch, use_cuda)
  File "main.py", line 737, in train
    loss.backward()
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/tensor.py", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/autograd/__init__.py", line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'LogSoftmaxBackward' returned nan values in its 0th output.
