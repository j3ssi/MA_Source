no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 2;widthofFirstLayer: 16; Epochen: 180; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/wider2/model.nn; checkpoint: ./output/experimente4/wider2; saveModell: True; LR: 0.1
random number: 9243
Files already downloaded and verified
width: 8

Arch Num:  [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
conv0: Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0
bn1: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
Relu; i: 2
i : 2; block: 0
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 2; block: 0
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 2
i : 3; block: 1
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 3; block: 1
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 3
i : 4; block: 2
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 4; block: 2
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 4
i : 5; block: 3
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 5; block: 3
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 5
i : 6; block: 4
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 6; block: 4
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 6
i : 7; block: 0
Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 7; block: 0
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
i : 7; block: 0
Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 2 if 2
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 2
seq: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 7
seq1: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 8
i : 8; block: 1
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 8; block: 1
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 8
i : 9; block: 2
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 9; block: 2
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 9
i : 10; block: 3
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 10; block: 3
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 10
i : 11; block: 4
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 11; block: 4
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 11
i : 12; block: 0
Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 12; block: 0
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
i : 12; block: 0
Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 2 if 2
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 2
seq: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 12
seq1: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 13
i : 13; block: 1
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 13; block: 1
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 13
i : 14; block: 2
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 14; block: 2
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 14
i : 15; block: 3
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 15; block: 3
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 15
i : 16; block: 4
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 16; block: 4
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 16
avgpoll: AdaptiveAvgPool2d(output_size=(1, 1))
linear: Linear(in_features=32, out_features=10, bias=True)
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 1
Startepoche: 1
deeper epoch: 0
Epoche: [1/180]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [1][0/196]	Time 0.281 (0.281)	Data 0.304 (0.304)	Loss 2.7269 (2.7269)	Acc@1 10.156 (10.156)	Acc@5 48.438 (48.438)
Epoch: [1][64/196]	Time 0.281 (0.272)	Data 0.000 (0.005)	Loss 1.8462 (2.0562)	Acc@1 23.047 (22.115)	Acc@5 87.500 (75.433)
Epoch: [1][128/196]	Time 0.299 (0.271)	Data 0.000 (0.003)	Loss 1.6452 (1.9001)	Acc@1 39.062 (27.589)	Acc@5 89.844 (81.386)
Epoch: [1][192/196]	Time 0.253 (0.260)	Data 0.000 (0.002)	Loss 1.5864 (1.8165)	Acc@1 44.141 (30.845)	Acc@5 89.062 (84.039)
after train
Epoche: [2/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [2][0/196]	Time 0.340 (0.340)	Data 0.379 (0.379)	Loss 1.4991 (1.4991)	Acc@1 42.188 (42.188)	Acc@5 91.797 (91.797)
Epoch: [2][64/196]	Time 0.275 (0.281)	Data 0.000 (0.006)	Loss 1.5209 (1.5676)	Acc@1 44.922 (41.010)	Acc@5 90.625 (90.643)
Epoch: [2][128/196]	Time 0.413 (0.341)	Data 0.000 (0.003)	Loss 1.3383 (1.5213)	Acc@1 50.781 (43.284)	Acc@5 91.406 (91.385)
Epoch: [2][192/196]	Time 0.577 (0.369)	Data 0.000 (0.002)	Loss 1.4373 (1.4784)	Acc@1 47.266 (45.116)	Acc@5 92.578 (92.060)
after train
Epoche: [3/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [3][0/196]	Time 0.574 (0.574)	Data 0.381 (0.381)	Loss 1.3321 (1.3321)	Acc@1 48.438 (48.438)	Acc@5 94.922 (94.922)
Epoch: [3][64/196]	Time 0.570 (0.577)	Data 0.000 (0.006)	Loss 1.3656 (1.3007)	Acc@1 49.219 (52.518)	Acc@5 94.141 (94.171)
Epoch: [3][128/196]	Time 0.584 (0.571)	Data 0.000 (0.003)	Loss 1.0837 (1.2490)	Acc@1 61.328 (54.554)	Acc@5 96.484 (94.777)
Epoch: [3][192/196]	Time 0.550 (0.565)	Data 0.000 (0.002)	Loss 1.2245 (1.2181)	Acc@1 53.906 (55.748)	Acc@5 93.359 (95.136)
after train
Epoche: [4/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [4][0/196]	Time 0.666 (0.666)	Data 0.307 (0.307)	Loss 0.9931 (0.9931)	Acc@1 67.188 (67.188)	Acc@5 95.312 (95.312)
Epoch: [4][64/196]	Time 0.695 (0.587)	Data 0.000 (0.005)	Loss 0.9953 (1.0590)	Acc@1 63.672 (62.037)	Acc@5 96.484 (96.478)
Epoch: [4][128/196]	Time 0.718 (0.649)	Data 0.000 (0.003)	Loss 1.0905 (1.0426)	Acc@1 61.719 (62.497)	Acc@5 95.312 (96.651)
Epoch: [4][192/196]	Time 0.693 (0.667)	Data 0.000 (0.002)	Loss 1.0137 (1.0205)	Acc@1 64.453 (63.453)	Acc@5 95.312 (96.770)
after train
Epoche: [5/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [5][0/196]	Time 0.850 (0.850)	Data 0.406 (0.406)	Loss 1.1173 (1.1173)	Acc@1 60.547 (60.547)	Acc@5 94.141 (94.141)
Epoch: [5][64/196]	Time 0.541 (0.712)	Data 0.000 (0.007)	Loss 0.9882 (0.9425)	Acc@1 65.234 (66.514)	Acc@5 96.094 (97.019)
Epoch: [5][128/196]	Time 0.725 (0.714)	Data 0.000 (0.004)	Loss 0.8463 (0.9220)	Acc@1 69.531 (67.451)	Acc@5 98.047 (97.181)
Epoch: [5][192/196]	Time 0.689 (0.711)	Data 0.000 (0.002)	Loss 0.9880 (0.9006)	Acc@1 64.453 (68.050)	Acc@5 97.656 (97.334)
after train
Epoche: [6/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [6][0/196]	Time 0.846 (0.846)	Data 0.482 (0.482)	Loss 0.8099 (0.8099)	Acc@1 73.047 (73.047)	Acc@5 98.438 (98.438)
Epoch: [6][64/196]	Time 0.675 (0.707)	Data 0.000 (0.008)	Loss 0.7641 (0.8237)	Acc@1 74.219 (71.370)	Acc@5 97.266 (97.794)
Epoch: [6][128/196]	Time 0.691 (0.709)	Data 0.000 (0.004)	Loss 0.7464 (0.8204)	Acc@1 73.047 (71.427)	Acc@5 98.828 (97.705)
Epoch: [6][192/196]	Time 0.711 (0.703)	Data 0.000 (0.003)	Loss 0.8033 (0.8085)	Acc@1 69.922 (71.717)	Acc@5 98.047 (97.826)
after train
Epoche: [7/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [7][0/196]	Time 0.793 (0.793)	Data 0.337 (0.337)	Loss 0.7095 (0.7095)	Acc@1 76.953 (76.953)	Acc@5 99.219 (99.219)
Epoch: [7][64/196]	Time 0.633 (0.708)	Data 0.000 (0.006)	Loss 0.8717 (0.7653)	Acc@1 67.188 (73.323)	Acc@5 98.828 (98.143)
Epoch: [7][128/196]	Time 0.675 (0.712)	Data 0.000 (0.003)	Loss 0.7495 (0.7544)	Acc@1 73.828 (73.652)	Acc@5 99.219 (98.198)
Epoch: [7][192/196]	Time 0.688 (0.709)	Data 0.000 (0.002)	Loss 0.7583 (0.7516)	Acc@1 73.047 (73.662)	Acc@5 97.656 (98.231)
after train
Epoche: [8/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [8][0/196]	Time 0.780 (0.780)	Data 0.372 (0.372)	Loss 0.6680 (0.6680)	Acc@1 77.734 (77.734)	Acc@5 97.266 (97.266)
Epoch: [8][64/196]	Time 0.720 (0.716)	Data 0.000 (0.006)	Loss 0.6252 (0.7082)	Acc@1 80.469 (75.024)	Acc@5 99.609 (98.444)
Epoch: [8][128/196]	Time 0.723 (0.711)	Data 0.000 (0.003)	Loss 0.7660 (0.7102)	Acc@1 73.438 (75.142)	Acc@5 97.656 (98.344)
Epoch: [8][192/196]	Time 0.740 (0.709)	Data 0.000 (0.002)	Loss 0.6536 (0.7051)	Acc@1 75.781 (75.437)	Acc@5 98.438 (98.389)
after train
Epoche: [9/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [9][0/196]	Time 0.786 (0.786)	Data 0.443 (0.443)	Loss 0.8269 (0.8269)	Acc@1 70.703 (70.703)	Acc@5 96.875 (96.875)
Epoch: [9][64/196]	Time 0.666 (0.705)	Data 0.000 (0.007)	Loss 0.5982 (0.6814)	Acc@1 80.859 (76.466)	Acc@5 98.828 (98.425)
Epoch: [9][128/196]	Time 0.697 (0.708)	Data 0.000 (0.004)	Loss 0.7410 (0.6832)	Acc@1 75.781 (76.248)	Acc@5 98.047 (98.504)
Epoch: [9][192/196]	Time 0.738 (0.705)	Data 0.000 (0.003)	Loss 0.5837 (0.6772)	Acc@1 78.516 (76.510)	Acc@5 97.656 (98.553)
after train
Epoche: [10/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [10][0/196]	Time 0.813 (0.813)	Data 0.349 (0.349)	Loss 0.6584 (0.6584)	Acc@1 77.344 (77.344)	Acc@5 97.266 (97.266)
Epoch: [10][64/196]	Time 0.652 (0.708)	Data 0.000 (0.006)	Loss 0.6725 (0.6632)	Acc@1 77.344 (76.761)	Acc@5 98.828 (98.516)
Epoch: [10][128/196]	Time 0.706 (0.710)	Data 0.000 (0.003)	Loss 0.6391 (0.6566)	Acc@1 77.734 (76.977)	Acc@5 98.047 (98.604)
Epoch: [10][192/196]	Time 0.695 (0.709)	Data 0.000 (0.002)	Loss 0.6150 (0.6536)	Acc@1 78.516 (77.184)	Acc@5 98.438 (98.603)
after train
Epoche: [11/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [11][0/196]	Time 0.755 (0.755)	Data 0.385 (0.385)	Loss 0.6116 (0.6116)	Acc@1 77.734 (77.734)	Acc@5 98.438 (98.438)
Epoch: [11][64/196]	Time 0.544 (0.702)	Data 0.000 (0.006)	Loss 0.6973 (0.6292)	Acc@1 75.391 (78.041)	Acc@5 98.438 (98.654)
Epoch: [11][128/196]	Time 0.682 (0.709)	Data 0.000 (0.003)	Loss 0.6912 (0.6385)	Acc@1 76.953 (77.855)	Acc@5 97.656 (98.622)
Epoch: [11][192/196]	Time 0.724 (0.710)	Data 0.000 (0.002)	Loss 0.6250 (0.6366)	Acc@1 77.734 (77.989)	Acc@5 98.438 (98.650)
after train
Epoche: [12/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [12][0/196]	Time 0.791 (0.791)	Data 0.472 (0.472)	Loss 0.6048 (0.6048)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [12][64/196]	Time 0.595 (0.707)	Data 0.000 (0.008)	Loss 0.6826 (0.6244)	Acc@1 75.781 (78.401)	Acc@5 98.047 (98.774)
Epoch: [12][128/196]	Time 0.651 (0.709)	Data 0.000 (0.004)	Loss 0.5817 (0.6199)	Acc@1 78.906 (78.688)	Acc@5 99.219 (98.777)
Epoch: [12][192/196]	Time 0.651 (0.707)	Data 0.000 (0.003)	Loss 0.6491 (0.6197)	Acc@1 76.953 (78.740)	Acc@5 98.828 (98.763)
after train
Epoche: [13/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [13][0/196]	Time 0.644 (0.644)	Data 0.410 (0.410)	Loss 0.5919 (0.5919)	Acc@1 79.297 (79.297)	Acc@5 99.609 (99.609)
Epoch: [13][64/196]	Time 0.643 (0.704)	Data 0.000 (0.007)	Loss 0.7096 (0.5999)	Acc@1 77.734 (79.279)	Acc@5 99.609 (98.858)
Epoch: [13][128/196]	Time 0.719 (0.702)	Data 0.000 (0.003)	Loss 0.6945 (0.6033)	Acc@1 75.391 (79.088)	Acc@5 98.047 (98.831)
Epoch: [13][192/196]	Time 0.701 (0.702)	Data 0.000 (0.002)	Loss 0.6263 (0.6054)	Acc@1 77.734 (79.030)	Acc@5 97.656 (98.808)
after train
Epoche: [14/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [14][0/196]	Time 0.831 (0.831)	Data 0.423 (0.423)	Loss 0.5956 (0.5956)	Acc@1 79.297 (79.297)	Acc@5 98.828 (98.828)
Epoch: [14][64/196]	Time 0.656 (0.710)	Data 0.000 (0.007)	Loss 0.4785 (0.5856)	Acc@1 82.422 (79.706)	Acc@5 100.000 (98.870)
Epoch: [14][128/196]	Time 0.690 (0.710)	Data 0.000 (0.004)	Loss 0.6818 (0.5966)	Acc@1 78.516 (79.379)	Acc@5 98.047 (98.813)
Epoch: [14][192/196]	Time 0.787 (0.708)	Data 0.000 (0.003)	Loss 0.6447 (0.5937)	Acc@1 74.219 (79.370)	Acc@5 98.828 (98.875)
after train
Epoche: [15/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [15][0/196]	Time 0.829 (0.829)	Data 0.407 (0.407)	Loss 0.6552 (0.6552)	Acc@1 77.734 (77.734)	Acc@5 96.875 (96.875)
Epoch: [15][64/196]	Time 0.617 (0.713)	Data 0.000 (0.007)	Loss 0.5597 (0.5727)	Acc@1 79.688 (80.078)	Acc@5 99.219 (98.954)
Epoch: [15][128/196]	Time 0.724 (0.704)	Data 0.000 (0.004)	Loss 0.6159 (0.5705)	Acc@1 77.344 (80.245)	Acc@5 98.828 (98.937)
Epoch: [15][192/196]	Time 0.729 (0.704)	Data 0.000 (0.002)	Loss 0.5628 (0.5754)	Acc@1 81.250 (80.058)	Acc@5 99.219 (98.935)
after train
Epoche: [16/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [16][0/196]	Time 0.744 (0.744)	Data 0.341 (0.341)	Loss 0.5752 (0.5752)	Acc@1 79.297 (79.297)	Acc@5 99.219 (99.219)
Epoch: [16][64/196]	Time 0.628 (0.701)	Data 0.000 (0.006)	Loss 0.5585 (0.5783)	Acc@1 81.641 (80.228)	Acc@5 98.438 (98.852)
Epoch: [16][128/196]	Time 0.698 (0.707)	Data 0.000 (0.003)	Loss 0.6301 (0.5732)	Acc@1 79.297 (80.181)	Acc@5 98.828 (98.858)
Epoch: [16][192/196]	Time 0.684 (0.704)	Data 0.000 (0.002)	Loss 0.6142 (0.5701)	Acc@1 77.344 (80.343)	Acc@5 98.828 (98.856)
after train
Epoche: [17/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [17][0/196]	Time 0.827 (0.827)	Data 0.469 (0.469)	Loss 0.4363 (0.4363)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [17][64/196]	Time 0.741 (0.716)	Data 0.000 (0.008)	Loss 0.6633 (0.5593)	Acc@1 79.688 (80.553)	Acc@5 98.438 (99.014)
Epoch: [17][128/196]	Time 0.706 (0.710)	Data 0.000 (0.004)	Loss 0.5580 (0.5625)	Acc@1 79.688 (80.614)	Acc@5 98.438 (99.022)
Epoch: [17][192/196]	Time 0.732 (0.710)	Data 0.000 (0.003)	Loss 0.6349 (0.5609)	Acc@1 79.297 (80.754)	Acc@5 98.047 (98.968)
after train
Epoche: [18/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [18][0/196]	Time 0.855 (0.855)	Data 0.332 (0.332)	Loss 0.5353 (0.5353)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [18][64/196]	Time 0.697 (0.722)	Data 0.000 (0.005)	Loss 0.5294 (0.5625)	Acc@1 83.203 (80.775)	Acc@5 98.828 (98.918)
Epoch: [18][128/196]	Time 0.724 (0.714)	Data 0.000 (0.003)	Loss 0.5937 (0.5602)	Acc@1 80.078 (80.802)	Acc@5 98.047 (98.958)
Epoch: [18][192/196]	Time 0.704 (0.709)	Data 0.000 (0.002)	Loss 0.5314 (0.5584)	Acc@1 83.203 (80.874)	Acc@5 98.828 (98.988)
after train
Epoche: [19/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [19][0/196]	Time 0.788 (0.788)	Data 0.479 (0.479)	Loss 0.4915 (0.4915)	Acc@1 82.031 (82.031)	Acc@5 100.000 (100.000)
Epoch: [19][64/196]	Time 0.664 (0.716)	Data 0.000 (0.008)	Loss 0.5797 (0.5489)	Acc@1 78.125 (81.028)	Acc@5 98.828 (99.020)
Epoch: [19][128/196]	Time 0.762 (0.708)	Data 0.000 (0.004)	Loss 0.6200 (0.5465)	Acc@1 74.609 (81.132)	Acc@5 99.219 (98.989)
Epoch: [19][192/196]	Time 0.747 (0.707)	Data 0.000 (0.003)	Loss 0.5448 (0.5503)	Acc@1 81.641 (81.106)	Acc@5 98.047 (99.002)
after train
Epoche: [20/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [20][0/196]	Time 0.815 (0.815)	Data 0.360 (0.360)	Loss 0.4676 (0.4676)	Acc@1 84.766 (84.766)	Acc@5 98.828 (98.828)
Epoch: [20][64/196]	Time 0.643 (0.720)	Data 0.000 (0.006)	Loss 0.5655 (0.5337)	Acc@1 79.688 (81.581)	Acc@5 99.219 (99.111)
Epoch: [20][128/196]	Time 0.706 (0.714)	Data 0.000 (0.003)	Loss 0.5929 (0.5426)	Acc@1 79.297 (81.241)	Acc@5 100.000 (99.058)
Epoch: [20][192/196]	Time 0.713 (0.710)	Data 0.000 (0.002)	Loss 0.4746 (0.5442)	Acc@1 84.766 (81.112)	Acc@5 99.609 (99.073)
after train
Epoche: [21/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [21][0/196]	Time 0.821 (0.821)	Data 0.324 (0.324)	Loss 0.4262 (0.4262)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [21][64/196]	Time 0.670 (0.715)	Data 0.000 (0.005)	Loss 0.5244 (0.5407)	Acc@1 83.203 (81.623)	Acc@5 99.219 (98.966)
Epoch: [21][128/196]	Time 0.720 (0.715)	Data 0.000 (0.003)	Loss 0.5554 (0.5377)	Acc@1 80.078 (81.553)	Acc@5 99.609 (99.004)
Epoch: [21][192/196]	Time 0.707 (0.712)	Data 0.000 (0.002)	Loss 0.5569 (0.5385)	Acc@1 79.688 (81.543)	Acc@5 99.219 (99.006)
after train
Epoche: [22/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [22][0/196]	Time 0.697 (0.697)	Data 0.364 (0.364)	Loss 0.5265 (0.5265)	Acc@1 80.078 (80.078)	Acc@5 99.219 (99.219)
Epoch: [22][64/196]	Time 0.620 (0.709)	Data 0.000 (0.006)	Loss 0.5333 (0.5325)	Acc@1 82.422 (81.520)	Acc@5 98.047 (98.936)
Epoch: [22][128/196]	Time 0.719 (0.705)	Data 0.000 (0.003)	Loss 0.4317 (0.5240)	Acc@1 84.766 (81.840)	Acc@5 99.609 (99.064)
Epoch: [22][192/196]	Time 0.680 (0.705)	Data 0.000 (0.002)	Loss 0.5993 (0.5307)	Acc@1 80.469 (81.592)	Acc@5 99.609 (99.043)
after train
Epoche: [23/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [23][0/196]	Time 0.776 (0.776)	Data 0.596 (0.596)	Loss 0.4989 (0.4989)	Acc@1 82.031 (82.031)	Acc@5 100.000 (100.000)
Epoch: [23][64/196]	Time 0.663 (0.722)	Data 0.000 (0.009)	Loss 0.5292 (0.5148)	Acc@1 82.422 (82.362)	Acc@5 99.609 (99.141)
Epoch: [23][128/196]	Time 0.690 (0.721)	Data 0.000 (0.005)	Loss 0.5190 (0.5296)	Acc@1 82.031 (81.731)	Acc@5 98.438 (99.067)
Epoch: [23][192/196]	Time 0.733 (0.718)	Data 0.000 (0.003)	Loss 0.4667 (0.5255)	Acc@1 86.719 (81.851)	Acc@5 99.609 (99.087)
after train
Epoche: [24/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [24][0/196]	Time 0.821 (0.821)	Data 0.385 (0.385)	Loss 0.4678 (0.4678)	Acc@1 81.250 (81.250)	Acc@5 99.609 (99.609)
Epoch: [24][64/196]	Time 0.724 (0.713)	Data 0.000 (0.006)	Loss 0.5433 (0.5056)	Acc@1 80.078 (82.428)	Acc@5 99.219 (99.291)
Epoch: [24][128/196]	Time 0.688 (0.707)	Data 0.000 (0.003)	Loss 0.5286 (0.5163)	Acc@1 80.859 (82.204)	Acc@5 98.438 (99.116)
Epoch: [24][192/196]	Time 0.815 (0.707)	Data 0.000 (0.002)	Loss 0.4696 (0.5210)	Acc@1 82.422 (81.956)	Acc@5 98.828 (99.109)
after train
Epoche: [25/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [25][0/196]	Time 0.724 (0.724)	Data 0.413 (0.413)	Loss 0.5613 (0.5613)	Acc@1 80.469 (80.469)	Acc@5 98.047 (98.047)
Epoch: [25][64/196]	Time 0.742 (0.715)	Data 0.000 (0.007)	Loss 0.5351 (0.5130)	Acc@1 80.078 (82.332)	Acc@5 99.219 (99.243)
Epoch: [25][128/196]	Time 0.731 (0.709)	Data 0.000 (0.004)	Loss 0.3572 (0.5131)	Acc@1 88.672 (82.355)	Acc@5 99.609 (99.191)
Epoch: [25][192/196]	Time 0.698 (0.710)	Data 0.000 (0.002)	Loss 0.5307 (0.5183)	Acc@1 79.688 (82.064)	Acc@5 99.609 (99.174)
after train
Epoche: [26/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [26][0/196]	Time 0.824 (0.824)	Data 0.437 (0.437)	Loss 0.5225 (0.5225)	Acc@1 82.422 (82.422)	Acc@5 100.000 (100.000)
Epoch: [26][64/196]	Time 0.578 (0.720)	Data 0.000 (0.007)	Loss 0.4618 (0.5159)	Acc@1 84.766 (82.428)	Acc@5 99.609 (99.225)
Epoch: [26][128/196]	Time 0.658 (0.714)	Data 0.000 (0.004)	Loss 0.5712 (0.5091)	Acc@1 80.859 (82.467)	Acc@5 99.219 (99.222)
Epoch: [26][192/196]	Time 0.781 (0.708)	Data 0.000 (0.003)	Loss 0.5308 (0.5131)	Acc@1 83.203 (82.341)	Acc@5 98.438 (99.166)
after train
Epoche: [27/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [27][0/196]	Time 0.822 (0.822)	Data 0.413 (0.413)	Loss 0.6233 (0.6233)	Acc@1 76.172 (76.172)	Acc@5 99.219 (99.219)
Epoch: [27][64/196]	Time 0.655 (0.717)	Data 0.000 (0.007)	Loss 0.4272 (0.4998)	Acc@1 85.547 (82.446)	Acc@5 99.609 (99.297)
Epoch: [27][128/196]	Time 0.686 (0.714)	Data 0.000 (0.004)	Loss 0.5059 (0.5034)	Acc@1 81.641 (82.640)	Acc@5 100.000 (99.252)
Epoch: [27][192/196]	Time 0.727 (0.711)	Data 0.000 (0.002)	Loss 0.5027 (0.5045)	Acc@1 83.984 (82.638)	Acc@5 99.219 (99.221)
after train
Epoche: [28/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [28][0/196]	Time 0.649 (0.649)	Data 0.456 (0.456)	Loss 0.6173 (0.6173)	Acc@1 80.859 (80.859)	Acc@5 98.438 (98.438)
Epoch: [28][64/196]	Time 0.689 (0.718)	Data 0.000 (0.007)	Loss 0.4598 (0.5089)	Acc@1 84.375 (82.602)	Acc@5 98.828 (99.038)
Epoch: [28][128/196]	Time 0.783 (0.714)	Data 0.000 (0.004)	Loss 0.5032 (0.5049)	Acc@1 82.812 (82.785)	Acc@5 98.828 (99.095)
Epoch: [28][192/196]	Time 0.711 (0.708)	Data 0.000 (0.003)	Loss 0.6355 (0.5064)	Acc@1 79.688 (82.604)	Acc@5 98.828 (99.146)
after train
Epoche: [29/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [29][0/196]	Time 0.898 (0.898)	Data 0.449 (0.449)	Loss 0.5063 (0.5063)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [29][64/196]	Time 0.539 (0.718)	Data 0.000 (0.007)	Loss 0.5296 (0.4954)	Acc@1 80.469 (82.812)	Acc@5 99.609 (99.207)
Epoch: [29][128/196]	Time 0.725 (0.716)	Data 0.000 (0.004)	Loss 0.4902 (0.5083)	Acc@1 80.469 (82.552)	Acc@5 100.000 (99.195)
Epoch: [29][192/196]	Time 0.717 (0.711)	Data 0.000 (0.003)	Loss 0.5264 (0.5075)	Acc@1 83.594 (82.568)	Acc@5 98.438 (99.188)
after train
Epoche: [30/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [30][0/196]	Time 0.671 (0.671)	Data 0.359 (0.359)	Loss 0.4470 (0.4470)	Acc@1 81.641 (81.641)	Acc@5 99.609 (99.609)
Epoch: [30][64/196]	Time 0.590 (0.706)	Data 0.000 (0.006)	Loss 0.5214 (0.4778)	Acc@1 82.812 (83.750)	Acc@5 98.047 (99.309)
Epoch: [30][128/196]	Time 0.784 (0.709)	Data 0.000 (0.003)	Loss 0.4556 (0.4874)	Acc@1 83.984 (83.324)	Acc@5 99.609 (99.255)
Epoch: [30][192/196]	Time 0.697 (0.706)	Data 0.000 (0.002)	Loss 0.4669 (0.4940)	Acc@1 82.422 (83.021)	Acc@5 99.609 (99.227)
after train
Epoche: [31/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [31][0/196]	Time 0.786 (0.786)	Data 0.449 (0.449)	Loss 0.4974 (0.4974)	Acc@1 81.641 (81.641)	Acc@5 100.000 (100.000)
Epoch: [31][64/196]	Time 0.307 (0.712)	Data 0.000 (0.007)	Loss 0.4496 (0.4916)	Acc@1 85.156 (83.083)	Acc@5 98.828 (99.177)
Epoch: [31][128/196]	Time 0.777 (0.707)	Data 0.000 (0.004)	Loss 0.4862 (0.4897)	Acc@1 83.594 (83.055)	Acc@5 99.219 (99.240)
Epoch: [31][192/196]	Time 0.759 (0.705)	Data 0.000 (0.003)	Loss 0.4595 (0.4995)	Acc@1 83.594 (82.709)	Acc@5 99.609 (99.160)
after train
Epoche: [32/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [32][0/196]	Time 0.768 (0.768)	Data 0.347 (0.347)	Loss 0.3611 (0.3611)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [32][64/196]	Time 0.712 (0.721)	Data 0.000 (0.006)	Loss 0.6117 (0.4990)	Acc@1 79.297 (82.993)	Acc@5 97.266 (99.165)
Epoch: [32][128/196]	Time 0.738 (0.709)	Data 0.000 (0.003)	Loss 0.4336 (0.4888)	Acc@1 85.156 (83.197)	Acc@5 98.828 (99.210)
Epoch: [32][192/196]	Time 0.676 (0.707)	Data 0.000 (0.002)	Loss 0.5012 (0.4867)	Acc@1 82.422 (83.254)	Acc@5 100.000 (99.231)
after train
Epoche: [33/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [33][0/196]	Time 0.837 (0.837)	Data 0.439 (0.439)	Loss 0.4883 (0.4883)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [33][64/196]	Time 0.690 (0.719)	Data 0.000 (0.008)	Loss 0.4700 (0.4942)	Acc@1 84.375 (82.506)	Acc@5 98.438 (99.141)
Epoch: [33][128/196]	Time 0.807 (0.711)	Data 0.000 (0.004)	Loss 0.5468 (0.4954)	Acc@1 83.594 (82.649)	Acc@5 99.219 (99.110)
Epoch: [33][192/196]	Time 0.740 (0.700)	Data 0.000 (0.003)	Loss 0.5233 (0.4984)	Acc@1 82.812 (82.598)	Acc@5 98.438 (99.146)
after train
Epoche: [34/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [34][0/196]	Time 0.791 (0.791)	Data 0.348 (0.348)	Loss 0.4348 (0.4348)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [34][64/196]	Time 0.779 (0.716)	Data 0.000 (0.006)	Loss 0.4029 (0.4877)	Acc@1 88.281 (83.329)	Acc@5 99.219 (99.177)
Epoch: [34][128/196]	Time 0.595 (0.706)	Data 0.000 (0.003)	Loss 0.4514 (0.4893)	Acc@1 83.594 (83.249)	Acc@5 99.609 (99.195)
Epoch: [34][192/196]	Time 0.647 (0.702)	Data 0.000 (0.002)	Loss 0.4020 (0.4875)	Acc@1 87.500 (83.286)	Acc@5 99.219 (99.178)
after train
Epoche: [35/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [35][0/196]	Time 0.852 (0.852)	Data 0.523 (0.523)	Loss 0.4397 (0.4397)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [35][64/196]	Time 0.666 (0.724)	Data 0.000 (0.009)	Loss 0.4739 (0.4847)	Acc@1 85.938 (83.444)	Acc@5 99.219 (99.165)
Epoch: [35][128/196]	Time 0.709 (0.718)	Data 0.000 (0.005)	Loss 0.5051 (0.4894)	Acc@1 81.641 (83.306)	Acc@5 99.219 (99.158)
Epoch: [35][192/196]	Time 0.700 (0.709)	Data 0.000 (0.003)	Loss 0.4339 (0.4891)	Acc@1 84.766 (83.199)	Acc@5 99.609 (99.152)
after train
Epoche: [36/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [36][0/196]	Time 0.785 (0.785)	Data 0.441 (0.441)	Loss 0.4800 (0.4800)	Acc@1 84.375 (84.375)	Acc@5 98.828 (98.828)
Epoch: [36][64/196]	Time 0.716 (0.720)	Data 0.000 (0.007)	Loss 0.4422 (0.4673)	Acc@1 85.547 (83.972)	Acc@5 99.219 (99.189)
Epoch: [36][128/196]	Time 0.727 (0.716)	Data 0.000 (0.004)	Loss 0.5268 (0.4812)	Acc@1 81.250 (83.424)	Acc@5 98.047 (99.225)
Epoch: [36][192/196]	Time 0.657 (0.711)	Data 0.000 (0.003)	Loss 0.5894 (0.4832)	Acc@1 82.031 (83.339)	Acc@5 99.609 (99.229)
after train
Epoche: [37/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [37][0/196]	Time 0.793 (0.793)	Data 0.421 (0.421)	Loss 0.4414 (0.4414)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [37][64/196]	Time 0.736 (0.714)	Data 0.000 (0.007)	Loss 0.4462 (0.4925)	Acc@1 85.547 (83.197)	Acc@5 99.219 (99.195)
Epoch: [37][128/196]	Time 0.680 (0.708)	Data 0.000 (0.004)	Loss 0.4483 (0.4802)	Acc@1 85.547 (83.442)	Acc@5 99.609 (99.267)
Epoch: [37][192/196]	Time 0.681 (0.713)	Data 0.000 (0.003)	Loss 0.4422 (0.4782)	Acc@1 82.812 (83.543)	Acc@5 99.609 (99.249)
after train
Epoche: [38/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [38][0/196]	Time 0.701 (0.701)	Data 0.581 (0.581)	Loss 0.4006 (0.4006)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [38][64/196]	Time 0.684 (0.720)	Data 0.000 (0.009)	Loss 0.4344 (0.4618)	Acc@1 82.812 (84.375)	Acc@5 99.219 (99.255)
Epoch: [38][128/196]	Time 0.770 (0.705)	Data 0.000 (0.005)	Loss 0.4105 (0.4770)	Acc@1 87.891 (83.754)	Acc@5 100.000 (99.237)
Epoch: [38][192/196]	Time 0.833 (0.701)	Data 0.000 (0.004)	Loss 0.5131 (0.4796)	Acc@1 82.031 (83.594)	Acc@5 99.219 (99.235)
after train
Epoche: [39/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [39][0/196]	Time 0.790 (0.790)	Data 0.544 (0.544)	Loss 0.4618 (0.4618)	Acc@1 83.203 (83.203)	Acc@5 98.828 (98.828)
Epoch: [39][64/196]	Time 0.692 (0.705)	Data 0.000 (0.009)	Loss 0.4462 (0.4766)	Acc@1 83.203 (83.510)	Acc@5 99.219 (99.261)
Epoch: [39][128/196]	Time 0.705 (0.697)	Data 0.000 (0.005)	Loss 0.4721 (0.4784)	Acc@1 84.375 (83.573)	Acc@5 99.609 (99.216)
Epoch: [39][192/196]	Time 0.556 (0.699)	Data 0.000 (0.003)	Loss 0.5095 (0.4823)	Acc@1 81.641 (83.347)	Acc@5 100.000 (99.207)
after train
Epoche: [40/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [40][0/196]	Time 0.584 (0.584)	Data 0.352 (0.352)	Loss 0.5803 (0.5803)	Acc@1 81.250 (81.250)	Acc@5 97.656 (97.656)
Epoch: [40][64/196]	Time 0.696 (0.728)	Data 0.000 (0.006)	Loss 0.4348 (0.4772)	Acc@1 85.547 (83.534)	Acc@5 99.609 (99.153)
Epoch: [40][128/196]	Time 0.706 (0.715)	Data 0.000 (0.003)	Loss 0.3987 (0.4719)	Acc@1 85.547 (83.712)	Acc@5 99.219 (99.225)
Epoch: [40][192/196]	Time 0.726 (0.713)	Data 0.000 (0.002)	Loss 0.4886 (0.4745)	Acc@1 83.594 (83.567)	Acc@5 98.438 (99.209)
after train
Epoche: [41/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [41][0/196]	Time 0.556 (0.556)	Data 0.407 (0.407)	Loss 0.4706 (0.4706)	Acc@1 83.594 (83.594)	Acc@5 99.609 (99.609)
Epoch: [41][64/196]	Time 0.675 (0.717)	Data 0.000 (0.007)	Loss 0.4607 (0.4777)	Acc@1 83.203 (83.636)	Acc@5 99.609 (99.177)
Epoch: [41][128/196]	Time 0.751 (0.712)	Data 0.000 (0.004)	Loss 0.6096 (0.4784)	Acc@1 79.297 (83.542)	Acc@5 99.219 (99.176)
Epoch: [41][192/196]	Time 0.786 (0.711)	Data 0.000 (0.003)	Loss 0.4817 (0.4802)	Acc@1 84.766 (83.430)	Acc@5 98.828 (99.201)
after train
Epoche: [42/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [42][0/196]	Time 0.773 (0.773)	Data 0.586 (0.586)	Loss 0.4416 (0.4416)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [42][64/196]	Time 0.769 (0.715)	Data 0.000 (0.010)	Loss 0.5642 (0.4741)	Acc@1 78.516 (83.726)	Acc@5 98.438 (99.399)
Epoch: [42][128/196]	Time 0.688 (0.711)	Data 0.000 (0.005)	Loss 0.4034 (0.4687)	Acc@1 84.766 (83.800)	Acc@5 99.609 (99.307)
Epoch: [42][192/196]	Time 0.761 (0.710)	Data 0.000 (0.004)	Loss 0.4356 (0.4719)	Acc@1 86.328 (83.750)	Acc@5 99.609 (99.279)
after train
Epoche: [43/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [43][0/196]	Time 0.674 (0.674)	Data 0.339 (0.339)	Loss 0.3913 (0.3913)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [43][64/196]	Time 0.696 (0.705)	Data 0.000 (0.006)	Loss 0.4352 (0.4623)	Acc@1 86.719 (84.201)	Acc@5 98.438 (99.303)
Epoch: [43][128/196]	Time 0.707 (0.698)	Data 0.000 (0.003)	Loss 0.4486 (0.4693)	Acc@1 82.812 (83.897)	Acc@5 99.609 (99.297)
Epoch: [43][192/196]	Time 0.716 (0.699)	Data 0.000 (0.002)	Loss 0.4420 (0.4658)	Acc@1 83.984 (83.895)	Acc@5 98.828 (99.304)
after train
Epoche: [44/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [44][0/196]	Time 0.849 (0.849)	Data 0.413 (0.413)	Loss 0.5242 (0.5242)	Acc@1 82.422 (82.422)	Acc@5 98.828 (98.828)
Epoch: [44][64/196]	Time 0.715 (0.726)	Data 0.000 (0.007)	Loss 0.4054 (0.4655)	Acc@1 85.156 (83.918)	Acc@5 99.219 (99.375)
Epoch: [44][128/196]	Time 0.726 (0.718)	Data 0.000 (0.004)	Loss 0.5162 (0.4690)	Acc@1 80.859 (83.821)	Acc@5 99.219 (99.301)
Epoch: [44][192/196]	Time 0.668 (0.710)	Data 0.000 (0.003)	Loss 0.5429 (0.4699)	Acc@1 83.594 (83.818)	Acc@5 99.609 (99.320)
after train
Epoche: [45/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [45][0/196]	Time 0.915 (0.915)	Data 0.578 (0.578)	Loss 0.3547 (0.3547)	Acc@1 89.453 (89.453)	Acc@5 99.219 (99.219)
Epoch: [45][64/196]	Time 0.703 (0.717)	Data 0.000 (0.010)	Loss 0.4638 (0.4482)	Acc@1 83.203 (84.333)	Acc@5 99.609 (99.351)
Epoch: [45][128/196]	Time 0.716 (0.709)	Data 0.000 (0.005)	Loss 0.5368 (0.4600)	Acc@1 81.250 (84.163)	Acc@5 99.219 (99.255)
Epoch: [45][192/196]	Time 0.698 (0.702)	Data 0.000 (0.004)	Loss 0.4264 (0.4636)	Acc@1 85.938 (83.976)	Acc@5 99.609 (99.263)
after train
Epoche: [46/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [46][0/196]	Time 0.669 (0.669)	Data 0.482 (0.482)	Loss 0.5039 (0.5039)	Acc@1 84.766 (84.766)	Acc@5 98.438 (98.438)
Epoch: [46][64/196]	Time 0.780 (0.716)	Data 0.000 (0.008)	Loss 0.5470 (0.4445)	Acc@1 81.250 (84.898)	Acc@5 98.438 (99.351)
Epoch: [46][128/196]	Time 0.769 (0.715)	Data 0.000 (0.004)	Loss 0.4966 (0.4527)	Acc@1 83.203 (84.493)	Acc@5 99.609 (99.331)
Epoch: [46][192/196]	Time 0.659 (0.708)	Data 0.000 (0.003)	Loss 0.4542 (0.4622)	Acc@1 82.422 (84.114)	Acc@5 99.219 (99.277)
after train
Epoche: [47/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [47][0/196]	Time 0.772 (0.772)	Data 0.526 (0.526)	Loss 0.3701 (0.3701)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [47][64/196]	Time 0.684 (0.697)	Data 0.000 (0.009)	Loss 0.5929 (0.4540)	Acc@1 78.906 (84.507)	Acc@5 99.219 (99.441)
Epoch: [47][128/196]	Time 0.711 (0.696)	Data 0.000 (0.005)	Loss 0.4564 (0.4555)	Acc@1 83.594 (84.324)	Acc@5 99.219 (99.391)
Epoch: [47][192/196]	Time 0.682 (0.690)	Data 0.000 (0.003)	Loss 0.3763 (0.4643)	Acc@1 86.328 (84.001)	Acc@5 100.000 (99.326)
after train
Epoche: [48/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [48][0/196]	Time 0.748 (0.748)	Data 0.412 (0.412)	Loss 0.4331 (0.4331)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [48][64/196]	Time 0.751 (0.718)	Data 0.000 (0.007)	Loss 0.4022 (0.4479)	Acc@1 85.156 (84.411)	Acc@5 99.609 (99.369)
Epoch: [48][128/196]	Time 0.732 (0.718)	Data 0.000 (0.004)	Loss 0.4908 (0.4581)	Acc@1 85.938 (84.175)	Acc@5 98.438 (99.288)
Epoch: [48][192/196]	Time 0.625 (0.709)	Data 0.000 (0.002)	Loss 0.3637 (0.4631)	Acc@1 85.547 (84.063)	Acc@5 99.609 (99.267)
after train
Epoche: [49/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [49][0/196]	Time 0.824 (0.824)	Data 0.509 (0.509)	Loss 0.4854 (0.4854)	Acc@1 84.766 (84.766)	Acc@5 98.047 (98.047)
Epoch: [49][64/196]	Time 0.696 (0.708)	Data 0.000 (0.008)	Loss 0.5773 (0.4559)	Acc@1 80.859 (84.573)	Acc@5 99.219 (99.339)
Epoch: [49][128/196]	Time 0.643 (0.698)	Data 0.000 (0.004)	Loss 0.4159 (0.4517)	Acc@1 85.156 (84.723)	Acc@5 99.609 (99.316)
Epoch: [49][192/196]	Time 0.750 (0.695)	Data 0.000 (0.003)	Loss 0.4289 (0.4544)	Acc@1 85.156 (84.577)	Acc@5 100.000 (99.294)
after train
Epoche: [50/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [50][0/196]	Time 0.816 (0.816)	Data 0.487 (0.487)	Loss 0.4730 (0.4730)	Acc@1 82.422 (82.422)	Acc@5 98.828 (98.828)
Epoch: [50][64/196]	Time 0.695 (0.708)	Data 0.000 (0.009)	Loss 0.4628 (0.4441)	Acc@1 81.641 (84.826)	Acc@5 99.219 (99.399)
Epoch: [50][128/196]	Time 0.691 (0.698)	Data 0.000 (0.005)	Loss 0.4525 (0.4578)	Acc@1 84.375 (84.278)	Acc@5 99.219 (99.297)
Epoch: [50][192/196]	Time 0.708 (0.699)	Data 0.000 (0.003)	Loss 0.3972 (0.4575)	Acc@1 86.328 (84.205)	Acc@5 99.219 (99.308)
after train
Epoche: [51/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [51][0/196]	Time 0.765 (0.765)	Data 0.385 (0.385)	Loss 0.4021 (0.4021)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [51][64/196]	Time 0.760 (0.726)	Data 0.000 (0.006)	Loss 0.4485 (0.4644)	Acc@1 84.375 (83.948)	Acc@5 99.609 (99.375)
Epoch: [51][128/196]	Time 0.750 (0.712)	Data 0.000 (0.003)	Loss 0.3795 (0.4531)	Acc@1 90.234 (84.396)	Acc@5 99.219 (99.337)
Epoch: [51][192/196]	Time 0.750 (0.711)	Data 0.000 (0.002)	Loss 0.5385 (0.4587)	Acc@1 80.469 (84.304)	Acc@5 98.828 (99.336)
after train
Epoche: [52/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [52][0/196]	Time 0.745 (0.745)	Data 0.396 (0.396)	Loss 0.3577 (0.3577)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [52][64/196]	Time 0.815 (0.716)	Data 0.000 (0.007)	Loss 0.5188 (0.4514)	Acc@1 83.203 (84.339)	Acc@5 99.219 (99.465)
Epoch: [52][128/196]	Time 0.732 (0.710)	Data 0.000 (0.004)	Loss 0.4846 (0.4485)	Acc@1 84.766 (84.566)	Acc@5 98.828 (99.434)
Epoch: [52][192/196]	Time 0.729 (0.707)	Data 0.000 (0.003)	Loss 0.4811 (0.4536)	Acc@1 83.984 (84.349)	Acc@5 99.219 (99.405)
after train
Epoche: [53/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [53][0/196]	Time 0.674 (0.674)	Data 0.363 (0.363)	Loss 0.3963 (0.3963)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [53][64/196]	Time 0.705 (0.721)	Data 0.000 (0.006)	Loss 0.4039 (0.4516)	Acc@1 85.938 (84.489)	Acc@5 100.000 (99.345)
Epoch: [53][128/196]	Time 0.739 (0.705)	Data 0.000 (0.003)	Loss 0.5466 (0.4560)	Acc@1 80.859 (84.042)	Acc@5 99.609 (99.316)
Epoch: [53][192/196]	Time 0.784 (0.703)	Data 0.000 (0.002)	Loss 0.4505 (0.4574)	Acc@1 82.031 (84.124)	Acc@5 98.828 (99.338)
after train
Epoche: [54/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [54][0/196]	Time 0.729 (0.729)	Data 0.453 (0.453)	Loss 0.4187 (0.4187)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [54][64/196]	Time 0.720 (0.720)	Data 0.000 (0.007)	Loss 0.4226 (0.4528)	Acc@1 87.109 (84.423)	Acc@5 98.438 (99.327)
Epoch: [54][128/196]	Time 0.790 (0.716)	Data 0.000 (0.004)	Loss 0.4719 (0.4539)	Acc@1 84.375 (84.227)	Acc@5 98.828 (99.343)
Epoch: [54][192/196]	Time 0.747 (0.710)	Data 0.000 (0.003)	Loss 0.4868 (0.4517)	Acc@1 83.984 (84.282)	Acc@5 98.828 (99.336)
after train
Epoche: [55/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [55][0/196]	Time 0.750 (0.750)	Data 0.595 (0.595)	Loss 0.4521 (0.4521)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [55][64/196]	Time 0.719 (0.715)	Data 0.000 (0.009)	Loss 0.5147 (0.4635)	Acc@1 82.422 (83.954)	Acc@5 99.219 (99.375)
Epoch: [55][128/196]	Time 0.698 (0.700)	Data 0.000 (0.005)	Loss 0.5113 (0.4489)	Acc@1 82.422 (84.451)	Acc@5 99.609 (99.431)
Epoch: [55][192/196]	Time 0.929 (0.703)	Data 0.000 (0.003)	Loss 0.4577 (0.4541)	Acc@1 83.984 (84.349)	Acc@5 99.609 (99.348)
after train
Epoche: [56/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [56][0/196]	Time 0.880 (0.880)	Data 0.485 (0.485)	Loss 0.5322 (0.5322)	Acc@1 80.859 (80.859)	Acc@5 99.219 (99.219)
Epoch: [56][64/196]	Time 0.794 (0.718)	Data 0.000 (0.008)	Loss 0.4813 (0.4500)	Acc@1 82.812 (84.513)	Acc@5 99.609 (99.381)
Epoch: [56][128/196]	Time 0.730 (0.713)	Data 0.000 (0.004)	Loss 0.3553 (0.4442)	Acc@1 88.281 (84.850)	Acc@5 99.219 (99.376)
Epoch: [56][192/196]	Time 0.638 (0.712)	Data 0.000 (0.003)	Loss 0.5048 (0.4452)	Acc@1 82.031 (84.707)	Acc@5 99.609 (99.375)
after train
Epoche: [57/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [57][0/196]	Time 0.868 (0.868)	Data 0.479 (0.479)	Loss 0.3925 (0.3925)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [57][64/196]	Time 0.642 (0.726)	Data 0.000 (0.008)	Loss 0.4728 (0.4329)	Acc@1 82.031 (84.964)	Acc@5 98.828 (99.405)
Epoch: [57][128/196]	Time 0.698 (0.715)	Data 0.000 (0.004)	Loss 0.5143 (0.4422)	Acc@1 82.812 (84.726)	Acc@5 98.828 (99.334)
Epoch: [57][192/196]	Time 0.682 (0.708)	Data 0.000 (0.003)	Loss 0.4255 (0.4458)	Acc@1 84.766 (84.642)	Acc@5 99.219 (99.324)
after train
Epoche: [58/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [58][0/196]	Time 0.763 (0.763)	Data 0.572 (0.572)	Loss 0.4915 (0.4915)	Acc@1 82.422 (82.422)	Acc@5 99.609 (99.609)
Epoch: [58][64/196]	Time 0.745 (0.731)	Data 0.000 (0.010)	Loss 0.4215 (0.4543)	Acc@1 85.156 (84.345)	Acc@5 99.219 (99.297)
Epoch: [58][128/196]	Time 0.697 (0.714)	Data 0.000 (0.005)	Loss 0.4182 (0.4566)	Acc@1 86.719 (84.324)	Acc@5 99.609 (99.316)
Epoch: [58][192/196]	Time 0.778 (0.720)	Data 0.000 (0.004)	Loss 0.4146 (0.4543)	Acc@1 84.766 (84.353)	Acc@5 100.000 (99.330)
after train
Epoche: [59/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [59][0/196]	Time 0.921 (0.921)	Data 0.467 (0.467)	Loss 0.4096 (0.4096)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [59][64/196]	Time 0.730 (0.711)	Data 0.000 (0.008)	Loss 0.4505 (0.4397)	Acc@1 87.109 (84.880)	Acc@5 100.000 (99.225)
Epoch: [59][128/196]	Time 0.666 (0.709)	Data 0.000 (0.004)	Loss 0.4268 (0.4440)	Acc@1 81.641 (84.699)	Acc@5 100.000 (99.279)
Epoch: [59][192/196]	Time 0.684 (0.707)	Data 0.000 (0.003)	Loss 0.3453 (0.4451)	Acc@1 88.672 (84.642)	Acc@5 99.609 (99.298)
after train
Epoche: [60/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [60][0/196]	Time 0.731 (0.731)	Data 0.341 (0.341)	Loss 0.4476 (0.4476)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [60][64/196]	Time 0.651 (0.714)	Data 0.000 (0.006)	Loss 0.4691 (0.4575)	Acc@1 83.984 (84.189)	Acc@5 99.219 (99.261)
Epoch: [60][128/196]	Time 0.672 (0.704)	Data 0.000 (0.003)	Loss 0.4674 (0.4514)	Acc@1 81.641 (84.442)	Acc@5 100.000 (99.325)
Epoch: [60][192/196]	Time 0.692 (0.697)	Data 0.000 (0.002)	Loss 0.3530 (0.4526)	Acc@1 89.453 (84.339)	Acc@5 100.000 (99.332)
after train
Epoche: [61/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [61][0/196]	Time 0.923 (0.923)	Data 0.348 (0.348)	Loss 0.3635 (0.3635)	Acc@1 88.281 (88.281)	Acc@5 99.609 (99.609)
Epoch: [61][64/196]	Time 0.688 (0.719)	Data 0.000 (0.006)	Loss 0.4597 (0.4473)	Acc@1 86.719 (84.495)	Acc@5 98.438 (99.291)
Epoch: [61][128/196]	Time 0.701 (0.711)	Data 0.000 (0.003)	Loss 0.4634 (0.4450)	Acc@1 83.984 (84.784)	Acc@5 99.219 (99.325)
Epoch: [61][192/196]	Time 0.737 (0.705)	Data 0.000 (0.002)	Loss 0.4808 (0.4416)	Acc@1 85.938 (84.919)	Acc@5 97.656 (99.332)
after train
Epoche: [62/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [62][0/196]	Time 0.738 (0.738)	Data 0.402 (0.402)	Loss 0.4726 (0.4726)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [62][64/196]	Time 0.752 (0.714)	Data 0.000 (0.006)	Loss 0.4424 (0.4319)	Acc@1 86.328 (85.054)	Acc@5 98.828 (99.381)
Epoch: [62][128/196]	Time 0.683 (0.707)	Data 0.000 (0.004)	Loss 0.3799 (0.4406)	Acc@1 86.328 (84.905)	Acc@5 100.000 (99.304)
Epoch: [62][192/196]	Time 0.752 (0.708)	Data 0.000 (0.003)	Loss 0.4496 (0.4442)	Acc@1 87.891 (84.675)	Acc@5 99.609 (99.279)
after train
Epoche: [63/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [63][0/196]	Time 0.775 (0.775)	Data 0.416 (0.416)	Loss 0.4143 (0.4143)	Acc@1 85.156 (85.156)	Acc@5 98.438 (98.438)
Epoch: [63][64/196]	Time 0.779 (0.713)	Data 0.000 (0.007)	Loss 0.3846 (0.4279)	Acc@1 85.938 (85.042)	Acc@5 100.000 (99.327)
Epoch: [63][128/196]	Time 0.753 (0.712)	Data 0.000 (0.004)	Loss 0.5049 (0.4432)	Acc@1 82.812 (84.708)	Acc@5 98.438 (99.340)
Epoch: [63][192/196]	Time 0.699 (0.708)	Data 0.000 (0.002)	Loss 0.3230 (0.4430)	Acc@1 89.453 (84.711)	Acc@5 99.609 (99.360)
after train
Epoche: [64/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [64][0/196]	Time 0.806 (0.806)	Data 0.395 (0.395)	Loss 0.3488 (0.3488)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [64][64/196]	Time 0.790 (0.715)	Data 0.000 (0.006)	Loss 0.4532 (0.4494)	Acc@1 83.984 (84.231)	Acc@5 100.000 (99.351)
Epoch: [64][128/196]	Time 0.731 (0.707)	Data 0.000 (0.003)	Loss 0.5507 (0.4453)	Acc@1 80.078 (84.502)	Acc@5 99.219 (99.310)
Epoch: [64][192/196]	Time 0.657 (0.707)	Data 0.000 (0.002)	Loss 0.6523 (0.4522)	Acc@1 77.734 (84.314)	Acc@5 98.438 (99.316)
after train
Epoche: [65/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [65][0/196]	Time 0.861 (0.861)	Data 0.366 (0.366)	Loss 0.3587 (0.3587)	Acc@1 87.109 (87.109)	Acc@5 100.000 (100.000)
Epoch: [65][64/196]	Time 0.693 (0.718)	Data 0.000 (0.006)	Loss 0.4352 (0.4156)	Acc@1 85.156 (85.643)	Acc@5 99.609 (99.363)
Epoch: [65][128/196]	Time 0.692 (0.711)	Data 0.000 (0.003)	Loss 0.4718 (0.4277)	Acc@1 85.938 (85.190)	Acc@5 99.219 (99.373)
Epoch: [65][192/196]	Time 0.717 (0.704)	Data 0.000 (0.002)	Loss 0.5307 (0.4394)	Acc@1 83.594 (84.741)	Acc@5 99.219 (99.338)
after train
Epoche: [66/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [66][0/196]	Time 0.841 (0.841)	Data 0.420 (0.420)	Loss 0.4603 (0.4603)	Acc@1 82.422 (82.422)	Acc@5 100.000 (100.000)
Epoch: [66][64/196]	Time 0.739 (0.725)	Data 0.000 (0.007)	Loss 0.4357 (0.4356)	Acc@1 85.547 (85.138)	Acc@5 100.000 (99.297)
Epoch: [66][128/196]	Time 0.840 (0.707)	Data 0.000 (0.004)	Loss 0.3948 (0.4369)	Acc@1 86.719 (84.896)	Acc@5 99.609 (99.367)
Epoch: [66][192/196]	Time 0.719 (0.702)	Data 0.000 (0.003)	Loss 0.4895 (0.4417)	Acc@1 85.156 (84.895)	Acc@5 99.219 (99.360)
after train
Epoche: [67/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [67][0/196]	Time 0.707 (0.707)	Data 0.419 (0.419)	Loss 0.3783 (0.3783)	Acc@1 89.453 (89.453)	Acc@5 98.828 (98.828)
Epoch: [67][64/196]	Time 0.716 (0.722)	Data 0.000 (0.007)	Loss 0.4538 (0.4410)	Acc@1 86.719 (84.669)	Acc@5 99.219 (99.321)
Epoch: [67][128/196]	Time 0.717 (0.712)	Data 0.000 (0.004)	Loss 0.4148 (0.4475)	Acc@1 87.891 (84.657)	Acc@5 99.609 (99.252)
Epoch: [67][192/196]	Time 0.745 (0.708)	Data 0.000 (0.002)	Loss 0.3466 (0.4396)	Acc@1 88.281 (84.911)	Acc@5 99.219 (99.312)
after train
Epoche: [68/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [68][0/196]	Time 0.823 (0.823)	Data 0.510 (0.510)	Loss 0.4266 (0.4266)	Acc@1 85.547 (85.547)	Acc@5 99.219 (99.219)
Epoch: [68][64/196]	Time 0.708 (0.719)	Data 0.000 (0.008)	Loss 0.5032 (0.4361)	Acc@1 84.375 (84.904)	Acc@5 98.438 (99.393)
Epoch: [68][128/196]	Time 0.683 (0.711)	Data 0.000 (0.004)	Loss 0.3878 (0.4394)	Acc@1 83.984 (84.744)	Acc@5 99.609 (99.394)
Epoch: [68][192/196]	Time 0.652 (0.710)	Data 0.000 (0.003)	Loss 0.4320 (0.4387)	Acc@1 85.547 (84.861)	Acc@5 100.000 (99.362)
after train
Epoche: [69/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [69][0/196]	Time 0.848 (0.848)	Data 0.457 (0.457)	Loss 0.4255 (0.4255)	Acc@1 85.156 (85.156)	Acc@5 98.828 (98.828)
Epoch: [69][64/196]	Time 0.731 (0.711)	Data 0.000 (0.007)	Loss 0.4376 (0.4215)	Acc@1 85.547 (85.613)	Acc@5 99.609 (99.357)
Epoch: [69][128/196]	Time 0.694 (0.708)	Data 0.000 (0.004)	Loss 0.4545 (0.4305)	Acc@1 82.422 (85.250)	Acc@5 100.000 (99.322)
Epoch: [69][192/196]	Time 0.750 (0.709)	Data 0.000 (0.003)	Loss 0.4099 (0.4342)	Acc@1 86.719 (85.098)	Acc@5 99.609 (99.328)
after train
Epoche: [70/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [70][0/196]	Time 0.755 (0.755)	Data 0.406 (0.406)	Loss 0.3101 (0.3101)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [70][64/196]	Time 0.774 (0.711)	Data 0.000 (0.007)	Loss 0.4009 (0.4150)	Acc@1 84.766 (85.841)	Acc@5 99.219 (99.357)
Epoch: [70][128/196]	Time 0.754 (0.708)	Data 0.000 (0.003)	Loss 0.4479 (0.4292)	Acc@1 85.547 (85.223)	Acc@5 100.000 (99.361)
Epoch: [70][192/196]	Time 0.678 (0.708)	Data 0.000 (0.002)	Loss 0.4049 (0.4340)	Acc@1 85.156 (85.033)	Acc@5 99.609 (99.375)
after train
Epoche: [71/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [71][0/196]	Time 0.782 (0.782)	Data 0.418 (0.418)	Loss 0.4056 (0.4056)	Acc@1 86.328 (86.328)	Acc@5 100.000 (100.000)
Epoch: [71][64/196]	Time 0.702 (0.720)	Data 0.000 (0.007)	Loss 0.3704 (0.4191)	Acc@1 89.453 (85.481)	Acc@5 99.219 (99.387)
Epoch: [71][128/196]	Time 0.702 (0.714)	Data 0.000 (0.004)	Loss 0.4246 (0.4307)	Acc@1 86.328 (85.190)	Acc@5 98.438 (99.352)
Epoch: [71][192/196]	Time 0.765 (0.712)	Data 0.000 (0.002)	Loss 0.3670 (0.4374)	Acc@1 85.156 (84.881)	Acc@5 99.609 (99.332)
after train
Epoche: [72/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [72][0/196]	Time 0.830 (0.830)	Data 0.429 (0.429)	Loss 0.3455 (0.3455)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [72][64/196]	Time 0.777 (0.718)	Data 0.000 (0.007)	Loss 0.4014 (0.4393)	Acc@1 86.328 (84.591)	Acc@5 98.828 (99.309)
Epoch: [72][128/196]	Time 0.716 (0.716)	Data 0.000 (0.004)	Loss 0.4491 (0.4373)	Acc@1 84.375 (84.838)	Acc@5 98.828 (99.328)
Epoch: [72][192/196]	Time 0.658 (0.713)	Data 0.000 (0.003)	Loss 0.3253 (0.4419)	Acc@1 89.453 (84.681)	Acc@5 99.609 (99.318)
after train
Epoche: [73/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [73][0/196]	Time 0.728 (0.728)	Data 0.457 (0.457)	Loss 0.4411 (0.4411)	Acc@1 83.203 (83.203)	Acc@5 100.000 (100.000)
Epoch: [73][64/196]	Time 0.720 (0.711)	Data 0.000 (0.007)	Loss 0.4638 (0.4334)	Acc@1 84.375 (85.000)	Acc@5 100.000 (99.417)
Epoch: [73][128/196]	Time 0.710 (0.709)	Data 0.000 (0.004)	Loss 0.4505 (0.4356)	Acc@1 85.547 (85.053)	Acc@5 99.609 (99.394)
Epoch: [73][192/196]	Time 0.653 (0.708)	Data 0.000 (0.003)	Loss 0.3463 (0.4349)	Acc@1 87.109 (85.096)	Acc@5 100.000 (99.415)
after train
Epoche: [74/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [74][0/196]	Time 0.758 (0.758)	Data 0.364 (0.364)	Loss 0.4062 (0.4062)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [74][64/196]	Time 0.712 (0.713)	Data 0.000 (0.006)	Loss 0.4416 (0.4282)	Acc@1 84.766 (85.499)	Acc@5 99.219 (99.315)
Epoch: [74][128/196]	Time 0.730 (0.709)	Data 0.000 (0.003)	Loss 0.3411 (0.4282)	Acc@1 89.062 (85.277)	Acc@5 100.000 (99.346)
Epoch: [74][192/196]	Time 0.779 (0.710)	Data 0.000 (0.002)	Loss 0.4589 (0.4342)	Acc@1 83.984 (85.061)	Acc@5 99.609 (99.350)
after train
Epoche: [75/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [75][0/196]	Time 0.756 (0.756)	Data 0.404 (0.404)	Loss 0.4500 (0.4500)	Acc@1 82.422 (82.422)	Acc@5 100.000 (100.000)
Epoch: [75][64/196]	Time 0.716 (0.718)	Data 0.000 (0.007)	Loss 0.3630 (0.4286)	Acc@1 86.719 (85.210)	Acc@5 99.609 (99.417)
Epoch: [75][128/196]	Time 0.742 (0.711)	Data 0.000 (0.003)	Loss 0.3584 (0.4330)	Acc@1 89.844 (85.177)	Acc@5 99.609 (99.397)
Epoch: [75][192/196]	Time 0.714 (0.710)	Data 0.000 (0.002)	Loss 0.4305 (0.4345)	Acc@1 86.328 (84.976)	Acc@5 98.438 (99.383)
after train
Epoche: [76/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [76][0/196]	Time 0.820 (0.820)	Data 0.394 (0.394)	Loss 0.2991 (0.2991)	Acc@1 88.672 (88.672)	Acc@5 100.000 (100.000)
Epoch: [76][64/196]	Time 0.713 (0.725)	Data 0.000 (0.006)	Loss 0.4849 (0.4302)	Acc@1 83.203 (84.994)	Acc@5 99.609 (99.393)
Epoch: [76][128/196]	Time 0.763 (0.717)	Data 0.000 (0.003)	Loss 0.5523 (0.4361)	Acc@1 79.297 (84.875)	Acc@5 98.438 (99.340)
Epoch: [76][192/196]	Time 0.706 (0.712)	Data 0.000 (0.002)	Loss 0.4755 (0.4356)	Acc@1 85.547 (84.946)	Acc@5 99.219 (99.330)
after train
Epoche: [77/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [77][0/196]	Time 0.742 (0.742)	Data 0.346 (0.346)	Loss 0.3361 (0.3361)	Acc@1 89.062 (89.062)	Acc@5 99.609 (99.609)
Epoch: [77][64/196]	Time 0.731 (0.722)	Data 0.000 (0.006)	Loss 0.4432 (0.4263)	Acc@1 84.766 (85.553)	Acc@5 98.047 (99.177)
Epoch: [77][128/196]	Time 0.731 (0.714)	Data 0.000 (0.003)	Loss 0.4359 (0.4270)	Acc@1 84.375 (85.498)	Acc@5 99.219 (99.310)
Epoch: [77][192/196]	Time 0.767 (0.710)	Data 0.000 (0.002)	Loss 0.4893 (0.4294)	Acc@1 82.031 (85.310)	Acc@5 99.219 (99.328)
after train
Epoche: [78/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [78][0/196]	Time 0.792 (0.792)	Data 0.403 (0.403)	Loss 0.4615 (0.4615)	Acc@1 84.766 (84.766)	Acc@5 100.000 (100.000)
Epoch: [78][64/196]	Time 0.719 (0.722)	Data 0.000 (0.007)	Loss 0.3567 (0.4369)	Acc@1 88.672 (84.928)	Acc@5 100.000 (99.501)
Epoch: [78][128/196]	Time 0.748 (0.718)	Data 0.000 (0.004)	Loss 0.4418 (0.4358)	Acc@1 84.375 (85.020)	Acc@5 99.219 (99.419)
Epoch: [78][192/196]	Time 0.716 (0.716)	Data 0.000 (0.002)	Loss 0.3603 (0.4340)	Acc@1 89.844 (85.110)	Acc@5 99.609 (99.389)
after train
Epoche: [79/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [79][0/196]	Time 0.764 (0.764)	Data 0.401 (0.401)	Loss 0.3834 (0.3834)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [79][64/196]	Time 0.667 (0.720)	Data 0.000 (0.006)	Loss 0.5089 (0.4249)	Acc@1 82.031 (85.409)	Acc@5 99.219 (99.399)
Epoch: [79][128/196]	Time 0.715 (0.711)	Data 0.000 (0.003)	Loss 0.4024 (0.4274)	Acc@1 85.938 (85.177)	Acc@5 99.219 (99.319)
Epoch: [79][192/196]	Time 0.702 (0.708)	Data 0.000 (0.002)	Loss 0.4084 (0.4291)	Acc@1 85.547 (85.130)	Acc@5 100.000 (99.352)
after train
Epoche: [80/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [80][0/196]	Time 0.801 (0.801)	Data 0.417 (0.417)	Loss 0.4223 (0.4223)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [80][64/196]	Time 0.690 (0.715)	Data 0.000 (0.007)	Loss 0.3890 (0.4263)	Acc@1 87.500 (85.565)	Acc@5 99.219 (99.369)
Epoch: [80][128/196]	Time 0.700 (0.710)	Data 0.000 (0.004)	Loss 0.3913 (0.4300)	Acc@1 86.719 (85.332)	Acc@5 100.000 (99.355)
Epoch: [80][192/196]	Time 0.716 (0.707)	Data 0.000 (0.003)	Loss 0.3557 (0.4346)	Acc@1 89.453 (85.057)	Acc@5 100.000 (99.352)
after train
Epoche: [81/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [81][0/196]	Time 0.735 (0.735)	Data 0.371 (0.371)	Loss 0.4570 (0.4570)	Acc@1 84.766 (84.766)	Acc@5 98.438 (98.438)
Epoch: [81][64/196]	Time 0.711 (0.723)	Data 0.000 (0.006)	Loss 0.3449 (0.4350)	Acc@1 86.719 (84.934)	Acc@5 100.000 (99.411)
Epoch: [81][128/196]	Time 0.756 (0.715)	Data 0.000 (0.003)	Loss 0.2845 (0.4275)	Acc@1 91.016 (85.199)	Acc@5 99.219 (99.376)
Epoch: [81][192/196]	Time 0.714 (0.713)	Data 0.000 (0.002)	Loss 0.5037 (0.4353)	Acc@1 80.078 (84.930)	Acc@5 98.828 (99.356)
after train
Epoche: [82/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [82][0/196]	Time 0.854 (0.854)	Data 0.534 (0.534)	Loss 0.3691 (0.3691)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [82][64/196]	Time 0.730 (0.724)	Data 0.000 (0.009)	Loss 0.4682 (0.4204)	Acc@1 83.594 (85.475)	Acc@5 98.828 (99.453)
Epoch: [82][128/196]	Time 0.717 (0.715)	Data 0.000 (0.004)	Loss 0.3280 (0.4240)	Acc@1 89.844 (85.347)	Acc@5 99.609 (99.443)
Epoch: [82][192/196]	Time 0.713 (0.712)	Data 0.000 (0.003)	Loss 0.4477 (0.4233)	Acc@1 84.375 (85.407)	Acc@5 99.609 (99.425)
after train
Epoche: [83/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [83][0/196]	Time 0.773 (0.773)	Data 0.480 (0.480)	Loss 0.4812 (0.4812)	Acc@1 81.641 (81.641)	Acc@5 99.609 (99.609)
Epoch: [83][64/196]	Time 0.713 (0.710)	Data 0.000 (0.008)	Loss 0.3829 (0.4331)	Acc@1 86.328 (85.048)	Acc@5 99.609 (99.447)
Epoch: [83][128/196]	Time 0.735 (0.708)	Data 0.000 (0.004)	Loss 0.4570 (0.4288)	Acc@1 83.203 (85.153)	Acc@5 100.000 (99.446)
Epoch: [83][192/196]	Time 0.695 (0.707)	Data 0.000 (0.003)	Loss 0.4517 (0.4316)	Acc@1 85.156 (85.067)	Acc@5 99.609 (99.399)
after train
Epoche: [84/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [84][0/196]	Time 0.872 (0.872)	Data 0.323 (0.323)	Loss 0.5498 (0.5498)	Acc@1 81.250 (81.250)	Acc@5 98.828 (98.828)
Epoch: [84][64/196]	Time 0.641 (0.724)	Data 0.000 (0.005)	Loss 0.3237 (0.4290)	Acc@1 89.844 (85.144)	Acc@5 100.000 (99.303)
Epoch: [84][128/196]	Time 0.763 (0.716)	Data 0.000 (0.003)	Loss 0.3778 (0.4199)	Acc@1 88.672 (85.426)	Acc@5 100.000 (99.373)
Epoch: [84][192/196]	Time 0.697 (0.712)	Data 0.000 (0.002)	Loss 0.5162 (0.4259)	Acc@1 82.031 (85.257)	Acc@5 99.219 (99.373)
after train
Epoche: [85/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [85][0/196]	Time 0.861 (0.861)	Data 0.368 (0.368)	Loss 0.5006 (0.5006)	Acc@1 80.469 (80.469)	Acc@5 100.000 (100.000)
Epoch: [85][64/196]	Time 0.725 (0.719)	Data 0.000 (0.006)	Loss 0.4325 (0.4374)	Acc@1 85.156 (84.760)	Acc@5 98.438 (99.351)
Epoch: [85][128/196]	Time 0.725 (0.710)	Data 0.000 (0.003)	Loss 0.3944 (0.4234)	Acc@1 84.375 (85.305)	Acc@5 99.609 (99.376)
Epoch: [85][192/196]	Time 0.709 (0.709)	Data 0.000 (0.002)	Loss 0.3285 (0.4334)	Acc@1 89.453 (85.055)	Acc@5 100.000 (99.371)
after train
Epoche: [86/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [86][0/196]	Time 0.820 (0.820)	Data 0.357 (0.357)	Loss 0.4850 (0.4850)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [86][64/196]	Time 0.735 (0.720)	Data 0.000 (0.006)	Loss 0.3142 (0.4086)	Acc@1 90.625 (85.950)	Acc@5 100.000 (99.465)
Epoch: [86][128/196]	Time 0.773 (0.711)	Data 0.000 (0.003)	Loss 0.4263 (0.4192)	Acc@1 87.109 (85.665)	Acc@5 99.219 (99.403)
Epoch: [86][192/196]	Time 0.694 (0.708)	Data 0.000 (0.002)	Loss 0.5240 (0.4258)	Acc@1 82.422 (85.425)	Acc@5 98.828 (99.379)
after train
Epoche: [87/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [87][0/196]	Time 0.798 (0.798)	Data 0.443 (0.443)	Loss 0.3972 (0.3972)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [87][64/196]	Time 0.732 (0.713)	Data 0.000 (0.007)	Loss 0.4578 (0.4111)	Acc@1 83.203 (85.733)	Acc@5 100.000 (99.483)
Epoch: [87][128/196]	Time 0.679 (0.710)	Data 0.000 (0.004)	Loss 0.3635 (0.4137)	Acc@1 85.938 (85.671)	Acc@5 99.609 (99.434)
Epoch: [87][192/196]	Time 0.711 (0.708)	Data 0.000 (0.003)	Loss 0.5056 (0.4183)	Acc@1 82.031 (85.425)	Acc@5 99.609 (99.411)
after train
Epoche: [88/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [88][0/196]	Time 0.866 (0.866)	Data 0.314 (0.314)	Loss 0.4136 (0.4136)	Acc@1 87.891 (87.891)	Acc@5 98.438 (98.438)
Epoch: [88][64/196]	Time 0.711 (0.724)	Data 0.000 (0.005)	Loss 0.3279 (0.4077)	Acc@1 88.672 (86.004)	Acc@5 100.000 (99.471)
Epoch: [88][128/196]	Time 0.678 (0.711)	Data 0.000 (0.003)	Loss 0.5479 (0.4114)	Acc@1 82.031 (85.777)	Acc@5 98.828 (99.452)
Epoch: [88][192/196]	Time 0.554 (0.675)	Data 0.000 (0.002)	Loss 0.4390 (0.4192)	Acc@1 84.766 (85.502)	Acc@5 99.609 (99.441)
after train
Epoche: [89/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [89][0/196]	Time 0.589 (0.589)	Data 0.286 (0.286)	Loss 0.4375 (0.4375)	Acc@1 83.984 (83.984)	Acc@5 100.000 (100.000)
Epoch: [89][64/196]	Time 0.534 (0.577)	Data 0.000 (0.005)	Loss 0.3982 (0.4173)	Acc@1 86.719 (85.511)	Acc@5 99.609 (99.513)
Epoch: [89][128/196]	Time 0.569 (0.565)	Data 0.000 (0.003)	Loss 0.3978 (0.4256)	Acc@1 86.719 (85.329)	Acc@5 98.828 (99.391)
Epoch: [89][192/196]	Time 0.562 (0.564)	Data 0.000 (0.002)	Loss 0.4969 (0.4272)	Acc@1 83.203 (85.310)	Acc@5 98.828 (99.371)
after train
Epoche: [90/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [90][0/196]	Time 0.720 (0.720)	Data 0.333 (0.333)	Loss 0.3979 (0.3979)	Acc@1 86.719 (86.719)	Acc@5 100.000 (100.000)
Epoch: [90][64/196]	Time 0.577 (0.579)	Data 0.000 (0.005)	Loss 0.4108 (0.4172)	Acc@1 85.156 (85.631)	Acc@5 99.609 (99.483)
Epoch: [90][128/196]	Time 0.519 (0.568)	Data 0.000 (0.003)	Loss 0.4159 (0.4259)	Acc@1 83.594 (85.232)	Acc@5 100.000 (99.394)
Epoch: [90][192/196]	Time 0.585 (0.567)	Data 0.000 (0.002)	Loss 0.3943 (0.4275)	Acc@1 86.719 (85.253)	Acc@5 99.609 (99.381)
after train
Traceback (most recent call last):
  File "main.py", line 884, in <module>
    main()
  File "main.py", line 453, in main
    model.wider(1.5, weight_norm=None, random_init=False, addNoise=True)
TypeError: wider() missing 1 required positional argument: 'delta_width'
