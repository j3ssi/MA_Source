no display found. Using non-interactive Agg backend
[5, 5, 5]
[8, 16, 32]
Pytorch Training main.py; workers: 6; numOfStages: 3; layerinBlock: 2;widthofFirstLayer: 16; Epochen: 180; reset: False; start epoche: 1; test: True pathtoModell: ./output/experimente4/test1/model.nn; checkpoint: ./output/experimente4/widerRnd_Lr1; saveModell: True; LR: 0.1
random number: 8244
Files already downloaded and verified
width: 8

Arch Num:  [[2, 2, 2, 2, 2], [3, 2, 2, 2, 2], [3, 2, 2, 2, 2]]
conv0: Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0
bn1: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
Relu; i: 2
i : 2; block: 0
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 2; block: 0
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 2
i : 3; block: 1
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 3; block: 1
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 3
i : 4; block: 2
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 4; block: 2
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 4
i : 5; block: 3
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 5; block: 3
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 5
i : 6; block: 4
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 6; block: 4
Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 6
i : 7; block: 0
Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 7; block: 0
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
i : 7; block: 0
Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 2 if 2
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 2
seq: Sequential(
  (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 7
seq1: Sequential(
  (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 8
i : 8; block: 1
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 8; block: 1
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 8
i : 9; block: 2
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 9; block: 2
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 9
i : 10; block: 3
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 10; block: 3
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 10
i : 11; block: 4
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 11; block: 4
Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 11
i : 12; block: 0
Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i=0; if 1
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i=0
relu: 0
i : 12; block: 0
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
i : 12; block: 0
Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 2 if 2
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 2
seq: Sequential(
  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 12
seq1: Sequential(
  (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 13
i : 13; block: 1
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 13; block: 1
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 13
i : 14; block: 2
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 14; block: 2
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 14
i : 15; block: 3
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 15; block: 3
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 15
i : 16; block: 4
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 if 4
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 0
relu; i: 0
i : 16; block: 4
Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 1 if 3
BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1
seq: Sequential(
  (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
); i: 16
avgpoll: AdaptiveAvgPool2d(output_size=(1, 1))
linear: Linear(in_features=32, out_features=10, bias=True)
Modell Erstellung
N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=32, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
device count: 4
Startepoche: 1
deeper epoch: 0
Epoche: [1/180]; Lr: 0.1
batch Size 256
befor train
/home/jessica.buehler/venv/lib/python3.6/site-packages/torch/cuda/memory.py:234: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  FutureWarning)
Epoch: [1][0/196]	Time 0.126 (0.126)	Data 0.251 (0.251)	Loss 2.8098 (2.8098)	Acc@1 7.422 (7.422)	Acc@5 49.219 (49.219)
Epoch: [1][64/196]	Time 0.073 (0.070)	Data 0.000 (0.004)	Loss 1.8503 (2.1327)	Acc@1 28.125 (19.315)	Acc@5 83.203 (70.571)
Epoch: [1][128/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 1.7564 (1.9933)	Acc@1 31.641 (23.631)	Acc@5 87.109 (77.201)
Epoch: [1][192/196]	Time 0.070 (0.071)	Data 0.000 (0.001)	Loss 1.6703 (1.8812)	Acc@1 37.891 (28.024)	Acc@5 88.672 (81.191)
after train
n1: 1 for:
wAcc: 35.28
test acc: 35.28
Epoche: [2/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [2][0/196]	Time 0.113 (0.113)	Data 0.275 (0.275)	Loss 1.5997 (1.5997)	Acc@1 39.844 (39.844)	Acc@5 90.625 (90.625)
Epoch: [2][64/196]	Time 0.069 (0.068)	Data 0.000 (0.004)	Loss 1.4127 (1.5180)	Acc@1 42.578 (42.734)	Acc@5 92.578 (91.526)
Epoch: [2][128/196]	Time 0.072 (0.069)	Data 0.000 (0.002)	Loss 1.2913 (1.4603)	Acc@1 48.828 (45.116)	Acc@5 95.312 (92.466)
Epoch: [2][192/196]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 1.1564 (1.4035)	Acc@1 56.641 (47.735)	Acc@5 94.141 (93.127)
after train
n1: 2 for:
wAcc: 35.28
test acc: 45.32
Epoche: [3/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [3][0/196]	Time 0.083 (0.083)	Data 0.256 (0.256)	Loss 1.0755 (1.0755)	Acc@1 62.500 (62.500)	Acc@5 95.703 (95.703)
Epoch: [3][64/196]	Time 0.073 (0.073)	Data 0.000 (0.004)	Loss 1.1024 (1.1800)	Acc@1 60.547 (56.893)	Acc@5 94.922 (95.535)
Epoch: [3][128/196]	Time 0.075 (0.074)	Data 0.000 (0.002)	Loss 1.0302 (1.1610)	Acc@1 60.156 (57.722)	Acc@5 97.656 (95.658)
Epoch: [3][192/196]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.9037 (1.1347)	Acc@1 61.719 (58.829)	Acc@5 98.438 (95.839)
after train
n1: 3 for:
wAcc: 40.3
test acc: 41.93
Epoche: [4/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [4][0/196]	Time 0.095 (0.095)	Data 0.233 (0.233)	Loss 1.0947 (1.0947)	Acc@1 64.062 (64.062)	Acc@5 93.359 (93.359)
Epoch: [4][64/196]	Time 0.074 (0.073)	Data 0.000 (0.004)	Loss 0.8086 (1.0291)	Acc@1 71.875 (63.083)	Acc@5 97.266 (96.412)
Epoch: [4][128/196]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 1.0157 (0.9988)	Acc@1 65.625 (63.975)	Acc@5 96.484 (96.733)
Epoch: [4][192/196]	Time 0.069 (0.072)	Data 0.000 (0.001)	Loss 1.0543 (0.9787)	Acc@1 61.328 (64.819)	Acc@5 98.438 (96.936)
after train
n1: 4 for:
wAcc: 40.3496
test acc: 56.22
Epoche: [5/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [5][0/196]	Time 0.109 (0.109)	Data 0.255 (0.255)	Loss 0.9508 (0.9508)	Acc@1 66.797 (66.797)	Acc@5 98.047 (98.047)
Epoch: [5][64/196]	Time 0.074 (0.074)	Data 0.000 (0.004)	Loss 0.7920 (0.8919)	Acc@1 69.531 (68.395)	Acc@5 99.219 (97.680)
Epoch: [5][128/196]	Time 0.077 (0.073)	Data 0.000 (0.002)	Loss 0.9531 (0.8881)	Acc@1 64.062 (68.480)	Acc@5 97.656 (97.614)
Epoch: [5][192/196]	Time 0.068 (0.073)	Data 0.000 (0.002)	Loss 0.7956 (0.8719)	Acc@1 70.703 (69.054)	Acc@5 98.828 (97.622)
after train
n1: 5 for:
wAcc: 45.22518518518518
test acc: 67.0
Epoche: [6/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [6][0/196]	Time 0.116 (0.116)	Data 0.234 (0.234)	Loss 0.7671 (0.7671)	Acc@1 73.828 (73.828)	Acc@5 98.438 (98.438)
Epoch: [6][64/196]	Time 0.073 (0.073)	Data 0.000 (0.004)	Loss 0.8211 (0.8009)	Acc@1 71.875 (72.224)	Acc@5 96.875 (97.909)
Epoch: [6][128/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.5419 (0.7869)	Acc@1 77.734 (72.408)	Acc@5 99.609 (98.017)
Epoch: [6][192/196]	Time 0.070 (0.072)	Data 0.000 (0.001)	Loss 0.8142 (0.7889)	Acc@1 73.047 (72.426)	Acc@5 98.828 (97.940)
after train
n1: 6 for:
wAcc: 50.631112036651395
test acc: 63.15
Epoche: [7/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [7][0/196]	Time 0.091 (0.091)	Data 0.279 (0.279)	Loss 0.6972 (0.6972)	Acc@1 76.953 (76.953)	Acc@5 97.266 (97.266)
Epoch: [7][64/196]	Time 0.065 (0.072)	Data 0.000 (0.004)	Loss 0.5961 (0.7604)	Acc@1 78.906 (73.552)	Acc@5 98.047 (98.173)
Epoch: [7][128/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.6115 (0.7492)	Acc@1 80.469 (73.989)	Acc@5 98.828 (98.168)
Epoch: [7][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.7138 (0.7394)	Acc@1 76.172 (74.362)	Acc@5 99.219 (98.231)
after train
n1: 7 for:
wAcc: 52.635234375
test acc: 69.03
Epoche: [8/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [8][0/196]	Time 0.112 (0.112)	Data 0.235 (0.235)	Loss 0.6362 (0.6362)	Acc@1 80.469 (80.469)	Acc@5 98.047 (98.047)
Epoch: [8][64/196]	Time 0.072 (0.072)	Data 0.000 (0.004)	Loss 0.6365 (0.6950)	Acc@1 79.297 (75.889)	Acc@5 98.047 (98.425)
Epoch: [8][128/196]	Time 0.078 (0.072)	Data 0.000 (0.002)	Loss 0.6822 (0.6978)	Acc@1 75.781 (75.890)	Acc@5 99.219 (98.410)
Epoch: [8][192/196]	Time 0.070 (0.073)	Data 0.000 (0.001)	Loss 0.6709 (0.6952)	Acc@1 79.688 (76.042)	Acc@5 98.047 (98.425)
after train
n1: 8 for:
wAcc: 55.22643811824831
test acc: 63.25
Epoche: [9/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [9][0/196]	Time 0.098 (0.098)	Data 0.241 (0.241)	Loss 0.6953 (0.6953)	Acc@1 78.125 (78.125)	Acc@5 98.828 (98.828)
Epoch: [9][64/196]	Time 0.070 (0.072)	Data 0.000 (0.004)	Loss 0.6831 (0.6678)	Acc@1 76.953 (76.587)	Acc@5 99.609 (98.552)
Epoch: [9][128/196]	Time 0.076 (0.072)	Data 0.000 (0.002)	Loss 0.7551 (0.6644)	Acc@1 75.000 (76.838)	Acc@5 97.266 (98.586)
Epoch: [9][192/196]	Time 0.069 (0.072)	Data 0.000 (0.001)	Loss 0.6690 (0.6652)	Acc@1 76.953 (76.903)	Acc@5 99.609 (98.583)
after train
n1: 9 for:
wAcc: 55.76709235200001
test acc: 75.48
Epoche: [10/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [10][0/196]	Time 0.107 (0.107)	Data 0.290 (0.290)	Loss 0.6053 (0.6053)	Acc@1 78.516 (78.516)	Acc@5 98.047 (98.047)
Epoch: [10][64/196]	Time 0.076 (0.074)	Data 0.000 (0.005)	Loss 0.6063 (0.6382)	Acc@1 79.297 (77.704)	Acc@5 99.219 (98.576)
Epoch: [10][128/196]	Time 0.068 (0.072)	Data 0.000 (0.002)	Loss 0.5790 (0.6449)	Acc@1 78.906 (77.689)	Acc@5 98.438 (98.634)
Epoch: [10][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.5908 (0.6454)	Acc@1 79.688 (77.619)	Acc@5 98.828 (98.662)
after train
n1: 10 for:
wAcc: 58.42425918373775
test acc: 68.92
Epoche: [11/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [11][0/196]	Time 0.117 (0.117)	Data 0.261 (0.261)	Loss 0.5725 (0.5725)	Acc@1 79.297 (79.297)	Acc@5 99.219 (99.219)
Epoch: [11][64/196]	Time 0.079 (0.074)	Data 0.000 (0.004)	Loss 0.6146 (0.6243)	Acc@1 80.469 (78.444)	Acc@5 98.438 (98.756)
Epoch: [11][128/196]	Time 0.073 (0.074)	Data 0.000 (0.002)	Loss 0.5802 (0.6255)	Acc@1 82.031 (78.497)	Acc@5 98.828 (98.743)
Epoch: [11][192/196]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.6089 (0.6223)	Acc@1 80.078 (78.566)	Acc@5 98.828 (98.753)
after train
n1: 11 for:
wAcc: 59.194408485828504
test acc: 72.12
Epoche: [12/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [12][0/196]	Time 0.089 (0.089)	Data 0.257 (0.257)	Loss 0.6844 (0.6844)	Acc@1 74.609 (74.609)	Acc@5 98.438 (98.438)
Epoch: [12][64/196]	Time 0.073 (0.073)	Data 0.000 (0.004)	Loss 0.6520 (0.6089)	Acc@1 78.516 (78.864)	Acc@5 98.047 (98.744)
Epoch: [12][128/196]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.5020 (0.6028)	Acc@1 83.203 (79.188)	Acc@5 99.609 (98.795)
Epoch: [12][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.5506 (0.6001)	Acc@1 79.297 (79.271)	Acc@5 99.219 (98.794)
after train
n1: 12 for:
wAcc: 60.28718780660886
test acc: 69.79
Epoche: [13/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [13][0/196]	Time 0.092 (0.092)	Data 0.286 (0.286)	Loss 0.6343 (0.6343)	Acc@1 76.953 (76.953)	Acc@5 98.438 (98.438)
Epoch: [13][64/196]	Time 0.076 (0.072)	Data 0.000 (0.005)	Loss 0.6362 (0.5992)	Acc@1 76.953 (79.333)	Acc@5 99.609 (98.768)
Epoch: [13][128/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.6215 (0.5988)	Acc@1 77.734 (79.209)	Acc@5 98.828 (98.834)
Epoch: [13][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.6185 (0.6002)	Acc@1 78.516 (79.232)	Acc@5 99.219 (98.808)
after train
n1: 13 for:
wAcc: 60.79079661186276
test acc: 67.0
Epoche: [14/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [14][0/196]	Time 0.107 (0.107)	Data 0.278 (0.278)	Loss 0.6009 (0.6009)	Acc@1 80.078 (80.078)	Acc@5 98.438 (98.438)
Epoch: [14][64/196]	Time 0.068 (0.073)	Data 0.000 (0.004)	Loss 0.6088 (0.5761)	Acc@1 76.562 (80.331)	Acc@5 99.609 (98.870)
Epoch: [14][128/196]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.5350 (0.5820)	Acc@1 81.250 (80.099)	Acc@5 99.219 (98.843)
Epoch: [14][192/196]	Time 0.072 (0.071)	Data 0.000 (0.002)	Loss 0.6137 (0.5799)	Acc@1 78.516 (80.139)	Acc@5 97.656 (98.873)
after train
n1: 14 for:
wAcc: 60.83101283032876
test acc: 70.16
Epoche: [15/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [15][0/196]	Time 0.108 (0.108)	Data 0.269 (0.269)	Loss 0.6625 (0.6625)	Acc@1 79.688 (79.688)	Acc@5 97.656 (97.656)
Epoch: [15][64/196]	Time 0.069 (0.070)	Data 0.000 (0.004)	Loss 0.5492 (0.5759)	Acc@1 80.469 (80.048)	Acc@5 98.828 (99.002)
Epoch: [15][128/196]	Time 0.077 (0.072)	Data 0.000 (0.002)	Loss 0.6171 (0.5749)	Acc@1 77.344 (80.108)	Acc@5 98.828 (98.980)
Epoch: [15][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.4983 (0.5746)	Acc@1 82.031 (79.938)	Acc@5 100.000 (99.012)
after train
n1: 15 for:
wAcc: 61.287008827919564
test acc: 64.52
Epoche: [16/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [16][0/196]	Time 0.093 (0.093)	Data 0.245 (0.245)	Loss 0.4646 (0.4646)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [16][64/196]	Time 0.069 (0.072)	Data 0.000 (0.004)	Loss 0.5576 (0.5611)	Acc@1 82.422 (80.445)	Acc@5 99.609 (98.948)
Epoch: [16][128/196]	Time 0.072 (0.072)	Data 0.000 (0.002)	Loss 0.6363 (0.5625)	Acc@1 76.172 (80.484)	Acc@5 98.828 (98.952)
Epoch: [16][192/196]	Time 0.069 (0.072)	Data 0.000 (0.001)	Loss 0.6841 (0.5612)	Acc@1 76.172 (80.687)	Acc@5 98.438 (98.964)
after train
n1: 16 for:
wAcc: 60.99805358093013
test acc: 71.5
Epoche: [17/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [17][0/196]	Time 0.093 (0.093)	Data 0.269 (0.269)	Loss 0.5132 (0.5132)	Acc@1 83.984 (83.984)	Acc@5 98.438 (98.438)
Epoch: [17][64/196]	Time 0.067 (0.071)	Data 0.000 (0.004)	Loss 0.5306 (0.5626)	Acc@1 80.469 (80.998)	Acc@5 99.219 (98.798)
Epoch: [17][128/196]	Time 0.068 (0.071)	Data 0.000 (0.002)	Loss 0.6048 (0.5565)	Acc@1 78.125 (81.002)	Acc@5 99.609 (98.907)
Epoch: [17][192/196]	Time 0.072 (0.071)	Data 0.000 (0.002)	Loss 0.5898 (0.5611)	Acc@1 80.469 (80.787)	Acc@5 99.219 (98.903)
after train
n1: 17 for:
wAcc: 61.56650164365342
test acc: 75.78
Epoche: [18/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [18][0/196]	Time 0.094 (0.094)	Data 0.267 (0.267)	Loss 0.5961 (0.5961)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [18][64/196]	Time 0.070 (0.070)	Data 0.000 (0.004)	Loss 0.5946 (0.5397)	Acc@1 80.469 (81.292)	Acc@5 99.219 (99.099)
Epoch: [18][128/196]	Time 0.073 (0.070)	Data 0.000 (0.002)	Loss 0.5503 (0.5392)	Acc@1 81.641 (81.453)	Acc@5 98.828 (99.046)
Epoch: [18][192/196]	Time 0.070 (0.070)	Data 0.000 (0.002)	Loss 0.4978 (0.5489)	Acc@1 79.297 (81.084)	Acc@5 100.000 (99.010)
after train
n1: 18 for:
wAcc: 62.482648779439835
test acc: 72.09
Epoche: [19/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [19][0/196]	Time 0.118 (0.118)	Data 0.237 (0.237)	Loss 0.4577 (0.4577)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [19][64/196]	Time 0.073 (0.073)	Data 0.000 (0.004)	Loss 0.6104 (0.5259)	Acc@1 78.906 (81.695)	Acc@5 98.828 (99.159)
Epoch: [19][128/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.5126 (0.5415)	Acc@1 81.250 (81.211)	Acc@5 99.219 (99.052)
Epoch: [19][192/196]	Time 0.070 (0.071)	Data 0.000 (0.001)	Loss 0.5839 (0.5399)	Acc@1 76.562 (81.371)	Acc@5 98.828 (99.047)
after train
n1: 19 for:
wAcc: 62.86427399231252
test acc: 75.42
Epoche: [20/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [20][0/196]	Time 0.084 (0.084)	Data 0.266 (0.266)	Loss 0.4919 (0.4919)	Acc@1 83.594 (83.594)	Acc@5 100.000 (100.000)
Epoch: [20][64/196]	Time 0.076 (0.074)	Data 0.000 (0.004)	Loss 0.6322 (0.5337)	Acc@1 78.125 (81.809)	Acc@5 98.047 (99.069)
Epoch: [20][128/196]	Time 0.075 (0.073)	Data 0.000 (0.002)	Loss 0.5415 (0.5324)	Acc@1 82.031 (81.853)	Acc@5 97.656 (99.010)
Epoch: [20][192/196]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.5119 (0.5325)	Acc@1 79.688 (81.778)	Acc@5 99.609 (99.018)
after train
n1: 20 for:
wAcc: 63.5058823143488
test acc: 73.73
Epoche: [21/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [21][0/196]	Time 0.089 (0.089)	Data 0.268 (0.268)	Loss 0.5615 (0.5615)	Acc@1 80.469 (80.469)	Acc@5 99.609 (99.609)
Epoch: [21][64/196]	Time 0.070 (0.069)	Data 0.000 (0.004)	Loss 0.5641 (0.5276)	Acc@1 79.688 (81.905)	Acc@5 99.219 (99.111)
Epoch: [21][128/196]	Time 0.071 (0.070)	Data 0.000 (0.002)	Loss 0.5593 (0.5245)	Acc@1 81.250 (81.904)	Acc@5 98.828 (99.113)
Epoch: [21][192/196]	Time 0.071 (0.070)	Data 0.000 (0.002)	Loss 0.4208 (0.5216)	Acc@1 87.109 (82.013)	Acc@5 100.000 (99.132)
after train
n1: 21 for:
wAcc: 63.89211401650809
test acc: 73.24
Epoche: [22/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [22][0/196]	Time 0.090 (0.090)	Data 0.262 (0.262)	Loss 0.5667 (0.5667)	Acc@1 80.078 (80.078)	Acc@5 98.047 (98.047)
Epoch: [22][64/196]	Time 0.071 (0.073)	Data 0.000 (0.004)	Loss 0.5186 (0.5259)	Acc@1 82.812 (81.731)	Acc@5 99.609 (99.020)
Epoch: [22][128/196]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.4051 (0.5183)	Acc@1 85.938 (82.040)	Acc@5 99.219 (99.082)
Epoch: [22][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.5393 (0.5182)	Acc@1 84.766 (82.074)	Acc@5 99.219 (99.091)
after train
n1: 22 for:
wAcc: 64.18189298989884
test acc: 75.21
Epoche: [23/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [23][0/196]	Time 0.087 (0.087)	Data 0.274 (0.274)	Loss 0.6194 (0.6194)	Acc@1 76.953 (76.953)	Acc@5 98.828 (98.828)
Epoch: [23][64/196]	Time 0.067 (0.070)	Data 0.000 (0.004)	Loss 0.4819 (0.5198)	Acc@1 84.766 (82.079)	Acc@5 98.438 (99.105)
Epoch: [23][128/196]	Time 0.072 (0.070)	Data 0.000 (0.002)	Loss 0.5259 (0.5295)	Acc@1 80.859 (81.747)	Acc@5 99.219 (99.073)
Epoch: [23][192/196]	Time 0.069 (0.070)	Data 0.000 (0.002)	Loss 0.5849 (0.5256)	Acc@1 78.906 (81.898)	Acc@5 99.609 (99.122)
after train
n1: 23 for:
wAcc: 64.5992681613081
test acc: 65.28
Epoche: [24/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [24][0/196]	Time 0.106 (0.106)	Data 0.259 (0.259)	Loss 0.5481 (0.5481)	Acc@1 82.031 (82.031)	Acc@5 98.438 (98.438)
Epoch: [24][64/196]	Time 0.068 (0.072)	Data 0.000 (0.004)	Loss 0.4958 (0.5195)	Acc@1 82.812 (82.278)	Acc@5 99.609 (99.135)
Epoch: [24][128/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.4979 (0.5184)	Acc@1 83.984 (82.168)	Acc@5 99.219 (99.092)
Epoch: [24][192/196]	Time 0.066 (0.071)	Data 0.000 (0.002)	Loss 0.5067 (0.5178)	Acc@1 78.125 (82.173)	Acc@5 99.609 (99.097)
after train
n1: 24 for:
wAcc: 64.16633464987136
test acc: 66.05
Epoche: [25/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [25][0/196]	Time 0.101 (0.101)	Data 0.261 (0.261)	Loss 0.5585 (0.5585)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [25][64/196]	Time 0.068 (0.071)	Data 0.000 (0.004)	Loss 0.4417 (0.5097)	Acc@1 83.594 (82.518)	Acc@5 99.609 (99.008)
Epoch: [25][128/196]	Time 0.073 (0.072)	Data 0.000 (0.002)	Loss 0.4143 (0.5062)	Acc@1 85.938 (82.670)	Acc@5 99.609 (99.067)
Epoch: [25][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.5947 (0.5119)	Acc@1 80.078 (82.479)	Acc@5 99.219 (99.085)
after train
n1: 25 for:
wAcc: 63.86686448353203
test acc: 75.52
Epoche: [26/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [26][0/196]	Time 0.113 (0.113)	Data 0.240 (0.240)	Loss 0.6044 (0.6044)	Acc@1 79.297 (79.297)	Acc@5 98.828 (98.828)
Epoch: [26][64/196]	Time 0.070 (0.071)	Data 0.000 (0.004)	Loss 0.5550 (0.5023)	Acc@1 81.641 (82.999)	Acc@5 98.438 (99.189)
Epoch: [26][128/196]	Time 0.069 (0.070)	Data 0.000 (0.002)	Loss 0.4769 (0.5028)	Acc@1 81.641 (82.740)	Acc@5 98.438 (99.143)
Epoch: [26][192/196]	Time 0.070 (0.070)	Data 0.000 (0.001)	Loss 0.5088 (0.5046)	Acc@1 80.859 (82.572)	Acc@5 99.219 (99.138)
after train
n1: 26 for:
wAcc: 64.31880314436332
test acc: 73.97
Epoche: [27/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [27][0/196]	Time 0.099 (0.099)	Data 0.263 (0.263)	Loss 0.5523 (0.5523)	Acc@1 82.812 (82.812)	Acc@5 98.438 (98.438)
Epoch: [27][64/196]	Time 0.075 (0.074)	Data 0.000 (0.004)	Loss 0.5479 (0.4956)	Acc@1 80.469 (83.011)	Acc@5 99.219 (99.105)
Epoch: [27][128/196]	Time 0.076 (0.074)	Data 0.000 (0.002)	Loss 0.6113 (0.4986)	Acc@1 78.125 (82.952)	Acc@5 98.438 (99.134)
Epoch: [27][192/196]	Time 0.070 (0.074)	Data 0.000 (0.002)	Loss 0.4601 (0.5021)	Acc@1 83.594 (82.691)	Acc@5 99.609 (99.154)
after train
n1: 27 for:
wAcc: 64.60136014345879
test acc: 68.06
Epoche: [28/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [28][0/196]	Time 0.104 (0.104)	Data 0.254 (0.254)	Loss 0.4839 (0.4839)	Acc@1 81.250 (81.250)	Acc@5 99.219 (99.219)
Epoch: [28][64/196]	Time 0.074 (0.073)	Data 0.000 (0.004)	Loss 0.4872 (0.5005)	Acc@1 83.203 (82.819)	Acc@5 99.219 (99.147)
Epoch: [28][128/196]	Time 0.075 (0.074)	Data 0.000 (0.002)	Loss 0.5242 (0.4965)	Acc@1 80.859 (82.849)	Acc@5 99.219 (99.161)
Epoch: [28][192/196]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.5641 (0.4971)	Acc@1 79.688 (82.788)	Acc@5 99.219 (99.164)
after train
n1: 28 for:
wAcc: 64.44303262427024
test acc: 76.96
Epoche: [29/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [29][0/196]	Time 0.087 (0.087)	Data 0.266 (0.266)	Loss 0.4802 (0.4802)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [29][64/196]	Time 0.082 (0.069)	Data 0.000 (0.004)	Loss 0.4881 (0.4978)	Acc@1 82.422 (82.903)	Acc@5 100.000 (99.231)
Epoch: [29][128/196]	Time 0.074 (0.069)	Data 0.000 (0.002)	Loss 0.4738 (0.4943)	Acc@1 84.375 (83.067)	Acc@5 96.875 (99.231)
Epoch: [29][192/196]	Time 0.068 (0.070)	Data 0.000 (0.002)	Loss 0.4530 (0.4953)	Acc@1 83.984 (82.970)	Acc@5 99.609 (99.209)
after train
n1: 29 for:
wAcc: 64.9036252467487
test acc: 70.43
Epoche: [30/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [30][0/196]	Time 0.099 (0.099)	Data 0.269 (0.269)	Loss 0.4724 (0.4724)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [30][64/196]	Time 0.070 (0.072)	Data 0.000 (0.004)	Loss 0.6152 (0.5037)	Acc@1 79.297 (82.788)	Acc@5 99.609 (99.213)
Epoch: [30][128/196]	Time 0.067 (0.072)	Data 0.000 (0.002)	Loss 0.5673 (0.4927)	Acc@1 78.906 (83.040)	Acc@5 99.609 (99.252)
Epoch: [30][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.5072 (0.4972)	Acc@1 82.031 (82.827)	Acc@5 99.609 (99.229)
after train
n1: 30 for:
wAcc: 64.88831181947029
test acc: 79.84
Epoche: [31/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [31][0/196]	Time 0.092 (0.092)	Data 0.286 (0.286)	Loss 0.3816 (0.3816)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [31][64/196]	Time 0.079 (0.071)	Data 0.000 (0.005)	Loss 0.4072 (0.4936)	Acc@1 85.938 (82.776)	Acc@5 100.000 (99.165)
Epoch: [31][128/196]	Time 0.077 (0.072)	Data 0.000 (0.002)	Loss 0.5480 (0.4920)	Acc@1 83.203 (82.909)	Acc@5 98.438 (99.173)
Epoch: [31][192/196]	Time 0.075 (0.072)	Data 0.000 (0.002)	Loss 0.4651 (0.4903)	Acc@1 83.594 (83.003)	Acc@5 99.219 (99.160)
after train
n1: 30 for:
wAcc: 67.30433489809435
test acc: 69.69
Epoche: [32/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [32][0/196]	Time 0.110 (0.110)	Data 0.257 (0.257)	Loss 0.5744 (0.5744)	Acc@1 77.734 (77.734)	Acc@5 99.219 (99.219)
Epoch: [32][64/196]	Time 0.074 (0.071)	Data 0.000 (0.004)	Loss 0.5305 (0.4995)	Acc@1 79.688 (82.728)	Acc@5 99.219 (99.177)
Epoch: [32][128/196]	Time 0.073 (0.072)	Data 0.000 (0.002)	Loss 0.5482 (0.4925)	Acc@1 81.641 (83.034)	Acc@5 98.828 (99.228)
Epoch: [32][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.4631 (0.4907)	Acc@1 84.375 (83.096)	Acc@5 98.828 (99.201)
after train
n1: 30 for:
wAcc: 66.96818509658318
test acc: 79.07
Epoche: [33/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [33][0/196]	Time 0.103 (0.103)	Data 0.274 (0.274)	Loss 0.4726 (0.4726)	Acc@1 83.203 (83.203)	Acc@5 99.219 (99.219)
Epoch: [33][64/196]	Time 0.072 (0.073)	Data 0.000 (0.004)	Loss 0.5194 (0.4788)	Acc@1 82.031 (83.263)	Acc@5 99.609 (99.219)
Epoch: [33][128/196]	Time 0.076 (0.072)	Data 0.000 (0.002)	Loss 0.4616 (0.4866)	Acc@1 84.375 (83.355)	Acc@5 100.000 (99.158)
Epoch: [33][192/196]	Time 0.068 (0.072)	Data 0.000 (0.002)	Loss 0.4328 (0.4859)	Acc@1 85.547 (83.306)	Acc@5 98.828 (99.182)
after train
n1: 30 for:
wAcc: 69.81473200141787
test acc: 64.06
Epoche: [34/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [34][0/196]	Time 0.105 (0.105)	Data 0.274 (0.274)	Loss 0.6001 (0.6001)	Acc@1 78.516 (78.516)	Acc@5 96.875 (96.875)
Epoch: [34][64/196]	Time 0.074 (0.072)	Data 0.000 (0.004)	Loss 0.4632 (0.4833)	Acc@1 85.938 (83.636)	Acc@5 99.219 (99.201)
Epoch: [34][128/196]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.5643 (0.4806)	Acc@1 81.250 (83.527)	Acc@5 98.828 (99.243)
Epoch: [34][192/196]	Time 0.068 (0.072)	Data 0.000 (0.002)	Loss 0.5101 (0.4842)	Acc@1 81.250 (83.278)	Acc@5 99.609 (99.235)
after train
n1: 30 for:
wAcc: 71.00183255614701
test acc: 80.27
Epoche: [35/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [35][0/196]	Time 0.128 (0.128)	Data 0.328 (0.328)	Loss 0.3339 (0.3339)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [35][64/196]	Time 0.068 (0.072)	Data 0.000 (0.005)	Loss 0.4674 (0.4750)	Acc@1 83.203 (83.257)	Acc@5 98.047 (99.345)
Epoch: [35][128/196]	Time 0.070 (0.072)	Data 0.000 (0.003)	Loss 0.4922 (0.4795)	Acc@1 83.594 (83.358)	Acc@5 99.219 (99.310)
Epoch: [35][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.4387 (0.4758)	Acc@1 86.328 (83.578)	Acc@5 99.609 (99.273)
after train
n1: 30 for:
wAcc: 71.04321684746195
test acc: 72.56
Epoche: [36/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [36][0/196]	Time 0.102 (0.102)	Data 0.272 (0.272)	Loss 0.4367 (0.4367)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [36][64/196]	Time 0.076 (0.073)	Data 0.000 (0.004)	Loss 0.3696 (0.4697)	Acc@1 86.719 (83.702)	Acc@5 99.609 (99.333)
Epoch: [36][128/196]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.4578 (0.4756)	Acc@1 85.156 (83.588)	Acc@5 99.219 (99.270)
Epoch: [36][192/196]	Time 0.068 (0.072)	Data 0.000 (0.002)	Loss 0.5399 (0.4768)	Acc@1 81.641 (83.588)	Acc@5 99.219 (99.227)
after train
n1: 30 for:
wAcc: 71.99109578163345
test acc: 77.24
Epoche: [37/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [37][0/196]	Time 0.119 (0.119)	Data 0.262 (0.262)	Loss 0.5185 (0.5185)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [37][64/196]	Time 0.068 (0.072)	Data 0.000 (0.004)	Loss 0.4661 (0.4758)	Acc@1 82.812 (83.558)	Acc@5 99.219 (99.255)
Epoch: [37][128/196]	Time 0.076 (0.073)	Data 0.000 (0.002)	Loss 0.4437 (0.4729)	Acc@1 84.375 (83.736)	Acc@5 98.828 (99.228)
Epoch: [37][192/196]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.4577 (0.4719)	Acc@1 83.203 (83.697)	Acc@5 100.000 (99.296)
after train
n1: 30 for:
wAcc: 71.49416896259946
test acc: 78.35
Epoche: [38/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [38][0/196]	Time 0.107 (0.107)	Data 0.264 (0.264)	Loss 0.4628 (0.4628)	Acc@1 84.375 (84.375)	Acc@5 98.828 (98.828)
Epoch: [38][64/196]	Time 0.070 (0.073)	Data 0.000 (0.004)	Loss 0.4269 (0.4664)	Acc@1 84.766 (83.816)	Acc@5 99.219 (99.249)
Epoch: [38][128/196]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.4455 (0.4719)	Acc@1 82.812 (83.642)	Acc@5 100.000 (99.255)
Epoch: [38][192/196]	Time 0.068 (0.073)	Data 0.000 (0.002)	Loss 0.4919 (0.4729)	Acc@1 82.812 (83.574)	Acc@5 98.828 (99.269)
after train
n1: 30 for:
wAcc: 73.70446848751266
test acc: 74.03
Epoche: [39/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [39][0/196]	Time 0.102 (0.102)	Data 0.254 (0.254)	Loss 0.5270 (0.5270)	Acc@1 80.859 (80.859)	Acc@5 99.609 (99.609)
Epoch: [39][64/196]	Time 0.066 (0.072)	Data 0.000 (0.004)	Loss 0.4893 (0.4615)	Acc@1 84.375 (84.219)	Acc@5 98.438 (99.321)
Epoch: [39][128/196]	Time 0.072 (0.071)	Data 0.000 (0.002)	Loss 0.5440 (0.4656)	Acc@1 82.031 (83.996)	Acc@5 99.609 (99.243)
Epoch: [39][192/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.4441 (0.4718)	Acc@1 84.766 (83.683)	Acc@5 98.438 (99.215)
after train
n1: 30 for:
wAcc: 72.7771467050628
test acc: 74.21
Epoche: [40/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [40][0/196]	Time 0.105 (0.105)	Data 0.258 (0.258)	Loss 0.5253 (0.5253)	Acc@1 81.250 (81.250)	Acc@5 98.438 (98.438)
Epoch: [40][64/196]	Time 0.070 (0.070)	Data 0.000 (0.004)	Loss 0.3798 (0.4648)	Acc@1 87.109 (84.111)	Acc@5 98.438 (99.225)
Epoch: [40][128/196]	Time 0.070 (0.070)	Data 0.000 (0.002)	Loss 0.4677 (0.4669)	Acc@1 83.203 (83.969)	Acc@5 100.000 (99.210)
Epoch: [40][192/196]	Time 0.071 (0.070)	Data 0.000 (0.002)	Loss 0.4466 (0.4675)	Acc@1 84.766 (83.958)	Acc@5 99.609 (99.243)
after train
n1: 30 for:
wAcc: 73.33218583630061
test acc: 73.36
Epoche: [41/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [41][0/196]	Time 0.094 (0.094)	Data 0.260 (0.260)	Loss 0.4139 (0.4139)	Acc@1 84.375 (84.375)	Acc@5 98.438 (98.438)
Epoch: [41][64/196]	Time 0.066 (0.071)	Data 0.000 (0.004)	Loss 0.5067 (0.4562)	Acc@1 82.812 (84.237)	Acc@5 97.656 (99.261)
Epoch: [41][128/196]	Time 0.078 (0.071)	Data 0.000 (0.002)	Loss 0.4764 (0.4637)	Acc@1 84.766 (84.054)	Acc@5 99.219 (99.188)
Epoch: [41][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.4200 (0.4652)	Acc@1 87.500 (83.950)	Acc@5 100.000 (99.233)
after train
n1: 30 for:
wAcc: 72.99715187009879
test acc: 78.65
Epoche: [42/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [42][0/196]	Time 0.091 (0.091)	Data 0.258 (0.258)	Loss 0.4931 (0.4931)	Acc@1 83.594 (83.594)	Acc@5 98.828 (98.828)
Epoch: [42][64/196]	Time 0.070 (0.070)	Data 0.000 (0.004)	Loss 0.4310 (0.4689)	Acc@1 85.547 (83.708)	Acc@5 99.219 (99.111)
Epoch: [42][128/196]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 0.4727 (0.4648)	Acc@1 81.641 (83.933)	Acc@5 99.609 (99.228)
Epoch: [42][192/196]	Time 0.070 (0.070)	Data 0.000 (0.002)	Loss 0.4128 (0.4686)	Acc@1 85.156 (83.782)	Acc@5 98.828 (99.227)
after train
n1: 30 for:
wAcc: 72.95852500473951
test acc: 78.75
Epoche: [43/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [43][0/196]	Time 0.109 (0.109)	Data 0.236 (0.236)	Loss 0.4459 (0.4459)	Acc@1 82.422 (82.422)	Acc@5 99.609 (99.609)
Epoch: [43][64/196]	Time 0.071 (0.073)	Data 0.000 (0.004)	Loss 0.4472 (0.4519)	Acc@1 85.156 (84.489)	Acc@5 99.609 (99.363)
Epoch: [43][128/196]	Time 0.065 (0.072)	Data 0.000 (0.002)	Loss 0.5360 (0.4615)	Acc@1 80.859 (84.069)	Acc@5 99.219 (99.270)
Epoch: [43][192/196]	Time 0.070 (0.072)	Data 0.000 (0.001)	Loss 0.4208 (0.4643)	Acc@1 87.500 (84.049)	Acc@5 99.609 (99.253)
after train
n1: 30 for:
wAcc: 73.78898307370845
test acc: 78.91
Epoche: [44/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [44][0/196]	Time 0.095 (0.095)	Data 0.273 (0.273)	Loss 0.4543 (0.4543)	Acc@1 85.156 (85.156)	Acc@5 99.219 (99.219)
Epoch: [44][64/196]	Time 0.072 (0.072)	Data 0.000 (0.004)	Loss 0.3935 (0.4511)	Acc@1 87.500 (84.567)	Acc@5 98.828 (99.279)
Epoch: [44][128/196]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.5135 (0.4601)	Acc@1 83.594 (84.354)	Acc@5 98.828 (99.252)
Epoch: [44][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.4376 (0.4660)	Acc@1 84.375 (84.051)	Acc@5 100.000 (99.247)
after train
n1: 30 for:
wAcc: 73.30404407965139
test acc: 71.93
Epoche: [45/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [45][0/196]	Time 0.084 (0.084)	Data 0.256 (0.256)	Loss 0.4345 (0.4345)	Acc@1 82.812 (82.812)	Acc@5 99.609 (99.609)
Epoch: [45][64/196]	Time 0.075 (0.071)	Data 0.000 (0.004)	Loss 0.4330 (0.4407)	Acc@1 83.984 (84.874)	Acc@5 99.609 (99.333)
Epoch: [45][128/196]	Time 0.078 (0.071)	Data 0.000 (0.002)	Loss 0.5003 (0.4609)	Acc@1 82.031 (84.084)	Acc@5 100.000 (99.319)
Epoch: [45][192/196]	Time 0.068 (0.072)	Data 0.000 (0.002)	Loss 0.4591 (0.4633)	Acc@1 83.984 (84.073)	Acc@5 99.609 (99.316)
after train
n1: 30 for:
wAcc: 74.2244357440682
test acc: 78.91
Epoche: [46/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [46][0/196]	Time 0.101 (0.101)	Data 0.239 (0.239)	Loss 0.5191 (0.5191)	Acc@1 83.594 (83.594)	Acc@5 98.828 (98.828)
Epoch: [46][64/196]	Time 0.073 (0.072)	Data 0.000 (0.004)	Loss 0.4469 (0.4554)	Acc@1 84.766 (84.327)	Acc@5 100.000 (99.339)
Epoch: [46][128/196]	Time 0.074 (0.072)	Data 0.000 (0.002)	Loss 0.3867 (0.4542)	Acc@1 88.672 (84.442)	Acc@5 99.609 (99.352)
Epoch: [46][192/196]	Time 0.070 (0.072)	Data 0.000 (0.001)	Loss 0.4646 (0.4639)	Acc@1 81.641 (83.962)	Acc@5 99.609 (99.312)
after train
n1: 30 for:
wAcc: 75.14545367719252
test acc: 72.82
Epoche: [47/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [47][0/196]	Time 0.097 (0.097)	Data 0.254 (0.254)	Loss 0.4004 (0.4004)	Acc@1 85.938 (85.938)	Acc@5 99.219 (99.219)
Epoch: [47][64/196]	Time 0.068 (0.071)	Data 0.000 (0.004)	Loss 0.4648 (0.4620)	Acc@1 85.156 (83.798)	Acc@5 99.219 (99.369)
Epoch: [47][128/196]	Time 0.071 (0.071)	Data 0.000 (0.002)	Loss 0.4117 (0.4545)	Acc@1 85.156 (84.142)	Acc@5 99.219 (99.367)
Epoch: [47][192/196]	Time 0.068 (0.072)	Data 0.000 (0.002)	Loss 0.4126 (0.4603)	Acc@1 86.328 (84.005)	Acc@5 99.609 (99.304)
after train
n1: 30 for:
wAcc: 74.46199226146985
test acc: 72.38
Epoche: [48/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [48][0/196]	Time 0.111 (0.111)	Data 0.255 (0.255)	Loss 0.4816 (0.4816)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [48][64/196]	Time 0.072 (0.074)	Data 0.000 (0.004)	Loss 0.4877 (0.4510)	Acc@1 84.766 (84.555)	Acc@5 99.219 (99.375)
Epoch: [48][128/196]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.3794 (0.4524)	Acc@1 85.938 (84.378)	Acc@5 99.609 (99.304)
Epoch: [48][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.4579 (0.4564)	Acc@1 82.422 (84.343)	Acc@5 100.000 (99.292)
after train
n1: 30 for:
wAcc: 74.80906016570364
test acc: 72.2
Epoche: [49/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [49][0/196]	Time 0.089 (0.089)	Data 0.258 (0.258)	Loss 0.3931 (0.3931)	Acc@1 87.891 (87.891)	Acc@5 99.609 (99.609)
Epoch: [49][64/196]	Time 0.072 (0.072)	Data 0.000 (0.004)	Loss 0.4812 (0.4417)	Acc@1 84.375 (84.772)	Acc@5 100.000 (99.369)
Epoch: [49][128/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.4240 (0.4523)	Acc@1 87.891 (84.445)	Acc@5 99.609 (99.310)
Epoch: [49][192/196]	Time 0.071 (0.073)	Data 0.000 (0.002)	Loss 0.4191 (0.4539)	Acc@1 85.547 (84.442)	Acc@5 99.609 (99.275)
after train
n1: 30 for:
wAcc: 74.39642467165964
test acc: 78.36
Epoche: [50/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [50][0/196]	Time 0.115 (0.115)	Data 0.254 (0.254)	Loss 0.3605 (0.3605)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [50][64/196]	Time 0.078 (0.074)	Data 0.000 (0.004)	Loss 0.4308 (0.4401)	Acc@1 84.766 (84.627)	Acc@5 99.609 (99.381)
Epoch: [50][128/196]	Time 0.079 (0.074)	Data 0.000 (0.002)	Loss 0.4344 (0.4482)	Acc@1 85.156 (84.345)	Acc@5 99.609 (99.316)
Epoch: [50][192/196]	Time 0.075 (0.073)	Data 0.000 (0.002)	Loss 0.3846 (0.4525)	Acc@1 88.281 (84.258)	Acc@5 100.000 (99.300)
after train
n1: 30 for:
wAcc: 74.58130404592292
test acc: 73.7
Epoche: [51/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [51][0/196]	Time 0.083 (0.083)	Data 0.246 (0.246)	Loss 0.4234 (0.4234)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [51][64/196]	Time 0.076 (0.071)	Data 0.000 (0.004)	Loss 0.3965 (0.4349)	Acc@1 85.156 (84.898)	Acc@5 100.000 (99.393)
Epoch: [51][128/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.3886 (0.4455)	Acc@1 84.766 (84.569)	Acc@5 100.000 (99.334)
Epoch: [51][192/196]	Time 0.071 (0.072)	Data 0.000 (0.001)	Loss 0.4683 (0.4473)	Acc@1 84.766 (84.452)	Acc@5 99.219 (99.360)
after train
n1: 30 for:
wAcc: 74.80923198814806
test acc: 79.33
Epoche: [52/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [52][0/196]	Time 0.080 (0.080)	Data 0.277 (0.277)	Loss 0.4630 (0.4630)	Acc@1 83.203 (83.203)	Acc@5 99.609 (99.609)
Epoch: [52][64/196]	Time 0.073 (0.071)	Data 0.000 (0.004)	Loss 0.4815 (0.4509)	Acc@1 83.984 (84.549)	Acc@5 99.609 (99.291)
Epoch: [52][128/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.4627 (0.4493)	Acc@1 80.859 (84.542)	Acc@5 99.609 (99.331)
Epoch: [52][192/196]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 0.3878 (0.4538)	Acc@1 85.156 (84.332)	Acc@5 99.609 (99.328)
after train
n1: 30 for:
wAcc: 73.66539817710338
test acc: 74.59
Epoche: [53/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [53][0/196]	Time 0.107 (0.107)	Data 0.256 (0.256)	Loss 0.4350 (0.4350)	Acc@1 84.766 (84.766)	Acc@5 99.219 (99.219)
Epoch: [53][64/196]	Time 0.070 (0.071)	Data 0.000 (0.004)	Loss 0.4204 (0.4508)	Acc@1 86.719 (84.543)	Acc@5 99.609 (99.393)
Epoch: [53][128/196]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.4778 (0.4493)	Acc@1 82.812 (84.581)	Acc@5 100.000 (99.397)
Epoch: [53][192/196]	Time 0.071 (0.071)	Data 0.000 (0.002)	Loss 0.4954 (0.4564)	Acc@1 81.250 (84.357)	Acc@5 99.609 (99.362)
after train
n1: 30 for:
wAcc: 73.8363623066899
test acc: 69.59
Epoche: [54/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [54][0/196]	Time 0.115 (0.115)	Data 0.244 (0.244)	Loss 0.4678 (0.4678)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [54][64/196]	Time 0.070 (0.073)	Data 0.000 (0.004)	Loss 0.5428 (0.4478)	Acc@1 80.078 (84.615)	Acc@5 98.828 (99.291)
Epoch: [54][128/196]	Time 0.069 (0.073)	Data 0.000 (0.002)	Loss 0.5042 (0.4496)	Acc@1 81.641 (84.545)	Acc@5 98.828 (99.331)
Epoch: [54][192/196]	Time 0.070 (0.073)	Data 0.000 (0.001)	Loss 0.4819 (0.4515)	Acc@1 83.203 (84.468)	Acc@5 98.828 (99.324)
after train
n1: 30 for:
wAcc: 74.93140139528425
test acc: 69.66
Epoche: [55/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [55][0/196]	Time 0.085 (0.085)	Data 0.264 (0.264)	Loss 0.3957 (0.3957)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [55][64/196]	Time 0.073 (0.072)	Data 0.000 (0.004)	Loss 0.4426 (0.4492)	Acc@1 85.547 (84.471)	Acc@5 98.828 (99.153)
Epoch: [55][128/196]	Time 0.074 (0.072)	Data 0.000 (0.002)	Loss 0.3375 (0.4470)	Acc@1 88.281 (84.529)	Acc@5 99.609 (99.276)
Epoch: [55][192/196]	Time 0.068 (0.072)	Data 0.000 (0.002)	Loss 0.4420 (0.4474)	Acc@1 82.422 (84.456)	Acc@5 99.219 (99.300)
after train
n1: 30 for:
wAcc: 74.36724056895875
test acc: 78.74
Epoche: [56/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [56][0/196]	Time 0.105 (0.105)	Data 0.267 (0.267)	Loss 0.4467 (0.4467)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [56][64/196]	Time 0.071 (0.072)	Data 0.000 (0.004)	Loss 0.3535 (0.4429)	Acc@1 87.891 (84.651)	Acc@5 100.000 (99.453)
Epoch: [56][128/196]	Time 0.072 (0.072)	Data 0.000 (0.002)	Loss 0.4301 (0.4444)	Acc@1 85.547 (84.514)	Acc@5 99.219 (99.370)
Epoch: [56][192/196]	Time 0.072 (0.072)	Data 0.000 (0.002)	Loss 0.5228 (0.4477)	Acc@1 79.688 (84.539)	Acc@5 98.438 (99.348)
after train
n1: 30 for:
wAcc: 73.79499527733317
test acc: 75.26
Epoche: [57/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [57][0/196]	Time 0.092 (0.092)	Data 0.267 (0.267)	Loss 0.4417 (0.4417)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [57][64/196]	Time 0.070 (0.073)	Data 0.000 (0.004)	Loss 0.4306 (0.4629)	Acc@1 83.594 (84.056)	Acc@5 100.000 (99.315)
Epoch: [57][128/196]	Time 0.067 (0.072)	Data 0.000 (0.002)	Loss 0.4741 (0.4459)	Acc@1 84.766 (84.566)	Acc@5 99.609 (99.413)
Epoch: [57][192/196]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.4099 (0.4516)	Acc@1 86.328 (84.413)	Acc@5 98.828 (99.373)
after train
n1: 30 for:
wAcc: 75.17610957051562
test acc: 76.34
Epoche: [58/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [58][0/196]	Time 0.092 (0.092)	Data 0.288 (0.288)	Loss 0.4156 (0.4156)	Acc@1 86.719 (86.719)	Acc@5 98.828 (98.828)
Epoch: [58][64/196]	Time 0.070 (0.072)	Data 0.000 (0.005)	Loss 0.4187 (0.4238)	Acc@1 85.156 (85.216)	Acc@5 99.609 (99.423)
Epoch: [58][128/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.4562 (0.4366)	Acc@1 85.938 (84.908)	Acc@5 100.000 (99.379)
Epoch: [58][192/196]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.4499 (0.4367)	Acc@1 85.156 (84.958)	Acc@5 98.828 (99.348)
after train
n1: 30 for:
wAcc: 74.30721230684735
test acc: 80.42
Epoche: [59/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [59][0/196]	Time 0.105 (0.105)	Data 0.274 (0.274)	Loss 0.4161 (0.4161)	Acc@1 85.547 (85.547)	Acc@5 98.828 (98.828)
Epoch: [59][64/196]	Time 0.070 (0.071)	Data 0.000 (0.004)	Loss 0.4203 (0.4449)	Acc@1 83.203 (84.489)	Acc@5 99.609 (99.273)
Epoch: [59][128/196]	Time 0.078 (0.071)	Data 0.000 (0.002)	Loss 0.5765 (0.4485)	Acc@1 81.250 (84.520)	Acc@5 99.219 (99.279)
Epoch: [59][192/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.4612 (0.4441)	Acc@1 85.156 (84.670)	Acc@5 99.219 (99.336)
after train
n1: 30 for:
wAcc: 76.06190996006151
test acc: 75.28
Epoche: [60/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [60][0/196]	Time 0.091 (0.091)	Data 0.244 (0.244)	Loss 0.4375 (0.4375)	Acc@1 84.766 (84.766)	Acc@5 99.609 (99.609)
Epoch: [60][64/196]	Time 0.078 (0.073)	Data 0.000 (0.004)	Loss 0.4677 (0.4459)	Acc@1 82.031 (84.609)	Acc@5 99.219 (99.171)
Epoch: [60][128/196]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.4975 (0.4450)	Acc@1 83.594 (84.626)	Acc@5 99.219 (99.273)
Epoch: [60][192/196]	Time 0.070 (0.072)	Data 0.000 (0.001)	Loss 0.4730 (0.4432)	Acc@1 84.375 (84.784)	Acc@5 98.828 (99.290)
after train
n1: 30 for:
wAcc: 74.5441643501709
test acc: 80.47
Epoche: [61/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [61][0/196]	Time 0.095 (0.095)	Data 0.272 (0.272)	Loss 0.3299 (0.3299)	Acc@1 87.500 (87.500)	Acc@5 99.609 (99.609)
Epoch: [61][64/196]	Time 0.073 (0.072)	Data 0.000 (0.004)	Loss 0.4533 (0.4322)	Acc@1 83.984 (85.300)	Acc@5 98.828 (99.387)
Epoch: [61][128/196]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.4650 (0.4304)	Acc@1 81.641 (85.256)	Acc@5 100.000 (99.388)
Epoch: [61][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.5884 (0.4388)	Acc@1 79.688 (84.901)	Acc@5 99.609 (99.358)
after train
n1: 30 for:
wAcc: 76.28246373451785
test acc: 77.45
Epoche: [62/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [62][0/196]	Time 0.088 (0.088)	Data 0.258 (0.258)	Loss 0.4401 (0.4401)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [62][64/196]	Time 0.076 (0.073)	Data 0.000 (0.004)	Loss 0.5270 (0.4570)	Acc@1 82.422 (84.117)	Acc@5 98.828 (99.315)
Epoch: [62][128/196]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.4501 (0.4572)	Acc@1 85.938 (84.272)	Acc@5 99.609 (99.270)
Epoch: [62][192/196]	Time 0.074 (0.072)	Data 0.000 (0.002)	Loss 0.4844 (0.4515)	Acc@1 80.078 (84.446)	Acc@5 99.609 (99.290)
after train
n1: 30 for:
wAcc: 74.18791968065528
test acc: 74.78
Epoche: [63/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [63][0/196]	Time 0.096 (0.096)	Data 0.241 (0.241)	Loss 0.4917 (0.4917)	Acc@1 82.031 (82.031)	Acc@5 98.828 (98.828)
Epoch: [63][64/196]	Time 0.068 (0.072)	Data 0.000 (0.004)	Loss 0.4581 (0.4470)	Acc@1 85.547 (84.615)	Acc@5 99.219 (99.453)
Epoch: [63][128/196]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 0.4966 (0.4403)	Acc@1 85.547 (84.878)	Acc@5 99.609 (99.400)
Epoch: [63][192/196]	Time 0.069 (0.071)	Data 0.000 (0.001)	Loss 0.4157 (0.4409)	Acc@1 87.891 (84.784)	Acc@5 98.047 (99.383)
after train
n1: 30 for:
wAcc: 76.56946125384336
test acc: 77.52
Epoche: [64/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [64][0/196]	Time 0.102 (0.102)	Data 0.280 (0.280)	Loss 0.4399 (0.4399)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [64][64/196]	Time 0.072 (0.071)	Data 0.000 (0.005)	Loss 0.3916 (0.4318)	Acc@1 88.672 (85.054)	Acc@5 99.609 (99.417)
Epoch: [64][128/196]	Time 0.074 (0.072)	Data 0.000 (0.002)	Loss 0.4529 (0.4384)	Acc@1 84.766 (84.829)	Acc@5 100.000 (99.340)
Epoch: [64][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.5090 (0.4372)	Acc@1 82.031 (84.936)	Acc@5 100.000 (99.356)
after train
n1: 30 for:
wAcc: 75.51621672789764
test acc: 67.25
Epoche: [65/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [65][0/196]	Time 0.104 (0.104)	Data 0.274 (0.274)	Loss 0.3213 (0.3213)	Acc@1 87.891 (87.891)	Acc@5 100.000 (100.000)
Epoch: [65][64/196]	Time 0.080 (0.072)	Data 0.000 (0.004)	Loss 0.4839 (0.4325)	Acc@1 83.203 (85.150)	Acc@5 99.609 (99.375)
Epoch: [65][128/196]	Time 0.077 (0.072)	Data 0.000 (0.002)	Loss 0.5171 (0.4349)	Acc@1 79.297 (84.978)	Acc@5 98.438 (99.388)
Epoch: [65][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.4148 (0.4414)	Acc@1 86.719 (84.707)	Acc@5 99.609 (99.328)
after train
n1: 30 for:
wAcc: 75.65946051076887
test acc: 78.72
Epoche: [66/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [66][0/196]	Time 0.103 (0.103)	Data 0.273 (0.273)	Loss 0.3913 (0.3913)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [66][64/196]	Time 0.070 (0.071)	Data 0.000 (0.004)	Loss 0.4633 (0.4496)	Acc@1 84.375 (84.603)	Acc@5 99.219 (99.297)
Epoch: [66][128/196]	Time 0.071 (0.071)	Data 0.000 (0.002)	Loss 0.4561 (0.4410)	Acc@1 83.984 (84.835)	Acc@5 98.828 (99.310)
Epoch: [66][192/196]	Time 0.068 (0.071)	Data 0.000 (0.002)	Loss 0.4339 (0.4425)	Acc@1 83.594 (84.772)	Acc@5 98.828 (99.340)
after train
n1: 30 for:
wAcc: 76.01737799990406
test acc: 73.35
Epoche: [67/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [67][0/196]	Time 0.110 (0.110)	Data 0.259 (0.259)	Loss 0.3732 (0.3732)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [67][64/196]	Time 0.067 (0.073)	Data 0.000 (0.004)	Loss 0.3522 (0.4295)	Acc@1 87.891 (84.946)	Acc@5 99.609 (99.267)
Epoch: [67][128/196]	Time 0.066 (0.072)	Data 0.000 (0.002)	Loss 0.3849 (0.4272)	Acc@1 86.328 (85.165)	Acc@5 100.000 (99.337)
Epoch: [67][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.4304 (0.4314)	Acc@1 85.156 (85.059)	Acc@5 98.828 (99.316)
after train
n1: 30 for:
wAcc: 75.22078316939503
test acc: 79.41
Epoche: [68/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [68][0/196]	Time 0.101 (0.101)	Data 0.254 (0.254)	Loss 0.3280 (0.3280)	Acc@1 90.234 (90.234)	Acc@5 99.609 (99.609)
Epoch: [68][64/196]	Time 0.072 (0.073)	Data 0.000 (0.004)	Loss 0.4245 (0.4254)	Acc@1 85.547 (85.535)	Acc@5 99.609 (99.381)
Epoch: [68][128/196]	Time 0.074 (0.073)	Data 0.000 (0.002)	Loss 0.3601 (0.4307)	Acc@1 84.375 (85.199)	Acc@5 100.000 (99.358)
Epoch: [68][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.4415 (0.4305)	Acc@1 83.594 (85.266)	Acc@5 100.000 (99.352)
after train
n1: 30 for:
wAcc: 75.51707630328617
test acc: 77.12
Epoche: [69/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [69][0/196]	Time 0.108 (0.108)	Data 0.270 (0.270)	Loss 0.3534 (0.3534)	Acc@1 88.672 (88.672)	Acc@5 99.219 (99.219)
Epoch: [69][64/196]	Time 0.070 (0.071)	Data 0.000 (0.004)	Loss 0.4108 (0.4392)	Acc@1 85.938 (84.874)	Acc@5 98.828 (99.321)
Epoch: [69][128/196]	Time 0.069 (0.070)	Data 0.000 (0.002)	Loss 0.4001 (0.4350)	Acc@1 84.766 (85.132)	Acc@5 100.000 (99.319)
Epoch: [69][192/196]	Time 0.070 (0.070)	Data 0.000 (0.002)	Loss 0.4861 (0.4345)	Acc@1 82.031 (85.069)	Acc@5 99.609 (99.312)
after train
n1: 30 for:
wAcc: 75.49761341167574
test acc: 72.05
Epoche: [70/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [70][0/196]	Time 0.101 (0.101)	Data 0.252 (0.252)	Loss 0.5146 (0.5146)	Acc@1 82.422 (82.422)	Acc@5 99.609 (99.609)
Epoch: [70][64/196]	Time 0.076 (0.071)	Data 0.000 (0.004)	Loss 0.4783 (0.4300)	Acc@1 81.641 (85.072)	Acc@5 99.609 (99.405)
Epoch: [70][128/196]	Time 0.073 (0.071)	Data 0.000 (0.002)	Loss 0.5244 (0.4391)	Acc@1 82.812 (84.866)	Acc@5 99.219 (99.355)
Epoch: [70][192/196]	Time 0.070 (0.072)	Data 0.000 (0.001)	Loss 0.4347 (0.4381)	Acc@1 85.547 (84.847)	Acc@5 99.609 (99.383)
after train
n1: 30 for:
wAcc: 76.0399173777698
test acc: 80.27
Epoche: [71/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [71][0/196]	Time 0.094 (0.094)	Data 0.260 (0.260)	Loss 0.4383 (0.4383)	Acc@1 83.984 (83.984)	Acc@5 99.219 (99.219)
Epoch: [71][64/196]	Time 0.069 (0.071)	Data 0.000 (0.004)	Loss 0.5162 (0.4343)	Acc@1 81.250 (84.874)	Acc@5 98.828 (99.381)
Epoch: [71][128/196]	Time 0.067 (0.071)	Data 0.000 (0.002)	Loss 0.4669 (0.4309)	Acc@1 83.594 (85.090)	Acc@5 99.219 (99.373)
Epoch: [71][192/196]	Time 0.066 (0.072)	Data 0.000 (0.002)	Loss 0.4128 (0.4324)	Acc@1 86.328 (85.067)	Acc@5 99.219 (99.375)
after train
n1: 30 for:
wAcc: 76.327282089767
test acc: 78.11
Epoche: [72/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [72][0/196]	Time 0.110 (0.110)	Data 0.235 (0.235)	Loss 0.4444 (0.4444)	Acc@1 83.594 (83.594)	Acc@5 98.828 (98.828)
Epoch: [72][64/196]	Time 0.067 (0.070)	Data 0.000 (0.004)	Loss 0.4313 (0.4289)	Acc@1 85.156 (85.042)	Acc@5 99.219 (99.429)
Epoch: [72][128/196]	Time 0.072 (0.071)	Data 0.000 (0.002)	Loss 0.4711 (0.4344)	Acc@1 83.984 (85.102)	Acc@5 98.438 (99.364)
Epoch: [72][192/196]	Time 0.070 (0.071)	Data 0.000 (0.001)	Loss 0.4608 (0.4342)	Acc@1 85.156 (85.100)	Acc@5 99.219 (99.360)
after train
n1: 30 for:
wAcc: 76.46542599765058
test acc: 75.82
Epoche: [73/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [73][0/196]	Time 0.108 (0.108)	Data 0.264 (0.264)	Loss 0.4269 (0.4269)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [73][64/196]	Time 0.067 (0.070)	Data 0.000 (0.004)	Loss 0.4206 (0.4315)	Acc@1 87.500 (85.018)	Acc@5 98.828 (99.267)
Epoch: [73][128/196]	Time 0.066 (0.069)	Data 0.000 (0.002)	Loss 0.4326 (0.4355)	Acc@1 84.375 (84.887)	Acc@5 99.609 (99.331)
Epoch: [73][192/196]	Time 0.069 (0.070)	Data 0.000 (0.002)	Loss 0.3968 (0.4322)	Acc@1 87.500 (85.059)	Acc@5 100.000 (99.350)
after train
n1: 30 for:
wAcc: 75.41474594114977
test acc: 73.36
Epoche: [74/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [74][0/196]	Time 0.113 (0.113)	Data 0.280 (0.280)	Loss 0.4430 (0.4430)	Acc@1 85.938 (85.938)	Acc@5 99.609 (99.609)
Epoch: [74][64/196]	Time 0.067 (0.072)	Data 0.000 (0.004)	Loss 0.4385 (0.4408)	Acc@1 85.938 (84.736)	Acc@5 99.609 (99.255)
Epoch: [74][128/196]	Time 0.066 (0.071)	Data 0.000 (0.002)	Loss 0.5113 (0.4373)	Acc@1 83.203 (84.811)	Acc@5 99.219 (99.307)
Epoch: [74][192/196]	Time 0.068 (0.071)	Data 0.000 (0.002)	Loss 0.4999 (0.4386)	Acc@1 85.156 (84.768)	Acc@5 98.828 (99.308)
after train
n1: 30 for:
wAcc: 76.29122135643766
test acc: 77.52
Epoche: [75/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [75][0/196]	Time 0.109 (0.109)	Data 0.284 (0.284)	Loss 0.4566 (0.4566)	Acc@1 83.594 (83.594)	Acc@5 98.047 (98.047)
Epoch: [75][64/196]	Time 0.073 (0.075)	Data 0.000 (0.005)	Loss 0.4405 (0.4391)	Acc@1 83.984 (84.880)	Acc@5 100.000 (99.351)
Epoch: [75][128/196]	Time 0.075 (0.074)	Data 0.000 (0.002)	Loss 0.4702 (0.4310)	Acc@1 85.156 (85.187)	Acc@5 99.609 (99.391)
Epoch: [75][192/196]	Time 0.070 (0.074)	Data 0.000 (0.002)	Loss 0.4650 (0.4293)	Acc@1 85.938 (85.294)	Acc@5 99.609 (99.379)
after train
n1: 30 for:
wAcc: 75.49011751434843
test acc: 79.64
Epoche: [76/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [76][0/196]	Time 0.099 (0.099)	Data 0.261 (0.261)	Loss 0.3933 (0.3933)	Acc@1 87.109 (87.109)	Acc@5 99.609 (99.609)
Epoch: [76][64/196]	Time 0.077 (0.076)	Data 0.000 (0.004)	Loss 0.4599 (0.4296)	Acc@1 82.422 (85.355)	Acc@5 100.000 (99.387)
Epoch: [76][128/196]	Time 0.070 (0.075)	Data 0.000 (0.002)	Loss 0.3683 (0.4339)	Acc@1 88.281 (85.338)	Acc@5 99.219 (99.367)
Epoch: [76][192/196]	Time 0.069 (0.074)	Data 0.000 (0.002)	Loss 0.4424 (0.4342)	Acc@1 84.766 (85.340)	Acc@5 99.609 (99.336)
after train
n1: 30 for:
wAcc: 75.69424478307452
test acc: 77.91
Epoche: [77/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [77][0/196]	Time 0.106 (0.106)	Data 0.283 (0.283)	Loss 0.3828 (0.3828)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [77][64/196]	Time 0.068 (0.071)	Data 0.000 (0.005)	Loss 0.3469 (0.4187)	Acc@1 86.719 (85.835)	Acc@5 99.609 (99.399)
Epoch: [77][128/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.4147 (0.4274)	Acc@1 88.281 (85.383)	Acc@5 99.219 (99.370)
Epoch: [77][192/196]	Time 0.069 (0.071)	Data 0.000 (0.002)	Loss 0.4393 (0.4313)	Acc@1 87.500 (85.203)	Acc@5 98.047 (99.330)
after train
n1: 30 for:
wAcc: 75.81117565224993
test acc: 67.66
Epoche: [78/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [78][0/196]	Time 0.087 (0.087)	Data 0.269 (0.269)	Loss 0.3291 (0.3291)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [78][64/196]	Time 0.075 (0.072)	Data 0.000 (0.004)	Loss 0.3290 (0.4250)	Acc@1 89.453 (85.625)	Acc@5 100.000 (99.429)
Epoch: [78][128/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.3959 (0.4298)	Acc@1 84.375 (85.453)	Acc@5 100.000 (99.340)
Epoch: [78][192/196]	Time 0.072 (0.072)	Data 0.000 (0.002)	Loss 0.3845 (0.4302)	Acc@1 88.672 (85.336)	Acc@5 98.828 (99.330)
after train
n1: 30 for:
wAcc: 76.17579254472112
test acc: 80.34
Epoche: [79/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [79][0/196]	Time 0.107 (0.107)	Data 0.287 (0.287)	Loss 0.3917 (0.3917)	Acc@1 86.719 (86.719)	Acc@5 99.609 (99.609)
Epoch: [79][64/196]	Time 0.073 (0.073)	Data 0.000 (0.005)	Loss 0.4298 (0.4192)	Acc@1 83.203 (85.415)	Acc@5 99.609 (99.417)
Epoch: [79][128/196]	Time 0.072 (0.073)	Data 0.000 (0.002)	Loss 0.3956 (0.4287)	Acc@1 87.500 (85.105)	Acc@5 99.219 (99.410)
Epoch: [79][192/196]	Time 0.068 (0.073)	Data 0.000 (0.002)	Loss 0.3554 (0.4303)	Acc@1 87.891 (85.189)	Acc@5 99.609 (99.364)
after train
n1: 30 for:
wAcc: 75.77079423347098
test acc: 77.87
Epoche: [80/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [80][0/196]	Time 0.081 (0.081)	Data 0.265 (0.265)	Loss 0.3944 (0.3944)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [80][64/196]	Time 0.073 (0.069)	Data 0.000 (0.004)	Loss 0.4578 (0.4315)	Acc@1 83.203 (85.367)	Acc@5 100.000 (99.435)
Epoch: [80][128/196]	Time 0.066 (0.069)	Data 0.000 (0.002)	Loss 0.4215 (0.4356)	Acc@1 84.375 (85.138)	Acc@5 99.609 (99.388)
Epoch: [80][192/196]	Time 0.070 (0.070)	Data 0.000 (0.002)	Loss 0.4320 (0.4339)	Acc@1 85.156 (85.187)	Acc@5 100.000 (99.397)
after train
n1: 30 for:
wAcc: 76.72010843084729
test acc: 78.34
Epoche: [81/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [81][0/196]	Time 0.103 (0.103)	Data 0.243 (0.243)	Loss 0.4818 (0.4818)	Acc@1 84.375 (84.375)	Acc@5 97.656 (97.656)
Epoch: [81][64/196]	Time 0.075 (0.073)	Data 0.000 (0.004)	Loss 0.4103 (0.4251)	Acc@1 85.547 (85.373)	Acc@5 99.609 (99.357)
Epoch: [81][128/196]	Time 0.072 (0.073)	Data 0.000 (0.002)	Loss 0.3948 (0.4229)	Acc@1 87.109 (85.405)	Acc@5 99.219 (99.364)
Epoch: [81][192/196]	Time 0.070 (0.073)	Data 0.000 (0.001)	Loss 0.3932 (0.4251)	Acc@1 85.938 (85.264)	Acc@5 99.609 (99.381)
after train
n1: 30 for:
wAcc: 76.13939578300958
test acc: 70.11
Epoche: [82/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [82][0/196]	Time 0.088 (0.088)	Data 0.240 (0.240)	Loss 0.3988 (0.3988)	Acc@1 85.156 (85.156)	Acc@5 98.828 (98.828)
Epoch: [82][64/196]	Time 0.071 (0.071)	Data 0.000 (0.004)	Loss 0.4235 (0.4160)	Acc@1 85.938 (85.709)	Acc@5 100.000 (99.387)
Epoch: [82][128/196]	Time 0.074 (0.071)	Data 0.000 (0.002)	Loss 0.4081 (0.4286)	Acc@1 88.281 (85.356)	Acc@5 98.828 (99.334)
Epoch: [82][192/196]	Time 0.069 (0.072)	Data 0.000 (0.001)	Loss 0.4163 (0.4265)	Acc@1 85.156 (85.442)	Acc@5 99.609 (99.346)
after train
n1: 30 for:
wAcc: 75.02759472047177
test acc: 81.5
Epoche: [83/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [83][0/196]	Time 0.107 (0.107)	Data 0.279 (0.279)	Loss 0.3953 (0.3953)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [83][64/196]	Time 0.070 (0.071)	Data 0.000 (0.004)	Loss 0.3769 (0.4370)	Acc@1 87.891 (84.874)	Acc@5 99.219 (99.399)
Epoch: [83][128/196]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.3492 (0.4279)	Acc@1 87.891 (85.253)	Acc@5 99.219 (99.428)
Epoch: [83][192/196]	Time 0.070 (0.072)	Data 0.000 (0.002)	Loss 0.3636 (0.4265)	Acc@1 88.672 (85.391)	Acc@5 99.609 (99.407)
after train
n1: 30 for:
wAcc: 75.45528856364189
test acc: 77.89
Epoche: [84/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [84][0/196]	Time 0.093 (0.093)	Data 0.276 (0.276)	Loss 0.4254 (0.4254)	Acc@1 83.984 (83.984)	Acc@5 99.609 (99.609)
Epoch: [84][64/196]	Time 0.074 (0.072)	Data 0.000 (0.004)	Loss 0.4062 (0.4152)	Acc@1 85.547 (85.739)	Acc@5 99.609 (99.381)
Epoch: [84][128/196]	Time 0.070 (0.073)	Data 0.000 (0.002)	Loss 0.4668 (0.4226)	Acc@1 85.938 (85.638)	Acc@5 98.438 (99.355)
Epoch: [84][192/196]	Time 0.068 (0.073)	Data 0.000 (0.002)	Loss 0.4278 (0.4249)	Acc@1 85.938 (85.466)	Acc@5 99.609 (99.344)
after train
n1: 30 for:
wAcc: 76.92498566059203
test acc: 77.77
Epoche: [85/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [85][0/196]	Time 0.105 (0.105)	Data 0.268 (0.268)	Loss 0.4016 (0.4016)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [85][64/196]	Time 0.074 (0.073)	Data 0.000 (0.004)	Loss 0.4957 (0.4261)	Acc@1 82.031 (85.391)	Acc@5 98.438 (99.441)
Epoch: [85][128/196]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.4903 (0.4227)	Acc@1 83.594 (85.435)	Acc@5 99.609 (99.449)
Epoch: [85][192/196]	Time 0.069 (0.072)	Data 0.000 (0.002)	Loss 0.4883 (0.4272)	Acc@1 82.031 (85.320)	Acc@5 99.219 (99.419)
after train
n1: 30 for:
wAcc: 76.476428495542
test acc: 79.85
Epoche: [86/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [86][0/196]	Time 0.103 (0.103)	Data 0.287 (0.287)	Loss 0.4057 (0.4057)	Acc@1 85.547 (85.547)	Acc@5 99.609 (99.609)
Epoch: [86][64/196]	Time 0.066 (0.074)	Data 0.000 (0.005)	Loss 0.3833 (0.4195)	Acc@1 86.719 (85.511)	Acc@5 98.828 (99.435)
Epoch: [86][128/196]	Time 0.072 (0.074)	Data 0.000 (0.002)	Loss 0.4740 (0.4213)	Acc@1 85.156 (85.501)	Acc@5 98.438 (99.416)
Epoch: [86][192/196]	Time 0.072 (0.073)	Data 0.000 (0.002)	Loss 0.4222 (0.4208)	Acc@1 86.719 (85.565)	Acc@5 100.000 (99.399)
after train
n1: 30 for:
wAcc: 76.85020475184555
test acc: 77.3
Epoche: [87/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [87][0/196]	Time 0.103 (0.103)	Data 0.250 (0.250)	Loss 0.3309 (0.3309)	Acc@1 89.844 (89.844)	Acc@5 99.609 (99.609)
Epoch: [87][64/196]	Time 0.070 (0.072)	Data 0.000 (0.004)	Loss 0.3338 (0.4244)	Acc@1 86.328 (85.505)	Acc@5 100.000 (99.519)
Epoch: [87][128/196]	Time 0.071 (0.071)	Data 0.000 (0.002)	Loss 0.4755 (0.4254)	Acc@1 83.203 (85.468)	Acc@5 99.609 (99.467)
Epoch: [87][192/196]	Time 0.070 (0.072)	Data 0.000 (0.001)	Loss 0.4801 (0.4253)	Acc@1 84.766 (85.296)	Acc@5 99.219 (99.431)
after train
n1: 30 for:
wAcc: 77.46903495366469
test acc: 79.24
Epoche: [88/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [88][0/196]	Time 0.078 (0.078)	Data 0.286 (0.286)	Loss 0.4381 (0.4381)	Acc@1 84.375 (84.375)	Acc@5 99.609 (99.609)
Epoch: [88][64/196]	Time 0.073 (0.073)	Data 0.000 (0.005)	Loss 0.4734 (0.4229)	Acc@1 84.375 (85.379)	Acc@5 99.609 (99.441)
Epoch: [88][128/196]	Time 0.071 (0.072)	Data 0.000 (0.002)	Loss 0.4780 (0.4274)	Acc@1 83.984 (85.226)	Acc@5 100.000 (99.406)
Epoch: [88][192/196]	Time 0.070 (0.071)	Data 0.000 (0.002)	Loss 0.5064 (0.4265)	Acc@1 80.078 (85.259)	Acc@5 99.609 (99.409)
after train
n1: 30 for:
wAcc: 76.84024435887706
test acc: 74.98
Epoche: [89/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [89][0/196]	Time 0.100 (0.100)	Data 0.262 (0.262)	Loss 0.4072 (0.4072)	Acc@1 85.156 (85.156)	Acc@5 99.609 (99.609)
Epoch: [89][64/196]	Time 0.074 (0.072)	Data 0.000 (0.004)	Loss 0.3567 (0.4246)	Acc@1 87.891 (85.745)	Acc@5 100.000 (99.393)
Epoch: [89][128/196]	Time 0.067 (0.071)	Data 0.000 (0.002)	Loss 0.3284 (0.4238)	Acc@1 87.109 (85.623)	Acc@5 100.000 (99.394)
Epoch: [89][192/196]	Time 0.068 (0.071)	Data 0.000 (0.002)	Loss 0.3513 (0.4250)	Acc@1 87.500 (85.452)	Acc@5 99.609 (99.393)
after train
n1: 30 for:
wAcc: 77.47050307587898
test acc: 67.39
Epoche: [90/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [90][0/196]	Time 0.082 (0.082)	Data 0.261 (0.261)	Loss 0.4211 (0.4211)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [90][64/196]	Time 0.075 (0.071)	Data 0.000 (0.004)	Loss 0.4593 (0.4254)	Acc@1 84.375 (85.517)	Acc@5 100.000 (99.339)
Epoch: [90][128/196]	Time 0.072 (0.072)	Data 0.000 (0.002)	Loss 0.4547 (0.4265)	Acc@1 85.938 (85.523)	Acc@5 99.219 (99.307)
Epoch: [90][192/196]	Time 0.073 (0.073)	Data 0.000 (0.002)	Loss 0.4698 (0.4255)	Acc@1 85.547 (85.391)	Acc@5 99.219 (99.375)
after train
n1: 30 for:
wAcc: 76.38357213585189
test acc: 73.31
IndexL: 0
Module= Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); indexL: 1
indexConv: 1
modulelist[indexConv]: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
indexConv: 2
modulelist[indexConv]: ReLU(inplace=True)
indexConv: 3
modulelist[indexConv]: Sequential(
  (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); indexConv: 3; index: 3
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 3, 3, 3); new shape: (8, 3, 3, 3)
new shape: (16, 3, 3, 3)
module after: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 3, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 3
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 3
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 3
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 3
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 3
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 3
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 3
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 4
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 4
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 4
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 4
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 4
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 4
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 4
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 4
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 5
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 5
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 5
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 5
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 5
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 5
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 5
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 5
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 6
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 6
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 6
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 6
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 6
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 6
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 6
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 6
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 7
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 7
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 7
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 7
module1: Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 7
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (8, 8, 3, 3); new shape w2: (8, 8, 3, 3)
new shape: (8, 16, 3, 3)
module1 after: Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 7
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 7
 moduleBn: BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 7
Module= Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 8
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (8, 16, 3, 3); new shape: (8, 16, 3, 3)
new shape: (16, 16, 3, 3)
module after: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([16, 16, 3, 3])
Batchnorm1
oldw2 shape: (16, 8, 3, 3); new shape w2: (16, 8, 3, 3)
new shape: (16, 16, 3, 3)
module1 after: Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
IndexL: 8
Module= Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i: 0 indexL: 8
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 8
module1: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 8
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 16, 3, 3); new shape: (16, 16, 3, 3)
new shape: (32, 16, 3, 3)
module after: Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 16, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 8
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 8
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 8
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 9
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 8, 1, 1); new shape w2: (16, 8, 1, 1)
new shape: (16, 16, 1, 1)
module1 after: Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
IndexL: 9
Module= Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 9
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 9
Module= Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 10
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 16, 1, 1); new shape: (16, 16, 1, 1)
new shape: (32, 16, 1, 1)
module after: Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
size of weight after: torch.Size([32, 16, 1, 1])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 10
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 10
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 10
module1: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 10
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 10
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 10
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 10
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 11
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 11
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 11
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 11
module1: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 11
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 11
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 11
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 11
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 12
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 12
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 12
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 12
module1: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 12
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 12
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 12
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 12
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 13
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 13
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 13
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 13
module1: Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 13
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (16, 16, 3, 3); new shape w2: (16, 16, 3, 3)
new shape: (16, 32, 3, 3)
module1 after: Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 13
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 13
 moduleBn: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 13
Module= Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 14
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (16, 32, 3, 3); new shape: (16, 32, 3, 3)
new shape: (32, 32, 3, 3)
module after: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([32, 32, 3, 3])
Batchnorm1
oldw2 shape: (32, 16, 3, 3); new shape w2: (32, 16, 3, 3)
new shape: (32, 32, 3, 3)
module1 after: Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
IndexL: 14
Module= Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False); i: 0 indexL: 14
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 14
module1: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 14
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 32, 3, 3); new shape: (32, 32, 3, 3)
new shape: (64, 32, 3, 3)
module after: Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 32, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 14
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 14
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 14
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 15
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 16, 1, 1); new shape w2: (32, 16, 1, 1)
new shape: (32, 32, 1, 1)
module1 after: Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
IndexL: 15
Module= Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 15
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 15
Module= Conv2d(32, 32, kernel_size=(1, 1), stride=(2, 2), bias=False); i: 0 indexL: 16
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 32, 1, 1); new shape: (32, 32, 1, 1)
new shape: (64, 32, 1, 1)
module after: Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
size of weight after: torch.Size([64, 32, 1, 1])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 16
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 16
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 16
module1: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 16
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 16
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 16
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 16
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 17
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 17
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 17
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 17
module1: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 17
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 17
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 17
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 17
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 18
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 18
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 18
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 18
module1: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 18
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 18
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 18
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 18
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 19
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 19
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 0 indexL: 19
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1; indexL: 19
module1: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3; indexL: 19
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (32, 32, 3, 3); new shape w2: (32, 32, 3, 3)
new shape: (32, 64, 3, 3)
module1 after: Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
IndexL: 19
Module= Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False); i: 3 indexL: 19
 moduleBn: BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 4; indexL: 19
dtype tensor: torch.float32; dtype numpy: float32
dtype new tensor: float64
oldw shape: (32, 64, 3, 3); new shape: (32, 64, 3, 3)
new shape: (64, 64, 3, 3)
module after: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
size of weight after: torch.Size([64, 64, 3, 3])
Batchnorm1
oldw2 shape: (10, 32); new shape w2: (10, 32)
new shape: (10, 64)
module after: Linear(in_features=64, out_features=10, bias=True)
size of weight after: torch.Size([10, 64])
self: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
layer: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)); i: 0
layer: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True); i: 1

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))

>new Layer:  [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]
Sequential: [Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))

>new Layer:  [Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))]
>new Layer: [Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))

>new Layer:  [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))

>new Layer:  [Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))]
>new Layer: [Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

>new Layer:  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

>new Layer:  [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]
>new Layer: [Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]

Linear:  Linear(in_features=64, out_features=10, bias=True)
 Modell: N2N(
  (module_list): ModuleList(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Sequential(
      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Sequential(
      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Sequential(
      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (13): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (14): Sequential(
      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (15): Sequential(
      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (16): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (17): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (18): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (19): Sequential(
      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (20): AdaptiveAvgPool2d(output_size=(1, 1))
    (21): Linear(in_features=64, out_features=10, bias=True)
  )
  (relu): ReLU(inplace=True)
)
Epoche: [91/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [91][0/196]	Time 0.130 (0.130)	Data 0.266 (0.266)	Loss 196.1824 (196.1824)	Acc@1 8.984 (8.984)	Acc@5 52.734 (52.734)
Epoch: [91][64/196]	Time 0.095 (0.098)	Data 0.000 (0.004)	Loss 2.6296 (95.2694)	Acc@1 12.891 (11.587)	Acc@5 58.594 (53.498)
Epoch: [91][128/196]	Time 0.096 (0.097)	Data 0.000 (0.002)	Loss 5.2240 (54.8661)	Acc@1 8.594 (11.301)	Acc@5 52.344 (54.012)
Epoch: [91][192/196]	Time 0.094 (0.097)	Data 0.000 (0.002)	Loss 3.0986 (38.5985)	Acc@1 12.891 (11.365)	Acc@5 54.297 (53.969)
after train
n1: 30 for:
wAcc: 75.79929780150671
test acc: 11.89
Epoche: [92/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [92][0/196]	Time 0.152 (0.152)	Data 0.275 (0.275)	Loss 4.4873 (4.4873)	Acc@1 9.766 (9.766)	Acc@5 54.688 (54.688)
Epoch: [92][64/196]	Time 0.096 (0.097)	Data 0.000 (0.004)	Loss 2.3359 (2.8357)	Acc@1 14.844 (14.111)	Acc@5 63.672 (60.180)
Epoch: [92][128/196]	Time 0.094 (0.096)	Data 0.000 (0.002)	Loss 2.4253 (2.6048)	Acc@1 12.500 (15.198)	Acc@5 62.891 (62.115)
Epoch: [92][192/196]	Time 0.093 (0.095)	Data 0.000 (0.002)	Loss 2.2333 (2.4930)	Acc@1 20.312 (15.884)	Acc@5 75.000 (63.453)
after train
n1: 30 for:
wAcc: 72.0722159650293
test acc: 18.05
Epoche: [93/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [93][0/196]	Time 0.128 (0.128)	Data 0.257 (0.257)	Loss 2.2067 (2.2067)	Acc@1 20.703 (20.703)	Acc@5 66.797 (66.797)
Epoch: [93][64/196]	Time 0.094 (0.095)	Data 0.000 (0.004)	Loss 2.1918 (2.1938)	Acc@1 20.312 (19.081)	Acc@5 66.797 (68.347)
Epoch: [93][128/196]	Time 0.090 (0.094)	Data 0.000 (0.002)	Loss 2.1180 (2.1827)	Acc@1 19.922 (19.216)	Acc@5 71.484 (68.726)
Epoch: [93][192/196]	Time 0.094 (0.094)	Data 0.000 (0.002)	Loss 2.1483 (2.1724)	Acc@1 18.359 (19.483)	Acc@5 72.266 (69.357)
after train
n1: 30 for:
wAcc: 67.10226451633616
test acc: 21.69
Epoche: [94/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [94][0/196]	Time 0.157 (0.157)	Data 0.269 (0.269)	Loss 2.1481 (2.1481)	Acc@1 22.656 (22.656)	Acc@5 68.750 (68.750)
Epoch: [94][64/196]	Time 0.093 (0.098)	Data 0.000 (0.004)	Loss 2.1809 (2.1261)	Acc@1 17.578 (20.499)	Acc@5 69.531 (71.989)
Epoch: [94][128/196]	Time 0.099 (0.098)	Data 0.000 (0.002)	Loss 2.0396 (2.1122)	Acc@1 23.438 (21.439)	Acc@5 76.953 (72.783)
Epoch: [94][192/196]	Time 0.094 (0.097)	Data 0.000 (0.002)	Loss 2.0798 (2.1015)	Acc@1 22.656 (21.691)	Acc@5 78.125 (73.421)
after train
n1: 30 for:
wAcc: 65.8305620607295
test acc: 22.91
Epoche: [95/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [95][0/196]	Time 0.133 (0.133)	Data 0.269 (0.269)	Loss 2.0696 (2.0696)	Acc@1 21.484 (21.484)	Acc@5 74.609 (74.609)
Epoch: [95][64/196]	Time 0.097 (0.096)	Data 0.000 (0.004)	Loss 2.0510 (2.0482)	Acc@1 21.484 (24.147)	Acc@5 72.266 (76.701)
Epoch: [95][128/196]	Time 0.094 (0.095)	Data 0.000 (0.002)	Loss 2.0420 (2.0359)	Acc@1 23.438 (24.337)	Acc@5 78.125 (77.077)
Epoch: [95][192/196]	Time 0.093 (0.095)	Data 0.000 (0.002)	Loss 2.0134 (2.0267)	Acc@1 26.562 (24.425)	Acc@5 77.344 (77.437)
after train
n1: 30 for:
wAcc: 62.28519797828797
test acc: 24.36
Epoche: [96/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [96][0/196]	Time 0.127 (0.127)	Data 0.287 (0.287)	Loss 2.0231 (2.0231)	Acc@1 23.047 (23.047)	Acc@5 75.781 (75.781)
Epoch: [96][64/196]	Time 0.094 (0.096)	Data 0.000 (0.005)	Loss 2.0089 (1.9837)	Acc@1 24.219 (25.150)	Acc@5 74.609 (79.249)
Epoch: [96][128/196]	Time 0.098 (0.096)	Data 0.000 (0.002)	Loss 2.0044 (1.9721)	Acc@1 25.391 (25.742)	Acc@5 79.297 (79.460)
Epoch: [96][192/196]	Time 0.094 (0.096)	Data 0.000 (0.002)	Loss 1.9685 (1.9602)	Acc@1 25.781 (26.142)	Acc@5 81.641 (79.997)
after train
n1: 30 for:
wAcc: 60.714454048838896
test acc: 27.76
Epoche: [97/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [97][0/196]	Time 0.117 (0.117)	Data 0.254 (0.254)	Loss 1.9466 (1.9466)	Acc@1 27.344 (27.344)	Acc@5 76.172 (76.172)
Epoch: [97][64/196]	Time 0.094 (0.097)	Data 0.000 (0.004)	Loss 1.8855 (1.9238)	Acc@1 26.953 (28.095)	Acc@5 84.766 (81.070)
Epoch: [97][128/196]	Time 0.098 (0.096)	Data 0.000 (0.002)	Loss 1.9885 (1.9178)	Acc@1 31.250 (28.083)	Acc@5 78.906 (81.417)
Epoch: [97][192/196]	Time 0.095 (0.097)	Data 0.000 (0.002)	Loss 1.8273 (1.9088)	Acc@1 33.203 (28.402)	Acc@5 82.422 (81.821)
after train
n1: 30 for:
wAcc: 58.257314273150115
test acc: 29.11
Epoche: [98/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [98][0/196]	Time 0.145 (0.145)	Data 0.270 (0.270)	Loss 1.8274 (1.8274)	Acc@1 32.422 (32.422)	Acc@5 82.422 (82.422)
Epoch: [98][64/196]	Time 0.101 (0.100)	Data 0.000 (0.004)	Loss 1.8847 (1.8872)	Acc@1 25.781 (28.588)	Acc@5 83.203 (83.113)
Epoch: [98][128/196]	Time 0.094 (0.098)	Data 0.000 (0.002)	Loss 1.9383 (1.8793)	Acc@1 27.734 (29.082)	Acc@5 82.812 (83.121)
Epoch: [98][192/196]	Time 0.095 (0.097)	Data 0.000 (0.002)	Loss 1.8552 (1.8768)	Acc@1 33.203 (29.307)	Acc@5 84.766 (83.015)
after train
n1: 30 for:
wAcc: 55.643915289338146
test acc: 28.23
Epoche: [99/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [99][0/196]	Time 0.151 (0.151)	Data 0.260 (0.260)	Loss 1.8198 (1.8198)	Acc@1 30.469 (30.469)	Acc@5 81.250 (81.250)
Epoch: [99][64/196]	Time 0.092 (0.097)	Data 0.000 (0.004)	Loss 1.8018 (1.8544)	Acc@1 30.859 (29.549)	Acc@5 85.156 (83.317)
Epoch: [99][128/196]	Time 0.096 (0.097)	Data 0.000 (0.002)	Loss 1.8467 (1.8511)	Acc@1 27.734 (30.181)	Acc@5 82.812 (83.718)
Epoch: [99][192/196]	Time 0.093 (0.097)	Data 0.000 (0.002)	Loss 1.7412 (1.8460)	Acc@1 31.641 (30.301)	Acc@5 84.375 (83.879)
after train
n1: 30 for:
wAcc: 55.06357159378864
test acc: 29.62
Epoche: [100/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [100][0/196]	Time 0.124 (0.124)	Data 0.249 (0.249)	Loss 1.9172 (1.9172)	Acc@1 27.344 (27.344)	Acc@5 82.031 (82.031)
Epoch: [100][64/196]	Time 0.097 (0.096)	Data 0.000 (0.004)	Loss 1.7364 (1.8234)	Acc@1 28.906 (31.136)	Acc@5 86.328 (84.477)
Epoch: [100][128/196]	Time 0.097 (0.097)	Data 0.000 (0.002)	Loss 1.8712 (1.8190)	Acc@1 26.562 (31.565)	Acc@5 85.938 (84.608)
Epoch: [100][192/196]	Time 0.093 (0.097)	Data 0.000 (0.002)	Loss 1.7553 (1.8158)	Acc@1 32.031 (31.426)	Acc@5 87.500 (84.784)
after train
n1: 30 for:
wAcc: 53.10979788215758
test acc: 31.97
Epoche: [101/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [101][0/196]	Time 0.125 (0.125)	Data 0.262 (0.262)	Loss 1.7837 (1.7837)	Acc@1 31.641 (31.641)	Acc@5 83.984 (83.984)
Epoch: [101][64/196]	Time 0.094 (0.097)	Data 0.000 (0.004)	Loss 1.8335 (1.7953)	Acc@1 29.297 (32.704)	Acc@5 83.594 (85.439)
Epoch: [101][128/196]	Time 0.097 (0.096)	Data 0.000 (0.002)	Loss 1.7408 (1.7890)	Acc@1 34.766 (33.067)	Acc@5 87.500 (85.574)
Epoch: [101][192/196]	Time 0.096 (0.096)	Data 0.000 (0.002)	Loss 1.7702 (1.7840)	Acc@1 33.594 (33.098)	Acc@5 86.328 (85.711)
after train
n1: 30 for:
wAcc: 51.41489398819016
test acc: 33.3
Epoche: [102/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [102][0/196]	Time 0.155 (0.155)	Data 0.271 (0.271)	Loss 1.7366 (1.7366)	Acc@1 36.719 (36.719)	Acc@5 86.328 (86.328)
Epoch: [102][64/196]	Time 0.096 (0.095)	Data 0.000 (0.004)	Loss 1.7974 (1.7768)	Acc@1 32.812 (32.680)	Acc@5 86.328 (86.130)
Epoch: [102][128/196]	Time 0.100 (0.095)	Data 0.000 (0.002)	Loss 1.6484 (1.7649)	Acc@1 35.547 (33.445)	Acc@5 88.672 (86.243)
Epoch: [102][192/196]	Time 0.092 (0.094)	Data 0.000 (0.002)	Loss 1.6762 (1.7545)	Acc@1 32.031 (33.831)	Acc@5 87.891 (86.488)
after train
n1: 30 for:
wAcc: 49.89056971942482
test acc: 34.25
Epoche: [103/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [103][0/196]	Time 0.148 (0.148)	Data 0.290 (0.290)	Loss 1.7770 (1.7770)	Acc@1 33.984 (33.984)	Acc@5 86.719 (86.719)
Epoch: [103][64/196]	Time 0.092 (0.097)	Data 0.000 (0.005)	Loss 1.6539 (1.7319)	Acc@1 36.719 (34.790)	Acc@5 89.453 (87.109)
Epoch: [103][128/196]	Time 0.091 (0.096)	Data 0.000 (0.002)	Loss 1.7932 (1.7334)	Acc@1 28.906 (34.575)	Acc@5 87.109 (87.100)
Epoch: [103][192/196]	Time 0.089 (0.095)	Data 0.000 (0.002)	Loss 1.6349 (1.7316)	Acc@1 41.016 (34.484)	Acc@5 89.062 (87.249)
after train
n1: 30 for:
wAcc: 49.48287678339896
test acc: 31.67
Epoche: [104/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [104][0/196]	Time 0.146 (0.146)	Data 0.238 (0.238)	Loss 1.6996 (1.6996)	Acc@1 35.547 (35.547)	Acc@5 88.281 (88.281)
Epoch: [104][64/196]	Time 0.092 (0.094)	Data 0.000 (0.004)	Loss 1.7841 (1.7216)	Acc@1 36.719 (34.778)	Acc@5 84.766 (87.344)
Epoch: [104][128/196]	Time 0.090 (0.094)	Data 0.000 (0.002)	Loss 1.7165 (1.7165)	Acc@1 34.766 (35.208)	Acc@5 86.719 (87.449)
Epoch: [104][192/196]	Time 0.092 (0.093)	Data 0.000 (0.001)	Loss 1.6502 (1.7141)	Acc@1 38.281 (35.193)	Acc@5 85.938 (87.449)
after train
n1: 30 for:
wAcc: 48.640129427760456
test acc: 34.98
Epoche: [105/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [105][0/196]	Time 0.147 (0.147)	Data 0.262 (0.262)	Loss 1.7218 (1.7218)	Acc@1 32.422 (32.422)	Acc@5 89.453 (89.453)
Epoch: [105][64/196]	Time 0.094 (0.094)	Data 0.000 (0.004)	Loss 1.7543 (1.7099)	Acc@1 32.812 (35.511)	Acc@5 87.109 (87.422)
Epoch: [105][128/196]	Time 0.104 (0.094)	Data 0.000 (0.002)	Loss 1.6870 (1.7039)	Acc@1 35.547 (36.174)	Acc@5 92.188 (87.712)
Epoch: [105][192/196]	Time 0.093 (0.095)	Data 0.000 (0.002)	Loss 1.7035 (1.6982)	Acc@1 36.719 (36.253)	Acc@5 87.891 (87.828)
after train
n1: 30 for:
wAcc: 47.50873926097145
test acc: 36.64
Epoche: [106/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [106][0/196]	Time 0.150 (0.150)	Data 0.251 (0.251)	Loss 1.6193 (1.6193)	Acc@1 39.453 (39.453)	Acc@5 88.672 (88.672)
Epoch: [106][64/196]	Time 0.094 (0.097)	Data 0.000 (0.004)	Loss 1.6867 (1.6826)	Acc@1 37.500 (36.857)	Acc@5 90.625 (88.474)
Epoch: [106][128/196]	Time 0.094 (0.097)	Data 0.000 (0.002)	Loss 1.5659 (1.6763)	Acc@1 47.266 (37.203)	Acc@5 91.797 (88.587)
Epoch: [106][192/196]	Time 0.093 (0.096)	Data 0.000 (0.002)	Loss 1.7090 (1.6737)	Acc@1 38.672 (37.395)	Acc@5 86.719 (88.508)
after train
n1: 30 for:
wAcc: 45.32577431465272
test acc: 39.05
Epoche: [107/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [107][0/196]	Time 0.134 (0.134)	Data 0.292 (0.292)	Loss 1.6316 (1.6316)	Acc@1 41.797 (41.797)	Acc@5 88.281 (88.281)
Epoch: [107][64/196]	Time 0.094 (0.097)	Data 0.000 (0.005)	Loss 1.6791 (1.6491)	Acc@1 39.062 (38.191)	Acc@5 85.547 (88.840)
Epoch: [107][128/196]	Time 0.091 (0.096)	Data 0.000 (0.002)	Loss 1.6785 (1.6528)	Acc@1 33.594 (37.997)	Acc@5 87.891 (88.869)
Epoch: [107][192/196]	Time 0.094 (0.095)	Data 0.000 (0.002)	Loss 1.5344 (1.6437)	Acc@1 40.234 (38.427)	Acc@5 91.406 (89.131)
after train
n1: 30 for:
wAcc: 46.75392619503149
test acc: 40.33
Epoche: [108/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [108][0/196]	Time 0.130 (0.130)	Data 0.238 (0.238)	Loss 1.6282 (1.6282)	Acc@1 35.156 (35.156)	Acc@5 90.625 (90.625)
Epoch: [108][64/196]	Time 0.097 (0.097)	Data 0.000 (0.004)	Loss 1.6665 (1.6279)	Acc@1 34.375 (38.906)	Acc@5 90.234 (89.375)
Epoch: [108][128/196]	Time 0.095 (0.097)	Data 0.000 (0.002)	Loss 1.5558 (1.6209)	Acc@1 40.234 (39.441)	Acc@5 87.891 (89.556)
Epoch: [108][192/196]	Time 0.093 (0.097)	Data 0.000 (0.001)	Loss 1.5075 (1.6133)	Acc@1 44.922 (39.921)	Acc@5 92.188 (89.615)
after train
n1: 30 for:
wAcc: 45.982412297349136
test acc: 41.96
Epoche: [109/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [109][0/196]	Time 0.153 (0.153)	Data 0.281 (0.281)	Loss 1.5907 (1.5907)	Acc@1 44.922 (44.922)	Acc@5 91.016 (91.016)
Epoch: [109][64/196]	Time 0.099 (0.095)	Data 0.000 (0.005)	Loss 1.7825 (1.6061)	Acc@1 37.500 (40.439)	Acc@5 83.984 (89.778)
Epoch: [109][128/196]	Time 0.096 (0.095)	Data 0.000 (0.002)	Loss 1.6280 (1.5948)	Acc@1 40.234 (40.507)	Acc@5 88.672 (90.077)
Epoch: [109][192/196]	Time 0.091 (0.095)	Data 0.000 (0.002)	Loss 1.5538 (1.5875)	Acc@1 41.797 (40.781)	Acc@5 90.625 (90.097)
after train
n1: 30 for:
wAcc: 45.790845758456584
test acc: 40.33
Epoche: [110/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [110][0/196]	Time 0.128 (0.128)	Data 0.238 (0.238)	Loss 1.6167 (1.6167)	Acc@1 41.016 (41.016)	Acc@5 87.500 (87.500)
Epoch: [110][64/196]	Time 0.090 (0.092)	Data 0.000 (0.004)	Loss 1.5456 (1.5651)	Acc@1 40.234 (41.106)	Acc@5 89.844 (90.661)
Epoch: [110][128/196]	Time 0.093 (0.093)	Data 0.000 (0.002)	Loss 1.5183 (1.5589)	Acc@1 43.750 (41.712)	Acc@5 91.406 (90.713)
Epoch: [110][192/196]	Time 0.093 (0.093)	Data 0.000 (0.001)	Loss 1.5870 (1.5610)	Acc@1 41.797 (41.805)	Acc@5 88.672 (90.647)
after train
n1: 30 for:
wAcc: 44.2487915127695
test acc: 43.34
Epoche: [111/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [111][0/196]	Time 0.149 (0.149)	Data 0.267 (0.267)	Loss 1.5414 (1.5414)	Acc@1 42.188 (42.188)	Acc@5 92.578 (92.578)
Epoch: [111][64/196]	Time 0.093 (0.098)	Data 0.000 (0.004)	Loss 1.3835 (1.5313)	Acc@1 44.141 (43.185)	Acc@5 94.141 (91.016)
Epoch: [111][128/196]	Time 0.095 (0.098)	Data 0.000 (0.002)	Loss 1.4682 (1.5286)	Acc@1 47.266 (43.562)	Acc@5 93.750 (91.143)
Epoch: [111][192/196]	Time 0.091 (0.097)	Data 0.000 (0.002)	Loss 1.4914 (1.5318)	Acc@1 46.094 (43.491)	Acc@5 90.625 (90.983)
after train
n1: 30 for:
wAcc: 45.83671593926511
test acc: 42.54
Epoche: [112/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [112][0/196]	Time 0.130 (0.130)	Data 0.262 (0.262)	Loss 1.5175 (1.5175)	Acc@1 42.578 (42.578)	Acc@5 91.016 (91.016)
Epoch: [112][64/196]	Time 0.099 (0.095)	Data 0.000 (0.004)	Loss 1.5094 (1.5310)	Acc@1 42.188 (43.287)	Acc@5 91.016 (91.346)
Epoch: [112][128/196]	Time 0.108 (0.095)	Data 0.000 (0.002)	Loss 1.5652 (1.5270)	Acc@1 43.359 (43.726)	Acc@5 89.453 (91.170)
Epoch: [112][192/196]	Time 0.092 (0.095)	Data 0.000 (0.002)	Loss 1.4714 (1.5188)	Acc@1 45.703 (43.807)	Acc@5 92.969 (91.350)
after train
n1: 30 for:
wAcc: 45.102157366697845
test acc: 37.56
Epoche: [113/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [113][0/196]	Time 0.133 (0.133)	Data 0.246 (0.246)	Loss 1.5076 (1.5076)	Acc@1 43.359 (43.359)	Acc@5 92.578 (92.578)
Epoch: [113][64/196]	Time 0.093 (0.095)	Data 0.000 (0.004)	Loss 1.5045 (1.4987)	Acc@1 46.484 (44.621)	Acc@5 92.969 (91.689)
Epoch: [113][128/196]	Time 0.096 (0.095)	Data 0.000 (0.002)	Loss 1.4338 (1.4942)	Acc@1 45.703 (44.743)	Acc@5 90.625 (91.667)
Epoch: [113][192/196]	Time 0.097 (0.095)	Data 0.000 (0.001)	Loss 1.5165 (1.4925)	Acc@1 43.359 (45.011)	Acc@5 91.797 (91.566)
after train
n1: 30 for:
wAcc: 44.598219181977214
test acc: 44.86
Epoche: [114/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [114][0/196]	Time 0.130 (0.130)	Data 0.288 (0.288)	Loss 1.4543 (1.4543)	Acc@1 51.172 (51.172)	Acc@5 91.016 (91.016)
Epoch: [114][64/196]	Time 0.099 (0.097)	Data 0.000 (0.005)	Loss 1.5164 (1.4651)	Acc@1 44.141 (46.190)	Acc@5 92.578 (92.151)
Epoch: [114][128/196]	Time 0.093 (0.097)	Data 0.000 (0.002)	Loss 1.5678 (1.4700)	Acc@1 44.531 (45.954)	Acc@5 89.844 (91.994)
Epoch: [114][192/196]	Time 0.103 (0.096)	Data 0.000 (0.002)	Loss 1.3972 (1.4660)	Acc@1 48.047 (46.041)	Acc@5 91.797 (92.133)
after train
n1: 30 for:
wAcc: 44.91579630607622
test acc: 41.64
Epoche: [115/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [115][0/196]	Time 0.146 (0.146)	Data 0.261 (0.261)	Loss 1.4596 (1.4596)	Acc@1 48.047 (48.047)	Acc@5 90.234 (90.234)
Epoch: [115][64/196]	Time 0.092 (0.095)	Data 0.000 (0.004)	Loss 1.4354 (1.4423)	Acc@1 46.094 (46.761)	Acc@5 92.578 (92.608)
Epoch: [115][128/196]	Time 0.096 (0.096)	Data 0.000 (0.002)	Loss 1.5388 (1.4493)	Acc@1 39.453 (46.500)	Acc@5 89.844 (92.451)
Epoch: [115][192/196]	Time 0.093 (0.097)	Data 0.000 (0.002)	Loss 1.4041 (1.4470)	Acc@1 46.484 (46.654)	Acc@5 92.578 (92.426)
after train
n1: 30 for:
wAcc: 44.335822637940566
test acc: 43.51
Epoche: [116/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [116][0/196]	Time 0.156 (0.156)	Data 0.291 (0.291)	Loss 1.3565 (1.3565)	Acc@1 48.047 (48.047)	Acc@5 95.703 (95.703)
Epoch: [116][64/196]	Time 0.097 (0.097)	Data 0.000 (0.005)	Loss 1.4656 (1.4431)	Acc@1 45.703 (46.959)	Acc@5 92.578 (92.512)
Epoch: [116][128/196]	Time 0.096 (0.097)	Data 0.000 (0.002)	Loss 1.3701 (1.4368)	Acc@1 48.438 (47.223)	Acc@5 94.922 (92.442)
Epoch: [116][192/196]	Time 0.094 (0.097)	Data 0.000 (0.002)	Loss 1.4773 (1.4291)	Acc@1 46.875 (47.591)	Acc@5 89.062 (92.540)
after train
n1: 30 for:
wAcc: 44.5629931791247
test acc: 45.92
Epoche: [117/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [117][0/196]	Time 0.134 (0.134)	Data 0.258 (0.258)	Loss 1.4552 (1.4552)	Acc@1 45.703 (45.703)	Acc@5 93.359 (93.359)
Epoch: [117][64/196]	Time 0.094 (0.098)	Data 0.000 (0.004)	Loss 1.4730 (1.4274)	Acc@1 48.438 (47.650)	Acc@5 91.406 (92.596)
Epoch: [117][128/196]	Time 0.093 (0.099)	Data 0.000 (0.002)	Loss 1.4483 (1.4162)	Acc@1 51.172 (48.201)	Acc@5 92.188 (92.642)
Epoch: [117][192/196]	Time 0.098 (0.098)	Data 0.000 (0.002)	Loss 1.3882 (1.4139)	Acc@1 46.875 (48.318)	Acc@5 92.969 (92.639)
after train
n1: 30 for:
wAcc: 44.03470977242311
test acc: 48.74
Epoche: [118/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [118][0/196]	Time 0.131 (0.131)	Data 0.278 (0.278)	Loss 1.4264 (1.4264)	Acc@1 46.875 (46.875)	Acc@5 92.578 (92.578)
Epoch: [118][64/196]	Time 0.096 (0.097)	Data 0.000 (0.004)	Loss 1.4044 (1.3872)	Acc@1 46.094 (49.543)	Acc@5 94.141 (93.041)
Epoch: [118][128/196]	Time 0.095 (0.097)	Data 0.000 (0.002)	Loss 1.3679 (1.3827)	Acc@1 50.000 (49.540)	Acc@5 91.797 (93.072)
Epoch: [118][192/196]	Time 0.095 (0.097)	Data 0.000 (0.002)	Loss 1.5238 (1.3854)	Acc@1 44.141 (49.537)	Acc@5 91.016 (93.060)
after train
n1: 30 for:
wAcc: 43.24105466440591
test acc: 45.64
Epoche: [119/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [119][0/196]	Time 0.145 (0.145)	Data 0.270 (0.270)	Loss 1.4400 (1.4400)	Acc@1 48.047 (48.047)	Acc@5 93.750 (93.750)
Epoch: [119][64/196]	Time 0.096 (0.095)	Data 0.000 (0.004)	Loss 1.4279 (1.3655)	Acc@1 47.266 (50.306)	Acc@5 92.578 (93.263)
Epoch: [119][128/196]	Time 0.096 (0.097)	Data 0.000 (0.002)	Loss 1.3807 (1.3623)	Acc@1 53.516 (50.345)	Acc@5 90.234 (93.347)
Epoch: [119][192/196]	Time 0.093 (0.096)	Data 0.000 (0.002)	Loss 1.3472 (1.3684)	Acc@1 52.344 (50.055)	Acc@5 94.531 (93.234)
after train
n1: 30 for:
wAcc: 44.25162975009653
test acc: 47.86
Epoche: [120/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [120][0/196]	Time 0.152 (0.152)	Data 0.259 (0.259)	Loss 1.4266 (1.4266)	Acc@1 45.703 (45.703)	Acc@5 92.578 (92.578)
Epoch: [120][64/196]	Time 0.093 (0.097)	Data 0.000 (0.004)	Loss 1.2196 (1.3611)	Acc@1 56.250 (50.319)	Acc@5 96.094 (93.798)
Epoch: [120][128/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 1.5056 (1.3516)	Acc@1 45.312 (50.684)	Acc@5 90.625 (93.671)
Epoch: [120][192/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 1.2040 (1.3507)	Acc@1 55.859 (50.648)	Acc@5 96.094 (93.578)
after train
n1: 30 for:
wAcc: 35.605456984875275
test acc: 47.44
Epoche: [121/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [121][0/196]	Time 0.124 (0.124)	Data 0.241 (0.241)	Loss 1.3530 (1.3530)	Acc@1 53.125 (53.125)	Acc@5 90.625 (90.625)
Epoch: [121][64/196]	Time 0.092 (0.093)	Data 0.000 (0.004)	Loss 1.3140 (1.3373)	Acc@1 51.562 (51.448)	Acc@5 93.359 (93.762)
Epoch: [121][128/196]	Time 0.098 (0.094)	Data 0.000 (0.002)	Loss 1.2611 (1.3288)	Acc@1 57.812 (51.729)	Acc@5 94.531 (93.868)
Epoch: [121][192/196]	Time 0.095 (0.094)	Data 0.000 (0.001)	Loss 1.3666 (1.3282)	Acc@1 54.297 (51.690)	Acc@5 94.531 (93.884)
after train
n1: 30 for:
wAcc: 37.25947508169326
test acc: 49.77
Epoche: [122/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [122][0/196]	Time 0.117 (0.117)	Data 0.251 (0.251)	Loss 1.3857 (1.3857)	Acc@1 48.047 (48.047)	Acc@5 93.359 (93.359)
Epoch: [122][64/196]	Time 0.090 (0.094)	Data 0.000 (0.004)	Loss 1.4082 (1.3104)	Acc@1 49.219 (51.899)	Acc@5 93.359 (94.069)
Epoch: [122][128/196]	Time 0.092 (0.094)	Data 0.000 (0.002)	Loss 1.3530 (1.3131)	Acc@1 48.828 (51.996)	Acc@5 93.750 (93.998)
Epoch: [122][192/196]	Time 0.093 (0.095)	Data 0.000 (0.002)	Loss 1.2774 (1.3081)	Acc@1 55.469 (52.259)	Acc@5 94.531 (94.039)
after train
n1: 30 for:
wAcc: 38.59280978994825
test acc: 45.59
Epoche: [123/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [123][0/196]	Time 0.117 (0.117)	Data 0.266 (0.266)	Loss 1.3035 (1.3035)	Acc@1 54.297 (54.297)	Acc@5 94.922 (94.922)
Epoch: [123][64/196]	Time 0.101 (0.094)	Data 0.000 (0.004)	Loss 1.4289 (1.3087)	Acc@1 47.266 (52.374)	Acc@5 89.453 (93.804)
Epoch: [123][128/196]	Time 0.097 (0.095)	Data 0.000 (0.002)	Loss 1.2930 (1.3001)	Acc@1 53.516 (52.574)	Acc@5 92.188 (93.959)
Epoch: [123][192/196]	Time 0.096 (0.096)	Data 0.000 (0.002)	Loss 1.3441 (1.2961)	Acc@1 49.609 (52.751)	Acc@5 92.578 (94.114)
after train
n1: 30 for:
wAcc: 39.22060651623957
test acc: 42.78
Epoche: [124/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [124][0/196]	Time 0.153 (0.153)	Data 0.264 (0.264)	Loss 1.2681 (1.2681)	Acc@1 51.562 (51.562)	Acc@5 94.922 (94.922)
Epoch: [124][64/196]	Time 0.092 (0.097)	Data 0.000 (0.004)	Loss 1.2927 (1.2770)	Acc@1 50.000 (53.125)	Acc@5 93.359 (94.543)
Epoch: [124][128/196]	Time 0.098 (0.097)	Data 0.000 (0.002)	Loss 1.3359 (1.2747)	Acc@1 54.688 (53.670)	Acc@5 93.750 (94.353)
Epoch: [124][192/196]	Time 0.094 (0.097)	Data 0.000 (0.002)	Loss 1.1880 (1.2710)	Acc@1 58.203 (53.874)	Acc@5 96.484 (94.379)
after train
n1: 30 for:
wAcc: 39.659859063516684
test acc: 46.09
Epoche: [125/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [125][0/196]	Time 0.123 (0.123)	Data 0.253 (0.253)	Loss 1.3092 (1.3092)	Acc@1 53.125 (53.125)	Acc@5 93.359 (93.359)
Epoch: [125][64/196]	Time 0.095 (0.097)	Data 0.000 (0.004)	Loss 1.2563 (1.2723)	Acc@1 52.734 (53.912)	Acc@5 93.359 (94.742)
Epoch: [125][128/196]	Time 0.097 (0.097)	Data 0.000 (0.002)	Loss 1.3345 (1.2709)	Acc@1 54.688 (53.846)	Acc@5 95.312 (94.552)
Epoch: [125][192/196]	Time 0.096 (0.097)	Data 0.000 (0.002)	Loss 1.2074 (1.2661)	Acc@1 56.250 (53.999)	Acc@5 94.531 (94.448)
after train
n1: 30 for:
wAcc: 40.56621616049637
test acc: 56.04
Epoche: [126/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [126][0/196]	Time 0.115 (0.115)	Data 0.276 (0.276)	Loss 1.2192 (1.2192)	Acc@1 56.250 (56.250)	Acc@5 94.922 (94.922)
Epoch: [126][64/196]	Time 0.098 (0.096)	Data 0.000 (0.004)	Loss 1.2177 (1.2501)	Acc@1 51.562 (54.904)	Acc@5 94.531 (94.772)
Epoch: [126][128/196]	Time 0.094 (0.097)	Data 0.000 (0.002)	Loss 1.2156 (1.2416)	Acc@1 55.469 (55.172)	Acc@5 94.922 (94.861)
Epoch: [126][192/196]	Time 0.092 (0.097)	Data 0.000 (0.002)	Loss 1.3571 (1.2406)	Acc@1 53.516 (55.262)	Acc@5 89.844 (94.877)
after train
n1: 30 for:
wAcc: 41.759682897580994
test acc: 50.94
Epoche: [127/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [127][0/196]	Time 0.153 (0.153)	Data 0.263 (0.263)	Loss 1.2171 (1.2171)	Acc@1 55.078 (55.078)	Acc@5 94.141 (94.141)
Epoch: [127][64/196]	Time 0.103 (0.097)	Data 0.000 (0.004)	Loss 1.3483 (1.2565)	Acc@1 53.516 (54.838)	Acc@5 93.359 (94.567)
Epoch: [127][128/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 1.1965 (1.2466)	Acc@1 54.688 (55.323)	Acc@5 95.703 (94.571)
Epoch: [127][192/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 1.1930 (1.2375)	Acc@1 55.859 (55.442)	Acc@5 97.656 (94.699)
after train
n1: 30 for:
wAcc: 42.224747249943945
test acc: 54.28
Epoche: [128/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [128][0/196]	Time 0.122 (0.122)	Data 0.263 (0.263)	Loss 1.2044 (1.2044)	Acc@1 55.469 (55.469)	Acc@5 95.703 (95.703)
Epoch: [128][64/196]	Time 0.091 (0.095)	Data 0.000 (0.004)	Loss 1.1765 (1.2236)	Acc@1 58.594 (55.102)	Acc@5 95.703 (94.838)
Epoch: [128][128/196]	Time 0.092 (0.095)	Data 0.000 (0.002)	Loss 1.1703 (1.2213)	Acc@1 58.203 (55.417)	Acc@5 94.531 (94.934)
Epoch: [128][192/196]	Time 0.091 (0.095)	Data 0.000 (0.002)	Loss 1.2274 (1.2162)	Acc@1 55.078 (55.839)	Acc@5 94.922 (94.910)
after train
n1: 30 for:
wAcc: 43.2034460564507
test acc: 50.0
Epoche: [129/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [129][0/196]	Time 0.156 (0.156)	Data 0.266 (0.266)	Loss 1.2260 (1.2260)	Acc@1 53.906 (53.906)	Acc@5 94.531 (94.531)
Epoch: [129][64/196]	Time 0.103 (0.099)	Data 0.000 (0.004)	Loss 1.1947 (1.2100)	Acc@1 54.688 (55.817)	Acc@5 94.141 (94.850)
Epoch: [129][128/196]	Time 0.096 (0.097)	Data 0.000 (0.002)	Loss 1.2238 (1.2050)	Acc@1 57.812 (56.598)	Acc@5 93.750 (94.961)
Epoch: [129][192/196]	Time 0.092 (0.096)	Data 0.000 (0.002)	Loss 1.1866 (1.2020)	Acc@1 56.641 (56.556)	Acc@5 95.312 (94.985)
after train
n1: 30 for:
wAcc: 43.9816530671683
test acc: 55.94
Epoche: [130/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [130][0/196]	Time 0.124 (0.124)	Data 0.266 (0.266)	Loss 1.1386 (1.1386)	Acc@1 56.641 (56.641)	Acc@5 96.875 (96.875)
Epoch: [130][64/196]	Time 0.097 (0.097)	Data 0.000 (0.004)	Loss 1.1851 (1.1758)	Acc@1 56.641 (57.470)	Acc@5 96.875 (95.439)
Epoch: [130][128/196]	Time 0.097 (0.096)	Data 0.000 (0.002)	Loss 1.1760 (1.1772)	Acc@1 58.984 (57.177)	Acc@5 94.531 (95.406)
Epoch: [130][192/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 1.1101 (1.1819)	Acc@1 61.719 (57.195)	Acc@5 94.531 (95.246)
after train
n1: 30 for:
wAcc: 44.945426192032464
test acc: 46.69
Epoche: [131/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [131][0/196]	Time 0.136 (0.136)	Data 0.285 (0.285)	Loss 1.1983 (1.1983)	Acc@1 57.422 (57.422)	Acc@5 97.656 (97.656)
Epoch: [131][64/196]	Time 0.094 (0.096)	Data 0.000 (0.005)	Loss 1.1333 (1.1595)	Acc@1 59.766 (58.299)	Acc@5 96.094 (95.607)
Epoch: [131][128/196]	Time 0.101 (0.096)	Data 0.000 (0.002)	Loss 1.0316 (1.1672)	Acc@1 60.938 (57.952)	Acc@5 96.484 (95.440)
Epoch: [131][192/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 1.1458 (1.1618)	Acc@1 57.812 (58.155)	Acc@5 97.266 (95.489)
after train
n1: 30 for:
wAcc: 45.19531282031438
test acc: 52.15
Epoche: [132/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [132][0/196]	Time 0.124 (0.124)	Data 0.255 (0.255)	Loss 1.1717 (1.1717)	Acc@1 59.766 (59.766)	Acc@5 96.875 (96.875)
Epoch: [132][64/196]	Time 0.093 (0.096)	Data 0.000 (0.004)	Loss 1.1308 (1.1508)	Acc@1 56.641 (58.702)	Acc@5 95.703 (95.583)
Epoch: [132][128/196]	Time 0.093 (0.096)	Data 0.000 (0.002)	Loss 1.2219 (1.1451)	Acc@1 55.469 (58.754)	Acc@5 94.922 (95.682)
Epoch: [132][192/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 1.1226 (1.1473)	Acc@1 57.812 (58.606)	Acc@5 95.703 (95.596)
after train
n1: 30 for:
wAcc: 45.271033498091235
test acc: 47.24
Epoche: [133/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [133][0/196]	Time 0.130 (0.130)	Data 0.257 (0.257)	Loss 1.0913 (1.0913)	Acc@1 60.156 (60.156)	Acc@5 94.531 (94.531)
Epoch: [133][64/196]	Time 0.098 (0.097)	Data 0.000 (0.004)	Loss 1.1387 (1.1367)	Acc@1 56.250 (58.702)	Acc@5 96.484 (95.613)
Epoch: [133][128/196]	Time 0.101 (0.097)	Data 0.000 (0.002)	Loss 1.1278 (1.1400)	Acc@1 58.594 (58.406)	Acc@5 95.703 (95.594)
Epoch: [133][192/196]	Time 0.095 (0.097)	Data 0.000 (0.002)	Loss 1.2210 (1.1376)	Acc@1 60.938 (58.748)	Acc@5 94.922 (95.624)
after train
n1: 30 for:
wAcc: 45.87656234946263
test acc: 56.26
Epoche: [134/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [134][0/196]	Time 0.162 (0.162)	Data 0.238 (0.238)	Loss 1.1556 (1.1556)	Acc@1 54.688 (54.688)	Acc@5 96.875 (96.875)
Epoch: [134][64/196]	Time 0.092 (0.100)	Data 0.000 (0.004)	Loss 1.1446 (1.1305)	Acc@1 61.719 (59.159)	Acc@5 94.922 (95.625)
Epoch: [134][128/196]	Time 0.099 (0.097)	Data 0.000 (0.002)	Loss 1.1942 (1.1261)	Acc@1 54.297 (59.375)	Acc@5 95.703 (95.815)
Epoch: [134][192/196]	Time 0.094 (0.097)	Data 0.000 (0.001)	Loss 1.2093 (1.1264)	Acc@1 51.562 (59.221)	Acc@5 96.094 (95.855)
after train
n1: 30 for:
wAcc: 46.78643373774637
test acc: 54.42
Epoche: [135/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [135][0/196]	Time 0.143 (0.143)	Data 0.284 (0.284)	Loss 1.1633 (1.1633)	Acc@1 56.641 (56.641)	Acc@5 92.188 (92.188)
Epoch: [135][64/196]	Time 0.101 (0.097)	Data 0.000 (0.005)	Loss 1.1605 (1.1178)	Acc@1 60.156 (59.651)	Acc@5 96.484 (95.931)
Epoch: [135][128/196]	Time 0.093 (0.096)	Data 0.000 (0.002)	Loss 1.2218 (1.1157)	Acc@1 57.031 (59.823)	Acc@5 94.531 (95.867)
Epoch: [135][192/196]	Time 0.094 (0.096)	Data 0.000 (0.002)	Loss 1.0447 (1.1096)	Acc@1 63.281 (59.948)	Acc@5 96.484 (95.926)
after train
n1: 30 for:
wAcc: 47.627315236653665
test acc: 50.62
Epoche: [136/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [136][0/196]	Time 0.148 (0.148)	Data 0.260 (0.260)	Loss 1.0678 (1.0678)	Acc@1 60.156 (60.156)	Acc@5 97.656 (97.656)
Epoch: [136][64/196]	Time 0.098 (0.095)	Data 0.000 (0.004)	Loss 1.0649 (1.1043)	Acc@1 62.891 (60.078)	Acc@5 96.875 (96.106)
Epoch: [136][128/196]	Time 0.095 (0.095)	Data 0.000 (0.002)	Loss 1.2323 (1.1022)	Acc@1 54.688 (60.059)	Acc@5 93.359 (96.027)
Epoch: [136][192/196]	Time 0.094 (0.095)	Data 0.000 (0.002)	Loss 1.0776 (1.0986)	Acc@1 62.109 (60.160)	Acc@5 96.484 (96.033)
after train
n1: 30 for:
wAcc: 48.005430466269516
test acc: 57.24
Epoche: [137/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [137][0/196]	Time 0.144 (0.144)	Data 0.252 (0.252)	Loss 1.1588 (1.1588)	Acc@1 60.156 (60.156)	Acc@5 95.312 (95.312)
Epoch: [137][64/196]	Time 0.095 (0.095)	Data 0.000 (0.004)	Loss 1.0933 (1.0886)	Acc@1 61.328 (60.709)	Acc@5 96.484 (95.931)
Epoch: [137][128/196]	Time 0.098 (0.097)	Data 0.000 (0.002)	Loss 1.1724 (1.0880)	Acc@1 55.078 (60.792)	Acc@5 96.484 (96.070)
Epoch: [137][192/196]	Time 0.093 (0.096)	Data 0.000 (0.002)	Loss 1.2274 (1.0849)	Acc@1 53.125 (60.859)	Acc@5 93.359 (96.118)
after train
n1: 30 for:
wAcc: 48.836844484171
test acc: 61.91
Epoche: [138/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [138][0/196]	Time 0.144 (0.144)	Data 0.283 (0.283)	Loss 1.0409 (1.0409)	Acc@1 61.328 (61.328)	Acc@5 96.094 (96.094)
Epoch: [138][64/196]	Time 0.095 (0.096)	Data 0.000 (0.005)	Loss 1.0767 (1.0867)	Acc@1 62.500 (60.715)	Acc@5 96.094 (96.064)
Epoch: [138][128/196]	Time 0.094 (0.096)	Data 0.000 (0.002)	Loss 1.0637 (1.0754)	Acc@1 62.891 (61.198)	Acc@5 94.922 (96.142)
Epoch: [138][192/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 1.1526 (1.0702)	Acc@1 57.031 (61.415)	Acc@5 97.266 (96.292)
after train
n1: 30 for:
wAcc: 49.44463853398304
test acc: 57.34
Epoche: [139/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [139][0/196]	Time 0.154 (0.154)	Data 0.274 (0.274)	Loss 1.1458 (1.1458)	Acc@1 58.594 (58.594)	Acc@5 95.703 (95.703)
Epoch: [139][64/196]	Time 0.093 (0.096)	Data 0.000 (0.004)	Loss 1.0486 (1.0454)	Acc@1 61.328 (62.254)	Acc@5 97.266 (96.665)
Epoch: [139][128/196]	Time 0.090 (0.093)	Data 0.000 (0.002)	Loss 1.0495 (1.0480)	Acc@1 62.891 (62.055)	Acc@5 96.484 (96.630)
Epoch: [139][192/196]	Time 0.092 (0.093)	Data 0.000 (0.002)	Loss 0.9632 (1.0535)	Acc@1 64.062 (61.838)	Acc@5 97.266 (96.545)
after train
n1: 30 for:
wAcc: 50.3891469803821
test acc: 52.87
Epoche: [140/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [140][0/196]	Time 0.131 (0.131)	Data 0.261 (0.261)	Loss 1.1517 (1.1517)	Acc@1 53.906 (53.906)	Acc@5 96.875 (96.875)
Epoch: [140][64/196]	Time 0.096 (0.095)	Data 0.000 (0.004)	Loss 1.0362 (1.0546)	Acc@1 60.547 (61.887)	Acc@5 96.875 (96.412)
Epoch: [140][128/196]	Time 0.093 (0.095)	Data 0.000 (0.002)	Loss 0.9891 (1.0458)	Acc@1 64.062 (62.234)	Acc@5 97.656 (96.433)
Epoch: [140][192/196]	Time 0.091 (0.094)	Data 0.000 (0.002)	Loss 0.9756 (1.0460)	Acc@1 67.578 (62.308)	Acc@5 96.094 (96.420)
after train
n1: 30 for:
wAcc: 50.43355276811151
test acc: 60.63
Epoche: [141/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [141][0/196]	Time 0.153 (0.153)	Data 0.253 (0.253)	Loss 1.0545 (1.0545)	Acc@1 58.594 (58.594)	Acc@5 98.047 (98.047)
Epoch: [141][64/196]	Time 0.095 (0.096)	Data 0.000 (0.004)	Loss 1.0633 (1.0332)	Acc@1 61.719 (62.728)	Acc@5 96.875 (96.484)
Epoch: [141][128/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 1.0019 (1.0343)	Acc@1 62.500 (62.766)	Acc@5 95.312 (96.548)
Epoch: [141][192/196]	Time 0.093 (0.096)	Data 0.000 (0.002)	Loss 1.0200 (1.0343)	Acc@1 58.203 (62.878)	Acc@5 97.656 (96.537)
after train
n1: 30 for:
wAcc: 50.37147151832491
test acc: 52.86
Epoche: [142/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [142][0/196]	Time 0.123 (0.123)	Data 0.245 (0.245)	Loss 1.0848 (1.0848)	Acc@1 58.984 (58.984)	Acc@5 97.266 (97.266)
Epoch: [142][64/196]	Time 0.091 (0.093)	Data 0.000 (0.004)	Loss 0.9272 (1.0247)	Acc@1 67.969 (62.458)	Acc@5 97.266 (96.605)
Epoch: [142][128/196]	Time 0.098 (0.094)	Data 0.000 (0.002)	Loss 1.0383 (1.0179)	Acc@1 62.109 (63.021)	Acc@5 97.656 (96.651)
Epoch: [142][192/196]	Time 0.091 (0.095)	Data 0.000 (0.001)	Loss 1.0851 (1.0186)	Acc@1 60.547 (63.089)	Acc@5 96.484 (96.634)
after train
n1: 30 for:
wAcc: 51.587321110822415
test acc: 56.33
Epoche: [143/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [143][0/196]	Time 0.146 (0.146)	Data 0.267 (0.267)	Loss 1.0290 (1.0290)	Acc@1 62.891 (62.891)	Acc@5 96.875 (96.875)
Epoch: [143][64/196]	Time 0.091 (0.095)	Data 0.000 (0.004)	Loss 0.9240 (1.0057)	Acc@1 64.844 (63.275)	Acc@5 96.875 (96.947)
Epoch: [143][128/196]	Time 0.091 (0.094)	Data 0.000 (0.002)	Loss 1.0630 (1.0053)	Acc@1 61.328 (63.675)	Acc@5 96.094 (96.848)
Epoch: [143][192/196]	Time 0.092 (0.094)	Data 0.000 (0.002)	Loss 0.9442 (1.0084)	Acc@1 68.359 (63.751)	Acc@5 94.922 (96.774)
after train
n1: 30 for:
wAcc: 51.427812179672976
test acc: 55.99
Epoche: [144/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [144][0/196]	Time 0.126 (0.126)	Data 0.269 (0.269)	Loss 0.9811 (0.9811)	Acc@1 64.844 (64.844)	Acc@5 96.484 (96.484)
Epoch: [144][64/196]	Time 0.095 (0.095)	Data 0.000 (0.004)	Loss 1.0514 (0.9981)	Acc@1 61.328 (63.540)	Acc@5 97.656 (96.605)
Epoch: [144][128/196]	Time 0.095 (0.095)	Data 0.000 (0.002)	Loss 1.0153 (0.9928)	Acc@1 62.500 (64.087)	Acc@5 97.656 (96.757)
Epoch: [144][192/196]	Time 0.095 (0.095)	Data 0.000 (0.002)	Loss 1.0559 (0.9907)	Acc@1 63.281 (64.255)	Acc@5 96.094 (96.911)
after train
n1: 30 for:
wAcc: 51.99247698980285
test acc: 51.78
Epoche: [145/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [145][0/196]	Time 0.146 (0.146)	Data 0.276 (0.276)	Loss 0.9104 (0.9104)	Acc@1 69.531 (69.531)	Acc@5 96.875 (96.875)
Epoch: [145][64/196]	Time 0.092 (0.095)	Data 0.000 (0.004)	Loss 0.8950 (0.9830)	Acc@1 65.625 (64.591)	Acc@5 98.047 (97.043)
Epoch: [145][128/196]	Time 0.093 (0.094)	Data 0.000 (0.002)	Loss 0.9012 (0.9721)	Acc@1 66.406 (64.907)	Acc@5 97.266 (97.048)
Epoch: [145][192/196]	Time 0.092 (0.093)	Data 0.000 (0.002)	Loss 0.9233 (0.9703)	Acc@1 66.016 (64.888)	Acc@5 97.656 (97.015)
after train
n1: 30 for:
wAcc: 52.32716214986777
test acc: 50.16
Epoche: [146/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [146][0/196]	Time 0.124 (0.124)	Data 0.289 (0.289)	Loss 0.7574 (0.7574)	Acc@1 72.656 (72.656)	Acc@5 98.047 (98.047)
Epoch: [146][64/196]	Time 0.096 (0.097)	Data 0.000 (0.005)	Loss 1.0266 (0.9736)	Acc@1 62.500 (65.126)	Acc@5 96.875 (96.893)
Epoch: [146][128/196]	Time 0.091 (0.094)	Data 0.000 (0.002)	Loss 1.0370 (0.9640)	Acc@1 65.234 (65.331)	Acc@5 97.656 (96.987)
Epoch: [146][192/196]	Time 0.096 (0.095)	Data 0.000 (0.002)	Loss 1.0042 (0.9624)	Acc@1 66.016 (65.344)	Acc@5 95.703 (97.045)
after train
n1: 30 for:
wAcc: 52.595008828398115
test acc: 55.66
Epoche: [147/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [147][0/196]	Time 0.148 (0.148)	Data 0.249 (0.249)	Loss 1.0064 (1.0064)	Acc@1 62.109 (62.109)	Acc@5 96.094 (96.094)
Epoch: [147][64/196]	Time 0.097 (0.099)	Data 0.000 (0.004)	Loss 1.0226 (0.9411)	Acc@1 67.188 (66.010)	Acc@5 94.922 (97.392)
Epoch: [147][128/196]	Time 0.094 (0.098)	Data 0.000 (0.002)	Loss 0.8864 (0.9442)	Acc@1 67.969 (65.804)	Acc@5 98.438 (97.402)
Epoch: [147][192/196]	Time 0.093 (0.097)	Data 0.000 (0.002)	Loss 1.0251 (0.9421)	Acc@1 64.844 (65.943)	Acc@5 97.656 (97.371)
after train
n1: 30 for:
wAcc: 52.344609366854876
test acc: 50.92
Epoche: [148/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [148][0/196]	Time 0.146 (0.146)	Data 0.244 (0.244)	Loss 0.9416 (0.9416)	Acc@1 63.281 (63.281)	Acc@5 98.438 (98.438)
Epoch: [148][64/196]	Time 0.094 (0.097)	Data 0.000 (0.004)	Loss 1.0173 (0.9582)	Acc@1 61.719 (65.565)	Acc@5 96.875 (97.157)
Epoch: [148][128/196]	Time 0.096 (0.097)	Data 0.000 (0.002)	Loss 1.0256 (0.9339)	Acc@1 65.625 (66.476)	Acc@5 94.141 (97.329)
Epoch: [148][192/196]	Time 0.095 (0.097)	Data 0.000 (0.001)	Loss 0.8634 (0.9304)	Acc@1 71.094 (66.574)	Acc@5 96.875 (97.294)
after train
n1: 30 for:
wAcc: 52.573625742201585
test acc: 47.87
Epoche: [149/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [149][0/196]	Time 0.123 (0.123)	Data 0.257 (0.257)	Loss 0.8059 (0.8059)	Acc@1 69.531 (69.531)	Acc@5 98.047 (98.047)
Epoch: [149][64/196]	Time 0.094 (0.095)	Data 0.000 (0.004)	Loss 0.8670 (0.9094)	Acc@1 67.969 (67.109)	Acc@5 98.828 (97.512)
Epoch: [149][128/196]	Time 0.096 (0.095)	Data 0.000 (0.002)	Loss 0.8980 (0.9067)	Acc@1 65.234 (67.393)	Acc@5 97.266 (97.487)
Epoch: [149][192/196]	Time 0.094 (0.095)	Data 0.000 (0.002)	Loss 0.9054 (0.8995)	Acc@1 69.922 (67.801)	Acc@5 97.266 (97.559)
after train
n1: 30 for:
wAcc: 52.20945016285623
test acc: 55.86
Epoche: [150/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [150][0/196]	Time 0.130 (0.130)	Data 0.274 (0.274)	Loss 0.8481 (0.8481)	Acc@1 67.969 (67.969)	Acc@5 97.266 (97.266)
Epoch: [150][64/196]	Time 0.093 (0.098)	Data 0.000 (0.004)	Loss 0.9260 (0.8855)	Acc@1 64.844 (68.203)	Acc@5 95.703 (97.614)
Epoch: [150][128/196]	Time 0.099 (0.097)	Data 0.000 (0.002)	Loss 0.9151 (0.8851)	Acc@1 66.797 (68.232)	Acc@5 98.047 (97.614)
Epoch: [150][192/196]	Time 0.094 (0.097)	Data 0.000 (0.002)	Loss 0.8258 (0.8818)	Acc@1 68.750 (68.319)	Acc@5 98.047 (97.652)
after train
n1: 30 for:
wAcc: 52.781797935564086
test acc: 46.9
Epoche: [151/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [151][0/196]	Time 0.122 (0.122)	Data 0.295 (0.295)	Loss 0.8514 (0.8514)	Acc@1 67.969 (67.969)	Acc@5 98.438 (98.438)
Epoch: [151][64/196]	Time 0.095 (0.098)	Data 0.000 (0.005)	Loss 0.8922 (0.8718)	Acc@1 67.188 (68.744)	Acc@5 97.266 (97.674)
Epoch: [151][128/196]	Time 0.097 (0.097)	Data 0.000 (0.003)	Loss 0.8032 (0.8610)	Acc@1 74.609 (69.156)	Acc@5 98.438 (97.659)
Epoch: [151][192/196]	Time 0.094 (0.097)	Data 0.000 (0.002)	Loss 0.9750 (0.8595)	Acc@1 66.016 (69.191)	Acc@5 95.703 (97.749)
after train
n1: 30 for:
wAcc: 51.79805979173617
test acc: 58.99
Epoche: [152/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [152][0/196]	Time 0.143 (0.143)	Data 0.262 (0.262)	Loss 0.8273 (0.8273)	Acc@1 68.750 (68.750)	Acc@5 99.609 (99.609)
Epoch: [152][64/196]	Time 0.095 (0.097)	Data 0.000 (0.004)	Loss 0.7491 (0.8557)	Acc@1 74.219 (69.056)	Acc@5 98.828 (97.903)
Epoch: [152][128/196]	Time 0.096 (0.096)	Data 0.000 (0.002)	Loss 1.0133 (0.8500)	Acc@1 64.453 (69.419)	Acc@5 94.922 (97.895)
Epoch: [152][192/196]	Time 0.094 (0.096)	Data 0.000 (0.002)	Loss 0.9049 (0.8451)	Acc@1 73.438 (69.748)	Acc@5 96.875 (97.838)
after train
n1: 30 for:
wAcc: 51.85583795835219
test acc: 63.04
Epoche: [153/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [153][0/196]	Time 0.142 (0.142)	Data 0.288 (0.288)	Loss 0.8371 (0.8371)	Acc@1 70.703 (70.703)	Acc@5 97.656 (97.656)
Epoch: [153][64/196]	Time 0.092 (0.095)	Data 0.000 (0.005)	Loss 0.8375 (0.8352)	Acc@1 70.703 (70.367)	Acc@5 96.875 (97.915)
Epoch: [153][128/196]	Time 0.093 (0.094)	Data 0.000 (0.002)	Loss 0.8571 (0.8242)	Acc@1 70.703 (70.776)	Acc@5 97.266 (97.968)
Epoch: [153][192/196]	Time 0.089 (0.094)	Data 0.000 (0.002)	Loss 0.8490 (0.8198)	Acc@1 71.484 (70.875)	Acc@5 98.047 (97.992)
after train
n1: 30 for:
wAcc: 53.05589555422289
test acc: 64.19
Epoche: [154/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [154][0/196]	Time 0.141 (0.141)	Data 0.241 (0.241)	Loss 0.7784 (0.7784)	Acc@1 73.828 (73.828)	Acc@5 98.047 (98.047)
Epoch: [154][64/196]	Time 0.093 (0.094)	Data 0.000 (0.004)	Loss 0.8093 (0.8021)	Acc@1 68.359 (71.611)	Acc@5 96.875 (97.999)
Epoch: [154][128/196]	Time 0.097 (0.094)	Data 0.000 (0.002)	Loss 0.7794 (0.7973)	Acc@1 73.047 (71.839)	Acc@5 98.438 (98.086)
Epoch: [154][192/196]	Time 0.093 (0.095)	Data 0.000 (0.001)	Loss 0.7434 (0.7963)	Acc@1 75.000 (71.913)	Acc@5 97.656 (98.104)
after train
n1: 30 for:
wAcc: 55.21261236787235
test acc: 54.36
Epoche: [155/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [155][0/196]	Time 0.153 (0.153)	Data 0.252 (0.252)	Loss 0.7888 (0.7888)	Acc@1 72.656 (72.656)	Acc@5 97.656 (97.656)
Epoch: [155][64/196]	Time 0.094 (0.095)	Data 0.000 (0.004)	Loss 0.7285 (0.7680)	Acc@1 73.047 (72.596)	Acc@5 98.047 (98.281)
Epoch: [155][128/196]	Time 0.092 (0.096)	Data 0.000 (0.002)	Loss 0.7700 (0.7791)	Acc@1 72.656 (72.290)	Acc@5 99.219 (98.241)
Epoch: [155][192/196]	Time 0.092 (0.095)	Data 0.000 (0.002)	Loss 0.6931 (0.7819)	Acc@1 73.438 (72.221)	Acc@5 98.828 (98.158)
after train
n1: 30 for:
wAcc: 54.42034117639331
test acc: 62.33
Epoche: [156/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [156][0/196]	Time 0.154 (0.154)	Data 0.266 (0.266)	Loss 0.6958 (0.6958)	Acc@1 76.953 (76.953)	Acc@5 97.656 (97.656)
Epoch: [156][64/196]	Time 0.095 (0.094)	Data 0.000 (0.004)	Loss 0.8135 (0.7574)	Acc@1 70.703 (73.498)	Acc@5 96.875 (98.161)
Epoch: [156][128/196]	Time 0.097 (0.096)	Data 0.000 (0.002)	Loss 0.6249 (0.7638)	Acc@1 74.609 (73.077)	Acc@5 99.219 (98.213)
Epoch: [156][192/196]	Time 0.094 (0.096)	Data 0.000 (0.002)	Loss 0.6732 (0.7534)	Acc@1 78.125 (73.419)	Acc@5 100.000 (98.308)
after train
n1: 30 for:
wAcc: 55.41347734684961
test acc: 59.99
Epoche: [157/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [157][0/196]	Time 0.128 (0.128)	Data 0.263 (0.263)	Loss 0.7508 (0.7508)	Acc@1 73.828 (73.828)	Acc@5 98.438 (98.438)
Epoch: [157][64/196]	Time 0.097 (0.097)	Data 0.000 (0.004)	Loss 0.8418 (0.7401)	Acc@1 71.484 (74.177)	Acc@5 98.047 (98.215)
Epoch: [157][128/196]	Time 0.094 (0.096)	Data 0.000 (0.002)	Loss 0.6276 (0.7388)	Acc@1 78.516 (74.316)	Acc@5 99.219 (98.226)
Epoch: [157][192/196]	Time 0.093 (0.096)	Data 0.000 (0.002)	Loss 0.7198 (0.7363)	Acc@1 72.656 (74.344)	Acc@5 98.438 (98.263)
after train
n1: 30 for:
wAcc: 55.090013407859615
test acc: 64.98
Epoche: [158/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [158][0/196]	Time 0.151 (0.151)	Data 0.238 (0.238)	Loss 0.6594 (0.6594)	Acc@1 75.391 (75.391)	Acc@5 98.828 (98.828)
Epoch: [158][64/196]	Time 0.094 (0.097)	Data 0.000 (0.004)	Loss 0.7298 (0.7189)	Acc@1 73.828 (74.688)	Acc@5 98.047 (98.365)
Epoch: [158][128/196]	Time 0.099 (0.096)	Data 0.000 (0.002)	Loss 0.7922 (0.7191)	Acc@1 69.922 (74.485)	Acc@5 97.656 (98.419)
Epoch: [158][192/196]	Time 0.093 (0.097)	Data 0.000 (0.001)	Loss 0.7012 (0.7154)	Acc@1 77.344 (74.773)	Acc@5 96.875 (98.415)
after train
n1: 30 for:
wAcc: 56.586772708988406
test acc: 53.83
Epoche: [159/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [159][0/196]	Time 0.126 (0.126)	Data 0.239 (0.239)	Loss 0.7118 (0.7118)	Acc@1 71.875 (71.875)	Acc@5 98.828 (98.828)
Epoch: [159][64/196]	Time 0.096 (0.097)	Data 0.000 (0.004)	Loss 0.7484 (0.7022)	Acc@1 73.828 (75.246)	Acc@5 98.047 (98.510)
Epoch: [159][128/196]	Time 0.094 (0.096)	Data 0.000 (0.002)	Loss 0.7055 (0.7038)	Acc@1 75.781 (75.200)	Acc@5 98.828 (98.547)
Epoch: [159][192/196]	Time 0.095 (0.096)	Data 0.000 (0.001)	Loss 0.6052 (0.6992)	Acc@1 76.953 (75.437)	Acc@5 99.219 (98.525)
after train
n1: 30 for:
wAcc: 55.071722000685696
test acc: 58.12
Epoche: [160/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [160][0/196]	Time 0.151 (0.151)	Data 0.276 (0.276)	Loss 0.6925 (0.6925)	Acc@1 76.953 (76.953)	Acc@5 98.438 (98.438)
Epoch: [160][64/196]	Time 0.092 (0.096)	Data 0.000 (0.004)	Loss 0.7925 (0.6985)	Acc@1 72.266 (75.541)	Acc@5 96.875 (98.540)
Epoch: [160][128/196]	Time 0.094 (0.096)	Data 0.000 (0.002)	Loss 0.6934 (0.6924)	Acc@1 77.734 (75.790)	Acc@5 96.875 (98.540)
Epoch: [160][192/196]	Time 0.093 (0.095)	Data 0.000 (0.002)	Loss 0.6730 (0.6899)	Acc@1 75.781 (75.880)	Acc@5 98.047 (98.523)
after train
n1: 30 for:
wAcc: 56.05769119996199
test acc: 68.53
Epoche: [161/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [161][0/196]	Time 0.145 (0.145)	Data 0.269 (0.269)	Loss 0.7567 (0.7567)	Acc@1 70.312 (70.312)	Acc@5 98.828 (98.828)
Epoch: [161][64/196]	Time 0.097 (0.095)	Data 0.000 (0.004)	Loss 0.6481 (0.6977)	Acc@1 77.344 (75.421)	Acc@5 99.609 (98.558)
Epoch: [161][128/196]	Time 0.092 (0.095)	Data 0.000 (0.002)	Loss 0.6945 (0.6901)	Acc@1 74.609 (75.902)	Acc@5 99.219 (98.583)
Epoch: [161][192/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 0.7135 (0.6789)	Acc@1 75.000 (76.352)	Acc@5 97.656 (98.555)
after train
n1: 30 for:
wAcc: 56.152559037772676
test acc: 67.12
Epoche: [162/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [162][0/196]	Time 0.129 (0.129)	Data 0.243 (0.243)	Loss 0.6700 (0.6700)	Acc@1 75.781 (75.781)	Acc@5 98.828 (98.828)
Epoch: [162][64/196]	Time 0.092 (0.094)	Data 0.000 (0.004)	Loss 0.6447 (0.6625)	Acc@1 78.125 (77.097)	Acc@5 99.609 (98.690)
Epoch: [162][128/196]	Time 0.098 (0.095)	Data 0.000 (0.002)	Loss 0.5691 (0.6541)	Acc@1 80.859 (77.410)	Acc@5 99.609 (98.692)
Epoch: [162][192/196]	Time 0.094 (0.096)	Data 0.000 (0.001)	Loss 0.7476 (0.6580)	Acc@1 75.000 (77.269)	Acc@5 97.266 (98.614)
after train
n1: 30 for:
wAcc: 58.16408112037655
test acc: 68.62
Epoche: [163/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [163][0/196]	Time 0.130 (0.130)	Data 0.246 (0.246)	Loss 0.5685 (0.5685)	Acc@1 80.859 (80.859)	Acc@5 99.609 (99.609)
Epoch: [163][64/196]	Time 0.098 (0.096)	Data 0.000 (0.004)	Loss 0.7462 (0.6445)	Acc@1 73.828 (77.680)	Acc@5 97.266 (98.672)
Epoch: [163][128/196]	Time 0.095 (0.097)	Data 0.000 (0.002)	Loss 0.5865 (0.6442)	Acc@1 80.078 (77.786)	Acc@5 98.828 (98.665)
Epoch: [163][192/196]	Time 0.093 (0.097)	Data 0.000 (0.001)	Loss 0.6465 (0.6406)	Acc@1 78.906 (77.819)	Acc@5 98.047 (98.666)
after train
n1: 30 for:
wAcc: 58.572663266638195
test acc: 60.9
Epoche: [164/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [164][0/196]	Time 0.141 (0.141)	Data 0.286 (0.286)	Loss 0.6912 (0.6912)	Acc@1 76.562 (76.562)	Acc@5 98.438 (98.438)
Epoch: [164][64/196]	Time 0.093 (0.095)	Data 0.000 (0.005)	Loss 0.6350 (0.6531)	Acc@1 77.734 (77.145)	Acc@5 100.000 (98.732)
Epoch: [164][128/196]	Time 0.097 (0.096)	Data 0.000 (0.002)	Loss 0.6104 (0.6439)	Acc@1 81.250 (77.750)	Acc@5 99.219 (98.771)
Epoch: [164][192/196]	Time 0.093 (0.096)	Data 0.000 (0.002)	Loss 0.6278 (0.6405)	Acc@1 79.688 (77.771)	Acc@5 98.047 (98.788)
after train
n1: 30 for:
wAcc: 58.17348010610618
test acc: 63.21
Epoche: [165/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [165][0/196]	Time 0.151 (0.151)	Data 0.246 (0.246)	Loss 0.6392 (0.6392)	Acc@1 76.953 (76.953)	Acc@5 98.047 (98.047)
Epoch: [165][64/196]	Time 0.094 (0.097)	Data 0.000 (0.004)	Loss 0.7400 (0.6354)	Acc@1 75.000 (78.071)	Acc@5 97.656 (98.696)
Epoch: [165][128/196]	Time 0.102 (0.098)	Data 0.000 (0.002)	Loss 0.6384 (0.6343)	Acc@1 75.391 (77.958)	Acc@5 98.828 (98.683)
Epoch: [165][192/196]	Time 0.097 (0.097)	Data 0.000 (0.001)	Loss 0.5874 (0.6331)	Acc@1 79.688 (78.089)	Acc@5 98.438 (98.723)
after train
n1: 30 for:
wAcc: 59.45541438240232
test acc: 66.94
Epoche: [166/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [166][0/196]	Time 0.122 (0.122)	Data 0.279 (0.279)	Loss 0.5805 (0.5805)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [166][64/196]	Time 0.100 (0.096)	Data 0.000 (0.005)	Loss 0.6784 (0.6213)	Acc@1 77.344 (78.654)	Acc@5 98.828 (98.630)
Epoch: [166][128/196]	Time 0.091 (0.096)	Data 0.000 (0.002)	Loss 0.5004 (0.6152)	Acc@1 84.375 (78.688)	Acc@5 98.438 (98.710)
Epoch: [166][192/196]	Time 0.092 (0.095)	Data 0.000 (0.002)	Loss 0.5614 (0.6158)	Acc@1 80.469 (78.629)	Acc@5 100.000 (98.713)
after train
n1: 30 for:
wAcc: 60.613393346184665
test acc: 67.3
Epoche: [167/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [167][0/196]	Time 0.131 (0.131)	Data 0.291 (0.291)	Loss 0.5388 (0.5388)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [167][64/196]	Time 0.093 (0.097)	Data 0.000 (0.005)	Loss 0.6052 (0.5978)	Acc@1 78.906 (79.441)	Acc@5 98.828 (98.786)
Epoch: [167][128/196]	Time 0.093 (0.096)	Data 0.000 (0.002)	Loss 0.5531 (0.6007)	Acc@1 80.469 (79.363)	Acc@5 99.609 (98.758)
Epoch: [167][192/196]	Time 0.089 (0.095)	Data 0.000 (0.002)	Loss 0.5897 (0.6021)	Acc@1 78.125 (79.287)	Acc@5 98.438 (98.780)
after train
n1: 30 for:
wAcc: 60.38414100725003
test acc: 68.58
Epoche: [168/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [168][0/196]	Time 0.153 (0.153)	Data 0.244 (0.244)	Loss 0.5301 (0.5301)	Acc@1 80.469 (80.469)	Acc@5 99.219 (99.219)
Epoch: [168][64/196]	Time 0.100 (0.094)	Data 0.000 (0.004)	Loss 0.5788 (0.5915)	Acc@1 80.859 (79.339)	Acc@5 99.219 (98.792)
Epoch: [168][128/196]	Time 0.092 (0.093)	Data 0.000 (0.002)	Loss 0.4878 (0.5956)	Acc@1 83.984 (79.258)	Acc@5 99.219 (98.834)
Epoch: [168][192/196]	Time 0.093 (0.093)	Data 0.000 (0.001)	Loss 0.6682 (0.5984)	Acc@1 79.688 (79.224)	Acc@5 99.609 (98.852)
after train
n1: 30 for:
wAcc: 60.26671594268062
test acc: 71.87
Epoche: [169/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [169][0/196]	Time 0.132 (0.132)	Data 0.268 (0.268)	Loss 0.5518 (0.5518)	Acc@1 80.859 (80.859)	Acc@5 98.438 (98.438)
Epoch: [169][64/196]	Time 0.097 (0.098)	Data 0.000 (0.004)	Loss 0.5580 (0.6089)	Acc@1 80.469 (78.966)	Acc@5 99.609 (98.792)
Epoch: [169][128/196]	Time 0.096 (0.097)	Data 0.000 (0.002)	Loss 0.6627 (0.5976)	Acc@1 76.172 (79.427)	Acc@5 98.438 (98.880)
Epoch: [169][192/196]	Time 0.096 (0.097)	Data 0.000 (0.002)	Loss 0.5806 (0.5958)	Acc@1 77.734 (79.439)	Acc@5 100.000 (98.856)
after train
n1: 30 for:
wAcc: 62.13711259832566
test acc: 54.56
Epoche: [170/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [170][0/196]	Time 0.119 (0.119)	Data 0.273 (0.273)	Loss 0.5289 (0.5289)	Acc@1 82.422 (82.422)	Acc@5 98.828 (98.828)
Epoch: [170][64/196]	Time 0.094 (0.095)	Data 0.000 (0.004)	Loss 0.6034 (0.5829)	Acc@1 76.953 (79.850)	Acc@5 98.828 (98.900)
Epoch: [170][128/196]	Time 0.097 (0.096)	Data 0.000 (0.002)	Loss 0.6633 (0.5803)	Acc@1 80.078 (80.024)	Acc@5 97.656 (98.871)
Epoch: [170][192/196]	Time 0.097 (0.096)	Data 0.000 (0.002)	Loss 0.5826 (0.5790)	Acc@1 81.250 (80.131)	Acc@5 98.828 (98.921)
after train
n1: 30 for:
wAcc: 60.52502332446263
test acc: 60.54
Epoche: [171/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [171][0/196]	Time 0.143 (0.143)	Data 0.269 (0.269)	Loss 0.5659 (0.5659)	Acc@1 81.641 (81.641)	Acc@5 98.828 (98.828)
Epoch: [171][64/196]	Time 0.094 (0.098)	Data 0.000 (0.004)	Loss 0.5967 (0.5692)	Acc@1 79.688 (80.403)	Acc@5 99.609 (98.930)
Epoch: [171][128/196]	Time 0.096 (0.096)	Data 0.000 (0.002)	Loss 0.4852 (0.5787)	Acc@1 86.719 (80.117)	Acc@5 99.219 (98.867)
Epoch: [171][192/196]	Time 0.093 (0.096)	Data 0.000 (0.002)	Loss 0.4694 (0.5740)	Acc@1 84.766 (80.246)	Acc@5 98.828 (98.931)
after train
n1: 30 for:
wAcc: 61.02761816522701
test acc: 69.21
Epoche: [172/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [172][0/196]	Time 0.145 (0.145)	Data 0.257 (0.257)	Loss 0.5534 (0.5534)	Acc@1 80.469 (80.469)	Acc@5 98.438 (98.438)
Epoch: [172][64/196]	Time 0.093 (0.095)	Data 0.000 (0.004)	Loss 0.5513 (0.5576)	Acc@1 82.422 (80.956)	Acc@5 98.828 (98.942)
Epoch: [172][128/196]	Time 0.094 (0.094)	Data 0.000 (0.002)	Loss 0.6320 (0.5703)	Acc@1 77.734 (80.578)	Acc@5 98.047 (98.892)
Epoch: [172][192/196]	Time 0.092 (0.094)	Data 0.000 (0.002)	Loss 0.5202 (0.5715)	Acc@1 83.594 (80.414)	Acc@5 98.828 (98.909)
after train
n1: 30 for:
wAcc: 61.506362838007846
test acc: 62.41
Epoche: [173/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [173][0/196]	Time 0.157 (0.157)	Data 0.265 (0.265)	Loss 0.6098 (0.6098)	Acc@1 78.125 (78.125)	Acc@5 99.609 (99.609)
Epoch: [173][64/196]	Time 0.098 (0.096)	Data 0.000 (0.004)	Loss 0.5772 (0.5671)	Acc@1 79.688 (80.186)	Acc@5 99.219 (99.050)
Epoch: [173][128/196]	Time 0.096 (0.096)	Data 0.000 (0.002)	Loss 0.6099 (0.5604)	Acc@1 78.125 (80.469)	Acc@5 99.609 (99.016)
Epoch: [173][192/196]	Time 0.093 (0.097)	Data 0.000 (0.002)	Loss 0.5374 (0.5598)	Acc@1 83.594 (80.596)	Acc@5 99.219 (99.002)
after train
n1: 30 for:
wAcc: 60.95605785375659
test acc: 65.68
Epoche: [174/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [174][0/196]	Time 0.149 (0.149)	Data 0.243 (0.243)	Loss 0.6722 (0.6722)	Acc@1 77.344 (77.344)	Acc@5 97.656 (97.656)
Epoch: [174][64/196]	Time 0.091 (0.097)	Data 0.000 (0.004)	Loss 0.5265 (0.5565)	Acc@1 83.594 (80.721)	Acc@5 99.219 (99.038)
Epoch: [174][128/196]	Time 0.092 (0.095)	Data 0.000 (0.002)	Loss 0.5995 (0.5504)	Acc@1 80.469 (81.111)	Acc@5 99.609 (99.037)
Epoch: [174][192/196]	Time 0.091 (0.094)	Data 0.000 (0.001)	Loss 0.5703 (0.5545)	Acc@1 80.078 (80.839)	Acc@5 99.609 (99.051)
after train
n1: 30 for:
wAcc: 61.02663859207101
test acc: 74.22
Epoche: [175/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [175][0/196]	Time 0.152 (0.152)	Data 0.269 (0.269)	Loss 0.5164 (0.5164)	Acc@1 80.469 (80.469)	Acc@5 100.000 (100.000)
Epoch: [175][64/196]	Time 0.097 (0.095)	Data 0.000 (0.004)	Loss 0.4164 (0.5481)	Acc@1 85.156 (80.974)	Acc@5 98.828 (98.996)
Epoch: [175][128/196]	Time 0.094 (0.095)	Data 0.000 (0.002)	Loss 0.5642 (0.5482)	Acc@1 77.734 (80.890)	Acc@5 99.219 (99.034)
Epoch: [175][192/196]	Time 0.095 (0.095)	Data 0.000 (0.002)	Loss 0.5534 (0.5549)	Acc@1 80.469 (80.669)	Acc@5 98.438 (99.004)
after train
n1: 30 for:
wAcc: 62.67291176387024
test acc: 75.44
Epoche: [176/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [176][0/196]	Time 0.112 (0.112)	Data 0.250 (0.250)	Loss 0.6150 (0.6150)	Acc@1 79.297 (79.297)	Acc@5 97.266 (97.266)
Epoch: [176][64/196]	Time 0.099 (0.097)	Data 0.000 (0.004)	Loss 0.4700 (0.5482)	Acc@1 85.156 (81.064)	Acc@5 99.609 (98.900)
Epoch: [176][128/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 0.5704 (0.5486)	Acc@1 80.469 (81.056)	Acc@5 99.219 (98.986)
Epoch: [176][192/196]	Time 0.095 (0.096)	Data 0.000 (0.002)	Loss 0.5031 (0.5449)	Acc@1 82.031 (81.282)	Acc@5 98.828 (98.988)
after train
n1: 30 for:
wAcc: 62.81137309454718
test acc: 76.43
Epoche: [177/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [177][0/196]	Time 0.145 (0.145)	Data 0.269 (0.269)	Loss 0.5106 (0.5106)	Acc@1 80.859 (80.859)	Acc@5 98.828 (98.828)
Epoch: [177][64/196]	Time 0.091 (0.093)	Data 0.000 (0.004)	Loss 0.5161 (0.5426)	Acc@1 81.250 (81.352)	Acc@5 98.828 (98.942)
Epoch: [177][128/196]	Time 0.098 (0.095)	Data 0.000 (0.002)	Loss 0.5609 (0.5466)	Acc@1 80.859 (81.086)	Acc@5 97.656 (98.967)
Epoch: [177][192/196]	Time 0.095 (0.095)	Data 0.000 (0.002)	Loss 0.5920 (0.5413)	Acc@1 79.297 (81.205)	Acc@5 99.609 (99.031)
after train
n1: 30 for:
wAcc: 63.24908143563063
test acc: 70.19
Epoche: [178/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [178][0/196]	Time 0.132 (0.132)	Data 0.277 (0.277)	Loss 0.5633 (0.5633)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [178][64/196]	Time 0.094 (0.097)	Data 0.000 (0.004)	Loss 0.6197 (0.5453)	Acc@1 78.125 (81.238)	Acc@5 98.828 (99.093)
Epoch: [178][128/196]	Time 0.102 (0.097)	Data 0.000 (0.002)	Loss 0.5374 (0.5434)	Acc@1 82.812 (81.265)	Acc@5 98.828 (99.034)
Epoch: [178][192/196]	Time 0.094 (0.096)	Data 0.000 (0.002)	Loss 0.5682 (0.5357)	Acc@1 78.906 (81.570)	Acc@5 98.828 (99.055)
after train
n1: 30 for:
wAcc: 64.85192947570283
test acc: 63.41
Epoche: [179/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [179][0/196]	Time 0.126 (0.126)	Data 0.249 (0.249)	Loss 0.5999 (0.5999)	Acc@1 76.953 (76.953)	Acc@5 99.609 (99.609)
Epoch: [179][64/196]	Time 0.094 (0.096)	Data 0.000 (0.004)	Loss 0.5882 (0.5313)	Acc@1 78.516 (81.725)	Acc@5 99.219 (99.135)
Epoch: [179][128/196]	Time 0.098 (0.096)	Data 0.000 (0.002)	Loss 0.5377 (0.5292)	Acc@1 81.250 (81.710)	Acc@5 98.438 (99.079)
Epoch: [179][192/196]	Time 0.091 (0.096)	Data 0.000 (0.002)	Loss 0.5336 (0.5319)	Acc@1 82.812 (81.645)	Acc@5 98.047 (99.031)
after train
n1: 30 for:
wAcc: 63.4636302146964
test acc: 73.41
Epoche: [180/180]; Lr: 0.1
batch Size 256
befor train
Epoch: [180][0/196]	Time 0.157 (0.157)	Data 0.241 (0.241)	Loss 0.6203 (0.6203)	Acc@1 79.688 (79.688)	Acc@5 98.438 (98.438)
Epoch: [180][64/196]	Time 0.094 (0.099)	Data 0.000 (0.004)	Loss 0.5091 (0.5277)	Acc@1 84.375 (81.785)	Acc@5 100.000 (99.189)
Epoch: [180][128/196]	Time 0.099 (0.098)	Data 0.000 (0.002)	Loss 0.5418 (0.5285)	Acc@1 79.297 (81.728)	Acc@5 99.219 (99.179)
Epoch: [180][192/196]	Time 0.092 (0.097)	Data 0.000 (0.001)	Loss 0.5111 (0.5232)	Acc@1 82.031 (81.851)	Acc@5 98.828 (99.142)
after train
n1: 30 for:
wAcc: 65.85308071823454
test acc: 75.19
Max memory: 101.188096
 19.316s  